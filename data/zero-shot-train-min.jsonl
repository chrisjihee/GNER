{"id":"0","dataset":"crossner_ai","split":"train","label_list":["programming language","country","task","algorithm","metric","location","field","university","organization","researcher","person","conference","product"],"instance":{"id":"0","words":["Popular","approaches","of","opinion-based","recommender","system","utilize","various","techniques","including","text","mining",",","information","retrieval",",","sentiment","analysis","(","see","also","Multimodal","sentiment","analysis",")","and","deep","learning","X.Y.","Feng",",","H.","Zhang",",","Y.J.","Ren",",","P.H.","Shang",",","Y.","Zhu",",","Y.C.","Liang",",","R.C.","Guan",",","D.","Xu",",","(","2019",")",",",",","21","(","5",")",":","e12957","."],"labels":["O","O","O","B-product","I-product","I-product","O","O","O","O","B-field","I-field","O","B-task","I-task","O","B-task","I-task","O","O","O","B-task","I-task","I-task","O","O","B-field","I-field","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, country, task, algorithm, metric, location, field, university, organization, researcher, person, conference, product and O.\nSentence: Popular approaches of opinion-based recommender system utilize various techniques including text mining , information retrieval , sentiment analysis ( see also Multimodal sentiment analysis ) and deep learning X.Y. Feng , H. Zhang , Y.J. Ren , P.H. Shang , Y. Zhu , Y.C. Liang , R.C. Guan , D. Xu , ( 2019 ) , , 21 ( 5 ) : e12957 .","prompt_labels":"Popular(O) approaches(O) of(O) opinion-based(B-product) recommender(I-product) system(I-product) utilize(O) various(O) techniques(O) including(O) text(B-field) mining(I-field) ,(O) information(B-task) retrieval(I-task) ,(O) sentiment(B-task) analysis(I-task) ((O) see(O) also(O) Multimodal(B-task) sentiment(I-task) analysis(I-task) )(O) and(O) deep(B-field) learning(I-field) X.Y.(B-researcher) Feng(I-researcher) ,(O) H.(B-researcher) Zhang(I-researcher) ,(O) Y.J.(B-researcher) Ren(I-researcher) ,(O) P.H.(B-researcher) Shang(I-researcher) ,(O) Y.(B-researcher) Zhu(I-researcher) ,(O) Y.C.(B-researcher) Liang(I-researcher) ,(O) R.C.(B-researcher) Guan(I-researcher) ,(O) D.(B-researcher) Xu(I-researcher) ,(O) ((O) 2019(O) )(O) ,(O) ,(O) 21(O) ((O) 5(O) )(O) :(O) e12957(O) .(O)"}}
{"id":"1","dataset":"crossner_ai","split":"train","label_list":["organization","metric","university","researcher","person","product","algorithm","location","conference","field","task","country","programming language"],"instance":{"id":"1","words":["Advocates","of","procedural","representations","were","mainly","centered","at","MIT",",","under","the","leadership","of","Marvin","Minsky","and","Seymour","Papert","."],"labels":["O","O","O","O","O","O","O","O","B-university","O","O","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, metric, university, researcher, person, product, algorithm, location, conference, field, task, country, programming language and O.\nSentence: Advocates of procedural representations were mainly centered at MIT , under the leadership of Marvin Minsky and Seymour Papert .","prompt_labels":"Advocates(O) of(O) procedural(O) representations(O) were(O) mainly(O) centered(O) at(O) MIT(B-university) ,(O) under(O) the(O) leadership(O) of(O) Marvin(B-researcher) Minsky(I-researcher) and(O) Seymour(B-researcher) Papert(I-researcher) .(O)"}}
{"id":"2","dataset":"crossner_ai","split":"train","label_list":["researcher","location","product","algorithm","field","metric","organization","conference","person","programming language","university","country","task"],"instance":{"id":"2","words":["The","standard","interface","and","calculator","interface","are","written","in","Java","."],"labels":["O","O","O","O","O","O","O","O","O","B-programming language","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, location, product, algorithm, field, metric, organization, conference, person, programming language, university, country, task and O.\nSentence: The standard interface and calculator interface are written in Java .","prompt_labels":"The(O) standard(O) interface(O) and(O) calculator(O) interface(O) are(O) written(O) in(O) Java(B-programming language) .(O)"}}
{"id":"3","dataset":"crossner_ai","split":"train","label_list":["person","field","location","researcher","organization","programming language","university","product","country","conference","algorithm","task","metric"],"instance":{"id":"3","words":["Octave","helps","in","solving","linear","and","nonlinear","problems","numerically",",","and","for","performing","other","numerical","experiments","using","a","that","is","mostly","compatible","with","MATLAB","."],"labels":["B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, field, location, researcher, organization, programming language, university, product, country, conference, algorithm, task, metric and O.\nSentence: Octave helps in solving linear and nonlinear problems numerically , and for performing other numerical experiments using a that is mostly compatible with MATLAB .","prompt_labels":"Octave(B-product) helps(O) in(O) solving(O) linear(O) and(O) nonlinear(O) problems(O) numerically(O) ,(O) and(O) for(O) performing(O) other(O) numerical(O) experiments(O) using(O) a(O) that(O) is(O) mostly(O) compatible(O) with(O) MATLAB(B-product) .(O)"}}
{"id":"4","dataset":"crossner_ai","split":"train","label_list":["location","metric","algorithm","researcher","field","task","university","product","programming language","country","person","organization","conference"],"instance":{"id":"4","words":["Variants","of","the","back-propagation","algorithm","as","well","as","unsupervised","methods","by","Geoff","Hinton","and","colleagues","at","the","University","of","Toronto","can","be","used","to","train","deep",",","highly","nonlinear","neural","architectures",",","{","{","cite","journal"],"labels":["O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","B-researcher","I-researcher","O","O","O","O","B-university","I-university","I-university","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, metric, algorithm, researcher, field, task, university, product, programming language, country, person, organization, conference and O.\nSentence: Variants of the back-propagation algorithm as well as unsupervised methods by Geoff Hinton and colleagues at the University of Toronto can be used to train deep , highly nonlinear neural architectures , { { cite journal","prompt_labels":"Variants(O) of(O) the(O) back-propagation(B-algorithm) algorithm(I-algorithm) as(O) well(O) as(O) unsupervised(O) methods(O) by(O) Geoff(B-researcher) Hinton(I-researcher) and(O) colleagues(O) at(O) the(O) University(B-university) of(I-university) Toronto(I-university) can(O) be(O) used(O) to(O) train(O) deep(O) ,(O) highly(O) nonlinear(O) neural(O) architectures(O) ,(O) {(O) {(O) cite(O) journal(O)"}}
{"id":"5","dataset":"crossner_ai","split":"train","label_list":["person","organization","task","metric","algorithm","country","researcher","university","field","location","product","programming language","conference"],"instance":{"id":"5","words":["or","equivalently","using","DCG","notation",":"],"labels":["O","O","O","B-metric","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, task, metric, algorithm, country, researcher, university, field, location, product, programming language, conference and O.\nSentence: or equivalently using DCG notation :","prompt_labels":"or(O) equivalently(O) using(O) DCG(B-metric) notation(O) :(O)"}}
{"id":"6","dataset":"crossner_ai","split":"train","label_list":["programming language","researcher","field","organization","location","product","conference","metric","university","task","person","country","algorithm"],"instance":{"id":"6","words":["Self-organizing","maps","differ","from","other","artificial","neural","networks","as","they","apply","competitive","learning","as","opposed","to","error-correction","learning","such","as","backpropagation","with","gradient","descent",")",",","and","in","the","sense","that","they","use","a","neighborhood","function","to","preserve","the","topological","properties","of","the","input","space","."],"labels":["O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","B-algorithm","I-algorithm","O","O","O","B-algorithm","I-algorithm","O","O","B-algorithm","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, researcher, field, organization, location, product, conference, metric, university, task, person, country, algorithm and O.\nSentence: Self-organizing maps differ from other artificial neural networks as they apply competitive learning as opposed to error-correction learning such as backpropagation with gradient descent ) , and in the sense that they use a neighborhood function to preserve the topological properties of the input space .","prompt_labels":"Self-organizing(O) maps(O) differ(O) from(O) other(O) artificial(B-algorithm) neural(I-algorithm) networks(I-algorithm) as(O) they(O) apply(O) competitive(B-algorithm) learning(I-algorithm) as(O) opposed(O) to(O) error-correction(B-algorithm) learning(I-algorithm) such(O) as(O) backpropagation(B-algorithm) with(O) gradient(B-algorithm) descent(I-algorithm) )(O) ,(O) and(O) in(O) the(O) sense(O) that(O) they(O) use(O) a(O) neighborhood(O) function(O) to(O) preserve(O) the(O) topological(O) properties(O) of(O) the(O) input(O) space(O) .(O)"}}
{"id":"7","dataset":"crossner_ai","split":"train","label_list":["conference","university","person","metric","field","country","location","organization","product","algorithm","task","researcher","programming language"],"instance":{"id":"7","words":["Since","the","early","1990s",",","it","has","been","recommended","by","several","authorities",",","including","the","Audio","Engineering","Society","that","measurements","of","dynamic","range","be","made","with","an","audio","signal","present",",","which","is","then","filtered","out","in","the","noise","floor","measurement","used","in","determining","dynamic","range",".","This","avoids","questionable","measurements","based","on","the","use","of","blank","media",",","or","muting","circuits","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, university, person, metric, field, country, location, organization, product, algorithm, task, researcher, programming language and O.\nSentence: Since the early 1990s , it has been recommended by several authorities , including the Audio Engineering Society that measurements of dynamic range be made with an audio signal present , which is then filtered out in the noise floor measurement used in determining dynamic range . This avoids questionable measurements based on the use of blank media , or muting circuits .","prompt_labels":"Since(O) the(O) early(O) 1990s(O) ,(O) it(O) has(O) been(O) recommended(O) by(O) several(O) authorities(O) ,(O) including(O) the(O) Audio(B-organization) Engineering(I-organization) Society(I-organization) that(O) measurements(O) of(O) dynamic(O) range(O) be(O) made(O) with(O) an(O) audio(O) signal(O) present(O) ,(O) which(O) is(O) then(O) filtered(O) out(O) in(O) the(O) noise(B-metric) floor(I-metric) measurement(I-metric) used(O) in(O) determining(O) dynamic(O) range(O) .(O) This(O) avoids(O) questionable(O) measurements(O) based(O) on(O) the(O) use(O) of(O) blank(O) media(O) ,(O) or(O) muting(O) circuits(O) .(O)"}}
{"id":"8","dataset":"crossner_ai","split":"train","label_list":["task","person","country","location","product","programming language","organization","field","metric","conference","algorithm","researcher","university"],"instance":{"id":"8","words":["The","technique","used","in","creating","eigenfaces","and","using","them","for","recognition","is","also","used","outside","of","face","recognition",":","handwriting","recognition",",","lip","reading",",","voice","recognition",",","sign","language","/","hand","gestures","interpretation","and","medical","imaging","analysis","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","I-task","O","B-field","I-field","I-field","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, person, country, location, product, programming language, organization, field, metric, conference, algorithm, researcher, university and O.\nSentence: The technique used in creating eigenfaces and using them for recognition is also used outside of face recognition : handwriting recognition , lip reading , voice recognition , sign language / hand gestures interpretation and medical imaging analysis .","prompt_labels":"The(O) technique(O) used(O) in(O) creating(O) eigenfaces(O) and(O) using(O) them(O) for(O) recognition(O) is(O) also(O) used(O) outside(O) of(O) face(B-task) recognition(I-task) :(O) handwriting(B-task) recognition(I-task) ,(O) lip(B-task) reading(I-task) ,(O) voice(B-task) recognition(I-task) ,(O) sign(B-task) language(I-task) /(O) hand(B-task) gestures(I-task) interpretation(I-task) and(O) medical(B-field) imaging(I-field) analysis(I-field) .(O)"}}
{"id":"9","dataset":"crossner_ai","split":"train","label_list":["field","researcher","programming language","task","algorithm","location","country","person","conference","metric","product","organization","university"],"instance":{"id":"9","words":["The","National","Science","Foundation","was","an","umbrella","for","the","National","Aeronautics","and","Space","Administration","(","NASA",")",",","the","US","Department","of","Energy",",","the","US","Department","of","Commerce","NIST",",","the","US","Department","of","Defense",",","Defense","Advanced","Research","Projects","Agency","(","DARPA",")",",","and","the","Office","of","Naval","Research","coordinated","studies","to","inform","strategic","planners","in","their","deliberations","."],"labels":["O","B-organization","I-organization","I-organization","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, researcher, programming language, task, algorithm, location, country, person, conference, metric, product, organization, university and O.\nSentence: The National Science Foundation was an umbrella for the National Aeronautics and Space Administration ( NASA ) , the US Department of Energy , the US Department of Commerce NIST , the US Department of Defense , Defense Advanced Research Projects Agency ( DARPA ) , and the Office of Naval Research coordinated studies to inform strategic planners in their deliberations .","prompt_labels":"The(O) National(B-organization) Science(I-organization) Foundation(I-organization) was(O) an(O) umbrella(O) for(O) the(O) National(B-organization) Aeronautics(I-organization) and(I-organization) Space(I-organization) Administration(I-organization) ((O) NASA(B-organization) )(O) ,(O) the(O) US(B-organization) Department(I-organization) of(I-organization) Energy(I-organization) ,(O) the(O) US(B-organization) Department(I-organization) of(I-organization) Commerce(I-organization) NIST(I-organization) ,(O) the(O) US(B-organization) Department(I-organization) of(I-organization) Defense(I-organization) ,(O) Defense(B-organization) Advanced(I-organization) Research(I-organization) Projects(I-organization) Agency(I-organization) ((O) DARPA(B-organization) )(O) ,(O) and(O) the(O) Office(B-organization) of(I-organization) Naval(I-organization) Research(I-organization) coordinated(O) studies(O) to(O) inform(O) strategic(O) planners(O) in(O) their(O) deliberations(O) .(O)"}}
{"id":"10","dataset":"crossner_ai","split":"train","label_list":["task","metric","programming language","product","algorithm","organization","field","person","conference","researcher","country","location","university"],"instance":{"id":"10","words":["A","fast","method","for","computing","maximum","likelihood","estimates","for","the","probit","model","was","proposed","by","Ronald","Fisher","as","an","appendix","to","Bliss","'","work","in","1935","."],"labels":["O","O","O","O","O","B-metric","I-metric","O","O","O","B-algorithm","I-algorithm","O","O","O","B-researcher","I-researcher","O","O","O","O","B-researcher","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, metric, programming language, product, algorithm, organization, field, person, conference, researcher, country, location, university and O.\nSentence: A fast method for computing maximum likelihood estimates for the probit model was proposed by Ronald Fisher as an appendix to Bliss ' work in 1935 .","prompt_labels":"A(O) fast(O) method(O) for(O) computing(O) maximum(B-metric) likelihood(I-metric) estimates(O) for(O) the(O) probit(B-algorithm) model(I-algorithm) was(O) proposed(O) by(O) Ronald(B-researcher) Fisher(I-researcher) as(O) an(O) appendix(O) to(O) Bliss(B-researcher) '(O) work(O) in(O) 1935(O) .(O)"}}
{"id":"11","dataset":"crossner_ai","split":"train","label_list":["algorithm","task","location","programming language","university","conference","field","product","researcher","organization","metric","country","person"],"instance":{"id":"11","words":["Several","of","these","programs","are","available","online",",","such","as","Google","Translate","and","the","SYSTRAN","system","that","powers","AltaVista","'s","BabelFish","(","now","Yahoo","'s","Babelfish","as","of","9","May","2008",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-product","I-product","O","O","B-product","I-product","O","O","B-organization","O","B-product","O","O","B-organization","O","B-product","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, task, location, programming language, university, conference, field, product, researcher, organization, metric, country, person and O.\nSentence: Several of these programs are available online , such as Google Translate and the SYSTRAN system that powers AltaVista 's BabelFish ( now Yahoo 's Babelfish as of 9 May 2008 ) .","prompt_labels":"Several(O) of(O) these(O) programs(O) are(O) available(O) online(O) ,(O) such(O) as(O) Google(B-product) Translate(I-product) and(O) the(O) SYSTRAN(B-product) system(I-product) that(O) powers(O) AltaVista(B-organization) 's(O) BabelFish(B-product) ((O) now(O) Yahoo(B-organization) 's(O) Babelfish(B-product) as(O) of(O) 9(O) May(O) 2008(O) )(O) .(O)"}}
{"id":"12","dataset":"crossner_ai","split":"train","label_list":["conference","researcher","product","country","person","programming language","field","task","algorithm","metric","organization","location","university"],"instance":{"id":"12","words":["In","2002","Hutter",",","with","Jürgen","Schmidhuber","and","Shane","Legg",",","developed","and","published","a","mathematical","theory","of","artificial","general","intelligence","based","on","idealised","intelligent","agents","and","reward-motivated","reinforcement","learning","."],"labels":["O","O","B-researcher","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","O","O","O","O","O","O","O","B-field","I-field","I-field","O","O","O","O","O","O","O","B-field","I-field","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, researcher, product, country, person, programming language, field, task, algorithm, metric, organization, location, university and O.\nSentence: In 2002 Hutter , with Jürgen Schmidhuber and Shane Legg , developed and published a mathematical theory of artificial general intelligence based on idealised intelligent agents and reward-motivated reinforcement learning .","prompt_labels":"In(O) 2002(O) Hutter(B-researcher) ,(O) with(O) Jürgen(B-researcher) Schmidhuber(I-researcher) and(O) Shane(B-researcher) Legg(I-researcher) ,(O) developed(O) and(O) published(O) a(O) mathematical(O) theory(O) of(O) artificial(B-field) general(I-field) intelligence(I-field) based(O) on(O) idealised(O) intelligent(O) agents(O) and(O) reward-motivated(O) reinforcement(B-field) learning(I-field) .(O)"}}
{"id":"13","dataset":"crossner_ai","split":"train","label_list":["university","researcher","organization","programming language","conference","metric","location","algorithm","product","country","field","task","person"],"instance":{"id":"13","words":["The","most","common","way","is","using","the","so-called","ROUGE","(","Recall-Oriented","Understudy","for","Gisting","Evaluation",")","measure","."],"labels":["O","O","O","O","O","O","O","O","B-metric","O","B-metric","I-metric","I-metric","I-metric","I-metric","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, researcher, organization, programming language, conference, metric, location, algorithm, product, country, field, task, person and O.\nSentence: The most common way is using the so-called ROUGE ( Recall-Oriented Understudy for Gisting Evaluation ) measure .","prompt_labels":"The(O) most(O) common(O) way(O) is(O) using(O) the(O) so-called(O) ROUGE(B-metric) ((O) Recall-Oriented(B-metric) Understudy(I-metric) for(I-metric) Gisting(I-metric) Evaluation(I-metric) )(O) measure(O) .(O)"}}
{"id":"14","dataset":"crossner_ai","split":"train","label_list":["person","programming language","researcher","organization","task","field","university","conference","location","product","algorithm","metric","country"],"instance":{"id":"14","words":["RapidMiner","provides","learning","schemes",",","models","and","algorithms","and","can","be","extended","using","R","and","Python","scripts",".","David","Norris",",","Bloor","Research",",","November","13",",","2013","."],"labels":["B-product","O","O","O","O","O","O","O","O","O","O","O","O","B-programming language","O","B-programming language","O","O","B-researcher","I-researcher","O","B-organization","I-organization","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, programming language, researcher, organization, task, field, university, conference, location, product, algorithm, metric, country and O.\nSentence: RapidMiner provides learning schemes , models and algorithms and can be extended using R and Python scripts . David Norris , Bloor Research , November 13 , 2013 .","prompt_labels":"RapidMiner(B-product) provides(O) learning(O) schemes(O) ,(O) models(O) and(O) algorithms(O) and(O) can(O) be(O) extended(O) using(O) R(B-programming language) and(O) Python(B-programming language) scripts(O) .(O) David(B-researcher) Norris(I-researcher) ,(O) Bloor(B-organization) Research(I-organization) ,(O) November(O) 13(O) ,(O) 2013(O) .(O)"}}
{"id":"15","dataset":"crossner_ai","split":"train","label_list":["metric","researcher","university","task","product","programming language","field","location","organization","country","algorithm","conference","person"],"instance":{"id":"15","words":["tity","contains","a","collection","of","visualization","tools","and","algorithms","for","data","analysis","and","predictive","modeling",",","together","with","graphical","user","interfaces","for","easy","access","to","these","functions.","but","the","more","recent","fully","Java","-based","version","(","Weka","3",")",",","for","which","development","started","in","1997",",","is","now","used","in","many","different","application","areas",",","in","particular","for","educational","purposes","and","research","."],"labels":["B-product","O","O","O","O","O","O","O","O","O","B-field","I-field","O","B-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-programming language","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, researcher, university, task, product, programming language, field, location, organization, country, algorithm, conference, person and O.\nSentence: tity contains a collection of visualization tools and algorithms for data analysis and predictive modeling , together with graphical user interfaces for easy access to these functions. but the more recent fully Java -based version ( Weka 3 ) , for which development started in 1997 , is now used in many different application areas , in particular for educational purposes and research .","prompt_labels":"tity(B-product) contains(O) a(O) collection(O) of(O) visualization(O) tools(O) and(O) algorithms(O) for(O) data(B-field) analysis(I-field) and(O) predictive(B-task) modeling(I-task) ,(O) together(O) with(O) graphical(O) user(O) interfaces(O) for(O) easy(O) access(O) to(O) these(O) functions.(O) but(O) the(O) more(O) recent(O) fully(O) Java(B-programming language) -based(O) version(O) ((O) Weka(B-product) 3(I-product) )(O) ,(O) for(O) which(O) development(O) started(O) in(O) 1997(O) ,(O) is(O) now(O) used(O) in(O) many(O) different(O) application(O) areas(O) ,(O) in(O) particular(O) for(O) educational(O) purposes(O) and(O) research(O) .(O)"}}
{"id":"16","dataset":"crossner_ai","split":"train","label_list":["field","product","researcher","programming language","task","university","location","country","person","conference","organization","algorithm","metric"],"instance":{"id":"16","words":["Eurisko","made","many","interesting","discoveries","and","enjoyed","significant","acclaim",",","with","his","paper","Heuretics",":","Theoretical","and","Study","of","Heuristic","Rules","winning","the","Best","Paper","award","at","the","1982","Association","for","the","Advancement","of","Artificial","Intelligence","."],"labels":["B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, product, researcher, programming language, task, university, location, country, person, conference, organization, algorithm, metric and O.\nSentence: Eurisko made many interesting discoveries and enjoyed significant acclaim , with his paper Heuretics : Theoretical and Study of Heuristic Rules winning the Best Paper award at the 1982 Association for the Advancement of Artificial Intelligence .","prompt_labels":"Eurisko(B-product) made(O) many(O) interesting(O) discoveries(O) and(O) enjoyed(O) significant(O) acclaim(O) ,(O) with(O) his(O) paper(O) Heuretics(O) :(O) Theoretical(O) and(O) Study(O) of(O) Heuristic(O) Rules(O) winning(O) the(O) Best(O) Paper(O) award(O) at(O) the(O) 1982(B-conference) Association(I-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) .(O)"}}
{"id":"17","dataset":"crossner_ai","split":"train","label_list":["algorithm","conference","task","product","researcher","country","university","programming language","person","metric","organization","field","location"],"instance":{"id":"17","words":["To","allow","for","multiple","entities",",","a","separate","Hinge","loss","is","computed","for","each","capsule","."],"labels":["O","O","O","O","O","O","O","O","B-metric","I-metric","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, conference, task, product, researcher, country, university, programming language, person, metric, organization, field, location and O.\nSentence: To allow for multiple entities , a separate Hinge loss is computed for each capsule .","prompt_labels":"To(O) allow(O) for(O) multiple(O) entities(O) ,(O) a(O) separate(O) Hinge(B-metric) loss(I-metric) is(O) computed(O) for(O) each(O) capsule(O) .(O)"}}
{"id":"18","dataset":"crossner_ai","split":"train","label_list":["university","metric","organization","country","task","field","location","product","programming language","person","conference","algorithm","researcher"],"instance":{"id":"18","words":["With","the","emergence","of","conversational","assistants","such","as","Apple","'s","Siri",",","Amazon","Alexa",",","Google","Assistant",",","Microsoft","Cortana",",","and","Samsung","'s","Bixby",",","Voice","Portals","can","now","be","accessed","through","mobile","devices","and","Far","Field","voice","smart","speakers","such","as","the","Amazon","Echo","and","Google","Home","."],"labels":["O","O","O","O","O","O","O","O","B-product","I-product","I-product","O","B-product","I-product","O","B-product","I-product","O","B-product","I-product","O","O","B-product","I-product","I-product","O","B-product","I-product","O","O","O","O","O","O","O","O","B-product","I-product","I-product","I-product","I-product","O","O","O","B-product","I-product","O","B-product","I-product","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, metric, organization, country, task, field, location, product, programming language, person, conference, algorithm, researcher and O.\nSentence: With the emergence of conversational assistants such as Apple 's Siri , Amazon Alexa , Google Assistant , Microsoft Cortana , and Samsung 's Bixby , Voice Portals can now be accessed through mobile devices and Far Field voice smart speakers such as the Amazon Echo and Google Home .","prompt_labels":"With(O) the(O) emergence(O) of(O) conversational(O) assistants(O) such(O) as(O) Apple(B-product) 's(I-product) Siri(I-product) ,(O) Amazon(B-product) Alexa(I-product) ,(O) Google(B-product) Assistant(I-product) ,(O) Microsoft(B-product) Cortana(I-product) ,(O) and(O) Samsung(B-product) 's(I-product) Bixby(I-product) ,(O) Voice(B-product) Portals(I-product) can(O) now(O) be(O) accessed(O) through(O) mobile(O) devices(O) and(O) Far(B-product) Field(I-product) voice(I-product) smart(I-product) speakers(I-product) such(O) as(O) the(O) Amazon(B-product) Echo(I-product) and(O) Google(B-product) Home(I-product) .(O)"}}
{"id":"19","dataset":"crossner_ai","split":"train","label_list":["programming language","location","product","metric","algorithm","conference","organization","task","researcher","person","university","field","country"],"instance":{"id":"19","words":["Examples","of","supervised","learning","are","Naive","Bayes","classifier",",","Support","vector","machine",",","mixtures","of","Gaussians",",","and","network","."],"labels":["O","O","B-field","I-field","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","I-algorithm","I-algorithm","O","O","B-algorithm","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, location, product, metric, algorithm, conference, organization, task, researcher, person, university, field, country and O.\nSentence: Examples of supervised learning are Naive Bayes classifier , Support vector machine , mixtures of Gaussians , and network .","prompt_labels":"Examples(O) of(O) supervised(B-field) learning(I-field) are(O) Naive(B-algorithm) Bayes(I-algorithm) classifier(I-algorithm) ,(O) Support(B-algorithm) vector(I-algorithm) machine(I-algorithm) ,(O) mixtures(B-algorithm) of(I-algorithm) Gaussians(I-algorithm) ,(O) and(O) network(B-algorithm) .(O)"}}
{"id":"20","dataset":"crossner_ai","split":"train","label_list":["algorithm","person","product","location","field","conference","researcher","programming language","university","country","task","metric","organization"],"instance":{"id":"20","words":["One","can","use","the","OSD","algorithm","to","derive","math","O","(","\\","sqrt","{","T","}",")","/","math","regret","bounds","for","the","online","version","of","Support","vector","machine","for","classification",",","which","use","the","hinge","loss","math","v","_","t","(","w",")","=","\\","max","\\","{","0",",","1","-","y","_","t","(","w","\\","cdot","x","_","t",")","\\","}","/","math"],"labels":["O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-task","O","O","O","O","B-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, person, product, location, field, conference, researcher, programming language, university, country, task, metric, organization and O.\nSentence: One can use the OSD algorithm to derive math O ( \\ sqrt { T } ) / math regret bounds for the online version of Support vector machine for classification , which use the hinge loss math v _ t ( w ) = \\ max \\ { 0 , 1 - y _ t ( w \\ cdot x _ t ) \\ } / math","prompt_labels":"One(O) can(O) use(O) the(O) OSD(B-algorithm) algorithm(I-algorithm) to(O) derive(O) math(O) O(O) ((O) \\(O) sqrt(O) {(O) T(O) }(O) )(O) /(O) math(O) regret(O) bounds(O) for(O) the(O) online(O) version(O) of(O) Support(B-algorithm) vector(I-algorithm) machine(I-algorithm) for(O) classification(B-task) ,(O) which(O) use(O) the(O) hinge(B-metric) loss(I-metric) math(O) v(O) _(O) t(O) ((O) w(O) )(O) =(O) \\(O) max(O) \\(O) {(O) 0(O) ,(O) 1(O) -(O) y(O) _(O) t(O) ((O) w(O) \\(O) cdot(O) x(O) _(O) t(O) )(O) \\(O) }(O) /(O) math(O)"}}
{"id":"21","dataset":"crossner_ai","split":"train","label_list":["conference","organization","metric","researcher","person","location","algorithm","country","programming language","product","university","task","field"],"instance":{"id":"21","words":["Applications","include","object","recognition",",","robotic","mapping","and","navigation",",","image","stitching",",","3D","modeling",",","gesture","recognition",",","video","tracking",",","individual","identification","of","wildlife","and","match","moving","."],"labels":["O","O","B-task","I-task","O","B-task","I-task","O","B-task","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","I-task","I-task","O","B-task","I-task","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, organization, metric, researcher, person, location, algorithm, country, programming language, product, university, task, field and O.\nSentence: Applications include object recognition , robotic mapping and navigation , image stitching , 3D modeling , gesture recognition , video tracking , individual identification of wildlife and match moving .","prompt_labels":"Applications(O) include(O) object(B-task) recognition(I-task) ,(O) robotic(B-task) mapping(I-task) and(O) navigation(B-task) ,(O) image(B-task) stitching(I-task) ,(O) 3D(B-task) modeling(I-task) ,(O) gesture(B-task) recognition(I-task) ,(O) video(B-task) tracking(I-task) ,(O) individual(B-task) identification(I-task) of(I-task) wildlife(I-task) and(O) match(B-task) moving(I-task) .(O)"}}
{"id":"22","dataset":"crossner_ai","split":"train","label_list":["location","field","algorithm","programming language","organization","country","researcher","person","product","task","metric","conference","university"],"instance":{"id":"22","words":["A","number","of","groups","and","companies","are","researching","pose","estimation",",","including","groups","at","Brown","University",",","Carnegie","Mellon","University",",","MPI","Saarbruecken",",","Stanford","University",",","the","University","of","California",",","San","Diego",",","the","University","of","Toronto",",","the","École","Centrale","Paris",",","ETH","Zurich",",","National","University","of","Sciences","and","Technology","(","NUST",")",",","and","the","University","of","California",",","Irvine","."],"labels":["O","O","O","O","O","O","O","O","B-task","I-task","O","O","O","O","B-university","I-university","O","B-university","I-university","I-university","O","B-university","I-university","O","B-university","I-university","O","O","B-university","I-university","I-university","I-university","I-university","I-university","O","O","B-university","I-university","I-university","O","O","B-university","I-university","I-university","O","B-university","I-university","O","B-university","I-university","I-university","I-university","I-university","I-university","O","B-university","O","O","O","O","B-university","I-university","I-university","I-university","I-university","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, field, algorithm, programming language, organization, country, researcher, person, product, task, metric, conference, university and O.\nSentence: A number of groups and companies are researching pose estimation , including groups at Brown University , Carnegie Mellon University , MPI Saarbruecken , Stanford University , the University of California , San Diego , the University of Toronto , the École Centrale Paris , ETH Zurich , National University of Sciences and Technology ( NUST ) , and the University of California , Irvine .","prompt_labels":"A(O) number(O) of(O) groups(O) and(O) companies(O) are(O) researching(O) pose(B-task) estimation(I-task) ,(O) including(O) groups(O) at(O) Brown(B-university) University(I-university) ,(O) Carnegie(B-university) Mellon(I-university) University(I-university) ,(O) MPI(B-university) Saarbruecken(I-university) ,(O) Stanford(B-university) University(I-university) ,(O) the(O) University(B-university) of(I-university) California(I-university) ,(I-university) San(I-university) Diego(I-university) ,(O) the(O) University(B-university) of(I-university) Toronto(I-university) ,(O) the(O) École(B-university) Centrale(I-university) Paris(I-university) ,(O) ETH(B-university) Zurich(I-university) ,(O) National(B-university) University(I-university) of(I-university) Sciences(I-university) and(I-university) Technology(I-university) ((O) NUST(B-university) )(O) ,(O) and(O) the(O) University(B-university) of(I-university) California(I-university) ,(I-university) Irvine(I-university) .(O)"}}
{"id":"23","dataset":"crossner_ai","split":"train","label_list":["task","researcher","programming language","field","algorithm","organization","country","person","product","location","metric","university","conference"],"instance":{"id":"23","words":["Sigmoid","function","Cross","entropy","loss","is","used","for","predicting","K","independent","probability","values","in","math","0,1","/","math","."],"labels":["B-metric","I-metric","I-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, researcher, programming language, field, algorithm, organization, country, person, product, location, metric, university, conference and O.\nSentence: Sigmoid function Cross entropy loss is used for predicting K independent probability values in math 0,1 / math .","prompt_labels":"Sigmoid(B-metric) function(I-metric) Cross(I-metric) entropy(I-metric) loss(I-metric) is(O) used(O) for(O) predicting(O) K(O) independent(O) probability(O) values(O) in(O) math(O) 0,1(O) /(O) math(O) .(O)"}}
{"id":"24","dataset":"crossner_ai","split":"train","label_list":["programming language","university","location","metric","algorithm","researcher","conference","task","product","country","field","person","organization"],"instance":{"id":"24","words":["He","held","the","Johann","Bernoulli","Chair","of","Mathematics","and","Informatics","at","the","University","of","Groningen","in","the","Netherlands",",","and","the","Toshiba","Endowed","Chair","at","the","Tokyo","Institute","of","Technology","in","Japan","before","becoming","Professor","at","Cambridge","."],"labels":["O","O","O","O","O","O","O","B-field","O","B-field","O","O","B-university","I-university","I-university","O","O","B-country","O","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","B-country","O","O","O","O","B-university","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, university, location, metric, algorithm, researcher, conference, task, product, country, field, person, organization and O.\nSentence: He held the Johann Bernoulli Chair of Mathematics and Informatics at the University of Groningen in the Netherlands , and the Toshiba Endowed Chair at the Tokyo Institute of Technology in Japan before becoming Professor at Cambridge .","prompt_labels":"He(O) held(O) the(O) Johann(O) Bernoulli(O) Chair(O) of(O) Mathematics(B-field) and(O) Informatics(B-field) at(O) the(O) University(B-university) of(I-university) Groningen(I-university) in(O) the(O) Netherlands(B-country) ,(O) and(O) the(O) Toshiba(O) Endowed(O) Chair(O) at(O) the(O) Tokyo(B-university) Institute(I-university) of(I-university) Technology(I-university) in(O) Japan(B-country) before(O) becoming(O) Professor(O) at(O) Cambridge(B-university) .(O)"}}
{"id":"25","dataset":"crossner_ai","split":"train","label_list":["researcher","country","programming language","organization","product","algorithm","conference","task","university","person","field","metric","location"],"instance":{"id":"25","words":["Another","technique","particularly","used","for","recurrent","neural","network","s","is","the","long","short-term","memory","(","LSTM",")","network","of","1997","by","Sepp","Hochreiter","&","Jürgen","Schmidhuber","."],"labels":["O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, country, programming language, organization, product, algorithm, conference, task, university, person, field, metric, location and O.\nSentence: Another technique particularly used for recurrent neural network s is the long short-term memory ( LSTM ) network of 1997 by Sepp Hochreiter & Jürgen Schmidhuber .","prompt_labels":"Another(O) technique(O) particularly(O) used(O) for(O) recurrent(B-algorithm) neural(I-algorithm) network(I-algorithm) s(O) is(O) the(O) long(B-algorithm) short-term(I-algorithm) memory(I-algorithm) ((O) LSTM(B-algorithm) )(O) network(O) of(O) 1997(O) by(O) Sepp(B-researcher) Hochreiter(I-researcher) &(O) Jürgen(B-researcher) Schmidhuber(I-researcher) .(O)"}}
{"id":"26","dataset":"crossner_ai","split":"train","label_list":["researcher","university","location","task","country","programming language","field","person","algorithm","metric","product","organization","conference"],"instance":{"id":"26","words":["The","inclusion","of","a","C","+","+","interpreter","(","CINT","until","version","5.34",",","Cling","from","version","6",")","makes","this","package","very","versatile","as","it","can","be","used","in","interactive",",","scripted","and","compiled","modes","in","a","manner","similar","to","commercial","products","like","MATLAB","."],"labels":["O","O","O","O","B-programming language","I-programming language","I-programming language","O","O","B-product","O","O","O","O","B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, university, location, task, country, programming language, field, person, algorithm, metric, product, organization, conference and O.\nSentence: The inclusion of a C + + interpreter ( CINT until version 5.34 , Cling from version 6 ) makes this package very versatile as it can be used in interactive , scripted and compiled modes in a manner similar to commercial products like MATLAB .","prompt_labels":"The(O) inclusion(O) of(O) a(O) C(B-programming language) +(I-programming language) +(I-programming language) interpreter(O) ((O) CINT(B-product) until(O) version(O) 5.34(O) ,(O) Cling(B-product) from(O) version(O) 6(O) )(O) makes(O) this(O) package(O) very(O) versatile(O) as(O) it(O) can(O) be(O) used(O) in(O) interactive(O) ,(O) scripted(O) and(O) compiled(O) modes(O) in(O) a(O) manner(O) similar(O) to(O) commercial(O) products(O) like(O) MATLAB(B-product) .(O)"}}
{"id":"27","dataset":"crossner_ai","split":"train","label_list":["location","metric","programming language","person","researcher","field","university","task","product","algorithm","organization","country","conference"],"instance":{"id":"27","words":["Voice","user","interfaces","that","interpret","and","manage","conversational","state","are","challenging","to","design","due","to","the","inherent","difficulty","of","integrating","complex","natural","language","processing","tasks","like","coreference","resolution",",","named-entity","recognition",",","information","retrieval",",","and","dialog","management","."],"labels":["B-product","I-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","I-field","O","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","O","B-task","I-task","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, metric, programming language, person, researcher, field, university, task, product, algorithm, organization, country, conference and O.\nSentence: Voice user interfaces that interpret and manage conversational state are challenging to design due to the inherent difficulty of integrating complex natural language processing tasks like coreference resolution , named-entity recognition , information retrieval , and dialog management .","prompt_labels":"Voice(B-product) user(I-product) interfaces(I-product) that(O) interpret(O) and(O) manage(O) conversational(O) state(O) are(O) challenging(O) to(O) design(O) due(O) to(O) the(O) inherent(O) difficulty(O) of(O) integrating(O) complex(O) natural(B-field) language(I-field) processing(I-field) tasks(O) like(O) coreference(B-task) resolution(I-task) ,(O) named-entity(B-task) recognition(I-task) ,(O) information(B-task) retrieval(I-task) ,(O) and(O) dialog(B-task) management(I-task) .(O)"}}
{"id":"28","dataset":"crossner_ai","split":"train","label_list":["programming language","country","field","task","researcher","university","person","conference","product","algorithm","location","metric","organization"],"instance":{"id":"28","words":["Between","2009","and","2012",",","the","recurrent","neural","network","s","and","deep","feedforward","neural","network","s","developed","in","the","research","group","of","Jürgen","Schmidhuber","at","the","Swiss","AI","Lab","IDSIA","have","won","eight","international","competitions","in","pattern","recognition","and","machine","learning","."],"labels":["O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","B-researcher","I-researcher","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","B-field","I-field","O","B-field","I-field","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, country, field, task, researcher, university, person, conference, product, algorithm, location, metric, organization and O.\nSentence: Between 2009 and 2012 , the recurrent neural network s and deep feedforward neural network s developed in the research group of Jürgen Schmidhuber at the Swiss AI Lab IDSIA have won eight international competitions in pattern recognition and machine learning .","prompt_labels":"Between(O) 2009(O) and(O) 2012(O) ,(O) the(O) recurrent(B-algorithm) neural(I-algorithm) network(I-algorithm) s(O) and(O) deep(B-algorithm) feedforward(I-algorithm) neural(I-algorithm) network(I-algorithm) s(O) developed(O) in(O) the(O) research(O) group(O) of(O) Jürgen(B-researcher) Schmidhuber(I-researcher) at(O) the(O) Swiss(B-organization) AI(I-organization) Lab(I-organization) IDSIA(I-organization) have(O) won(O) eight(O) international(O) competitions(O) in(O) pattern(B-field) recognition(I-field) and(O) machine(B-field) learning(I-field) .(O)"}}
{"id":"29","dataset":"crossner_ai","split":"train","label_list":["organization","task","metric","researcher","university","product","field","country","location","person","programming language","algorithm","conference"],"instance":{"id":"29","words":["Modern","Windows","desktop","systems","can","use","SAPI","4","and","SAPI","5","components","to","support","speech","synthesis","and","speech","."],"labels":["O","B-product","I-product","I-product","O","O","B-product","I-product","O","B-product","I-product","O","O","O","B-task","I-task","O","B-task","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, task, metric, researcher, university, product, field, country, location, person, programming language, algorithm, conference and O.\nSentence: Modern Windows desktop systems can use SAPI 4 and SAPI 5 components to support speech synthesis and speech .","prompt_labels":"Modern(O) Windows(B-product) desktop(I-product) systems(I-product) can(O) use(O) SAPI(B-product) 4(I-product) and(O) SAPI(B-product) 5(I-product) components(O) to(O) support(O) speech(B-task) synthesis(I-task) and(O) speech(B-task) .(O)"}}
{"id":"30","dataset":"crossner_ai","split":"train","label_list":["algorithm","field","organization","researcher","university","person","country","metric","task","product","location","conference","programming language"],"instance":{"id":"30","words":["He","received","two","honorary","degree","s",",","one","S.","V.","della","laurea","ad","honorem","in","Psychology","from","the","University","of","Padua","in","1995","and","one","doctorate","in","Industrial","Design","and","Engineering","from","Delft","University","of","Technology","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","O","O","B-university","I-university","I-university","O","O","O","O","O","O","B-field","I-field","I-field","I-field","O","B-university","I-university","I-university","I-university","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, field, organization, researcher, university, person, country, metric, task, product, location, conference, programming language and O.\nSentence: He received two honorary degree s , one S. V. della laurea ad honorem in Psychology from the University of Padua in 1995 and one doctorate in Industrial Design and Engineering from Delft University of Technology .","prompt_labels":"He(O) received(O) two(O) honorary(O) degree(O) s(O) ,(O) one(O) S.(O) V.(O) della(O) laurea(O) ad(O) honorem(O) in(O) Psychology(B-field) from(O) the(O) University(B-university) of(I-university) Padua(I-university) in(O) 1995(O) and(O) one(O) doctorate(O) in(O) Industrial(B-field) Design(I-field) and(I-field) Engineering(I-field) from(O) Delft(B-university) University(I-university) of(I-university) Technology(I-university) .(O)"}}
{"id":"31","dataset":"crossner_ai","split":"train","label_list":["location","organization","country","task","metric","algorithm","university","programming language","person","researcher","product","field","conference"],"instance":{"id":"31","words":["With","long-time","collaborator","Laurent","Cohen",",","a","neurologist","at","the","Pitié-Salpêtrière","Hospital","in","Paris",",","Dehaene","also","identified","patients","with","lesions","in","different","regions","of","the","parietal","lobe","with","impaired","multiplication",",","but","preserved","subtraction","(","associated","with","lesions","of","the","inferior","parietal","lobule",")","and","others","with","impaired","subtraction",",","but","preserved","multiplication","(","associated","with","lesions","to","the","intraparietal","sulcus",")","."],"labels":["O","O","O","B-researcher","I-researcher","O","O","O","O","O","B-organization","I-organization","O","B-location","O","B-researcher","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, country, task, metric, algorithm, university, programming language, person, researcher, product, field, conference and O.\nSentence: With long-time collaborator Laurent Cohen , a neurologist at the Pitié-Salpêtrière Hospital in Paris , Dehaene also identified patients with lesions in different regions of the parietal lobe with impaired multiplication , but preserved subtraction ( associated with lesions of the inferior parietal lobule ) and others with impaired subtraction , but preserved multiplication ( associated with lesions to the intraparietal sulcus ) .","prompt_labels":"With(O) long-time(O) collaborator(O) Laurent(B-researcher) Cohen(I-researcher) ,(O) a(O) neurologist(O) at(O) the(O) Pitié-Salpêtrière(B-organization) Hospital(I-organization) in(O) Paris(B-location) ,(O) Dehaene(B-researcher) also(O) identified(O) patients(O) with(O) lesions(O) in(O) different(O) regions(O) of(O) the(O) parietal(O) lobe(O) with(O) impaired(O) multiplication(O) ,(O) but(O) preserved(O) subtraction(O) ((O) associated(O) with(O) lesions(O) of(O) the(O) inferior(O) parietal(O) lobule(O) )(O) and(O) others(O) with(O) impaired(O) subtraction(O) ,(O) but(O) preserved(O) multiplication(O) ((O) associated(O) with(O) lesions(O) to(O) the(O) intraparietal(O) sulcus(O) )(O) .(O)"}}
{"id":"32","dataset":"crossner_ai","split":"train","label_list":["field","task","programming language","product","metric","organization","researcher","conference","university","country","person","algorithm","location"],"instance":{"id":"32","words":["More","recently",",","fictional","representations","of","artificially","intelligent","robots","in","films","such","as","A.I.","Artificial","Intelligence","and","Ex","Machina","and","the","2016","TV","adaptation","of","Westworld","have","engaged","audience","sympathy","for","the","robots","themselves","."],"labels":["O","O","O","O","O","O","B-product","I-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, task, programming language, product, metric, organization, researcher, conference, university, country, person, algorithm, location and O.\nSentence: More recently , fictional representations of artificially intelligent robots in films such as A.I. Artificial Intelligence and Ex Machina and the 2016 TV adaptation of Westworld have engaged audience sympathy for the robots themselves .","prompt_labels":"More(O) recently(O) ,(O) fictional(O) representations(O) of(O) artificially(B-product) intelligent(I-product) robots(I-product) in(O) films(O) such(O) as(O) A.I.(O) Artificial(O) Intelligence(O) and(O) Ex(O) Machina(O) and(O) the(O) 2016(O) TV(O) adaptation(O) of(O) Westworld(O) have(O) engaged(O) audience(O) sympathy(O) for(O) the(O) robots(O) themselves(O) .(O)"}}
{"id":"33","dataset":"crossner_ai","split":"train","label_list":["algorithm","university","country","organization","conference","location","metric","task","field","programming language","product","researcher","person"],"instance":{"id":"33","words":["Two","of","the","main","methods","used","in","unsupervised","learning","are","principal","component","analysis","and","cluster","analysis","."],"labels":["O","O","O","O","O","O","O","B-field","I-field","O","B-algorithm","I-algorithm","I-algorithm","O","B-task","I-task","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, university, country, organization, conference, location, metric, task, field, programming language, product, researcher, person and O.\nSentence: Two of the main methods used in unsupervised learning are principal component analysis and cluster analysis .","prompt_labels":"Two(O) of(O) the(O) main(O) methods(O) used(O) in(O) unsupervised(B-field) learning(I-field) are(O) principal(B-algorithm) component(I-algorithm) analysis(I-algorithm) and(O) cluster(B-task) analysis(I-task) .(O)"}}
{"id":"34","dataset":"crossner_ai","split":"train","label_list":["field","algorithm","country","organization","location","product","researcher","task","person","university","metric","programming language","conference"],"instance":{"id":"34","words":["The","Walt","Disney","Company","also","began","more","prominent","use","of","3D","films","in","special","venues","to","impress","audiences","with","Magic","Journeys","(","1982",")","and","Captain","EO","(","Francis","Ford","Coppola",",","1986",",","starring","Michael","Jackson",")","being","notable","examples","."],"labels":["B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","O","O","O","O","B-person","I-person","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, algorithm, country, organization, location, product, researcher, task, person, university, metric, programming language, conference and O.\nSentence: The Walt Disney Company also began more prominent use of 3D films in special venues to impress audiences with Magic Journeys ( 1982 ) and Captain EO ( Francis Ford Coppola , 1986 , starring Michael Jackson ) being notable examples .","prompt_labels":"The(B-organization) Walt(I-organization) Disney(I-organization) Company(I-organization) also(O) began(O) more(O) prominent(O) use(O) of(O) 3D(O) films(O) in(O) special(O) venues(O) to(O) impress(O) audiences(O) with(O) Magic(O) Journeys(O) ((O) 1982(O) )(O) and(O) Captain(O) EO(O) ((O) Francis(B-person) Ford(I-person) Coppola(I-person) ,(O) 1986(O) ,(O) starring(O) Michael(B-person) Jackson(I-person) )(O) being(O) notable(O) examples(O) .(O)"}}
{"id":"35","dataset":"crossner_ai","split":"train","label_list":["product","location","algorithm","person","country","conference","task","field","organization","metric","programming language","university","researcher"],"instance":{"id":"35","words":["Since","2002",",","perceptron","training","has","become","popular","in","the","field","of","natural","language","processing","for","such","tasks","as","part-of-speech","tagging","and","syntactic","parsing","(","Collins",",","2002",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","I-field","O","O","O","O","B-task","I-task","O","B-task","I-task","O","B-researcher","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, location, algorithm, person, country, conference, task, field, organization, metric, programming language, university, researcher and O.\nSentence: Since 2002 , perceptron training has become popular in the field of natural language processing for such tasks as part-of-speech tagging and syntactic parsing ( Collins , 2002 ) .","prompt_labels":"Since(O) 2002(O) ,(O) perceptron(O) training(O) has(O) become(O) popular(O) in(O) the(O) field(O) of(O) natural(B-field) language(I-field) processing(I-field) for(O) such(O) tasks(O) as(O) part-of-speech(B-task) tagging(I-task) and(O) syntactic(B-task) parsing(I-task) ((O) Collins(B-researcher) ,(O) 2002(O) )(O) .(O)"}}
{"id":"36","dataset":"crossner_ai","split":"train","label_list":["researcher","field","organization","programming language","product","conference","country","university","metric","task","algorithm","person","location"],"instance":{"id":"36","words":["The","first","palletizing","robot","was","introduced","in","1963","by","the","Fuji","Yusoki","Kogyo","Company.","by","KUKA","robotics","in","Germany",",","and","the","Programmable","Universal","Machine","for","Assembly","was","invented","by","Victor","Scheinman","in","1976",",","and","the","design","was","sold","to","Unimation","."],"labels":["O","O","B-product","I-product","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-country","O","O","O","B-product","I-product","I-product","I-product","I-product","O","O","O","B-researcher","I-researcher","O","O","O","O","O","O","O","O","O","B-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, field, organization, programming language, product, conference, country, university, metric, task, algorithm, person, location and O.\nSentence: The first palletizing robot was introduced in 1963 by the Fuji Yusoki Kogyo Company. by KUKA robotics in Germany , and the Programmable Universal Machine for Assembly was invented by Victor Scheinman in 1976 , and the design was sold to Unimation .","prompt_labels":"The(O) first(O) palletizing(B-product) robot(I-product) was(O) introduced(O) in(O) 1963(O) by(O) the(O) Fuji(B-organization) Yusoki(I-organization) Kogyo(I-organization) Company.(I-organization) by(O) KUKA(B-organization) robotics(I-organization) in(O) Germany(B-country) ,(O) and(O) the(O) Programmable(B-product) Universal(I-product) Machine(I-product) for(I-product) Assembly(I-product) was(O) invented(O) by(O) Victor(B-researcher) Scheinman(I-researcher) in(O) 1976(O) ,(O) and(O) the(O) design(O) was(O) sold(O) to(O) Unimation(B-organization) .(O)"}}
{"id":"37","dataset":"crossner_ai","split":"train","label_list":["country","location","product","algorithm","conference","researcher","task","field","person","organization","metric","university","programming language"],"instance":{"id":"37","words":["In","the","middle","of","the","1990s",",","while","serving","as","president","of","the","AAAI",",","Hayes","began","a","series","of","attacks","on","critics","of","AI",",","mostly","phrased","in","an","ironic","light",",","and","(","together","with","his","colleague","Kenneth","Ford",")","invented","an","award","named","after","Simon","Newcomb","to","be","given","for","the","most","ridiculous","argument","disproving","the","possibility","of","AI","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","O","B-researcher","O","O","O","O","O","O","O","O","B-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-researcher","I-researcher","O","O","O","O","O","O","B-researcher","I-researcher","O","O","O","O","O","O","O","O","O","O","O","O","B-field","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, product, algorithm, conference, researcher, task, field, person, organization, metric, university, programming language and O.\nSentence: In the middle of the 1990s , while serving as president of the AAAI , Hayes began a series of attacks on critics of AI , mostly phrased in an ironic light , and ( together with his colleague Kenneth Ford ) invented an award named after Simon Newcomb to be given for the most ridiculous argument disproving the possibility of AI .","prompt_labels":"In(O) the(O) middle(O) of(O) the(O) 1990s(O) ,(O) while(O) serving(O) as(O) president(O) of(O) the(O) AAAI(B-conference) ,(O) Hayes(B-researcher) began(O) a(O) series(O) of(O) attacks(O) on(O) critics(O) of(O) AI(B-field) ,(O) mostly(O) phrased(O) in(O) an(O) ironic(O) light(O) ,(O) and(O) ((O) together(O) with(O) his(O) colleague(O) Kenneth(B-researcher) Ford(I-researcher) )(O) invented(O) an(O) award(O) named(O) after(O) Simon(B-researcher) Newcomb(I-researcher) to(O) be(O) given(O) for(O) the(O) most(O) ridiculous(O) argument(O) disproving(O) the(O) possibility(O) of(O) AI(B-field) .(O)"}}
{"id":"38","dataset":"crossner_ai","split":"train","label_list":["researcher","location","conference","field","country","task","algorithm","person","product","organization","programming language","university","metric"],"instance":{"id":"38","words":["An","optimal","value","for","math","\\","alpha","/","math","can","be","found","by","using","a","line","search","algorithm",",","that","is",",","the","magnitude","of","math","\\","alpha","/","math","is","determined","by","finding","the","value","that","minimizes","S",",","usually","using","a","line","search","in","the","interval","math0","\\","alpha","1","/","math","or","a","backtracking","line","search","such","as","Armijo-line","search","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, location, conference, field, country, task, algorithm, person, product, organization, programming language, university, metric and O.\nSentence: An optimal value for math \\ alpha / math can be found by using a line search algorithm , that is , the magnitude of math \\ alpha / math is determined by finding the value that minimizes S , usually using a line search in the interval math0 \\ alpha 1 / math or a backtracking line search such as Armijo-line search .","prompt_labels":"An(O) optimal(O) value(O) for(O) math(O) \\(O) alpha(O) /(O) math(O) can(O) be(O) found(O) by(O) using(O) a(O) line(B-algorithm) search(I-algorithm) algorithm(I-algorithm) ,(O) that(O) is(O) ,(O) the(O) magnitude(O) of(O) math(O) \\(O) alpha(O) /(O) math(O) is(O) determined(O) by(O) finding(O) the(O) value(O) that(O) minimizes(O) S(O) ,(O) usually(O) using(O) a(O) line(B-algorithm) search(I-algorithm) in(O) the(O) interval(O) math0(O) \\(O) alpha(O) 1(O) /(O) math(O) or(O) a(O) backtracking(B-algorithm) line(I-algorithm) search(I-algorithm) such(O) as(O) Armijo-line(B-algorithm) search(I-algorithm) .(O)"}}
{"id":"39","dataset":"crossner_ai","split":"train","label_list":["country","product","programming language","location","researcher","organization","metric","task","conference","person","field","university","algorithm"],"instance":{"id":"39","words":["He","discusses","Breadth-first","search","and","Depth-first","search","techniques",",","but","eventually","concludes","that","the","results","represent","expert","system","s","that","incarnate","a","lot","of","technical","knowledge","but","don","'t","shine","much","light","on","the","mental","processes","that","humans","use","to","solve","such","puzzles","."],"labels":["O","O","B-algorithm","I-algorithm","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, product, programming language, location, researcher, organization, metric, task, conference, person, field, university, algorithm and O.\nSentence: He discusses Breadth-first search and Depth-first search techniques , but eventually concludes that the results represent expert system s that incarnate a lot of technical knowledge but don 't shine much light on the mental processes that humans use to solve such puzzles .","prompt_labels":"He(O) discusses(O) Breadth-first(B-algorithm) search(I-algorithm) and(O) Depth-first(B-algorithm) search(I-algorithm) techniques(O) ,(O) but(O) eventually(O) concludes(O) that(O) the(O) results(O) represent(O) expert(B-product) system(I-product) s(O) that(O) incarnate(O) a(O) lot(O) of(O) technical(O) knowledge(O) but(O) don(O) 't(O) shine(O) much(O) light(O) on(O) the(O) mental(O) processes(O) that(O) humans(O) use(O) to(O) solve(O) such(O) puzzles(O) .(O)"}}
{"id":"40","dataset":"crossner_ai","split":"train","label_list":["conference","field","product","programming language","metric","person","algorithm","task","country","university","researcher","location","organization"],"instance":{"id":"40","words":["Speech","recognition","and","speech","synthesis","deal","with","how","spoken","language","can","be","understood","or","created","using","computers","."],"labels":["B-task","I-task","O","B-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, field, product, programming language, metric, person, algorithm, task, country, university, researcher, location, organization and O.\nSentence: Speech recognition and speech synthesis deal with how spoken language can be understood or created using computers .","prompt_labels":"Speech(B-task) recognition(I-task) and(O) speech(B-task) synthesis(I-task) deal(O) with(O) how(O) spoken(O) language(O) can(O) be(O) understood(O) or(O) created(O) using(O) computers(O) .(O)"}}
{"id":"41","dataset":"crossner_ai","split":"train","label_list":["country","university","product","location","algorithm","programming language","task","field","metric","researcher","conference","person","organization"],"instance":{"id":"41","words":["This","math","\\","theta","^","{","*","}","/","math","is","normally","estimated","using","a","Maximum","Likelihood","(","math","\\","theta","^","{","*","}","=","\\","theta","^","{","ML","}","/","math",")","or","Maximum","A","Posteriori","(","math","\\","theta","^","{","*","}","=","\\","theta","^","{","MAP","}","/","math",")","procedure","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, product, location, algorithm, programming language, task, field, metric, researcher, conference, person, organization and O.\nSentence: This math \\ theta ^ { * } / math is normally estimated using a Maximum Likelihood ( math \\ theta ^ { * } = \\ theta ^ { ML } / math ) or Maximum A Posteriori ( math \\ theta ^ { * } = \\ theta ^ { MAP } / math ) procedure .","prompt_labels":"This(O) math(O) \\(O) theta(O) ^(O) {(O) *(O) }(O) /(O) math(O) is(O) normally(O) estimated(O) using(O) a(O) Maximum(B-algorithm) Likelihood(I-algorithm) ((O) math(O) \\(O) theta(O) ^(O) {(O) *(O) }(O) =(O) \\(O) theta(O) ^(O) {(O) ML(O) }(O) /(O) math(O) )(O) or(O) Maximum(B-algorithm) A(I-algorithm) Posteriori(I-algorithm) ((O) math(O) \\(O) theta(O) ^(O) {(O) *(O) }(O) =(O) \\(O) theta(O) ^(O) {(O) MAP(B-algorithm) }(O) /(O) math(O) )(O) procedure(O) .(O)"}}
{"id":"42","dataset":"crossner_ai","split":"train","label_list":["location","conference","metric","field","product","algorithm","programming language","organization","university","person","country","researcher","task"],"instance":{"id":"42","words":["Some","less","widely","spoken","languages","use","the","open-source","eSpeak","synthesizer","for","their","speech",";","producing","a","robotic",",","awkward","voice","that","may","be","difficult","to","understand","."],"labels":["O","O","O","O","O","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, conference, metric, field, product, algorithm, programming language, organization, university, person, country, researcher, task and O.\nSentence: Some less widely spoken languages use the open-source eSpeak synthesizer for their speech ; producing a robotic , awkward voice that may be difficult to understand .","prompt_labels":"Some(O) less(O) widely(O) spoken(O) languages(O) use(O) the(O) open-source(O) eSpeak(B-product) synthesizer(I-product) for(O) their(O) speech(O) ;(O) producing(O) a(O) robotic(O) ,(O) awkward(O) voice(O) that(O) may(O) be(O) difficult(O) to(O) understand(O) .(O)"}}
{"id":"43","dataset":"crossner_ai","split":"train","label_list":["task","product","researcher","country","university","field","algorithm","organization","conference","location","metric","programming language","person"],"instance":{"id":"43","words":["Although","used","mainly","by","statisticians","and","other","practitioners","requiring","an","environment","for","statistical","computation","and","software","development",",","R","can","also","operate","as","a","general","matrix","calculation","toolbox","-","with","performance","benchmarks","comparable","to","GNU","Octave","or","MATLAB","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-programming language","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-programming language","I-programming language","O","B-product","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, product, researcher, country, university, field, algorithm, organization, conference, location, metric, programming language, person and O.\nSentence: Although used mainly by statisticians and other practitioners requiring an environment for statistical computation and software development , R can also operate as a general matrix calculation toolbox - with performance benchmarks comparable to GNU Octave or MATLAB .","prompt_labels":"Although(O) used(O) mainly(O) by(O) statisticians(O) and(O) other(O) practitioners(O) requiring(O) an(O) environment(O) for(O) statistical(O) computation(O) and(O) software(O) development(O) ,(O) R(B-programming language) can(O) also(O) operate(O) as(O) a(O) general(O) matrix(O) calculation(O) toolbox(O) -(O) with(O) performance(O) benchmarks(O) comparable(O) to(O) GNU(B-programming language) Octave(I-programming language) or(O) MATLAB(B-product) .(O)"}}
{"id":"44","dataset":"crossner_ai","split":"train","label_list":["metric","researcher","task","organization","country","algorithm","location","university","conference","person","field","programming language","product"],"instance":{"id":"44","words":["Heterodyning","is","a","signal","processing","technique","invented","by","Canadian","inventor-engineer","Reginald","Fessenden","that","creates","new","frequencies","by","combining","mixing","two","frequencies","."],"labels":["B-algorithm","O","O","B-field","I-field","O","O","O","O","O","B-researcher","I-researcher","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, researcher, task, organization, country, algorithm, location, university, conference, person, field, programming language, product and O.\nSentence: Heterodyning is a signal processing technique invented by Canadian inventor-engineer Reginald Fessenden that creates new frequencies by combining mixing two frequencies .","prompt_labels":"Heterodyning(B-algorithm) is(O) a(O) signal(B-field) processing(I-field) technique(O) invented(O) by(O) Canadian(O) inventor-engineer(O) Reginald(B-researcher) Fessenden(I-researcher) that(O) creates(O) new(O) frequencies(O) by(O) combining(O) mixing(O) two(O) frequencies(O) .(O)"}}
{"id":"45","dataset":"crossner_ai","split":"train","label_list":["conference","university","product","task","metric","algorithm","field","researcher","location","organization","person","programming language","country"],"instance":{"id":"45","words":["Several","other","features","that","helped","put","3D","back","on","the","map","that","month","were","the","John","Wayne","feature","Hondo","(","distributed","by","Warner","Bros.",")",",","Columbia","'s","Miss","Sadie","Thompson","with","Rita","Hayworth",",","and","Paramount","'s","Money","From","Home","with","Dean","Martin","and","Jerry","Lewis","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","B-organization","I-organization","O","O","B-organization","O","O","O","O","O","B-person","I-person","O","O","B-organization","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, university, product, task, metric, algorithm, field, researcher, location, organization, person, programming language, country and O.\nSentence: Several other features that helped put 3D back on the map that month were the John Wayne feature Hondo ( distributed by Warner Bros. ) , Columbia 's Miss Sadie Thompson with Rita Hayworth , and Paramount 's Money From Home with Dean Martin and Jerry Lewis .","prompt_labels":"Several(O) other(O) features(O) that(O) helped(O) put(O) 3D(O) back(O) on(O) the(O) map(O) that(O) month(O) were(O) the(O) John(B-person) Wayne(I-person) feature(O) Hondo(O) ((O) distributed(O) by(O) Warner(B-organization) Bros.(I-organization) )(O) ,(O) Columbia(B-organization) 's(O) Miss(O) Sadie(O) Thompson(O) with(O) Rita(B-person) Hayworth(I-person) ,(O) and(O) Paramount(B-organization) 's(O) Money(O) From(O) Home(O) with(O) Dean(B-person) Martin(I-person) and(O) Jerry(B-person) Lewis(I-person) .(O)"}}
{"id":"46","dataset":"crossner_ai","split":"train","label_list":["programming language","person","algorithm","location","product","field","metric","conference","task","organization","country","researcher","university"],"instance":{"id":"46","words":["DeepFace","is","a","deep","learning","facial","recognition","system","created","by","a","research","group","at","Facebook","."],"labels":["B-product","O","O","B-field","I-field","B-task","I-task","O","O","O","O","O","O","O","B-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, person, algorithm, location, product, field, metric, conference, task, organization, country, researcher, university and O.\nSentence: DeepFace is a deep learning facial recognition system created by a research group at Facebook .","prompt_labels":"DeepFace(B-product) is(O) a(O) deep(B-field) learning(I-field) facial(B-task) recognition(I-task) system(O) created(O) by(O) a(O) research(O) group(O) at(O) Facebook(B-organization) .(O)"}}
{"id":"47","dataset":"crossner_ai","split":"train","label_list":["field","conference","researcher","location","person","organization","metric","university","country","programming language","algorithm","product","task"],"instance":{"id":"47","words":["Geometry","processing","is","a","common","research","topic","at","SIGGRAPH",",","the","premier","computer","graphics","academic","conference",",","and","the","main","topic","of","the","annual","Symposium","on","Geometry","Processing","."],"labels":["B-field","I-field","O","O","O","O","O","O","B-conference","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, conference, researcher, location, person, organization, metric, university, country, programming language, algorithm, product, task and O.\nSentence: Geometry processing is a common research topic at SIGGRAPH , the premier computer graphics academic conference , and the main topic of the annual Symposium on Geometry Processing .","prompt_labels":"Geometry(B-field) processing(I-field) is(O) a(O) common(O) research(O) topic(O) at(O) SIGGRAPH(B-conference) ,(O) the(O) premier(O) computer(B-field) graphics(I-field) academic(O) conference(O) ,(O) and(O) the(O) main(O) topic(O) of(O) the(O) annual(O) Symposium(B-conference) on(I-conference) Geometry(I-conference) Processing(I-conference) .(O)"}}
{"id":"48","dataset":"crossner_ai","split":"train","label_list":["university","metric","field","person","researcher","country","product","conference","location","task","organization","algorithm","programming language"],"instance":{"id":"48","words":["Feature","extraction","and","dimension","reduction","can","be","combined","in","one","step","using","Principal","Component","Analysis","(","PCA",")",",","linear","discriminant","analysis","(","LDA",")",",","or","canonical","correlation","analysis","(","CCA",")","techniques","as","a","pre-processing","step",",","followed","by","clustering","by","k","-NN","on","feature","vectors","in","reduced-dimension","space","."],"labels":["B-task","I-task","O","B-task","I-task","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, metric, field, person, researcher, country, product, conference, location, task, organization, algorithm, programming language and O.\nSentence: Feature extraction and dimension reduction can be combined in one step using Principal Component Analysis ( PCA ) , linear discriminant analysis ( LDA ) , or canonical correlation analysis ( CCA ) techniques as a pre-processing step , followed by clustering by k -NN on feature vectors in reduced-dimension space .","prompt_labels":"Feature(B-task) extraction(I-task) and(O) dimension(B-task) reduction(I-task) can(O) be(O) combined(O) in(O) one(O) step(O) using(O) Principal(B-algorithm) Component(I-algorithm) Analysis(I-algorithm) ((O) PCA(B-algorithm) )(O) ,(O) linear(B-algorithm) discriminant(I-algorithm) analysis(I-algorithm) ((O) LDA(B-algorithm) )(O) ,(O) or(O) canonical(B-algorithm) correlation(I-algorithm) analysis(I-algorithm) ((O) CCA(B-algorithm) )(O) techniques(O) as(O) a(O) pre-processing(O) step(O) ,(O) followed(O) by(O) clustering(O) by(O) k(B-algorithm) -NN(I-algorithm) on(O) feature(O) vectors(O) in(O) reduced-dimension(O) space(O) .(O)"}}
{"id":"49","dataset":"crossner_ai","split":"train","label_list":["person","field","product","researcher","location","country","university","conference","organization","algorithm","metric","task","programming language"],"instance":{"id":"49","words":["Artificial","neural","networks","are","computational","models","that","excel","at","machine","learning","and","pattern","recognition","."],"labels":["B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","B-field","I-field","O","B-field","I-field","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, field, product, researcher, location, country, university, conference, organization, algorithm, metric, task, programming language and O.\nSentence: Artificial neural networks are computational models that excel at machine learning and pattern recognition .","prompt_labels":"Artificial(B-algorithm) neural(I-algorithm) networks(I-algorithm) are(O) computational(O) models(O) that(O) excel(O) at(O) machine(B-field) learning(I-field) and(O) pattern(B-field) recognition(I-field) .(O)"}}
{"id":"50","dataset":"crossner_ai","split":"train","label_list":["researcher","organization","conference","algorithm","field","programming language","location","product","country","metric","person","university","task"],"instance":{"id":"50","words":[",","C.","Papageorgiou","and","T.","Poggio",",","A","Trainable","Pedestrian","Detection","system",",","International","Journal","of","Computer","Vision","(","IJCV",")",",","pages","1",":","15-33",",","2000","others","uses","local","features","like","histogram","of","oriented","gradients","N.","Dalal",",","B.","Triggs",",","Histograms","of","oriented","gradients","for","human","detection",",","IEEE","Computer","Society","Conference","on","Computer","Vision","and","Pattern","Recognition","(","CVPR",")",",","pages","1",":","886-893",",","2005","descriptors","."],"labels":["O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","O","B-product","I-product","I-product","I-product","O","B-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","I-algorithm","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-algorithm","I-algorithm","I-algorithm","I-algorithm","O","B-task","I-task","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, organization, conference, algorithm, field, programming language, location, product, country, metric, person, university, task and O.\nSentence: , C. Papageorgiou and T. Poggio , A Trainable Pedestrian Detection system , International Journal of Computer Vision ( IJCV ) , pages 1 : 15-33 , 2000 others uses local features like histogram of oriented gradients N. Dalal , B. Triggs , Histograms of oriented gradients for human detection , IEEE Computer Society Conference on Computer Vision and Pattern Recognition ( CVPR ) , pages 1 : 886-893 , 2005 descriptors .","prompt_labels":",(O) C.(B-researcher) Papageorgiou(I-researcher) and(O) T.(B-researcher) Poggio(I-researcher) ,(O) A(O) Trainable(B-product) Pedestrian(I-product) Detection(I-product) system(I-product) ,(O) International(B-conference) Journal(I-conference) of(I-conference) Computer(I-conference) Vision(I-conference) ((O) IJCV(B-conference) )(O) ,(O) pages(O) 1(O) :(O) 15-33(O) ,(O) 2000(O) others(O) uses(O) local(O) features(O) like(O) histogram(B-algorithm) of(I-algorithm) oriented(I-algorithm) gradients(I-algorithm) N.(B-researcher) Dalal(I-researcher) ,(O) B.(B-researcher) Triggs(I-researcher) ,(O) Histograms(B-algorithm) of(I-algorithm) oriented(I-algorithm) gradients(I-algorithm) for(O) human(B-task) detection(I-task) ,(O) IEEE(B-conference) Computer(I-conference) Society(I-conference) Conference(I-conference) on(I-conference) Computer(I-conference) Vision(I-conference) and(I-conference) Pattern(I-conference) Recognition(I-conference) ((O) CVPR(B-conference) )(O) ,(O) pages(O) 1(O) :(O) 886-893(O) ,(O) 2005(O) descriptors(O) .(O)"}}
{"id":"51","dataset":"crossner_ai","split":"train","label_list":["algorithm","organization","country","task","person","field","conference","location","metric","programming language","researcher","university","product"],"instance":{"id":"51","words":["An","autoencoder","is","a","type","of","artificial","neural","network","used","to","learn","Feature","learning","in","an","unsupervised","learning","manner","."],"labels":["O","B-algorithm","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","B-task","I-task","O","O","B-field","I-field","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, organization, country, task, person, field, conference, location, metric, programming language, researcher, university, product and O.\nSentence: An autoencoder is a type of artificial neural network used to learn Feature learning in an unsupervised learning manner .","prompt_labels":"An(O) autoencoder(B-algorithm) is(O) a(O) type(O) of(O) artificial(B-algorithm) neural(I-algorithm) network(I-algorithm) used(O) to(O) learn(O) Feature(B-task) learning(I-task) in(O) an(O) unsupervised(B-field) learning(I-field) manner(O) .(O)"}}
{"id":"52","dataset":"crossner_ai","split":"train","label_list":["location","metric","programming language","product","conference","country","field","task","organization","person","algorithm","university","researcher"],"instance":{"id":"52","words":["Haralick","is","a","Fellow","of","IEEE","for","his","contributions","in","computer","vision","and","image","processing","and","a","Fellow","of","the","International","Association","for","Pattern","Recognition","(","IAPR",")","for","his","contributions","in","pattern","recognition",",","image","processing",",","and","for","service","to","IAPR","."],"labels":["B-researcher","O","O","O","O","B-organization","O","O","O","O","B-field","I-field","O","B-field","I-field","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","B-field","I-field","O","B-field","I-field","O","O","O","O","O","B-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, metric, programming language, product, conference, country, field, task, organization, person, algorithm, university, researcher and O.\nSentence: Haralick is a Fellow of IEEE for his contributions in computer vision and image processing and a Fellow of the International Association for Pattern Recognition ( IAPR ) for his contributions in pattern recognition , image processing , and for service to IAPR .","prompt_labels":"Haralick(B-researcher) is(O) a(O) Fellow(O) of(O) IEEE(B-organization) for(O) his(O) contributions(O) in(O) computer(B-field) vision(I-field) and(O) image(B-field) processing(I-field) and(O) a(O) Fellow(O) of(O) the(O) International(B-organization) Association(I-organization) for(I-organization) Pattern(I-organization) Recognition(I-organization) ((O) IAPR(B-organization) )(O) for(O) his(O) contributions(O) in(O) pattern(B-field) recognition(I-field) ,(O) image(B-field) processing(I-field) ,(O) and(O) for(O) service(O) to(O) IAPR(B-organization) .(O)"}}
{"id":"53","dataset":"crossner_ai","split":"train","label_list":["conference","location","algorithm","researcher","organization","task","country","field","person","programming language","product","metric","university"],"instance":{"id":"53","words":["The","first","attempt","at","end-to-end","ASR","was","with","Connectionist","Temporal","Classification","(","CTC",")","-based","systems","introduced","by","Alex","Graves","of","Google","DeepMind","and","Navdeep","Jaitly","of","the","University","of","Toronto","in","2014","."],"labels":["O","O","O","O","B-task","I-task","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","O","O","O","B-researcher","I-researcher","O","B-organization","I-organization","O","B-researcher","I-researcher","O","O","B-university","I-university","I-university","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, location, algorithm, researcher, organization, task, country, field, person, programming language, product, metric, university and O.\nSentence: The first attempt at end-to-end ASR was with Connectionist Temporal Classification ( CTC ) -based systems introduced by Alex Graves of Google DeepMind and Navdeep Jaitly of the University of Toronto in 2014 .","prompt_labels":"The(O) first(O) attempt(O) at(O) end-to-end(B-task) ASR(I-task) was(O) with(O) Connectionist(B-algorithm) Temporal(I-algorithm) Classification(I-algorithm) ((O) CTC(B-algorithm) )(O) -based(O) systems(O) introduced(O) by(O) Alex(B-researcher) Graves(I-researcher) of(O) Google(B-organization) DeepMind(I-organization) and(O) Navdeep(B-researcher) Jaitly(I-researcher) of(O) the(O) University(B-university) of(I-university) Toronto(I-university) in(O) 2014(O) .(O)"}}
{"id":"54","dataset":"crossner_ai","split":"train","label_list":["task","researcher","algorithm","university","programming language","location","person","conference","country","metric","product","field","organization"],"instance":{"id":"54","words":["Linear-fractional","programming","(","LFP",")","is","a","generalization","of","linear","programming","(","LP",")","."],"labels":["B-algorithm","I-algorithm","O","B-algorithm","O","O","O","O","O","B-algorithm","I-algorithm","O","B-algorithm","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, researcher, algorithm, university, programming language, location, person, conference, country, metric, product, field, organization and O.\nSentence: Linear-fractional programming ( LFP ) is a generalization of linear programming ( LP ) .","prompt_labels":"Linear-fractional(B-algorithm) programming(I-algorithm) ((O) LFP(B-algorithm) )(O) is(O) a(O) generalization(O) of(O) linear(B-algorithm) programming(I-algorithm) ((O) LP(B-algorithm) )(O) .(O)"}}
{"id":"55","dataset":"crossner_ai","split":"train","label_list":["university","person","product","organization","algorithm","researcher","task","country","programming language","location","field","metric","conference"],"instance":{"id":"55","words":["Lafferty","received","numerous","awards",",","including","two","Test-of-Time","awards","at","the","International","Conference","on","Machine","Learning","2011","&","2012",","],"labels":["B-researcher","O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, person, product, organization, algorithm, researcher, task, country, programming language, location, field, metric, conference and O.\nSentence: Lafferty received numerous awards , including two Test-of-Time awards at the International Conference on Machine Learning 2011 & 2012 ,","prompt_labels":"Lafferty(B-researcher) received(O) numerous(O) awards(O) ,(O) including(O) two(O) Test-of-Time(O) awards(O) at(O) the(O) International(B-conference) Conference(I-conference) on(I-conference) Machine(I-conference) Learning(I-conference) 2011(I-conference) &(I-conference) 2012(I-conference) ,(O)"}}
{"id":"56","dataset":"crossner_ai","split":"train","label_list":["field","location","conference","university","researcher","task","product","organization","algorithm","country","person","programming language","metric"],"instance":{"id":"56","words":["With","the","advent","of","component-based","frameworks","such","as",".NET","and","Java",",","component","based","development","environments","are","capable","of","deploying","the","developed","neural","network","to","these","frameworks","as","inheritable","components","."],"labels":["O","O","O","O","O","O","O","O","B-product","O","B-programming language","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, location, conference, university, researcher, task, product, organization, algorithm, country, person, programming language, metric and O.\nSentence: With the advent of component-based frameworks such as .NET and Java , component based development environments are capable of deploying the developed neural network to these frameworks as inheritable components .","prompt_labels":"With(O) the(O) advent(O) of(O) component-based(O) frameworks(O) such(O) as(O) .NET(B-product) and(O) Java(B-programming language) ,(O) component(O) based(O) development(O) environments(O) are(O) capable(O) of(O) deploying(O) the(O) developed(O) neural(B-algorithm) network(I-algorithm) to(O) these(O) frameworks(O) as(O) inheritable(O) components(O) .(O)"}}
{"id":"57","dataset":"crossner_ai","split":"train","label_list":["algorithm","task","conference","researcher","product","person","metric","university","organization","programming language","field","location","country"],"instance":{"id":"57","words":["As","with","BLEU",",","the","basic","unit","of","evaluation","is","the","sentence",",","the","algorithm","first","creates","an","alignment","(","see","illustrations",")","between","two","sentence","s",",","the","candidate","translation","string",",","and","the","reference","translation","string","."],"labels":["O","O","B-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, task, conference, researcher, product, person, metric, university, organization, programming language, field, location, country and O.\nSentence: As with BLEU , the basic unit of evaluation is the sentence , the algorithm first creates an alignment ( see illustrations ) between two sentence s , the candidate translation string , and the reference translation string .","prompt_labels":"As(O) with(O) BLEU(B-metric) ,(O) the(O) basic(O) unit(O) of(O) evaluation(O) is(O) the(O) sentence(O) ,(O) the(O) algorithm(O) first(O) creates(O) an(O) alignment(O) ((O) see(O) illustrations(O) )(O) between(O) two(O) sentence(O) s(O) ,(O) the(O) candidate(O) translation(O) string(O) ,(O) and(O) the(O) reference(O) translation(O) string(O) .(O)"}}
{"id":"58","dataset":"crossner_ai","split":"train","label_list":["researcher","conference","task","university","algorithm","location","organization","field","person","programming language","product","metric","country"],"instance":{"id":"58","words":["One","of","the","metrics","used","in","NIST","'","s","annual","Document","Understanding","Conferences",",","in","which","research","groups","submit","their","systems","for","both","summarization","and","translation","tasks",",","is","the","ROUGE","metric","(","Recall-Oriented","Understudy","for","Gisting","Evaluation",",","In","Advances","of","Neural","Information","Processing","Systems","(","NIPS",")",",","Montreal",",","Canada",",","December","-","2014","."],"labels":["O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","O","O","O","O","O","O","O","O","O","B-task","O","B-task","I-task","O","O","O","B-metric","I-metric","O","B-metric","I-metric","I-metric","I-metric","I-metric","O","O","O","O","B-conference","I-conference","I-conference","I-conference","O","B-conference","O","O","B-location","O","B-country","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, conference, task, university, algorithm, location, organization, field, person, programming language, product, metric, country and O.\nSentence: One of the metrics used in NIST ' s annual Document Understanding Conferences , in which research groups submit their systems for both summarization and translation tasks , is the ROUGE metric ( Recall-Oriented Understudy for Gisting Evaluation , In Advances of Neural Information Processing Systems ( NIPS ) , Montreal , Canada , December - 2014 .","prompt_labels":"One(O) of(O) the(O) metrics(O) used(O) in(O) NIST(B-conference) '(I-conference) s(I-conference) annual(I-conference) Document(I-conference) Understanding(I-conference) Conferences(I-conference) ,(O) in(O) which(O) research(O) groups(O) submit(O) their(O) systems(O) for(O) both(O) summarization(B-task) and(O) translation(B-task) tasks(I-task) ,(O) is(O) the(O) ROUGE(B-metric) metric(I-metric) ((O) Recall-Oriented(B-metric) Understudy(I-metric) for(I-metric) Gisting(I-metric) Evaluation(I-metric) ,(O) In(O) Advances(O) of(O) Neural(B-conference) Information(I-conference) Processing(I-conference) Systems(I-conference) ((O) NIPS(B-conference) )(O) ,(O) Montreal(B-location) ,(O) Canada(B-country) ,(O) December(O) -(O) 2014(O) .(O)"}}
{"id":"59","dataset":"crossner_ai","split":"train","label_list":["task","conference","metric","field","location","university","organization","country","programming language","researcher","algorithm","person","product"],"instance":{"id":"59","words":["Same","implementation",",","to","run","in","Java","with","JShell","(","Java","9","minimum",")",":","codejshell","scriptfile","/","codesyntaxhighlight","lang","=","java"],"labels":["O","O","O","O","O","O","B-programming language","O","B-product","O","B-programming language","I-programming language","O","O","O","B-product","O","O","O","O","O","B-programming language"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, conference, metric, field, location, university, organization, country, programming language, researcher, algorithm, person, product and O.\nSentence: Same implementation , to run in Java with JShell ( Java 9 minimum ) : codejshell scriptfile / codesyntaxhighlight lang = java","prompt_labels":"Same(O) implementation(O) ,(O) to(O) run(O) in(O) Java(B-programming language) with(O) JShell(B-product) ((O) Java(B-programming language) 9(I-programming language) minimum(O) )(O) :(O) codejshell(B-product) scriptfile(O) /(O) codesyntaxhighlight(O) lang(O) =(O) java(B-programming language)"}}
{"id":"60","dataset":"crossner_ai","split":"train","label_list":["metric","university","programming language","organization","country","field","product","person","researcher","conference","location","algorithm","task"],"instance":{"id":"60","words":["The","NIST","metric","is","based","on","the","BLEU","metric",",","but","with","some","alterations","."],"labels":["O","B-metric","I-metric","O","O","O","O","B-metric","I-metric","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, university, programming language, organization, country, field, product, person, researcher, conference, location, algorithm, task and O.\nSentence: The NIST metric is based on the BLEU metric , but with some alterations .","prompt_labels":"The(O) NIST(B-metric) metric(I-metric) is(O) based(O) on(O) the(O) BLEU(B-metric) metric(I-metric) ,(O) but(O) with(O) some(O) alterations(O) .(O)"}}
{"id":"61","dataset":"crossner_ai","split":"train","label_list":["programming language","location","conference","product","field","algorithm","organization","task","university","person","country","metric","researcher"],"instance":{"id":"61","words":["In","the","late","1980s",",","two","Netherlands","universities",",","University","of","Groningen","and","University","of","Twente",",","jointly","began","a","project","called","Knowledge","Graphs",",","which","are","semantic","networks","but","with","the","added","constraint","that","edges","are","restricted","to","be","from","a","limited","set","of","possible","relations",",","to","facilitate","algebras","on","the","graph","."],"labels":["O","O","O","O","O","O","B-country","O","O","B-university","I-university","I-university","O","B-university","I-university","I-university","O","O","O","O","O","O","B-product","I-product","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, location, conference, product, field, algorithm, organization, task, university, person, country, metric, researcher and O.\nSentence: In the late 1980s , two Netherlands universities , University of Groningen and University of Twente , jointly began a project called Knowledge Graphs , which are semantic networks but with the added constraint that edges are restricted to be from a limited set of possible relations , to facilitate algebras on the graph .","prompt_labels":"In(O) the(O) late(O) 1980s(O) ,(O) two(O) Netherlands(B-country) universities(O) ,(O) University(B-university) of(I-university) Groningen(I-university) and(O) University(B-university) of(I-university) Twente(I-university) ,(O) jointly(O) began(O) a(O) project(O) called(O) Knowledge(B-product) Graphs(I-product) ,(O) which(O) are(O) semantic(B-algorithm) networks(I-algorithm) but(O) with(O) the(O) added(O) constraint(O) that(O) edges(O) are(O) restricted(O) to(O) be(O) from(O) a(O) limited(O) set(O) of(O) possible(O) relations(O) ,(O) to(O) facilitate(O) algebras(O) on(O) the(O) graph(O) .(O)"}}
{"id":"62","dataset":"crossner_ai","split":"train","label_list":["field","programming language","person","task","country","metric","researcher","product","university","algorithm","location","organization","conference"],"instance":{"id":"62","words":["Grammar","checkers","are","most","often","implemented","as","a","feature","of","a","larger","program",",","such","as","a","word","processor",",","but","are","also","available","as","a","stand-alone","application","that","can","be","activated","from","within","programs","that","work","with","editable","text","."],"labels":["B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, programming language, person, task, country, metric, researcher, product, university, algorithm, location, organization, conference and O.\nSentence: Grammar checkers are most often implemented as a feature of a larger program , such as a word processor , but are also available as a stand-alone application that can be activated from within programs that work with editable text .","prompt_labels":"Grammar(B-product) checkers(I-product) are(O) most(O) often(O) implemented(O) as(O) a(O) feature(O) of(O) a(O) larger(O) program(O) ,(O) such(O) as(O) a(O) word(B-product) processor(I-product) ,(O) but(O) are(O) also(O) available(O) as(O) a(O) stand-alone(O) application(O) that(O) can(O) be(O) activated(O) from(O) within(O) programs(O) that(O) work(O) with(O) editable(O) text(O) .(O)"}}
{"id":"63","dataset":"crossner_ai","split":"train","label_list":["metric","task","location","algorithm","university","conference","field","product","programming language","researcher","person","country","organization"],"instance":{"id":"63","words":["He","is","a","Fellow","of","the","American","Association","for","the","Advancement","of","Science",",","Association","for","the","Advancement","Artificial","Intelligence",",","and","Cognitive","Science","Society",",","and","an","editor","of","the","J.","Automated","Reasoning",",","J.","Learning","Sciences",",","and","J.","Applied","Ontology","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-conference","I-conference","I-conference","O","B-conference","I-conference","I-conference","O","O","B-conference","I-conference","I-conference","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, task, location, algorithm, university, conference, field, product, programming language, researcher, person, country, organization and O.\nSentence: He is a Fellow of the American Association for the Advancement of Science , Association for the Advancement Artificial Intelligence , and Cognitive Science Society , and an editor of the J. Automated Reasoning , J. Learning Sciences , and J. Applied Ontology .","prompt_labels":"He(O) is(O) a(O) Fellow(O) of(O) the(O) American(B-organization) Association(I-organization) for(I-organization) the(I-organization) Advancement(I-organization) of(I-organization) Science(I-organization) ,(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) Artificial(I-conference) Intelligence(I-conference) ,(O) and(O) Cognitive(B-organization) Science(I-organization) Society(I-organization) ,(O) and(O) an(O) editor(O) of(O) the(O) J.(B-conference) Automated(I-conference) Reasoning(I-conference) ,(O) J.(B-conference) Learning(I-conference) Sciences(I-conference) ,(O) and(O) J.(B-conference) Applied(I-conference) Ontology(I-conference) .(O)"}}
{"id":"64","dataset":"crossner_ai","split":"train","label_list":["location","product","country","programming language","conference","organization","metric","researcher","university","task","field","person","algorithm"],"instance":{"id":"64","words":["Linear","predictive","coding","(","LPC",")",",","a","form","of","speech","coding",",","began","development","with","the","work","Fumitada","Itakura","of","Nagoya","University","and","Shuzo","Saito","of","Nippon","Telegraph","and","Telephone","(","NTT",")","in","1966","."],"labels":["B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","O","O","O","B-task","I-task","O","O","O","O","O","O","B-researcher","I-researcher","O","B-university","I-university","O","B-researcher","I-researcher","O","B-university","I-university","I-university","I-university","O","B-university","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, product, country, programming language, conference, organization, metric, researcher, university, task, field, person, algorithm and O.\nSentence: Linear predictive coding ( LPC ) , a form of speech coding , began development with the work Fumitada Itakura of Nagoya University and Shuzo Saito of Nippon Telegraph and Telephone ( NTT ) in 1966 .","prompt_labels":"Linear(B-algorithm) predictive(I-algorithm) coding(I-algorithm) ((O) LPC(B-algorithm) )(O) ,(O) a(O) form(O) of(O) speech(B-task) coding(I-task) ,(O) began(O) development(O) with(O) the(O) work(O) Fumitada(B-researcher) Itakura(I-researcher) of(O) Nagoya(B-university) University(I-university) and(O) Shuzo(B-researcher) Saito(I-researcher) of(O) Nippon(B-university) Telegraph(I-university) and(I-university) Telephone(I-university) ((O) NTT(B-university) )(O) in(O) 1966(O) .(O)"}}
{"id":"65","dataset":"crossner_ai","split":"train","label_list":["product","programming language","metric","location","country","field","task","organization","university","algorithm","researcher","conference","person"],"instance":{"id":"65","words":["If","the","signal","is","further","ergodic",",","all","sample","paths","exhibits","the","same","time-average","and","thus","mathR","_","x","^","{","n","/","T","_","0","}","(","\\","tau",")","=","\\","widehat","{","R","}","_","x","^","{","n","/","T","_","0","}","(","\\","tau",")","/","math","in","mean","square","error","sense","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, programming language, metric, location, country, field, task, organization, university, algorithm, researcher, conference, person and O.\nSentence: If the signal is further ergodic , all sample paths exhibits the same time-average and thus mathR _ x ^ { n / T _ 0 } ( \\ tau ) = \\ widehat { R } _ x ^ { n / T _ 0 } ( \\ tau ) / math in mean square error sense .","prompt_labels":"If(O) the(O) signal(O) is(O) further(O) ergodic(O) ,(O) all(O) sample(O) paths(O) exhibits(O) the(O) same(O) time-average(O) and(O) thus(O) mathR(O) _(O) x(O) ^(O) {(O) n(O) /(O) T(O) _(O) 0(O) }(O) ((O) \\(O) tau(O) )(O) =(O) \\(O) widehat(O) {(O) R(O) }(O) _(O) x(O) ^(O) {(O) n(O) /(O) T(O) _(O) 0(O) }(O) ((O) \\(O) tau(O) )(O) /(O) math(O) in(O) mean(B-metric) square(I-metric) error(I-metric) sense(O) .(O)"}}
{"id":"66","dataset":"crossner_ai","split":"train","label_list":["location","field","organization","person","conference","metric","programming language","country","algorithm","university","task","product","researcher"],"instance":{"id":"66","words":["Feature","extraction","and","dimension","reduction","can","be","combined","in","one","step","using","principal","component","analysis","(","PCA",")",",","linear","discriminant","analysis","(","LDA",")",",","canonical","correlation","analysis","(","CCA",")",",","or","non-negative","matrix","factorization","(","NMF",")","techniques","as","a","pre-processing","step","followed","by","clustering","by","K-NN","on","feature","vectors","in","reduced-dimension","space","."],"labels":["B-task","I-task","O","B-task","I-task","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","O","O","O","O","O","O","O","O","B-algorithm","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, field, organization, person, conference, metric, programming language, country, algorithm, university, task, product, researcher and O.\nSentence: Feature extraction and dimension reduction can be combined in one step using principal component analysis ( PCA ) , linear discriminant analysis ( LDA ) , canonical correlation analysis ( CCA ) , or non-negative matrix factorization ( NMF ) techniques as a pre-processing step followed by clustering by K-NN on feature vectors in reduced-dimension space .","prompt_labels":"Feature(B-task) extraction(I-task) and(O) dimension(B-task) reduction(I-task) can(O) be(O) combined(O) in(O) one(O) step(O) using(O) principal(B-algorithm) component(I-algorithm) analysis(I-algorithm) ((O) PCA(B-algorithm) )(O) ,(O) linear(B-algorithm) discriminant(I-algorithm) analysis(I-algorithm) ((O) LDA(B-algorithm) )(O) ,(O) canonical(B-algorithm) correlation(I-algorithm) analysis(I-algorithm) ((O) CCA(B-algorithm) )(O) ,(O) or(O) non-negative(B-algorithm) matrix(I-algorithm) factorization(I-algorithm) ((O) NMF(B-algorithm) )(O) techniques(O) as(O) a(O) pre-processing(O) step(O) followed(O) by(O) clustering(O) by(O) K-NN(B-algorithm) on(O) feature(O) vectors(O) in(O) reduced-dimension(O) space(O) .(O)"}}
{"id":"67","dataset":"crossner_ai","split":"train","label_list":["product","location","task","person","metric","conference","researcher","programming language","country","algorithm","field","university","organization"],"instance":{"id":"67","words":["Libraries","written","in","Perl",",","Java",",","ActiveX","or",".NET","can","be","directly","called","from","MATLAB",","],"labels":["O","O","O","B-programming language","O","B-programming language","O","B-programming language","O","B-programming language","O","O","O","O","O","B-product","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, location, task, person, metric, conference, researcher, programming language, country, algorithm, field, university, organization and O.\nSentence: Libraries written in Perl , Java , ActiveX or .NET can be directly called from MATLAB ,","prompt_labels":"Libraries(O) written(O) in(O) Perl(B-programming language) ,(O) Java(B-programming language) ,(O) ActiveX(B-programming language) or(O) .NET(B-programming language) can(O) be(O) directly(O) called(O) from(O) MATLAB(B-product) ,(O)"}}
{"id":"68","dataset":"crossner_ai","split":"train","label_list":["organization","product","programming language","person","metric","university","algorithm","country","task","field","conference","location","researcher"],"instance":{"id":"68","words":["The","task","of","recognizing","named","entities","in","text","is","Named","Entity","Recognition","while","the","task","of","determining","the","identity","of","the","named","entities","mentioned","in","text","is","called","Entity","Linking","."],"labels":["O","O","O","B-task","I-task","I-task","I-task","I-task","O","B-task","I-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-task","I-task","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, product, programming language, person, metric, university, algorithm, country, task, field, conference, location, researcher and O.\nSentence: The task of recognizing named entities in text is Named Entity Recognition while the task of determining the identity of the named entities mentioned in text is called Entity Linking .","prompt_labels":"The(O) task(O) of(O) recognizing(B-task) named(I-task) entities(I-task) in(I-task) text(I-task) is(O) Named(B-task) Entity(I-task) Recognition(I-task) while(O) the(O) task(O) of(O) determining(O) the(O) identity(O) of(O) the(O) named(O) entities(O) mentioned(O) in(O) text(O) is(O) called(O) Entity(B-task) Linking(I-task) .(O)"}}
{"id":"69","dataset":"crossner_ai","split":"train","label_list":["metric","programming language","person","researcher","field","location","task","algorithm","conference","country","university","organization","product"],"instance":{"id":"69","words":["The","sigmoid","function","s","and","derivatives","used","in","the","package","were","originally","included","in","the","package",",","from","version","0.8.0","onwards",",","these","were","released","in","a","separate","R","package","sigmoid",",","with","the","intention","to","enable","more","general","use","."],"labels":["O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-programming language","O","B-algorithm","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, programming language, person, researcher, field, location, task, algorithm, conference, country, university, organization, product and O.\nSentence: The sigmoid function s and derivatives used in the package were originally included in the package , from version 0.8.0 onwards , these were released in a separate R package sigmoid , with the intention to enable more general use .","prompt_labels":"The(O) sigmoid(B-algorithm) function(I-algorithm) s(O) and(O) derivatives(O) used(O) in(O) the(O) package(O) were(O) originally(O) included(O) in(O) the(O) package(O) ,(O) from(O) version(O) 0.8.0(O) onwards(O) ,(O) these(O) were(O) released(O) in(O) a(O) separate(O) R(B-programming language) package(O) sigmoid(B-algorithm) ,(O) with(O) the(O) intention(O) to(O) enable(O) more(O) general(O) use(O) .(O)"}}
{"id":"70","dataset":"crossner_ai","split":"train","label_list":["researcher","metric","conference","university","programming language","task","organization","person","location","field","algorithm","product","country"],"instance":{"id":"70","words":["Logo","was","created","in","1967","at","Bolt",",","Beranek","and","Newman","(","BBN",")",",","a","Cambridge",",","Massachusetts","research","firm",",","by","Wally","Feurzeig",",","Cynthia","Solomon",",","and","Seymour","Papert","."],"labels":["B-programming language","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-university","O","B-university","O","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","O","B-researcher","I-researcher","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, metric, conference, university, programming language, task, organization, person, location, field, algorithm, product, country and O.\nSentence: Logo was created in 1967 at Bolt , Beranek and Newman ( BBN ) , a Cambridge , Massachusetts research firm , by Wally Feurzeig , Cynthia Solomon , and Seymour Papert .","prompt_labels":"Logo(B-programming language) was(O) created(O) in(O) 1967(O) at(O) Bolt(B-organization) ,(I-organization) Beranek(I-organization) and(I-organization) Newman(I-organization) ((O) BBN(B-organization) )(O) ,(O) a(O) Cambridge(B-university) ,(O) Massachusetts(B-university) research(O) firm(O) ,(O) by(O) Wally(B-researcher) Feurzeig(I-researcher) ,(O) Cynthia(B-researcher) Solomon(I-researcher) ,(O) and(O) Seymour(B-researcher) Papert(I-researcher) .(O)"}}
{"id":"71","dataset":"crossner_ai","split":"train","label_list":["person","algorithm","metric","country","task","product","location","conference","programming language","field","university","organization","researcher"],"instance":{"id":"71","words":["Neuroevolution","is","commonly","used","as","part","of","the","reinforcement","learning","paradigm",",","and","it","can","be","contrasted","with","conventional","deep","learning","techniques","that","use","gradient","descent","on","a","neural","network","with","a","fixed","topology","."],"labels":["O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","B-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, algorithm, metric, country, task, product, location, conference, programming language, field, university, organization, researcher and O.\nSentence: Neuroevolution is commonly used as part of the reinforcement learning paradigm , and it can be contrasted with conventional deep learning techniques that use gradient descent on a neural network with a fixed topology .","prompt_labels":"Neuroevolution(O) is(O) commonly(O) used(O) as(O) part(O) of(O) the(O) reinforcement(B-field) learning(I-field) paradigm(O) ,(O) and(O) it(O) can(O) be(O) contrasted(O) with(O) conventional(O) deep(B-field) learning(I-field) techniques(O) that(O) use(O) gradient(B-algorithm) descent(I-algorithm) on(O) a(O) neural(B-algorithm) network(I-algorithm) with(O) a(O) fixed(O) topology(O) .(O)"}}
{"id":"72","dataset":"crossner_ai","split":"train","label_list":["country","algorithm","product","conference","person","programming language","researcher","location","metric","task","university","organization","field"],"instance":{"id":"72","words":["If","we","use","least","squares","to","fit","a","function","in","the","form","of","a","hyperplane","ŷ","=","a","+","β","supT","/","sup","x","to","the","data","(","x","sub","i","/","sub",",","y","sub","i","/","sub",")","sub","1","≤","i","≤","n","/","sub",",","we","could","then","assess","the","fit","using","the","mean","squared","error","(","MSE",")","."],"labels":["O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","B-metric","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, algorithm, product, conference, person, programming language, researcher, location, metric, task, university, organization, field and O.\nSentence: If we use least squares to fit a function in the form of a hyperplane ŷ = a + β supT / sup x to the data ( x sub i / sub , y sub i / sub ) sub 1 ≤ i ≤ n / sub , we could then assess the fit using the mean squared error ( MSE ) .","prompt_labels":"If(O) we(O) use(O) least(B-algorithm) squares(I-algorithm) to(O) fit(O) a(O) function(O) in(O) the(O) form(O) of(O) a(O) hyperplane(O) ŷ(O) =(O) a(O) +(O) β(O) supT(O) /(O) sup(O) x(O) to(O) the(O) data(O) ((O) x(O) sub(O) i(O) /(O) sub(O) ,(O) y(O) sub(O) i(O) /(O) sub(O) )(O) sub(O) 1(O) ≤(O) i(O) ≤(O) n(O) /(O) sub(O) ,(O) we(O) could(O) then(O) assess(O) the(O) fit(O) using(O) the(O) mean(B-metric) squared(I-metric) error(I-metric) ((O) MSE(B-metric) )(O) .(O)"}}
{"id":"73","dataset":"crossner_ai","split":"train","label_list":["country","location","researcher","programming language","metric","university","algorithm","field","task","product","organization","person","conference"],"instance":{"id":"73","words":["The","company","has","international","locations","in","Australia",",","Brazil",",","Canada",",","China",",","Germany",",","India",",","Italy",",","Japan",",","Korea",",","Lithuania",",","Poland",",","Malaysia",",","the","Philippines",",","Russia",",","Singapore",",","South","Africa",",","Spain",",","Taiwan",",","Thailand",",","Turkey","and","the","United","Kingdom","."],"labels":["O","O","O","O","O","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","O","B-country","O","B-country","O","B-country","O","B-country","I-country","O","B-country","O","B-country","O","B-country","O","B-country","O","O","B-country","I-country","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, researcher, programming language, metric, university, algorithm, field, task, product, organization, person, conference and O.\nSentence: The company has international locations in Australia , Brazil , Canada , China , Germany , India , Italy , Japan , Korea , Lithuania , Poland , Malaysia , the Philippines , Russia , Singapore , South Africa , Spain , Taiwan , Thailand , Turkey and the United Kingdom .","prompt_labels":"The(O) company(O) has(O) international(O) locations(O) in(O) Australia(B-country) ,(O) Brazil(B-country) ,(O) Canada(B-country) ,(O) China(B-country) ,(O) Germany(B-country) ,(O) India(B-country) ,(O) Italy(B-country) ,(O) Japan(B-country) ,(O) Korea(B-country) ,(O) Lithuania(B-country) ,(O) Poland(B-country) ,(O) Malaysia(B-country) ,(O) the(O) Philippines(B-country) ,(O) Russia(B-country) ,(O) Singapore(B-country) ,(O) South(B-country) Africa(I-country) ,(O) Spain(B-country) ,(O) Taiwan(B-country) ,(O) Thailand(B-country) ,(O) Turkey(B-country) and(O) the(O) United(B-country) Kingdom(I-country) .(O)"}}
{"id":"74","dataset":"crossner_ai","split":"train","label_list":["person","university","researcher","algorithm","conference","metric","country","field","task","organization","programming language","location","product"],"instance":{"id":"74","words":["He","holds","a","D.Sc.","degree","in","electrical","and","computer","engineering","(","2000",")","from","Inria","and","the","University","of","Nice","Sophia","Antipolis",",","and","has","held","permanent","positions","at","Siemens","Corporate","Technology",",","École","des","ponts","ParisTech","as","well","as","visiting","positions","at","Rutgers","University",",","Yale","University","and","University","of","Houston","."],"labels":["O","O","O","O","O","O","B-field","I-field","I-field","I-field","O","O","O","O","B-organization","O","O","B-university","I-university","I-university","I-university","I-university","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-university","I-university","I-university","I-university","O","O","O","O","O","O","B-university","I-university","O","B-university","I-university","O","B-university","I-university","I-university","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, university, researcher, algorithm, conference, metric, country, field, task, organization, programming language, location, product and O.\nSentence: He holds a D.Sc. degree in electrical and computer engineering ( 2000 ) from Inria and the University of Nice Sophia Antipolis , and has held permanent positions at Siemens Corporate Technology , École des ponts ParisTech as well as visiting positions at Rutgers University , Yale University and University of Houston .","prompt_labels":"He(O) holds(O) a(O) D.Sc.(O) degree(O) in(O) electrical(B-field) and(I-field) computer(I-field) engineering(I-field) ((O) 2000(O) )(O) from(O) Inria(B-organization) and(O) the(O) University(B-university) of(I-university) Nice(I-university) Sophia(I-university) Antipolis(I-university) ,(O) and(O) has(O) held(O) permanent(O) positions(O) at(O) Siemens(B-organization) Corporate(I-organization) Technology(I-organization) ,(O) École(B-university) des(I-university) ponts(I-university) ParisTech(I-university) as(O) well(O) as(O) visiting(O) positions(O) at(O) Rutgers(B-university) University(I-university) ,(O) Yale(B-university) University(I-university) and(O) University(B-university) of(I-university) Houston(I-university) .(O)"}}
{"id":"75","dataset":"crossner_ai","split":"train","label_list":["conference","organization","product","field","researcher","country","algorithm","metric","programming language","location","person","university","task"],"instance":{"id":"75","words":["Licensing","the","original","patent","awarded","to","inventor","George","Devol",",","Engelberger","developed","the","first","industrial","robot","in","the","United","States",",","the","Unimate",",","in","the","1950s","."],"labels":["O","O","O","O","O","O","O","B-researcher","I-researcher","O","B-researcher","O","O","O","B-product","I-product","O","O","B-country","I-country","O","O","B-product","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, organization, product, field, researcher, country, algorithm, metric, programming language, location, person, university, task and O.\nSentence: Licensing the original patent awarded to inventor George Devol , Engelberger developed the first industrial robot in the United States , the Unimate , in the 1950s .","prompt_labels":"Licensing(O) the(O) original(O) patent(O) awarded(O) to(O) inventor(O) George(B-researcher) Devol(I-researcher) ,(O) Engelberger(B-researcher) developed(O) the(O) first(O) industrial(B-product) robot(I-product) in(O) the(O) United(B-country) States(I-country) ,(O) the(O) Unimate(B-product) ,(O) in(O) the(O) 1950s(O) .(O)"}}
{"id":"76","dataset":"crossner_ai","split":"train","label_list":["field","organization","location","metric","product","person","researcher","university","conference","country","programming language","task","algorithm"],"instance":{"id":"76","words":["The","input","is","called","speech","recognition","and","the","output","is","called","speech","synthesis","."],"labels":["O","O","O","O","B-task","I-task","O","O","O","O","O","B-task","I-task","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, organization, location, metric, product, person, researcher, university, conference, country, programming language, task, algorithm and O.\nSentence: The input is called speech recognition and the output is called speech synthesis .","prompt_labels":"The(O) input(O) is(O) called(O) speech(B-task) recognition(I-task) and(O) the(O) output(O) is(O) called(O) speech(B-task) synthesis(I-task) .(O)"}}
{"id":"77","dataset":"crossner_ai","split":"train","label_list":["conference","country","researcher","metric","field","task","programming language","product","organization","person","algorithm","university","location"],"instance":{"id":"77","words":["Descendants","of","the","CLIPS","language","include","Jess","(","rule-based","portion","of","CLIPS","rewritten","in","Java",",","it","later","grew","up","in","different","direction",")",",","JESS","was","originally","inspired"],"labels":["O","O","O","B-programming language","O","O","B-programming language","O","O","O","O","B-programming language","O","O","B-programming language","O","O","O","O","O","O","O","O","O","O","B-programming language","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, country, researcher, metric, field, task, programming language, product, organization, person, algorithm, university, location and O.\nSentence: Descendants of the CLIPS language include Jess ( rule-based portion of CLIPS rewritten in Java , it later grew up in different direction ) , JESS was originally inspired","prompt_labels":"Descendants(O) of(O) the(O) CLIPS(B-programming language) language(O) include(O) Jess(B-programming language) ((O) rule-based(O) portion(O) of(O) CLIPS(B-programming language) rewritten(O) in(O) Java(B-programming language) ,(O) it(O) later(O) grew(O) up(O) in(O) different(O) direction(O) )(O) ,(O) JESS(B-programming language) was(O) originally(O) inspired(O)"}}
{"id":"78","dataset":"crossner_ai","split":"train","label_list":["country","university","algorithm","location","metric","researcher","person","organization","task","programming language","product","conference","field"],"instance":{"id":"78","words":["It","also","created","flexible","intelligent","AGV","applications",",","designing","the","Motivity","control","system","used","by","RMT","Robotics","to","develop","its","ADAM","iAGV","(","Self-Guided","Vehicle",")",",","used","for","complex","pick","and","place","operations",",","in","conjunction","with","gantry","systems","and","industrial","robot","arms",",","used","in","first-tier","auto","supply","factories","to","move","products","from","process","to","process","in","non-linear","layouts","."],"labels":["O","O","O","O","O","B-product","O","O","O","O","B-product","I-product","I-product","O","O","B-organization","I-organization","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","I-product","O","B-product","I-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, algorithm, location, metric, researcher, person, organization, task, programming language, product, conference, field and O.\nSentence: It also created flexible intelligent AGV applications , designing the Motivity control system used by RMT Robotics to develop its ADAM iAGV ( Self-Guided Vehicle ) , used for complex pick and place operations , in conjunction with gantry systems and industrial robot arms , used in first-tier auto supply factories to move products from process to process in non-linear layouts .","prompt_labels":"It(O) also(O) created(O) flexible(O) intelligent(O) AGV(B-product) applications(O) ,(O) designing(O) the(O) Motivity(B-product) control(I-product) system(I-product) used(O) by(O) RMT(B-organization) Robotics(I-organization) to(O) develop(O) its(O) ADAM(B-product) iAGV(I-product) ((O) Self-Guided(O) Vehicle(O) )(O) ,(O) used(O) for(O) complex(O) pick(O) and(O) place(O) operations(O) ,(O) in(O) conjunction(O) with(O) gantry(B-product) systems(I-product) and(O) industrial(B-product) robot(I-product) arms(I-product) ,(O) used(O) in(O) first-tier(O) auto(O) supply(O) factories(O) to(O) move(O) products(O) from(O) process(O) to(O) process(O) in(O) non-linear(O) layouts(O) .(O)"}}
{"id":"79","dataset":"crossner_ai","split":"train","label_list":["organization","conference","location","algorithm","programming language","person","metric","field","university","product","researcher","country","task"],"instance":{"id":"79","words":["The","parameters","β","are","typically","estimated","by","maximum","likelihood","."],"labels":["O","O","O","O","O","O","O","B-metric","I-metric","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, conference, location, algorithm, programming language, person, metric, field, university, product, researcher, country, task and O.\nSentence: The parameters β are typically estimated by maximum likelihood .","prompt_labels":"The(O) parameters(O) β(O) are(O) typically(O) estimated(O) by(O) maximum(B-metric) likelihood(I-metric) .(O)"}}
{"id":"80","dataset":"crossner_ai","split":"train","label_list":["country","researcher","metric","location","task","programming language","university","field","person","conference","product","organization","algorithm"],"instance":{"id":"80","words":["The","information","retrieval","metrics","such","as","precision","and","recall","or","DCG","are","useful","to","assess","the","quality","of","a","recommendation","method","."],"labels":["O","B-task","I-task","O","O","O","B-metric","O","B-metric","O","B-metric","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, researcher, metric, location, task, programming language, university, field, person, conference, product, organization, algorithm and O.\nSentence: The information retrieval metrics such as precision and recall or DCG are useful to assess the quality of a recommendation method .","prompt_labels":"The(O) information(B-task) retrieval(I-task) metrics(O) such(O) as(O) precision(B-metric) and(O) recall(B-metric) or(O) DCG(B-metric) are(O) useful(O) to(O) assess(O) the(O) quality(O) of(O) a(O) recommendation(O) method(O) .(O)"}}
{"id":"81","dataset":"crossner_ai","split":"train","label_list":["programming language","metric","person","university","field","task","conference","researcher","country","product","location","organization","algorithm"],"instance":{"id":"81","words":["A","typical","factory","contains","hundreds","of","industrial","robot","s","working","on","fully","automated","production","lines",",","with","one","robot","for","every","ten","human","workers","."],"labels":["O","O","O","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, metric, person, university, field, task, conference, researcher, country, product, location, organization, algorithm and O.\nSentence: A typical factory contains hundreds of industrial robot s working on fully automated production lines , with one robot for every ten human workers .","prompt_labels":"A(O) typical(O) factory(O) contains(O) hundreds(O) of(O) industrial(B-product) robot(I-product) s(O) working(O) on(O) fully(O) automated(O) production(O) lines(O) ,(O) with(O) one(O) robot(O) for(O) every(O) ten(O) human(O) workers(O) .(O)"}}
{"id":"82","dataset":"crossner_ai","split":"train","label_list":["researcher","programming language","algorithm","field","product","conference","metric","person","country","university","task","location","organization"],"instance":{"id":"82","words":["Over","the","past","decade",",","PCNNs","have","been","used","in","a","variety","of","image","processing","applications",",","including",":","image","segmentation",",","feature","generation",",","face","extraction",",","motion","detection",",","region","growing",",","and","noise","reduction","."],"labels":["O","O","O","O","O","B-product","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","O","B-task","I-task","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, programming language, algorithm, field, product, conference, metric, person, country, university, task, location, organization and O.\nSentence: Over the past decade , PCNNs have been used in a variety of image processing applications , including : image segmentation , feature generation , face extraction , motion detection , region growing , and noise reduction .","prompt_labels":"Over(O) the(O) past(O) decade(O) ,(O) PCNNs(B-product) have(O) been(O) used(O) in(O) a(O) variety(O) of(O) image(B-field) processing(I-field) applications(O) ,(O) including(O) :(O) image(B-task) segmentation(I-task) ,(O) feature(B-task) generation(I-task) ,(O) face(B-task) extraction(I-task) ,(O) motion(B-task) detection(I-task) ,(O) region(B-task) growing(I-task) ,(O) and(O) noise(B-task) reduction(I-task) .(O)"}}
{"id":"83","dataset":"crossner_ai","split":"train","label_list":["product","metric","university","country","researcher","field","conference","algorithm","programming language","task","person","organization","location"],"instance":{"id":"83","words":["Xu","has","published","more","than","50","papers","at","international","conferences","and","in","journals","in","the","field","of","computer","vision","and","won","the","Best","Paper","Award","at","the","international","conference","on","Non-Photorealistic","Rendering","and","Animation","(","NPAR",")","2012","and","the","Best","Reviewer","Award","at","the","international","conferences","Asian","Conference","on","Computer","Vision","ACCV","2012","and","International","Conference","on","Computer","Vision","(","ICCV",")","2015","."],"labels":["B-researcher","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","B-conference","I-conference","O","B-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, metric, university, country, researcher, field, conference, algorithm, programming language, task, person, organization, location and O.\nSentence: Xu has published more than 50 papers at international conferences and in journals in the field of computer vision and won the Best Paper Award at the international conference on Non-Photorealistic Rendering and Animation ( NPAR ) 2012 and the Best Reviewer Award at the international conferences Asian Conference on Computer Vision ACCV 2012 and International Conference on Computer Vision ( ICCV ) 2015 .","prompt_labels":"Xu(B-researcher) has(O) published(O) more(O) than(O) 50(O) papers(O) at(O) international(O) conferences(O) and(O) in(O) journals(O) in(O) the(O) field(O) of(O) computer(B-field) vision(I-field) and(O) won(O) the(O) Best(O) Paper(O) Award(O) at(O) the(O) international(B-conference) conference(I-conference) on(I-conference) Non-Photorealistic(I-conference) Rendering(I-conference) and(I-conference) Animation(I-conference) ((O) NPAR(B-conference) )(O) 2012(O) and(O) the(O) Best(O) Reviewer(O) Award(O) at(O) the(O) international(B-conference) conferences(I-conference) Asian(I-conference) Conference(I-conference) on(I-conference) Computer(I-conference) Vision(I-conference) ACCV(B-conference) 2012(I-conference) and(O) International(B-conference) Conference(I-conference) on(I-conference) Computer(I-conference) Vision(I-conference) ((O) ICCV(B-conference) )(O) 2015(O) .(O)"}}
{"id":"84","dataset":"crossner_ai","split":"train","label_list":["field","programming language","university","metric","person","researcher","conference","organization","location","product","task","country","algorithm"],"instance":{"id":"84","words":["CycL","in","computer","science","and","artificial","intelligence","is","an","ontology","language","used","by","Doug","Lenat","'s","Cyc","artificial","project","."],"labels":["B-programming language","O","B-field","I-field","O","B-field","I-field","O","O","O","O","O","O","B-researcher","I-researcher","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, programming language, university, metric, person, researcher, conference, organization, location, product, task, country, algorithm and O.\nSentence: CycL in computer science and artificial intelligence is an ontology language used by Doug Lenat 's Cyc artificial project .","prompt_labels":"CycL(B-programming language) in(O) computer(B-field) science(I-field) and(O) artificial(B-field) intelligence(I-field) is(O) an(O) ontology(O) language(O) used(O) by(O) Doug(B-researcher) Lenat(I-researcher) 's(O) Cyc(O) artificial(O) project(O) .(O)"}}
{"id":"85","dataset":"crossner_ai","split":"train","label_list":["conference","country","metric","product","organization","researcher","programming language","person","university","location","field","algorithm","task"],"instance":{"id":"85","words":["Also","in","regression","analysis",",","mean","squared","error",",","often","referred","to","as","mean","squared","prediction","error","or","out-of-sample","mean","squared","error",",","can","refer","to","the","mean","value","of","the","squared","deviations","of","the","predictions","from","the","TRUE","values",",","over","an","out-of-sample","test","space",",","generated","by","a","model","estimated","over","a","particular","sample","space","."],"labels":["O","O","B-task","I-task","O","B-metric","I-metric","I-metric","O","O","O","O","O","B-metric","I-metric","I-metric","I-metric","O","B-metric","I-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, country, metric, product, organization, researcher, programming language, person, university, location, field, algorithm, task and O.\nSentence: Also in regression analysis , mean squared error , often referred to as mean squared prediction error or out-of-sample mean squared error , can refer to the mean value of the squared deviations of the predictions from the TRUE values , over an out-of-sample test space , generated by a model estimated over a particular sample space .","prompt_labels":"Also(O) in(O) regression(B-task) analysis(I-task) ,(O) mean(B-metric) squared(I-metric) error(I-metric) ,(O) often(O) referred(O) to(O) as(O) mean(B-metric) squared(I-metric) prediction(I-metric) error(I-metric) or(O) out-of-sample(B-metric) mean(I-metric) squared(I-metric) error(I-metric) ,(O) can(O) refer(O) to(O) the(O) mean(O) value(O) of(O) the(O) squared(O) deviations(O) of(O) the(O) predictions(O) from(O) the(O) TRUE(O) values(O) ,(O) over(O) an(O) out-of-sample(O) test(O) space(O) ,(O) generated(O) by(O) a(O) model(O) estimated(O) over(O) a(O) particular(O) sample(O) space(O) .(O)"}}
{"id":"86","dataset":"crossner_ai","split":"train","label_list":["researcher","country","university","algorithm","organization","person","conference","task","metric","product","programming language","location","field"],"instance":{"id":"86","words":["As","for","the","results",",","the","C-HOG","and","R-HOG","block","descriptors","perform","comparably",",","with","the","C-HOG","descriptors","maintaining","a","slight","advantage","in","the","detection","miss","rate","at","fixed","FALSE","positive","rate","s","across","both","data","sets","."],"labels":["O","O","O","O","O","O","B-algorithm","O","B-algorithm","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, country, university, algorithm, organization, person, conference, task, metric, product, programming language, location, field and O.\nSentence: As for the results , the C-HOG and R-HOG block descriptors perform comparably , with the C-HOG descriptors maintaining a slight advantage in the detection miss rate at fixed FALSE positive rate s across both data sets .","prompt_labels":"As(O) for(O) the(O) results(O) ,(O) the(O) C-HOG(B-algorithm) and(O) R-HOG(B-algorithm) block(O) descriptors(O) perform(O) comparably(O) ,(O) with(O) the(O) C-HOG(B-algorithm) descriptors(I-algorithm) maintaining(O) a(O) slight(O) advantage(O) in(O) the(O) detection(O) miss(O) rate(O) at(O) fixed(O) FALSE(B-metric) positive(I-metric) rate(I-metric) s(O) across(O) both(O) data(O) sets(O) .(O)"}}
{"id":"87","dataset":"crossner_ai","split":"train","label_list":["researcher","conference","person","product","algorithm","task","organization","country","location","field","university","programming language","metric"],"instance":{"id":"87","words":["Popular","recognition","algorithms","include","principal","component","analysis","using","eigenface","s",",","linear","discriminant","analysis",",","Elastic","matching","using","the","Fisherface","algorithm",",","the","hidden","Markov","model",",","the","multilinear","subspace","learning","using","tensor","representation",",","and","the","neuronal","motivated","dynamic","link","matching","."],"labels":["O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, conference, person, product, algorithm, task, organization, country, location, field, university, programming language, metric and O.\nSentence: Popular recognition algorithms include principal component analysis using eigenface s , linear discriminant analysis , Elastic matching using the Fisherface algorithm , the hidden Markov model , the multilinear subspace learning using tensor representation , and the neuronal motivated dynamic link matching .","prompt_labels":"Popular(O) recognition(O) algorithms(O) include(O) principal(B-algorithm) component(I-algorithm) analysis(I-algorithm) using(O) eigenface(O) s(O) ,(O) linear(B-algorithm) discriminant(I-algorithm) analysis(I-algorithm) ,(O) Elastic(B-algorithm) matching(I-algorithm) using(O) the(O) Fisherface(B-algorithm) algorithm(I-algorithm) ,(O) the(O) hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) ,(O) the(O) multilinear(B-algorithm) subspace(I-algorithm) learning(I-algorithm) using(O) tensor(O) representation(O) ,(O) and(O) the(O) neuronal(O) motivated(O) dynamic(B-algorithm) link(I-algorithm) matching(I-algorithm) .(O)"}}
{"id":"88","dataset":"crossner_ai","split":"train","label_list":["algorithm","country","field","product","university","person","organization","metric","location","researcher","task","programming language","conference"],"instance":{"id":"88","words":["Beginning","at","the","2019","Toronto","International","Film","Festival",",","films","may","now","be","restricted","from","screening","at","Scotiabank","Theatre","Toronto","-","one","of","the","festival","'s","main","venues","-","and","screened","elsewhere","(","such","as","TIFF","Bell","Lightbox","and","other","local","cinemas",")","if","distributed","by","a","service","such","as","Netflix","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, country, field, product, university, person, organization, metric, location, researcher, task, programming language, conference and O.\nSentence: Beginning at the 2019 Toronto International Film Festival , films may now be restricted from screening at Scotiabank Theatre Toronto - one of the festival 's main venues - and screened elsewhere ( such as TIFF Bell Lightbox and other local cinemas ) if distributed by a service such as Netflix .","prompt_labels":"Beginning(O) at(O) the(O) 2019(O) Toronto(O) International(O) Film(O) Festival(O) ,(O) films(O) may(O) now(O) be(O) restricted(O) from(O) screening(O) at(O) Scotiabank(B-location) Theatre(I-location) Toronto(I-location) -(O) one(O) of(O) the(O) festival(O) 's(O) main(O) venues(O) -(O) and(O) screened(O) elsewhere(O) ((O) such(O) as(O) TIFF(B-location) Bell(I-location) Lightbox(I-location) and(O) other(O) local(O) cinemas(O) )(O) if(O) distributed(O) by(O) a(O) service(O) such(O) as(O) Netflix(B-organization) .(O)"}}
{"id":"89","dataset":"crossner_ai","split":"train","label_list":["organization","task","country","person","field","conference","university","metric","algorithm","product","location","programming language","researcher"],"instance":{"id":"89","words":["Unimation","purchased","Victor","Scheinman","'","s","Vicarm","Inc.","in","1977",",","and","with","Scheinman","'s","help",",","the","company","created","and","began","producing","the","Programmable","Universal","Machine","for","Assembly",",","a","new","model","of","robotic","arm",",","and","using","Scheinman","'s","cutting-edge","VAL","programming","language","."],"labels":["B-organization","O","B-researcher","I-researcher","O","O","B-organization","I-organization","O","O","O","O","O","B-researcher","O","O","O","O","O","O","O","O","O","O","B-product","I-product","I-product","I-product","I-product","O","O","O","O","O","O","O","O","O","O","B-researcher","O","O","B-programming language","I-programming language","I-programming language","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, task, country, person, field, conference, university, metric, algorithm, product, location, programming language, researcher and O.\nSentence: Unimation purchased Victor Scheinman ' s Vicarm Inc. in 1977 , and with Scheinman 's help , the company created and began producing the Programmable Universal Machine for Assembly , a new model of robotic arm , and using Scheinman 's cutting-edge VAL programming language .","prompt_labels":"Unimation(B-organization) purchased(O) Victor(B-researcher) Scheinman(I-researcher) '(O) s(O) Vicarm(B-organization) Inc.(I-organization) in(O) 1977(O) ,(O) and(O) with(O) Scheinman(B-researcher) 's(O) help(O) ,(O) the(O) company(O) created(O) and(O) began(O) producing(O) the(O) Programmable(B-product) Universal(I-product) Machine(I-product) for(I-product) Assembly(I-product) ,(O) a(O) new(O) model(O) of(O) robotic(O) arm(O) ,(O) and(O) using(O) Scheinman(B-researcher) 's(O) cutting-edge(O) VAL(B-programming language) programming(I-programming language) language(I-programming language) .(O)"}}
{"id":"90","dataset":"crossner_ai","split":"train","label_list":["field","researcher","metric","person","task","country","organization","conference","university","location","programming language","product","algorithm"],"instance":{"id":"90","words":["J48","is","an","open","source","Java","implementation","of","the","C4.5","algorithm","in","the","Weka","data","mining","tool","."],"labels":["B-product","O","O","O","O","B-programming language","O","O","O","B-algorithm","I-algorithm","O","O","B-product","I-product","I-product","I-product","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, researcher, metric, person, task, country, organization, conference, university, location, programming language, product, algorithm and O.\nSentence: J48 is an open source Java implementation of the C4.5 algorithm in the Weka data mining tool .","prompt_labels":"J48(B-product) is(O) an(O) open(O) source(O) Java(B-programming language) implementation(O) of(O) the(O) C4.5(B-algorithm) algorithm(I-algorithm) in(O) the(O) Weka(B-product) data(I-product) mining(I-product) tool(I-product) .(O)"}}
{"id":"91","dataset":"crossner_ai","split":"train","label_list":["programming language","country","metric","university","person","organization","algorithm","conference","task","field","product","location","researcher"],"instance":{"id":"91","words":["The","2004","SSIM","paper","has","been","cited","over","20,000","times","according","to","Google","Scholar",",","It","also","received","the","IEEE","Signal","Processing","Society","Sustained","Impact","Award","for","2016",",","indicative","of","a","paper","having","an","unusually","high","impact","for","at","least","10","years","following","its","publication","."],"labels":["O","O","B-metric","O","O","O","O","O","O","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, country, metric, university, person, organization, algorithm, conference, task, field, product, location, researcher and O.\nSentence: The 2004 SSIM paper has been cited over 20,000 times according to Google Scholar , It also received the IEEE Signal Processing Society Sustained Impact Award for 2016 , indicative of a paper having an unusually high impact for at least 10 years following its publication .","prompt_labels":"The(O) 2004(O) SSIM(B-metric) paper(O) has(O) been(O) cited(O) over(O) 20,000(O) times(O) according(O) to(O) Google(B-product) Scholar(I-product) ,(O) It(O) also(O) received(O) the(O) IEEE(O) Signal(O) Processing(O) Society(O) Sustained(O) Impact(O) Award(O) for(O) 2016(O) ,(O) indicative(O) of(O) a(O) paper(O) having(O) an(O) unusually(O) high(O) impact(O) for(O) at(O) least(O) 10(O) years(O) following(O) its(O) publication(O) .(O)"}}
{"id":"92","dataset":"crossner_ai","split":"train","label_list":["country","researcher","location","product","conference","algorithm","field","programming language","organization","university","metric","person","task"],"instance":{"id":"92","words":["The","speech","synthesis","is","verging","on","being","completely","indistinguishable","from","a","real","human","'s","voice","with","the","2016","introduction","of","the","voice","editing","and","generation","software","Adobe","Voco",",","a","prototype","slated","to","be","a","part","of","the","Adobe","Creative","Suite","and","DeepMind","WaveNet",",","a","prototype","from","Google","."],"labels":["O","B-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","B-product","I-product","I-product","O","B-organization","B-product","O","O","O","O","B-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, researcher, location, product, conference, algorithm, field, programming language, organization, university, metric, person, task and O.\nSentence: The speech synthesis is verging on being completely indistinguishable from a real human 's voice with the 2016 introduction of the voice editing and generation software Adobe Voco , a prototype slated to be a part of the Adobe Creative Suite and DeepMind WaveNet , a prototype from Google .","prompt_labels":"The(O) speech(B-task) synthesis(I-task) is(O) verging(O) on(O) being(O) completely(O) indistinguishable(O) from(O) a(O) real(O) human(O) 's(O) voice(O) with(O) the(O) 2016(O) introduction(O) of(O) the(O) voice(O) editing(O) and(O) generation(O) software(O) Adobe(B-product) Voco(I-product) ,(O) a(O) prototype(O) slated(O) to(O) be(O) a(O) part(O) of(O) the(O) Adobe(B-product) Creative(I-product) Suite(I-product) and(O) DeepMind(B-organization) WaveNet(B-product) ,(O) a(O) prototype(O) from(O) Google(B-organization) .(O)"}}
{"id":"93","dataset":"crossner_ai","split":"train","label_list":["conference","metric","organization","programming language","university","country","task","algorithm","person","product","researcher","field","location"],"instance":{"id":"93","words":["Poggio","is","an","honorary","member","of","the","Neuroscience","Research","Program",",","a","member","of","the","American","Academy","of","Arts","and","Sciences","and","a","founding","fellow","of","AAAI","and","a","founding","member","of","the","McGovern","Institute","for","Brain","Research","."],"labels":["B-researcher","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","B-conference","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, metric, organization, programming language, university, country, task, algorithm, person, product, researcher, field, location and O.\nSentence: Poggio is an honorary member of the Neuroscience Research Program , a member of the American Academy of Arts and Sciences and a founding fellow of AAAI and a founding member of the McGovern Institute for Brain Research .","prompt_labels":"Poggio(B-researcher) is(O) an(O) honorary(O) member(O) of(O) the(O) Neuroscience(B-organization) Research(I-organization) Program(I-organization) ,(O) a(O) member(O) of(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) and(O) a(O) founding(O) fellow(O) of(O) AAAI(B-conference) and(O) a(O) founding(O) member(O) of(O) the(O) McGovern(B-organization) Institute(I-organization) for(I-organization) Brain(I-organization) Research(I-organization) .(O)"}}
{"id":"94","dataset":"crossner_ai","split":"train","label_list":["metric","location","algorithm","organization","product","task","country","conference","researcher","person","university","field","programming language"],"instance":{"id":"94","words":["During","the","1990s",",","encouraged","by","successes","in","speech","recognition","and","speech","synthesis",",","research","began","into","speech","translation","with","the","development","of","the","German","Verbmobil","project","."],"labels":["O","O","O","O","O","O","O","O","B-task","I-task","O","B-task","I-task","O","O","O","O","B-task","I-task","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, location, algorithm, organization, product, task, country, conference, researcher, person, university, field, programming language and O.\nSentence: During the 1990s , encouraged by successes in speech recognition and speech synthesis , research began into speech translation with the development of the German Verbmobil project .","prompt_labels":"During(O) the(O) 1990s(O) ,(O) encouraged(O) by(O) successes(O) in(O) speech(B-task) recognition(I-task) and(O) speech(B-task) synthesis(I-task) ,(O) research(O) began(O) into(O) speech(B-task) translation(I-task) with(O) the(O) development(O) of(O) the(O) German(O) Verbmobil(O) project(O) .(O)"}}
{"id":"95","dataset":"crossner_ai","split":"train","label_list":["person","researcher","university","metric","programming language","algorithm","organization","product","conference","task","field","location","country"],"instance":{"id":"95","words":["In","1999",",","Felix","Gers","and","his","advisor","Jürgen","Schmidhuber","and","Fred","Cummins","introduced","the","forget","gate","(","also","called","keep","gate",")","into","LSTM","architecture",","],"labels":["O","O","O","B-researcher","I-researcher","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","O","B-algorithm","I-algorithm","O","O","O","B-algorithm","I-algorithm","O","O","B-algorithm","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, researcher, university, metric, programming language, algorithm, organization, product, conference, task, field, location, country and O.\nSentence: In 1999 , Felix Gers and his advisor Jürgen Schmidhuber and Fred Cummins introduced the forget gate ( also called keep gate ) into LSTM architecture ,","prompt_labels":"In(O) 1999(O) ,(O) Felix(B-researcher) Gers(I-researcher) and(O) his(O) advisor(O) Jürgen(B-researcher) Schmidhuber(I-researcher) and(O) Fred(B-researcher) Cummins(I-researcher) introduced(O) the(O) forget(B-algorithm) gate(I-algorithm) ((O) also(O) called(O) keep(B-algorithm) gate(I-algorithm) )(O) into(O) LSTM(B-algorithm) architecture(O) ,(O)"}}
{"id":"96","dataset":"crossner_ai","split":"train","label_list":["task","programming language","organization","field","product","location","person","metric","algorithm","university","country","researcher","conference"],"instance":{"id":"96","words":["In","digital","signal","processing","and","information","theory",",","the","normalized","sinc","function","is","commonly","defined","for","by"],"labels":["O","B-field","I-field","I-field","O","B-field","I-field","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, programming language, organization, field, product, location, person, metric, algorithm, university, country, researcher, conference and O.\nSentence: In digital signal processing and information theory , the normalized sinc function is commonly defined for by","prompt_labels":"In(O) digital(B-field) signal(I-field) processing(I-field) and(O) information(B-field) theory(I-field) ,(O) the(O) normalized(B-algorithm) sinc(I-algorithm) function(I-algorithm) is(O) commonly(O) defined(O) for(O) by(O)"}}
{"id":"97","dataset":"crossner_ai","split":"train","label_list":["person","metric","conference","researcher","product","programming language","university","task","location","organization","field","country","algorithm"],"instance":{"id":"97","words":["The","term","computational","linguistics","itself","was","first","coined","by","David","Hays",",","a","founding","member","of","both","the","Association","for","Computational","Linguistics","and","the","International","Committee","on","Computational","Linguistics","(","ICCL",")","."],"labels":["O","O","B-field","I-field","O","O","O","O","O","B-researcher","I-researcher","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, metric, conference, researcher, product, programming language, university, task, location, organization, field, country, algorithm and O.\nSentence: The term computational linguistics itself was first coined by David Hays , a founding member of both the Association for Computational Linguistics and the International Committee on Computational Linguistics ( ICCL ) .","prompt_labels":"The(O) term(O) computational(B-field) linguistics(I-field) itself(O) was(O) first(O) coined(O) by(O) David(B-researcher) Hays(I-researcher) ,(O) a(O) founding(O) member(O) of(O) both(O) the(O) Association(B-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) and(O) the(O) International(B-organization) Committee(I-organization) on(I-organization) Computational(I-organization) Linguistics(I-organization) ((O) ICCL(B-organization) )(O) .(O)"}}
{"id":"98","dataset":"crossner_ai","split":"train","label_list":["country","university","researcher","location","metric","person","organization","programming language","conference","field","task","algorithm","product"],"instance":{"id":"98","words":["59",",","pp.","2547-2553",",","Oct.","2011","In","one","dimensional","polynomial-based","memory","(","or","memoryless",")","DPD",",","in","order","to","solve","for","the","digital","pre-distorter","polynomials","coefficients","and","minimize","the","mean","squared","error","(","MSE",")",",","the","distorted","output","of","the","nonlinear","system","must","be","over-sampled","at","a","rate","that","enables","the","capture","of","the","nonlinear","products","of","the","order","of","the","digital","pre-distorter","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","B-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, researcher, location, metric, person, organization, programming language, conference, field, task, algorithm, product and O.\nSentence: 59 , pp. 2547-2553 , Oct. 2011 In one dimensional polynomial-based memory ( or memoryless ) DPD , in order to solve for the digital pre-distorter polynomials coefficients and minimize the mean squared error ( MSE ) , the distorted output of the nonlinear system must be over-sampled at a rate that enables the capture of the nonlinear products of the order of the digital pre-distorter .","prompt_labels":"59(O) ,(O) pp.(O) 2547-2553(O) ,(O) Oct.(O) 2011(O) In(O) one(O) dimensional(O) polynomial-based(O) memory(O) ((O) or(O) memoryless(O) )(O) DPD(O) ,(O) in(O) order(O) to(O) solve(O) for(O) the(O) digital(O) pre-distorter(O) polynomials(O) coefficients(O) and(O) minimize(O) the(O) mean(B-metric) squared(I-metric) error(I-metric) ((O) MSE(B-metric) )(O) ,(O) the(O) distorted(O) output(O) of(O) the(O) nonlinear(O) system(O) must(O) be(O) over-sampled(O) at(O) a(O) rate(O) that(O) enables(O) the(O) capture(O) of(O) the(O) nonlinear(O) products(O) of(O) the(O) order(O) of(O) the(O) digital(O) pre-distorter(O) .(O)"}}
{"id":"99","dataset":"crossner_ai","split":"train","label_list":["task","organization","country","programming language","university","algorithm","field","location","person","researcher","conference","metric","product"],"instance":{"id":"99","words":["Boris","Katz",",","(","born","October","5",",","1947",",","Chișinău",",","Moldavian","SSR",",","Soviet","Union",",","(","now","Chișinău",",","Moldova",")",")","is","a","principal","American","research","scientist","(","computer","scientist",")","at","the","MIT","Computer","Science","and","Artificial","Intelligence","Laboratory","at","the","Massachusetts","Institute","of","Technology","in","Cambridge","and","head","of","the","Laboratory","'s","InfoLab","Group","."],"labels":["B-researcher","I-researcher","O","O","O","O","O","O","O","O","B-location","O","B-location","I-location","O","B-country","I-country","O","O","O","B-location","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","B-university","O","O","O","O","O","O","B-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, organization, country, programming language, university, algorithm, field, location, person, researcher, conference, metric, product and O.\nSentence: Boris Katz , ( born October 5 , 1947 , Chișinău , Moldavian SSR , Soviet Union , ( now Chișinău , Moldova ) ) is a principal American research scientist ( computer scientist ) at the MIT Computer Science and Artificial Intelligence Laboratory at the Massachusetts Institute of Technology in Cambridge and head of the Laboratory 's InfoLab Group .","prompt_labels":"Boris(B-researcher) Katz(I-researcher) ,(O) ((O) born(O) October(O) 5(O) ,(O) 1947(O) ,(O) Chișinău(B-location) ,(O) Moldavian(B-location) SSR(I-location) ,(O) Soviet(B-country) Union(I-country) ,(O) ((O) now(O) Chișinău(B-location) ,(O) Moldova(B-country) )(O) )(O) is(O) a(O) principal(O) American(O) research(O) scientist(O) ((O) computer(O) scientist(O) )(O) at(O) the(O) MIT(B-organization) Computer(I-organization) Science(I-organization) and(I-organization) Artificial(I-organization) Intelligence(I-organization) Laboratory(I-organization) at(O) the(O) Massachusetts(B-organization) Institute(I-organization) of(I-organization) Technology(I-organization) in(O) Cambridge(B-university) and(O) head(O) of(O) the(O) Laboratory(O) 's(O) InfoLab(B-organization) Group(I-organization) .(O)"}}
{"id":"0","dataset":"crossner_literature","split":"train","label_list":["location","event","organization","writer","literary genre","magazine","poem","country","award","person","book"],"instance":{"id":"0","words":["In","1351",",","during","the","reign","of","Emperor","Toghon","Temür","of","the","Yuan","dynasty",",","93rd-generation","descendant","Kong","Huan","(","孔浣",")","'","s","2nd","son","Kong","Shao","(","孔昭",")","moved","from","China","to","Korea","during","the","Goryeo",",","and","was","received","courteously","by","Princess","Noguk","(","the","Mongolian-born","wife","of","the","future","king","Gongmin",")","."],"labels":["O","O","O","O","O","O","O","B-person","I-person","I-person","O","O","B-country","I-country","O","O","O","B-writer","I-writer","O","B-writer","O","O","O","O","O","B-writer","I-writer","O","B-writer","O","O","O","B-country","O","B-country","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","B-person","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, organization, writer, literary genre, magazine, poem, country, award, person, book and O.\nSentence: In 1351 , during the reign of Emperor Toghon Temür of the Yuan dynasty , 93rd-generation descendant Kong Huan ( 孔浣 ) ' s 2nd son Kong Shao ( 孔昭 ) moved from China to Korea during the Goryeo , and was received courteously by Princess Noguk ( the Mongolian-born wife of the future king Gongmin ) .","prompt_labels":"In(O) 1351(O) ,(O) during(O) the(O) reign(O) of(O) Emperor(B-person) Toghon(I-person) Temür(I-person) of(O) the(O) Yuan(B-country) dynasty(I-country) ,(O) 93rd-generation(O) descendant(O) Kong(B-writer) Huan(I-writer) ((O) 孔浣(B-writer) )(O) '(O) s(O) 2nd(O) son(O) Kong(B-writer) Shao(I-writer) ((O) 孔昭(B-writer) )(O) moved(O) from(O) China(B-country) to(O) Korea(B-country) during(O) the(O) Goryeo(O) ,(O) and(O) was(O) received(O) courteously(O) by(O) Princess(B-person) Noguk(I-person) ((O) the(O) Mongolian-born(O) wife(O) of(O) the(O) future(O) king(O) Gongmin(B-person) )(O) .(O)"}}
{"id":"1","dataset":"crossner_literature","split":"train","label_list":["country","organization","poem","magazine","book","writer","literary genre","event","location","person","award"],"instance":{"id":"1","words":["In","1990",",","he","published","the","last","Rabbit","novel",",","Rabbit","at","Rest",",","which","won","the","Pulitzer","Prize","and","the","National","Book","Critics","Circle","Award","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, poem, magazine, book, writer, literary genre, event, location, person, award and O.\nSentence: In 1990 , he published the last Rabbit novel , Rabbit at Rest , which won the Pulitzer Prize and the National Book Critics Circle Award .","prompt_labels":"In(O) 1990(O) ,(O) he(O) published(O) the(O) last(O) Rabbit(O) novel(O) ,(O) Rabbit(B-book) at(I-book) Rest(I-book) ,(O) which(O) won(O) the(O) Pulitzer(B-award) Prize(I-award) and(O) the(O) National(B-award) Book(I-award) Critics(I-award) Circle(I-award) Award(I-award) .(O)"}}
{"id":"2","dataset":"crossner_literature","split":"train","label_list":["poem","event","person","country","award","location","organization","magazine","literary genre","writer","book"],"instance":{"id":"2","words":["She","is","known","for","her","two","best-selling","novels",",","The","Fountainhead","and","Atlas","Shrugged",",","and","for","developing","a","philosophical","system","she","named","Objectivism","."],"labels":["O","O","O","O","O","O","O","B-literary genre","O","B-book","I-book","O","B-book","I-book","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, event, person, country, award, location, organization, magazine, literary genre, writer, book and O.\nSentence: She is known for her two best-selling novels , The Fountainhead and Atlas Shrugged , and for developing a philosophical system she named Objectivism .","prompt_labels":"She(O) is(O) known(O) for(O) her(O) two(O) best-selling(O) novels(B-literary genre) ,(O) The(B-book) Fountainhead(I-book) and(O) Atlas(B-book) Shrugged(I-book) ,(O) and(O) for(O) developing(O) a(O) philosophical(O) system(O) she(O) named(O) Objectivism(O) .(O)"}}
{"id":"3","dataset":"crossner_literature","split":"train","label_list":["book","location","event","magazine","writer","organization","poem","country","literary genre","person","award"],"instance":{"id":"3","words":["The","following","year","they","collaborated","on","a","musical","film","version","of","The","Little","Prince",",","based","on","the","classic","children","'s","tale","by","Antoine","de","Saint-Exupéry","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, location, event, magazine, writer, organization, poem, country, literary genre, person, award and O.\nSentence: The following year they collaborated on a musical film version of The Little Prince , based on the classic children 's tale by Antoine de Saint-Exupéry .","prompt_labels":"The(O) following(O) year(O) they(O) collaborated(O) on(O) a(O) musical(O) film(O) version(O) of(O) The(B-book) Little(I-book) Prince(I-book) ,(O) based(O) on(O) the(O) classic(O) children(O) 's(O) tale(O) by(O) Antoine(B-writer) de(I-writer) Saint-Exupéry(I-writer) .(O)"}}
{"id":"4","dataset":"crossner_literature","split":"train","label_list":["magazine","literary genre","event","poem","location","book","country","organization","award","person","writer"],"instance":{"id":"4","words":["Lindbergh","'s","Pulitzer","Prize","-winning","biographer",",","A.","Scott","Berg",",","contended","that","Lindbergh","was","not","so","much","a","supporter","of","the","Nazi","regime","as","someone","so","stubborn","in","his","convictions","and","relatively","inexperienced","in","political","maneuvering","that","he","easily","allowed","rivals","to","portray","him","as","one","."],"labels":["B-person","O","B-award","I-award","O","O","O","B-writer","I-writer","I-writer","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, literary genre, event, poem, location, book, country, organization, award, person, writer and O.\nSentence: Lindbergh 's Pulitzer Prize -winning biographer , A. Scott Berg , contended that Lindbergh was not so much a supporter of the Nazi regime as someone so stubborn in his convictions and relatively inexperienced in political maneuvering that he easily allowed rivals to portray him as one .","prompt_labels":"Lindbergh(B-person) 's(O) Pulitzer(B-award) Prize(I-award) -winning(O) biographer(O) ,(O) A.(B-writer) Scott(I-writer) Berg(I-writer) ,(O) contended(O) that(O) Lindbergh(B-person) was(O) not(O) so(O) much(O) a(O) supporter(O) of(O) the(O) Nazi(O) regime(O) as(O) someone(O) so(O) stubborn(O) in(O) his(O) convictions(O) and(O) relatively(O) inexperienced(O) in(O) political(O) maneuvering(O) that(O) he(O) easily(O) allowed(O) rivals(O) to(O) portray(O) him(O) as(O) one(O) .(O)"}}
{"id":"5","dataset":"crossner_literature","split":"train","label_list":["country","writer","event","location","literary genre","magazine","organization","award","person","book","poem"],"instance":{"id":"5","words":["Highly","regarded","in","his","lifetime","and","for","a","period","thereafter",",","he","is","now","largely","remembered","for","his","anti-slavery","writings","and","his","poems","Barbara","Frietchie",",","The","Barefoot","Boy",",","Maud","Muller","and","Snow-Bound","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","B-literary genre","B-poem","I-poem","O","B-poem","I-poem","I-poem","O","B-poem","I-poem","O","B-poem","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, writer, event, location, literary genre, magazine, organization, award, person, book, poem and O.\nSentence: Highly regarded in his lifetime and for a period thereafter , he is now largely remembered for his anti-slavery writings and his poems Barbara Frietchie , The Barefoot Boy , Maud Muller and Snow-Bound .","prompt_labels":"Highly(O) regarded(O) in(O) his(O) lifetime(O) and(O) for(O) a(O) period(O) thereafter(O) ,(O) he(O) is(O) now(O) largely(O) remembered(O) for(O) his(O) anti-slavery(B-literary genre) writings(I-literary genre) and(O) his(O) poems(B-literary genre) Barbara(B-poem) Frietchie(I-poem) ,(O) The(B-poem) Barefoot(I-poem) Boy(I-poem) ,(O) Maud(B-poem) Muller(I-poem) and(O) Snow-Bound(B-poem) .(O)"}}
{"id":"6","dataset":"crossner_literature","split":"train","label_list":["book","location","organization","award","writer","poem","event","country","magazine","literary genre","person"],"instance":{"id":"6","words":["7th","Century","CE",")",",","author","of","Shishupala","Vadha",",","an","epic","famous","for","its","linguistic","ingenuity",",","and","Śrīharṣa","(","12th","century","CE",")",",","author","of","Naishadha","Charita","(","Naiṣadhīya-carita",")","."],"labels":["O","O","O","O","O","O","O","B-poem","I-poem","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","B-poem","I-poem","O","B-poem","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, location, organization, award, writer, poem, event, country, magazine, literary genre, person and O.\nSentence: 7th Century CE ) , author of Shishupala Vadha , an epic famous for its linguistic ingenuity , and Śrīharṣa ( 12th century CE ) , author of Naishadha Charita ( Naiṣadhīya-carita ) .","prompt_labels":"7th(O) Century(O) CE(O) )(O) ,(O) author(O) of(O) Shishupala(B-poem) Vadha(I-poem) ,(O) an(O) epic(O) famous(O) for(O) its(O) linguistic(O) ingenuity(O) ,(O) and(O) Śrīharṣa(B-writer) ((O) 12th(O) century(O) CE(O) )(O) ,(O) author(O) of(O) Naishadha(B-poem) Charita(I-poem) ((O) Naiṣadhīya-carita(B-poem) )(O) .(O)"}}
{"id":"7","dataset":"crossner_literature","split":"train","label_list":["location","writer","organization","magazine","country","book","literary genre","poem","event","person","award"],"instance":{"id":"7","words":["He","then","went","to","live","at","Chalcedon",",","whence","in","367","he","was","banished","to","Mauretania","for","harbouring","the","rebel","Procopius","."],"labels":["O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","B-country","O","O","O","O","B-writer","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, writer, organization, magazine, country, book, literary genre, poem, event, person, award and O.\nSentence: He then went to live at Chalcedon , whence in 367 he was banished to Mauretania for harbouring the rebel Procopius .","prompt_labels":"He(O) then(O) went(O) to(O) live(O) at(O) Chalcedon(B-location) ,(O) whence(O) in(O) 367(O) he(O) was(O) banished(O) to(O) Mauretania(B-country) for(O) harbouring(O) the(O) rebel(O) Procopius(B-writer) .(O)"}}
{"id":"8","dataset":"crossner_literature","split":"train","label_list":["award","literary genre","country","magazine","writer","poem","person","book","organization","event","location"],"instance":{"id":"8","words":["Far","more","manuscripts","of","the","Prick","of","Conscience","than","any","other","Middle","English","poem","survive","."],"labels":["O","O","O","O","O","B-poem","I-poem","I-poem","O","O","O","O","O","B-literary genre","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, literary genre, country, magazine, writer, poem, person, book, organization, event, location and O.\nSentence: Far more manuscripts of the Prick of Conscience than any other Middle English poem survive .","prompt_labels":"Far(O) more(O) manuscripts(O) of(O) the(O) Prick(B-poem) of(I-poem) Conscience(I-poem) than(O) any(O) other(O) Middle(O) English(O) poem(B-literary genre) survive(O) .(O)"}}
{"id":"9","dataset":"crossner_literature","split":"train","label_list":["literary genre","country","organization","award","writer","poem","location","person","event","magazine","book"],"instance":{"id":"9","words":["This","uses","the","words","of","war","poet","Wilfred","Owen","'","s","At","a","Calvary","near","the","Ancre","."],"labels":["O","O","O","O","O","O","O","B-writer","I-writer","O","O","B-poem","I-poem","I-poem","I-poem","I-poem","I-poem","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, country, organization, award, writer, poem, location, person, event, magazine, book and O.\nSentence: This uses the words of war poet Wilfred Owen ' s At a Calvary near the Ancre .","prompt_labels":"This(O) uses(O) the(O) words(O) of(O) war(O) poet(O) Wilfred(B-writer) Owen(I-writer) '(O) s(O) At(B-poem) a(I-poem) Calvary(I-poem) near(I-poem) the(I-poem) Ancre(I-poem) .(O)"}}
{"id":"10","dataset":"crossner_literature","split":"train","label_list":["country","book","person","writer","award","location","organization","event","literary genre","poem","magazine"],"instance":{"id":"10","words":["Arrian",",","Anabasis","Alexandri","1.12.1",",","Cicero",",","Pro","Archia","Poeta","24","."],"labels":["B-writer","O","B-book","I-book","O","O","B-writer","O","B-book","I-book","I-book","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, book, person, writer, award, location, organization, event, literary genre, poem, magazine and O.\nSentence: Arrian , Anabasis Alexandri 1.12.1 , Cicero , Pro Archia Poeta 24 .","prompt_labels":"Arrian(B-writer) ,(O) Anabasis(B-book) Alexandri(I-book) 1.12.1(O) ,(O) Cicero(B-writer) ,(O) Pro(B-book) Archia(I-book) Poeta(I-book) 24(O) .(O)"}}
{"id":"11","dataset":"crossner_literature","split":"train","label_list":["country","organization","person","magazine","award","event","writer","literary genre","book","poem","location"],"instance":{"id":"11","words":["United","States","poets","such","as","John","Ashbery",",","Marilyn","Hacker",",","Donald","Justice","(","Pantoum","of","the","Great","Depression",")",",","and","David","Trinidad","have","done","work","in","this","form",",","as","has","Irish","poet","Caitriona","O","'Reilly","."],"labels":["B-country","I-country","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-book","I-book","I-book","I-book","I-book","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, person, magazine, award, event, writer, literary genre, book, poem, location and O.\nSentence: United States poets such as John Ashbery , Marilyn Hacker , Donald Justice ( Pantoum of the Great Depression ) , and David Trinidad have done work in this form , as has Irish poet Caitriona O 'Reilly .","prompt_labels":"United(B-country) States(I-country) poets(O) such(O) as(O) John(B-writer) Ashbery(I-writer) ,(O) Marilyn(B-writer) Hacker(I-writer) ,(O) Donald(B-writer) Justice(I-writer) ((O) Pantoum(B-book) of(I-book) the(I-book) Great(I-book) Depression(I-book) )(O) ,(O) and(O) David(B-writer) Trinidad(I-writer) have(O) done(O) work(O) in(O) this(O) form(O) ,(O) as(O) has(O) Irish(O) poet(O) Caitriona(B-writer) O(I-writer) 'Reilly(I-writer) .(O)"}}
{"id":"12","dataset":"crossner_literature","split":"train","label_list":["location","literary genre","award","event","organization","book","poem","magazine","person","country","writer"],"instance":{"id":"12","words":["In","2009",",","Erdrich","was","a","Pulitzer","Prize","finalist","for","The","Plague","of","Doves","The","Plague","of","Doves","focuses","on","the","historical","lynching","of","four","Native","people","wrongly","accused","of","murdering","a","Caucasian","family",",","and","the","effect","of","this","injustice","on","the","current","generations","."],"labels":["O","O","O","B-writer","O","O","B-award","I-award","O","O","B-book","I-book","I-book","I-book","B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, literary genre, award, event, organization, book, poem, magazine, person, country, writer and O.\nSentence: In 2009 , Erdrich was a Pulitzer Prize finalist for The Plague of Doves The Plague of Doves focuses on the historical lynching of four Native people wrongly accused of murdering a Caucasian family , and the effect of this injustice on the current generations .","prompt_labels":"In(O) 2009(O) ,(O) Erdrich(B-writer) was(O) a(O) Pulitzer(B-award) Prize(I-award) finalist(O) for(O) The(B-book) Plague(I-book) of(I-book) Doves(I-book) The(B-book) Plague(I-book) of(I-book) Doves(I-book) focuses(O) on(O) the(O) historical(O) lynching(O) of(O) four(O) Native(O) people(O) wrongly(O) accused(O) of(O) murdering(O) a(O) Caucasian(O) family(O) ,(O) and(O) the(O) effect(O) of(O) this(O) injustice(O) on(O) the(O) current(O) generations(O) .(O)"}}
{"id":"13","dataset":"crossner_literature","split":"train","label_list":["event","literary genre","person","poem","writer","book","organization","country","location","award","magazine"],"instance":{"id":"13","words":["Under","the","influence","of","Adrian","Maniu",",","the","adolescent","Tzara","became","interested","in","Symbolism","and","co-founded","the","magazine","Simbolul","with","Ion","Vinea","(","with","whom","he","also","wrote","experimental","poetry",")","and","painter","Marcel","Janco","."],"labels":["O","O","O","O","B-writer","I-writer","O","O","O","B-writer","O","O","O","B-literary genre","O","O","O","O","B-magazine","O","B-writer","I-writer","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","O","B-person","I-person","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, literary genre, person, poem, writer, book, organization, country, location, award, magazine and O.\nSentence: Under the influence of Adrian Maniu , the adolescent Tzara became interested in Symbolism and co-founded the magazine Simbolul with Ion Vinea ( with whom he also wrote experimental poetry ) and painter Marcel Janco .","prompt_labels":"Under(O) the(O) influence(O) of(O) Adrian(B-writer) Maniu(I-writer) ,(O) the(O) adolescent(O) Tzara(B-writer) became(O) interested(O) in(O) Symbolism(B-literary genre) and(O) co-founded(O) the(O) magazine(O) Simbolul(B-magazine) with(O) Ion(B-writer) Vinea(I-writer) ((O) with(O) whom(O) he(O) also(O) wrote(O) experimental(B-literary genre) poetry(I-literary genre) )(O) and(O) painter(O) Marcel(B-person) Janco(I-person) .(O)"}}
{"id":"14","dataset":"crossner_literature","split":"train","label_list":["organization","literary genre","person","event","award","writer","location","magazine","poem","book","country"],"instance":{"id":"14","words":["The","film","was","presented","at","the","Cannes","Film","Festival",",","won","the","Grand","Prix","Spécial","du","Jury","and","the","FIPRESCI","prize",",","and","was","nominated","for","the","Palme","d","'Or","."],"labels":["O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","O","O","O","O","O","O","B-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, literary genre, person, event, award, writer, location, magazine, poem, book, country and O.\nSentence: The film was presented at the Cannes Film Festival , won the Grand Prix Spécial du Jury and the FIPRESCI prize , and was nominated for the Palme d 'Or .","prompt_labels":"The(O) film(O) was(O) presented(O) at(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) ,(O) won(O) the(O) Grand(B-award) Prix(I-award) Spécial(I-award) du(I-award) Jury(I-award) and(O) the(O) FIPRESCI(B-award) prize(I-award) ,(O) and(O) was(O) nominated(O) for(O) the(O) Palme(B-award) d(I-award) 'Or(I-award) .(O)"}}
{"id":"15","dataset":"crossner_literature","split":"train","label_list":["writer","person","location","country","poem","book","event","literary genre","organization","magazine","award"],"instance":{"id":"15","words":["The","Story","of","Civilization",":","Volume","8",",","The","Age","of","Louis","XIV","by","Will","Durant",";","chapter","II",",","subsection","4.1","p.56",")"],"labels":["B-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","B-writer","I-writer","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, person, location, country, poem, book, event, literary genre, organization, magazine, award and O.\nSentence: The Story of Civilization : Volume 8 , The Age of Louis XIV by Will Durant ; chapter II , subsection 4.1 p.56 )","prompt_labels":"The(B-book) Story(I-book) of(I-book) Civilization(I-book) :(O) Volume(O) 8(O) ,(O) The(B-book) Age(I-book) of(I-book) Louis(I-book) XIV(I-book) by(O) Will(B-writer) Durant(I-writer) ;(O) chapter(O) II(O) ,(O) subsection(O) 4.1(O) p.56(O) )(O)"}}
{"id":"16","dataset":"crossner_literature","split":"train","label_list":["literary genre","writer","book","location","magazine","country","event","organization","poem","award","person"],"instance":{"id":"16","words":["The","title","of","the","story","refers","to","a","bedtime","poem","recited","in","the","narrative","entitled","There","Will","Come","Soft","Rains",",","an","actual","poem","by","Sara","Teasdale","originally","published","in","1920","."],"labels":["O","O","O","O","O","O","O","O","O","B-literary genre","O","O","O","O","O","B-poem","I-poem","I-poem","I-poem","I-poem","O","O","O","B-literary genre","O","B-writer","I-writer","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, writer, book, location, magazine, country, event, organization, poem, award, person and O.\nSentence: The title of the story refers to a bedtime poem recited in the narrative entitled There Will Come Soft Rains , an actual poem by Sara Teasdale originally published in 1920 .","prompt_labels":"The(O) title(O) of(O) the(O) story(O) refers(O) to(O) a(O) bedtime(O) poem(B-literary genre) recited(O) in(O) the(O) narrative(O) entitled(O) There(B-poem) Will(I-poem) Come(I-poem) Soft(I-poem) Rains(I-poem) ,(O) an(O) actual(O) poem(B-literary genre) by(O) Sara(B-writer) Teasdale(I-writer) originally(O) published(O) in(O) 1920(O) .(O)"}}
{"id":"17","dataset":"crossner_literature","split":"train","label_list":["poem","award","magazine","writer","organization","location","event","person","book","country","literary genre"],"instance":{"id":"17","words":["The","illustrated","and","audio","adventure","is","titled","Winnie-the-Pooh","Meets","the","Queen",",","and","has","been","narrated","by","actor","Jim","Broadbent","."],"labels":["O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","B-person","I-person","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, award, magazine, writer, organization, location, event, person, book, country, literary genre and O.\nSentence: The illustrated and audio adventure is titled Winnie-the-Pooh Meets the Queen , and has been narrated by actor Jim Broadbent .","prompt_labels":"The(O) illustrated(O) and(O) audio(O) adventure(O) is(O) titled(O) Winnie-the-Pooh(B-book) Meets(I-book) the(I-book) Queen(I-book) ,(O) and(O) has(O) been(O) narrated(O) by(O) actor(O) Jim(B-person) Broadbent(I-person) .(O)"}}
{"id":"18","dataset":"crossner_literature","split":"train","label_list":["organization","country","book","person","writer","award","magazine","literary genre","poem","event","location"],"instance":{"id":"18","words":["It","was","selected","as","one","of","the","best","science","fiction","short","stories","of","the","pre-","Nebula","Award","period","by","the","Science","Fiction","and","Fantasy","Writers","of","America","."],"labels":["O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","I-literary genre","I-literary genre","O","O","O","B-award","I-award","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, book, person, writer, award, magazine, literary genre, poem, event, location and O.\nSentence: It was selected as one of the best science fiction short stories of the pre- Nebula Award period by the Science Fiction and Fantasy Writers of America .","prompt_labels":"It(O) was(O) selected(O) as(O) one(O) of(O) the(O) best(O) science(B-literary genre) fiction(I-literary genre) short(I-literary genre) stories(I-literary genre) of(O) the(O) pre-(O) Nebula(B-award) Award(I-award) period(O) by(O) the(O) Science(B-organization) Fiction(I-organization) and(I-organization) Fantasy(I-organization) Writers(I-organization) of(I-organization) America(I-organization) .(O)"}}
{"id":"19","dataset":"crossner_literature","split":"train","label_list":["location","poem","writer","person","magazine","award","book","event","organization","country","literary genre"],"instance":{"id":"19","words":["Private","Eye","parodied","Sue","Townsend","'","s","The","Secret","Diary","of","Adrian","Mole",",","age","13","¾","to","write","The","Secret","Diary","of","John","Major",",","age","47","¾",",","in","which","Major","was","portrayed","as","naïve","and","childish",",","keeping","lists","of","his","enemies","in","a","Rymans","Notebook","called","his","Bastards","Book",",","and","featuring","my","wife","Norman","and","Mr","Dr","Mawhinney","as","recurring","character","s","."],"labels":["B-magazine","I-magazine","O","B-writer","I-writer","O","O","B-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","O","O","B-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","B-person","I-person","I-person","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, poem, writer, person, magazine, award, book, event, organization, country, literary genre and O.\nSentence: Private Eye parodied Sue Townsend ' s The Secret Diary of Adrian Mole , age 13 ¾ to write The Secret Diary of John Major , age 47 ¾ , in which Major was portrayed as naïve and childish , keeping lists of his enemies in a Rymans Notebook called his Bastards Book , and featuring my wife Norman and Mr Dr Mawhinney as recurring character s .","prompt_labels":"Private(B-magazine) Eye(I-magazine) parodied(O) Sue(B-writer) Townsend(I-writer) '(O) s(O) The(B-book) Secret(I-book) Diary(I-book) of(I-book) Adrian(I-book) Mole(I-book) ,(I-book) age(I-book) 13(I-book) ¾(I-book) to(O) write(O) The(B-book) Secret(I-book) Diary(I-book) of(I-book) John(I-book) Major(I-book) ,(I-book) age(I-book) 47(I-book) ¾(I-book) ,(O) in(O) which(O) Major(B-person) was(O) portrayed(O) as(O) naïve(O) and(O) childish(O) ,(O) keeping(O) lists(O) of(O) his(O) enemies(O) in(O) a(O) Rymans(O) Notebook(O) called(O) his(O) Bastards(O) Book(O) ,(O) and(O) featuring(O) my(O) wife(O) Norman(B-person) and(O) Mr(B-person) Dr(I-person) Mawhinney(I-person) as(O) recurring(O) character(O) s(O) .(O)"}}
{"id":"20","dataset":"crossner_literature","split":"train","label_list":["person","magazine","organization","event","writer","literary genre","location","book","poem","award","country"],"instance":{"id":"20","words":["The","British","military","historian","John","Keegan","attacked","Clausewitz","'s","theory","in","his","book","A","History","of","Warfare",".","John","Keegan",",","A","History","of","Warfare","."],"labels":["O","O","O","O","B-writer","I-writer","O","B-writer","O","O","O","O","O","B-book","I-book","I-book","I-book","O","B-writer","I-writer","O","B-book","I-book","I-book","I-book","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, magazine, organization, event, writer, literary genre, location, book, poem, award, country and O.\nSentence: The British military historian John Keegan attacked Clausewitz 's theory in his book A History of Warfare . John Keegan , A History of Warfare .","prompt_labels":"The(O) British(O) military(O) historian(O) John(B-writer) Keegan(I-writer) attacked(O) Clausewitz(B-writer) 's(O) theory(O) in(O) his(O) book(O) A(B-book) History(I-book) of(I-book) Warfare(I-book) .(O) John(B-writer) Keegan(I-writer) ,(O) A(B-book) History(I-book) of(I-book) Warfare(I-book) .(O)"}}
{"id":"21","dataset":"crossner_literature","split":"train","label_list":["event","literary genre","organization","person","award","location","writer","magazine","poem","book","country"],"instance":{"id":"21","words":["The","film","'s","soundtrack","often","forms","a","major","component","of","the","narrative",",","just","as","with","other","important","arthouse","films","of","the","era","such","as","Donald","Cammell","and","Nicolas","Roeg","'","s","Performance","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, literary genre, organization, person, award, location, writer, magazine, poem, book, country and O.\nSentence: The film 's soundtrack often forms a major component of the narrative , just as with other important arthouse films of the era such as Donald Cammell and Nicolas Roeg ' s Performance .","prompt_labels":"The(O) film(O) 's(O) soundtrack(O) often(O) forms(O) a(O) major(O) component(O) of(O) the(O) narrative(O) ,(O) just(O) as(O) with(O) other(O) important(O) arthouse(O) films(O) of(O) the(O) era(O) such(O) as(O) Donald(B-person) Cammell(I-person) and(O) Nicolas(B-person) Roeg(I-person) '(O) s(O) Performance(O) .(O)"}}
{"id":"22","dataset":"crossner_literature","split":"train","label_list":["writer","literary genre","book","organization","poem","award","event","magazine","location","country","person"],"instance":{"id":"22","words":["John","Updike",",","comparing","Abner","to","a","hillbilly","Candide",",","added","that","the","strip","'s","richness","of","social","and","philosophical","commentary","approached","the","Voltairean","."],"labels":["B-writer","I-writer","O","O","B-person","O","O","O","B-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, literary genre, book, organization, poem, award, event, magazine, location, country, person and O.\nSentence: John Updike , comparing Abner to a hillbilly Candide , added that the strip 's richness of social and philosophical commentary approached the Voltairean .","prompt_labels":"John(B-writer) Updike(I-writer) ,(O) comparing(O) Abner(B-person) to(O) a(O) hillbilly(O) Candide(B-book) ,(O) added(O) that(O) the(O) strip(O) 's(O) richness(O) of(O) social(O) and(O) philosophical(O) commentary(O) approached(O) the(O) Voltairean(O) .(O)"}}
{"id":"23","dataset":"crossner_literature","split":"train","label_list":["award","literary genre","magazine","organization","book","country","poem","person","writer","location","event"],"instance":{"id":"23","words":["In","an","article","for","The","Atlantic",",","film","critic","Ty","Burr","deemed","The","Birth","of","a","Nation","the","most","influential","film","in","history","while","criticizing","its","portrayal","of","black","men","as","savage","."],"labels":["O","O","O","O","B-magazine","I-magazine","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, literary genre, magazine, organization, book, country, poem, person, writer, location, event and O.\nSentence: In an article for The Atlantic , film critic Ty Burr deemed The Birth of a Nation the most influential film in history while criticizing its portrayal of black men as savage .","prompt_labels":"In(O) an(O) article(O) for(O) The(B-magazine) Atlantic(I-magazine) ,(O) film(O) critic(O) Ty(B-person) Burr(I-person) deemed(O) The(O) Birth(O) of(O) a(O) Nation(O) the(O) most(O) influential(O) film(O) in(O) history(O) while(O) criticizing(O) its(O) portrayal(O) of(O) black(O) men(O) as(O) savage(O) .(O)"}}
{"id":"24","dataset":"crossner_literature","split":"train","label_list":["country","location","literary genre","person","event","writer","organization","poem","book","magazine","award"],"instance":{"id":"24","words":["Sergei","Rachmaninoff",",","Rainer","Maria","Rilke","and","Leo","Tolstoy","were","all","visitors","to","the","family","home","."],"labels":["B-person","I-person","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, literary genre, person, event, writer, organization, poem, book, magazine, award and O.\nSentence: Sergei Rachmaninoff , Rainer Maria Rilke and Leo Tolstoy were all visitors to the family home .","prompt_labels":"Sergei(B-person) Rachmaninoff(I-person) ,(O) Rainer(B-writer) Maria(I-writer) Rilke(I-writer) and(O) Leo(B-writer) Tolstoy(I-writer) were(O) all(O) visitors(O) to(O) the(O) family(O) home(O) .(O)"}}
{"id":"25","dataset":"crossner_literature","split":"train","label_list":["person","writer","magazine","poem","location","literary genre","book","award","organization","event","country"],"instance":{"id":"25","words":["At","the","47th","Berlin","International","Film","Festival","in","1997",",","DiCaprio","won","the","Silver","Bear","for","Best","Actor","and","Luhrmann","won","the","Alfred","Bauer","Prize","."],"labels":["O","O","B-event","I-event","I-event","I-event","I-event","O","O","O","B-person","O","O","B-award","I-award","I-award","I-award","I-award","O","B-person","O","O","B-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, magazine, poem, location, literary genre, book, award, organization, event, country and O.\nSentence: At the 47th Berlin International Film Festival in 1997 , DiCaprio won the Silver Bear for Best Actor and Luhrmann won the Alfred Bauer Prize .","prompt_labels":"At(O) the(O) 47th(B-event) Berlin(I-event) International(I-event) Film(I-event) Festival(I-event) in(O) 1997(O) ,(O) DiCaprio(B-person) won(O) the(O) Silver(B-award) Bear(I-award) for(I-award) Best(I-award) Actor(I-award) and(O) Luhrmann(B-person) won(O) the(O) Alfred(B-award) Bauer(I-award) Prize(I-award) .(O)"}}
{"id":"26","dataset":"crossner_literature","split":"train","label_list":["book","location","poem","organization","person","award","country","writer","magazine","literary genre","event"],"instance":{"id":"26","words":["Nin","was","a","friend",",","and","in","some","cases","lover",",","of","many","literary","figures",",","including","Henry","Miller",",","John","Steinbeck",",","Antonin","Artaud",",","Edmund","Wilson",",","Gore","Vidal",",","James","Agee",",","James","Leo","Herlihy",",","and","Lawrence","Durrell","."],"labels":["B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","B-writer","I-writer","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, location, poem, organization, person, award, country, writer, magazine, literary genre, event and O.\nSentence: Nin was a friend , and in some cases lover , of many literary figures , including Henry Miller , John Steinbeck , Antonin Artaud , Edmund Wilson , Gore Vidal , James Agee , James Leo Herlihy , and Lawrence Durrell .","prompt_labels":"Nin(B-writer) was(O) a(O) friend(O) ,(O) and(O) in(O) some(O) cases(O) lover(O) ,(O) of(O) many(O) literary(O) figures(O) ,(O) including(O) Henry(B-writer) Miller(I-writer) ,(O) John(B-writer) Steinbeck(I-writer) ,(O) Antonin(B-writer) Artaud(I-writer) ,(O) Edmund(B-writer) Wilson(I-writer) ,(O) Gore(B-writer) Vidal(I-writer) ,(O) James(B-writer) Agee(I-writer) ,(O) James(B-writer) Leo(I-writer) Herlihy(I-writer) ,(O) and(O) Lawrence(B-writer) Durrell(I-writer) .(O)"}}
{"id":"27","dataset":"crossner_literature","split":"train","label_list":["location","person","literary genre","country","poem","writer","organization","magazine","event","award","book"],"instance":{"id":"27","words":["These","include","the","Charles","Dickens","Museum","in","London",",","the","historic","home","where","he","wrote","Oliver","Twist",",","The","Pickwick","Papers","and","Nicholas","Nickleby",";","and","the","Charles","Dickens","Birthplace","Museum","in","Portsmouth",",","the","house","in","which","he","was","born","."],"labels":["O","O","O","B-location","I-location","I-location","O","B-location","O","O","O","O","O","O","O","B-book","I-book","O","B-book","I-book","I-book","O","B-book","I-book","O","O","O","B-location","I-location","I-location","I-location","O","B-location","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, literary genre, country, poem, writer, organization, magazine, event, award, book and O.\nSentence: These include the Charles Dickens Museum in London , the historic home where he wrote Oliver Twist , The Pickwick Papers and Nicholas Nickleby ; and the Charles Dickens Birthplace Museum in Portsmouth , the house in which he was born .","prompt_labels":"These(O) include(O) the(O) Charles(B-location) Dickens(I-location) Museum(I-location) in(O) London(B-location) ,(O) the(O) historic(O) home(O) where(O) he(O) wrote(O) Oliver(B-book) Twist(I-book) ,(O) The(B-book) Pickwick(I-book) Papers(I-book) and(O) Nicholas(B-book) Nickleby(I-book) ;(O) and(O) the(O) Charles(B-location) Dickens(I-location) Birthplace(I-location) Museum(I-location) in(O) Portsmouth(B-location) ,(O) the(O) house(O) in(O) which(O) he(O) was(O) born(O) .(O)"}}
{"id":"28","dataset":"crossner_literature","split":"train","label_list":["literary genre","writer","organization","event","poem","person","book","magazine","location","award","country"],"instance":{"id":"28","words":["It","tied","with","Roger","Zelazny","'","s","This","Immortal","for","the","Hugo","Award","in","1966",","],"labels":["O","O","O","B-writer","I-writer","O","O","B-book","I-book","O","O","B-award","I-award","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, writer, organization, event, poem, person, book, magazine, location, award, country and O.\nSentence: It tied with Roger Zelazny ' s This Immortal for the Hugo Award in 1966 ,","prompt_labels":"It(O) tied(O) with(O) Roger(B-writer) Zelazny(I-writer) '(O) s(O) This(B-book) Immortal(I-book) for(O) the(O) Hugo(B-award) Award(I-award) in(O) 1966(O) ,(O)"}}
{"id":"29","dataset":"crossner_literature","split":"train","label_list":["country","poem","literary genre","book","person","magazine","writer","event","location","organization","award"],"instance":{"id":"29","words":["Her","stage","credits","include","Norman","Mailer","'","s","The","Deer","Park",",","Israel","Horovitz","'","s","The","Indian","Wants","the","Bronx",",","Neil","Simon","'s","The","Good","Doctor","and","Joseph","Papp","'","s","1974","Richard","III","at","the","Lincoln","Center","."],"labels":["O","O","O","O","B-writer","I-writer","O","O","B-book","I-book","I-book","O","B-writer","I-writer","O","O","B-book","I-book","I-book","I-book","I-book","O","B-writer","I-writer","O","B-book","I-book","I-book","O","B-person","I-person","O","O","O","O","O","O","O","B-location","I-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, poem, literary genre, book, person, magazine, writer, event, location, organization, award and O.\nSentence: Her stage credits include Norman Mailer ' s The Deer Park , Israel Horovitz ' s The Indian Wants the Bronx , Neil Simon 's The Good Doctor and Joseph Papp ' s 1974 Richard III at the Lincoln Center .","prompt_labels":"Her(O) stage(O) credits(O) include(O) Norman(B-writer) Mailer(I-writer) '(O) s(O) The(B-book) Deer(I-book) Park(I-book) ,(O) Israel(B-writer) Horovitz(I-writer) '(O) s(O) The(B-book) Indian(I-book) Wants(I-book) the(I-book) Bronx(I-book) ,(O) Neil(B-writer) Simon(I-writer) 's(O) The(B-book) Good(I-book) Doctor(I-book) and(O) Joseph(B-person) Papp(I-person) '(O) s(O) 1974(O) Richard(O) III(O) at(O) the(O) Lincoln(B-location) Center(I-location) .(O)"}}
{"id":"30","dataset":"crossner_literature","split":"train","label_list":["literary genre","award","location","event","organization","poem","country","magazine","person","book","writer"],"instance":{"id":"30","words":["In","The","Crisis","magazine","in","1943",",","Harold","Preece","criticized","Hurston","for","her","perpetuation","of","Negro","primitivism","in","order","to","advance","her","own","literary","career","."],"labels":["O","B-magazine","I-magazine","O","O","O","O","B-writer","I-writer","O","B-writer","O","O","O","O","B-literary genre","I-literary genre","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, award, location, event, organization, poem, country, magazine, person, book, writer and O.\nSentence: In The Crisis magazine in 1943 , Harold Preece criticized Hurston for her perpetuation of Negro primitivism in order to advance her own literary career .","prompt_labels":"In(O) The(B-magazine) Crisis(I-magazine) magazine(O) in(O) 1943(O) ,(O) Harold(B-writer) Preece(I-writer) criticized(O) Hurston(B-writer) for(O) her(O) perpetuation(O) of(O) Negro(B-literary genre) primitivism(I-literary genre) in(O) order(O) to(O) advance(O) her(O) own(O) literary(O) career(O) .(O)"}}
{"id":"31","dataset":"crossner_literature","split":"train","label_list":["event","poem","award","writer","organization","literary genre","country","book","person","location","magazine"],"instance":{"id":"31","words":["Most","notably",",","she","was","the","food","editor","of","The","New","York","Times","Magazine",",","the","editor","of","T","Living",",","a","quarterly","publication","of","The","New","York","Times",",","author","of","The","Essential","New","York","Times","Cookbook","which","was","a","New","York","Times","bestseller",",","and","co-founder","and","CEO","of","Food52","."],"labels":["O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","I-magazine","I-magazine","I-magazine","O","O","O","O","B-magazine","I-magazine","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","B-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, poem, award, writer, organization, literary genre, country, book, person, location, magazine and O.\nSentence: Most notably , she was the food editor of The New York Times Magazine , the editor of T Living , a quarterly publication of The New York Times , author of The Essential New York Times Cookbook which was a New York Times bestseller , and co-founder and CEO of Food52 .","prompt_labels":"Most(O) notably(O) ,(O) she(O) was(O) the(O) food(O) editor(O) of(O) The(B-magazine) New(I-magazine) York(I-magazine) Times(I-magazine) Magazine(I-magazine) ,(O) the(O) editor(O) of(O) T(B-magazine) Living(I-magazine) ,(O) a(O) quarterly(O) publication(O) of(O) The(B-organization) New(I-organization) York(I-organization) Times(I-organization) ,(O) author(O) of(O) The(B-book) Essential(I-book) New(I-book) York(I-book) Times(I-book) Cookbook(I-book) which(O) was(O) a(O) New(B-organization) York(I-organization) Times(I-organization) bestseller(O) ,(O) and(O) co-founder(O) and(O) CEO(O) of(O) Food52(B-organization) .(O)"}}
{"id":"32","dataset":"crossner_literature","split":"train","label_list":["writer","event","country","person","poem","award","organization","literary genre","magazine","book","location"],"instance":{"id":"32","words":["He","was","also","an","admirer","of","Richard","Condon",",","author","of","The","Manchurian","Candidate","(","1959",")",",","Prizzi","'s","Honor","(","1982",")",",","and","numerous","other","novels","."],"labels":["O","O","O","O","O","O","B-writer","I-writer","O","O","O","B-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","B-literary genre","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, event, country, person, poem, award, organization, literary genre, magazine, book, location and O.\nSentence: He was also an admirer of Richard Condon , author of The Manchurian Candidate ( 1959 ) , Prizzi 's Honor ( 1982 ) , and numerous other novels .","prompt_labels":"He(O) was(O) also(O) an(O) admirer(O) of(O) Richard(B-writer) Condon(I-writer) ,(O) author(O) of(O) The(B-book) Manchurian(I-book) Candidate(I-book) ((O) 1959(O) )(O) ,(O) Prizzi(B-book) 's(I-book) Honor(I-book) ((O) 1982(O) )(O) ,(O) and(O) numerous(O) other(O) novels(B-literary genre) .(O)"}}
{"id":"33","dataset":"crossner_literature","split":"train","label_list":["country","person","award","poem","location","event","magazine","book","literary genre","writer","organization"],"instance":{"id":"33","words":["He","also","gave","a","lecture","at","the","Beethovensaal","in","Berlin","on","13","October","1922",",","which","appeared","in","Neue","Rundschau","in","November","1922","in","which","he","developed","his","eccentric","defence","of","the","Republic",",","based","on","extensive","close","readings","of","Novalis","and","Walt","Whitman","."],"labels":["O","O","O","O","O","O","O","B-location","O","B-location","O","O","O","O","O","O","O","O","B-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","O","B-writer","I-writer","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, award, poem, location, event, magazine, book, literary genre, writer, organization and O.\nSentence: He also gave a lecture at the Beethovensaal in Berlin on 13 October 1922 , which appeared in Neue Rundschau in November 1922 in which he developed his eccentric defence of the Republic , based on extensive close readings of Novalis and Walt Whitman .","prompt_labels":"He(O) also(O) gave(O) a(O) lecture(O) at(O) the(O) Beethovensaal(B-location) in(O) Berlin(B-location) on(O) 13(O) October(O) 1922(O) ,(O) which(O) appeared(O) in(O) Neue(B-magazine) Rundschau(I-magazine) in(O) November(O) 1922(O) in(O) which(O) he(O) developed(O) his(O) eccentric(O) defence(O) of(O) the(O) Republic(O) ,(O) based(O) on(O) extensive(O) close(O) readings(O) of(O) Novalis(B-writer) and(O) Walt(B-writer) Whitman(I-writer) .(O)"}}
{"id":"34","dataset":"crossner_literature","split":"train","label_list":["location","book","organization","magazine","literary genre","writer","event","person","country","award","poem"],"instance":{"id":"34","words":["One","of","his","poems",",","Ikke","Bødlen",",","was","featured","as","one","of","the","best","poems","on","Human","Rights","on","a","1979","book","published","by","Amnesty","International","Denmark",",","and","would","be","later","translated","into","the","first","verse","of","Roger","Waters","'","song","Each","Small","Candle","."],"labels":["O","O","O","B-literary genre","O","B-poem","I-poem","O","O","O","O","O","O","O","O","B-literary genre","O","O","O","O","O","O","O","O","O","B-organization","I-organization","B-country","O","O","O","O","O","O","O","O","O","B-literary genre","O","B-writer","I-writer","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, book, organization, magazine, literary genre, writer, event, person, country, award, poem and O.\nSentence: One of his poems , Ikke Bødlen , was featured as one of the best poems on Human Rights on a 1979 book published by Amnesty International Denmark , and would be later translated into the first verse of Roger Waters ' song Each Small Candle .","prompt_labels":"One(O) of(O) his(O) poems(B-literary genre) ,(O) Ikke(B-poem) Bødlen(I-poem) ,(O) was(O) featured(O) as(O) one(O) of(O) the(O) best(O) poems(B-literary genre) on(O) Human(O) Rights(O) on(O) a(O) 1979(O) book(O) published(O) by(O) Amnesty(B-organization) International(I-organization) Denmark(B-country) ,(O) and(O) would(O) be(O) later(O) translated(O) into(O) the(O) first(O) verse(B-literary genre) of(O) Roger(B-writer) Waters(I-writer) '(O) song(O) Each(O) Small(O) Candle(O) .(O)"}}
{"id":"35","dataset":"crossner_literature","split":"train","label_list":["book","award","writer","person","location","magazine","poem","event","organization","literary genre","country"],"instance":{"id":"35","words":["Anthony","Boucher",",","reviewing","the","volume","in","The","Magazine","of","Fantasy","&","Science","Fiction",",","wrote","that","The","Two","Towers","makes","inordinate","demands","upon","the","patience","of","its","readers","with","passages","which","could","be","lopped","away","without","affecting","form","or","content","."],"labels":["B-writer","I-writer","O","O","O","O","O","B-magazine","I-magazine","I-magazine","I-magazine","I-magazine","I-magazine","I-magazine","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, award, writer, person, location, magazine, poem, event, organization, literary genre, country and O.\nSentence: Anthony Boucher , reviewing the volume in The Magazine of Fantasy & Science Fiction , wrote that The Two Towers makes inordinate demands upon the patience of its readers with passages which could be lopped away without affecting form or content .","prompt_labels":"Anthony(B-writer) Boucher(I-writer) ,(O) reviewing(O) the(O) volume(O) in(O) The(B-magazine) Magazine(I-magazine) of(I-magazine) Fantasy(I-magazine) &(I-magazine) Science(I-magazine) Fiction(I-magazine) ,(O) wrote(O) that(O) The(B-book) Two(I-book) Towers(I-book) makes(O) inordinate(O) demands(O) upon(O) the(O) patience(O) of(O) its(O) readers(O) with(O) passages(O) which(O) could(O) be(O) lopped(O) away(O) without(O) affecting(O) form(O) or(O) content(O) .(O)"}}
{"id":"36","dataset":"crossner_literature","split":"train","label_list":["country","organization","book","magazine","person","location","literary genre","poem","award","event","writer"],"instance":{"id":"36","words":["See","Gubbinal","and","Nuances","of","a","Theme","by","Williams","for","comparisons","."],"labels":["O","B-poem","O","B-poem","I-poem","I-poem","I-poem","I-poem","I-poem","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, book, magazine, person, location, literary genre, poem, award, event, writer and O.\nSentence: See Gubbinal and Nuances of a Theme by Williams for comparisons .","prompt_labels":"See(O) Gubbinal(B-poem) and(O) Nuances(B-poem) of(I-poem) a(I-poem) Theme(I-poem) by(I-poem) Williams(I-poem) for(O) comparisons(O) .(O)"}}
{"id":"37","dataset":"crossner_literature","split":"train","label_list":["event","person","magazine","book","country","location","literary genre","award","organization","poem","writer"],"instance":{"id":"37","words":["Cuarón","'s","feature","Children","of","Men",",","an","adaptation","of","the","P.","D.","James","The","Children","of","Men","starring","Clive","Owen",",","Julianne","Moore","and","Michael","Caine",",","received","wide","critical","acclaim",",","including","three","Academy","Awards","nominations","."],"labels":["B-writer","O","O","B-book","I-book","I-book","O","O","O","O","O","B-writer","I-writer","I-writer","B-book","I-book","I-book","I-book","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","O","O","O","O","O","O","B-award","I-award","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, magazine, book, country, location, literary genre, award, organization, poem, writer and O.\nSentence: Cuarón 's feature Children of Men , an adaptation of the P. D. James The Children of Men starring Clive Owen , Julianne Moore and Michael Caine , received wide critical acclaim , including three Academy Awards nominations .","prompt_labels":"Cuarón(B-writer) 's(O) feature(O) Children(B-book) of(I-book) Men(I-book) ,(O) an(O) adaptation(O) of(O) the(O) P.(B-writer) D.(I-writer) James(I-writer) The(B-book) Children(I-book) of(I-book) Men(I-book) starring(O) Clive(B-person) Owen(I-person) ,(O) Julianne(B-person) Moore(I-person) and(O) Michael(B-person) Caine(I-person) ,(O) received(O) wide(O) critical(O) acclaim(O) ,(O) including(O) three(O) Academy(B-award) Awards(I-award) nominations(O) .(O)"}}
{"id":"38","dataset":"crossner_literature","split":"train","label_list":["country","location","award","event","literary genre","person","magazine","organization","writer","book","poem"],"instance":{"id":"38","words":["According","to","Willmott",",","Yeats","'s","poems","often","move","from","the","world","of","social","interaction","to","a","place","where","the","individual","finds","seclusion",",","as","is","also","the","case","in","the","pastoral","Yeats","'s","earlier","poems","The","Lake","Isle","of","Innisfree",",","The","Song","of","the","Happy","Shepherd",",","and","The","Sad","Shepherd","."],"labels":["O","O","B-writer","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","B-poem","I-poem","I-poem","I-poem","I-poem","O","B-poem","I-poem","I-poem","I-poem","I-poem","I-poem","O","O","B-poem","I-poem","I-poem","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, award, event, literary genre, person, magazine, organization, writer, book, poem and O.\nSentence: According to Willmott , Yeats 's poems often move from the world of social interaction to a place where the individual finds seclusion , as is also the case in the pastoral Yeats 's earlier poems The Lake Isle of Innisfree , The Song of the Happy Shepherd , and The Sad Shepherd .","prompt_labels":"According(O) to(O) Willmott(B-writer) ,(O) Yeats(B-writer) 's(O) poems(O) often(O) move(O) from(O) the(O) world(O) of(O) social(O) interaction(O) to(O) a(O) place(O) where(O) the(O) individual(O) finds(O) seclusion(O) ,(O) as(O) is(O) also(O) the(O) case(O) in(O) the(O) pastoral(O) Yeats(B-writer) 's(O) earlier(O) poems(O) The(B-poem) Lake(I-poem) Isle(I-poem) of(I-poem) Innisfree(I-poem) ,(O) The(B-poem) Song(I-poem) of(I-poem) the(I-poem) Happy(I-poem) Shepherd(I-poem) ,(O) and(O) The(B-poem) Sad(I-poem) Shepherd(I-poem) .(O)"}}
{"id":"39","dataset":"crossner_literature","split":"train","label_list":["award","event","organization","book","poem","literary genre","country","writer","location","magazine","person"],"instance":{"id":"39","words":["Robert","Caro","has","cited","it","as","the","strongest","influence","on","The","Power","Broker",",","his","Pulitzer","Prize","-winning","biography","of","Robert","Moses",",","though","Caro","does","not","mention","Jacobs","by","name","even","once","in","the","book","despite","Jacobs","'","battles","with","Moses","over","his","proposed","Lower","Manhattan","Expressway","."],"labels":["B-writer","I-writer","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","O","B-award","I-award","O","O","O","B-person","I-person","O","O","B-writer","O","O","O","B-writer","O","O","O","O","O","O","O","O","B-writer","O","O","O","B-person","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, organization, book, poem, literary genre, country, writer, location, magazine, person and O.\nSentence: Robert Caro has cited it as the strongest influence on The Power Broker , his Pulitzer Prize -winning biography of Robert Moses , though Caro does not mention Jacobs by name even once in the book despite Jacobs ' battles with Moses over his proposed Lower Manhattan Expressway .","prompt_labels":"Robert(B-writer) Caro(I-writer) has(O) cited(O) it(O) as(O) the(O) strongest(O) influence(O) on(O) The(B-book) Power(I-book) Broker(I-book) ,(O) his(O) Pulitzer(B-award) Prize(I-award) -winning(O) biography(O) of(O) Robert(B-person) Moses(I-person) ,(O) though(O) Caro(B-writer) does(O) not(O) mention(O) Jacobs(B-writer) by(O) name(O) even(O) once(O) in(O) the(O) book(O) despite(O) Jacobs(B-writer) '(O) battles(O) with(O) Moses(B-person) over(O) his(O) proposed(O) Lower(O) Manhattan(O) Expressway(O) .(O)"}}
{"id":"40","dataset":"crossner_literature","split":"train","label_list":["organization","literary genre","event","magazine","location","writer","country","person","award","poem","book"],"instance":{"id":"40","words":["Gravity","'s","Rainbow","shared","the","1974","National","Book","Award","with","A","Crown","of","Feathers","and","Other","Stories","by","Isaac","Bashevis","Singer","(","split","award",")","."],"labels":["B-book","I-book","I-book","O","O","O","B-award","I-award","I-award","O","B-book","I-book","I-book","I-book","I-book","I-book","I-book","O","B-writer","I-writer","I-writer","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, literary genre, event, magazine, location, writer, country, person, award, poem, book and O.\nSentence: Gravity 's Rainbow shared the 1974 National Book Award with A Crown of Feathers and Other Stories by Isaac Bashevis Singer ( split award ) .","prompt_labels":"Gravity(B-book) 's(I-book) Rainbow(I-book) shared(O) the(O) 1974(O) National(B-award) Book(I-award) Award(I-award) with(O) A(B-book) Crown(I-book) of(I-book) Feathers(I-book) and(I-book) Other(I-book) Stories(I-book) by(O) Isaac(B-writer) Bashevis(I-writer) Singer(I-writer) ((O) split(O) award(O) )(O) .(O)"}}
{"id":"41","dataset":"crossner_literature","split":"train","label_list":["award","location","writer","magazine","literary genre","book","poem","organization","event","country","person"],"instance":{"id":"41","words":["The","original","Chanson","d","'Antioche","is","lost",",","but","it","was","edited","in","the","12th","century","by","Graindor","de","Douai",",","who","also","edited","the","Chanson","de","Jérusalem",",","and","possibly","wrote","the","Chanson","des","Chétifs","himself","."],"labels":["O","O","B-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","B-poem","I-poem","I-poem","O","O","O","O","O","B-poem","I-poem","I-poem","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, writer, magazine, literary genre, book, poem, organization, event, country, person and O.\nSentence: The original Chanson d 'Antioche is lost , but it was edited in the 12th century by Graindor de Douai , who also edited the Chanson de Jérusalem , and possibly wrote the Chanson des Chétifs himself .","prompt_labels":"The(O) original(O) Chanson(B-poem) d(I-poem) 'Antioche(I-poem) is(O) lost(O) ,(O) but(O) it(O) was(O) edited(O) in(O) the(O) 12th(O) century(O) by(O) Graindor(B-writer) de(I-writer) Douai(I-writer) ,(O) who(O) also(O) edited(O) the(O) Chanson(B-poem) de(I-poem) Jérusalem(I-poem) ,(O) and(O) possibly(O) wrote(O) the(O) Chanson(B-poem) des(I-poem) Chétifs(I-poem) himself(O) .(O)"}}
{"id":"42","dataset":"crossner_literature","split":"train","label_list":["magazine","event","poem","organization","country","writer","literary genre","book","person","location","award"],"instance":{"id":"42","words":["Although","not","accessible","for","years","within","Germany","to","comply","with","a","court","order","from","S.","Fischer","Verlag","regarding","the","works","of","Heinrich","Mann",",","Thomas","Mann","and","Alfred","Döblin",",","Project","Gutenberg","is","once","more","accessible","."],"labels":["O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-organization","I-organization","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, event, poem, organization, country, writer, literary genre, book, person, location, award and O.\nSentence: Although not accessible for years within Germany to comply with a court order from S. Fischer Verlag regarding the works of Heinrich Mann , Thomas Mann and Alfred Döblin , Project Gutenberg is once more accessible .","prompt_labels":"Although(O) not(O) accessible(O) for(O) years(O) within(O) Germany(B-country) to(O) comply(O) with(O) a(O) court(O) order(O) from(O) S.(B-organization) Fischer(I-organization) Verlag(I-organization) regarding(O) the(O) works(O) of(O) Heinrich(B-writer) Mann(I-writer) ,(O) Thomas(B-writer) Mann(I-writer) and(O) Alfred(B-writer) Döblin(I-writer) ,(O) Project(B-organization) Gutenberg(I-organization) is(O) once(O) more(O) accessible(O) .(O)"}}
{"id":"43","dataset":"crossner_literature","split":"train","label_list":["location","literary genre","event","country","person","poem","award","magazine","book","organization","writer"],"instance":{"id":"43","words":["Darkness","at","Noon","for","the","New","Statesman","in","1941",",","saying",":"],"labels":["B-book","I-book","I-book","O","O","B-magazine","I-magazine","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, literary genre, event, country, person, poem, award, magazine, book, organization, writer and O.\nSentence: Darkness at Noon for the New Statesman in 1941 , saying :","prompt_labels":"Darkness(B-book) at(I-book) Noon(I-book) for(O) the(O) New(B-magazine) Statesman(I-magazine) in(O) 1941(O) ,(O) saying(O) :(O)"}}
{"id":"44","dataset":"crossner_literature","split":"train","label_list":["country","literary genre","book","organization","event","magazine","writer","poem","award","person","location"],"instance":{"id":"44","words":["The","Han","dynasty","Records","of","the","Grand","Historian","records","that","it","had","already","become","a","place","of","pilgrimage","for","ministers","."],"labels":["O","B-country","I-country","B-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, literary genre, book, organization, event, magazine, writer, poem, award, person, location and O.\nSentence: The Han dynasty Records of the Grand Historian records that it had already become a place of pilgrimage for ministers .","prompt_labels":"The(O) Han(B-country) dynasty(I-country) Records(B-book) of(I-book) the(I-book) Grand(I-book) Historian(I-book) records(O) that(O) it(O) had(O) already(O) become(O) a(O) place(O) of(O) pilgrimage(O) for(O) ministers(O) .(O)"}}
{"id":"45","dataset":"crossner_literature","split":"train","label_list":["event","organization","person","book","award","location","poem","writer","literary genre","country","magazine"],"instance":{"id":"45","words":["In","1960",",","aged","42",",","he","approached","Aleksandr","Tvardovsky",",","a","poet","and","the","chief","editor","of","the","Novy","Mir","magazine",",","with","the","manuscript","of","One","Day","in","the","Life","of","Ivan","Denisovich","."],"labels":["O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","O","O","O","O","O","O","B-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, person, book, award, location, poem, writer, literary genre, country, magazine and O.\nSentence: In 1960 , aged 42 , he approached Aleksandr Tvardovsky , a poet and the chief editor of the Novy Mir magazine , with the manuscript of One Day in the Life of Ivan Denisovich .","prompt_labels":"In(O) 1960(O) ,(O) aged(O) 42(O) ,(O) he(O) approached(O) Aleksandr(B-writer) Tvardovsky(I-writer) ,(O) a(O) poet(O) and(O) the(O) chief(O) editor(O) of(O) the(O) Novy(B-magazine) Mir(I-magazine) magazine(O) ,(O) with(O) the(O) manuscript(O) of(O) One(B-book) Day(I-book) in(I-book) the(I-book) Life(I-book) of(I-book) Ivan(I-book) Denisovich(I-book) .(O)"}}
{"id":"46","dataset":"crossner_literature","split":"train","label_list":["country","event","book","organization","location","writer","literary genre","magazine","person","award","poem"],"instance":{"id":"46","words":["The","first","writer","to","use","the","term","classic","was","Aulus","Gellius",",","a","2nd-century","Ancient","Rome","writer","who",",","in","the","miscellany","Noctes","Atticae","(","19",",","8",",","15",")",",","refers","to","a","writer","as","a","classicus","scriptor",",","non","proletarius","(","A","distinguished",",","not","a","commonplace","writer",")","."],"labels":["O","O","O","O","O","O","O","B-literary genre","O","B-writer","I-writer","O","O","O","B-country","I-country","O","O","O","O","O","O","B-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, book, organization, location, writer, literary genre, magazine, person, award, poem and O.\nSentence: The first writer to use the term classic was Aulus Gellius , a 2nd-century Ancient Rome writer who , in the miscellany Noctes Atticae ( 19 , 8 , 15 ) , refers to a writer as a classicus scriptor , non proletarius ( A distinguished , not a commonplace writer ) .","prompt_labels":"The(O) first(O) writer(O) to(O) use(O) the(O) term(O) classic(B-literary genre) was(O) Aulus(B-writer) Gellius(I-writer) ,(O) a(O) 2nd-century(O) Ancient(B-country) Rome(I-country) writer(O) who(O) ,(O) in(O) the(O) miscellany(O) Noctes(B-book) Atticae(I-book) ((O) 19(O) ,(O) 8(O) ,(O) 15(O) )(O) ,(O) refers(O) to(O) a(O) writer(O) as(O) a(O) classicus(O) scriptor(O) ,(O) non(O) proletarius(O) ((O) A(O) distinguished(O) ,(O) not(O) a(O) commonplace(O) writer(O) )(O) .(O)"}}
{"id":"47","dataset":"crossner_literature","split":"train","label_list":["writer","person","location","award","poem","magazine","literary genre","book","event","country","organization"],"instance":{"id":"47","words":["Dickens","has","been","praised","by","many","of","his","fellow","writers","-","from","Leo","Tolstoy","to","George","Orwell",",","G.","K.","Chesterton",",","and","Tom","Wolfe","-","for","his","realism",",","comedy",",","prose","style",",","unique","characterisations",",","and","social","criticism","."],"labels":["B-writer","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, person, location, award, poem, magazine, literary genre, book, event, country, organization and O.\nSentence: Dickens has been praised by many of his fellow writers - from Leo Tolstoy to George Orwell , G. K. Chesterton , and Tom Wolfe - for his realism , comedy , prose style , unique characterisations , and social criticism .","prompt_labels":"Dickens(B-writer) has(O) been(O) praised(O) by(O) many(O) of(O) his(O) fellow(O) writers(O) -(O) from(O) Leo(B-writer) Tolstoy(I-writer) to(O) George(B-writer) Orwell(I-writer) ,(O) G.(B-writer) K.(I-writer) Chesterton(I-writer) ,(O) and(O) Tom(B-writer) Wolfe(I-writer) -(O) for(O) his(O) realism(O) ,(O) comedy(O) ,(O) prose(O) style(O) ,(O) unique(O) characterisations(O) ,(O) and(O) social(O) criticism(O) .(O)"}}
{"id":"48","dataset":"crossner_literature","split":"train","label_list":["event","location","poem","organization","book","person","magazine","award","country","literary genre","writer"],"instance":{"id":"48","words":["Hughes","wrote","of","inequality","(","I",",","Too",")",",","of","resilience","(","Mother","to","Son","and","The","Negro","Speaks","of","Rivers",")",",","of","pride","(","My","People",")",",","of","hope","(","Freedom","'s","Plow",")",",","and","of","music","(","The","Trumpet","Player","and","Juke","Box","Love","Song",")","."],"labels":["B-writer","O","O","O","O","B-poem","I-poem","I-poem","O","O","O","O","O","B-poem","I-poem","I-poem","O","B-poem","I-poem","I-poem","I-poem","I-poem","O","O","O","O","O","B-book","I-book","O","O","O","O","O","B-poem","I-poem","I-poem","O","O","O","O","O","O","B-poem","I-poem","I-poem","O","B-poem","I-poem","I-poem","I-poem","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, poem, organization, book, person, magazine, award, country, literary genre, writer and O.\nSentence: Hughes wrote of inequality ( I , Too ) , of resilience ( Mother to Son and The Negro Speaks of Rivers ) , of pride ( My People ) , of hope ( Freedom 's Plow ) , and of music ( The Trumpet Player and Juke Box Love Song ) .","prompt_labels":"Hughes(B-writer) wrote(O) of(O) inequality(O) ((O) I(B-poem) ,(I-poem) Too(I-poem) )(O) ,(O) of(O) resilience(O) ((O) Mother(B-poem) to(I-poem) Son(I-poem) and(O) The(B-poem) Negro(I-poem) Speaks(I-poem) of(I-poem) Rivers(I-poem) )(O) ,(O) of(O) pride(O) ((O) My(B-book) People(I-book) )(O) ,(O) of(O) hope(O) ((O) Freedom(B-poem) 's(I-poem) Plow(I-poem) )(O) ,(O) and(O) of(O) music(O) ((O) The(B-poem) Trumpet(I-poem) Player(I-poem) and(O) Juke(B-poem) Box(I-poem) Love(I-poem) Song(I-poem) )(O) .(O)"}}
{"id":"49","dataset":"crossner_literature","split":"train","label_list":["location","literary genre","writer","poem","magazine","book","event","award","organization","person","country"],"instance":{"id":"49","words":["Fry","helped","to","fund","a","1988","London","re-staging","of","Stanshall","'s","Stinkfoot",",","a","Comic","Opera",",","written","by","Vivian","and","Ki","Longfellow","for","the","Bristol","-based","The","Thekla","."],"labels":["B-person","O","O","O","O","O","B-location","O","O","B-writer","O","B-book","I-book","I-book","I-book","I-book","O","O","O","B-writer","O","B-writer","I-writer","O","O","B-location","O","B-location","I-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, literary genre, writer, poem, magazine, book, event, award, organization, person, country and O.\nSentence: Fry helped to fund a 1988 London re-staging of Stanshall 's Stinkfoot , a Comic Opera , written by Vivian and Ki Longfellow for the Bristol -based The Thekla .","prompt_labels":"Fry(B-person) helped(O) to(O) fund(O) a(O) 1988(O) London(B-location) re-staging(O) of(O) Stanshall(B-writer) 's(O) Stinkfoot(B-book) ,(I-book) a(I-book) Comic(I-book) Opera(I-book) ,(O) written(O) by(O) Vivian(B-writer) and(O) Ki(B-writer) Longfellow(I-writer) for(O) the(O) Bristol(B-location) -based(O) The(B-location) Thekla(I-location) .(O)"}}
{"id":"50","dataset":"crossner_literature","split":"train","label_list":["location","book","poem","literary genre","writer","country","person","organization","award","magazine","event"],"instance":{"id":"50","words":["Another","Gruelle","family","friends","was","Hoosier","poet","James","Whitcomb","Riley",",","whose","poems","The","Elf-Child",",","later","titled","Little","Orphant","Annie","!","--","Orphant","is","correct","--","not","the","comic","strip--","(","1885",")",",","and","The","Raggedy","Man","(","1888",")",",","eventually","formed","the","name","for","John","Gruelle","'s","iconic","Raggedy","Ann","character","."],"labels":["O","B-person","O","O","O","B-writer","I-writer","B-writer","I-writer","I-writer","O","O","B-literary genre","B-poem","I-poem","O","O","O","B-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, book, poem, literary genre, writer, country, person, organization, award, magazine, event and O.\nSentence: Another Gruelle family friends was Hoosier poet James Whitcomb Riley , whose poems The Elf-Child , later titled Little Orphant Annie ! -- Orphant is correct -- not the comic strip-- ( 1885 ) , and The Raggedy Man ( 1888 ) , eventually formed the name for John Gruelle 's iconic Raggedy Ann character .","prompt_labels":"Another(O) Gruelle(B-person) family(O) friends(O) was(O) Hoosier(B-writer) poet(I-writer) James(B-writer) Whitcomb(I-writer) Riley(I-writer) ,(O) whose(O) poems(B-literary genre) The(B-poem) Elf-Child(I-poem) ,(O) later(O) titled(O) Little(B-poem) Orphant(I-poem) Annie(I-poem) !(O) --(O) Orphant(O) is(O) correct(O) --(O) not(O) the(O) comic(O) strip--(O) ((O) 1885(O) )(O) ,(O) and(O) The(B-poem) Raggedy(I-poem) Man(I-poem) ((O) 1888(O) )(O) ,(O) eventually(O) formed(O) the(O) name(O) for(O) John(B-person) Gruelle(I-person) 's(O) iconic(O) Raggedy(O) Ann(O) character(O) .(O)"}}
{"id":"51","dataset":"crossner_literature","split":"train","label_list":["event","person","magazine","book","award","writer","location","literary genre","country","poem","organization"],"instance":{"id":"51","words":["In","1927","Lu","was","considered","for","the","Nobel","Prize","in","Literature",",","for","the","short","story","The","TRUE","Story","of","Ah","Q",",","despite","a","poor","English","translation","and","annotations","that","were","nearly","double","the","size","of","the","text.","Kowallis","3","Lu","rejected","the","possibility","of","accepting","the","nomination","."],"labels":["O","O","B-writer","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","B-literary genre","I-literary genre","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, magazine, book, award, writer, location, literary genre, country, poem, organization and O.\nSentence: In 1927 Lu was considered for the Nobel Prize in Literature , for the short story The TRUE Story of Ah Q , despite a poor English translation and annotations that were nearly double the size of the text. Kowallis 3 Lu rejected the possibility of accepting the nomination .","prompt_labels":"In(O) 1927(O) Lu(B-writer) was(O) considered(O) for(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) ,(O) for(O) the(O) short(B-literary genre) story(I-literary genre) The(B-book) TRUE(I-book) Story(I-book) of(I-book) Ah(I-book) Q(I-book) ,(O) despite(O) a(O) poor(O) English(O) translation(O) and(O) annotations(O) that(O) were(O) nearly(O) double(O) the(O) size(O) of(O) the(O) text.(O) Kowallis(B-writer) 3(I-writer) Lu(I-writer) rejected(O) the(O) possibility(O) of(O) accepting(O) the(O) nomination(O) .(O)"}}
{"id":"52","dataset":"crossner_literature","split":"train","label_list":["writer","magazine","literary genre","poem","location","award","book","event","country","person","organization"],"instance":{"id":"52","words":["Sagan","and","his","works","received","numerous","awards","and","honors",",","including","the","NASA","Distinguished","Public","Service","Medal",",","the","National","Academy","of","Sciences","Public","Welfare","Medal",",","the","Pulitzer","Prize","for","General","Non-Fiction","for","his","book","The","Dragons","of","Eden",",","and",",","regarding","Cosmos",":","A","Personal","Voyage",",","two","Emmy","Award","s",",","the","Peabody","Award",",","and","the","Hugo","Award","."],"labels":["B-writer","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","B-award","I-award","O","O","O","B-award","I-award","O","O","O","B-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, magazine, literary genre, poem, location, award, book, event, country, person, organization and O.\nSentence: Sagan and his works received numerous awards and honors , including the NASA Distinguished Public Service Medal , the National Academy of Sciences Public Welfare Medal , the Pulitzer Prize for General Non-Fiction for his book The Dragons of Eden , and , regarding Cosmos : A Personal Voyage , two Emmy Award s , the Peabody Award , and the Hugo Award .","prompt_labels":"Sagan(B-writer) and(O) his(O) works(O) received(O) numerous(O) awards(O) and(O) honors(O) ,(O) including(O) the(O) NASA(B-award) Distinguished(I-award) Public(I-award) Service(I-award) Medal(I-award) ,(O) the(O) National(B-award) Academy(I-award) of(I-award) Sciences(I-award) Public(I-award) Welfare(I-award) Medal(I-award) ,(O) the(O) Pulitzer(B-award) Prize(I-award) for(I-award) General(I-award) Non-Fiction(I-award) for(O) his(O) book(O) The(B-book) Dragons(I-book) of(I-book) Eden(I-book) ,(O) and(O) ,(O) regarding(O) Cosmos(B-book) :(I-book) A(I-book) Personal(I-book) Voyage(I-book) ,(O) two(O) Emmy(B-award) Award(I-award) s(O) ,(O) the(O) Peabody(B-award) Award(I-award) ,(O) and(O) the(O) Hugo(B-award) Award(I-award) .(O)"}}
{"id":"53","dataset":"crossner_literature","split":"train","label_list":["organization","book","writer","country","award","literary genre","event","location","magazine","person","poem"],"instance":{"id":"53","words":["Jackie","was","a","play-by-play","announcer","for","the","Luge","at","the","1976","Winter","Olympics","and","the","Equestrian","at","the","1976","Summer","Olympics","(","partnered","with","Chris","Schenkel",")","on","ABC","'s","Wide","World","of","Sports","."],"labels":["B-person","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, book, writer, country, award, literary genre, event, location, magazine, person, poem and O.\nSentence: Jackie was a play-by-play announcer for the Luge at the 1976 Winter Olympics and the Equestrian at the 1976 Summer Olympics ( partnered with Chris Schenkel ) on ABC 's Wide World of Sports .","prompt_labels":"Jackie(B-person) was(O) a(O) play-by-play(O) announcer(O) for(O) the(O) Luge(B-event) at(I-event) the(I-event) 1976(I-event) Winter(I-event) Olympics(I-event) and(O) the(O) Equestrian(B-event) at(I-event) the(I-event) 1976(I-event) Summer(I-event) Olympics(I-event) ((O) partnered(O) with(O) Chris(B-person) Schenkel(I-person) )(O) on(O) ABC(O) 's(O) Wide(O) World(O) of(O) Sports(O) .(O)"}}
{"id":"54","dataset":"crossner_literature","split":"train","label_list":["person","country","event","literary genre","location","organization","award","writer","book","poem","magazine"],"instance":{"id":"54","words":["Augustine","was","born","in","the","year","354","AD","in","the","municipium","of","Thagaste","(","now","Souk","Ahras",",","Algeria",")","in","the","Roman","province","of","Numidia","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","B-location","I-location","O","B-country","O","O","O","B-location","I-location","I-location","I-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, event, literary genre, location, organization, award, writer, book, poem, magazine and O.\nSentence: Augustine was born in the year 354 AD in the municipium of Thagaste ( now Souk Ahras , Algeria ) in the Roman province of Numidia .","prompt_labels":"Augustine(B-person) was(O) born(O) in(O) the(O) year(O) 354(O) AD(O) in(O) the(O) municipium(O) of(O) Thagaste(B-location) ((O) now(O) Souk(B-location) Ahras(I-location) ,(O) Algeria(B-country) )(O) in(O) the(O) Roman(B-location) province(I-location) of(I-location) Numidia(I-location) .(O)"}}
{"id":"55","dataset":"crossner_literature","split":"train","label_list":["writer","literary genre","location","award","event","book","country","organization","magazine","person","poem"],"instance":{"id":"55","words":["In","addition","to","writing","for","The","New","Yorker",",","he","has","written","for","The","Atlantic","Monthly","and","National","Geographic","."],"labels":["O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","B-magazine","I-magazine","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, literary genre, location, award, event, book, country, organization, magazine, person, poem and O.\nSentence: In addition to writing for The New Yorker , he has written for The Atlantic Monthly and National Geographic .","prompt_labels":"In(O) addition(O) to(O) writing(O) for(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) ,(O) he(O) has(O) written(O) for(O) The(B-magazine) Atlantic(I-magazine) Monthly(I-magazine) and(O) National(B-magazine) Geographic(I-magazine) .(O)"}}
{"id":"56","dataset":"crossner_literature","split":"train","label_list":["magazine","writer","event","person","poem","organization","book","country","award","location","literary genre"],"instance":{"id":"56","words":["In","2012",",","Nichols","won","the","Best","Direction","of","a","Play","Tony","Award","Award","for","Arthur","Miller","'","s","Death","of","a","Salesman","."],"labels":["O","O","O","B-person","O","O","O","O","O","O","O","B-award","I-award","O","O","B-writer","I-writer","O","O","B-book","I-book","I-book","I-book","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, writer, event, person, poem, organization, book, country, award, location, literary genre and O.\nSentence: In 2012 , Nichols won the Best Direction of a Play Tony Award Award for Arthur Miller ' s Death of a Salesman .","prompt_labels":"In(O) 2012(O) ,(O) Nichols(B-person) won(O) the(O) Best(O) Direction(O) of(O) a(O) Play(O) Tony(B-award) Award(I-award) Award(O) for(O) Arthur(B-writer) Miller(I-writer) '(O) s(O) Death(B-book) of(I-book) a(I-book) Salesman(I-book) .(O)"}}
{"id":"57","dataset":"crossner_literature","split":"train","label_list":["magazine","location","person","writer","poem","event","country","literary genre","award","organization","book"],"instance":{"id":"57","words":["To","please","his","wife",",","Diederichs","agreed","to","publish","Hesse","'s","collection","of","prose","entitled","One","Hour","After","Midnight","in","1898","(","although","it","is","dated","1899",")",".Freedman","(","1978",")","pp.","78-80","."],"labels":["O","O","O","O","O","B-person","O","O","O","B-writer","O","O","O","B-literary genre","O","B-poem","I-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","B-book","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, person, writer, poem, event, country, literary genre, award, organization, book and O.\nSentence: To please his wife , Diederichs agreed to publish Hesse 's collection of prose entitled One Hour After Midnight in 1898 ( although it is dated 1899 ) .Freedman ( 1978 ) pp. 78-80 .","prompt_labels":"To(O) please(O) his(O) wife(O) ,(O) Diederichs(B-person) agreed(O) to(O) publish(O) Hesse(B-writer) 's(O) collection(O) of(O) prose(B-literary genre) entitled(O) One(B-poem) Hour(I-poem) After(I-poem) Midnight(I-poem) in(O) 1898(O) ((O) although(O) it(O) is(O) dated(O) 1899(O) )(O) .Freedman(B-book) ((O) 1978(O) )(O) pp.(O) 78-80(O) .(O)"}}
{"id":"58","dataset":"crossner_literature","split":"train","label_list":["award","poem","person","literary genre","event","country","book","location","magazine","writer","organization"],"instance":{"id":"58","words":["In","Far","from","the","Madding","Crowd",",","Hardy","first","introduced","the","idea","of","calling","the","region","in","the","west","of","England",",","where","his","novels","are","set",",","Wessex","."],"labels":["O","B-book","I-book","I-book","I-book","I-book","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","B-literary genre","O","O","O","B-country","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, poem, person, literary genre, event, country, book, location, magazine, writer, organization and O.\nSentence: In Far from the Madding Crowd , Hardy first introduced the idea of calling the region in the west of England , where his novels are set , Wessex .","prompt_labels":"In(O) Far(B-book) from(I-book) the(I-book) Madding(I-book) Crowd(I-book) ,(O) Hardy(B-writer) first(O) introduced(O) the(O) idea(O) of(O) calling(O) the(O) region(O) in(O) the(O) west(O) of(O) England(B-country) ,(O) where(O) his(O) novels(B-literary genre) are(O) set(O) ,(O) Wessex(B-country) .(O)"}}
{"id":"59","dataset":"crossner_literature","split":"train","label_list":["literary genre","book","award","event","location","person","country","poem","magazine","organization","writer"],"instance":{"id":"59","words":["Charles","spent","time","outdoors",",","but","also","read","voraciously",",","including","the","picaresque","novel","s","of","Tobias","Smollett","and","Henry","Fielding",",","as","well","as","Robinson","Crusoe","and","Gil","Blas","."],"labels":["B-writer","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O","B-book","I-book","O","B-book","I-book","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, book, award, event, location, person, country, poem, magazine, organization, writer and O.\nSentence: Charles spent time outdoors , but also read voraciously , including the picaresque novel s of Tobias Smollett and Henry Fielding , as well as Robinson Crusoe and Gil Blas .","prompt_labels":"Charles(B-writer) spent(O) time(O) outdoors(O) ,(O) but(O) also(O) read(O) voraciously(O) ,(O) including(O) the(O) picaresque(B-literary genre) novel(I-literary genre) s(O) of(O) Tobias(B-writer) Smollett(I-writer) and(O) Henry(B-writer) Fielding(I-writer) ,(O) as(O) well(O) as(O) Robinson(B-book) Crusoe(I-book) and(O) Gil(B-book) Blas(I-book) .(O)"}}
{"id":"60","dataset":"crossner_literature","split":"train","label_list":["organization","country","event","writer","award","poem","person","magazine","location","literary genre","book"],"instance":{"id":"60","words":["Paul","Christopher",",","J.","D.","Salinger","'","s","Holden","Caulfield",",","and","J.","P.","Donleavy","'","s","Sebastion","Dangerfield","in","The","Ginger","Man","(","1955",")","are","among","the","post-World","War","II","literary","heroes","who","have","stymied","Hollywood","efforts","to","depict","them.see","J.P.","Donnelley",",","91",",","Author","Who","Stirred","Controversy","With","'","Ginger","Man",",","'","Dies",",","The","New","York","Times",",","September","14",",","2017","."],"labels":["B-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","B-person","I-person","O","O","B-writer","I-writer","I-writer","O","O","B-person","I-person","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","B-book","I-book","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, event, writer, award, poem, person, magazine, location, literary genre, book and O.\nSentence: Paul Christopher , J. D. Salinger ' s Holden Caulfield , and J. P. Donleavy ' s Sebastion Dangerfield in The Ginger Man ( 1955 ) are among the post-World War II literary heroes who have stymied Hollywood efforts to depict them.see J.P. Donnelley , 91 , Author Who Stirred Controversy With ' Ginger Man , ' Dies , The New York Times , September 14 , 2017 .","prompt_labels":"Paul(B-writer) Christopher(I-writer) ,(O) J.(B-writer) D.(I-writer) Salinger(I-writer) '(O) s(O) Holden(B-person) Caulfield(I-person) ,(O) and(O) J.(B-writer) P.(I-writer) Donleavy(I-writer) '(O) s(O) Sebastion(B-person) Dangerfield(I-person) in(O) The(B-book) Ginger(I-book) Man(I-book) ((O) 1955(O) )(O) are(O) among(O) the(O) post-World(O) War(O) II(O) literary(O) heroes(O) who(O) have(O) stymied(O) Hollywood(B-organization) efforts(O) to(O) depict(O) them.see(O) J.P.(B-writer) Donnelley(I-writer) ,(O) 91(O) ,(O) Author(O) Who(O) Stirred(O) Controversy(O) With(O) '(O) Ginger(B-book) Man(I-book) ,(O) '(O) Dies(O) ,(O) The(B-organization) New(I-organization) York(I-organization) Times(I-organization) ,(O) September(O) 14(O) ,(O) 2017(O) .(O)"}}
{"id":"61","dataset":"crossner_literature","split":"train","label_list":["award","country","location","person","literary genre","organization","event","writer","magazine","book","poem"],"instance":{"id":"61","words":["1789",")","with","the","addition","of","five","of","his","poems",":","the","Introduction","and","The","Divine","Image","from","the","Songs","of","Innocence","(","1789",")",",","The","Tyger","and","A","Divine","Image","from","the","Songs","of","Experience","(","1789-1794",")",",","and","A","Cradle","Song","from","his","Note-book","(","Manuscript","Dante","Gabriel","Rossetti",",","1793",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-poem","I-poem","I-poem","O","O","B-book","I-book","I-book","O","O","O","O","B-poem","I-poem","O","B-poem","I-poem","I-poem","O","O","B-book","I-book","I-book","O","O","O","O","O","B-poem","I-poem","I-poem","O","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, location, person, literary genre, organization, event, writer, magazine, book, poem and O.\nSentence: 1789 ) with the addition of five of his poems : the Introduction and The Divine Image from the Songs of Innocence ( 1789 ) , The Tyger and A Divine Image from the Songs of Experience ( 1789-1794 ) , and A Cradle Song from his Note-book ( Manuscript Dante Gabriel Rossetti , 1793 ) .","prompt_labels":"1789(O) )(O) with(O) the(O) addition(O) of(O) five(O) of(O) his(O) poems(O) :(O) the(O) Introduction(O) and(O) The(B-poem) Divine(I-poem) Image(I-poem) from(O) the(O) Songs(B-book) of(I-book) Innocence(I-book) ((O) 1789(O) )(O) ,(O) The(B-poem) Tyger(I-poem) and(O) A(B-poem) Divine(I-poem) Image(I-poem) from(O) the(O) Songs(B-book) of(I-book) Experience(I-book) ((O) 1789-1794(O) )(O) ,(O) and(O) A(B-poem) Cradle(I-poem) Song(I-poem) from(O) his(O) Note-book(O) ((O) Manuscript(O) Dante(B-writer) Gabriel(I-writer) Rossetti(I-writer) ,(O) 1793(O) )(O) .(O)"}}
{"id":"62","dataset":"crossner_literature","split":"train","label_list":["organization","person","book","country","poem","literary genre","writer","magazine","event","location","award"],"instance":{"id":"62","words":["Under","the","succeeding","Han","dynasty","and","Tang","dynasty","dynasties",",","Confucian","ideas","gained","even","more","widespread","prominence","."],"labels":["O","O","O","B-country","I-country","O","B-country","I-country","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, book, country, poem, literary genre, writer, magazine, event, location, award and O.\nSentence: Under the succeeding Han dynasty and Tang dynasty dynasties , Confucian ideas gained even more widespread prominence .","prompt_labels":"Under(O) the(O) succeeding(O) Han(B-country) dynasty(I-country) and(O) Tang(B-country) dynasty(I-country) dynasties(O) ,(O) Confucian(O) ideas(O) gained(O) even(O) more(O) widespread(O) prominence(O) .(O)"}}
{"id":"63","dataset":"crossner_literature","split":"train","label_list":["book","literary genre","person","poem","organization","location","magazine","award","writer","event","country"],"instance":{"id":"63","words":["In","2012",",","when","the","Nobel","Prize","Records","were","opened","after","50","years",",","it","was","revealed","that","Durrell","had","been","on","a","shortlist","of","authors","considered","for","the","1962","Nobel","Prize","in","Literature",",","along","with","American","John","Steinbeck","(","winner",")",",","British","poet","Robert","Graves",",","French","writer","Jean","Anouilh",",","and","the","Danish","Karen","Blixen","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","B-writer","I-writer","O","O","O","B-writer","I-writer","O","O","O","O","B-writer","I-writer","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, literary genre, person, poem, organization, location, magazine, award, writer, event, country and O.\nSentence: In 2012 , when the Nobel Prize Records were opened after 50 years , it was revealed that Durrell had been on a shortlist of authors considered for the 1962 Nobel Prize in Literature , along with American John Steinbeck ( winner ) , British poet Robert Graves , French writer Jean Anouilh , and the Danish Karen Blixen .","prompt_labels":"In(O) 2012(O) ,(O) when(O) the(O) Nobel(O) Prize(O) Records(O) were(O) opened(O) after(O) 50(O) years(O) ,(O) it(O) was(O) revealed(O) that(O) Durrell(B-writer) had(O) been(O) on(O) a(O) shortlist(O) of(O) authors(O) considered(O) for(O) the(O) 1962(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) ,(O) along(O) with(O) American(O) John(B-writer) Steinbeck(I-writer) ((O) winner(O) )(O) ,(O) British(O) poet(O) Robert(B-writer) Graves(I-writer) ,(O) French(O) writer(O) Jean(B-writer) Anouilh(I-writer) ,(O) and(O) the(O) Danish(O) Karen(B-writer) Blixen(I-writer) .(O)"}}
{"id":"64","dataset":"crossner_literature","split":"train","label_list":["location","book","poem","magazine","person","writer","organization","country","event","award","literary genre"],"instance":{"id":"64","words":["His","time-travel","novel","Timescape","(","1980",")","won","both","the","Nebula","Award","and","the","John","W.","Campbell","Memorial","Award","."],"labels":["O","O","B-literary genre","B-book","O","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, book, poem, magazine, person, writer, organization, country, event, award, literary genre and O.\nSentence: His time-travel novel Timescape ( 1980 ) won both the Nebula Award and the John W. Campbell Memorial Award .","prompt_labels":"His(O) time-travel(O) novel(B-literary genre) Timescape(B-book) ((O) 1980(O) )(O) won(O) both(O) the(O) Nebula(B-award) Award(I-award) and(O) the(O) John(B-award) W.(I-award) Campbell(I-award) Memorial(I-award) Award(I-award) .(O)"}}
{"id":"65","dataset":"crossner_literature","split":"train","label_list":["literary genre","poem","book","event","award","location","writer","magazine","person","country","organization"],"instance":{"id":"65","words":["In","1985",",","Spielberg","released","The","Color","Purple",",","an","adaptation","of","Alice","Walker","'","s","Pulitzer","Prize","-winning","The","Color","Purple",",","about","a","generation","of","empowered","African-American","women","during","depression-era","America","."],"labels":["O","O","O","B-writer","O","B-book","I-book","I-book","O","O","O","O","B-writer","I-writer","O","O","B-award","I-award","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","B-country","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, poem, book, event, award, location, writer, magazine, person, country, organization and O.\nSentence: In 1985 , Spielberg released The Color Purple , an adaptation of Alice Walker ' s Pulitzer Prize -winning The Color Purple , about a generation of empowered African-American women during depression-era America .","prompt_labels":"In(O) 1985(O) ,(O) Spielberg(B-writer) released(O) The(B-book) Color(I-book) Purple(I-book) ,(O) an(O) adaptation(O) of(O) Alice(B-writer) Walker(I-writer) '(O) s(O) Pulitzer(B-award) Prize(I-award) -winning(O) The(B-book) Color(I-book) Purple(I-book) ,(O) about(O) a(O) generation(O) of(O) empowered(O) African-American(O) women(O) during(O) depression-era(O) America(B-country) .(O)"}}
{"id":"66","dataset":"crossner_literature","split":"train","label_list":["poem","writer","book","event","person","award","magazine","literary genre","location","organization","country"],"instance":{"id":"66","words":["It","is","largely","based","on","the","Alexandreis","of","Walter","of","Châtillon",",","but","also","contains","many","fantastical","elements","common","to","the","Alexander","romance","."],"labels":["O","O","O","O","O","O","B-poem","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","O","O","B-book","I-book","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, writer, book, event, person, award, magazine, literary genre, location, organization, country and O.\nSentence: It is largely based on the Alexandreis of Walter of Châtillon , but also contains many fantastical elements common to the Alexander romance .","prompt_labels":"It(O) is(O) largely(O) based(O) on(O) the(O) Alexandreis(B-poem) of(O) Walter(B-writer) of(I-writer) Châtillon(I-writer) ,(O) but(O) also(O) contains(O) many(O) fantastical(O) elements(O) common(O) to(O) the(O) Alexander(B-book) romance(I-book) .(O)"}}
{"id":"67","dataset":"crossner_literature","split":"train","label_list":["book","location","country","poem","person","event","award","literary genre","magazine","organization","writer"],"instance":{"id":"67","words":["For","the","general","reader",",","Jonson","'s","reputation","rests","on","a","few","lyrics","that",",","though","brief",",","are","surpassed","for","grace","and","precision","by","very","few","Renaissance","poems",":","On","My","First","Sonne",";","To","Celia",";","To","Penshurst",";","and","the","epitaph","on","Salomon","Pavy",",","a","boy","player","abducted","from","his","parents","who","acted","in","Jonson","'s","plays","."],"labels":["O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","B-poem","I-poem","I-poem","I-poem","O","B-poem","I-poem","O","B-poem","I-poem","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, location, country, poem, person, event, award, literary genre, magazine, organization, writer and O.\nSentence: For the general reader , Jonson 's reputation rests on a few lyrics that , though brief , are surpassed for grace and precision by very few Renaissance poems : On My First Sonne ; To Celia ; To Penshurst ; and the epitaph on Salomon Pavy , a boy player abducted from his parents who acted in Jonson 's plays .","prompt_labels":"For(O) the(O) general(O) reader(O) ,(O) Jonson(B-person) 's(O) reputation(O) rests(O) on(O) a(O) few(O) lyrics(O) that(O) ,(O) though(O) brief(O) ,(O) are(O) surpassed(O) for(O) grace(O) and(O) precision(O) by(O) very(O) few(O) Renaissance(B-literary genre) poems(I-literary genre) :(O) On(B-poem) My(I-poem) First(I-poem) Sonne(I-poem) ;(O) To(B-poem) Celia(I-poem) ;(O) To(B-poem) Penshurst(I-poem) ;(O) and(O) the(O) epitaph(O) on(O) Salomon(B-person) Pavy(I-person) ,(O) a(O) boy(O) player(O) abducted(O) from(O) his(O) parents(O) who(O) acted(O) in(O) Jonson(B-writer) 's(O) plays(O) .(O)"}}
{"id":"68","dataset":"crossner_literature","split":"train","label_list":["literary genre","location","magazine","event","book","person","organization","award","writer","poem","country"],"instance":{"id":"68","words":["During","most","of","his","career",",","Orwell","was","best","known","for","his","journalism",",","in","essays",",","reviews",",","columns","in","newspapers","and","magazines","and","in","his","books","of","reportage",":","Down","and","Out","in","Paris","and","London","(","describing","a","period","of","poverty","in","these","cities",")",",","The","Road","to","Wigan","Pier","(","describing","the","living","conditions","of","the","poor","in","northern","England",",","and","class","division","generally",")","and","Homage","to","Catalonia","."],"labels":["O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","B-book","I-book","I-book","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, location, magazine, event, book, person, organization, award, writer, poem, country and O.\nSentence: During most of his career , Orwell was best known for his journalism , in essays , reviews , columns in newspapers and magazines and in his books of reportage : Down and Out in Paris and London ( describing a period of poverty in these cities ) , The Road to Wigan Pier ( describing the living conditions of the poor in northern England , and class division generally ) and Homage to Catalonia .","prompt_labels":"During(O) most(O) of(O) his(O) career(O) ,(O) Orwell(B-writer) was(O) best(O) known(O) for(O) his(O) journalism(O) ,(O) in(O) essays(O) ,(O) reviews(O) ,(O) columns(O) in(O) newspapers(O) and(O) magazines(O) and(O) in(O) his(O) books(O) of(O) reportage(O) :(O) Down(B-book) and(I-book) Out(I-book) in(I-book) Paris(I-book) and(I-book) London(I-book) ((O) describing(O) a(O) period(O) of(O) poverty(O) in(O) these(O) cities(O) )(O) ,(O) The(B-book) Road(I-book) to(I-book) Wigan(I-book) Pier(I-book) ((O) describing(O) the(O) living(O) conditions(O) of(O) the(O) poor(O) in(O) northern(O) England(B-country) ,(O) and(O) class(O) division(O) generally(O) )(O) and(O) Homage(B-book) to(I-book) Catalonia(I-book) .(O)"}}
{"id":"69","dataset":"crossner_literature","split":"train","label_list":["poem","book","organization","literary genre","event","person","country","location","magazine","writer","award"],"instance":{"id":"69","words":["A","late","(","1890s",")","reference","to","the","urban","legend","of","the","murderous","barber","can","be","found","in","the","poem","by","the","Australian","bush","poet","Banjo","Paterson","-","The","Man","from","Ironbark","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","O","O","O","O","O","B-person","I-person","O","B-poem","I-poem","I-poem","I-poem","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, book, organization, literary genre, event, person, country, location, magazine, writer, award and O.\nSentence: A late ( 1890s ) reference to the urban legend of the murderous barber can be found in the poem by the Australian bush poet Banjo Paterson - The Man from Ironbark .","prompt_labels":"A(O) late(O) ((O) 1890s(O) )(O) reference(O) to(O) the(O) urban(O) legend(O) of(O) the(O) murderous(O) barber(O) can(O) be(O) found(O) in(O) the(O) poem(B-literary genre) by(O) the(O) Australian(O) bush(O) poet(O) Banjo(B-person) Paterson(I-person) -(O) The(B-poem) Man(I-poem) from(I-poem) Ironbark(I-poem) .(O)"}}
{"id":"70","dataset":"crossner_literature","split":"train","label_list":["event","person","location","literary genre","magazine","organization","country","poem","award","book","writer"],"instance":{"id":"70","words":["This","acted","as","a","prelude","to","the","release","the","following","year","of","The","Whitsun","Weddings",",","the","volume","which","cemented","his","reputation",";","almost","immediately","after","its","publication","he","was","granted","a","Fellowship","of","the","Royal","Society","of","Literature","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, location, literary genre, magazine, organization, country, poem, award, book, writer and O.\nSentence: This acted as a prelude to the release the following year of The Whitsun Weddings , the volume which cemented his reputation ; almost immediately after its publication he was granted a Fellowship of the Royal Society of Literature .","prompt_labels":"This(O) acted(O) as(O) a(O) prelude(O) to(O) the(O) release(O) the(O) following(O) year(O) of(O) The(B-book) Whitsun(I-book) Weddings(I-book) ,(O) the(O) volume(O) which(O) cemented(O) his(O) reputation(O) ;(O) almost(O) immediately(O) after(O) its(O) publication(O) he(O) was(O) granted(O) a(O) Fellowship(O) of(O) the(O) Royal(B-organization) Society(I-organization) of(I-organization) Literature(I-organization) .(O)"}}
{"id":"71","dataset":"crossner_literature","split":"train","label_list":["person","country","writer","literary genre","organization","event","book","poem","award","location","magazine"],"instance":{"id":"71","words":["Bova","holds","the","position","of","President","Emeritus","of","the","National","Space","Society","and","served","as","President","of","Science","Fiction","and","Fantasy","Writers","of","America","(","SFWA",")","from","1990","to","1992","."],"labels":["B-writer","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, writer, literary genre, organization, event, book, poem, award, location, magazine and O.\nSentence: Bova holds the position of President Emeritus of the National Space Society and served as President of Science Fiction and Fantasy Writers of America ( SFWA ) from 1990 to 1992 .","prompt_labels":"Bova(B-writer) holds(O) the(O) position(O) of(O) President(O) Emeritus(O) of(O) the(O) National(B-organization) Space(I-organization) Society(I-organization) and(O) served(O) as(O) President(O) of(O) Science(B-organization) Fiction(I-organization) and(I-organization) Fantasy(I-organization) Writers(I-organization) of(I-organization) America(I-organization) ((O) SFWA(B-organization) )(O) from(O) 1990(O) to(O) 1992(O) .(O)"}}
{"id":"72","dataset":"crossner_literature","split":"train","label_list":["literary genre","country","award","magazine","book","writer","person","location","event","poem","organization"],"instance":{"id":"72","words":["This","strand","continues","in","Latin","accounts","of","the","Trojan","War","by","writers","such","as","Dictys","Cretensis","and","Dares","Phrygius","and","in","Benoît","de","Sainte-Maure","'","s","Roman","de","Troie","and","Guido","delle","Colonne","'","s","Historia","destructionis","Troiae",",","which","remained","the","most","widely","read","and","retold","versions","of","the","Matter","of","Troy","until","the","17th","century","."],"labels":["O","O","O","O","O","O","O","O","B-event","I-event","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","B-writer","I-writer","I-writer","O","O","B-book","I-book","I-book","O","B-writer","I-writer","I-writer","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, country, award, magazine, book, writer, person, location, event, poem, organization and O.\nSentence: This strand continues in Latin accounts of the Trojan War by writers such as Dictys Cretensis and Dares Phrygius and in Benoît de Sainte-Maure ' s Roman de Troie and Guido delle Colonne ' s Historia destructionis Troiae , which remained the most widely read and retold versions of the Matter of Troy until the 17th century .","prompt_labels":"This(O) strand(O) continues(O) in(O) Latin(O) accounts(O) of(O) the(O) Trojan(B-event) War(I-event) by(O) writers(O) such(O) as(O) Dictys(B-writer) Cretensis(I-writer) and(O) Dares(B-writer) Phrygius(I-writer) and(O) in(O) Benoît(B-writer) de(I-writer) Sainte-Maure(I-writer) '(O) s(O) Roman(B-book) de(I-book) Troie(I-book) and(O) Guido(B-writer) delle(I-writer) Colonne(I-writer) '(O) s(O) Historia(B-book) destructionis(I-book) Troiae(I-book) ,(O) which(O) remained(O) the(O) most(O) widely(O) read(O) and(O) retold(O) versions(O) of(O) the(O) Matter(O) of(O) Troy(O) until(O) the(O) 17th(O) century(O) .(O)"}}
{"id":"73","dataset":"crossner_literature","split":"train","label_list":["magazine","location","book","literary genre","person","organization","writer","event","country","award","poem"],"instance":{"id":"73","words":["Other","works","soon","followed",",","including","A","Tale","of","Two","Cities","(","1859",")","and","Great","Expectations","(","1861",")",",","which","were","resounding","successes","."],"labels":["O","O","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, book, literary genre, person, organization, writer, event, country, award, poem and O.\nSentence: Other works soon followed , including A Tale of Two Cities ( 1859 ) and Great Expectations ( 1861 ) , which were resounding successes .","prompt_labels":"Other(O) works(O) soon(O) followed(O) ,(O) including(O) A(B-book) Tale(I-book) of(I-book) Two(I-book) Cities(I-book) ((O) 1859(O) )(O) and(O) Great(B-book) Expectations(I-book) ((O) 1861(O) )(O) ,(O) which(O) were(O) resounding(O) successes(O) .(O)"}}
{"id":"74","dataset":"crossner_literature","split":"train","label_list":["book","event","person","country","poem","writer","organization","literary genre","location","magazine","award"],"instance":{"id":"74","words":["In","1952",",","Capp","and","his","characters","graced","the","covers","of","both","Life","and","TV","Guide","."],"labels":["O","O","O","B-writer","O","O","O","O","O","O","O","O","B-magazine","O","B-magazine","I-magazine","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, event, person, country, poem, writer, organization, literary genre, location, magazine, award and O.\nSentence: In 1952 , Capp and his characters graced the covers of both Life and TV Guide .","prompt_labels":"In(O) 1952(O) ,(O) Capp(B-writer) and(O) his(O) characters(O) graced(O) the(O) covers(O) of(O) both(O) Life(B-magazine) and(O) TV(B-magazine) Guide(I-magazine) .(O)"}}
{"id":"75","dataset":"crossner_literature","split":"train","label_list":["literary genre","organization","writer","event","award","country","magazine","poem","person","location","book"],"instance":{"id":"75","words":["Nobel","Prize","-winning","writer","Isaac","Bashevis","Singer","translated","some","of","his","works","."],"labels":["B-award","I-award","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, organization, writer, event, award, country, magazine, poem, person, location, book and O.\nSentence: Nobel Prize -winning writer Isaac Bashevis Singer translated some of his works .","prompt_labels":"Nobel(B-award) Prize(I-award) -winning(O) writer(O) Isaac(B-writer) Bashevis(I-writer) Singer(I-writer) translated(O) some(O) of(O) his(O) works(O) .(O)"}}
{"id":"76","dataset":"crossner_literature","split":"train","label_list":["book","magazine","person","award","poem","organization","location","country","event","literary genre","writer"],"instance":{"id":"76","words":["The","story","of","a","powerful","(","fairy",")","woman","who","takes","a","lover","on","condition","that","he","obey","a","particular","prohibition","is","common","in","medieval","poetry",":","the","French","lais","of","Desiré",",","Graelent",",","and","Guingamor",",","and","Chrétien","de","Troyes","'","s","romance","Yvain",",","the","Knight","of","the","Lion",",","all","share","similar","plot","elements","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","O","B-poem","O","B-poem","O","O","B-poem","O","O","B-writer","I-writer","I-writer","O","O","O","B-poem","I-poem","I-poem","I-poem","I-poem","I-poem","I-poem","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, magazine, person, award, poem, organization, location, country, event, literary genre, writer and O.\nSentence: The story of a powerful ( fairy ) woman who takes a lover on condition that he obey a particular prohibition is common in medieval poetry : the French lais of Desiré , Graelent , and Guingamor , and Chrétien de Troyes ' s romance Yvain , the Knight of the Lion , all share similar plot elements .","prompt_labels":"The(O) story(O) of(O) a(O) powerful(O) ((O) fairy(O) )(O) woman(O) who(O) takes(O) a(O) lover(O) on(O) condition(O) that(O) he(O) obey(O) a(O) particular(O) prohibition(O) is(O) common(O) in(O) medieval(O) poetry(O) :(O) the(O) French(O) lais(B-literary genre) of(O) Desiré(B-poem) ,(O) Graelent(B-poem) ,(O) and(O) Guingamor(B-poem) ,(O) and(O) Chrétien(B-writer) de(I-writer) Troyes(I-writer) '(O) s(O) romance(O) Yvain(B-poem) ,(I-poem) the(I-poem) Knight(I-poem) of(I-poem) the(I-poem) Lion(I-poem) ,(O) all(O) share(O) similar(O) plot(O) elements(O) .(O)"}}
{"id":"77","dataset":"crossner_literature","split":"train","label_list":["location","poem","book","award","country","literary genre","person","event","organization","magazine","writer"],"instance":{"id":"77","words":["It","was","nominated","for","seven","Academy","Awards","and","won","four",",","including","Academy","Award","for","Best","Picture","and","Academy","Award","for","Best","Original","Screenplay","."],"labels":["O","O","O","O","O","B-award","I-award","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, poem, book, award, country, literary genre, person, event, organization, magazine, writer and O.\nSentence: It was nominated for seven Academy Awards and won four , including Academy Award for Best Picture and Academy Award for Best Original Screenplay .","prompt_labels":"It(O) was(O) nominated(O) for(O) seven(O) Academy(B-award) Awards(I-award) and(O) won(O) four(O) ,(O) including(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Screenplay(I-award) .(O)"}}
{"id":"78","dataset":"crossner_literature","split":"train","label_list":["award","person","magazine","location","book","organization","poem","event","writer","literary genre","country"],"instance":{"id":"78","words":["Also","in","2007","he","provided","consulting","services","to","Silver","Pictures","on","the","film","adaptation","of","Richard","K.","Morgan","'","s","hardboiled","cyberpunk","science-fiction","novel","Altered","Carbon","(","2002",")","."],"labels":["O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","B-literary genre","I-literary genre","B-book","I-book","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, magazine, location, book, organization, poem, event, writer, literary genre, country and O.\nSentence: Also in 2007 he provided consulting services to Silver Pictures on the film adaptation of Richard K. Morgan ' s hardboiled cyberpunk science-fiction novel Altered Carbon ( 2002 ) .","prompt_labels":"Also(O) in(O) 2007(O) he(O) provided(O) consulting(O) services(O) to(O) Silver(B-organization) Pictures(I-organization) on(O) the(O) film(O) adaptation(O) of(O) Richard(B-writer) K.(I-writer) Morgan(I-writer) '(O) s(O) hardboiled(O) cyberpunk(O) science-fiction(B-literary genre) novel(I-literary genre) Altered(B-book) Carbon(I-book) ((O) 2002(O) )(O) .(O)"}}
{"id":"79","dataset":"crossner_literature","split":"train","label_list":["book","country","writer","literary genre","poem","person","organization","event","award","location","magazine"],"instance":{"id":"79","words":["In","Beyond","Good","and","Evil","and","On","the","Genealogy","of","Morality",",","Nietzsche","'s","genealogical","account","of","the","development","of","modern","moral","systems","occupies","a","central","place","."],"labels":["O","B-book","I-book","I-book","I-book","O","B-book","I-book","I-book","I-book","I-book","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, country, writer, literary genre, poem, person, organization, event, award, location, magazine and O.\nSentence: In Beyond Good and Evil and On the Genealogy of Morality , Nietzsche 's genealogical account of the development of modern moral systems occupies a central place .","prompt_labels":"In(O) Beyond(B-book) Good(I-book) and(I-book) Evil(I-book) and(O) On(B-book) the(I-book) Genealogy(I-book) of(I-book) Morality(I-book) ,(O) Nietzsche(B-writer) 's(O) genealogical(O) account(O) of(O) the(O) development(O) of(O) modern(O) moral(O) systems(O) occupies(O) a(O) central(O) place(O) .(O)"}}
{"id":"80","dataset":"crossner_literature","split":"train","label_list":["award","writer","organization","magazine","book","literary genre","poem","event","location","country","person"],"instance":{"id":"80","words":["Due","to","pressure","by","fans","on","Asimov","to","write","another","book","in","his","Foundation","series",",","he","did","so","with","Foundation","'s","Edge","(","1982",")","and","Foundation","and","Earth","(","1986",")",",","and","then","went","back","to","before","the","original","trilogy","with","Prelude","to","Foundation","(","1988",")","and","Forward","the","Foundation","(","1992",")",",","his","last","novel","."],"labels":["O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","B-literary genre","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, writer, organization, magazine, book, literary genre, poem, event, location, country, person and O.\nSentence: Due to pressure by fans on Asimov to write another book in his Foundation series , he did so with Foundation 's Edge ( 1982 ) and Foundation and Earth ( 1986 ) , and then went back to before the original trilogy with Prelude to Foundation ( 1988 ) and Forward the Foundation ( 1992 ) , his last novel .","prompt_labels":"Due(O) to(O) pressure(O) by(O) fans(O) on(O) Asimov(B-writer) to(O) write(O) another(O) book(O) in(O) his(O) Foundation(O) series(O) ,(O) he(O) did(O) so(O) with(O) Foundation(B-book) 's(I-book) Edge(I-book) ((O) 1982(O) )(O) and(O) Foundation(B-book) and(I-book) Earth(I-book) ((O) 1986(O) )(O) ,(O) and(O) then(O) went(O) back(O) to(O) before(O) the(O) original(O) trilogy(O) with(O) Prelude(B-book) to(I-book) Foundation(I-book) ((O) 1988(O) )(O) and(O) Forward(B-book) the(I-book) Foundation(I-book) ((O) 1992(O) )(O) ,(O) his(O) last(O) novel(B-literary genre) .(O)"}}
{"id":"81","dataset":"crossner_literature","split":"train","label_list":["book","person","award","event","poem","literary genre","writer","organization","magazine","location","country"],"instance":{"id":"81","words":["Swift","is","remembered","for","works","such","as","A","Tale","of","a","Tub","(","1704",")",",","An","Argument","Against","Abolishing","Christianity","(","1712",")",",","Gulliver","'s","Travels","(","1726",")",",","and","A","Modest","Proposal","(","1729",")","."],"labels":["B-writer","O","O","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, person, award, event, poem, literary genre, writer, organization, magazine, location, country and O.\nSentence: Swift is remembered for works such as A Tale of a Tub ( 1704 ) , An Argument Against Abolishing Christianity ( 1712 ) , Gulliver 's Travels ( 1726 ) , and A Modest Proposal ( 1729 ) .","prompt_labels":"Swift(B-writer) is(O) remembered(O) for(O) works(O) such(O) as(O) A(B-book) Tale(I-book) of(I-book) a(I-book) Tub(I-book) ((O) 1704(O) )(O) ,(O) An(B-book) Argument(I-book) Against(I-book) Abolishing(I-book) Christianity(I-book) ((O) 1712(O) )(O) ,(O) Gulliver(B-book) 's(I-book) Travels(I-book) ((O) 1726(O) )(O) ,(O) and(O) A(B-book) Modest(I-book) Proposal(I-book) ((O) 1729(O) )(O) .(O)"}}
{"id":"82","dataset":"crossner_literature","split":"train","label_list":["person","location","writer","book","award","magazine","poem","country","event","literary genre","organization"],"instance":{"id":"82","words":["Seymour","Hersh",",","Nixon","'s","Last","Cover-Up",":","The","Tapes","He","Wants","the","Archives","to","Suppress",";","The","New","Yorker",",","December","14",",","1992",",","pp.","80-81","In","passing","sentence","in","February","1972",",","the","judge","rejected","the","D.A.","'","s","motion","that","Capp","agree","to","undergo","psychiatric","treatment","."],"labels":["B-writer","I-writer","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, writer, book, award, magazine, poem, country, event, literary genre, organization and O.\nSentence: Seymour Hersh , Nixon 's Last Cover-Up : The Tapes He Wants the Archives to Suppress ; The New Yorker , December 14 , 1992 , pp. 80-81 In passing sentence in February 1972 , the judge rejected the D.A. ' s motion that Capp agree to undergo psychiatric treatment .","prompt_labels":"Seymour(B-writer) Hersh(I-writer) ,(O) Nixon(B-person) 's(O) Last(O) Cover-Up(O) :(O) The(O) Tapes(O) He(O) Wants(O) the(O) Archives(O) to(O) Suppress(O) ;(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) ,(O) December(O) 14(O) ,(O) 1992(O) ,(O) pp.(O) 80-81(O) In(O) passing(O) sentence(O) in(O) February(O) 1972(O) ,(O) the(O) judge(O) rejected(O) the(O) D.A.(O) '(O) s(O) motion(O) that(O) Capp(B-writer) agree(O) to(O) undergo(O) psychiatric(O) treatment(O) .(O)"}}
{"id":"83","dataset":"crossner_literature","split":"train","label_list":["book","writer","literary genre","location","magazine","award","organization","country","event","poem","person"],"instance":{"id":"83","words":["To","Dolapdere",",","he","sarcastically","gave","the","name","Cholera","(","Kolera","in","Turkish",")","in","Ağır","Roman",",","thereby","recalling","both","its","shabbiness","and","the","fact","that","the","great","Poland","poet","Adam","Mickiewicz","died","there","from","the","cholera","in","1855","."],"labels":["O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","B-writer","I-writer","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, writer, literary genre, location, magazine, award, organization, country, event, poem, person and O.\nSentence: To Dolapdere , he sarcastically gave the name Cholera ( Kolera in Turkish ) in Ağır Roman , thereby recalling both its shabbiness and the fact that the great Poland poet Adam Mickiewicz died there from the cholera in 1855 .","prompt_labels":"To(O) Dolapdere(B-location) ,(O) he(O) sarcastically(O) gave(O) the(O) name(O) Cholera(O) ((O) Kolera(O) in(O) Turkish(O) )(O) in(O) Ağır(B-book) Roman(I-book) ,(O) thereby(O) recalling(O) both(O) its(O) shabbiness(O) and(O) the(O) fact(O) that(O) the(O) great(O) Poland(B-country) poet(O) Adam(B-writer) Mickiewicz(I-writer) died(O) there(O) from(O) the(O) cholera(O) in(O) 1855(O) .(O)"}}
{"id":"84","dataset":"crossner_literature","split":"train","label_list":["writer","location","country","magazine","person","event","organization","literary genre","poem","book","award"],"instance":{"id":"84","words":["He","was","influenced","by","the","Marxist","playwright","Bertolt","Brecht","and","was","invited","by","Brecht","to","be","his","assistant","at","the","East","Berlin","State","Opera","but","turned","down","the","offer","."],"labels":["O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","B-writer","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, location, country, magazine, person, event, organization, literary genre, poem, book, award and O.\nSentence: He was influenced by the Marxist playwright Bertolt Brecht and was invited by Brecht to be his assistant at the East Berlin State Opera but turned down the offer .","prompt_labels":"He(O) was(O) influenced(O) by(O) the(O) Marxist(O) playwright(O) Bertolt(B-writer) Brecht(I-writer) and(O) was(O) invited(O) by(O) Brecht(B-writer) to(O) be(O) his(O) assistant(O) at(O) the(O) East(B-location) Berlin(I-location) State(I-location) Opera(I-location) but(O) turned(O) down(O) the(O) offer(O) .(O)"}}
{"id":"85","dataset":"crossner_literature","split":"train","label_list":["organization","book","literary genre","award","event","poem","writer","country","magazine","location","person"],"instance":{"id":"85","words":["The","poems","he","had","written","during","his","time","in","prison","were","so","effective","that","Dudley","Randall",",","a","poet","and","owner","of","Broadside","Press",",","published","Knight","'s","first","volume","of","verse",",","Poems","from","Prison",",","and","hailed","Knight","as","one","of","the","major","poets","of","the","Black","Arts","Movement","."],"labels":["O","B-literary genre","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","B-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, book, literary genre, award, event, poem, writer, country, magazine, location, person and O.\nSentence: The poems he had written during his time in prison were so effective that Dudley Randall , a poet and owner of Broadside Press , published Knight 's first volume of verse , Poems from Prison , and hailed Knight as one of the major poets of the Black Arts Movement .","prompt_labels":"The(O) poems(B-literary genre) he(O) had(O) written(O) during(O) his(O) time(O) in(O) prison(O) were(O) so(O) effective(O) that(O) Dudley(B-writer) Randall(I-writer) ,(O) a(O) poet(O) and(O) owner(O) of(O) Broadside(B-organization) Press(I-organization) ,(O) published(O) Knight(O) 's(O) first(O) volume(O) of(O) verse(O) ,(O) Poems(B-poem) from(I-poem) Prison(I-poem) ,(O) and(O) hailed(O) Knight(O) as(O) one(O) of(O) the(O) major(O) poets(O) of(O) the(O) Black(B-event) Arts(I-event) Movement(I-event) .(O)"}}
{"id":"86","dataset":"crossner_literature","split":"train","label_list":["book","event","magazine","writer","award","poem","country","organization","person","literary genre","location"],"instance":{"id":"86","words":["Verne","'s","collaboration","with","the","publisher","Pierre-Jules","Hetzel","led","to","the","creation","of","the","Voyages","extraordinaires",",","a","widely","popular","series","of","scrupulously","researched","adventure","novels","including","Journey","to","the","Center","of","the","Earth","(","1864",")",",","Twenty","Thousand","Leagues","Under","the","Sea","(","1870",")",",","and","Around","the","World","in","Eighty","Days","(","1873",")","."],"labels":["B-writer","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","B-book","I-book","O","O","O","O","O","O","O","O","O","B-literary genre","O","B-book","I-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, event, magazine, writer, award, poem, country, organization, person, literary genre, location and O.\nSentence: Verne 's collaboration with the publisher Pierre-Jules Hetzel led to the creation of the Voyages extraordinaires , a widely popular series of scrupulously researched adventure novels including Journey to the Center of the Earth ( 1864 ) , Twenty Thousand Leagues Under the Sea ( 1870 ) , and Around the World in Eighty Days ( 1873 ) .","prompt_labels":"Verne(B-writer) 's(O) collaboration(O) with(O) the(O) publisher(O) Pierre-Jules(B-writer) Hetzel(I-writer) led(O) to(O) the(O) creation(O) of(O) the(O) Voyages(B-book) extraordinaires(I-book) ,(O) a(O) widely(O) popular(O) series(O) of(O) scrupulously(O) researched(O) adventure(O) novels(B-literary genre) including(O) Journey(B-book) to(I-book) the(I-book) Center(I-book) of(I-book) the(I-book) Earth(I-book) ((O) 1864(O) )(O) ,(O) Twenty(B-book) Thousand(I-book) Leagues(I-book) Under(I-book) the(I-book) Sea(I-book) ((O) 1870(O) )(O) ,(O) and(O) Around(B-book) the(I-book) World(I-book) in(I-book) Eighty(I-book) Days(I-book) ((O) 1873(O) )(O) .(O)"}}
{"id":"87","dataset":"crossner_literature","split":"train","label_list":["person","award","poem","country","writer","organization","event","literary genre","location","book","magazine"],"instance":{"id":"87","words":["With","his","poem","Yo","soy","Joaquín",",","known","in","English","as","I","Am","Joaquin",",","Gonzales","shared","his","new","cosmological","vision","of","the","Chicano",",","who","was","neither","Indian","nor","European",",","neither","Mexican","nor","American",",","but","a","combination","of","all","the","conflicting","identities","."],"labels":["O","O","B-literary genre","B-poem","I-poem","I-poem","O","O","O","O","O","B-poem","I-poem","I-poem","O","B-writer","O","O","O","O","O","O","O","B-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, poem, country, writer, organization, event, literary genre, location, book, magazine and O.\nSentence: With his poem Yo soy Joaquín , known in English as I Am Joaquin , Gonzales shared his new cosmological vision of the Chicano , who was neither Indian nor European , neither Mexican nor American , but a combination of all the conflicting identities .","prompt_labels":"With(O) his(O) poem(B-literary genre) Yo(B-poem) soy(I-poem) Joaquín(I-poem) ,(O) known(O) in(O) English(O) as(O) I(B-poem) Am(I-poem) Joaquin(I-poem) ,(O) Gonzales(B-writer) shared(O) his(O) new(O) cosmological(O) vision(O) of(O) the(O) Chicano(B-poem) ,(O) who(O) was(O) neither(O) Indian(O) nor(O) European(O) ,(O) neither(O) Mexican(O) nor(O) American(O) ,(O) but(O) a(O) combination(O) of(O) all(O) the(O) conflicting(O) identities(O) .(O)"}}
{"id":"88","dataset":"crossner_literature","split":"train","label_list":["person","literary genre","poem","book","magazine","location","award","event","writer","country","organization"],"instance":{"id":"88","words":["In","response","to","what","at","least","appeared","to","be","a","blatant","snub","by","his","own","countrymen",",","the","director","Sidney","Lumet","led","a","successful","campaign","to","have","Kurosawa","receive","an","Oscar","nomination","for","Academy","Award","for","Best","Director","that","year","(","Sydney","Pollack","ultimately","won","the","award","for","directing","Out","of","Africa",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","B-person","O","O","B-award","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, literary genre, poem, book, magazine, location, award, event, writer, country, organization and O.\nSentence: In response to what at least appeared to be a blatant snub by his own countrymen , the director Sidney Lumet led a successful campaign to have Kurosawa receive an Oscar nomination for Academy Award for Best Director that year ( Sydney Pollack ultimately won the award for directing Out of Africa ) .","prompt_labels":"In(O) response(O) to(O) what(O) at(O) least(O) appeared(O) to(O) be(O) a(O) blatant(O) snub(O) by(O) his(O) own(O) countrymen(O) ,(O) the(O) director(O) Sidney(B-person) Lumet(I-person) led(O) a(O) successful(O) campaign(O) to(O) have(O) Kurosawa(B-person) receive(O) an(O) Oscar(B-award) nomination(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) that(O) year(O) ((O) Sydney(B-person) Pollack(I-person) ultimately(O) won(O) the(O) award(O) for(O) directing(O) Out(O) of(O) Africa(O) )(O) .(O)"}}
{"id":"89","dataset":"crossner_literature","split":"train","label_list":["person","literary genre","location","award","poem","event","organization","book","writer","country","magazine"],"instance":{"id":"89","words":["After","a","1995","staging","at","the","La","Jolla","Playhouse",",","he","retained","David","Mamet","to","help","rework","the","book","before","its","relaunch","on","the","Chicago","Goodman","Theatre","mainstage","in","1996","."],"labels":["O","O","O","O","O","O","B-location","I-location","I-location","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, literary genre, location, award, poem, event, organization, book, writer, country, magazine and O.\nSentence: After a 1995 staging at the La Jolla Playhouse , he retained David Mamet to help rework the book before its relaunch on the Chicago Goodman Theatre mainstage in 1996 .","prompt_labels":"After(O) a(O) 1995(O) staging(O) at(O) the(O) La(B-location) Jolla(I-location) Playhouse(I-location) ,(O) he(O) retained(O) David(B-writer) Mamet(I-writer) to(O) help(O) rework(O) the(O) book(O) before(O) its(O) relaunch(O) on(O) the(O) Chicago(B-location) Goodman(I-location) Theatre(I-location) mainstage(O) in(O) 1996(O) .(O)"}}
{"id":"90","dataset":"crossner_literature","split":"train","label_list":["location","literary genre","person","event","book","writer","magazine","country","poem","organization","award"],"instance":{"id":"90","words":["The","period","around","World","War","II","also","saw","the","publication","of","the","time","travel","novel","Lest","Darkness","Fall","by","L.","Sprague","de","Camp",",","in","which","an","American","academic","travels","to","Italy","at","the","time","of","the","Byzantine","invasion","of","the","Ostrogoths","."],"labels":["O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","B-literary genre","I-literary genre","I-literary genre","B-book","I-book","I-book","O","B-writer","I-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, literary genre, person, event, book, writer, magazine, country, poem, organization, award and O.\nSentence: The period around World War II also saw the publication of the time travel novel Lest Darkness Fall by L. Sprague de Camp , in which an American academic travels to Italy at the time of the Byzantine invasion of the Ostrogoths .","prompt_labels":"The(O) period(O) around(O) World(B-event) War(I-event) II(I-event) also(O) saw(O) the(O) publication(O) of(O) the(O) time(B-literary genre) travel(I-literary genre) novel(I-literary genre) Lest(B-book) Darkness(I-book) Fall(I-book) by(O) L.(B-writer) Sprague(I-writer) de(I-writer) Camp(I-writer) ,(O) in(O) which(O) an(O) American(O) academic(O) travels(O) to(O) Italy(B-country) at(O) the(O) time(O) of(O) the(O) Byzantine(O) invasion(O) of(O) the(O) Ostrogoths(O) .(O)"}}
{"id":"91","dataset":"crossner_literature","split":"train","label_list":["magazine","event","writer","book","literary genre","poem","country","award","location","person","organization"],"instance":{"id":"91","words":["His","most","famous","poem","is","The","Airs","of","Palestine","."],"labels":["O","O","O","B-literary genre","O","B-poem","I-poem","I-poem","I-poem","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, event, writer, book, literary genre, poem, country, award, location, person, organization and O.\nSentence: His most famous poem is The Airs of Palestine .","prompt_labels":"His(O) most(O) famous(O) poem(B-literary genre) is(O) The(B-poem) Airs(I-poem) of(I-poem) Palestine(I-poem) .(O)"}}
{"id":"92","dataset":"crossner_literature","split":"train","label_list":["magazine","location","person","writer","award","literary genre","poem","country","event","organization","book"],"instance":{"id":"92","words":["A","tribute","show","to","Wilson",",","organized","by","Coldcut","and","Mixmaster","Morris","and","performed","in","London","as","a","part","of","the","Ether","7","Festival","held","at","the","Queen","Elizabeth","Hall","on","March","18",",","2007",",","also","included","Ken","Campbell",",","Bill","Drummond","and","Alan","Moore","."],"labels":["O","O","O","O","B-writer","O","O","O","B-person","O","B-person","I-person","O","O","O","B-location","O","O","O","O","O","B-event","I-event","I-event","O","O","O","B-location","I-location","I-location","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, person, writer, award, literary genre, poem, country, event, organization, book and O.\nSentence: A tribute show to Wilson , organized by Coldcut and Mixmaster Morris and performed in London as a part of the Ether 7 Festival held at the Queen Elizabeth Hall on March 18 , 2007 , also included Ken Campbell , Bill Drummond and Alan Moore .","prompt_labels":"A(O) tribute(O) show(O) to(O) Wilson(B-writer) ,(O) organized(O) by(O) Coldcut(B-person) and(O) Mixmaster(B-person) Morris(I-person) and(O) performed(O) in(O) London(B-location) as(O) a(O) part(O) of(O) the(O) Ether(B-event) 7(I-event) Festival(I-event) held(O) at(O) the(O) Queen(B-location) Elizabeth(I-location) Hall(I-location) on(O) March(O) 18(O) ,(O) 2007(O) ,(O) also(O) included(O) Ken(B-writer) Campbell(I-writer) ,(O) Bill(B-writer) Drummond(I-writer) and(O) Alan(B-writer) Moore(I-writer) .(O)"}}
{"id":"93","dataset":"crossner_literature","split":"train","label_list":["person","location","book","writer","magazine","event","country","literary genre","poem","award","organization"],"instance":{"id":"93","words":["Former","House","Speaker","Newt","Gingrich","and","William","R.","Forstchen","have","written","a","novel",",","1945",",","in","which","the","US","defeated","Empire","of","Japan","but","not","Nazi","Germany","in","World","War","II",",","resulting","in","a","Cold","War","with","Germany","rather","than","the","Soviet","Union","."],"labels":["O","B-organization","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","O","B-literary genre","O","B-book","O","O","O","O","B-country","O","B-country","I-country","I-country","O","O","B-country","I-country","O","B-event","I-event","I-event","O","O","O","O","B-event","I-event","O","B-country","O","O","O","B-country","I-country","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, book, writer, magazine, event, country, literary genre, poem, award, organization and O.\nSentence: Former House Speaker Newt Gingrich and William R. Forstchen have written a novel , 1945 , in which the US defeated Empire of Japan but not Nazi Germany in World War II , resulting in a Cold War with Germany rather than the Soviet Union .","prompt_labels":"Former(O) House(B-organization) Speaker(O) Newt(B-writer) Gingrich(I-writer) and(O) William(B-writer) R.(I-writer) Forstchen(I-writer) have(O) written(O) a(O) novel(B-literary genre) ,(O) 1945(B-book) ,(O) in(O) which(O) the(O) US(B-country) defeated(O) Empire(B-country) of(I-country) Japan(I-country) but(O) not(O) Nazi(B-country) Germany(I-country) in(O) World(B-event) War(I-event) II(I-event) ,(O) resulting(O) in(O) a(O) Cold(B-event) War(I-event) with(O) Germany(B-country) rather(O) than(O) the(O) Soviet(B-country) Union(I-country) .(O)"}}
{"id":"94","dataset":"crossner_literature","split":"train","label_list":["location","magazine","literary genre","person","writer","organization","poem","country","book","event","award"],"instance":{"id":"94","words":["With","the","publications","of","Sense","and","Sensibility","(","1811",")",",","Pride","and","Prejudice","(","1813",")",",","Mansfield","Park","(","1814",")","and","Emma","(","1816",")",",","she","achieved","success","as","a","published","writer","."],"labels":["O","O","O","O","B-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O","B-book","I-book","O","O","O","O","B-book","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, magazine, literary genre, person, writer, organization, poem, country, book, event, award and O.\nSentence: With the publications of Sense and Sensibility ( 1811 ) , Pride and Prejudice ( 1813 ) , Mansfield Park ( 1814 ) and Emma ( 1816 ) , she achieved success as a published writer .","prompt_labels":"With(O) the(O) publications(O) of(O) Sense(B-book) and(I-book) Sensibility(I-book) ((O) 1811(O) )(O) ,(O) Pride(B-book) and(I-book) Prejudice(I-book) ((O) 1813(O) )(O) ,(O) Mansfield(B-book) Park(I-book) ((O) 1814(O) )(O) and(O) Emma(B-book) ((O) 1816(O) )(O) ,(O) she(O) achieved(O) success(O) as(O) a(O) published(O) writer(O) .(O)"}}
{"id":"95","dataset":"crossner_literature","split":"train","label_list":["country","location","person","magazine","event","book","award","organization","poem","writer","literary genre"],"instance":{"id":"95","words":["For","example",",","Russ","criticized","Ursula","K.","Le","Guin","'","s","1969","The","Left","Hand","of","Darkness",",","which","won","both","the","1969","Nebula","Award","for","Best","Novel","and","1970","Hugo","Award","for","Best","Novel","awards","for","best","science","fiction","novel",",","arguing","that","gender","discriminations","that","permeated","science","fiction","by","men","showed","up","just","as","frequently","in","science","fiction","by","women","."],"labels":["O","O","O","B-writer","O","B-writer","I-writer","I-writer","I-writer","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","B-literary genre","I-literary genre","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, person, magazine, event, book, award, organization, poem, writer, literary genre and O.\nSentence: For example , Russ criticized Ursula K. Le Guin ' s 1969 The Left Hand of Darkness , which won both the 1969 Nebula Award for Best Novel and 1970 Hugo Award for Best Novel awards for best science fiction novel , arguing that gender discriminations that permeated science fiction by men showed up just as frequently in science fiction by women .","prompt_labels":"For(O) example(O) ,(O) Russ(B-writer) criticized(O) Ursula(B-writer) K.(I-writer) Le(I-writer) Guin(I-writer) '(O) s(O) 1969(O) The(B-book) Left(I-book) Hand(I-book) of(I-book) Darkness(I-book) ,(O) which(O) won(O) both(O) the(O) 1969(O) Nebula(B-award) Award(I-award) for(I-award) Best(I-award) Novel(I-award) and(O) 1970(O) Hugo(B-award) Award(I-award) for(I-award) Best(I-award) Novel(I-award) awards(O) for(O) best(O) science(B-literary genre) fiction(I-literary genre) novel(O) ,(O) arguing(O) that(O) gender(O) discriminations(O) that(O) permeated(O) science(B-literary genre) fiction(I-literary genre) by(O) men(O) showed(O) up(O) just(O) as(O) frequently(O) in(O) science(B-literary genre) fiction(I-literary genre) by(O) women(O) .(O)"}}
{"id":"96","dataset":"crossner_literature","split":"train","label_list":["event","poem","person","literary genre","writer","organization","country","location","award","magazine","book"],"instance":{"id":"96","words":["Caterina","di","Giacomo","di","Benincasa","was","born","on","25","March","1347","(","shortly","before","the","Black","Death","ravaged","Europe",")","in","Siena",",","Republic","of","Siena","(","today","Italy",")",",","to","Lapa","Piagenti",",","the","daughter","of","a","local","poet",",","and","Giacomo","di","Benincasa",",","a","cloth","dyer","who","ran","his","enterprise","with","the","help","of","his","sons","."],"labels":["B-person","I-person","B-person","I-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","B-location","O","B-country","I-country","I-country","O","O","B-country","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, poem, person, literary genre, writer, organization, country, location, award, magazine, book and O.\nSentence: Caterina di Giacomo di Benincasa was born on 25 March 1347 ( shortly before the Black Death ravaged Europe ) in Siena , Republic of Siena ( today Italy ) , to Lapa Piagenti , the daughter of a local poet , and Giacomo di Benincasa , a cloth dyer who ran his enterprise with the help of his sons .","prompt_labels":"Caterina(B-person) di(I-person) Giacomo(B-person) di(I-person) Benincasa(I-person) was(O) born(O) on(O) 25(O) March(O) 1347(O) ((O) shortly(O) before(O) the(O) Black(O) Death(O) ravaged(O) Europe(B-location) )(O) in(O) Siena(B-location) ,(O) Republic(B-country) of(I-country) Siena(I-country) ((O) today(O) Italy(B-country) )(O) ,(O) to(O) Lapa(B-person) Piagenti(I-person) ,(O) the(O) daughter(O) of(O) a(O) local(O) poet(O) ,(O) and(O) Giacomo(B-person) di(I-person) Benincasa(I-person) ,(O) a(O) cloth(O) dyer(O) who(O) ran(O) his(O) enterprise(O) with(O) the(O) help(O) of(O) his(O) sons(O) .(O)"}}
{"id":"97","dataset":"crossner_literature","split":"train","label_list":["location","organization","literary genre","event","poem","person","country","writer","award","magazine","book"],"instance":{"id":"97","words":["These","include","the","Universal","Natural","History","and","Theory","of","the","Heavens","(","1755",")",",","the","Critique","of","Practical","Reason","(","1788",")",",","the","Metaphysics","of","Morals","(","1797",")",",","the","Critique","of","Judgment","(","1790",")",",","which","looks","at","aesthetics","and","teleology",",","and","Religion","within","the","Bounds","of","Bare","Reason","(","1793",")","."],"labels":["O","O","O","B-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, literary genre, event, poem, person, country, writer, award, magazine, book and O.\nSentence: These include the Universal Natural History and Theory of the Heavens ( 1755 ) , the Critique of Practical Reason ( 1788 ) , the Metaphysics of Morals ( 1797 ) , the Critique of Judgment ( 1790 ) , which looks at aesthetics and teleology , and Religion within the Bounds of Bare Reason ( 1793 ) .","prompt_labels":"These(O) include(O) the(O) Universal(B-book) Natural(I-book) History(I-book) and(I-book) Theory(I-book) of(I-book) the(I-book) Heavens(I-book) ((O) 1755(O) )(O) ,(O) the(O) Critique(B-book) of(I-book) Practical(I-book) Reason(I-book) ((O) 1788(O) )(O) ,(O) the(O) Metaphysics(B-book) of(I-book) Morals(I-book) ((O) 1797(O) )(O) ,(O) the(O) Critique(B-book) of(I-book) Judgment(I-book) ((O) 1790(O) )(O) ,(O) which(O) looks(O) at(O) aesthetics(O) and(O) teleology(O) ,(O) and(O) Religion(B-book) within(I-book) the(I-book) Bounds(I-book) of(I-book) Bare(I-book) Reason(I-book) ((O) 1793(O) )(O) .(O)"}}
{"id":"98","dataset":"crossner_literature","split":"train","label_list":["location","event","magazine","person","award","literary genre","organization","country","writer","book","poem"],"instance":{"id":"98","words":["His","interest","in","space",",","however",",","was","his","primary","focus",",","especially","after","reading","science","fiction","stories","by","writers","such","as","H.","G.","Wells","and","Edgar","Rice","Burroughs",",","which","stirred","his","imagination","about","life","on","other","planets","such","as","Mars","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","I-literary genre","O","O","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, magazine, person, award, literary genre, organization, country, writer, book, poem and O.\nSentence: His interest in space , however , was his primary focus , especially after reading science fiction stories by writers such as H. G. Wells and Edgar Rice Burroughs , which stirred his imagination about life on other planets such as Mars .","prompt_labels":"His(O) interest(O) in(O) space(O) ,(O) however(O) ,(O) was(O) his(O) primary(O) focus(O) ,(O) especially(O) after(O) reading(O) science(B-literary genre) fiction(I-literary genre) stories(I-literary genre) by(O) writers(O) such(O) as(O) H.(B-writer) G.(I-writer) Wells(I-writer) and(O) Edgar(B-writer) Rice(I-writer) Burroughs(I-writer) ,(O) which(O) stirred(O) his(O) imagination(O) about(O) life(O) on(O) other(O) planets(O) such(O) as(O) Mars(O) .(O)"}}
{"id":"99","dataset":"crossner_literature","split":"train","label_list":["literary genre","person","organization","writer","magazine","book","event","country","award","poem","location"],"instance":{"id":"99","words":["Baron","Cohen","was","educated","at","The","Haberdashers","'","Aske","'s","Boys","'","School",",","an","independent","school","in","Elstree",",","Hertfordshire",",","While","a","member","of","the","Cambridge","University","Amateur","Dramatic","Club",",","Baron","Cohen","performed","in","plays","such","as","Fiddler","on","the","Roof","and","Cyrano","de","Bergerac",",","as","well","as","in","Habonim","Dror","Jewish","theatre","."],"labels":["B-person","I-person","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","B-location","O","B-location","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, person, organization, writer, magazine, book, event, country, award, poem, location and O.\nSentence: Baron Cohen was educated at The Haberdashers ' Aske 's Boys ' School , an independent school in Elstree , Hertfordshire , While a member of the Cambridge University Amateur Dramatic Club , Baron Cohen performed in plays such as Fiddler on the Roof and Cyrano de Bergerac , as well as in Habonim Dror Jewish theatre .","prompt_labels":"Baron(B-person) Cohen(I-person) was(O) educated(O) at(O) The(B-organization) Haberdashers(I-organization) '(I-organization) Aske(I-organization) 's(I-organization) Boys(I-organization) '(I-organization) School(I-organization) ,(O) an(O) independent(O) school(O) in(O) Elstree(B-location) ,(O) Hertfordshire(B-location) ,(O) While(O) a(O) member(O) of(O) the(O) Cambridge(B-organization) University(I-organization) Amateur(I-organization) Dramatic(I-organization) Club(I-organization) ,(O) Baron(B-person) Cohen(I-person) performed(O) in(O) plays(O) such(O) as(O) Fiddler(O) on(O) the(O) Roof(O) and(O) Cyrano(O) de(O) Bergerac(O) ,(O) as(O) well(O) as(O) in(O) Habonim(B-location) Dror(I-location) Jewish(I-location) theatre(I-location) .(O)"}}
{"id":"0","dataset":"crossner_music","split":"train","label_list":["event","song","music genre","location","country","award","organization","band","musical instrument","person","album","musical artist"],"instance":{"id":"0","words":["In","2003",",","the","Stade","de","France","was","the","primary","site","of","the","2003","World","Championships","in","Athletics","."],"labels":["O","O","O","O","B-location","I-location","I-location","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, song, music genre, location, country, award, organization, band, musical instrument, person, album, musical artist and O.\nSentence: In 2003 , the Stade de France was the primary site of the 2003 World Championships in Athletics .","prompt_labels":"In(O) 2003(O) ,(O) the(O) Stade(B-location) de(I-location) France(I-location) was(O) the(O) primary(O) site(O) of(O) the(O) 2003(B-event) World(I-event) Championships(I-event) in(I-event) Athletics(I-event) .(O)"}}
{"id":"1","dataset":"crossner_music","split":"train","label_list":["person","location","music genre","album","award","musical artist","organization","country","band","song","musical instrument","event"],"instance":{"id":"1","words":["In","addition","to","relentless","touring","in","the","U.S.","and","Canada",",","PUSA","made","multiple","tours","of","Europe",",","Australia",",","New","Zealand","and","Japan","."],"labels":["O","O","O","O","O","O","O","B-country","O","B-country","O","B-band","O","O","O","O","B-location","O","B-country","O","B-country","I-country","O","B-country","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, music genre, album, award, musical artist, organization, country, band, song, musical instrument, event and O.\nSentence: In addition to relentless touring in the U.S. and Canada , PUSA made multiple tours of Europe , Australia , New Zealand and Japan .","prompt_labels":"In(O) addition(O) to(O) relentless(O) touring(O) in(O) the(O) U.S.(B-country) and(O) Canada(B-country) ,(O) PUSA(B-band) made(O) multiple(O) tours(O) of(O) Europe(B-location) ,(O) Australia(B-country) ,(O) New(B-country) Zealand(I-country) and(O) Japan(B-country) .(O)"}}
{"id":"2","dataset":"crossner_music","split":"train","label_list":["event","musical instrument","award","band","location","country","person","song","music genre","organization","album","musical artist"],"instance":{"id":"2","words":["Barney","Bubbles","directed","several","videos",",","including","the","Specials","'","Ghost","Town",",","Squeeze","'","s","Is","That","Love","and","Tempted",",","Elvis","Costello","'","s","Clubland","and","New","Lace","Sleeves",",","and","Fun","Boy","Three","'","s","The","Lunatics","(","Have","Taken","Over","the","Asylum",")","."],"labels":["B-person","I-person","O","O","O","O","O","B-band","I-band","O","B-band","I-band","O","B-band","O","O","B-song","I-song","I-song","O","B-song","O","B-musical artist","I-musical artist","O","O","B-song","O","B-song","I-song","I-song","O","O","B-band","I-band","I-band","O","O","B-song","I-song","I-song","I-song","I-song","I-song","I-song","I-song","I-song","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical instrument, award, band, location, country, person, song, music genre, organization, album, musical artist and O.\nSentence: Barney Bubbles directed several videos , including the Specials ' Ghost Town , Squeeze ' s Is That Love and Tempted , Elvis Costello ' s Clubland and New Lace Sleeves , and Fun Boy Three ' s The Lunatics ( Have Taken Over the Asylum ) .","prompt_labels":"Barney(B-person) Bubbles(I-person) directed(O) several(O) videos(O) ,(O) including(O) the(B-band) Specials(I-band) '(O) Ghost(B-band) Town(I-band) ,(O) Squeeze(B-band) '(O) s(O) Is(B-song) That(I-song) Love(I-song) and(O) Tempted(B-song) ,(O) Elvis(B-musical artist) Costello(I-musical artist) '(O) s(O) Clubland(B-song) and(O) New(B-song) Lace(I-song) Sleeves(I-song) ,(O) and(O) Fun(B-band) Boy(I-band) Three(I-band) '(O) s(O) The(B-song) Lunatics(I-song) ((I-song) Have(I-song) Taken(I-song) Over(I-song) the(I-song) Asylum(I-song) )(I-song) .(O)"}}
{"id":"3","dataset":"crossner_music","split":"train","label_list":["music genre","country","award","musical instrument","location","musical artist","album","band","song","organization","event","person"],"instance":{"id":"3","words":["Since","then","there","has","been","a","renaissance","in","Sacred","Harp","singing",",","with","annual","conventions","popping","up","in","United","States","and","in","a","number","of","European","countries","recently",",","including","the","United","Kingdom",",","Germany",",","Ireland","and","Poland",",","as","well","as","in","Australia","."],"labels":["O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","B-country","O","B-country","O","B-country","O","O","O","O","O","B-country","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, country, award, musical instrument, location, musical artist, album, band, song, organization, event, person and O.\nSentence: Since then there has been a renaissance in Sacred Harp singing , with annual conventions popping up in United States and in a number of European countries recently , including the United Kingdom , Germany , Ireland and Poland , as well as in Australia .","prompt_labels":"Since(O) then(O) there(O) has(O) been(O) a(O) renaissance(O) in(O) Sacred(B-music genre) Harp(I-music genre) singing(O) ,(O) with(O) annual(O) conventions(O) popping(O) up(O) in(O) United(B-country) States(I-country) and(O) in(O) a(O) number(O) of(O) European(O) countries(O) recently(O) ,(O) including(O) the(O) United(B-country) Kingdom(I-country) ,(O) Germany(B-country) ,(O) Ireland(B-country) and(O) Poland(B-country) ,(O) as(O) well(O) as(O) in(O) Australia(B-country) .(O)"}}
{"id":"4","dataset":"crossner_music","split":"train","label_list":["person","organization","country","location","band","music genre","album","event","award","song","musical instrument","musical artist"],"instance":{"id":"4","words":["Three","of","the","labels","rejected","her",",","saying","that","audiences","wanted","pop","bands","such","as","the","Backstreet","Boys","and","the","Spice","Girls",",","and","there","wasn","'t","going","to","be","another","Madonna",",","another","Debbie","Gibson",",","or","another","Tiffany","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","O","O","B-band","I-band","O","O","O","O","O","O","O","O","O","B-musical artist","O","O","B-musical artist","I-musical artist","O","O","O","B-musical artist","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, country, location, band, music genre, album, event, award, song, musical instrument, musical artist and O.\nSentence: Three of the labels rejected her , saying that audiences wanted pop bands such as the Backstreet Boys and the Spice Girls , and there wasn 't going to be another Madonna , another Debbie Gibson , or another Tiffany .","prompt_labels":"Three(O) of(O) the(O) labels(O) rejected(O) her(O) ,(O) saying(O) that(O) audiences(O) wanted(O) pop(O) bands(O) such(O) as(O) the(O) Backstreet(B-band) Boys(I-band) and(O) the(O) Spice(B-band) Girls(I-band) ,(O) and(O) there(O) wasn(O) 't(O) going(O) to(O) be(O) another(O) Madonna(B-musical artist) ,(O) another(O) Debbie(B-musical artist) Gibson(I-musical artist) ,(O) or(O) another(O) Tiffany(B-musical artist) .(O)"}}
{"id":"5","dataset":"crossner_music","split":"train","label_list":["musical artist","band","music genre","person","album","location","musical instrument","song","organization","country","event","award"],"instance":{"id":"5","words":["Notable","Sun","Ra","albums","from","the","1950s","include","Sun","Ra","Visits","Planet","Earth",",","Interstellar","Low","Ways",",","Super-Sonic","Jazz",",","We","Travel","the","Space","Ways",",","The","Nubians","of","Plutonia","and","Jazz","In","Silhouette","."],"labels":["O","B-musical artist","I-musical artist","O","O","O","O","O","B-album","I-album","I-album","I-album","I-album","O","B-album","I-album","I-album","O","B-album","I-album","O","B-album","I-album","I-album","I-album","I-album","O","B-album","I-album","I-album","I-album","O","B-album","I-album","I-album","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, band, music genre, person, album, location, musical instrument, song, organization, country, event, award and O.\nSentence: Notable Sun Ra albums from the 1950s include Sun Ra Visits Planet Earth , Interstellar Low Ways , Super-Sonic Jazz , We Travel the Space Ways , The Nubians of Plutonia and Jazz In Silhouette .","prompt_labels":"Notable(O) Sun(B-musical artist) Ra(I-musical artist) albums(O) from(O) the(O) 1950s(O) include(O) Sun(B-album) Ra(I-album) Visits(I-album) Planet(I-album) Earth(I-album) ,(O) Interstellar(B-album) Low(I-album) Ways(I-album) ,(O) Super-Sonic(B-album) Jazz(I-album) ,(O) We(B-album) Travel(I-album) the(I-album) Space(I-album) Ways(I-album) ,(O) The(B-album) Nubians(I-album) of(I-album) Plutonia(I-album) and(O) Jazz(B-album) In(I-album) Silhouette(I-album) .(O)"}}
{"id":"6","dataset":"crossner_music","split":"train","label_list":["musical instrument","country","band","location","song","award","organization","album","musical artist","music genre","event","person"],"instance":{"id":"6","words":["Stevens","'","albums","Tea","for","the","Tillerman","(","1970",")","and","Teaser","and","the","Firecat","(","1971",")","were","certified","triple","platinum","in","the","US","by","the","Recording","Industry","Association","of","America","..","BBC","News","."],"labels":["B-musical artist","O","O","B-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","O","O","O","O","B-country","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, country, band, location, song, award, organization, album, musical artist, music genre, event, person and O.\nSentence: Stevens ' albums Tea for the Tillerman ( 1970 ) and Teaser and the Firecat ( 1971 ) were certified triple platinum in the US by the Recording Industry Association of America .. BBC News .","prompt_labels":"Stevens(B-musical artist) '(O) albums(O) Tea(B-album) for(I-album) the(I-album) Tillerman(I-album) ((O) 1970(O) )(O) and(O) Teaser(B-album) and(I-album) the(I-album) Firecat(I-album) ((O) 1971(O) )(O) were(O) certified(O) triple(O) platinum(O) in(O) the(O) US(B-country) by(O) the(O) Recording(B-organization) Industry(I-organization) Association(I-organization) of(I-organization) America(I-organization) ..(O) BBC(B-organization) News(I-organization) .(O)"}}
{"id":"7","dataset":"crossner_music","split":"train","label_list":["award","country","album","organization","song","musical artist","person","musical instrument","band","location","event","music genre"],"instance":{"id":"7","words":["He","has","won","Top","New","Male","Vocalist","from","Billboard","in","1992","and","from","Academy","of","Country","Music","in","1993",",","and","Vocal","Event","of","the","Year","from","the","Country","Music","Association","in","2007","."],"labels":["O","O","O","B-award","I-award","I-award","I-award","O","B-organization","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","B-event","I-event","I-event","I-event","I-event","O","O","B-organization","I-organization","I-organization","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, album, organization, song, musical artist, person, musical instrument, band, location, event, music genre and O.\nSentence: He has won Top New Male Vocalist from Billboard in 1992 and from Academy of Country Music in 1993 , and Vocal Event of the Year from the Country Music Association in 2007 .","prompt_labels":"He(O) has(O) won(O) Top(B-award) New(I-award) Male(I-award) Vocalist(I-award) from(O) Billboard(B-organization) in(O) 1992(O) and(O) from(O) Academy(B-organization) of(I-organization) Country(I-organization) Music(I-organization) in(O) 1993(O) ,(O) and(O) Vocal(B-event) Event(I-event) of(I-event) the(I-event) Year(I-event) from(O) the(O) Country(B-organization) Music(I-organization) Association(I-organization) in(O) 2007(O) .(O)"}}
{"id":"8","dataset":"crossner_music","split":"train","label_list":["song","organization","musical artist","event","band","music genre","musical instrument","person","location","country","award","album"],"instance":{"id":"8","words":["In","1983",",","the","lineup","of","Verni",",","Skates",",","Ellsworth",",","and","Gustafson","released","the","Power","in","Black","demo",",","a","recording","that","made","as","much","impact","in","the","underground","tape","trading","circuit","as","demos","by","up-and-coming","Bay","Area","thrash","metal","bands","such","as","Exodus","and","Testament","."],"labels":["O","O","O","O","O","O","B-musical artist","O","B-musical artist","O","B-musical artist","O","O","B-musical artist","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","B-music genre","I-music genre","O","O","O","B-band","O","B-band","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, organization, musical artist, event, band, music genre, musical instrument, person, location, country, award, album and O.\nSentence: In 1983 , the lineup of Verni , Skates , Ellsworth , and Gustafson released the Power in Black demo , a recording that made as much impact in the underground tape trading circuit as demos by up-and-coming Bay Area thrash metal bands such as Exodus and Testament .","prompt_labels":"In(O) 1983(O) ,(O) the(O) lineup(O) of(O) Verni(B-musical artist) ,(O) Skates(B-musical artist) ,(O) Ellsworth(B-musical artist) ,(O) and(O) Gustafson(B-musical artist) released(O) the(O) Power(B-album) in(I-album) Black(I-album) demo(O) ,(O) a(O) recording(O) that(O) made(O) as(O) much(O) impact(O) in(O) the(O) underground(O) tape(O) trading(O) circuit(O) as(O) demos(O) by(O) up-and-coming(O) Bay(B-location) Area(I-location) thrash(B-music genre) metal(I-music genre) bands(O) such(O) as(O) Exodus(B-band) and(O) Testament(B-band) .(O)"}}
{"id":"9","dataset":"crossner_music","split":"train","label_list":["song","award","person","country","event","musical artist","organization","location","music genre","musical instrument","band","album"],"instance":{"id":"9","words":["Their","style","has","been","described","as","Rock","and","roll","with","a","renegade","stance",",","what","in","later","years","would","be","dubbed","'","Punk","rock","'","."],"labels":["O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, award, person, country, event, musical artist, organization, location, music genre, musical instrument, band, album and O.\nSentence: Their style has been described as Rock and roll with a renegade stance , what in later years would be dubbed ' Punk rock ' .","prompt_labels":"Their(O) style(O) has(O) been(O) described(O) as(O) Rock(B-music genre) and(I-music genre) roll(I-music genre) with(O) a(O) renegade(O) stance(O) ,(O) what(O) in(O) later(O) years(O) would(O) be(O) dubbed(O) '(O) Punk(B-music genre) rock(I-music genre) '(O) .(O)"}}
{"id":"10","dataset":"crossner_music","split":"train","label_list":["song","location","music genre","country","musical instrument","award","person","album","event","organization","musical artist","band"],"instance":{"id":"10","words":["He","also","appeared","with","RBX",",","Nas","and","KRS-One","on","East","Coast","Killer",",","West","Coast","Killer","from","Dr.","Dre","'s","Dr.","Dre","Presents","the","Aftermath","album",",","and","contributed","to","an","album","entitled","The","Psycho","Realm","with","Psycho","Realm","."],"labels":["O","O","O","O","B-musical artist","O","B-musical artist","O","B-musical artist","O","B-song","I-song","I-song","O","B-song","I-song","I-song","O","B-musical artist","I-musical artist","O","B-album","I-album","I-album","I-album","I-album","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","B-band","I-band","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, location, music genre, country, musical instrument, award, person, album, event, organization, musical artist, band and O.\nSentence: He also appeared with RBX , Nas and KRS-One on East Coast Killer , West Coast Killer from Dr. Dre 's Dr. Dre Presents the Aftermath album , and contributed to an album entitled The Psycho Realm with Psycho Realm .","prompt_labels":"He(O) also(O) appeared(O) with(O) RBX(B-musical artist) ,(O) Nas(B-musical artist) and(O) KRS-One(B-musical artist) on(O) East(B-song) Coast(I-song) Killer(I-song) ,(O) West(B-song) Coast(I-song) Killer(I-song) from(O) Dr.(B-musical artist) Dre(I-musical artist) 's(O) Dr.(B-album) Dre(I-album) Presents(I-album) the(I-album) Aftermath(I-album) album(O) ,(O) and(O) contributed(O) to(O) an(O) album(O) entitled(O) The(B-album) Psycho(I-album) Realm(I-album) with(O) Psycho(B-band) Realm(I-band) .(O)"}}
{"id":"11","dataset":"crossner_music","split":"train","label_list":["album","band","musical instrument","organization","musical artist","song","award","country","music genre","location","event","person"],"instance":{"id":"11","words":["On","9","June","1892","the","Paris","Opéra-Comique","staged","Les","Troyens","à","Carthage","(","in","the","Théâtre","de","la","Ville","as","its","premiere",")","and","witnessed","a","triumphant","debut","for","the","17-year-old","Marie","Delna","as","Didon",",","with","Stéphane","Lafarge","as","Énée",",","conducted","by","Jules","Danbé",";","these","staged","performances","of","Part","2","continued","into","the","next","year","."],"labels":["O","O","O","O","O","B-location","B-organization","O","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","B-person","I-person","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, band, musical instrument, organization, musical artist, song, award, country, music genre, location, event, person and O.\nSentence: On 9 June 1892 the Paris Opéra-Comique staged Les Troyens à Carthage ( in the Théâtre de la Ville as its premiere ) and witnessed a triumphant debut for the 17-year-old Marie Delna as Didon , with Stéphane Lafarge as Énée , conducted by Jules Danbé ; these staged performances of Part 2 continued into the next year .","prompt_labels":"On(O) 9(O) June(O) 1892(O) the(O) Paris(B-location) Opéra-Comique(B-organization) staged(O) Les(O) Troyens(O) à(O) Carthage(O) ((O) in(O) the(O) Théâtre(B-location) de(I-location) la(I-location) Ville(I-location) as(O) its(O) premiere(O) )(O) and(O) witnessed(O) a(O) triumphant(O) debut(O) for(O) the(O) 17-year-old(O) Marie(B-musical artist) Delna(I-musical artist) as(O) Didon(O) ,(O) with(O) Stéphane(B-person) Lafarge(I-person) as(O) Énée(O) ,(O) conducted(O) by(O) Jules(B-musical artist) Danbé(I-musical artist) ;(O) these(O) staged(O) performances(O) of(O) Part(O) 2(O) continued(O) into(O) the(O) next(O) year(O) .(O)"}}
{"id":"12","dataset":"crossner_music","split":"train","label_list":["organization","person","country","album","song","band","location","music genre","musical instrument","musical artist","event","award"],"instance":{"id":"12","words":["As","a","group",",","the","Spice","Girls","have","received","a","number","of","notable","awards","including","five","Brit","Awards",",","three","American","Music","Awards",",","three","MTV","Europe","Music","Awards",",","one","MTV","Video","Music","Award","and","three","World","Music","Awards","."],"labels":["O","O","O","O","O","B-band","I-band","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, country, album, song, band, location, music genre, musical instrument, musical artist, event, award and O.\nSentence: As a group , the Spice Girls have received a number of notable awards including five Brit Awards , three American Music Awards , three MTV Europe Music Awards , one MTV Video Music Award and three World Music Awards .","prompt_labels":"As(O) a(O) group(O) ,(O) the(O) Spice(B-band) Girls(I-band) have(O) received(O) a(O) number(O) of(O) notable(O) awards(O) including(O) five(O) Brit(B-award) Awards(I-award) ,(O) three(O) American(B-award) Music(I-award) Awards(I-award) ,(O) three(O) MTV(B-award) Europe(I-award) Music(I-award) Awards(I-award) ,(O) one(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) and(O) three(O) World(B-award) Music(I-award) Awards(I-award) .(O)"}}
{"id":"13","dataset":"crossner_music","split":"train","label_list":["musical instrument","country","music genre","musical artist","person","award","song","event","album","band","organization","location"],"instance":{"id":"13","words":["The","album","experimented","with","a","diverse","number","of","genres",",","including","contemporary","R","&","B",",","deep","house",",","Swing","music",",","Hip","hop","music",",","Rock","music",",","and","Pop","music",",","with","Billboard","describing","each","as","being","delivered","with","consummate","skill","and","passion","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, country, music genre, musical artist, person, award, song, event, album, band, organization, location and O.\nSentence: The album experimented with a diverse number of genres , including contemporary R & B , deep house , Swing music , Hip hop music , Rock music , and Pop music , with Billboard describing each as being delivered with consummate skill and passion .","prompt_labels":"The(O) album(O) experimented(O) with(O) a(O) diverse(O) number(O) of(O) genres(O) ,(O) including(O) contemporary(B-music genre) R(I-music genre) &(I-music genre) B(I-music genre) ,(O) deep(B-music genre) house(I-music genre) ,(O) Swing(B-music genre) music(I-music genre) ,(O) Hip(B-music genre) hop(I-music genre) music(I-music genre) ,(O) Rock(B-music genre) music(I-music genre) ,(O) and(O) Pop(B-music genre) music(I-music genre) ,(O) with(O) Billboard(O) describing(O) each(O) as(O) being(O) delivered(O) with(O) consummate(O) skill(O) and(O) passion(O) .(O)"}}
{"id":"14","dataset":"crossner_music","split":"train","label_list":["event","location","album","musical instrument","musical artist","person","band","award","music genre","country","song","organization"],"instance":{"id":"14","words":["Chuck","Burgi","(","1991-1992",",","1992-1995",",","1996-1997",")",",","John","Miceli","(","1992",",","1995",")",",","John","O","'Reilly","(","1995-1996",")","and","Bobby","Rondinelli","(","1997-2004",")","."],"labels":["B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","B-musical artist","I-musical artist","I-musical artist","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, album, musical instrument, musical artist, person, band, award, music genre, country, song, organization and O.\nSentence: Chuck Burgi ( 1991-1992 , 1992-1995 , 1996-1997 ) , John Miceli ( 1992 , 1995 ) , John O 'Reilly ( 1995-1996 ) and Bobby Rondinelli ( 1997-2004 ) .","prompt_labels":"Chuck(B-musical artist) Burgi(I-musical artist) ((O) 1991-1992(O) ,(O) 1992-1995(O) ,(O) 1996-1997(O) )(O) ,(O) John(B-musical artist) Miceli(I-musical artist) ((O) 1992(O) ,(O) 1995(O) )(O) ,(O) John(B-musical artist) O(I-musical artist) 'Reilly(I-musical artist) ((O) 1995-1996(O) )(O) and(O) Bobby(B-musical artist) Rondinelli(I-musical artist) ((O) 1997-2004(O) )(O) .(O)"}}
{"id":"15","dataset":"crossner_music","split":"train","label_list":["music genre","award","musical instrument","country","event","band","album","song","person","musical artist","location","organization"],"instance":{"id":"15","words":["In","addition","to","Lady","Antebellum",",","groups","such","as","Herrick",",","The","Quebe","Sisters","Band",",","Little","Big","Town",",","The","Band","Perry",",","Gloriana",",","Thompson","Square",",","Eli","Young","Band",",","Zac","Brown","Band","and","British","duo","The","Shires","have","emerged","to","occupy","a","large","portion","of","the","new","country","artists","in","the","popular","scene","along","with","solo","singers","Kacey","Musgraves","and","Miranda","Lambert","."],"labels":["O","O","O","B-band","I-band","O","O","O","O","B-band","O","B-band","I-band","I-band","I-band","O","B-band","I-band","I-band","O","B-band","I-band","I-band","O","B-band","O","B-band","I-band","O","B-band","I-band","I-band","O","B-band","I-band","I-band","O","O","O","B-band","I-band","O","O","O","O","O","O","O","O","O","O","B-music genre","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, award, musical instrument, country, event, band, album, song, person, musical artist, location, organization and O.\nSentence: In addition to Lady Antebellum , groups such as Herrick , The Quebe Sisters Band , Little Big Town , The Band Perry , Gloriana , Thompson Square , Eli Young Band , Zac Brown Band and British duo The Shires have emerged to occupy a large portion of the new country artists in the popular scene along with solo singers Kacey Musgraves and Miranda Lambert .","prompt_labels":"In(O) addition(O) to(O) Lady(B-band) Antebellum(I-band) ,(O) groups(O) such(O) as(O) Herrick(B-band) ,(O) The(B-band) Quebe(I-band) Sisters(I-band) Band(I-band) ,(O) Little(B-band) Big(I-band) Town(I-band) ,(O) The(B-band) Band(I-band) Perry(I-band) ,(O) Gloriana(B-band) ,(O) Thompson(B-band) Square(I-band) ,(O) Eli(B-band) Young(I-band) Band(I-band) ,(O) Zac(B-band) Brown(I-band) Band(I-band) and(O) British(O) duo(O) The(B-band) Shires(I-band) have(O) emerged(O) to(O) occupy(O) a(O) large(O) portion(O) of(O) the(O) new(O) country(B-music genre) artists(O) in(O) the(O) popular(O) scene(O) along(O) with(O) solo(O) singers(O) Kacey(B-musical artist) Musgraves(I-musical artist) and(O) Miranda(B-musical artist) Lambert(I-musical artist) .(O)"}}
{"id":"16","dataset":"crossner_music","split":"train","label_list":["song","event","award","country","location","band","music genre","person","musical artist","album","musical instrument","organization"],"instance":{"id":"16","words":["Two","of","his","most","popular","recordings","were","Layla",",","recorded","with","Derek","and","the","Dominos",";","and","Robert","Johnson","'","s","Cross","Road","Blues",",","recorded","with","Cream","."],"labels":["O","O","O","O","O","O","O","B-song","O","O","O","B-band","I-band","I-band","I-band","O","O","B-musical artist","I-musical artist","O","O","B-song","I-song","I-song","O","O","O","B-band","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, award, country, location, band, music genre, person, musical artist, album, musical instrument, organization and O.\nSentence: Two of his most popular recordings were Layla , recorded with Derek and the Dominos ; and Robert Johnson ' s Cross Road Blues , recorded with Cream .","prompt_labels":"Two(O) of(O) his(O) most(O) popular(O) recordings(O) were(O) Layla(B-song) ,(O) recorded(O) with(O) Derek(B-band) and(I-band) the(I-band) Dominos(I-band) ;(O) and(O) Robert(B-musical artist) Johnson(I-musical artist) '(O) s(O) Cross(B-song) Road(I-song) Blues(I-song) ,(O) recorded(O) with(O) Cream(B-band) .(O)"}}
{"id":"17","dataset":"crossner_music","split":"train","label_list":["song","band","award","location","music genre","organization","musical artist","musical instrument","event","country","album","person"],"instance":{"id":"17","words":["Flood","has","been","certified","platinum","and","their","children","'s","music","albums","Here","Come","the","ABCs",",","Here","Come","the","123s",",","and","Here","Comes","Science","have","all","been","certified","gold","."],"labels":["B-album","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","B-album","I-album","I-album","I-album","O","O","B-album","I-album","I-album","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, band, award, location, music genre, organization, musical artist, musical instrument, event, country, album, person and O.\nSentence: Flood has been certified platinum and their children 's music albums Here Come the ABCs , Here Come the 123s , and Here Comes Science have all been certified gold .","prompt_labels":"Flood(B-album) has(O) been(O) certified(O) platinum(O) and(O) their(O) children(O) 's(O) music(O) albums(O) Here(B-album) Come(I-album) the(I-album) ABCs(I-album) ,(O) Here(B-album) Come(I-album) the(I-album) 123s(I-album) ,(O) and(O) Here(B-album) Comes(I-album) Science(I-album) have(O) all(O) been(O) certified(O) gold(O) .(O)"}}
{"id":"18","dataset":"crossner_music","split":"train","label_list":["band","musical artist","musical instrument","event","location","music genre","person","song","country","award","organization","album"],"instance":{"id":"18","words":["She","is","the","recipient","of","various","accolades","including","an","Academy","Awards",",","three","Golden","Globe","Awards",",","two","Critics","'","Choice","Movie","Awards",",","a","Screen","Actors","Guild","Award",",","and","nominations","for","four","BAFTA","Awards",",","three","Primetime","Emmy","Awards",",","and","a","Grammy","Award","."],"labels":["O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","O","O","O","B-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical artist, musical instrument, event, location, music genre, person, song, country, award, organization, album and O.\nSentence: She is the recipient of various accolades including an Academy Awards , three Golden Globe Awards , two Critics ' Choice Movie Awards , a Screen Actors Guild Award , and nominations for four BAFTA Awards , three Primetime Emmy Awards , and a Grammy Award .","prompt_labels":"She(O) is(O) the(O) recipient(O) of(O) various(O) accolades(O) including(O) an(O) Academy(B-award) Awards(I-award) ,(O) three(O) Golden(B-award) Globe(I-award) Awards(I-award) ,(O) two(O) Critics(B-award) '(I-award) Choice(I-award) Movie(I-award) Awards(I-award) ,(O) a(O) Screen(B-award) Actors(I-award) Guild(I-award) Award(I-award) ,(O) and(O) nominations(O) for(O) four(O) BAFTA(B-award) Awards(I-award) ,(O) three(O) Primetime(B-award) Emmy(I-award) Awards(I-award) ,(O) and(O) a(O) Grammy(B-award) Award(I-award) .(O)"}}
{"id":"19","dataset":"crossner_music","split":"train","label_list":["song","organization","award","person","album","band","location","music genre","musical instrument","country","event","musical artist"],"instance":{"id":"19","words":["James","Brown","is","said","to","be","the","most","sampled","artist","in","the","history","of","hip","hop",",","while","P-Funk","is","the","second","most","sampled","artist",";","samples","of","old","Parliament","and","Funkadelic","songs","formed","the","basis","of","West","Coast","G-funk","."],"labels":["B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","B-band","O","O","O","O","O","O","O","O","O","O","B-band","I-band","I-band","O","O","O","O","O","B-music genre","I-music genre","I-music genre","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, organization, award, person, album, band, location, music genre, musical instrument, country, event, musical artist and O.\nSentence: James Brown is said to be the most sampled artist in the history of hip hop , while P-Funk is the second most sampled artist ; samples of old Parliament and Funkadelic songs formed the basis of West Coast G-funk .","prompt_labels":"James(B-musical artist) Brown(I-musical artist) is(O) said(O) to(O) be(O) the(O) most(O) sampled(O) artist(O) in(O) the(O) history(O) of(O) hip(B-music genre) hop(I-music genre) ,(O) while(O) P-Funk(B-band) is(O) the(O) second(O) most(O) sampled(O) artist(O) ;(O) samples(O) of(O) old(O) Parliament(B-band) and(I-band) Funkadelic(I-band) songs(O) formed(O) the(O) basis(O) of(O) West(B-music genre) Coast(I-music genre) G-funk(I-music genre) .(O)"}}
{"id":"20","dataset":"crossner_music","split":"train","label_list":["organization","award","musical instrument","location","band","song","album","event","musical artist","music genre","country","person"],"instance":{"id":"20","words":["Her","early","works","in","the","1960s","(","her","debut","The","Barbra","Streisand","Album",",","The","Second","Barbra","Streisand","Album",",","The","Third","Album",",","My","Name","Is","Barbra",",","etc","."],"labels":["O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","B-album","I-album","I-album","I-album","I-album","O","B-album","I-album","I-album","O","B-album","I-album","I-album","I-album","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, musical instrument, location, band, song, album, event, musical artist, music genre, country, person and O.\nSentence: Her early works in the 1960s ( her debut The Barbra Streisand Album , The Second Barbra Streisand Album , The Third Album , My Name Is Barbra , etc .","prompt_labels":"Her(O) early(O) works(O) in(O) the(O) 1960s(O) ((O) her(O) debut(O) The(B-album) Barbra(I-album) Streisand(I-album) Album(I-album) ,(O) The(B-album) Second(I-album) Barbra(I-album) Streisand(I-album) Album(I-album) ,(O) The(B-album) Third(I-album) Album(I-album) ,(O) My(B-album) Name(I-album) Is(I-album) Barbra(I-album) ,(O) etc(O) .(O)"}}
{"id":"21","dataset":"crossner_music","split":"train","label_list":["album","person","organization","song","location","musical artist","musical instrument","event","country","music genre","band","award"],"instance":{"id":"21","words":["At","these","labels",",","Bubbles","created","more","designs","for","Elvis","Costello",",","as","well","as","other","artists","such","as","Nick","Lowe",",","Carlene","Carter","and","Clive","Langer","&","amp",";","The","Boxes","."],"labels":["O","O","O","O","B-person","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-band","I-band","I-band","I-band","I-band","I-band","I-band","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, person, organization, song, location, musical artist, musical instrument, event, country, music genre, band, award and O.\nSentence: At these labels , Bubbles created more designs for Elvis Costello , as well as other artists such as Nick Lowe , Carlene Carter and Clive Langer & amp ; The Boxes .","prompt_labels":"At(O) these(O) labels(O) ,(O) Bubbles(B-person) created(O) more(O) designs(O) for(O) Elvis(B-musical artist) Costello(I-musical artist) ,(O) as(O) well(O) as(O) other(O) artists(O) such(O) as(O) Nick(B-musical artist) Lowe(I-musical artist) ,(O) Carlene(B-musical artist) Carter(I-musical artist) and(O) Clive(B-band) Langer(I-band) &(I-band) amp(I-band) ;(I-band) The(I-band) Boxes(I-band) .(O)"}}
{"id":"22","dataset":"crossner_music","split":"train","label_list":["musical artist","organization","music genre","band","event","musical instrument","person","song","country","award","album","location"],"instance":{"id":"22","words":["The","band","have","received","a","total","of","11","nominations","for","ARIA","Music","Awards","in","ARIA","Music","Awards","of","1999",",","ARIA","Music","Awards","of","2001","and","ARIA","Music","Awards","of","2003","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, organization, music genre, band, event, musical instrument, person, song, country, award, album, location and O.\nSentence: The band have received a total of 11 nominations for ARIA Music Awards in ARIA Music Awards of 1999 , ARIA Music Awards of 2001 and ARIA Music Awards of 2003 .","prompt_labels":"The(O) band(O) have(O) received(O) a(O) total(O) of(O) 11(O) nominations(O) for(O) ARIA(B-award) Music(I-award) Awards(I-award) in(O) ARIA(B-award) Music(I-award) Awards(I-award) of(I-award) 1999(I-award) ,(O) ARIA(B-award) Music(I-award) Awards(I-award) of(I-award) 2001(I-award) and(O) ARIA(B-award) Music(I-award) Awards(I-award) of(I-award) 2003(I-award) .(O)"}}
{"id":"23","dataset":"crossner_music","split":"train","label_list":["album","song","music genre","musical artist","location","band","country","person","event","award","organization","musical instrument"],"instance":{"id":"23","words":["Their","music","has","a","particular","rumba","flamenca","style",",","with","Pop","music","influences",";","many","songs","of","the","Gipsy","Kings","fit","social","dance","s",",","such","as","salsa","and","Rhumba","."],"labels":["O","O","O","O","O","B-music genre","I-music genre","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","B-band","I-band","O","O","O","O","O","O","O","B-music genre","O","B-music genre","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, song, music genre, musical artist, location, band, country, person, event, award, organization, musical instrument and O.\nSentence: Their music has a particular rumba flamenca style , with Pop music influences ; many songs of the Gipsy Kings fit social dance s , such as salsa and Rhumba .","prompt_labels":"Their(O) music(O) has(O) a(O) particular(O) rumba(B-music genre) flamenca(I-music genre) style(O) ,(O) with(O) Pop(B-music genre) music(I-music genre) influences(O) ;(O) many(O) songs(O) of(O) the(O) Gipsy(B-band) Kings(I-band) fit(O) social(O) dance(O) s(O) ,(O) such(O) as(O) salsa(B-music genre) and(O) Rhumba(B-music genre) .(O)"}}
{"id":"24","dataset":"crossner_music","split":"train","label_list":["musical artist","event","person","country","musical instrument","location","band","award","organization","song","album","music genre"],"instance":{"id":"24","words":["Parton","received","nominations","for","Drama","Desk","Award","for","Outstanding","Music","and","Drama","Desk","Award","for","Outstanding","Lyrics",",","as","well","as","a","nomination","for","Tony","Award","for","Best","Original","Score","."],"labels":["B-musical artist","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, event, person, country, musical instrument, location, band, award, organization, song, album, music genre and O.\nSentence: Parton received nominations for Drama Desk Award for Outstanding Music and Drama Desk Award for Outstanding Lyrics , as well as a nomination for Tony Award for Best Original Score .","prompt_labels":"Parton(B-musical artist) received(O) nominations(O) for(O) Drama(B-award) Desk(I-award) Award(I-award) for(I-award) Outstanding(I-award) Music(I-award) and(O) Drama(B-award) Desk(I-award) Award(I-award) for(I-award) Outstanding(I-award) Lyrics(I-award) ,(O) as(O) well(O) as(O) a(O) nomination(O) for(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) .(O)"}}
{"id":"25","dataset":"crossner_music","split":"train","label_list":["country","musical artist","musical instrument","award","organization","album","song","music genre","band","person","event","location"],"instance":{"id":"25","words":["By","the","end","of","World","War","II",",","mountaineer","string","band","music","known","as","Bluegrass","music","had","emerged","when","Bill","Monroe","joined","with","Lester","Flatt","and","Earl","Scruggs",",","introduced","by","Roy","Acuff","at","the","Grand","Ole","Opry","."],"labels":["O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","B-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","B-musical artist","I-musical artist","O","O","B-location","I-location","I-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, musical artist, musical instrument, award, organization, album, song, music genre, band, person, event, location and O.\nSentence: By the end of World War II , mountaineer string band music known as Bluegrass music had emerged when Bill Monroe joined with Lester Flatt and Earl Scruggs , introduced by Roy Acuff at the Grand Ole Opry .","prompt_labels":"By(O) the(O) end(O) of(O) World(B-event) War(I-event) II(I-event) ,(O) mountaineer(O) string(O) band(O) music(O) known(O) as(O) Bluegrass(B-music genre) music(I-music genre) had(O) emerged(O) when(O) Bill(B-musical artist) Monroe(I-musical artist) joined(O) with(O) Lester(B-musical artist) Flatt(I-musical artist) and(O) Earl(B-musical artist) Scruggs(I-musical artist) ,(O) introduced(O) by(O) Roy(B-musical artist) Acuff(I-musical artist) at(O) the(O) Grand(B-location) Ole(I-location) Opry(I-location) .(O)"}}
{"id":"26","dataset":"crossner_music","split":"train","label_list":["organization","music genre","country","musical artist","award","song","location","band","musical instrument","event","album","person"],"instance":{"id":"26","words":["At","the","59th","Annual","Grammy","Awards","on","12","February","2017",",","Bowie","won","all","five","nominated","awards",":","Grammy","Award","for","Best","Rock","Performance",";","Grammy","Award","for","Best","Alternative","Music","Album",";","Best","Engineered","Album",",","Non-Classical",";","Grammy","Award","for","Best","Recording","Package",";","and","Grammy","Award","for","Best","Rock","Song","."],"labels":["O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","B-musical artist","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, music genre, country, musical artist, award, song, location, band, musical instrument, event, album, person and O.\nSentence: At the 59th Annual Grammy Awards on 12 February 2017 , Bowie won all five nominated awards : Grammy Award for Best Rock Performance ; Grammy Award for Best Alternative Music Album ; Best Engineered Album , Non-Classical ; Grammy Award for Best Recording Package ; and Grammy Award for Best Rock Song .","prompt_labels":"At(O) the(O) 59th(B-award) Annual(I-award) Grammy(I-award) Awards(I-award) on(O) 12(O) February(O) 2017(O) ,(O) Bowie(B-musical artist) won(O) all(O) five(O) nominated(O) awards(O) :(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Rock(I-award) Performance(I-award) ;(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Alternative(I-award) Music(I-award) Album(I-award) ;(O) Best(B-award) Engineered(I-award) Album(I-award) ,(I-award) Non-Classical(I-award) ;(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Recording(I-award) Package(I-award) ;(O) and(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Rock(I-award) Song(I-award) .(O)"}}
{"id":"27","dataset":"crossner_music","split":"train","label_list":["album","country","organization","music genre","event","award","musical artist","location","song","person","band","musical instrument"],"instance":{"id":"27","words":["The","group","has","been","nominated","for","20","Grammy","awards","and","has","won","five","of","them","with","Best","Alternative","Album","for","Dookie",",","Best","Rock","Album","for","American","Idiot","and","21st","Century","Breakdown",",","Record","of","the","Year","for","Boulevard","of","Broken","Dreams",",","and","Best","Musical","Show","Album","for","American","Idiot",":","The","Original","Broadway","Cast","Recording","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O","O","B-award","I-award","I-award","O","B-album","O","B-award","I-award","I-award","O","B-album","I-album","O","B-album","I-album","I-album","O","B-award","I-award","I-award","I-award","O","B-album","I-album","I-album","I-album","O","O","B-award","I-award","I-award","I-award","O","B-album","I-album","O","B-album","I-album","I-album","I-album","I-album","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, country, organization, music genre, event, award, musical artist, location, song, person, band, musical instrument and O.\nSentence: The group has been nominated for 20 Grammy awards and has won five of them with Best Alternative Album for Dookie , Best Rock Album for American Idiot and 21st Century Breakdown , Record of the Year for Boulevard of Broken Dreams , and Best Musical Show Album for American Idiot : The Original Broadway Cast Recording .","prompt_labels":"The(O) group(O) has(O) been(O) nominated(O) for(O) 20(O) Grammy(B-award) awards(I-award) and(O) has(O) won(O) five(O) of(O) them(O) with(O) Best(B-award) Alternative(I-award) Album(I-award) for(O) Dookie(B-album) ,(O) Best(B-award) Rock(I-award) Album(I-award) for(O) American(B-album) Idiot(I-album) and(O) 21st(B-album) Century(I-album) Breakdown(I-album) ,(O) Record(B-award) of(I-award) the(I-award) Year(I-award) for(O) Boulevard(B-album) of(I-album) Broken(I-album) Dreams(I-album) ,(O) and(O) Best(B-award) Musical(I-award) Show(I-award) Album(I-award) for(O) American(B-album) Idiot(I-album) :(O) The(B-album) Original(I-album) Broadway(I-album) Cast(I-album) Recording(I-album) .(O)"}}
{"id":"28","dataset":"crossner_music","split":"train","label_list":["award","musical artist","song","organization","country","music genre","musical instrument","event","band","location","album","person"],"instance":{"id":"28","words":["Today","there","are","Celtic-influenced","subgenres","of","virtually","every","type","of","popular","music","including","electronica",",","Celtic","rock",",","Celtic","metal",",","Celtic","punk",",","Hip","hop","music",",","reggae",",","New-age","music",",","Latin",",","Andean","and","Pop","music","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","O","B-music genre","I-music genre","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical artist, song, organization, country, music genre, musical instrument, event, band, location, album, person and O.\nSentence: Today there are Celtic-influenced subgenres of virtually every type of popular music including electronica , Celtic rock , Celtic metal , Celtic punk , Hip hop music , reggae , New-age music , Latin , Andean and Pop music .","prompt_labels":"Today(O) there(O) are(O) Celtic-influenced(O) subgenres(O) of(O) virtually(O) every(O) type(O) of(O) popular(O) music(O) including(O) electronica(B-music genre) ,(O) Celtic(B-music genre) rock(I-music genre) ,(O) Celtic(B-music genre) metal(I-music genre) ,(O) Celtic(B-music genre) punk(I-music genre) ,(O) Hip(B-music genre) hop(I-music genre) music(I-music genre) ,(O) reggae(B-music genre) ,(O) New-age(B-music genre) music(I-music genre) ,(O) Latin(B-music genre) ,(O) Andean(B-music genre) and(O) Pop(B-music genre) music(I-music genre) .(O)"}}
{"id":"29","dataset":"crossner_music","split":"train","label_list":["band","musical artist","music genre","album","event","award","person","country","organization","musical instrument","song","location"],"instance":{"id":"29","words":["The","Righteous","Brothers",",","Bobby","Hatfield","and","Bill","Medley",",","also","guest-starred","in","different","episodes","."],"labels":["B-band","I-band","I-band","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical artist, music genre, album, event, award, person, country, organization, musical instrument, song, location and O.\nSentence: The Righteous Brothers , Bobby Hatfield and Bill Medley , also guest-starred in different episodes .","prompt_labels":"The(B-band) Righteous(I-band) Brothers(I-band) ,(O) Bobby(B-musical artist) Hatfield(I-musical artist) and(O) Bill(B-musical artist) Medley(I-musical artist) ,(O) also(O) guest-starred(O) in(O) different(O) episodes(O) .(O)"}}
{"id":"30","dataset":"crossner_music","split":"train","label_list":["band","location","event","organization","music genre","song","album","country","person","award","musical artist","musical instrument"],"instance":{"id":"30","words":["Outside","the","south",",","the","accordion","(","predominantly","the","piano","accordion",")","is","used","in","almost","all","styles","of","Forró","(","in","particular","in","the","subgenres","of","Xote","and","Baião",")","as","the","principal","instrument",",","Luiz","Gonzaga","(","the","King","of","the","Baião",")","and","Dominguinhos","being","among","the","notable","musicians","in","this","style","from","the","northeast","."],"labels":["O","O","O","O","O","O","O","O","O","B-musical instrument","I-musical instrument","O","O","O","O","O","O","O","O","B-music genre","O","O","O","O","O","O","O","B-music genre","O","B-music genre","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, location, event, organization, music genre, song, album, country, person, award, musical artist, musical instrument and O.\nSentence: Outside the south , the accordion ( predominantly the piano accordion ) is used in almost all styles of Forró ( in particular in the subgenres of Xote and Baião ) as the principal instrument , Luiz Gonzaga ( the King of the Baião ) and Dominguinhos being among the notable musicians in this style from the northeast .","prompt_labels":"Outside(O) the(O) south(O) ,(O) the(O) accordion(O) ((O) predominantly(O) the(O) piano(B-musical instrument) accordion(I-musical instrument) )(O) is(O) used(O) in(O) almost(O) all(O) styles(O) of(O) Forró(B-music genre) ((O) in(O) particular(O) in(O) the(O) subgenres(O) of(O) Xote(B-music genre) and(O) Baião(B-music genre) )(O) as(O) the(O) principal(O) instrument(O) ,(O) Luiz(B-musical artist) Gonzaga(I-musical artist) ((O) the(O) King(O) of(O) the(O) Baião(O) )(O) and(O) Dominguinhos(B-musical artist) being(O) among(O) the(O) notable(O) musicians(O) in(O) this(O) style(O) from(O) the(O) northeast(O) .(O)"}}
{"id":"31","dataset":"crossner_music","split":"train","label_list":["award","music genre","location","event","person","organization","song","musical instrument","musical artist","album","band","country"],"instance":{"id":"31","words":["These","were",":","Now","and","Zen","in","1988",",","Manic","Nirvana","in","1990",",","and","the","1993","Fate","of","Nations","(","which","features","Moya","Brennan","of","Clannad","and","former","Cutting","Crew","guitarist","Kevin","MacMichael",")","."],"labels":["O","O","O","B-album","I-album","I-album","O","O","O","B-album","I-album","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","B-musical artist","I-musical artist","O","B-band","O","O","B-band","I-band","O","B-musical artist","I-musical artist","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, music genre, location, event, person, organization, song, musical instrument, musical artist, album, band, country and O.\nSentence: These were : Now and Zen in 1988 , Manic Nirvana in 1990 , and the 1993 Fate of Nations ( which features Moya Brennan of Clannad and former Cutting Crew guitarist Kevin MacMichael ) .","prompt_labels":"These(O) were(O) :(O) Now(B-album) and(I-album) Zen(I-album) in(O) 1988(O) ,(O) Manic(B-album) Nirvana(I-album) in(O) 1990(O) ,(O) and(O) the(O) 1993(O) Fate(B-album) of(I-album) Nations(I-album) ((O) which(O) features(O) Moya(B-musical artist) Brennan(I-musical artist) of(O) Clannad(B-band) and(O) former(O) Cutting(B-band) Crew(I-band) guitarist(O) Kevin(B-musical artist) MacMichael(I-musical artist) )(O) .(O)"}}
{"id":"32","dataset":"crossner_music","split":"train","label_list":["location","musical artist","album","award","music genre","song","country","musical instrument","organization","person","band","event"],"instance":{"id":"32","words":["In","Finland",",","there","emerged","a","scene","that","mixed","the","first","wave","black","metal","style","with","elements","of","death","metal","and","grindcore",";","this","included","Beherit",",","Archgoat","and","Impaled","Nazarene",",","whose","debut","album","Tol","Cormpt","Norz","Norz","Norz","Rock","Hard","journalist","Wolf-Rüdiger","Mühlmann","considers","a","part","of","war","metal","'s","roots","."],"labels":["O","B-country","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","B-music genre","I-music genre","O","B-music genre","O","O","O","B-band","O","B-band","O","B-band","I-band","O","O","O","O","B-album","I-album","I-album","I-album","I-album","B-album","I-album","O","B-person","I-person","O","O","O","O","B-music genre","I-music genre","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical artist, album, award, music genre, song, country, musical instrument, organization, person, band, event and O.\nSentence: In Finland , there emerged a scene that mixed the first wave black metal style with elements of death metal and grindcore ; this included Beherit , Archgoat and Impaled Nazarene , whose debut album Tol Cormpt Norz Norz Norz Rock Hard journalist Wolf-Rüdiger Mühlmann considers a part of war metal 's roots .","prompt_labels":"In(O) Finland(B-country) ,(O) there(O) emerged(O) a(O) scene(O) that(O) mixed(O) the(O) first(O) wave(O) black(B-music genre) metal(I-music genre) style(O) with(O) elements(O) of(O) death(B-music genre) metal(I-music genre) and(O) grindcore(B-music genre) ;(O) this(O) included(O) Beherit(B-band) ,(O) Archgoat(B-band) and(O) Impaled(B-band) Nazarene(I-band) ,(O) whose(O) debut(O) album(O) Tol(B-album) Cormpt(I-album) Norz(I-album) Norz(I-album) Norz(I-album) Rock(B-album) Hard(I-album) journalist(O) Wolf-Rüdiger(B-person) Mühlmann(I-person) considers(O) a(O) part(O) of(O) war(B-music genre) metal(I-music genre) 's(O) roots(O) .(O)"}}
{"id":"33","dataset":"crossner_music","split":"train","label_list":["location","musical instrument","organization","band","song","country","album","person","musical artist","award","music genre","event"],"instance":{"id":"33","words":["Arguably","the","most","successful","boy","band","manager","from","the","U.S.","was","Lou","Pearlman",",","who","founded","commercially","successful","acts","such","as","the","Backstreet","Boys","in","1993",",","NSYNC","and","LFO","in","1995",",","O-Town","in","2000",",","and","US5","in","2005","."],"labels":["O","O","O","O","O","O","O","O","O","B-country","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","B-band","I-band","O","O","O","B-band","O","B-band","O","O","O","B-band","O","O","O","O","B-band","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical instrument, organization, band, song, country, album, person, musical artist, award, music genre, event and O.\nSentence: Arguably the most successful boy band manager from the U.S. was Lou Pearlman , who founded commercially successful acts such as the Backstreet Boys in 1993 , NSYNC and LFO in 1995 , O-Town in 2000 , and US5 in 2005 .","prompt_labels":"Arguably(O) the(O) most(O) successful(O) boy(O) band(O) manager(O) from(O) the(O) U.S.(B-country) was(O) Lou(B-musical artist) Pearlman(I-musical artist) ,(O) who(O) founded(O) commercially(O) successful(O) acts(O) such(O) as(O) the(O) Backstreet(B-band) Boys(I-band) in(O) 1993(O) ,(O) NSYNC(B-band) and(O) LFO(B-band) in(O) 1995(O) ,(O) O-Town(B-band) in(O) 2000(O) ,(O) and(O) US5(B-band) in(O) 2005(O) .(O)"}}
{"id":"34","dataset":"crossner_music","split":"train","label_list":["person","organization","country","event","album","music genre","song","award","band","musical artist","musical instrument","location"],"instance":{"id":"34","words":["Extreme",",","Red","Hot","Chili","Peppers",",","Living","Colour",",","Jane","'s","Addiction",",","Prince",",","Primus",",","Fishbone",",","Faith","No","More",",","Rage","Against","the","Machine",",","Infectious","Grooves",",","and","Incubus","spread","the","approach","and","styles","garnered","from","funk","pioneers","to","new","audiences","in","the","mid-to-late","1980s","and","the","1990s","."],"labels":["B-band","O","B-band","I-band","I-band","I-band","O","B-band","I-band","O","B-band","I-band","I-band","O","B-musical artist","O","B-band","O","B-band","O","B-band","I-band","I-band","O","B-band","I-band","I-band","I-band","O","B-band","I-band","O","O","B-band","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, country, event, album, music genre, song, award, band, musical artist, musical instrument, location and O.\nSentence: Extreme , Red Hot Chili Peppers , Living Colour , Jane 's Addiction , Prince , Primus , Fishbone , Faith No More , Rage Against the Machine , Infectious Grooves , and Incubus spread the approach and styles garnered from funk pioneers to new audiences in the mid-to-late 1980s and the 1990s .","prompt_labels":"Extreme(B-band) ,(O) Red(B-band) Hot(I-band) Chili(I-band) Peppers(I-band) ,(O) Living(B-band) Colour(I-band) ,(O) Jane(B-band) 's(I-band) Addiction(I-band) ,(O) Prince(B-musical artist) ,(O) Primus(B-band) ,(O) Fishbone(B-band) ,(O) Faith(B-band) No(I-band) More(I-band) ,(O) Rage(B-band) Against(I-band) the(I-band) Machine(I-band) ,(O) Infectious(B-band) Grooves(I-band) ,(O) and(O) Incubus(B-band) spread(O) the(O) approach(O) and(O) styles(O) garnered(O) from(O) funk(O) pioneers(O) to(O) new(O) audiences(O) in(O) the(O) mid-to-late(O) 1980s(O) and(O) the(O) 1990s(O) .(O)"}}
{"id":"35","dataset":"crossner_music","split":"train","label_list":["person","location","music genre","organization","award","band","country","musical artist","musical instrument","album","event","song"],"instance":{"id":"35","words":["Bands","like","Flogging","Molly",",","Black","47",",","Dropkick","Murphys",",","The","Young","Dubliners",",","The","Tossers","introduced","a","hybrid","of","Celtic","rock",",","Punk","rock",",","reggae",",","Hardcore","punk","and","other","elements","in","the","1990s","that","has","become","popular","with","Irish-American","youth","."],"labels":["O","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","I-band","O","B-band","I-band","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, music genre, organization, award, band, country, musical artist, musical instrument, album, event, song and O.\nSentence: Bands like Flogging Molly , Black 47 , Dropkick Murphys , The Young Dubliners , The Tossers introduced a hybrid of Celtic rock , Punk rock , reggae , Hardcore punk and other elements in the 1990s that has become popular with Irish-American youth .","prompt_labels":"Bands(O) like(O) Flogging(B-band) Molly(I-band) ,(O) Black(B-band) 47(I-band) ,(O) Dropkick(B-band) Murphys(I-band) ,(O) The(B-band) Young(I-band) Dubliners(I-band) ,(O) The(B-band) Tossers(I-band) introduced(O) a(O) hybrid(O) of(O) Celtic(B-music genre) rock(I-music genre) ,(O) Punk(B-music genre) rock(I-music genre) ,(O) reggae(B-music genre) ,(O) Hardcore(B-music genre) punk(I-music genre) and(O) other(O) elements(O) in(O) the(O) 1990s(O) that(O) has(O) become(O) popular(O) with(O) Irish-American(O) youth(O) .(O)"}}
{"id":"36","dataset":"crossner_music","split":"train","label_list":["location","musical artist","album","band","music genre","musical instrument","award","organization","person","song","event","country"],"instance":{"id":"36","words":["The","Stone","Roses","'","influences","included","garage","rock",",","electronic","dance","music",",","Krautrock",",","Northern","soul",",","punk","rock",",","reggae",",","Soul","music","and","artists","such","as","the","Beatles",","],"labels":["B-band","I-band","I-band","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","O","O","O","B-band","I-band","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical artist, album, band, music genre, musical instrument, award, organization, person, song, event, country and O.\nSentence: The Stone Roses ' influences included garage rock , electronic dance music , Krautrock , Northern soul , punk rock , reggae , Soul music and artists such as the Beatles ,","prompt_labels":"The(B-band) Stone(I-band) Roses(I-band) '(O) influences(O) included(O) garage(B-music genre) rock(I-music genre) ,(O) electronic(B-music genre) dance(I-music genre) music(I-music genre) ,(O) Krautrock(B-music genre) ,(O) Northern(B-music genre) soul(I-music genre) ,(O) punk(B-music genre) rock(I-music genre) ,(O) reggae(B-music genre) ,(O) Soul(B-music genre) music(I-music genre) and(O) artists(O) such(O) as(O) the(B-band) Beatles(I-band) ,(O)"}}
{"id":"37","dataset":"crossner_music","split":"train","label_list":["album","song","musical instrument","country","location","event","music genre","person","award","musical artist","band","organization"],"instance":{"id":"37","words":["In","2018",",","Buckingham","was","fired","from","the","band","and","was","replaced","by","Mike","Campbell",",","formerly","of","Tom","Petty","and","the","Heartbreakers",",","and","Neil","Finn","of","Split","Enz","and","Crowded","House","."],"labels":["O","O","O","B-musical artist","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","B-band","I-band","I-band","I-band","I-band","O","O","B-musical artist","I-musical artist","O","B-band","I-band","O","B-band","I-band","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, song, musical instrument, country, location, event, music genre, person, award, musical artist, band, organization and O.\nSentence: In 2018 , Buckingham was fired from the band and was replaced by Mike Campbell , formerly of Tom Petty and the Heartbreakers , and Neil Finn of Split Enz and Crowded House .","prompt_labels":"In(O) 2018(O) ,(O) Buckingham(B-musical artist) was(O) fired(O) from(O) the(O) band(O) and(O) was(O) replaced(O) by(O) Mike(B-musical artist) Campbell(I-musical artist) ,(O) formerly(O) of(O) Tom(B-band) Petty(I-band) and(I-band) the(I-band) Heartbreakers(I-band) ,(O) and(O) Neil(B-musical artist) Finn(I-musical artist) of(O) Split(B-band) Enz(I-band) and(O) Crowded(B-band) House(I-band) .(O)"}}
{"id":"38","dataset":"crossner_music","split":"train","label_list":["album","event","musical instrument","organization","song","music genre","country","musical artist","person","location","award","band"],"instance":{"id":"38","words":["In","Paris",",","he","performed","at","the","Stade","de","France","for","Saint","Patrick","'s","Day",",","in","AccorHotels","Arena",",","the","Bataclan",",","the","Casino","de","Paris","and","the","Théâtre","de","la","Ville","with","guest","singers","Mari","Boine","and","Karen","Matheson","as","well","as","Donald","Shaw","."],"labels":["O","B-location","O","O","O","O","O","B-location","I-location","I-location","O","B-event","I-event","I-event","I-event","O","O","B-location","I-location","O","O","B-location","O","O","B-location","I-location","I-location","O","O","B-location","I-location","I-location","I-location","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","B-musical artist","I-musical artist","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, event, musical instrument, organization, song, music genre, country, musical artist, person, location, award, band and O.\nSentence: In Paris , he performed at the Stade de France for Saint Patrick 's Day , in AccorHotels Arena , the Bataclan , the Casino de Paris and the Théâtre de la Ville with guest singers Mari Boine and Karen Matheson as well as Donald Shaw .","prompt_labels":"In(O) Paris(B-location) ,(O) he(O) performed(O) at(O) the(O) Stade(B-location) de(I-location) France(I-location) for(O) Saint(B-event) Patrick(I-event) 's(I-event) Day(I-event) ,(O) in(O) AccorHotels(B-location) Arena(I-location) ,(O) the(O) Bataclan(B-location) ,(O) the(O) Casino(B-location) de(I-location) Paris(I-location) and(O) the(O) Théâtre(B-location) de(I-location) la(I-location) Ville(I-location) with(O) guest(O) singers(O) Mari(B-musical artist) Boine(I-musical artist) and(O) Karen(B-musical artist) Matheson(I-musical artist) as(O) well(O) as(O) Donald(B-musical artist) Shaw(I-musical artist) .(O)"}}
{"id":"39","dataset":"crossner_music","split":"train","label_list":["musical artist","musical instrument","organization","event","award","album","song","music genre","location","country","band","person"],"instance":{"id":"39","words":["On","26","February","1987",",","A","Hard","Day","'s","Night","was","officially","released","on","compact","disc","in","mono",",","along","with","Please","Please","Me",",","With","the","Beatles",",","and","Beatles","for","Sale","."],"labels":["O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","B-album","I-album","I-album","O","O","B-album","I-album","I-album","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, musical instrument, organization, event, award, album, song, music genre, location, country, band, person and O.\nSentence: On 26 February 1987 , A Hard Day 's Night was officially released on compact disc in mono , along with Please Please Me , With the Beatles , and Beatles for Sale .","prompt_labels":"On(O) 26(O) February(O) 1987(O) ,(O) A(O) Hard(B-album) Day(I-album) 's(I-album) Night(I-album) was(O) officially(O) released(O) on(O) compact(O) disc(O) in(O) mono(O) ,(O) along(O) with(O) Please(B-album) Please(I-album) Me(I-album) ,(O) With(B-album) the(I-album) Beatles(I-album) ,(O) and(O) Beatles(B-album) for(I-album) Sale(I-album) .(O)"}}
{"id":"40","dataset":"crossner_music","split":"train","label_list":["award","country","musical artist","musical instrument","album","organization","event","location","song","music genre","band","person"],"instance":{"id":"40","words":["It","started","with","pop","music","singers","like","Glen","Campbell",",","Bobbie","Gentry",",","John","Denver",",","Olivia","Newton-John",",","Anne","Murray",",","B.","J.","Thomas",",","The","Bellamy","Brothers",",","and","Linda","Ronstadt","having","hits","on","the","country","charts","."],"labels":["O","O","O","B-music genre","I-music genre","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","B-band","I-band","I-band","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, musical artist, musical instrument, album, organization, event, location, song, music genre, band, person and O.\nSentence: It started with pop music singers like Glen Campbell , Bobbie Gentry , John Denver , Olivia Newton-John , Anne Murray , B. J. Thomas , The Bellamy Brothers , and Linda Ronstadt having hits on the country charts .","prompt_labels":"It(O) started(O) with(O) pop(B-music genre) music(I-music genre) singers(O) like(O) Glen(B-person) Campbell(I-person) ,(O) Bobbie(B-person) Gentry(I-person) ,(O) John(B-person) Denver(I-person) ,(O) Olivia(B-person) Newton-John(I-person) ,(O) Anne(B-musical artist) Murray(I-musical artist) ,(O) B.(B-musical artist) J.(I-musical artist) Thomas(I-musical artist) ,(O) The(B-band) Bellamy(I-band) Brothers(I-band) ,(O) and(O) Linda(B-musical artist) Ronstadt(I-musical artist) having(O) hits(O) on(O) the(O) country(O) charts(O) .(O)"}}
{"id":"41","dataset":"crossner_music","split":"train","label_list":["musical instrument","organization","person","album","event","band","award","song","music genre","location","musical artist","country"],"instance":{"id":"41","words":["The","shows","were","later","taken","into","Europe",",","and","featured","such","stars","as","Johnny","Cash",",","Dolly","Parton",",","Tammy","Wynette",",","David","Allan","Coe",",","Emmylou","Harris",",","Boxcar","Willie",",","Johnny","Russell","and","Jerry","Lee","Lewis","."],"labels":["O","O","O","O","O","O","B-location","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, organization, person, album, event, band, award, song, music genre, location, musical artist, country and O.\nSentence: The shows were later taken into Europe , and featured such stars as Johnny Cash , Dolly Parton , Tammy Wynette , David Allan Coe , Emmylou Harris , Boxcar Willie , Johnny Russell and Jerry Lee Lewis .","prompt_labels":"The(O) shows(O) were(O) later(O) taken(O) into(O) Europe(B-location) ,(O) and(O) featured(O) such(O) stars(O) as(O) Johnny(B-musical artist) Cash(I-musical artist) ,(O) Dolly(B-musical artist) Parton(I-musical artist) ,(O) Tammy(B-musical artist) Wynette(I-musical artist) ,(O) David(B-musical artist) Allan(I-musical artist) Coe(I-musical artist) ,(O) Emmylou(B-musical artist) Harris(I-musical artist) ,(O) Boxcar(B-musical artist) Willie(I-musical artist) ,(O) Johnny(B-musical artist) Russell(I-musical artist) and(O) Jerry(B-musical artist) Lee(I-musical artist) Lewis(I-musical artist) .(O)"}}
{"id":"42","dataset":"crossner_music","split":"train","label_list":["music genre","album","location","musical instrument","band","song","organization","event","person","musical artist","country","award"],"instance":{"id":"42","words":["Arguably","the","most","successful","boy","band","manager","from","the","U.S.","was","Lou","Pearlman",",","who","founded","commercially","successful","acts","such","as","the","Backstreet","Boys","in","1993",",","NSYNC","and","LFO","in","1995",",","O-Town","in","2000",",","and","US5","in","2005","."],"labels":["O","O","O","O","O","O","O","O","O","B-country","O","B-person","I-person","O","O","O","O","O","O","O","O","O","B-band","I-band","O","O","O","B-band","O","B-band","O","O","O","B-band","O","O","O","O","B-band","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, album, location, musical instrument, band, song, organization, event, person, musical artist, country, award and O.\nSentence: Arguably the most successful boy band manager from the U.S. was Lou Pearlman , who founded commercially successful acts such as the Backstreet Boys in 1993 , NSYNC and LFO in 1995 , O-Town in 2000 , and US5 in 2005 .","prompt_labels":"Arguably(O) the(O) most(O) successful(O) boy(O) band(O) manager(O) from(O) the(O) U.S.(B-country) was(O) Lou(B-person) Pearlman(I-person) ,(O) who(O) founded(O) commercially(O) successful(O) acts(O) such(O) as(O) the(O) Backstreet(B-band) Boys(I-band) in(O) 1993(O) ,(O) NSYNC(B-band) and(O) LFO(B-band) in(O) 1995(O) ,(O) O-Town(B-band) in(O) 2000(O) ,(O) and(O) US5(B-band) in(O) 2005(O) .(O)"}}
{"id":"43","dataset":"crossner_music","split":"train","label_list":["musical artist","location","musical instrument","award","music genre","band","person","event","song","album","country","organization"],"instance":{"id":"43","words":["In","addition",",","the","film","won","the","BAFTA","Award","for","Best","Film",",","BAFTA","Award","for","Best","Direction","(","Nichols",")",",","BAFTA","Award","for","Most","Promising","Newcomer","to","Leading","Film","Roles","(","Hoffman",")",",","the","BAFTA","Award","for","Best","Editing","(","Sam","O","'Steen",")","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-person","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-person","O","O","O","B-award","I-award","I-award","I-award","I-award","O","B-person","I-person","I-person","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, musical instrument, award, music genre, band, person, event, song, album, country, organization and O.\nSentence: In addition , the film won the BAFTA Award for Best Film , BAFTA Award for Best Direction ( Nichols ) , BAFTA Award for Most Promising Newcomer to Leading Film Roles ( Hoffman ) , the BAFTA Award for Best Editing ( Sam O 'Steen ) .","prompt_labels":"In(O) addition(O) ,(O) the(O) film(O) won(O) the(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Film(I-award) ,(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Direction(I-award) ((O) Nichols(B-person) )(O) ,(O) BAFTA(B-award) Award(I-award) for(I-award) Most(I-award) Promising(I-award) Newcomer(I-award) to(I-award) Leading(I-award) Film(I-award) Roles(I-award) ((O) Hoffman(B-person) )(O) ,(O) the(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Editing(I-award) ((O) Sam(B-person) O(I-person) 'Steen(I-person) )(O) .(O)"}}
{"id":"44","dataset":"crossner_music","split":"train","label_list":["musical artist","song","location","person","band","country","organization","album","music genre","event","award","musical instrument"],"instance":{"id":"44","words":["With","the","influence","of","Tropicalismo",",","Traditional","Samba","and","Bossa","Nova",",","MPB","(","Música","popular","brasileira",")",",","or","Brazilian","Popular","Music",",","became","highly","singer-songwriter","based","."],"labels":["O","O","O","O","B-music genre","O","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","I-music genre","O","O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, song, location, person, band, country, organization, album, music genre, event, award, musical instrument and O.\nSentence: With the influence of Tropicalismo , Traditional Samba and Bossa Nova , MPB ( Música popular brasileira ) , or Brazilian Popular Music , became highly singer-songwriter based .","prompt_labels":"With(O) the(O) influence(O) of(O) Tropicalismo(B-music genre) ,(O) Traditional(O) Samba(B-music genre) and(O) Bossa(B-music genre) Nova(I-music genre) ,(O) MPB(B-music genre) ((O) Música(B-music genre) popular(I-music genre) brasileira(I-music genre) )(O) ,(O) or(O) Brazilian(B-music genre) Popular(I-music genre) Music(I-music genre) ,(O) became(O) highly(O) singer-songwriter(O) based(O) .(O)"}}
{"id":"45","dataset":"crossner_music","split":"train","label_list":["musical instrument","event","musical artist","album","country","award","organization","song","location","music genre","band","person"],"instance":{"id":"45","words":["Following","the","hard","rock","and","heavy","metal","origins","on","the","band","'s","first","two","albums",",","Too","Fast","for","Love","(","1981",")","and","Shout","at","the","Devil","(","1983",")",",","the","release","of","its","third","album","Theatre","of","Pain","(","1985",")","saw","Mötley","Crüe","joining","the","first","wave","of","glam","metal","."],"labels":["O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, event, musical artist, album, country, award, organization, song, location, music genre, band, person and O.\nSentence: Following the hard rock and heavy metal origins on the band 's first two albums , Too Fast for Love ( 1981 ) and Shout at the Devil ( 1983 ) , the release of its third album Theatre of Pain ( 1985 ) saw Mötley Crüe joining the first wave of glam metal .","prompt_labels":"Following(O) the(O) hard(B-music genre) rock(I-music genre) and(O) heavy(B-music genre) metal(I-music genre) origins(O) on(O) the(O) band(O) 's(O) first(O) two(O) albums(O) ,(O) Too(B-album) Fast(I-album) for(I-album) Love(I-album) ((O) 1981(O) )(O) and(O) Shout(B-album) at(I-album) the(I-album) Devil(I-album) ((O) 1983(O) )(O) ,(O) the(O) release(O) of(O) its(O) third(O) album(O) Theatre(B-album) of(I-album) Pain(I-album) ((O) 1985(O) )(O) saw(O) Mötley(O) Crüe(O) joining(O) the(B-event) first(I-event) wave(I-event) of(I-event) glam(I-event) metal(I-event) .(O)"}}
{"id":"46","dataset":"crossner_music","split":"train","label_list":["person","award","song","musical instrument","musical artist","event","location","music genre","organization","country","album","band"],"instance":{"id":"46","words":["In","June","1985",",","the","United","Way","of","Canada","invited","Lata","Mangeshkar","to","perform","at","Maple","Leaf","Gardens","."],"labels":["O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-musical artist","I-musical artist","O","O","O","B-location","I-location","I-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, song, musical instrument, musical artist, event, location, music genre, organization, country, album, band and O.\nSentence: In June 1985 , the United Way of Canada invited Lata Mangeshkar to perform at Maple Leaf Gardens .","prompt_labels":"In(O) June(O) 1985(O) ,(O) the(O) United(B-organization) Way(I-organization) of(I-organization) Canada(I-organization) invited(O) Lata(B-musical artist) Mangeshkar(I-musical artist) to(O) perform(O) at(O) Maple(B-location) Leaf(I-location) Gardens(I-location) .(O)"}}
{"id":"47","dataset":"crossner_music","split":"train","label_list":["award","musical instrument","music genre","album","musical artist","organization","country","event","person","location","song","band"],"instance":{"id":"47","words":["Soundgarden","achieved","its","biggest","success","with","the","1994","album","Superunknown",",","which","debuted","at","number","one","on","the","Billboard","200","and","yielded","the","Grammy","Award","-winning","singles","Spoonman","and","Black","Hole","Sun","."],"labels":["B-band","O","O","O","O","O","O","O","O","B-album","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","B-song","O","B-song","I-song","I-song","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical instrument, music genre, album, musical artist, organization, country, event, person, location, song, band and O.\nSentence: Soundgarden achieved its biggest success with the 1994 album Superunknown , which debuted at number one on the Billboard 200 and yielded the Grammy Award -winning singles Spoonman and Black Hole Sun .","prompt_labels":"Soundgarden(B-band) achieved(O) its(O) biggest(O) success(O) with(O) the(O) 1994(O) album(O) Superunknown(B-album) ,(O) which(O) debuted(O) at(O) number(O) one(O) on(O) the(O) Billboard(O) 200(O) and(O) yielded(O) the(O) Grammy(B-award) Award(I-award) -winning(O) singles(O) Spoonman(B-song) and(O) Black(B-song) Hole(I-song) Sun(I-song) .(O)"}}
{"id":"48","dataset":"crossner_music","split":"train","label_list":["award","music genre","person","location","country","album","organization","musical artist","band","song","event","musical instrument"],"instance":{"id":"48","words":["Despite","this",",","The","Godfather","Part","III","went","on","to","gather","7","Academy","Awards","nominations",",","including","Academy","Award","for","Best","Director","and","Academy","Award","for","Best","Picture","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, music genre, person, location, country, album, organization, musical artist, band, song, event, musical instrument and O.\nSentence: Despite this , The Godfather Part III went on to gather 7 Academy Awards nominations , including Academy Award for Best Director and Academy Award for Best Picture .","prompt_labels":"Despite(O) this(O) ,(O) The(O) Godfather(O) Part(O) III(O) went(O) on(O) to(O) gather(O) 7(O) Academy(B-award) Awards(I-award) nominations(O) ,(O) including(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) .(O)"}}
{"id":"49","dataset":"crossner_music","split":"train","label_list":["musical instrument","album","music genre","organization","person","location","event","country","award","band","musical artist","song"],"instance":{"id":"49","words":["In","2005",",","American","Idiot","won","a","Grammy","Award","for","Grammy","Award","for","Best","Rock","Album","and","was","nominated","in","six","other","categories","including","Grammy","Award","for","Album","of","the","Year","."],"labels":["O","O","O","B-album","I-album","O","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, album, music genre, organization, person, location, event, country, award, band, musical artist, song and O.\nSentence: In 2005 , American Idiot won a Grammy Award for Grammy Award for Best Rock Album and was nominated in six other categories including Grammy Award for Album of the Year .","prompt_labels":"In(O) 2005(O) ,(O) American(B-album) Idiot(I-album) won(O) a(O) Grammy(B-award) Award(I-award) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Rock(I-award) Album(I-award) and(O) was(O) nominated(O) in(O) six(O) other(O) categories(O) including(O) Grammy(B-award) Award(I-award) for(I-award) Album(I-award) of(I-award) the(I-award) Year(I-award) .(O)"}}
{"id":"50","dataset":"crossner_music","split":"train","label_list":["album","music genre","musical instrument","person","location","musical artist","song","band","organization","country","event","award"],"instance":{"id":"50","words":["A","sleeper","hit","in","United","Kingdom",",","the","record","eventually","hit","the","top","three","in","the","Official","Charts","Company","and","received","platinum","certifications","from","British","Phonographic","Industry",",","Recording","Industry","Association","of","America","and","ARIA","."],"labels":["O","O","O","O","B-country","I-country","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, music genre, musical instrument, person, location, musical artist, song, band, organization, country, event, award and O.\nSentence: A sleeper hit in United Kingdom , the record eventually hit the top three in the Official Charts Company and received platinum certifications from British Phonographic Industry , Recording Industry Association of America and ARIA .","prompt_labels":"A(O) sleeper(O) hit(O) in(O) United(B-country) Kingdom(I-country) ,(O) the(O) record(O) eventually(O) hit(O) the(O) top(O) three(O) in(O) the(O) Official(B-organization) Charts(I-organization) Company(I-organization) and(O) received(O) platinum(O) certifications(O) from(O) British(B-organization) Phonographic(I-organization) Industry(I-organization) ,(O) Recording(B-organization) Industry(I-organization) Association(I-organization) of(I-organization) America(I-organization) and(O) ARIA(B-organization) .(O)"}}
{"id":"51","dataset":"crossner_music","split":"train","label_list":["musical artist","album","music genre","musical instrument","organization","award","person","song","location","country","event","band"],"instance":{"id":"51","words":["For","the","2008","/","2009","season",",","he","played","Captain","Hook","at","the","Milton","Keynes","Theatre","and","donned","the","hook","once","again","for","the","2009","/","2010","panto","season","at","the","Liverpool","Empire","Theatre","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","O","B-location","I-location","I-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, album, music genre, musical instrument, organization, award, person, song, location, country, event, band and O.\nSentence: For the 2008 / 2009 season , he played Captain Hook at the Milton Keynes Theatre and donned the hook once again for the 2009 / 2010 panto season at the Liverpool Empire Theatre .","prompt_labels":"For(O) the(O) 2008(O) /(O) 2009(O) season(O) ,(O) he(O) played(O) Captain(O) Hook(O) at(O) the(O) Milton(B-location) Keynes(I-location) Theatre(I-location) and(O) donned(O) the(O) hook(O) once(O) again(O) for(O) the(O) 2009(O) /(O) 2010(O) panto(B-event) season(I-event) at(O) the(O) Liverpool(B-location) Empire(I-location) Theatre(I-location) .(O)"}}
{"id":"52","dataset":"crossner_music","split":"train","label_list":["band","event","musical instrument","person","music genre","album","organization","award","country","location","musical artist","song"],"instance":{"id":"52","words":["The","band","also","released","three","full-length","albums","dubbed","(","and","later","packaged","together","as",")","The","Trilogy",":","The","Maggot",",","The","Bootlicker",",","and","The","Crybaby","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","O","B-album","I-album","O","B-album","I-album","O","O","B-album","I-album","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, event, musical instrument, person, music genre, album, organization, award, country, location, musical artist, song and O.\nSentence: The band also released three full-length albums dubbed ( and later packaged together as ) The Trilogy : The Maggot , The Bootlicker , and The Crybaby .","prompt_labels":"The(O) band(O) also(O) released(O) three(O) full-length(O) albums(O) dubbed(O) ((O) and(O) later(O) packaged(O) together(O) as(O) )(O) The(B-album) Trilogy(I-album) :(O) The(B-album) Maggot(I-album) ,(O) The(B-album) Bootlicker(I-album) ,(O) and(O) The(B-album) Crybaby(I-album) .(O)"}}
{"id":"53","dataset":"crossner_music","split":"train","label_list":["musical artist","location","music genre","band","musical instrument","country","award","person","song","event","organization","album"],"instance":{"id":"53","words":["He","toured","with","his","band","Les","Mistigris","(","not","related","to","Mistigris",")","in","Germany",",","Belgium",",","France","and","Turkey","until","1967","."],"labels":["O","O","O","O","O","B-band","I-band","O","O","O","O","B-organization","O","O","B-country","O","B-country","O","B-country","O","B-country","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, music genre, band, musical instrument, country, award, person, song, event, organization, album and O.\nSentence: He toured with his band Les Mistigris ( not related to Mistigris ) in Germany , Belgium , France and Turkey until 1967 .","prompt_labels":"He(O) toured(O) with(O) his(O) band(O) Les(B-band) Mistigris(I-band) ((O) not(O) related(O) to(O) Mistigris(B-organization) )(O) in(O) Germany(B-country) ,(O) Belgium(B-country) ,(O) France(B-country) and(O) Turkey(B-country) until(O) 1967(O) .(O)"}}
{"id":"54","dataset":"crossner_music","split":"train","label_list":["person","event","song","location","album","musical instrument","musical artist","music genre","organization","award","country","band"],"instance":{"id":"54","words":["On","24","August","2012",",","Westenra","staged","a","concert","in","the","Gŵyl","Gobaith","Music","Festival","in","Wales","to","support","for","charities","Cancer","Research","UK",",","Wales","Air","Ambulance",",","CLIC","Sargent","and","HeadtoHeart","."],"labels":["O","O","O","O","O","B-musical artist","O","O","O","O","O","B-event","I-event","I-event","I-event","O","B-country","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, song, location, album, musical instrument, musical artist, music genre, organization, award, country, band and O.\nSentence: On 24 August 2012 , Westenra staged a concert in the Gŵyl Gobaith Music Festival in Wales to support for charities Cancer Research UK , Wales Air Ambulance , CLIC Sargent and HeadtoHeart .","prompt_labels":"On(O) 24(O) August(O) 2012(O) ,(O) Westenra(B-musical artist) staged(O) a(O) concert(O) in(O) the(O) Gŵyl(B-event) Gobaith(I-event) Music(I-event) Festival(I-event) in(O) Wales(B-country) to(O) support(O) for(O) charities(O) Cancer(B-organization) Research(I-organization) UK(I-organization) ,(O) Wales(B-organization) Air(I-organization) Ambulance(I-organization) ,(O) CLIC(B-organization) Sargent(I-organization) and(O) HeadtoHeart(B-organization) .(O)"}}
{"id":"55","dataset":"crossner_music","split":"train","label_list":["organization","song","musical instrument","musical artist","award","music genre","location","event","band","country","person","album"],"instance":{"id":"55","words":["These","albums","spawned","some","of","Carey","'s","most","successful","singles",",","including","Hero",",","Without","You",",","All","I","Want","for","Christmas","Is","You",",","Fantasy",",","Always","Be","My","Baby",",","as","well","as","One","Sweet","Day",",","which","peaked","at","number","one","in","the","U.S.","for","16","weeks","and","became","Billboard","s","Song","Of","The","Decade","(","1990s","Decade",")","."],"labels":["O","O","O","O","O","B-musical artist","O","O","O","O","O","O","B-song","O","B-song","I-song","O","B-song","I-song","I-song","I-song","I-song","I-song","I-song","O","B-song","O","B-song","I-song","I-song","I-song","O","O","O","O","B-song","I-song","I-song","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","B-organization","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, song, musical instrument, musical artist, award, music genre, location, event, band, country, person, album and O.\nSentence: These albums spawned some of Carey 's most successful singles , including Hero , Without You , All I Want for Christmas Is You , Fantasy , Always Be My Baby , as well as One Sweet Day , which peaked at number one in the U.S. for 16 weeks and became Billboard s Song Of The Decade ( 1990s Decade ) .","prompt_labels":"These(O) albums(O) spawned(O) some(O) of(O) Carey(B-musical artist) 's(O) most(O) successful(O) singles(O) ,(O) including(O) Hero(B-song) ,(O) Without(B-song) You(I-song) ,(O) All(B-song) I(I-song) Want(I-song) for(I-song) Christmas(I-song) Is(I-song) You(I-song) ,(O) Fantasy(B-song) ,(O) Always(B-song) Be(I-song) My(I-song) Baby(I-song) ,(O) as(O) well(O) as(O) One(B-song) Sweet(I-song) Day(I-song) ,(O) which(O) peaked(O) at(O) number(O) one(O) in(O) the(O) U.S.(B-country) for(O) 16(O) weeks(O) and(O) became(O) Billboard(B-organization) s(O) Song(O) Of(O) The(O) Decade(O) ((O) 1990s(O) Decade(O) )(O) .(O)"}}
{"id":"56","dataset":"crossner_music","split":"train","label_list":["album","award","musical instrument","musical artist","location","song","band","music genre","event","organization","country","person"],"instance":{"id":"56","words":["Michael","won","various","music","awards","including","two","Grammy","Award","s",",","three","Brit","Awards",",","three","American","Music","Award","s",",","four","MTV","Video","Music","Award","s","and","six","Ivor","Novello","Awards","."],"labels":["B-musical artist","O","O","O","O","O","O","B-award","I-award","I-award","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, award, musical instrument, musical artist, location, song, band, music genre, event, organization, country, person and O.\nSentence: Michael won various music awards including two Grammy Award s , three Brit Awards , three American Music Award s , four MTV Video Music Award s and six Ivor Novello Awards .","prompt_labels":"Michael(B-musical artist) won(O) various(O) music(O) awards(O) including(O) two(O) Grammy(B-award) Award(I-award) s(I-award) ,(O) three(O) Brit(B-award) Awards(I-award) ,(O) three(O) American(B-award) Music(I-award) Award(I-award) s(I-award) ,(O) four(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) s(I-award) and(O) six(O) Ivor(B-award) Novello(I-award) Awards(I-award) .(O)"}}
{"id":"57","dataset":"crossner_music","split":"train","label_list":["album","award","song","organization","location","band","musical artist","country","musical instrument","event","music genre","person"],"instance":{"id":"57","words":["This","style","emerged","in","the","United","States","in","the","early","and","mid-1980s",",","with","innovators","such","as","Queensrÿche",",","Fates","Warning",",","and","Dream","Theater","."],"labels":["O","O","O","O","O","B-country","I-country","O","O","O","O","O","O","O","O","O","O","B-band","O","B-band","I-band","O","O","B-band","I-band","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, award, song, organization, location, band, musical artist, country, musical instrument, event, music genre, person and O.\nSentence: This style emerged in the United States in the early and mid-1980s , with innovators such as Queensrÿche , Fates Warning , and Dream Theater .","prompt_labels":"This(O) style(O) emerged(O) in(O) the(O) United(B-country) States(I-country) in(O) the(O) early(O) and(O) mid-1980s(O) ,(O) with(O) innovators(O) such(O) as(O) Queensrÿche(B-band) ,(O) Fates(B-band) Warning(I-band) ,(O) and(O) Dream(B-band) Theater(I-band) .(O)"}}
{"id":"58","dataset":"crossner_music","split":"train","label_list":["song","musical instrument","album","band","location","music genre","event","organization","musical artist","person","country","award"],"instance":{"id":"58","words":["For","Mirrors",",","instead","of","working","with","previous","producers","Sandy","Pearlman","(","who","instead","went","on","to","manage","Black","Sabbath",")","and","Murray","Krugman",",","Blue","Öyster","Cult","chose","Tom","Werman",",","who","had","worked","with","acts","such","as","Cheap","Trick","and","Ted","Nugent","."],"labels":["O","B-album","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","B-band","I-band","O","O","B-person","I-person","O","B-band","I-band","I-band","O","B-person","I-person","O","O","O","O","O","O","O","O","B-band","I-band","O","B-musical artist","I-musical artist","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, musical instrument, album, band, location, music genre, event, organization, musical artist, person, country, award and O.\nSentence: For Mirrors , instead of working with previous producers Sandy Pearlman ( who instead went on to manage Black Sabbath ) and Murray Krugman , Blue Öyster Cult chose Tom Werman , who had worked with acts such as Cheap Trick and Ted Nugent .","prompt_labels":"For(O) Mirrors(B-album) ,(O) instead(O) of(O) working(O) with(O) previous(O) producers(O) Sandy(B-person) Pearlman(I-person) ((O) who(O) instead(O) went(O) on(O) to(O) manage(O) Black(B-band) Sabbath(I-band) )(O) and(O) Murray(B-person) Krugman(I-person) ,(O) Blue(B-band) Öyster(I-band) Cult(I-band) chose(O) Tom(B-person) Werman(I-person) ,(O) who(O) had(O) worked(O) with(O) acts(O) such(O) as(O) Cheap(B-band) Trick(I-band) and(O) Ted(B-musical artist) Nugent(I-musical artist) .(O)"}}
{"id":"59","dataset":"crossner_music","split":"train","label_list":["musical instrument","event","song","person","award","musical artist","organization","album","country","location","band","music genre"],"instance":{"id":"59","words":["The","self-titled","record","was","an","instant","success","both","critically","and","commercially",",","led","by","the","official","theme","song","for","UAAP","Season","71","of","the","UAAP","-","Puso","(","trans",":","Heart",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","B-organization","O","B-song","O","O","O","B-song","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, event, song, person, award, musical artist, organization, album, country, location, band, music genre and O.\nSentence: The self-titled record was an instant success both critically and commercially , led by the official theme song for UAAP Season 71 of the UAAP - Puso ( trans : Heart ) .","prompt_labels":"The(O) self-titled(O) record(O) was(O) an(O) instant(O) success(O) both(O) critically(O) and(O) commercially(O) ,(O) led(O) by(O) the(O) official(O) theme(O) song(O) for(O) UAAP(B-event) Season(I-event) 71(I-event) of(O) the(O) UAAP(B-organization) -(O) Puso(B-song) ((O) trans(O) :(O) Heart(B-song) )(O) .(O)"}}
{"id":"60","dataset":"crossner_music","split":"train","label_list":["band","event","album","musical instrument","location","award","person","musical artist","music genre","song","country","organization"],"instance":{"id":"60","words":["Examples","of","such","professional","groups","include","Straight","No","Chaser",",","Pentatonix",",","The","House","Jacks",",","Rockapella",",","Mosaic",",","Home","Free","and","M-pact","."],"labels":["O","O","O","O","O","O","B-band","I-band","I-band","O","B-band","O","B-band","I-band","I-band","O","B-band","O","B-band","O","B-band","I-band","O","B-band","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, event, album, musical instrument, location, award, person, musical artist, music genre, song, country, organization and O.\nSentence: Examples of such professional groups include Straight No Chaser , Pentatonix , The House Jacks , Rockapella , Mosaic , Home Free and M-pact .","prompt_labels":"Examples(O) of(O) such(O) professional(O) groups(O) include(O) Straight(B-band) No(I-band) Chaser(I-band) ,(O) Pentatonix(B-band) ,(O) The(B-band) House(I-band) Jacks(I-band) ,(O) Rockapella(B-band) ,(O) Mosaic(B-band) ,(O) Home(B-band) Free(I-band) and(O) M-pact(B-band) .(O)"}}
{"id":"61","dataset":"crossner_music","split":"train","label_list":["award","music genre","musical artist","event","album","musical instrument","organization","song","country","band","person","location"],"instance":{"id":"61","words":["Simon","has","won","12","Grammy","Award","s","(","one","of","them","a","Grammy","Lifetime","Achievement","Award",")","and","five","Grammy","Award","for","Album","of","the","Year","Grammy","nominations",",","the","most","recent","for","You","'re","the","One","in","2001","."],"labels":["B-musical artist","O","O","O","B-award","I-award","I-award","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","B-song","I-song","I-song","I-song","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, music genre, musical artist, event, album, musical instrument, organization, song, country, band, person, location and O.\nSentence: Simon has won 12 Grammy Award s ( one of them a Grammy Lifetime Achievement Award ) and five Grammy Award for Album of the Year Grammy nominations , the most recent for You 're the One in 2001 .","prompt_labels":"Simon(B-musical artist) has(O) won(O) 12(O) Grammy(B-award) Award(I-award) s(I-award) ((O) one(O) of(O) them(O) a(O) Grammy(B-award) Lifetime(I-award) Achievement(I-award) Award(I-award) )(O) and(O) five(O) Grammy(B-award) Award(I-award) for(I-award) Album(I-award) of(I-award) the(I-award) Year(I-award) Grammy(I-award) nominations(I-award) ,(O) the(O) most(O) recent(O) for(O) You(B-song) 're(I-song) the(I-song) One(I-song) in(O) 2001(O) .(O)"}}
{"id":"62","dataset":"crossner_music","split":"train","label_list":["band","musical instrument","event","musical artist","music genre","song","organization","award","person","location","album","country"],"instance":{"id":"62","words":["Brazil","Classics","!","--","redirects","here","--",",","the","label","'s","first","compilation","series",",","consists","of","seven","albums","surveying","genres","ranging","from","Samba","to","Tropicália",",","as","well","as","individual","artists","."],"labels":["B-album","I-album","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","O","B-music genre","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical instrument, event, musical artist, music genre, song, organization, award, person, location, album, country and O.\nSentence: Brazil Classics ! -- redirects here -- , the label 's first compilation series , consists of seven albums surveying genres ranging from Samba to Tropicália , as well as individual artists .","prompt_labels":"Brazil(B-album) Classics(I-album) !(O) --(O) redirects(O) here(O) --(O) ,(O) the(O) label(O) 's(O) first(O) compilation(O) series(O) ,(O) consists(O) of(O) seven(O) albums(O) surveying(O) genres(O) ranging(O) from(O) Samba(B-music genre) to(O) Tropicália(B-music genre) ,(O) as(O) well(O) as(O) individual(O) artists(O) .(O)"}}
{"id":"63","dataset":"crossner_music","split":"train","label_list":["event","album","person","country","award","organization","location","musical instrument","song","musical artist","music genre","band"],"instance":{"id":"63","words":["In","addition",",","a","vintage","siren",",","just","as","the","original","Boston","Garden","had","used",",","was","added","to","replace","the","end-of-period","horn","for","hockey","only",",","a","feature","of","the","Montreal","Canadiens",",","the","Bruins","'","arch-rivals",",","at","the","Montreal","Forum","(","now","the","Pepsi","Forum","shopping","centre",")","and","the","current","Bell","Centre","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","B-location","I-location","O","O","O","O","O","O","B-location","I-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, album, person, country, award, organization, location, musical instrument, song, musical artist, music genre, band and O.\nSentence: In addition , a vintage siren , just as the original Boston Garden had used , was added to replace the end-of-period horn for hockey only , a feature of the Montreal Canadiens , the Bruins ' arch-rivals , at the Montreal Forum ( now the Pepsi Forum shopping centre ) and the current Bell Centre .","prompt_labels":"In(O) addition(O) ,(O) a(O) vintage(O) siren(O) ,(O) just(O) as(O) the(O) original(O) Boston(B-location) Garden(I-location) had(O) used(O) ,(O) was(O) added(O) to(O) replace(O) the(O) end-of-period(O) horn(O) for(O) hockey(O) only(O) ,(O) a(O) feature(O) of(O) the(O) Montreal(O) Canadiens(O) ,(O) the(O) Bruins(O) '(O) arch-rivals(O) ,(O) at(O) the(O) Montreal(B-location) Forum(I-location) ((O) now(O) the(O) Pepsi(B-location) Forum(I-location) shopping(O) centre(O) )(O) and(O) the(O) current(O) Bell(B-location) Centre(I-location) .(O)"}}
{"id":"64","dataset":"crossner_music","split":"train","label_list":["musical artist","band","organization","person","musical instrument","music genre","song","location","award","event","country","album"],"instance":{"id":"64","words":["This","was","followed","by","a","series","of","small",",","intimate","gigs","at","UK","venues","such","as","Liverpool","'s","The","Cavern","Club",",","London","'s","Mean","Fiddler",",","and","Glasgow","'s","Barrowland","Ballroom","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","B-location","O","B-location","I-location","I-location","O","B-location","O","B-location","I-location","O","O","B-location","O","B-location","I-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, band, organization, person, musical instrument, music genre, song, location, award, event, country, album and O.\nSentence: This was followed by a series of small , intimate gigs at UK venues such as Liverpool 's The Cavern Club , London 's Mean Fiddler , and Glasgow 's Barrowland Ballroom .","prompt_labels":"This(O) was(O) followed(O) by(O) a(O) series(O) of(O) small(O) ,(O) intimate(O) gigs(O) at(O) UK(B-country) venues(O) such(O) as(O) Liverpool(B-location) 's(O) The(B-location) Cavern(I-location) Club(I-location) ,(O) London(B-location) 's(O) Mean(B-location) Fiddler(I-location) ,(O) and(O) Glasgow(B-location) 's(O) Barrowland(B-location) Ballroom(I-location) .(O)"}}
{"id":"65","dataset":"crossner_music","split":"train","label_list":["person","band","location","song","musical instrument","album","award","event","country","musical artist","music genre","organization"],"instance":{"id":"65","words":["Attracting","over","200,000","fans",",","Black","Sabbath","appeared","alongside","popular","1970s","rock","and","pop","bands","Deep","Purple",",","Eagles",",","Emerson",",","Lake","&","Palmer",",","Rare","Earth",",","Seals","and","Crofts",",","Black","Oak","Arkansas",",","and","Earth",",","Wind","&","Fire","."],"labels":["O","O","O","O","O","B-band","I-band","O","O","O","O","B-music genre","O","B-music genre","O","B-band","I-band","O","B-band","O","B-band","I-band","I-band","I-band","I-band","O","B-band","I-band","O","B-band","I-band","I-band","O","B-band","I-band","I-band","O","O","B-band","I-band","I-band","I-band","I-band","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, band, location, song, musical instrument, album, award, event, country, musical artist, music genre, organization and O.\nSentence: Attracting over 200,000 fans , Black Sabbath appeared alongside popular 1970s rock and pop bands Deep Purple , Eagles , Emerson , Lake & Palmer , Rare Earth , Seals and Crofts , Black Oak Arkansas , and Earth , Wind & Fire .","prompt_labels":"Attracting(O) over(O) 200,000(O) fans(O) ,(O) Black(B-band) Sabbath(I-band) appeared(O) alongside(O) popular(O) 1970s(O) rock(B-music genre) and(O) pop(B-music genre) bands(O) Deep(B-band) Purple(I-band) ,(O) Eagles(B-band) ,(O) Emerson(B-band) ,(I-band) Lake(I-band) &(I-band) Palmer(I-band) ,(O) Rare(B-band) Earth(I-band) ,(O) Seals(B-band) and(I-band) Crofts(I-band) ,(O) Black(B-band) Oak(I-band) Arkansas(I-band) ,(O) and(O) Earth(B-band) ,(I-band) Wind(I-band) &(I-band) Fire(I-band) .(O)"}}
{"id":"66","dataset":"crossner_music","split":"train","label_list":["country","song","award","musical instrument","band","event","musical artist","person","location","music genre","album","organization"],"instance":{"id":"66","words":["Staind","has","recorded","seven","studio","albums",":","Tormented","(","1996",")",",","Dysfunction","(","1999",")",",","Break","the","Cycle","(","2001",")",",","14","Shades","of","Grey","(","2003",")",",","Chapter","V","(","2005",")",",","The","Illusion","of","Progress","(","2008",")",",","and","Staind","(","2011",")","."],"labels":["B-musical artist","O","O","O","O","O","O","B-album","O","O","O","O","B-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","B-album","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, song, award, musical instrument, band, event, musical artist, person, location, music genre, album, organization and O.\nSentence: Staind has recorded seven studio albums : Tormented ( 1996 ) , Dysfunction ( 1999 ) , Break the Cycle ( 2001 ) , 14 Shades of Grey ( 2003 ) , Chapter V ( 2005 ) , The Illusion of Progress ( 2008 ) , and Staind ( 2011 ) .","prompt_labels":"Staind(B-musical artist) has(O) recorded(O) seven(O) studio(O) albums(O) :(O) Tormented(B-album) ((O) 1996(O) )(O) ,(O) Dysfunction(B-album) ((O) 1999(O) )(O) ,(O) Break(B-album) the(I-album) Cycle(I-album) ((O) 2001(O) )(O) ,(O) 14(B-album) Shades(I-album) of(I-album) Grey(I-album) ((O) 2003(O) )(O) ,(O) Chapter(B-album) V(I-album) ((O) 2005(O) )(O) ,(O) The(B-album) Illusion(I-album) of(I-album) Progress(I-album) ((O) 2008(O) )(O) ,(O) and(O) Staind(B-album) ((O) 2011(O) )(O) .(O)"}}
{"id":"67","dataset":"crossner_music","split":"train","label_list":["album","musical instrument","country","musical artist","award","song","organization","event","music genre","location","band","person"],"instance":{"id":"67","words":["Following","the","end","of","Nirvana",",","Novoselic","worked","on","completing","the","With","the","Lights","Out","box","set","and","From","the","Muddy","Banks","of","the","Wishkah","album",",","as","well","as","pushing","for","release","of","a","Nevermind","."],"labels":["O","O","O","O","B-band","O","B-musical artist","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","B-album","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical instrument, country, musical artist, award, song, organization, event, music genre, location, band, person and O.\nSentence: Following the end of Nirvana , Novoselic worked on completing the With the Lights Out box set and From the Muddy Banks of the Wishkah album , as well as pushing for release of a Nevermind .","prompt_labels":"Following(O) the(O) end(O) of(O) Nirvana(B-band) ,(O) Novoselic(B-musical artist) worked(O) on(O) completing(O) the(O) With(B-album) the(I-album) Lights(I-album) Out(I-album) box(O) set(O) and(O) From(B-album) the(I-album) Muddy(I-album) Banks(I-album) of(I-album) the(I-album) Wishkah(I-album) album(O) ,(O) as(O) well(O) as(O) pushing(O) for(O) release(O) of(O) a(O) Nevermind(B-album) .(O)"}}
{"id":"68","dataset":"crossner_music","split":"train","label_list":["music genre","musical instrument","band","person","country","organization","album","event","award","location","song","musical artist"],"instance":{"id":"68","words":["However",",","some","bands","were","created","around","the","talent","of","a","songwriter","within","the","group","like","Gary","Barlow","of","Take","That","or","Tony","Mortimer","of","East","17","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-band","I-band","O","B-musical artist","I-musical artist","O","B-band","I-band","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical instrument, band, person, country, organization, album, event, award, location, song, musical artist and O.\nSentence: However , some bands were created around the talent of a songwriter within the group like Gary Barlow of Take That or Tony Mortimer of East 17 .","prompt_labels":"However(O) ,(O) some(O) bands(O) were(O) created(O) around(O) the(O) talent(O) of(O) a(O) songwriter(O) within(O) the(O) group(O) like(O) Gary(B-musical artist) Barlow(I-musical artist) of(O) Take(B-band) That(I-band) or(O) Tony(B-musical artist) Mortimer(I-musical artist) of(O) East(B-band) 17(I-band) .(O)"}}
{"id":"69","dataset":"crossner_music","split":"train","label_list":["person","location","musical artist","musical instrument","song","organization","award","event","country","band","album","music genre"],"instance":{"id":"69","words":["Since","then","the","band","have","released","five","albums",":","In","It","for","the","Money","(","1997",")",",","Supergrass","(","1999",")",",","Life","on","Other","Planets","(","2002",")",",","Road","to","Rouen","(","2005",")","and","Diamond","Hoo","Ha","(","2008",")",",","as","well","as","a","decade-ending","compilation","called","Supergrass","Is","10","(","2004",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","I-album","O","O","O","O","B-album","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, musical artist, musical instrument, song, organization, award, event, country, band, album, music genre and O.\nSentence: Since then the band have released five albums : In It for the Money ( 1997 ) , Supergrass ( 1999 ) , Life on Other Planets ( 2002 ) , Road to Rouen ( 2005 ) and Diamond Hoo Ha ( 2008 ) , as well as a decade-ending compilation called Supergrass Is 10 ( 2004 ) .","prompt_labels":"Since(O) then(O) the(O) band(O) have(O) released(O) five(O) albums(O) :(O) In(B-album) It(I-album) for(I-album) the(I-album) Money(I-album) ((O) 1997(O) )(O) ,(O) Supergrass(B-album) ((O) 1999(O) )(O) ,(O) Life(B-album) on(I-album) Other(I-album) Planets(I-album) ((O) 2002(O) )(O) ,(O) Road(B-album) to(I-album) Rouen(I-album) ((O) 2005(O) )(O) and(O) Diamond(B-album) Hoo(I-album) Ha(I-album) ((O) 2008(O) )(O) ,(O) as(O) well(O) as(O) a(O) decade-ending(O) compilation(O) called(O) Supergrass(B-album) Is(I-album) 10(I-album) ((O) 2004(O) )(O) .(O)"}}
{"id":"70","dataset":"crossner_music","split":"train","label_list":["event","country","band","award","organization","music genre","musical artist","album","musical instrument","person","song","location"],"instance":{"id":"70","words":["At","the","33rd","Academy","Awards",",","The","Apartment","was","nominated","for","ten","awards","and","won","five",",","including","Academy","Award","for","Best","Picture",",","Academy","Award","for","Best","Director",",","and","Academy","Award","for","Best","Original","Screenplay","."],"labels":["O","O","B-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, band, award, organization, music genre, musical artist, album, musical instrument, person, song, location and O.\nSentence: At the 33rd Academy Awards , The Apartment was nominated for ten awards and won five , including Academy Award for Best Picture , Academy Award for Best Director , and Academy Award for Best Original Screenplay .","prompt_labels":"At(O) the(O) 33rd(B-award) Academy(I-award) Awards(I-award) ,(O) The(O) Apartment(O) was(O) nominated(O) for(O) ten(O) awards(O) and(O) won(O) five(O) ,(O) including(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) ,(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Screenplay(I-award) .(O)"}}
{"id":"71","dataset":"crossner_music","split":"train","label_list":["song","musical artist","person","event","country","music genre","album","musical instrument","award","organization","location","band"],"instance":{"id":"71","words":["Dream","On",",","I","Feel","Loved",",","Freelove","and","Goodnight","Lovers","were","released","as","singles","in","2001","and","2002","."],"labels":["B-song","I-song","O","B-song","I-song","I-song","O","B-song","O","B-song","I-song","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, musical artist, person, event, country, music genre, album, musical instrument, award, organization, location, band and O.\nSentence: Dream On , I Feel Loved , Freelove and Goodnight Lovers were released as singles in 2001 and 2002 .","prompt_labels":"Dream(B-song) On(I-song) ,(O) I(B-song) Feel(I-song) Loved(I-song) ,(O) Freelove(B-song) and(O) Goodnight(B-song) Lovers(I-song) were(O) released(O) as(O) singles(O) in(O) 2001(O) and(O) 2002(O) .(O)"}}
{"id":"72","dataset":"crossner_music","split":"train","label_list":["organization","song","music genre","musical artist","musical instrument","award","band","album","person","country","event","location"],"instance":{"id":"72","words":["He","has","musically","encompassed","Folk","music",",","funk",",","Soul","music",",","Hip","hop","music",",","Electronic","music",",","alternative","rock",",","Country","music",",","and","psychedelia","."],"labels":["O","O","O","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","B-music genre","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, song, music genre, musical artist, musical instrument, award, band, album, person, country, event, location and O.\nSentence: He has musically encompassed Folk music , funk , Soul music , Hip hop music , Electronic music , alternative rock , Country music , and psychedelia .","prompt_labels":"He(O) has(O) musically(O) encompassed(O) Folk(B-music genre) music(I-music genre) ,(O) funk(B-music genre) ,(O) Soul(B-music genre) music(I-music genre) ,(O) Hip(B-music genre) hop(I-music genre) music(I-music genre) ,(O) Electronic(B-music genre) music(I-music genre) ,(O) alternative(B-music genre) rock(I-music genre) ,(O) Country(B-music genre) music(I-music genre) ,(O) and(O) psychedelia(B-music genre) .(O)"}}
{"id":"73","dataset":"crossner_music","split":"train","label_list":["event","musical artist","country","song","organization","award","location","album","musical instrument","person","band","music genre"],"instance":{"id":"73","words":["Coldcut","returned","with","the","single","Everything","Is","Under","Control","at","the","end","of","2005",",","featuring","Jon","Spencer","(","of","Jon","Spencer","Blues","Explosion",")","and","Mike","Ladd","."],"labels":["B-band","O","O","O","O","B-song","I-song","I-song","I-song","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","B-band","I-band","I-band","I-band","O","O","B-musical artist","I-musical artist","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical artist, country, song, organization, award, location, album, musical instrument, person, band, music genre and O.\nSentence: Coldcut returned with the single Everything Is Under Control at the end of 2005 , featuring Jon Spencer ( of Jon Spencer Blues Explosion ) and Mike Ladd .","prompt_labels":"Coldcut(B-band) returned(O) with(O) the(O) single(O) Everything(B-song) Is(I-song) Under(I-song) Control(I-song) at(O) the(O) end(O) of(O) 2005(O) ,(O) featuring(O) Jon(B-musical artist) Spencer(I-musical artist) ((O) of(O) Jon(B-band) Spencer(I-band) Blues(I-band) Explosion(I-band) )(O) and(O) Mike(B-musical artist) Ladd(I-musical artist) .(O)"}}
{"id":"74","dataset":"crossner_music","split":"train","label_list":["song","music genre","organization","event","album","band","person","musical artist","country","location","award","musical instrument"],"instance":{"id":"74","words":["The","series","featured","five","albums","of","Masada","themes","including","Masada","Guitars","by","Marc","Ribot",",","Bill","Frisell",",","and","Tim","Sparks",";","Masada","Recital","by","Mark","Feldman","and","Sylvie","Courvoisier",";","Masada","Rock","by","Rashanim",";","and","two","albums","featuring","various","artists",",","Voices","in","the","Wilderness","and","The","Unknown","Masada","."],"labels":["O","O","O","O","O","O","O","O","O","B-album","I-album","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","B-album","I-album","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-album","I-album","O","B-musical artist","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","B-album","I-album","I-album","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, music genre, organization, event, album, band, person, musical artist, country, location, award, musical instrument and O.\nSentence: The series featured five albums of Masada themes including Masada Guitars by Marc Ribot , Bill Frisell , and Tim Sparks ; Masada Recital by Mark Feldman and Sylvie Courvoisier ; Masada Rock by Rashanim ; and two albums featuring various artists , Voices in the Wilderness and The Unknown Masada .","prompt_labels":"The(O) series(O) featured(O) five(O) albums(O) of(O) Masada(O) themes(O) including(O) Masada(B-album) Guitars(I-album) by(O) Marc(B-musical artist) Ribot(I-musical artist) ,(O) Bill(B-musical artist) Frisell(I-musical artist) ,(O) and(O) Tim(B-musical artist) Sparks(I-musical artist) ;(O) Masada(B-album) Recital(I-album) by(O) Mark(B-musical artist) Feldman(I-musical artist) and(O) Sylvie(B-musical artist) Courvoisier(I-musical artist) ;(O) Masada(B-album) Rock(I-album) by(O) Rashanim(B-musical artist) ;(O) and(O) two(O) albums(O) featuring(O) various(O) artists(O) ,(O) Voices(B-album) in(I-album) the(I-album) Wilderness(I-album) and(O) The(B-album) Unknown(I-album) Masada(I-album) .(O)"}}
{"id":"75","dataset":"crossner_music","split":"train","label_list":["music genre","location","musical artist","organization","song","award","band","musical instrument","person","event","country","album"],"instance":{"id":"75","words":["Depeche","Mode","contributed","their","cover","of","the","U2","song","So","Cruel","to","the","tribute","album","AHK-toong","BAY-bi","Covered","honouring","the","20th","anniversary","of","Achtung","Baby",",","a","1991","album","by","U2","."],"labels":["B-band","O","O","O","O","O","O","B-band","O","B-song","I-song","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","B-album","I-album","O","O","O","O","O","B-band","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, location, musical artist, organization, song, award, band, musical instrument, person, event, country, album and O.\nSentence: Depeche Mode contributed their cover of the U2 song So Cruel to the tribute album AHK-toong BAY-bi Covered honouring the 20th anniversary of Achtung Baby , a 1991 album by U2 .","prompt_labels":"Depeche(B-band) Mode(O) contributed(O) their(O) cover(O) of(O) the(O) U2(B-band) song(O) So(B-song) Cruel(I-song) to(O) the(O) tribute(O) album(O) AHK-toong(B-album) BAY-bi(I-album) Covered(I-album) honouring(O) the(O) 20th(O) anniversary(O) of(O) Achtung(B-album) Baby(I-album) ,(O) a(O) 1991(O) album(O) by(O) U2(B-band) .(O)"}}
{"id":"76","dataset":"crossner_music","split":"train","label_list":["song","music genre","person","musical instrument","organization","album","band","location","musical artist","country","event","award"],"instance":{"id":"76","words":["The","stadium","also","hosted","such","events","as","1973","Summer","Universiade",",","1986","Goodwill","Games","and","2013","World","Championships","in","Athletics","."],"labels":["O","O","O","O","O","O","O","B-event","I-event","I-event","O","B-event","I-event","I-event","O","B-event","I-event","I-event","I-event","I-event","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, music genre, person, musical instrument, organization, album, band, location, musical artist, country, event, award and O.\nSentence: The stadium also hosted such events as 1973 Summer Universiade , 1986 Goodwill Games and 2013 World Championships in Athletics .","prompt_labels":"The(O) stadium(O) also(O) hosted(O) such(O) events(O) as(O) 1973(B-event) Summer(I-event) Universiade(I-event) ,(O) 1986(B-event) Goodwill(I-event) Games(I-event) and(O) 2013(B-event) World(I-event) Championships(I-event) in(I-event) Athletics(I-event) .(O)"}}
{"id":"77","dataset":"crossner_music","split":"train","label_list":["music genre","organization","country","song","event","album","location","musical instrument","person","musical artist","band","award"],"instance":{"id":"77","words":["In","1995",",","Dookie","won","the","Grammy","Award","for","Grammy","Award","for","Best","Alternative","Music","Album","and","the","band","was","nominated","for","nine","MTV","Video","Music","Award","s","including","Video","of","the","Year","."],"labels":["O","O","O","B-album","O","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, organization, country, song, event, album, location, musical instrument, person, musical artist, band, award and O.\nSentence: In 1995 , Dookie won the Grammy Award for Grammy Award for Best Alternative Music Album and the band was nominated for nine MTV Video Music Award s including Video of the Year .","prompt_labels":"In(O) 1995(O) ,(O) Dookie(B-album) won(O) the(O) Grammy(B-award) Award(I-award) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Alternative(I-award) Music(I-award) Album(I-award) and(O) the(O) band(O) was(O) nominated(O) for(O) nine(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) s(O) including(O) Video(B-award) of(I-award) the(I-award) Year(I-award) .(O)"}}
{"id":"78","dataset":"crossner_music","split":"train","label_list":["band","organization","musical instrument","album","song","event","musical artist","music genre","location","award","country","person"],"instance":{"id":"78","words":["He","also","became","progressively","more","involved","with","Irish-American","organizations",":","in","1908","he","joined","the","Friendly","Sons","of","St.","Patrick","(","becoming","president","in","1916",")",",","the","oldest","Irish","association","in","New","York",",","and","in","1911","he","became","a","member","of","the","American","Irish","Historical","Society","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, organization, musical instrument, album, song, event, musical artist, music genre, location, award, country, person and O.\nSentence: He also became progressively more involved with Irish-American organizations : in 1908 he joined the Friendly Sons of St. Patrick ( becoming president in 1916 ) , the oldest Irish association in New York , and in 1911 he became a member of the American Irish Historical Society .","prompt_labels":"He(O) also(O) became(O) progressively(O) more(O) involved(O) with(O) Irish-American(O) organizations(O) :(O) in(O) 1908(O) he(O) joined(O) the(O) Friendly(B-organization) Sons(I-organization) of(I-organization) St.(I-organization) Patrick(I-organization) ((O) becoming(O) president(O) in(O) 1916(O) )(O) ,(O) the(O) oldest(O) Irish(O) association(O) in(O) New(B-location) York(I-location) ,(O) and(O) in(O) 1911(O) he(O) became(O) a(O) member(O) of(O) the(O) American(B-organization) Irish(I-organization) Historical(I-organization) Society(I-organization) .(O)"}}
{"id":"79","dataset":"crossner_music","split":"train","label_list":["person","award","song","event","album","country","location","musical instrument","organization","band","musical artist","music genre"],"instance":{"id":"79","words":["The","Benedictines","and","their","offshoots","(","Cistercians","and","Trappists","among","them",")",",","the","Premonstratensians",",","and","the","military","orders","distinguish","between","conventual","and","simple","or","obedientiary","priories","."],"labels":["O","B-organization","O","O","O","O","B-organization","O","B-organization","O","O","O","O","O","B-organization","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, song, event, album, country, location, musical instrument, organization, band, musical artist, music genre and O.\nSentence: The Benedictines and their offshoots ( Cistercians and Trappists among them ) , the Premonstratensians , and the military orders distinguish between conventual and simple or obedientiary priories .","prompt_labels":"The(O) Benedictines(B-organization) and(O) their(O) offshoots(O) ((O) Cistercians(B-organization) and(O) Trappists(B-organization) among(O) them(O) )(O) ,(O) the(O) Premonstratensians(B-organization) ,(O) and(O) the(O) military(B-organization) orders(I-organization) distinguish(O) between(O) conventual(O) and(O) simple(O) or(O) obedientiary(O) priories(O) .(O)"}}
{"id":"80","dataset":"crossner_music","split":"train","label_list":["award","musical artist","event","song","country","band","location","music genre","musical instrument","album","person","organization"],"instance":{"id":"80","words":["Poland","has","always","been","a","very","open","country","to","new","music","genres","and","even","before","the","fall","of","the","communism",",","music","styles","like","rock",",","Heavy","metal","music",",","jazz",",","Electronic","music",",","and","New","wave","music","were","well-known","."],"labels":["B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O","O","O","O","B-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","O","B-music genre","I-music genre","I-music genre","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical artist, event, song, country, band, location, music genre, musical instrument, album, person, organization and O.\nSentence: Poland has always been a very open country to new music genres and even before the fall of the communism , music styles like rock , Heavy metal music , jazz , Electronic music , and New wave music were well-known .","prompt_labels":"Poland(B-country) has(O) always(O) been(O) a(O) very(O) open(O) country(O) to(O) new(O) music(O) genres(O) and(O) even(O) before(O) the(O) fall(B-event) of(I-event) the(I-event) communism(I-event) ,(O) music(O) styles(O) like(O) rock(B-music genre) ,(O) Heavy(B-music genre) metal(I-music genre) music(I-music genre) ,(O) jazz(B-music genre) ,(O) Electronic(B-music genre) music(I-music genre) ,(O) and(O) New(B-music genre) wave(I-music genre) music(I-music genre) were(O) well-known(O) .(O)"}}
{"id":"81","dataset":"crossner_music","split":"train","label_list":["song","band","album","musical artist","person","event","country","musical instrument","award","music genre","location","organization"],"instance":{"id":"81","words":["Blues","subgenres","include","country","blues",",","such","as","Delta","blues","and","Piedmont","blues",",","as","well","as","urban","blues","styles","such","as","Chicago","blues","and","West","Coast","blues","."],"labels":["B-music genre","O","O","B-music genre","I-music genre","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","O","B-music genre","I-music genre","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, band, album, musical artist, person, event, country, musical instrument, award, music genre, location, organization and O.\nSentence: Blues subgenres include country blues , such as Delta blues and Piedmont blues , as well as urban blues styles such as Chicago blues and West Coast blues .","prompt_labels":"Blues(B-music genre) subgenres(O) include(O) country(B-music genre) blues(I-music genre) ,(O) such(O) as(O) Delta(B-music genre) blues(I-music genre) and(O) Piedmont(B-music genre) blues(I-music genre) ,(O) as(O) well(O) as(O) urban(B-music genre) blues(I-music genre) styles(O) such(O) as(O) Chicago(B-music genre) blues(I-music genre) and(O) West(B-music genre) Coast(I-music genre) blues(I-music genre) .(O)"}}
{"id":"82","dataset":"crossner_music","split":"train","label_list":["music genre","person","award","country","album","musical artist","location","event","song","organization","musical instrument","band"],"instance":{"id":"82","words":["The","musical","was","an","immediate","hit",",","winning","Tony","Award","s","for","Tony","Award","for","Best","Musical",",","Tony","Award","for","Best","Actress","in","a","Musical","(","for","Lawrence",")","and","Tony","Award","for","Best","Featured","Actor","in","a","Musical","(","for","Brynner",")","."],"labels":["O","O","O","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","B-musical artist","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","B-musical artist","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, person, award, country, album, musical artist, location, event, song, organization, musical instrument, band and O.\nSentence: The musical was an immediate hit , winning Tony Award s for Tony Award for Best Musical , Tony Award for Best Actress in a Musical ( for Lawrence ) and Tony Award for Best Featured Actor in a Musical ( for Brynner ) .","prompt_labels":"The(O) musical(O) was(O) an(O) immediate(O) hit(O) ,(O) winning(O) Tony(B-award) Award(I-award) s(O) for(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Musical(I-award) ,(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) in(I-award) a(I-award) Musical(I-award) ((O) for(O) Lawrence(B-musical artist) )(O) and(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Featured(I-award) Actor(I-award) in(I-award) a(I-award) Musical(I-award) ((O) for(O) Brynner(B-musical artist) )(O) .(O)"}}
{"id":"83","dataset":"crossner_music","split":"train","label_list":["music genre","location","musical instrument","award","band","musical artist","event","person","album","organization","song","country"],"instance":{"id":"83","words":["In","1995",",","Nas","did","guest","performances","on","the","albums","Doe","or","Die","by","AZ",",","The","Infamous","by","Mobb","Deep",",","Only","Built","4","Cuban","Linx","by","Raekwon","and","4,5,6","by","Kool","G","Rap","."],"labels":["O","O","O","B-musical artist","O","O","O","O","O","O","B-album","I-album","I-album","O","B-musical artist","O","B-album","I-album","O","B-band","I-band","O","B-album","I-album","I-album","I-album","I-album","O","B-musical artist","O","B-album","O","B-musical artist","I-musical artist","I-musical artist","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, location, musical instrument, award, band, musical artist, event, person, album, organization, song, country and O.\nSentence: In 1995 , Nas did guest performances on the albums Doe or Die by AZ , The Infamous by Mobb Deep , Only Built 4 Cuban Linx by Raekwon and 4,5,6 by Kool G Rap .","prompt_labels":"In(O) 1995(O) ,(O) Nas(B-musical artist) did(O) guest(O) performances(O) on(O) the(O) albums(O) Doe(B-album) or(I-album) Die(I-album) by(O) AZ(B-musical artist) ,(O) The(B-album) Infamous(I-album) by(O) Mobb(B-band) Deep(I-band) ,(O) Only(B-album) Built(I-album) 4(I-album) Cuban(I-album) Linx(I-album) by(O) Raekwon(B-musical artist) and(O) 4,5,6(B-album) by(O) Kool(B-musical artist) G(I-musical artist) Rap(I-musical artist) .(O)"}}
{"id":"84","dataset":"crossner_music","split":"train","label_list":["award","country","band","location","person","musical artist","musical instrument","event","music genre","organization","album","song"],"instance":{"id":"84","words":["In","1995",",","he","guested","on","two","tracks","on","Tom","Cochrane","'","s","Ragged","Ass","Road","album","and","then","in","1996","on","I","Mother","Earth","'","s","Like","a","Girl","from","the","Scenery","and","Fish","album","."],"labels":["O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","B-album","I-album","I-album","O","O","O","O","O","O","B-band","I-band","I-band","O","O","B-song","I-song","I-song","O","O","B-album","I-album","I-album","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, band, location, person, musical artist, musical instrument, event, music genre, organization, album, song and O.\nSentence: In 1995 , he guested on two tracks on Tom Cochrane ' s Ragged Ass Road album and then in 1996 on I Mother Earth ' s Like a Girl from the Scenery and Fish album .","prompt_labels":"In(O) 1995(O) ,(O) he(O) guested(O) on(O) two(O) tracks(O) on(O) Tom(B-musical artist) Cochrane(I-musical artist) '(O) s(O) Ragged(B-album) Ass(I-album) Road(I-album) album(O) and(O) then(O) in(O) 1996(O) on(O) I(B-band) Mother(I-band) Earth(I-band) '(O) s(O) Like(B-song) a(I-song) Girl(I-song) from(O) the(O) Scenery(B-album) and(I-album) Fish(I-album) album(O) .(O)"}}
{"id":"85","dataset":"crossner_music","split":"train","label_list":["person","band","album","musical instrument","country","song","musical artist","award","music genre","location","organization","event"],"instance":{"id":"85","words":["She","released","her","first","Spanish","language","album",",","Mi","Plan",",","in","2009",",","which","won","her","a","Latin","Grammy","Award","for","Latin","Grammy","Award","for","Best","Female","Pop","Vocal","Album","."],"labels":["O","O","O","O","O","O","O","O","B-album","I-album","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, band, album, musical instrument, country, song, musical artist, award, music genre, location, organization, event and O.\nSentence: She released her first Spanish language album , Mi Plan , in 2009 , which won her a Latin Grammy Award for Latin Grammy Award for Best Female Pop Vocal Album .","prompt_labels":"She(O) released(O) her(O) first(O) Spanish(O) language(O) album(O) ,(O) Mi(B-album) Plan(I-album) ,(O) in(O) 2009(O) ,(O) which(O) won(O) her(O) a(O) Latin(B-award) Grammy(I-award) Award(I-award) for(I-award) Latin(I-award) Grammy(I-award) Award(I-award) for(I-award) Best(I-award) Female(I-award) Pop(I-award) Vocal(I-award) Album(I-award) .(O)"}}
{"id":"86","dataset":"crossner_music","split":"train","label_list":["person","musical instrument","award","country","organization","location","event","music genre","song","album","musical artist","band"],"instance":{"id":"86","words":["Bands","sponsored","by","factories","include","The","Black","Dyke","Mills","Band",",","Yorkshire","Imperial","Band","(","originally","the","Yorkshire","Copperworks","Band",")",",","Foden","'s","sponsored","by","the","truck","manufacturer",",","Fairey","Band","sponsored","by","the","aircraft","manufacturer",",","and","Leyland","Band","sponsored","by","the","vehicle","manufacturer","."],"labels":["O","O","O","O","O","O","B-band","I-band","I-band","I-band","O","B-band","I-band","I-band","O","O","O","B-band","I-band","I-band","O","O","B-band","I-band","O","O","O","O","O","O","B-band","I-band","O","O","O","O","O","O","O","B-band","I-band","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, musical instrument, award, country, organization, location, event, music genre, song, album, musical artist, band and O.\nSentence: Bands sponsored by factories include The Black Dyke Mills Band , Yorkshire Imperial Band ( originally the Yorkshire Copperworks Band ) , Foden 's sponsored by the truck manufacturer , Fairey Band sponsored by the aircraft manufacturer , and Leyland Band sponsored by the vehicle manufacturer .","prompt_labels":"Bands(O) sponsored(O) by(O) factories(O) include(O) The(O) Black(B-band) Dyke(I-band) Mills(I-band) Band(I-band) ,(O) Yorkshire(B-band) Imperial(I-band) Band(I-band) ((O) originally(O) the(O) Yorkshire(B-band) Copperworks(I-band) Band(I-band) )(O) ,(O) Foden(B-band) 's(I-band) sponsored(O) by(O) the(O) truck(O) manufacturer(O) ,(O) Fairey(B-band) Band(I-band) sponsored(O) by(O) the(O) aircraft(O) manufacturer(O) ,(O) and(O) Leyland(B-band) Band(I-band) sponsored(O) by(O) the(O) vehicle(O) manufacturer(O) .(O)"}}
{"id":"87","dataset":"crossner_music","split":"train","label_list":["musical instrument","album","person","organization","band","music genre","song","award","location","event","country","musical artist"],"instance":{"id":"87","words":["He","has","been","involved","in","charitable","work",",","including","ONE","Campaign",",","H2O","Africa","Foundation",",","Feeding","America",",","and","Water.org","."],"labels":["O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","O","B-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, album, person, organization, band, music genre, song, award, location, event, country, musical artist and O.\nSentence: He has been involved in charitable work , including ONE Campaign , H2O Africa Foundation , Feeding America , and Water.org .","prompt_labels":"He(O) has(O) been(O) involved(O) in(O) charitable(O) work(O) ,(O) including(O) ONE(B-organization) Campaign(I-organization) ,(O) H2O(B-organization) Africa(I-organization) Foundation(I-organization) ,(O) Feeding(B-organization) America(I-organization) ,(O) and(O) Water.org(B-organization) .(O)"}}
{"id":"88","dataset":"crossner_music","split":"train","label_list":["person","band","album","organization","event","award","musical instrument","music genre","country","location","song","musical artist"],"instance":{"id":"88","words":["Prestwich","joined","Little","River","Band","in","1984","and","appeared","on","the","albums",",","Playing","to","Win","and","No","Reins",",","before","departing","in","1986","to","join","Farnham","'s","touring","band","."],"labels":["O","O","B-band","I-band","I-band","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","B-album","I-album","O","O","O","O","O","O","O","B-musical artist","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, band, album, organization, event, award, musical instrument, music genre, country, location, song, musical artist and O.\nSentence: Prestwich joined Little River Band in 1984 and appeared on the albums , Playing to Win and No Reins , before departing in 1986 to join Farnham 's touring band .","prompt_labels":"Prestwich(O) joined(O) Little(B-band) River(I-band) Band(I-band) in(O) 1984(O) and(O) appeared(O) on(O) the(O) albums(O) ,(O) Playing(B-album) to(I-album) Win(I-album) and(O) No(B-album) Reins(I-album) ,(O) before(O) departing(O) in(O) 1986(O) to(O) join(O) Farnham(B-musical artist) 's(O) touring(O) band(O) .(O)"}}
{"id":"89","dataset":"crossner_music","split":"train","label_list":["album","award","person","organization","musical artist","band","country","song","location","music genre","musical instrument","event"],"instance":{"id":"89","words":["Their","debut","album","The","Magnificent","Moodies",",","produced","by","Denny","Cordell","with","a","strong","Beat","music","/","Rhythm","and","blues","flavour",",","was","released","on","Decca","in","mono","only","in","1965","."],"labels":["O","O","O","B-album","I-album","I-album","O","O","O","B-musical artist","I-musical artist","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","O","O","O","O","B-organization","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, award, person, organization, musical artist, band, country, song, location, music genre, musical instrument, event and O.\nSentence: Their debut album The Magnificent Moodies , produced by Denny Cordell with a strong Beat music / Rhythm and blues flavour , was released on Decca in mono only in 1965 .","prompt_labels":"Their(O) debut(O) album(O) The(B-album) Magnificent(I-album) Moodies(I-album) ,(O) produced(O) by(O) Denny(B-musical artist) Cordell(I-musical artist) with(O) a(O) strong(O) Beat(B-music genre) music(I-music genre) /(O) Rhythm(B-music genre) and(I-music genre) blues(I-music genre) flavour(O) ,(O) was(O) released(O) on(O) Decca(B-organization) in(O) mono(O) only(O) in(O) 1965(O) .(O)"}}
{"id":"90","dataset":"crossner_music","split":"train","label_list":["music genre","song","event","album","organization","musical instrument","location","country","award","person","band","musical artist"],"instance":{"id":"90","words":["According","to","the","band","'s","biographer","Dave","Lewis",",","while","the","barnstorming","effect","of","the","early","era","was","now","levelling","off",",","and","though","devoid","of","the","electricity","of","Zeppelin","I","and","Led","Zeppelin","II",",","the","sheer","diversity","of","Led","Zeppelin","III",",","and","lacking","the","classic","status","of","Led","Zeppelin","IV",",","Houses","of","the","Holy","nevertheless","found","its","rightful","niche","."],"labels":["O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","O","B-album","I-album","I-album","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O","B-album","I-album","I-album","O","B-album","I-album","I-album","I-album","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, song, event, album, organization, musical instrument, location, country, award, person, band, musical artist and O.\nSentence: According to the band 's biographer Dave Lewis , while the barnstorming effect of the early era was now levelling off , and though devoid of the electricity of Zeppelin I and Led Zeppelin II , the sheer diversity of Led Zeppelin III , and lacking the classic status of Led Zeppelin IV , Houses of the Holy nevertheless found its rightful niche .","prompt_labels":"According(O) to(O) the(O) band(O) 's(O) biographer(O) Dave(B-musical artist) Lewis(I-musical artist) ,(O) while(O) the(O) barnstorming(O) effect(O) of(O) the(O) early(O) era(O) was(O) now(O) levelling(O) off(O) ,(O) and(O) though(O) devoid(O) of(O) the(O) electricity(O) of(O) Zeppelin(B-album) I(I-album) and(O) Led(B-album) Zeppelin(I-album) II(I-album) ,(O) the(O) sheer(O) diversity(O) of(O) Led(B-album) Zeppelin(I-album) III(I-album) ,(O) and(O) lacking(O) the(O) classic(O) status(O) of(O) Led(B-album) Zeppelin(I-album) IV(I-album) ,(O) Houses(B-album) of(I-album) the(I-album) Holy(I-album) nevertheless(O) found(O) its(O) rightful(O) niche(O) .(O)"}}
{"id":"91","dataset":"crossner_music","split":"train","label_list":["award","event","song","music genre","musical artist","musical instrument","country","album","person","location","organization","band"],"instance":{"id":"91","words":["In","1961",",","he","performed","ten","recitals","in","Carnegie","Hall","to","raise","roughly","$","100,000","for","charities","including","Big","Brothers","Big","Sisters","of","America",",","United","Jewish","Appeal",",","Polish","Assistance",",","Musicians","Emergency","fund",",","the","National","Association","for","Mental","Health",",","and","the","Legal","Defense","Fund","of","the","National","Advancement","of","Colored","People","."],"labels":["O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, song, music genre, musical artist, musical instrument, country, album, person, location, organization, band and O.\nSentence: In 1961 , he performed ten recitals in Carnegie Hall to raise roughly $ 100,000 for charities including Big Brothers Big Sisters of America , United Jewish Appeal , Polish Assistance , Musicians Emergency fund , the National Association for Mental Health , and the Legal Defense Fund of the National Advancement of Colored People .","prompt_labels":"In(O) 1961(O) ,(O) he(O) performed(O) ten(O) recitals(O) in(O) Carnegie(B-location) Hall(I-location) to(O) raise(O) roughly(O) $(O) 100,000(O) for(O) charities(O) including(O) Big(B-organization) Brothers(I-organization) Big(I-organization) Sisters(I-organization) of(I-organization) America(I-organization) ,(O) United(B-organization) Jewish(I-organization) Appeal(I-organization) ,(O) Polish(B-organization) Assistance(I-organization) ,(O) Musicians(B-organization) Emergency(I-organization) fund(I-organization) ,(O) the(O) National(B-organization) Association(I-organization) for(I-organization) Mental(I-organization) Health(I-organization) ,(O) and(O) the(O) Legal(B-organization) Defense(I-organization) Fund(I-organization) of(I-organization) the(I-organization) National(I-organization) Advancement(I-organization) of(I-organization) Colored(I-organization) People(I-organization) .(O)"}}
{"id":"92","dataset":"crossner_music","split":"train","label_list":["award","album","song","musical instrument","musical artist","organization","event","country","band","location","person","music genre"],"instance":{"id":"92","words":["Today",",","musicians","as","diverse","as","Keith","Urban",",","Rod","Stewart",",","Taj","Mahal",",","Joe","Satriani",",","David","Hidalgo",",","Larry","Lalonde","and","Doc","Watson","play","the","six-string","guitar","banjo","."],"labels":["O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-musical instrument","I-musical instrument","I-musical instrument","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, album, song, musical instrument, musical artist, organization, event, country, band, location, person, music genre and O.\nSentence: Today , musicians as diverse as Keith Urban , Rod Stewart , Taj Mahal , Joe Satriani , David Hidalgo , Larry Lalonde and Doc Watson play the six-string guitar banjo .","prompt_labels":"Today(O) ,(O) musicians(O) as(O) diverse(O) as(O) Keith(B-musical artist) Urban(I-musical artist) ,(O) Rod(B-musical artist) Stewart(I-musical artist) ,(O) Taj(B-musical artist) Mahal(I-musical artist) ,(O) Joe(B-musical artist) Satriani(I-musical artist) ,(O) David(B-musical artist) Hidalgo(I-musical artist) ,(O) Larry(B-musical artist) Lalonde(I-musical artist) and(O) Doc(B-musical artist) Watson(I-musical artist) play(O) the(O) six-string(B-musical instrument) guitar(I-musical instrument) banjo(I-musical instrument) .(O)"}}
{"id":"93","dataset":"crossner_music","split":"train","label_list":["album","location","musical artist","organization","musical instrument","band","person","music genre","song","event","award","country"],"instance":{"id":"93","words":["The","initial","volume","of","the","album","set","(","Anthology","1",")","was","released","the","same","week","of","the","documentary","'s","airdate",",","with","the","subsequent","two","volumes","(","Anthology","2","and","Anthology","3",")","released","in","1996","."],"labels":["O","O","O","O","O","O","O","O","B-album","I-album","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","O","B-album","I-album","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, location, musical artist, organization, musical instrument, band, person, music genre, song, event, award, country and O.\nSentence: The initial volume of the album set ( Anthology 1 ) was released the same week of the documentary 's airdate , with the subsequent two volumes ( Anthology 2 and Anthology 3 ) released in 1996 .","prompt_labels":"The(O) initial(O) volume(O) of(O) the(O) album(O) set(O) ((O) Anthology(B-album) 1(I-album) )(O) was(O) released(O) the(O) same(O) week(O) of(O) the(O) documentary(O) 's(O) airdate(O) ,(O) with(O) the(O) subsequent(O) two(O) volumes(O) ((O) Anthology(B-album) 2(I-album) and(O) Anthology(B-album) 3(I-album) )(O) released(O) in(O) 1996(O) .(O)"}}
{"id":"94","dataset":"crossner_music","split":"train","label_list":["album","song","award","music genre","musical artist","location","band","musical instrument","country","organization","person","event"],"instance":{"id":"94","words":["International","who","'s","who","in","popular","music",",","Volume","4","p.37.","Routledge",",","2002","The","band","was","renamed","Rocket","Baby","Dolls","and","adopted","a","Gothic","rock","-","Glam","rock","image","."],"labels":["O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","I-band","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, song, award, music genre, musical artist, location, band, musical instrument, country, organization, person, event and O.\nSentence: International who 's who in popular music , Volume 4 p.37. Routledge , 2002 The band was renamed Rocket Baby Dolls and adopted a Gothic rock - Glam rock image .","prompt_labels":"International(O) who(O) 's(O) who(O) in(O) popular(B-music genre) music(I-music genre) ,(O) Volume(O) 4(O) p.37.(O) Routledge(O) ,(O) 2002(O) The(O) band(O) was(O) renamed(O) Rocket(B-band) Baby(I-band) Dolls(I-band) and(O) adopted(O) a(O) Gothic(B-music genre) rock(I-music genre) -(O) Glam(B-music genre) rock(I-music genre) image(O) .(O)"}}
{"id":"95","dataset":"crossner_music","split":"train","label_list":["band","event","organization","music genre","musical artist","musical instrument","award","song","location","country","album","person"],"instance":{"id":"95","words":["In","September","2007",",","Boney","M.","'","s","last","four","original","albums",",","Boonoonoonoos",",","Ten","Thousand","Lightyears",",","Kalimba","de","Luna","-","16","Happy","Songs","and","Eye","Dance","were","reissued","on","compact","disc","in","Europe","and","the","United","States",",","all","including","bonus","tracks","."],"labels":["O","O","O","O","B-band","I-band","O","O","O","O","O","O","O","B-album","O","B-album","I-album","I-album","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","O","B-album","I-album","O","O","O","O","O","O","B-location","O","O","B-country","I-country","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, event, organization, music genre, musical artist, musical instrument, award, song, location, country, album, person and O.\nSentence: In September 2007 , Boney M. ' s last four original albums , Boonoonoonoos , Ten Thousand Lightyears , Kalimba de Luna - 16 Happy Songs and Eye Dance were reissued on compact disc in Europe and the United States , all including bonus tracks .","prompt_labels":"In(O) September(O) 2007(O) ,(O) Boney(B-band) M.(I-band) '(O) s(O) last(O) four(O) original(O) albums(O) ,(O) Boonoonoonoos(B-album) ,(O) Ten(B-album) Thousand(I-album) Lightyears(I-album) ,(O) Kalimba(B-album) de(I-album) Luna(I-album) -(I-album) 16(I-album) Happy(I-album) Songs(I-album) and(O) Eye(B-album) Dance(I-album) were(O) reissued(O) on(O) compact(O) disc(O) in(O) Europe(B-location) and(O) the(O) United(B-country) States(I-country) ,(O) all(O) including(O) bonus(O) tracks(O) .(O)"}}
{"id":"96","dataset":"crossner_music","split":"train","label_list":["country","musical instrument","organization","album","location","musical artist","song","music genre","person","award","band","event"],"instance":{"id":"96","words":["Despite","the","appeal","of","the","Nashville","sound",",","many","traditional","country","artists","emerged","during","this","period","and","dominated","the","genre",":","Loretta","Lynn",",","Merle","Haggard",",","Buck","Owens",",","Porter","Wagoner",",","George","Jones",",","and","Sonny","James","among","them","."],"labels":["O","O","O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, musical instrument, organization, album, location, musical artist, song, music genre, person, award, band, event and O.\nSentence: Despite the appeal of the Nashville sound , many traditional country artists emerged during this period and dominated the genre : Loretta Lynn , Merle Haggard , Buck Owens , Porter Wagoner , George Jones , and Sonny James among them .","prompt_labels":"Despite(O) the(O) appeal(O) of(O) the(B-music genre) Nashville(I-music genre) sound(I-music genre) ,(O) many(O) traditional(O) country(O) artists(O) emerged(O) during(O) this(O) period(O) and(O) dominated(O) the(O) genre(O) :(O) Loretta(B-musical artist) Lynn(I-musical artist) ,(O) Merle(B-musical artist) Haggard(I-musical artist) ,(O) Buck(B-musical artist) Owens(I-musical artist) ,(O) Porter(B-musical artist) Wagoner(I-musical artist) ,(O) George(B-musical artist) Jones(I-musical artist) ,(O) and(O) Sonny(B-musical artist) James(I-musical artist) among(O) them(O) .(O)"}}
{"id":"97","dataset":"crossner_music","split":"train","label_list":["country","album","band","musical instrument","event","song","person","award","music genre","organization","location","musical artist"],"instance":{"id":"97","words":["Christian","alternative","music","has","its","roots","in","the","early","1980s",",","as","the","earliest","efforts","at","Christian","punk","and","new","wave","were","recorded","by","artists","like","Andy","McCarroll","and","Moral","Support",",","Undercover",",","the","77s",",","Steve","Scott",",","Adam","Again",",","Quickflight",",","Daniel","Amos",",","Youth","Choir","(","later","renamed","the","Choir",")",",","Lifesavers","Underground",",","Michael","Knott",",","the","Prayer","Chain",",","Altar","Boys",",","Breakfast","with","Amy",",","Steve","Taylor",",","4-4-1",",","David","Edwards","and","Vector","."],"labels":["B-music genre","I-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","O","O","B-band","I-band","O","B-band","I-band","O","B-band","O","B-band","I-band","O","B-musical artist","I-musical artist","O","B-band","I-band","O","B-band","O","B-band","I-band","O","B-band","I-band","O","O","O","B-band","I-band","O","O","B-band","I-band","O","B-musical artist","I-musical artist","O","B-band","I-band","I-band","O","B-band","I-band","O","B-band","I-band","I-band","O","B-musical artist","I-musical artist","O","B-band","O","B-musical artist","I-musical artist","O","B-musical artist","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, album, band, musical instrument, event, song, person, award, music genre, organization, location, musical artist and O.\nSentence: Christian alternative music has its roots in the early 1980s , as the earliest efforts at Christian punk and new wave were recorded by artists like Andy McCarroll and Moral Support , Undercover , the 77s , Steve Scott , Adam Again , Quickflight , Daniel Amos , Youth Choir ( later renamed the Choir ) , Lifesavers Underground , Michael Knott , the Prayer Chain , Altar Boys , Breakfast with Amy , Steve Taylor , 4-4-1 , David Edwards and Vector .","prompt_labels":"Christian(B-music genre) alternative(I-music genre) music(I-music genre) has(O) its(O) roots(O) in(O) the(O) early(O) 1980s(O) ,(O) as(O) the(O) earliest(O) efforts(O) at(O) Christian(B-music genre) punk(I-music genre) and(O) new(B-music genre) wave(I-music genre) were(O) recorded(O) by(O) artists(O) like(O) Andy(B-band) McCarroll(I-band) and(O) Moral(B-band) Support(I-band) ,(O) Undercover(B-band) ,(O) the(B-band) 77s(I-band) ,(O) Steve(B-musical artist) Scott(I-musical artist) ,(O) Adam(B-band) Again(I-band) ,(O) Quickflight(B-band) ,(O) Daniel(B-band) Amos(I-band) ,(O) Youth(B-band) Choir(I-band) ((O) later(O) renamed(O) the(B-band) Choir(I-band) )(O) ,(O) Lifesavers(B-band) Underground(I-band) ,(O) Michael(B-musical artist) Knott(I-musical artist) ,(O) the(B-band) Prayer(I-band) Chain(I-band) ,(O) Altar(B-band) Boys(I-band) ,(O) Breakfast(B-band) with(I-band) Amy(I-band) ,(O) Steve(B-musical artist) Taylor(I-musical artist) ,(O) 4-4-1(B-band) ,(O) David(B-musical artist) Edwards(I-musical artist) and(O) Vector(B-musical artist) .(O)"}}
{"id":"98","dataset":"crossner_music","split":"train","label_list":["country","organization","award","musical instrument","music genre","person","musical artist","location","album","song","event","band"],"instance":{"id":"98","words":["He","has","won","numerous","accolades","for","his","work",",","including","an","Academy","Award","for","Best","Adapted","Screenplay",",","a","Student","Academy","Award",",","a","BAFTA","Award","for","Best","Adapted","Screenplay",",","two","Emmy","Awards",",","two","Peabody","Award","s",",","and","the","Cannes","Grand","Prix","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","O","O","B-award","I-award","O","O","O","O","B-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, award, musical instrument, music genre, person, musical artist, location, album, song, event, band and O.\nSentence: He has won numerous accolades for his work , including an Academy Award for Best Adapted Screenplay , a Student Academy Award , a BAFTA Award for Best Adapted Screenplay , two Emmy Awards , two Peabody Award s , and the Cannes Grand Prix .","prompt_labels":"He(O) has(O) won(O) numerous(O) accolades(O) for(O) his(O) work(O) ,(O) including(O) an(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) ,(O) a(O) Student(B-award) Academy(I-award) Award(I-award) ,(O) a(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) ,(O) two(O) Emmy(B-award) Awards(I-award) ,(O) two(O) Peabody(B-award) Award(I-award) s(O) ,(O) and(O) the(O) Cannes(B-award) Grand(I-award) Prix(I-award) .(O)"}}
{"id":"99","dataset":"crossner_music","split":"train","label_list":["event","person","song","album","band","musical instrument","country","organization","musical artist","music genre","location","award"],"instance":{"id":"99","words":["The","mid","2000s",",","especially","the","United","Kingdom","and","the","rest","of","Europe",",","saw","the","continued","longevity","of","nineties","boy","bands","such","as","Backstreet","Boys","and","Westlife","(","before","they","disbanded","in","2012",")",",","and","the","successful","comeback","of","Take","That","in","2005",",","Boyzone","in","2007",",","and","New","Kids","on","the","Block","in","2008","."],"labels":["O","O","O","O","O","O","B-country","I-country","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","O","B-band","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","O","O","O","B-band","O","O","O","O","B-band","I-band","I-band","I-band","I-band","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, song, album, band, musical instrument, country, organization, musical artist, music genre, location, award and O.\nSentence: The mid 2000s , especially the United Kingdom and the rest of Europe , saw the continued longevity of nineties boy bands such as Backstreet Boys and Westlife ( before they disbanded in 2012 ) , and the successful comeback of Take That in 2005 , Boyzone in 2007 , and New Kids on the Block in 2008 .","prompt_labels":"The(O) mid(O) 2000s(O) ,(O) especially(O) the(O) United(B-country) Kingdom(I-country) and(O) the(O) rest(O) of(O) Europe(B-location) ,(O) saw(O) the(O) continued(O) longevity(O) of(O) nineties(O) boy(O) bands(O) such(O) as(O) Backstreet(B-band) Boys(I-band) and(O) Westlife(B-band) ((O) before(O) they(O) disbanded(O) in(O) 2012(O) )(O) ,(O) and(O) the(O) successful(O) comeback(O) of(O) Take(B-band) That(I-band) in(O) 2005(O) ,(O) Boyzone(B-band) in(O) 2007(O) ,(O) and(O) New(B-band) Kids(I-band) on(I-band) the(I-band) Block(I-band) in(O) 2008(O) .(O)"}}
{"id":"0","dataset":"crossner_politics","split":"train","label_list":["organization","political party","location","event","person","election","politician","country"],"instance":{"id":"0","words":["Parties","with","mainly","Eurosceptic","views","are","the","ruling","United","Russia",",","and","opposition","parties","the","Communist","Party","of","the","Russian","Federation","and","Liberal","Democratic","Party","of","Russia","."],"labels":["O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, location, event, person, election, politician, country and O.\nSentence: Parties with mainly Eurosceptic views are the ruling United Russia , and opposition parties the Communist Party of the Russian Federation and Liberal Democratic Party of Russia .","prompt_labels":"Parties(O) with(O) mainly(O) Eurosceptic(O) views(O) are(O) the(O) ruling(O) United(B-political party) Russia(I-political party) ,(O) and(O) opposition(O) parties(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) the(I-political party) Russian(I-political party) Federation(I-political party) and(O) Liberal(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Russia(I-political party) .(O)"}}
{"id":"1","dataset":"crossner_politics","split":"train","label_list":["country","election","political party","politician","organization","person","event","location"],"instance":{"id":"1","words":["Parties","with","mainly","Eurosceptic","views","are","Serbian","Radical","Party",",","Democratic","Party","of","Serbia",",","Dveri",",","DJB","and","the","Serbian","People","'s","Party","of","Nenad","Popović","."],"labels":["O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","B-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, political party, politician, organization, person, event, location and O.\nSentence: Parties with mainly Eurosceptic views are Serbian Radical Party , Democratic Party of Serbia , Dveri , DJB and the Serbian People 's Party of Nenad Popović .","prompt_labels":"Parties(O) with(O) mainly(O) Eurosceptic(O) views(O) are(O) Serbian(B-political party) Radical(I-political party) Party(I-political party) ,(O) Democratic(B-political party) Party(I-political party) of(I-political party) Serbia(I-political party) ,(O) Dveri(B-political party) ,(O) DJB(B-political party) and(O) the(O) Serbian(B-political party) People(I-political party) 's(I-political party) Party(I-political party) of(O) Nenad(B-politician) Popović(I-politician) .(O)"}}
{"id":"2","dataset":"crossner_politics","split":"train","label_list":["politician","person","election","country","political party","event","location","organization"],"instance":{"id":"2","words":["On","12","April","2019","a","new","Eurosceptic","party",",","the","Brexit","Party","was","officially","launched","by","former","UK","Independence","Party","Leader","Nigel","Farage","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, election, country, political party, event, location, organization and O.\nSentence: On 12 April 2019 a new Eurosceptic party , the Brexit Party was officially launched by former UK Independence Party Leader Nigel Farage .","prompt_labels":"On(O) 12(O) April(O) 2019(O) a(O) new(O) Eurosceptic(O) party(O) ,(O) the(O) Brexit(B-political party) Party(I-political party) was(O) officially(O) launched(O) by(O) former(O) UK(B-political party) Independence(I-political party) Party(I-political party) Leader(O) Nigel(B-politician) Farage(I-politician) .(O)"}}
{"id":"3","dataset":"crossner_politics","split":"train","label_list":["politician","election","person","country","event","location","political party","organization"],"instance":{"id":"3","words":["Page","returned","to","cabinet","after","the","1934","Australian","federal","election",",","when","the","Country","Party","entered","a","new","coalition","with","Joseph","Lyons","'","United","Australia","Party","."],"labels":["O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","B-political party","I-political party","O","O","O","O","O","B-politician","I-politician","O","B-political party","I-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, person, country, event, location, political party, organization and O.\nSentence: Page returned to cabinet after the 1934 Australian federal election , when the Country Party entered a new coalition with Joseph Lyons ' United Australia Party .","prompt_labels":"Page(O) returned(O) to(O) cabinet(O) after(O) the(O) 1934(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) when(O) the(O) Country(B-political party) Party(I-political party) entered(O) a(O) new(O) coalition(O) with(O) Joseph(B-politician) Lyons(I-politician) '(O) United(B-political party) Australia(I-political party) Party(I-political party) .(O)"}}
{"id":"4","dataset":"crossner_politics","split":"train","label_list":["event","person","political party","politician","location","election","organization","country"],"instance":{"id":"4","words":["He","was","the","last","former","Prime","Minister","to","lose","his","seat","until","Tony","Abbott","lost","his","seat","of","Warringah","in","2019","Australian","federal","election",",","though","John","Howard","would","lose","his","seat","of","Bennelong","as","a","sitting","Prime","Minister","in","2007","Australian","federal","election","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","B-location","O","B-election","I-election","I-election","I-election","O","O","B-politician","I-politician","O","O","O","O","O","B-person","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, political party, politician, location, election, organization, country and O.\nSentence: He was the last former Prime Minister to lose his seat until Tony Abbott lost his seat of Warringah in 2019 Australian federal election , though John Howard would lose his seat of Bennelong as a sitting Prime Minister in 2007 Australian federal election .","prompt_labels":"He(O) was(O) the(O) last(O) former(O) Prime(O) Minister(O) to(O) lose(O) his(O) seat(O) until(O) Tony(B-politician) Abbott(I-politician) lost(O) his(O) seat(O) of(O) Warringah(B-location) in(O) 2019(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) though(O) John(B-politician) Howard(I-politician) would(O) lose(O) his(O) seat(O) of(O) Bennelong(B-person) as(O) a(O) sitting(O) Prime(O) Minister(O) in(O) 2007(B-election) Australian(I-election) federal(I-election) election(I-election) .(O)"}}
{"id":"5","dataset":"crossner_politics","split":"train","label_list":["person","event","politician","political party","location","country","organization","election"],"instance":{"id":"5","words":["Zappa","set","excerpts","from","the","PMRC","hearings","to","Synclavier","music","in","his","composition","Porn","Wars","on","the","1985","album","Frank","Zappa","Meets","the","Mothers","of","Prevention",",","and","the","full","recording","was","released","in","2010","as","Congress","Shall","Make","No","Law","...","Zappa","is","heard","interacting","with","Senators","Fritz","Hollings",",","Slade","Gorton","and","Al","Gore","."],"labels":["B-person","O","O","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, politician, political party, location, country, organization, election and O.\nSentence: Zappa set excerpts from the PMRC hearings to Synclavier music in his composition Porn Wars on the 1985 album Frank Zappa Meets the Mothers of Prevention , and the full recording was released in 2010 as Congress Shall Make No Law ... Zappa is heard interacting with Senators Fritz Hollings , Slade Gorton and Al Gore .","prompt_labels":"Zappa(B-person) set(O) excerpts(O) from(O) the(O) PMRC(B-organization) hearings(O) to(O) Synclavier(O) music(O) in(O) his(O) composition(O) Porn(O) Wars(O) on(O) the(O) 1985(O) album(O) Frank(O) Zappa(O) Meets(O) the(O) Mothers(O) of(O) Prevention(O) ,(O) and(O) the(O) full(O) recording(O) was(O) released(O) in(O) 2010(O) as(O) Congress(O) Shall(O) Make(O) No(O) Law(O) ...(O) Zappa(B-politician) is(O) heard(O) interacting(O) with(O) Senators(O) Fritz(B-politician) Hollings(I-politician) ,(O) Slade(B-politician) Gorton(I-politician) and(O) Al(B-politician) Gore(I-politician) .(O)"}}
{"id":"6","dataset":"crossner_politics","split":"train","label_list":["event","organization","election","location","country","person","politician","political party"],"instance":{"id":"6","words":["The","Walwari","has","one","deputy",",","Christiane","Taubira",",","and","the","PSG","has","one","deputy",",","Chantal","Berthelot",",","who","defeated","long-time","UMP","incumbent","Léon","Bertrand","."],"labels":["O","B-political party","O","O","O","O","B-politician","I-politician","O","O","O","B-organization","O","O","O","O","B-politician","I-politician","O","O","O","O","B-organization","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, election, location, country, person, politician, political party and O.\nSentence: The Walwari has one deputy , Christiane Taubira , and the PSG has one deputy , Chantal Berthelot , who defeated long-time UMP incumbent Léon Bertrand .","prompt_labels":"The(O) Walwari(B-political party) has(O) one(O) deputy(O) ,(O) Christiane(B-politician) Taubira(I-politician) ,(O) and(O) the(O) PSG(B-organization) has(O) one(O) deputy(O) ,(O) Chantal(B-politician) Berthelot(I-politician) ,(O) who(O) defeated(O) long-time(O) UMP(B-organization) incumbent(O) Léon(B-politician) Bertrand(I-politician) .(O)"}}
{"id":"7","dataset":"crossner_politics","split":"train","label_list":["organization","location","politician","political party","event","person","election","country"],"instance":{"id":"7","words":["In","late","December","2011",",","President","Barack","Obama","nominated","Jeremy","C.","Stein",",","a","Harvard","University","finance","professor","and","a","Democrat",",","and","Jerome","Powell",",","formerly","of","Dillon","Read",",","Bankers","Trust"],"labels":["O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","I-politician","O","O","B-organization","I-organization","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","B-organization","I-organization","O","B-organization","I-organization"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, politician, political party, event, person, election, country and O.\nSentence: In late December 2011 , President Barack Obama nominated Jeremy C. Stein , a Harvard University finance professor and a Democrat , and Jerome Powell , formerly of Dillon Read , Bankers Trust","prompt_labels":"In(O) late(O) December(O) 2011(O) ,(O) President(O) Barack(B-politician) Obama(I-politician) nominated(O) Jeremy(B-politician) C.(I-politician) Stein(I-politician) ,(O) a(O) Harvard(B-organization) University(I-organization) finance(O) professor(O) and(O) a(O) Democrat(O) ,(O) and(O) Jerome(B-politician) Powell(I-politician) ,(O) formerly(O) of(O) Dillon(B-organization) Read(I-organization) ,(O) Bankers(B-organization) Trust(I-organization)"}}
{"id":"8","dataset":"crossner_politics","split":"train","label_list":["country","politician","location","political party","election","organization","person","event"],"instance":{"id":"8","words":["Westerwelle","stepped","down","as","party","leader","following","the","2011","state","elections",",","in","which","the","party","was","wiped","out","in","2011","Saxony-Anhalt","state","election","and","2011","Rhineland-Palatinate","state","election","and","lost","half","its","seats","in","2011","Baden-Württemberg","state","election","."],"labels":["B-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, politician, location, political party, election, organization, person, event and O.\nSentence: Westerwelle stepped down as party leader following the 2011 state elections , in which the party was wiped out in 2011 Saxony-Anhalt state election and 2011 Rhineland-Palatinate state election and lost half its seats in 2011 Baden-Württemberg state election .","prompt_labels":"Westerwelle(B-politician) stepped(O) down(O) as(O) party(O) leader(O) following(O) the(O) 2011(O) state(O) elections(O) ,(O) in(O) which(O) the(O) party(O) was(O) wiped(O) out(O) in(O) 2011(B-election) Saxony-Anhalt(I-election) state(I-election) election(I-election) and(O) 2011(B-election) Rhineland-Palatinate(I-election) state(I-election) election(I-election) and(O) lost(O) half(O) its(O) seats(O) in(O) 2011(B-election) Baden-Württemberg(I-election) state(I-election) election(I-election) .(O)"}}
{"id":"9","dataset":"crossner_politics","split":"train","label_list":["organization","election","person","location","country","event","political party","politician"],"instance":{"id":"9","words":["The","change","in","leadership","failed","to","revive","the","FDP","'s","fortunes",",","however",",","and","in","the","next","series","of","state","elections",",","the","party","lost","all","its","seats","in","2011","Bremen","state","election",",","2011","Mecklenburg-Vorpommern","state","election",",","and","2011","Berlin","state","election","."],"labels":["O","O","O","O","O","O","O","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, person, location, country, event, political party, politician and O.\nSentence: The change in leadership failed to revive the FDP 's fortunes , however , and in the next series of state elections , the party lost all its seats in 2011 Bremen state election , 2011 Mecklenburg-Vorpommern state election , and 2011 Berlin state election .","prompt_labels":"The(O) change(O) in(O) leadership(O) failed(O) to(O) revive(O) the(O) FDP(B-political party) 's(O) fortunes(O) ,(O) however(O) ,(O) and(O) in(O) the(O) next(O) series(O) of(O) state(O) elections(O) ,(O) the(O) party(O) lost(O) all(O) its(O) seats(O) in(O) 2011(B-election) Bremen(I-election) state(I-election) election(I-election) ,(O) 2011(B-election) Mecklenburg-Vorpommern(I-election) state(I-election) election(I-election) ,(O) and(O) 2011(B-election) Berlin(I-election) state(I-election) election(I-election) .(O)"}}
{"id":"10","dataset":"crossner_politics","split":"train","label_list":["politician","location","political party","organization","person","election","event","country"],"instance":{"id":"10","words":["Roosevelt","was","James","M.","Cox","'","s","running","mate","on","the","Democratic","Party","'s","1920","United","States","presidential","election","national","ticket",",","but","Cox","was","defeated","by","Republican","Warren","G.","Harding","."],"labels":["B-politician","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","B-political party","I-political party","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","B-politician","O","O","O","O","B-politician","I-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, political party, organization, person, election, event, country and O.\nSentence: Roosevelt was James M. Cox ' s running mate on the Democratic Party 's 1920 United States presidential election national ticket , but Cox was defeated by Republican Warren G. Harding .","prompt_labels":"Roosevelt(B-politician) was(O) James(B-politician) M.(I-politician) Cox(I-politician) '(O) s(O) running(O) mate(O) on(O) the(O) Democratic(B-political party) Party(I-political party) 's(O) 1920(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) national(O) ticket(O) ,(O) but(O) Cox(B-politician) was(O) defeated(O) by(O) Republican(O) Warren(B-politician) G.(I-politician) Harding(I-politician) .(O)"}}
{"id":"11","dataset":"crossner_politics","split":"train","label_list":["person","location","country","election","organization","event","politician","political party"],"instance":{"id":"11","words":["Assisted","by","his","top","aide","Harry","Hopkins","and","with","very","strong","national","support",",","he","worked","closely","with","British","Prime","Minister","Winston","Churchill",",","Soviet","leader","Joseph","Stalin","and","Chinese","Generalissimo","Chiang","Kai-shek","in","leading","the","Allied","Powers","against","the","Axis","Powers","."],"labels":["O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","B-politician","I-politician","O","O","O","B-politician","I-politician","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, country, election, organization, event, politician, political party and O.\nSentence: Assisted by his top aide Harry Hopkins and with very strong national support , he worked closely with British Prime Minister Winston Churchill , Soviet leader Joseph Stalin and Chinese Generalissimo Chiang Kai-shek in leading the Allied Powers against the Axis Powers .","prompt_labels":"Assisted(O) by(O) his(O) top(O) aide(O) Harry(B-politician) Hopkins(I-politician) and(O) with(O) very(O) strong(O) national(O) support(O) ,(O) he(O) worked(O) closely(O) with(O) British(O) Prime(O) Minister(O) Winston(B-politician) Churchill(I-politician) ,(O) Soviet(O) leader(O) Joseph(B-politician) Stalin(I-politician) and(O) Chinese(O) Generalissimo(O) Chiang(B-politician) Kai-shek(I-politician) in(O) leading(O) the(O) Allied(B-organization) Powers(I-organization) against(O) the(O) Axis(B-organization) Powers(I-organization) .(O)"}}
{"id":"12","dataset":"crossner_politics","split":"train","label_list":["event","person","election","organization","location","politician","political party","country"],"instance":{"id":"12","words":["Though","Roosevelt","won","the","backing","of","Treasury","Secretary","William","Gibbs","McAdoo","and","Governor","Martin","H.","Glynn",",","he","faced","a","formidable","opponent","in","the","Tammany-backed","James","W.","Gerard","."],"labels":["O","B-politician","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","O","O","B-politician","I-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, election, organization, location, politician, political party, country and O.\nSentence: Though Roosevelt won the backing of Treasury Secretary William Gibbs McAdoo and Governor Martin H. Glynn , he faced a formidable opponent in the Tammany-backed James W. Gerard .","prompt_labels":"Though(O) Roosevelt(B-politician) won(O) the(O) backing(O) of(O) Treasury(O) Secretary(O) William(B-politician) Gibbs(I-politician) McAdoo(I-politician) and(O) Governor(O) Martin(B-politician) H.(I-politician) Glynn(I-politician) ,(O) he(O) faced(O) a(O) formidable(O) opponent(O) in(O) the(O) Tammany-backed(O) James(B-politician) W.(I-politician) Gerard(I-politician) .(O)"}}
{"id":"13","dataset":"crossner_politics","split":"train","label_list":["election","organization","political party","person","politician","event","country","location"],"instance":{"id":"13","words":["In","Italy",",","the","Italian","Social","Movement","led","by","Giorgio","Almirante","was","a","major","neo-fascist","movement","that","transformed","itself","into","a","self-described","post-fascist","movement","called","the","National","Alliance","(","AN",")",",","which","has","been","an","ally","of","Silvio","Berlusconi","'","s","Forza","Italia","for","a","decade","."],"labels":["O","B-location","O","O","B-political party","I-political party","I-political party","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","B-political party","I-political party","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, political party, person, politician, event, country, location and O.\nSentence: In Italy , the Italian Social Movement led by Giorgio Almirante was a major neo-fascist movement that transformed itself into a self-described post-fascist movement called the National Alliance ( AN ) , which has been an ally of Silvio Berlusconi ' s Forza Italia for a decade .","prompt_labels":"In(O) Italy(B-location) ,(O) the(O) Italian(B-political party) Social(I-political party) Movement(I-political party) led(O) by(O) Giorgio(B-politician) Almirante(I-politician) was(O) a(O) major(O) neo-fascist(O) movement(O) that(O) transformed(O) itself(O) into(O) a(O) self-described(O) post-fascist(O) movement(O) called(O) the(O) National(B-political party) Alliance(I-political party) ((O) AN(B-political party) )(O) ,(O) which(O) has(O) been(O) an(O) ally(O) of(O) Silvio(B-politician) Berlusconi(I-politician) '(O) s(O) Forza(B-political party) Italia(I-political party) for(O) a(O) decade(O) .(O)"}}
{"id":"14","dataset":"crossner_politics","split":"train","label_list":["location","political party","organization","person","election","politician","event","country"],"instance":{"id":"14","words":["Fianna","Fáil","was","last","in","government","from","1997","Irish","general","election","to","2011","Irish","general","election","under","Bertie","Ahern","and","Brian","Cowen",",","with","a","periodic","high","of","81","seats","in","2002","Irish","general","election",",","reduced","to","77","in","2007","Irish","general","election","."],"labels":["B-political party","I-political party","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","B-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, organization, person, election, politician, event, country and O.\nSentence: Fianna Fáil was last in government from 1997 Irish general election to 2011 Irish general election under Bertie Ahern and Brian Cowen , with a periodic high of 81 seats in 2002 Irish general election , reduced to 77 in 2007 Irish general election .","prompt_labels":"Fianna(B-political party) Fáil(I-political party) was(O) last(O) in(O) government(O) from(O) 1997(B-election) Irish(I-election) general(I-election) election(I-election) to(O) 2011(B-election) Irish(I-election) general(I-election) election(I-election) under(O) Bertie(B-politician) Ahern(I-politician) and(O) Brian(B-politician) Cowen(I-politician) ,(O) with(O) a(O) periodic(O) high(O) of(O) 81(O) seats(O) in(O) 2002(B-election) Irish(I-election) general(I-election) election(I-election) ,(O) reduced(O) to(O) 77(O) in(O) 2007(B-election) Irish(I-election) general(I-election) election(I-election) .(O)"}}
{"id":"15","dataset":"crossner_politics","split":"train","label_list":["organization","country","election","event","political party","location","person","politician"],"instance":{"id":"15","words":["After","winning","the","1979","United","Kingdom","general","election",",","Margaret","Thatcher","appointed","Keith","Joseph",",","the","director","of","the","Hayekian","Centre","for","Policy","Studies",",","as","her","secretary","of","state","for","industry","in","an","effort","to","redirect","parliament","'s","economic","strategies","."],"labels":["O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, election, event, political party, location, person, politician and O.\nSentence: After winning the 1979 United Kingdom general election , Margaret Thatcher appointed Keith Joseph , the director of the Hayekian Centre for Policy Studies , as her secretary of state for industry in an effort to redirect parliament 's economic strategies .","prompt_labels":"After(O) winning(O) the(O) 1979(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) Margaret(B-politician) Thatcher(I-politician) appointed(O) Keith(B-politician) Joseph(I-politician) ,(O) the(O) director(O) of(O) the(O) Hayekian(B-organization) Centre(I-organization) for(I-organization) Policy(I-organization) Studies(I-organization) ,(O) as(O) her(O) secretary(O) of(O) state(O) for(O) industry(O) in(O) an(O) effort(O) to(O) redirect(O) parliament(O) 's(O) economic(O) strategies(O) .(O)"}}
{"id":"16","dataset":"crossner_politics","split":"train","label_list":["election","organization","location","political party","event","politician","country","person"],"instance":{"id":"16","words":["He","led","a","government","'s","coalition","composed","by","Christian","Democrats",",","Italian","Socialist","Party",",","Italian","Democratic","Socialist","Party",",","Italian","Republican","Party","and","Italian","Liberal","Party","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, location, political party, event, politician, country, person and O.\nSentence: He led a government 's coalition composed by Christian Democrats , Italian Socialist Party , Italian Democratic Socialist Party , Italian Republican Party and Italian Liberal Party .","prompt_labels":"He(O) led(O) a(O) government(O) 's(O) coalition(O) composed(O) by(O) Christian(O) Democrats(O) ,(O) Italian(B-political party) Socialist(I-political party) Party(I-political party) ,(O) Italian(B-political party) Democratic(I-political party) Socialist(I-political party) Party(I-political party) ,(O) Italian(B-political party) Republican(I-political party) Party(I-political party) and(O) Italian(B-political party) Liberal(I-political party) Party(I-political party) .(O)"}}
{"id":"17","dataset":"crossner_politics","split":"train","label_list":["political party","organization","politician","election","location","country","person","event"],"instance":{"id":"17","words":["With","George","W.","Bush","'s","victory","in","the","2000","United","States","presidential","election",",","Bush","and","his","son","became","the","second","father-son","pair","to","serve","as","the","nation","'s","president",",","following","John","Adams","and","John","Quincy","Adams","."],"labels":["O","B-politician","I-politician","I-politician","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, politician, election, location, country, person, event and O.\nSentence: With George W. Bush 's victory in the 2000 United States presidential election , Bush and his son became the second father-son pair to serve as the nation 's president , following John Adams and John Quincy Adams .","prompt_labels":"With(O) George(B-politician) W.(I-politician) Bush(I-politician) 's(O) victory(O) in(O) the(O) 2000(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) Bush(B-politician) and(O) his(O) son(O) became(O) the(O) second(O) father-son(O) pair(O) to(O) serve(O) as(O) the(O) nation(O) 's(O) president(O) ,(O) following(O) John(B-politician) Adams(I-politician) and(O) John(B-politician) Quincy(I-politician) Adams(I-politician) .(O)"}}
{"id":"18","dataset":"crossner_politics","split":"train","label_list":["politician","election","location","event","political party","person","organization","country"],"instance":{"id":"18","words":["Motivated","by","Tower","'s","victory",",","and","hoping","to","prevent","the","far-right","John","Birch","Society","from","coming","to","power",",","Bush","ran","for","the","chairmanship","of","the","Harris","County",",","Texas","Republican","Party",",","winning","election","in","February","1963",".","Like","most","other","Texas","Republicans",",","Bush","supported","conservative","Senator","Barry","Goldwater","over","the","more","centrist","Nelson","Rockefeller","in","the","1964","Republican","Party","presidential","primaries","."],"labels":["O","O","B-person","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","B-politician","O","O","O","O","O","O","B-location","I-location","O","B-location","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","O","O","O","B-politician","I-politician","O","O","O","O","B-politician","I-politician","O","O","B-election","I-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, location, event, political party, person, organization, country and O.\nSentence: Motivated by Tower 's victory , and hoping to prevent the far-right John Birch Society from coming to power , Bush ran for the chairmanship of the Harris County , Texas Republican Party , winning election in February 1963 . Like most other Texas Republicans , Bush supported conservative Senator Barry Goldwater over the more centrist Nelson Rockefeller in the 1964 Republican Party presidential primaries .","prompt_labels":"Motivated(O) by(O) Tower(B-person) 's(O) victory(O) ,(O) and(O) hoping(O) to(O) prevent(O) the(O) far-right(O) John(B-organization) Birch(I-organization) Society(I-organization) from(O) coming(O) to(O) power(O) ,(O) Bush(B-politician) ran(O) for(O) the(O) chairmanship(O) of(O) the(O) Harris(B-location) County(I-location) ,(O) Texas(B-location) Republican(B-political party) Party(I-political party) ,(O) winning(O) election(O) in(O) February(O) 1963(O) .(O) Like(O) most(O) other(O) Texas(O) Republicans(O) ,(O) Bush(B-politician) supported(O) conservative(O) Senator(O) Barry(B-politician) Goldwater(I-politician) over(O) the(O) more(O) centrist(O) Nelson(B-politician) Rockefeller(I-politician) in(O) the(O) 1964(B-election) Republican(I-election) Party(I-election) presidential(I-election) primaries(I-election) .(O)"}}
{"id":"19","dataset":"crossner_politics","split":"train","label_list":["location","event","country","person","politician","election","political party","organization"],"instance":{"id":"19","words":["Though","most","other","Texas","Republicans","supported","Ronald","Reagan","in","the","1968","Republican","Party","presidential","primaries",",","Bush","endorsed","Richard","Nixon",",","who","went","on","to","win","the","party","'s","nomination","."],"labels":["O","O","O","O","O","O","B-politician","I-politician","O","O","B-election","I-election","I-election","I-election","I-election","O","B-politician","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, country, person, politician, election, political party, organization and O.\nSentence: Though most other Texas Republicans supported Ronald Reagan in the 1968 Republican Party presidential primaries , Bush endorsed Richard Nixon , who went on to win the party 's nomination .","prompt_labels":"Though(O) most(O) other(O) Texas(O) Republicans(O) supported(O) Ronald(B-politician) Reagan(I-politician) in(O) the(O) 1968(B-election) Republican(I-election) Party(I-election) presidential(I-election) primaries(I-election) ,(O) Bush(B-politician) endorsed(O) Richard(B-politician) Nixon(I-politician) ,(O) who(O) went(O) on(O) to(O) win(O) the(O) party(O) 's(O) nomination(O) .(O)"}}
{"id":"20","dataset":"crossner_politics","split":"train","label_list":["country","political party","location","organization","event","election","politician","person"],"instance":{"id":"20","words":["Meanwhile",",","he","began","to","lay","the","groundwork","for","his","candidacy","in","the","1980","Republican","Party","presidential","primaries",".","In","the","1980","Republican","primary","campaign",",","Bush","would","face","Ronald","Reagan",",","who","was","widely","regarded","as","the","front-runner",",","as","well","as","other","contenders","like","Senator","Bob","Dole",",","Senator","Howard","Baker",",","Texas","Governor","John","Connally",",","Congressman","Phil","Crane",",","and","Congressman","John","B.","Anderson","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","B-politician","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","B-politician","I-politician","O","O","O","B-politician","I-politician","O","O","B-politician","I-politician","O","O","O","B-politician","I-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, location, organization, event, election, politician, person and O.\nSentence: Meanwhile , he began to lay the groundwork for his candidacy in the 1980 Republican Party presidential primaries . In the 1980 Republican primary campaign , Bush would face Ronald Reagan , who was widely regarded as the front-runner , as well as other contenders like Senator Bob Dole , Senator Howard Baker , Texas Governor John Connally , Congressman Phil Crane , and Congressman John B. Anderson .","prompt_labels":"Meanwhile(O) ,(O) he(O) began(O) to(O) lay(O) the(O) groundwork(O) for(O) his(O) candidacy(O) in(O) the(O) 1980(B-election) Republican(I-election) Party(I-election) presidential(I-election) primaries(I-election) .(O) In(O) the(O) 1980(O) Republican(O) primary(O) campaign(O) ,(O) Bush(B-politician) would(O) face(O) Ronald(B-politician) Reagan(I-politician) ,(O) who(O) was(O) widely(O) regarded(O) as(O) the(O) front-runner(O) ,(O) as(O) well(O) as(O) other(O) contenders(O) like(O) Senator(O) Bob(B-politician) Dole(I-politician) ,(O) Senator(O) Howard(B-politician) Baker(I-politician) ,(O) Texas(O) Governor(O) John(B-politician) Connally(I-politician) ,(O) Congressman(O) Phil(B-politician) Crane(I-politician) ,(O) and(O) Congressman(O) John(B-politician) B.(I-politician) Anderson(I-politician) .(O)"}}
{"id":"21","dataset":"crossner_politics","split":"train","label_list":["election","event","location","country","politician","person","organization","political party"],"instance":{"id":"21","words":["Congress","chose","his","primary","staff","officers",",","including","Major","General","Artemas","Ward",",","Adjutant","General","Horatio","Gates",",","Major","General","Charles","Lee",",","Major","General","Philip","Schuyler",",","Major","General","Nathanael","Greene",",","Colonel","Henry","Knox",",","and","Colonel","Alexander","Hamilton","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","B-politician","I-politician","O","O","O","B-politician","I-politician","O","O","O","B-politician","I-politician","O","O","O","B-politician","I-politician","O","O","B-politician","I-politician","O","O","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, location, country, politician, person, organization, political party and O.\nSentence: Congress chose his primary staff officers , including Major General Artemas Ward , Adjutant General Horatio Gates , Major General Charles Lee , Major General Philip Schuyler , Major General Nathanael Greene , Colonel Henry Knox , and Colonel Alexander Hamilton .","prompt_labels":"Congress(O) chose(O) his(O) primary(O) staff(O) officers(O) ,(O) including(O) Major(O) General(O) Artemas(B-politician) Ward(I-politician) ,(O) Adjutant(O) General(O) Horatio(B-politician) Gates(I-politician) ,(O) Major(O) General(O) Charles(B-politician) Lee(I-politician) ,(O) Major(O) General(O) Philip(B-politician) Schuyler(I-politician) ,(O) Major(O) General(O) Nathanael(B-politician) Greene(I-politician) ,(O) Colonel(O) Henry(B-politician) Knox(I-politician) ,(O) and(O) Colonel(O) Alexander(B-politician) Hamilton(I-politician) .(O)"}}
{"id":"22","dataset":"crossner_politics","split":"train","label_list":["election","country","political party","event","organization","politician","person","location"],"instance":{"id":"22","words":["Washington","appointed","fellow","Virginian","Edmund","Randolph","as","Attorney","General",",","Samuel","Osgood","as","Postmaster","General",",","Thomas","Jefferson","as","Secretary","of","State",",","and","Henry","Knox","as","Secretary","of","War","."],"labels":["B-politician","O","O","O","B-politician","I-politician","O","O","O","O","B-politician","I-politician","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, country, political party, event, organization, politician, person, location and O.\nSentence: Washington appointed fellow Virginian Edmund Randolph as Attorney General , Samuel Osgood as Postmaster General , Thomas Jefferson as Secretary of State , and Henry Knox as Secretary of War .","prompt_labels":"Washington(B-politician) appointed(O) fellow(O) Virginian(O) Edmund(B-politician) Randolph(I-politician) as(O) Attorney(O) General(O) ,(O) Samuel(B-politician) Osgood(I-politician) as(O) Postmaster(O) General(O) ,(O) Thomas(B-politician) Jefferson(I-politician) as(O) Secretary(O) of(O) State(O) ,(O) and(O) Henry(B-politician) Knox(I-politician) as(O) Secretary(O) of(O) War(O) .(O)"}}
{"id":"23","dataset":"crossner_politics","split":"train","label_list":["organization","political party","election","person","politician","country","event","location"],"instance":{"id":"23","words":["The","head","of","Government","is","the","Chief","Minister",",","currently","the","Hon.","Fabian","Picardo","of","the","Gibraltar","Socialist","Labour","Party","(","GSLP",")","who","has","been","in","office","since","9","December","2011",",","in","alliance","with","the","Gibraltar","Liberal","Party","(","Liberals",")",",","following","the","2011","Gibraltar","general","election","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","B-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, election, person, politician, country, event, location and O.\nSentence: The head of Government is the Chief Minister , currently the Hon. Fabian Picardo of the Gibraltar Socialist Labour Party ( GSLP ) who has been in office since 9 December 2011 , in alliance with the Gibraltar Liberal Party ( Liberals ) , following the 2011 Gibraltar general election .","prompt_labels":"The(O) head(O) of(O) Government(O) is(O) the(O) Chief(O) Minister(O) ,(O) currently(O) the(O) Hon.(O) Fabian(B-politician) Picardo(I-politician) of(O) the(O) Gibraltar(B-political party) Socialist(I-political party) Labour(I-political party) Party(I-political party) ((O) GSLP(B-political party) )(O) who(O) has(O) been(O) in(O) office(O) since(O) 9(O) December(O) 2011(O) ,(O) in(O) alliance(O) with(O) the(O) Gibraltar(B-political party) Liberal(I-political party) Party(I-political party) ((O) Liberals(B-political party) )(O) ,(O) following(O) the(O) 2011(B-election) Gibraltar(I-election) general(I-election) election(I-election) .(O)"}}
{"id":"24","dataset":"crossner_politics","split":"train","label_list":["event","person","political party","country","election","location","politician","organization"],"instance":{"id":"24","words":["On","4","October","2009",",","George","Papandreou",",","president","of","the","Panhellenic","Socialist","Movement","party","and","son","and","grandson","of","Prime","Ministers",",","2009","Greek","legislative","election","as","the","new","Prime","Minister","of","Greece",",","following","five","years","of","government","under","New","Democracy","leader","Kostas","Karamanlis",",","the","nephew","of","long-time","Prime","Minister","and","President","Konstantinos","Karamanlis","."],"labels":["O","O","O","O","O","B-politician","I-politician","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, political party, country, election, location, politician, organization and O.\nSentence: On 4 October 2009 , George Papandreou , president of the Panhellenic Socialist Movement party and son and grandson of Prime Ministers , 2009 Greek legislative election as the new Prime Minister of Greece , following five years of government under New Democracy leader Kostas Karamanlis , the nephew of long-time Prime Minister and President Konstantinos Karamanlis .","prompt_labels":"On(O) 4(O) October(O) 2009(O) ,(O) George(B-politician) Papandreou(I-politician) ,(O) president(O) of(O) the(O) Panhellenic(B-political party) Socialist(I-political party) Movement(I-political party) party(O) and(O) son(O) and(O) grandson(O) of(O) Prime(O) Ministers(O) ,(O) 2009(B-election) Greek(I-election) legislative(I-election) election(I-election) as(O) the(O) new(O) Prime(O) Minister(O) of(O) Greece(B-country) ,(O) following(O) five(O) years(O) of(O) government(O) under(O) New(O) Democracy(O) leader(O) Kostas(B-politician) Karamanlis(I-politician) ,(O) the(O) nephew(O) of(O) long-time(O) Prime(O) Minister(O) and(O) President(O) Konstantinos(B-politician) Karamanlis(I-politician) .(O)"}}
{"id":"25","dataset":"crossner_politics","split":"train","label_list":["political party","person","event","politician","election","organization","country","location"],"instance":{"id":"25","words":["In","the","latest","elections",",","which","took","place","in","2011",",","Retired","General","Otto","Pérez","Molina","of","the","Patriotic","Party","won","the","presidential","election","in","a","runoff","against","populist","Manuel","Baldizón","of","the","LIDER","party","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","B-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, event, politician, election, organization, country, location and O.\nSentence: In the latest elections , which took place in 2011 , Retired General Otto Pérez Molina of the Patriotic Party won the presidential election in a runoff against populist Manuel Baldizón of the LIDER party .","prompt_labels":"In(O) the(O) latest(O) elections(O) ,(O) which(O) took(O) place(O) in(O) 2011(O) ,(O) Retired(O) General(O) Otto(B-politician) Pérez(I-politician) Molina(I-politician) of(O) the(O) Patriotic(B-political party) Party(I-political party) won(O) the(O) presidential(O) election(O) in(O) a(O) runoff(O) against(O) populist(O) Manuel(B-politician) Baldizón(I-politician) of(O) the(O) LIDER(B-political party) party(I-political party) .(O)"}}
{"id":"26","dataset":"crossner_politics","split":"train","label_list":["location","political party","election","person","country","organization","event","politician"],"instance":{"id":"26","words":["And","in","2011",",","Retired","General","Otto","Pérez","Molina","of","the","Patriotic","Party","won","the","presidential","election","in","a","runoff","against","populist","Manuel","Baldizón","of","the","LIDER","party","."],"labels":["O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","B-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, election, person, country, organization, event, politician and O.\nSentence: And in 2011 , Retired General Otto Pérez Molina of the Patriotic Party won the presidential election in a runoff against populist Manuel Baldizón of the LIDER party .","prompt_labels":"And(O) in(O) 2011(O) ,(O) Retired(O) General(O) Otto(B-politician) Pérez(I-politician) Molina(I-politician) of(O) the(O) Patriotic(B-political party) Party(I-political party) won(O) the(O) presidential(O) election(O) in(O) a(O) runoff(O) against(O) populist(O) Manuel(B-politician) Baldizón(I-politician) of(O) the(O) LIDER(B-political party) party(I-political party) .(O)"}}
{"id":"27","dataset":"crossner_politics","split":"train","label_list":["election","political party","politician","location","country","event","person","organization"],"instance":{"id":"27","words":["Joschka","Fischer","became","Vice-Chancellor","of","Germany","and","foreign","minister","in","the","new","government",",","which","had","two","other","Green","ministers","(","Andrea","Fischer",",","later","Renate","Künast",",","and","Jürgen","Trittin",")","."],"labels":["B-politician","I-politician","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","B-politician","I-politician","O","O","B-politician","I-politician","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, politician, location, country, event, person, organization and O.\nSentence: Joschka Fischer became Vice-Chancellor of Germany and foreign minister in the new government , which had two other Green ministers ( Andrea Fischer , later Renate Künast , and Jürgen Trittin ) .","prompt_labels":"Joschka(B-politician) Fischer(I-politician) became(O) Vice-Chancellor(O) of(O) Germany(B-country) and(O) foreign(O) minister(O) in(O) the(O) new(O) government(O) ,(O) which(O) had(O) two(O) other(O) Green(O) ministers(O) ((O) Andrea(B-person) Fischer(I-person) ,(O) later(O) Renate(B-politician) Künast(I-politician) ,(O) and(O) Jürgen(B-politician) Trittin(I-politician) )(O) .(O)"}}
{"id":"28","dataset":"crossner_politics","split":"train","label_list":["political party","election","country","politician","event","person","organization","location"],"instance":{"id":"28","words":["Despite","losses","for","the","SPD",",","the","Red-Green","coalition","government","commanded","a","very","slight","majority","in","the","Bundestag","and","was","renewed",",","with","Joschka","Fischer","as","foreign","minister",",","Renate","Künast","as","minister","for","consumer","protection",",","nutrition","and","agriculture",",","and","Jürgen","Trittin","as","minister","for","the","environment","."],"labels":["O","O","O","O","B-political party","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","B-politician","I-politician","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, country, politician, event, person, organization, location and O.\nSentence: Despite losses for the SPD , the Red-Green coalition government commanded a very slight majority in the Bundestag and was renewed , with Joschka Fischer as foreign minister , Renate Künast as minister for consumer protection , nutrition and agriculture , and Jürgen Trittin as minister for the environment .","prompt_labels":"Despite(O) losses(O) for(O) the(O) SPD(B-political party) ,(O) the(O) Red-Green(B-political party) coalition(I-political party) government(O) commanded(O) a(O) very(O) slight(O) majority(O) in(O) the(O) Bundestag(B-organization) and(O) was(O) renewed(O) ,(O) with(O) Joschka(B-politician) Fischer(I-politician) as(O) foreign(O) minister(O) ,(O) Renate(B-politician) Künast(I-politician) as(O) minister(O) for(O) consumer(O) protection(O) ,(O) nutrition(O) and(O) agriculture(O) ,(O) and(O) Jürgen(B-politician) Trittin(I-politician) as(O) minister(O) for(O) the(O) environment(O) .(O)"}}
{"id":"29","dataset":"crossner_politics","split":"train","label_list":["political party","event","election","politician","country","person","organization","location"],"instance":{"id":"29","words":["As","a","result",",","former","party","chairpersons","Fritz","Kuhn","and","Claudia","Roth","(","who","had","been","elected","to","parliament","that","year",")","were","no","longer","able","to","continue","in","their","executive","function","and","were","replaced","by","former","party","secretary","general","Reinhard","Bütikofer","and","former","Bundestag","member","Angelika","Beer","."],"labels":["O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, election, politician, country, person, organization, location and O.\nSentence: As a result , former party chairpersons Fritz Kuhn and Claudia Roth ( who had been elected to parliament that year ) were no longer able to continue in their executive function and were replaced by former party secretary general Reinhard Bütikofer and former Bundestag member Angelika Beer .","prompt_labels":"As(O) a(O) result(O) ,(O) former(O) party(O) chairpersons(O) Fritz(B-politician) Kuhn(I-politician) and(O) Claudia(B-politician) Roth(I-politician) ((O) who(O) had(O) been(O) elected(O) to(O) parliament(O) that(O) year(O) )(O) were(O) no(O) longer(O) able(O) to(O) continue(O) in(O) their(O) executive(O) function(O) and(O) were(O) replaced(O) by(O) former(O) party(O) secretary(O) general(O) Reinhard(B-politician) Bütikofer(I-politician) and(O) former(O) Bundestag(O) member(O) Angelika(B-politician) Beer(I-politician) .(O)"}}
{"id":"30","dataset":"crossner_politics","split":"train","label_list":["organization","politician","election","person","location","event","political party","country"],"instance":{"id":"30","words":["On","the","train","trip","from","Washington","D.C.",",","to","Gettysburg","on","November","18",",","Lincoln","was","accompanied","by","three","members","of","his","Cabinet",",","William","H.","Seward",",","John","Palmer","Usher","and","Montgomery","Blair",",","several","foreign","officials",",","his","secretary","John","Nicolay",",","and","his","assistant","secretary",",","John","Hay","."],"labels":["O","O","O","O","O","B-location","I-location","O","O","B-location","O","O","O","O","B-politician","O","O","O","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","B-politician","I-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, election, person, location, event, political party, country and O.\nSentence: On the train trip from Washington D.C. , to Gettysburg on November 18 , Lincoln was accompanied by three members of his Cabinet , William H. Seward , John Palmer Usher and Montgomery Blair , several foreign officials , his secretary John Nicolay , and his assistant secretary , John Hay .","prompt_labels":"On(O) the(O) train(O) trip(O) from(O) Washington(B-location) D.C.(I-location) ,(O) to(O) Gettysburg(B-location) on(O) November(O) 18(O) ,(O) Lincoln(B-politician) was(O) accompanied(O) by(O) three(O) members(O) of(O) his(O) Cabinet(O) ,(O) William(B-politician) H.(I-politician) Seward(I-politician) ,(O) John(B-politician) Palmer(I-politician) Usher(I-politician) and(O) Montgomery(B-politician) Blair(I-politician) ,(O) several(O) foreign(O) officials(O) ,(O) his(O) secretary(O) John(B-politician) Nicolay(I-politician) ,(O) and(O) his(O) assistant(O) secretary(O) ,(O) John(B-politician) Hay(I-politician) .(O)"}}
{"id":"31","dataset":"crossner_politics","split":"train","label_list":["organization","location","election","country","person","political party","politician","event"],"instance":{"id":"31","words":["After","narrowly","losing","the","1969","Australian","federal","election",",","Whitlam","led","Labor","to","victory","at","the","1972","election","after","23","years","of","continuous","Liberal","Party","of","Australia","-","National","Party","of","Australia","Coalition","Government","."],"labels":["O","O","O","O","B-election","I-election","I-election","I-election","O","B-politician","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, election, country, person, political party, politician, event and O.\nSentence: After narrowly losing the 1969 Australian federal election , Whitlam led Labor to victory at the 1972 election after 23 years of continuous Liberal Party of Australia - National Party of Australia Coalition Government .","prompt_labels":"After(O) narrowly(O) losing(O) the(O) 1969(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) Whitlam(B-politician) led(O) Labor(B-political party) to(O) victory(O) at(O) the(O) 1972(O) election(O) after(O) 23(O) years(O) of(O) continuous(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) -(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) Coalition(O) Government(O) .(O)"}}
{"id":"32","dataset":"crossner_politics","split":"train","label_list":["country","election","person","politician","political party","location","organization","event"],"instance":{"id":"32","words":["The","Prime","Minister",",","Robert","Menzies",",","adroitly","used","the","defection","of","a","Soviet","official","to","his","advantage",",","and","his","coalition","of","the","Liberal","and","National","Party","of","Australia","parties","was","returned","in","the","1954","Australian","federal","election","with","a","seven-seat","majority","."],"labels":["O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, person, politician, political party, location, organization, event and O.\nSentence: The Prime Minister , Robert Menzies , adroitly used the defection of a Soviet official to his advantage , and his coalition of the Liberal and National Party of Australia parties was returned in the 1954 Australian federal election with a seven-seat majority .","prompt_labels":"The(O) Prime(O) Minister(O) ,(O) Robert(B-politician) Menzies(I-politician) ,(O) adroitly(O) used(O) the(O) defection(O) of(O) a(O) Soviet(O) official(O) to(O) his(O) advantage(O) ,(O) and(O) his(O) coalition(O) of(O) the(O) Liberal(B-political party) and(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) parties(O) was(O) returned(O) in(O) the(O) 1954(B-election) Australian(I-election) federal(I-election) election(I-election) with(O) a(O) seven-seat(O) majority(O) .(O)"}}
{"id":"33","dataset":"crossner_politics","split":"train","label_list":["organization","political party","event","politician","location","person","election","country"],"instance":{"id":"33","words":["Most","of","the","party","'s","major","figures",",","including","Evatt",",","Deputy","Leader","Arthur","Calwell",",","Eddie","Ward",",","and","Reg","Pollard",",","were","in","their","sixties",",","twenty","years","older","than","Whitlam","."],"labels":["O","O","O","O","O","O","O","O","O","B-politician","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","B-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, event, politician, location, person, election, country and O.\nSentence: Most of the party 's major figures , including Evatt , Deputy Leader Arthur Calwell , Eddie Ward , and Reg Pollard , were in their sixties , twenty years older than Whitlam .","prompt_labels":"Most(O) of(O) the(O) party(O) 's(O) major(O) figures(O) ,(O) including(O) Evatt(B-politician) ,(O) Deputy(O) Leader(O) Arthur(B-politician) Calwell(I-politician) ,(O) Eddie(B-politician) Ward(I-politician) ,(O) and(O) Reg(B-politician) Pollard(I-politician) ,(O) were(O) in(O) their(O) sixties(O) ,(O) twenty(O) years(O) older(O) than(O) Whitlam(B-politician) .(O)"}}
{"id":"34","dataset":"crossner_politics","split":"train","label_list":["location","country","political party","election","event","politician","organization","person"],"instance":{"id":"34","words":["Stephen","Grover","Cleveland","(","March","18",",","1837","He","won","the","popular","vote","for","three","presidential","elections","-","in","1884","United","States","presidential","election",",","1888","United","States","presidential","election",",","and","1892","United","States","presidential","election","-","and","was","one","of","two","Democrats","(","with","Woodrow","Wilson",")","to","be","elected","president","during","the","era","of","Republican","political","domination","dating","from","1861","to","1933","."],"labels":["B-politician","I-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, political party, election, event, politician, organization, person and O.\nSentence: Stephen Grover Cleveland ( March 18 , 1837 He won the popular vote for three presidential elections - in 1884 United States presidential election , 1888 United States presidential election , and 1892 United States presidential election - and was one of two Democrats ( with Woodrow Wilson ) to be elected president during the era of Republican political domination dating from 1861 to 1933 .","prompt_labels":"Stephen(B-politician) Grover(I-politician) Cleveland(I-politician) ((O) March(O) 18(O) ,(O) 1837(O) He(O) won(O) the(O) popular(O) vote(O) for(O) three(O) presidential(O) elections(O) -(O) in(O) 1884(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) 1888(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) and(O) 1892(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) -(O) and(O) was(O) one(O) of(O) two(O) Democrats(O) ((O) with(O) Woodrow(B-politician) Wilson(I-politician) )(O) to(O) be(O) elected(O) president(O) during(O) the(O) era(O) of(O) Republican(O) political(O) domination(O) dating(O) from(O) 1861(O) to(O) 1933(O) .(O)"}}
{"id":"35","dataset":"crossner_politics","split":"train","label_list":["country","election","event","location","person","political party","politician","organization"],"instance":{"id":"35","words":["The","Republicans","gained","the","upper","hand","in","the","campaign",",","as","Cleveland","'s","campaign","was","poorly","managed","by","Calvin","S.","Brice","and","William","H.","Barnum",",","whereas","Harrison","had","engaged","more","aggressive","fundraisers","and","tacticians","in","Matthew","Quay","and","John","Wanamaker","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, event, location, person, political party, politician, organization and O.\nSentence: The Republicans gained the upper hand in the campaign , as Cleveland 's campaign was poorly managed by Calvin S. Brice and William H. Barnum , whereas Harrison had engaged more aggressive fundraisers and tacticians in Matthew Quay and John Wanamaker .","prompt_labels":"The(O) Republicans(O) gained(O) the(O) upper(O) hand(O) in(O) the(O) campaign(O) ,(O) as(O) Cleveland(O) 's(O) campaign(O) was(O) poorly(O) managed(O) by(O) Calvin(B-politician) S.(I-politician) Brice(I-politician) and(O) William(B-politician) H.(I-politician) Barnum(I-politician) ,(O) whereas(O) Harrison(O) had(O) engaged(O) more(O) aggressive(O) fundraisers(O) and(O) tacticians(O) in(O) Matthew(B-politician) Quay(I-politician) and(O) John(B-politician) Wanamaker(I-politician) .(O)"}}
{"id":"36","dataset":"crossner_politics","split":"train","label_list":["location","organization","politician","political party","election","person","country","event"],"instance":{"id":"36","words":["Various","Governors-General","had","previously","served","as","governors","of","an","Australian","state","or","colony",":","Lord","Hopetoun","(","Victoria","1889-1895",")",";","Lord","Tennyson","(","South","Australia","1899-1902",")",";","Lord","Gowrie","(","South","Australia","1928-34",";","and","New","South","Wales","1935-1936",")",";","Major","General","Michael","Jeffery","(","Western","Australia","1993-2000",")",";","Quentin","Bryce","(","Queensland","2003-2008",")",";","General","David","Hurley","(","New","South","Wales","2014","-","2019",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-location","O","O","O","B-politician","I-politician","O","B-location","I-location","O","O","O","B-politician","I-politician","O","B-location","I-location","O","O","O","B-location","I-location","I-location","O","O","O","O","O","B-politician","I-politician","O","B-location","I-location","O","O","O","B-politician","I-politician","O","B-location","O","O","O","O","B-politician","I-politician","O","B-location","I-location","I-location","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, politician, political party, election, person, country, event and O.\nSentence: Various Governors-General had previously served as governors of an Australian state or colony : Lord Hopetoun ( Victoria 1889-1895 ) ; Lord Tennyson ( South Australia 1899-1902 ) ; Lord Gowrie ( South Australia 1928-34 ; and New South Wales 1935-1936 ) ; Major General Michael Jeffery ( Western Australia 1993-2000 ) ; Quentin Bryce ( Queensland 2003-2008 ) ; General David Hurley ( New South Wales 2014 - 2019 ) .","prompt_labels":"Various(O) Governors-General(O) had(O) previously(O) served(O) as(O) governors(O) of(O) an(O) Australian(O) state(O) or(O) colony(O) :(O) Lord(B-politician) Hopetoun(I-politician) ((O) Victoria(B-location) 1889-1895(O) )(O) ;(O) Lord(B-politician) Tennyson(I-politician) ((O) South(B-location) Australia(I-location) 1899-1902(O) )(O) ;(O) Lord(B-politician) Gowrie(I-politician) ((O) South(B-location) Australia(I-location) 1928-34(O) ;(O) and(O) New(B-location) South(I-location) Wales(I-location) 1935-1936(O) )(O) ;(O) Major(O) General(O) Michael(B-politician) Jeffery(I-politician) ((O) Western(B-location) Australia(I-location) 1993-2000(O) )(O) ;(O) Quentin(B-politician) Bryce(I-politician) ((O) Queensland(B-location) 2003-2008(O) )(O) ;(O) General(O) David(B-politician) Hurley(I-politician) ((O) New(B-location) South(I-location) Wales(I-location) 2014(O) -(O) 2019(O) )(O) .(O)"}}
{"id":"37","dataset":"crossner_politics","split":"train","label_list":["location","political party","person","election","politician","organization","country","event"],"instance":{"id":"37","words":["More","recent","Governors-General","in","this","category","include","Lord","Casey",",","Paul","Hasluck",",","Sir","John","Kerr",",","Ninian","Stephen",",","Bill","Hayden","and","Sir","William","Deane","."],"labels":["O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, person, election, politician, organization, country, event and O.\nSentence: More recent Governors-General in this category include Lord Casey , Paul Hasluck , Sir John Kerr , Ninian Stephen , Bill Hayden and Sir William Deane .","prompt_labels":"More(O) recent(O) Governors-General(O) in(O) this(O) category(O) include(O) Lord(B-politician) Casey(I-politician) ,(O) Paul(B-politician) Hasluck(I-politician) ,(O) Sir(B-politician) John(I-politician) Kerr(I-politician) ,(O) Ninian(B-politician) Stephen(I-politician) ,(O) Bill(B-politician) Hayden(I-politician) and(O) Sir(B-politician) William(I-politician) Deane(I-politician) .(O)"}}
{"id":"38","dataset":"crossner_politics","split":"train","label_list":["event","politician","location","country","person","organization","political party","election"],"instance":{"id":"38","words":["In","the","nineteenth","century",",","the","members","of","the","failed","Decembrist","revolt",",","Polish","nobles","who","resisted","Russian","rule",",","and","members","of","various","socialist","revolutionary","groups",",","including","Bolsheviks","such","as","Sergo","Ordzhonikidze",",","Leon","Trotsky",",","and","Joseph","Stalin","were","all","sent","into","exile","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","O","B-politician","I-politician","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, location, country, person, organization, political party, election and O.\nSentence: In the nineteenth century , the members of the failed Decembrist revolt , Polish nobles who resisted Russian rule , and members of various socialist revolutionary groups , including Bolsheviks such as Sergo Ordzhonikidze , Leon Trotsky , and Joseph Stalin were all sent into exile .","prompt_labels":"In(O) the(O) nineteenth(O) century(O) ,(O) the(O) members(O) of(O) the(O) failed(O) Decembrist(B-event) revolt(I-event) ,(O) Polish(O) nobles(O) who(O) resisted(O) Russian(O) rule(O) ,(O) and(O) members(O) of(O) various(O) socialist(O) revolutionary(O) groups(O) ,(O) including(O) Bolsheviks(O) such(O) as(O) Sergo(B-politician) Ordzhonikidze(I-politician) ,(O) Leon(B-politician) Trotsky(I-politician) ,(O) and(O) Joseph(B-politician) Stalin(I-politician) were(O) all(O) sent(O) into(O) exile(O) .(O)"}}
{"id":"39","dataset":"crossner_politics","split":"train","label_list":["person","location","organization","politician","event","election","political party","country"],"instance":{"id":"39","words":["It","is","often","stated","that","incidents","precede","the","1789","election","of","the","First","U.S.","Congress",":","namely",",","when","Patrick","Henry","and","his","Anti-Federalist","allies","were","in","control","of","the","Virginia","House","of","Delegates","in","1788",",","they","drew","the","boundaries","of","Virginia","'s","5th","congressional","district","in","an","unsuccessful","attempt","to","keep","James","Madison","out","of","the","U.S.","House","of","Representatives","via","the","candidacy","of","James","Monroe","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, organization, politician, event, election, political party, country and O.\nSentence: It is often stated that incidents precede the 1789 election of the First U.S. Congress : namely , when Patrick Henry and his Anti-Federalist allies were in control of the Virginia House of Delegates in 1788 , they drew the boundaries of Virginia 's 5th congressional district in an unsuccessful attempt to keep James Madison out of the U.S. House of Representatives via the candidacy of James Monroe .","prompt_labels":"It(O) is(O) often(O) stated(O) that(O) incidents(O) precede(O) the(O) 1789(O) election(O) of(O) the(O) First(O) U.S.(O) Congress(O) :(O) namely(O) ,(O) when(O) Patrick(B-politician) Henry(I-politician) and(O) his(O) Anti-Federalist(O) allies(O) were(O) in(O) control(O) of(O) the(O) Virginia(B-organization) House(I-organization) of(I-organization) Delegates(I-organization) in(O) 1788(O) ,(O) they(O) drew(O) the(O) boundaries(O) of(O) Virginia(B-location) 's(O) 5th(O) congressional(O) district(O) in(O) an(O) unsuccessful(O) attempt(O) to(O) keep(O) James(B-politician) Madison(I-politician) out(O) of(O) the(O) U.S.(O) House(O) of(O) Representatives(O) via(O) the(O) candidacy(O) of(O) James(B-politician) Monroe(I-politician) .(O)"}}
{"id":"40","dataset":"crossner_politics","split":"train","label_list":["event","country","election","politician","political party","organization","location","person"],"instance":{"id":"40","words":["In","the","subsequent","election",",","Hugo","Chávez","'s","political","party",",","the","United","Socialist","Party","of","Venezuela","drew","48","%","of","the","votes","overall",",","while","the","opposition","parties","(","the","Democratic","Unity","Roundtable","and","the","Fatherland","for","All","parties",")","drew","52","%","of","the","votes","."],"labels":["O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, election, politician, political party, organization, location, person and O.\nSentence: In the subsequent election , Hugo Chávez 's political party , the United Socialist Party of Venezuela drew 48 % of the votes overall , while the opposition parties ( the Democratic Unity Roundtable and the Fatherland for All parties ) drew 52 % of the votes .","prompt_labels":"In(O) the(O) subsequent(O) election(O) ,(O) Hugo(B-politician) Chávez(I-politician) 's(O) political(O) party(O) ,(O) the(O) United(B-political party) Socialist(I-political party) Party(I-political party) of(I-political party) Venezuela(I-political party) drew(O) 48(O) %(O) of(O) the(O) votes(O) overall(O) ,(O) while(O) the(O) opposition(O) parties(O) ((O) the(O) Democratic(B-political party) Unity(I-political party) Roundtable(I-political party) and(O) the(O) Fatherland(B-political party) for(I-political party) All(I-political party) parties(O) )(O) drew(O) 52(O) %(O) of(O) the(O) votes(O) .(O)"}}
{"id":"41","dataset":"crossner_politics","split":"train","label_list":["location","country","election","event","organization","person","political party","politician"],"instance":{"id":"41","words":["In","the","last","years",",","Honduras","has","had","five","Liberal","presidents",":","Roberto","Suazo","Córdova",",","José","Azcona","del","Hoyo",",","Carlos","Roberto","Reina",",","Carlos","Roberto","Flores","and","Manuel","Zelaya",",","and","three","Nationalists",":","Rafael","Leonardo","Callejas","Romero",",","Porfirio","Lobo","Sosa","and","Ricardo","Maduro","."],"labels":["O","O","O","O","O","B-country","O","O","O","B-political party","O","O","B-politician","I-politician","I-politician","O","B-politician","I-politician","I-politician","I-politician","O","B-politician","I-politician","I-politician","O","B-politician","I-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","B-politician","I-politician","I-politician","I-politician","O","B-politician","I-politician","I-politician","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, election, event, organization, person, political party, politician and O.\nSentence: In the last years , Honduras has had five Liberal presidents : Roberto Suazo Córdova , José Azcona del Hoyo , Carlos Roberto Reina , Carlos Roberto Flores and Manuel Zelaya , and three Nationalists : Rafael Leonardo Callejas Romero , Porfirio Lobo Sosa and Ricardo Maduro .","prompt_labels":"In(O) the(O) last(O) years(O) ,(O) Honduras(B-country) has(O) had(O) five(O) Liberal(B-political party) presidents(O) :(O) Roberto(B-politician) Suazo(I-politician) Córdova(I-politician) ,(O) José(B-politician) Azcona(I-politician) del(I-politician) Hoyo(I-politician) ,(O) Carlos(B-politician) Roberto(I-politician) Reina(I-politician) ,(O) Carlos(B-politician) Roberto(I-politician) Flores(I-politician) and(O) Manuel(B-politician) Zelaya(I-politician) ,(O) and(O) three(O) Nationalists(O) :(O) Rafael(B-politician) Leonardo(I-politician) Callejas(I-politician) Romero(I-politician) ,(O) Porfirio(B-politician) Lobo(I-politician) Sosa(I-politician) and(O) Ricardo(B-politician) Maduro(I-politician) .(O)"}}
{"id":"42","dataset":"crossner_politics","split":"train","label_list":["politician","country","political party","event","person","election","location","organization"],"instance":{"id":"42","words":["Manuel","Zelaya","of","the","Liberal","Party","of","Honduras","(","Partido","Liberal","de","Honduras",":","PLH",")","won",",","with","Porfirio","Pepe","Lobo","of","the","National","Party","of","Honduras","(","Partido","Nacional","de","Honduras",":","PNH",")","coming","in","second","."],"labels":["B-politician","I-politician","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","B-politician","I-politician","I-politician","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, political party, event, person, election, location, organization and O.\nSentence: Manuel Zelaya of the Liberal Party of Honduras ( Partido Liberal de Honduras : PLH ) won , with Porfirio Pepe Lobo of the National Party of Honduras ( Partido Nacional de Honduras : PNH ) coming in second .","prompt_labels":"Manuel(B-politician) Zelaya(I-politician) of(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Honduras(I-political party) ((O) Partido(B-political party) Liberal(I-political party) de(I-political party) Honduras(I-political party) :(O) PLH(B-political party) )(O) won(O) ,(O) with(O) Porfirio(B-politician) Pepe(I-politician) Lobo(I-politician) of(O) the(O) National(B-political party) Party(I-political party) of(I-political party) Honduras(I-political party) ((O) Partido(B-political party) Nacional(I-political party) de(I-political party) Honduras(I-political party) :(O) PNH(B-political party) )(O) coming(O) in(O) second(O) .(O)"}}
{"id":"43","dataset":"crossner_politics","split":"train","label_list":["location","country","event","person","political party","politician","election","organization"],"instance":{"id":"43","words":["This","did","not","happen","and","the","presidency",",","having","been","damaged","by","three","late","nineteenth","and","early","twentieth","century","assassinations","(","Abraham","Lincoln",",","Garfield","and","William","McKinley",")","and","one","impeachment","(","Johnson",")",",","reasserted","its","political","dominance","by","the","early","twentieth","century","through","such","figures","as","Theodore","Roosevelt","and","Woodrow","Wilson","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","O","B-politician","I-politician","O","O","O","O","O","B-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, event, person, political party, politician, election, organization and O.\nSentence: This did not happen and the presidency , having been damaged by three late nineteenth and early twentieth century assassinations ( Abraham Lincoln , Garfield and William McKinley ) and one impeachment ( Johnson ) , reasserted its political dominance by the early twentieth century through such figures as Theodore Roosevelt and Woodrow Wilson .","prompt_labels":"This(O) did(O) not(O) happen(O) and(O) the(O) presidency(O) ,(O) having(O) been(O) damaged(O) by(O) three(O) late(O) nineteenth(O) and(O) early(O) twentieth(O) century(O) assassinations(O) ((O) Abraham(B-politician) Lincoln(I-politician) ,(O) Garfield(B-politician) and(O) William(B-politician) McKinley(I-politician) )(O) and(O) one(O) impeachment(O) ((O) Johnson(B-politician) )(O) ,(O) reasserted(O) its(O) political(O) dominance(O) by(O) the(O) early(O) twentieth(O) century(O) through(O) such(O) figures(O) as(O) Theodore(B-politician) Roosevelt(I-politician) and(O) Woodrow(B-politician) Wilson(I-politician) .(O)"}}
{"id":"44","dataset":"crossner_politics","split":"train","label_list":["country","politician","location","political party","person","event","election","organization"],"instance":{"id":"44","words":["Genscher","was","one","of","the","FDP","'s","driving","forces","when",",","in","1982",",","the","party","switched","sides","from","its","coalition","with","the","SPD","to","support","the","Christian","Democratic","Union","of","Germany","in","their","Constructive","vote","of","no","confidence","to","have","incumbent","Helmut","Schmidt","replaced","with","opposition","leader","Helmut","Kohl","as","Chancellor","."],"labels":["B-politician","O","O","O","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","B-politician","I-politician","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, politician, location, political party, person, event, election, organization and O.\nSentence: Genscher was one of the FDP 's driving forces when , in 1982 , the party switched sides from its coalition with the SPD to support the Christian Democratic Union of Germany in their Constructive vote of no confidence to have incumbent Helmut Schmidt replaced with opposition leader Helmut Kohl as Chancellor .","prompt_labels":"Genscher(B-politician) was(O) one(O) of(O) the(O) FDP(B-political party) 's(O) driving(O) forces(O) when(O) ,(O) in(O) 1982(O) ,(O) the(O) party(O) switched(O) sides(O) from(O) its(O) coalition(O) with(O) the(O) SPD(B-political party) to(O) support(O) the(O) Christian(B-political party) Democratic(I-political party) Union(I-political party) of(I-political party) Germany(I-political party) in(O) their(O) Constructive(O) vote(O) of(O) no(O) confidence(O) to(O) have(O) incumbent(O) Helmut(B-politician) Schmidt(I-politician) replaced(O) with(O) opposition(O) leader(O) Helmut(B-politician) Kohl(I-politician) as(O) Chancellor(O) .(O)"}}
{"id":"45","dataset":"crossner_politics","split":"train","label_list":["person","election","politician","location","event","country","political party","organization"],"instance":{"id":"45","words":["Following","Genscher","'s","resignation",",","Chancellor","Helmut","Kohl","and","FDP","chairman","Otto","Graf","Lambsdorff","named","Irmgard","Schwaetzer",",","a","former","aide","to","Genscher",",","to","be","the","new","Foreign","Minister","."],"labels":["O","B-politician","O","O","O","O","B-politician","I-politician","O","B-political party","O","B-politician","I-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","B-politician","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, politician, location, event, country, political party, organization and O.\nSentence: Following Genscher 's resignation , Chancellor Helmut Kohl and FDP chairman Otto Graf Lambsdorff named Irmgard Schwaetzer , a former aide to Genscher , to be the new Foreign Minister .","prompt_labels":"Following(O) Genscher(B-politician) 's(O) resignation(O) ,(O) Chancellor(O) Helmut(B-politician) Kohl(I-politician) and(O) FDP(B-political party) chairman(O) Otto(B-politician) Graf(I-politician) Lambsdorff(I-politician) named(O) Irmgard(B-politician) Schwaetzer(I-politician) ,(O) a(O) former(O) aide(O) to(O) Genscher(B-politician) ,(O) to(O) be(O) the(O) new(O) Foreign(O) Minister(O) .(O)"}}
{"id":"46","dataset":"crossner_politics","split":"train","label_list":["location","event","country","political party","politician","organization","person","election"],"instance":{"id":"46","words":["Hoover","backed","conservative","leader","Robert","A.","Taft","at","the","1952","Republican","National","Convention",",","but","the","party","'s","presidential","nomination","instead","went","to","Dwight","D.","Eisenhower",",","who","went","on","to","win","the","1952","United","States","presidential","election","."],"labels":["B-politician","O","O","O","B-politician","I-politician","I-politician","O","O","B-event","I-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, country, political party, politician, organization, person, election and O.\nSentence: Hoover backed conservative leader Robert A. Taft at the 1952 Republican National Convention , but the party 's presidential nomination instead went to Dwight D. Eisenhower , who went on to win the 1952 United States presidential election .","prompt_labels":"Hoover(B-politician) backed(O) conservative(O) leader(O) Robert(B-politician) A.(I-politician) Taft(I-politician) at(O) the(O) 1952(B-event) Republican(I-event) National(I-event) Convention(I-event) ,(O) but(O) the(O) party(O) 's(O) presidential(O) nomination(O) instead(O) went(O) to(O) Dwight(B-politician) D.(I-politician) Eisenhower(I-politician) ,(O) who(O) went(O) on(O) to(O) win(O) the(O) 1952(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) .(O)"}}
{"id":"47","dataset":"crossner_politics","split":"train","label_list":["political party","person","organization","event","country","location","politician","election"],"instance":{"id":"47","words":["Kissinger","-","along","with","William","Perry",",","Sam","Nunn",",","and","George","Shultz","-","has","called","upon","governments","to","embrace","the","vision","of","a","world","free","of","nuclear","weapons",",","and","in","three","Wall","Street","Journal","op-ed","s","proposed","an","ambitious","program","of","urgent","steps","to","that","end","."],"labels":["B-politician","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, organization, event, country, location, politician, election and O.\nSentence: Kissinger - along with William Perry , Sam Nunn , and George Shultz - has called upon governments to embrace the vision of a world free of nuclear weapons , and in three Wall Street Journal op-ed s proposed an ambitious program of urgent steps to that end .","prompt_labels":"Kissinger(B-politician) -(O) along(O) with(O) William(B-politician) Perry(I-politician) ,(O) Sam(B-politician) Nunn(I-politician) ,(O) and(O) George(B-politician) Shultz(I-politician) -(O) has(O) called(O) upon(O) governments(O) to(O) embrace(O) the(O) vision(O) of(O) a(O) world(O) free(O) of(O) nuclear(O) weapons(O) ,(O) and(O) in(O) three(O) Wall(B-organization) Street(I-organization) Journal(I-organization) op-ed(O) s(O) proposed(O) an(O) ambitious(O) program(O) of(O) urgent(O) steps(O) to(O) that(O) end(O) .(O)"}}
{"id":"48","dataset":"crossner_politics","split":"train","label_list":["political party","location","election","organization","person","politician","country","event"],"instance":{"id":"48","words":["Congress","members","Tom","Lantos",",","Jim","Saxton",",","Thad","McCotter",",","Chris","Shays",",","Charles","Boustany",",","Alcee","Hastings",",","and","Robert","Wexler","referred","to","Hezbollah","as","a","terrorist","organization","in","their","speeches","supporting","the","legislation.Congress.","denounces","terrorism",",","and","I","have","to","take","him","at","his","word","."],"labels":["O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","O","B-politician","I-politician","O","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, location, election, organization, person, politician, country, event and O.\nSentence: Congress members Tom Lantos , Jim Saxton , Thad McCotter , Chris Shays , Charles Boustany , Alcee Hastings , and Robert Wexler referred to Hezbollah as a terrorist organization in their speeches supporting the legislation.Congress. denounces terrorism , and I have to take him at his word .","prompt_labels":"Congress(O) members(O) Tom(B-politician) Lantos(I-politician) ,(O) Jim(B-politician) Saxton(I-politician) ,(O) Thad(B-politician) McCotter(I-politician) ,(O) Chris(B-politician) Shays(I-politician) ,(O) Charles(B-politician) Boustany(I-politician) ,(O) Alcee(B-politician) Hastings(I-politician) ,(O) and(O) Robert(B-politician) Wexler(I-politician) referred(O) to(O) Hezbollah(B-political party) as(O) a(O) terrorist(O) organization(O) in(O) their(O) speeches(O) supporting(O) the(O) legislation.Congress.(O) denounces(O) terrorism(O) ,(O) and(O) I(O) have(O) to(O) take(O) him(O) at(O) his(O) word(O) .(O)"}}
{"id":"49","dataset":"crossner_politics","split":"train","label_list":["organization","politician","person","country","event","location","election","political party"],"instance":{"id":"49","words":["Hamlin","survived","six","of","his","successors","in","the","vice","presidency",":","Andrew","Johnson",",","Schuyler","Colfax",",","Henry","Wilson",",","William","A.","Wheeler",",","Chester","A.","Arthur",",","and","Thomas","A.","Hendricks","."],"labels":["B-politician","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","I-politician","O","B-politician","I-politician","I-politician","O","O","B-politician","I-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, person, country, event, location, election, political party and O.\nSentence: Hamlin survived six of his successors in the vice presidency : Andrew Johnson , Schuyler Colfax , Henry Wilson , William A. Wheeler , Chester A. Arthur , and Thomas A. Hendricks .","prompt_labels":"Hamlin(B-politician) survived(O) six(O) of(O) his(O) successors(O) in(O) the(O) vice(O) presidency(O) :(O) Andrew(B-politician) Johnson(I-politician) ,(O) Schuyler(B-politician) Colfax(I-politician) ,(O) Henry(B-politician) Wilson(I-politician) ,(O) William(B-politician) A.(I-politician) Wheeler(I-politician) ,(O) Chester(B-politician) A.(I-politician) Arthur(I-politician) ,(O) and(O) Thomas(B-politician) A.(I-politician) Hendricks(I-politician) .(O)"}}
{"id":"50","dataset":"crossner_politics","split":"train","label_list":["location","country","politician","election","person","organization","political party","event"],"instance":{"id":"50","words":["Thomas","Wolsey",",","Thomas","More",",","Thomas","Cromwell",",","Richard","Rich",",","and","Thomas","Cranmer","all","figured","prominently","in","his","administration","."],"labels":["B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","O","B-politician","I-politician","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, politician, election, person, organization, political party, event and O.\nSentence: Thomas Wolsey , Thomas More , Thomas Cromwell , Richard Rich , and Thomas Cranmer all figured prominently in his administration .","prompt_labels":"Thomas(B-politician) Wolsey(I-politician) ,(O) Thomas(B-politician) More(I-politician) ,(O) Thomas(B-politician) Cromwell(I-politician) ,(O) Richard(B-politician) Rich(I-politician) ,(O) and(O) Thomas(B-politician) Cranmer(I-politician) all(O) figured(O) prominently(O) in(O) his(O) administration(O) .(O)"}}
{"id":"51","dataset":"crossner_politics","split":"train","label_list":["location","person","election","country","organization","event","politician","political party"],"instance":{"id":"51","words":["Piłsudski",",","together","with","Colonel","Kazimierz","Sosnkowski",",","was","greeted","at","Warsaw","'s","railway","station","by","Regent","Zdzisław","Lubomirski","and","by","Colonel","Adam","Koc","."],"labels":["B-politician","O","O","O","O","B-politician","I-politician","O","O","O","O","B-location","I-location","I-location","I-location","O","O","B-politician","I-politician","O","O","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, election, country, organization, event, politician, political party and O.\nSentence: Piłsudski , together with Colonel Kazimierz Sosnkowski , was greeted at Warsaw 's railway station by Regent Zdzisław Lubomirski and by Colonel Adam Koc .","prompt_labels":"Piłsudski(B-politician) ,(O) together(O) with(O) Colonel(O) Kazimierz(B-politician) Sosnkowski(I-politician) ,(O) was(O) greeted(O) at(O) Warsaw(B-location) 's(I-location) railway(I-location) station(I-location) by(O) Regent(O) Zdzisław(B-politician) Lubomirski(I-politician) and(O) by(O) Colonel(O) Adam(B-politician) Koc(I-politician) .(O)"}}
{"id":"52","dataset":"crossner_politics","split":"train","label_list":["politician","location","person","political party","country","event","election","organization"],"instance":{"id":"52","words":["The","major","political","parties","at","this","time","were","the","Polish","Socialist","Party",",","National","Democracy",",","various","Peasant","Parties",",","Polish","Christian","Democratic","Party",",","and","political","groups","of","ethnic","minorities","(","German",":","German","Social","Democratic","Party","of","Poland",",","Jewish",":","General","Jewish","Labour","Bund","in","Poland",",","United","Jewish","Socialist","Workers","Party",",","and","Ukrainian",":","Ukrainian","National","Democratic","Alliance",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","O","O","B-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, person, political party, country, event, election, organization and O.\nSentence: The major political parties at this time were the Polish Socialist Party , National Democracy , various Peasant Parties , Polish Christian Democratic Party , and political groups of ethnic minorities ( German : German Social Democratic Party of Poland , Jewish : General Jewish Labour Bund in Poland , United Jewish Socialist Workers Party , and Ukrainian : Ukrainian National Democratic Alliance ) .","prompt_labels":"The(O) major(O) political(O) parties(O) at(O) this(O) time(O) were(O) the(O) Polish(B-political party) Socialist(I-political party) Party(I-political party) ,(O) National(B-political party) Democracy(I-political party) ,(O) various(O) Peasant(B-political party) Parties(I-political party) ,(O) Polish(B-political party) Christian(I-political party) Democratic(I-political party) Party(I-political party) ,(O) and(O) political(O) groups(O) of(O) ethnic(O) minorities(O) ((O) German(O) :(O) German(B-political party) Social(I-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Poland(I-political party) ,(O) Jewish(O) :(O) General(B-political party) Jewish(I-political party) Labour(I-political party) Bund(I-political party) in(I-political party) Poland(I-political party) ,(O) United(B-political party) Jewish(I-political party) Socialist(I-political party) Workers(I-political party) Party(I-political party) ,(O) and(O) Ukrainian(O) :(O) Ukrainian(B-political party) National(I-political party) Democratic(I-political party) Alliance(I-political party) )(O) .(O)"}}
{"id":"53","dataset":"crossner_politics","split":"train","label_list":["location","political party","election","organization","event","politician","country","person"],"instance":{"id":"53","words":["The","following","three","parliamentary","elections","(","in","1930","Polish","legislative","election",",","1935","Polish","legislative","election","and","1938","Polish","legislative","election",")","were","manipulated",",","with","opposition","activists","sent","to","Bereza","Kartuska","prison","(","see","also","Brest","trials",")","."],"labels":["O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, election, organization, event, politician, country, person and O.\nSentence: The following three parliamentary elections ( in 1930 Polish legislative election , 1935 Polish legislative election and 1938 Polish legislative election ) were manipulated , with opposition activists sent to Bereza Kartuska prison ( see also Brest trials ) .","prompt_labels":"The(O) following(O) three(O) parliamentary(O) elections(O) ((O) in(O) 1930(B-election) Polish(I-election) legislative(I-election) election(I-election) ,(O) 1935(B-election) Polish(I-election) legislative(I-election) election(I-election) and(O) 1938(B-election) Polish(I-election) legislative(I-election) election(I-election) )(O) were(O) manipulated(O) ,(O) with(O) opposition(O) activists(O) sent(O) to(O) Bereza(B-location) Kartuska(I-location) prison(I-location) ((O) see(O) also(O) Brest(O) trials(O) )(O) .(O)"}}
{"id":"54","dataset":"crossner_politics","split":"train","label_list":["location","event","country","election","politician","political party","organization","person"],"instance":{"id":"54","words":["During","the","last","four","years","of","the","Second","Polish","Republic",",","the","major","politicians","included","President","Ignacy","Mościcki",",","Foreign","Minister","Józef","Beck","and","the","Commander-in-Chief","of","the","Polish","Army",",","Edward","Rydz-Śmigły","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, country, election, politician, political party, organization, person and O.\nSentence: During the last four years of the Second Polish Republic , the major politicians included President Ignacy Mościcki , Foreign Minister Józef Beck and the Commander-in-Chief of the Polish Army , Edward Rydz-Śmigły .","prompt_labels":"During(O) the(O) last(O) four(O) years(O) of(O) the(O) Second(O) Polish(O) Republic(O) ,(O) the(O) major(O) politicians(O) included(O) President(O) Ignacy(B-politician) Mościcki(I-politician) ,(O) Foreign(O) Minister(O) Józef(B-politician) Beck(I-politician) and(O) the(O) Commander-in-Chief(O) of(O) the(O) Polish(O) Army(O) ,(O) Edward(B-politician) Rydz-Śmigły(I-politician) .(O)"}}
{"id":"55","dataset":"crossner_politics","split":"train","label_list":["location","politician","event","country","political party","person","organization","election"],"instance":{"id":"55","words":["The","Soviet","Politburo","decisions","were","guided","by","a","Special","Commission","on","Afghanistan",",","which","consisted","of","Yuri","Andropov","the","KGB","chairman",",","Andrei","Gromyko","the","Minister","of","Foreign","Affairs",",","Defence","Minister","Dmitriy","Ustinov",",","and","Boris","Ponomarev",",","the","head","of","the","International","Department","of","the","Central","Committee","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","B-politician","I-politician","O","B-organization","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","B-politician","I-politician","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, event, country, political party, person, organization, election and O.\nSentence: The Soviet Politburo decisions were guided by a Special Commission on Afghanistan , which consisted of Yuri Andropov the KGB chairman , Andrei Gromyko the Minister of Foreign Affairs , Defence Minister Dmitriy Ustinov , and Boris Ponomarev , the head of the International Department of the Central Committee .","prompt_labels":"The(O) Soviet(O) Politburo(O) decisions(O) were(O) guided(O) by(O) a(O) Special(O) Commission(O) on(O) Afghanistan(B-country) ,(O) which(O) consisted(O) of(O) Yuri(B-politician) Andropov(I-politician) the(O) KGB(B-organization) chairman(O) ,(O) Andrei(B-politician) Gromyko(I-politician) the(O) Minister(O) of(O) Foreign(O) Affairs(O) ,(O) Defence(O) Minister(O) Dmitriy(B-politician) Ustinov(I-politician) ,(O) and(O) Boris(B-politician) Ponomarev(I-politician) ,(O) the(O) head(O) of(O) the(O) International(B-organization) Department(I-organization) of(I-organization) the(I-organization) Central(I-organization) Committee(I-organization) .(O)"}}
{"id":"56","dataset":"crossner_politics","split":"train","label_list":["person","location","event","organization","country","politician","political party","election"],"instance":{"id":"56","words":["In","2004",",","Thompson","wrote",":","Richard","Nixon","was","a","professional","politician",",","and","I","despised","everything","he","stood","for","-","but","if","he","were","running","for","president","this","year","against","the","evil","George","W.","Bush","-","Dick","Cheney","gang",",","I","would","happily","vote","for","him","."],"labels":["O","O","O","B-politician","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, event, organization, country, politician, political party, election and O.\nSentence: In 2004 , Thompson wrote : Richard Nixon was a professional politician , and I despised everything he stood for - but if he were running for president this year against the evil George W. Bush - Dick Cheney gang , I would happily vote for him .","prompt_labels":"In(O) 2004(O) ,(O) Thompson(B-politician) wrote(O) :(O) Richard(B-politician) Nixon(I-politician) was(O) a(O) professional(O) politician(O) ,(O) and(O) I(O) despised(O) everything(O) he(O) stood(O) for(O) -(O) but(O) if(O) he(O) were(O) running(O) for(O) president(O) this(O) year(O) against(O) the(O) evil(O) George(B-politician) W.(I-politician) Bush(I-politician) -(O) Dick(B-politician) Cheney(I-politician) gang(O) ,(O) I(O) would(O) happily(O) vote(O) for(O) him(O) .(O)"}}
{"id":"57","dataset":"crossner_politics","split":"train","label_list":["country","organization","election","political party","event","location","politician","person"],"instance":{"id":"57","words":["Behind","the","long","table","set","up","on","the","steps","of","the","Rathaus","Schöneberg","were","U.S.","and","German","dignitaries",",","including","Dean","Rusk","(","Kennedy","'s","Secretary","of","State",")",",","Lucius","D.","Clay","(","the","US","administrator","of","Germany",")",",","Konrad","Adenauer","(","the","German","chancellor",")",",","Willy","Brandt",",","and","Otto","Bach","(","President","of","the","Abgeordnetenhaus","of","Berlin",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-country","O","O","O","O","O","B-politician","I-politician","O","B-politician","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","B-country","O","O","B-country","O","O","B-politician","I-politician","O","O","O","O","O","O","B-politician","I-politician","O","O","B-politician","I-politician","O","O","O","O","O","O","B-location","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, election, political party, event, location, politician, person and O.\nSentence: Behind the long table set up on the steps of the Rathaus Schöneberg were U.S. and German dignitaries , including Dean Rusk ( Kennedy 's Secretary of State ) , Lucius D. Clay ( the US administrator of Germany ) , Konrad Adenauer ( the German chancellor ) , Willy Brandt , and Otto Bach ( President of the Abgeordnetenhaus of Berlin ) .","prompt_labels":"Behind(O) the(O) long(O) table(O) set(O) up(O) on(O) the(O) steps(O) of(O) the(O) Rathaus(B-location) Schöneberg(I-location) were(O) U.S.(B-country) and(O) German(O) dignitaries(O) ,(O) including(O) Dean(B-politician) Rusk(I-politician) ((O) Kennedy(B-politician) 's(O) Secretary(O) of(O) State(O) )(O) ,(O) Lucius(B-politician) D.(I-politician) Clay(I-politician) ((O) the(O) US(B-country) administrator(O) of(O) Germany(B-country) )(O) ,(O) Konrad(B-politician) Adenauer(I-politician) ((O) the(O) German(O) chancellor(O) )(O) ,(O) Willy(B-politician) Brandt(I-politician) ,(O) and(O) Otto(B-politician) Bach(I-politician) ((O) President(O) of(O) the(O) Abgeordnetenhaus(O) of(O) Berlin(B-location) )(O) .(O)"}}
{"id":"58","dataset":"crossner_politics","split":"train","label_list":["politician","election","person","country","event","location","political party","organization"],"instance":{"id":"58","words":["(","1961","Turkish","general","election",",","1965","Turkish","general","election",",","1969","Turkish","general","election",")"],"labels":["O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, person, country, event, location, political party, organization and O.\nSentence: ( 1961 Turkish general election , 1965 Turkish general election , 1969 Turkish general election )","prompt_labels":"((O) 1961(B-election) Turkish(I-election) general(I-election) election(I-election) ,(O) 1965(B-election) Turkish(I-election) general(I-election) election(I-election) ,(O) 1969(B-election) Turkish(I-election) general(I-election) election(I-election) )(O)"}}
{"id":"59","dataset":"crossner_politics","split":"train","label_list":["event","person","country","politician","location","organization","political party","election"],"instance":{"id":"59","words":["At","the","1943","Tehran","Conference",",","the","Allies","of","World","War","II",",","Big","Three","-","Joseph","Stalin",",","Franklin","D.","Roosevelt",",","and","Winston","Churchill","-","issued","the","Tehran","Declaration","to","guarantee","the","post-war","independence","and","boundaries","of","Iran","."],"labels":["O","O","B-event","I-event","I-event","O","O","B-country","O","B-event","I-event","I-event","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","I-politician","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, country, politician, location, organization, political party, election and O.\nSentence: At the 1943 Tehran Conference , the Allies of World War II , Big Three - Joseph Stalin , Franklin D. Roosevelt , and Winston Churchill - issued the Tehran Declaration to guarantee the post-war independence and boundaries of Iran .","prompt_labels":"At(O) the(O) 1943(B-event) Tehran(I-event) Conference(I-event) ,(O) the(O) Allies(B-country) of(O) World(B-event) War(I-event) II(I-event) ,(O) Big(O) Three(O) -(O) Joseph(B-politician) Stalin(I-politician) ,(O) Franklin(B-politician) D.(I-politician) Roosevelt(I-politician) ,(O) and(O) Winston(B-politician) Churchill(I-politician) -(O) issued(O) the(O) Tehran(O) Declaration(O) to(O) guarantee(O) the(O) post-war(O) independence(O) and(O) boundaries(O) of(O) Iran(B-country) .(O)"}}
{"id":"60","dataset":"crossner_politics","split":"train","label_list":["organization","person","location","election","political party","country","politician","event"],"instance":{"id":"60","words":["The","Italian","Socialist","Party",",","led","by","Bettino","Craxi",",","became","more","and","more","critical","of","the","communists","and","of","the","Soviet","Union",";","Craxi","himself","pushed","in","favor","of","Ronald","Reagan","'","s","positioning","of","Pershing","II","missiles","in","Italy",",","a","move","many","communists","strongly","disapproved","of","."],"labels":["O","B-political party","I-political party","I-political party","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","B-politician","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, location, election, political party, country, politician, event and O.\nSentence: The Italian Socialist Party , led by Bettino Craxi , became more and more critical of the communists and of the Soviet Union ; Craxi himself pushed in favor of Ronald Reagan ' s positioning of Pershing II missiles in Italy , a move many communists strongly disapproved of .","prompt_labels":"The(O) Italian(B-political party) Socialist(I-political party) Party(I-political party) ,(O) led(O) by(O) Bettino(B-politician) Craxi(I-politician) ,(O) became(O) more(O) and(O) more(O) critical(O) of(O) the(O) communists(O) and(O) of(O) the(O) Soviet(B-country) Union(I-country) ;(O) Craxi(B-politician) himself(O) pushed(O) in(O) favor(O) of(O) Ronald(B-politician) Reagan(I-politician) '(O) s(O) positioning(O) of(O) Pershing(O) II(O) missiles(O) in(O) Italy(B-country) ,(O) a(O) move(O) many(O) communists(O) strongly(O) disapproved(O) of(O) .(O)"}}
{"id":"61","dataset":"crossner_politics","split":"train","label_list":["politician","location","organization","event","election","person","political party","country"],"instance":{"id":"61","words":["National","elections","held","on","13","May","2001","returned","Berlusconi","to","power","at","the","head","of","the","five-party","center-right","House","of","Freedoms","coalition",",","comprising","the","Prime","Minister","'s","own","party",",","Forza","Italia",",","the","National","Alliance",",","the","Lega","Nord",",","the","Christian","Democratic","Center","and","the","United","Christian","Democrats","."],"labels":["O","O","O","O","O","O","O","O","B-politician","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","B-political party","I-political party","O","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, organization, event, election, person, political party, country and O.\nSentence: National elections held on 13 May 2001 returned Berlusconi to power at the head of the five-party center-right House of Freedoms coalition , comprising the Prime Minister 's own party , Forza Italia , the National Alliance , the Lega Nord , the Christian Democratic Center and the United Christian Democrats .","prompt_labels":"National(O) elections(O) held(O) on(O) 13(O) May(O) 2001(O) returned(O) Berlusconi(B-politician) to(O) power(O) at(O) the(O) head(O) of(O) the(O) five-party(O) center-right(O) House(B-political party) of(I-political party) Freedoms(I-political party) coalition(O) ,(O) comprising(O) the(O) Prime(O) Minister(O) 's(O) own(O) party(O) ,(O) Forza(B-political party) Italia(I-political party) ,(O) the(O) National(B-political party) Alliance(I-political party) ,(O) the(O) Lega(B-political party) Nord(I-political party) ,(O) the(O) Christian(B-political party) Democratic(I-political party) Center(I-political party) and(O) the(O) United(B-political party) Christian(I-political party) Democrats(I-political party) .(O)"}}
{"id":"62","dataset":"crossner_politics","split":"train","label_list":["location","country","person","politician","event","election","political party","organization"],"instance":{"id":"62","words":["On","25","November","1986",",","President","Reagan","announced","the","creation","of","a","Special","Review","Board","to","look","into","the","matter",";","the","following","day",",","he","appointed","former","Senator","John","Tower",",","former","Secretary","of","State","Edmund","Muskie",",","and","former","National","Security","Adviser","Brent","Scowcroft","to","serve","as","members","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, person, politician, event, election, political party, organization and O.\nSentence: On 25 November 1986 , President Reagan announced the creation of a Special Review Board to look into the matter ; the following day , he appointed former Senator John Tower , former Secretary of State Edmund Muskie , and former National Security Adviser Brent Scowcroft to serve as members .","prompt_labels":"On(O) 25(O) November(O) 1986(O) ,(O) President(O) Reagan(O) announced(O) the(O) creation(O) of(O) a(O) Special(O) Review(O) Board(O) to(O) look(O) into(O) the(O) matter(O) ;(O) the(O) following(O) day(O) ,(O) he(O) appointed(O) former(O) Senator(O) John(B-politician) Tower(I-politician) ,(O) former(O) Secretary(O) of(O) State(O) Edmund(B-politician) Muskie(I-politician) ,(O) and(O) former(O) National(O) Security(O) Adviser(O) Brent(B-politician) Scowcroft(I-politician) to(O) serve(O) as(O) members(O) .(O)"}}
{"id":"63","dataset":"crossner_politics","split":"train","label_list":["location","election","event","political party","politician","organization","country","person"],"instance":{"id":"63","words":["Necmettin","Erbakan","(","1926-2011",")","was","the","leader","of","several","of","the","parties",",","the","National","Order","Party","(","Milli","Nizam","Partisi",",","1970-1971",")",",","the","National","Salvation","Party","(","Milli","Selamet","Partisi",",","1972-1981",")",",","and","the","Welfare","Party","(","Refah","Partisi",",","1983-1998",")",";","he","also","became","a","member","of","the","Felicity","Party","(","Saadet","Partisi",",","2003-2011",")","."],"labels":["B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","B-political party","I-political party","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, election, event, political party, politician, organization, country, person and O.\nSentence: Necmettin Erbakan ( 1926-2011 ) was the leader of several of the parties , the National Order Party ( Milli Nizam Partisi , 1970-1971 ) , the National Salvation Party ( Milli Selamet Partisi , 1972-1981 ) , and the Welfare Party ( Refah Partisi , 1983-1998 ) ; he also became a member of the Felicity Party ( Saadet Partisi , 2003-2011 ) .","prompt_labels":"Necmettin(B-politician) Erbakan(I-politician) ((O) 1926-2011(O) )(O) was(O) the(O) leader(O) of(O) several(O) of(O) the(O) parties(O) ,(O) the(O) National(B-political party) Order(I-political party) Party(I-political party) ((O) Milli(B-political party) Nizam(I-political party) Partisi(I-political party) ,(O) 1970-1971(O) )(O) ,(O) the(O) National(B-political party) Salvation(I-political party) Party(I-political party) ((O) Milli(B-political party) Selamet(I-political party) Partisi(I-political party) ,(O) 1972-1981(O) )(O) ,(O) and(O) the(O) Welfare(B-political party) Party(I-political party) ((O) Refah(B-political party) Partisi(I-political party) ,(O) 1983-1998(O) )(O) ;(O) he(O) also(O) became(O) a(O) member(O) of(O) the(O) Felicity(B-political party) Party(I-political party) ((O) Saadet(B-political party) Partisi(I-political party) ,(O) 2003-2011(O) )(O) .(O)"}}
{"id":"64","dataset":"crossner_politics","split":"train","label_list":["election","event","organization","politician","person","location","political party","country"],"instance":{"id":"64","words":["In","the","1994","European","Parliament","election","in","Ireland",",","Patricia","McKenna","topped","the","poll","in","the","Dublin","constituency","and","Nuala","Ahern","won","a","seat","in","Leinster","."],"labels":["O","O","B-election","I-election","I-election","I-election","I-election","I-election","O","B-politician","I-politician","O","O","O","O","O","B-location","O","O","B-politician","I-politician","O","O","O","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, organization, politician, person, location, political party, country and O.\nSentence: In the 1994 European Parliament election in Ireland , Patricia McKenna topped the poll in the Dublin constituency and Nuala Ahern won a seat in Leinster .","prompt_labels":"In(O) the(O) 1994(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) Ireland(I-election) ,(O) Patricia(B-politician) McKenna(I-politician) topped(O) the(O) poll(O) in(O) the(O) Dublin(B-location) constituency(O) and(O) Nuala(B-politician) Ahern(I-politician) won(O) a(O) seat(O) in(O) Leinster(B-location) .(O)"}}
{"id":"65","dataset":"crossner_politics","split":"train","label_list":["political party","politician","country","election","location","person","organization","event"],"instance":{"id":"65","words":["In","the","1980s",",","Indira","Gandhi","along","with","Canadian","Prime","Minister","Pierre","Trudeau",",","Zambia","'s","President","Kenneth","Kaunda",",","Australian","prime","minister","Malcolm","Fraser","and","Singapore","Prime","Minister","Lee","Kuan","Yew","was","regarded","as","one","of","the","pillars","of","the","commonwealth","India","under","Indira","also","hosted","the","1983","Commonwealth","heads","of","Government","summit","in","New","Delhi","in","1983","."],"labels":["O","O","O","O","B-politician","I-politician","O","O","O","O","O","B-politician","I-politician","O","B-country","O","O","B-politician","I-politician","O","O","O","O","B-politician","I-politician","O","O","O","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","O","O","O","B-country","O","B-politician","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","B-location","I-location","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, country, election, location, person, organization, event and O.\nSentence: In the 1980s , Indira Gandhi along with Canadian Prime Minister Pierre Trudeau , Zambia 's President Kenneth Kaunda , Australian prime minister Malcolm Fraser and Singapore Prime Minister Lee Kuan Yew was regarded as one of the pillars of the commonwealth India under Indira also hosted the 1983 Commonwealth heads of Government summit in New Delhi in 1983 .","prompt_labels":"In(O) the(O) 1980s(O) ,(O) Indira(B-politician) Gandhi(I-politician) along(O) with(O) Canadian(O) Prime(O) Minister(O) Pierre(B-politician) Trudeau(I-politician) ,(O) Zambia(B-country) 's(O) President(O) Kenneth(B-politician) Kaunda(I-politician) ,(O) Australian(O) prime(O) minister(O) Malcolm(B-politician) Fraser(I-politician) and(O) Singapore(O) Prime(O) Minister(O) Lee(B-politician) Kuan(I-politician) Yew(I-politician) was(O) regarded(O) as(O) one(O) of(O) the(O) pillars(O) of(O) the(O) commonwealth(O) India(B-country) under(O) Indira(B-politician) also(O) hosted(O) the(O) 1983(B-event) Commonwealth(I-event) heads(I-event) of(I-event) Government(I-event) summit(I-event) in(O) New(B-location) Delhi(I-location) in(O) 1983(O) .(O)"}}
{"id":"66","dataset":"crossner_politics","split":"train","label_list":["person","election","location","political party","organization","country","event","politician"],"instance":{"id":"66","words":["Three","United","States","presidents","have","been","impeached","by","the","House","of","Representatives",":","Andrew","Johnson","in","1868",",","Bill","Clinton","in","1998",",","and","Donald","Trump","in","2019","."],"labels":["O","B-country","I-country","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","B-politician","I-politician","O","O","O","O","B-politician","I-politician","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, location, political party, organization, country, event, politician and O.\nSentence: Three United States presidents have been impeached by the House of Representatives : Andrew Johnson in 1868 , Bill Clinton in 1998 , and Donald Trump in 2019 .","prompt_labels":"Three(O) United(B-country) States(I-country) presidents(O) have(O) been(O) impeached(O) by(O) the(O) House(O) of(O) Representatives(O) :(O) Andrew(B-politician) Johnson(I-politician) in(O) 1868(O) ,(O) Bill(B-politician) Clinton(I-politician) in(O) 1998(O) ,(O) and(O) Donald(B-politician) Trump(I-politician) in(O) 2019(O) .(O)"}}
{"id":"67","dataset":"crossner_politics","split":"train","label_list":["organization","political party","person","location","election","event","country","politician"],"instance":{"id":"67","words":["After","losing","the","2000","nomination",",","Jello","became","highly","active","in","Ralph","Nader","'","s","presidential","campaign",",","as","well","as","in","2004","United","States","presidential","election","and","2008","United","States","presidential","election","."],"labels":["O","O","O","O","O","O","B-politician","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, person, location, election, event, country, politician and O.\nSentence: After losing the 2000 nomination , Jello became highly active in Ralph Nader ' s presidential campaign , as well as in 2004 United States presidential election and 2008 United States presidential election .","prompt_labels":"After(O) losing(O) the(O) 2000(O) nomination(O) ,(O) Jello(B-politician) became(O) highly(O) active(O) in(O) Ralph(B-politician) Nader(I-politician) '(O) s(O) presidential(O) campaign(O) ,(O) as(O) well(O) as(O) in(O) 2004(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) 2008(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) .(O)"}}
{"id":"68","dataset":"crossner_politics","split":"train","label_list":["location","election","politician","event","organization","political party","person","country"],"instance":{"id":"68","words":["The","1824","United","States","presidential","election","was","contested","by","Adams",",","Andrew","Jackson",",","William","H.","Crawford",",","and","Henry","Clay",",","all","of","whom","were","members","of","the","Democratic-Republican","Party","."],"labels":["O","B-election","I-election","I-election","I-election","I-election","O","O","O","B-politician","O","B-politician","I-politician","O","B-politician","I-politician","I-politician","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","B-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, election, politician, event, organization, political party, person, country and O.\nSentence: The 1824 United States presidential election was contested by Adams , Andrew Jackson , William H. Crawford , and Henry Clay , all of whom were members of the Democratic-Republican Party .","prompt_labels":"The(O) 1824(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) was(O) contested(O) by(O) Adams(B-politician) ,(O) Andrew(B-politician) Jackson(I-politician) ,(O) William(B-politician) H.(I-politician) Crawford(I-politician) ,(O) and(O) Henry(B-politician) Clay(I-politician) ,(O) all(O) of(O) whom(O) were(O) members(O) of(O) the(O) Democratic-Republican(B-political party) Party(I-political party) .(O)"}}
{"id":"69","dataset":"crossner_politics","split":"train","label_list":["politician","political party","event","person","location","election","organization","country"],"instance":{"id":"69","words":["Following","boundary","changes",",","Major","became","the","MP","for","the","newly","formed","seat","of","Huntingdon","in","1983","United","Kingdom","general","election","(","retaining","the","seat","in","1987","United","Kingdom","general","election",",","1992","United","Kingdom","general","election","and","1997","United","Kingdom","general","election","before","retiring","from","Parliament","in","2001","United","Kingdom","general","election",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, event, person, location, election, organization, country and O.\nSentence: Following boundary changes , Major became the MP for the newly formed seat of Huntingdon in 1983 United Kingdom general election ( retaining the seat in 1987 United Kingdom general election , 1992 United Kingdom general election and 1997 United Kingdom general election before retiring from Parliament in 2001 United Kingdom general election ) .","prompt_labels":"Following(O) boundary(O) changes(O) ,(O) Major(O) became(O) the(O) MP(O) for(O) the(O) newly(O) formed(O) seat(O) of(O) Huntingdon(B-location) in(O) 1983(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ((O) retaining(O) the(O) seat(O) in(O) 1987(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 1992(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1997(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) before(O) retiring(O) from(O) Parliament(O) in(O) 2001(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) )(O) .(O)"}}
{"id":"70","dataset":"crossner_politics","split":"train","label_list":["country","organization","location","political party","event","election","politician","person"],"instance":{"id":"70","words":["He","secured","re-nomination","for","vice","president","in","1992","United","States","presidential","election",",","but","Democrat","Bill","Clinton","and","his","running","mate",",","Al","Gore",",","defeated","the","Bush","/","Quayle","ticket","."],"labels":["O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","B-politician","I-politician","O","O","O","O","O","B-politician","I-politician","O","O","O","B-politician","O","B-politician","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, location, political party, event, election, politician, person and O.\nSentence: He secured re-nomination for vice president in 1992 United States presidential election , but Democrat Bill Clinton and his running mate , Al Gore , defeated the Bush / Quayle ticket .","prompt_labels":"He(O) secured(O) re-nomination(O) for(O) vice(O) president(O) in(O) 1992(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) but(O) Democrat(O) Bill(B-politician) Clinton(I-politician) and(O) his(O) running(O) mate(O) ,(O) Al(B-politician) Gore(I-politician) ,(O) defeated(O) the(O) Bush(B-politician) /(O) Quayle(B-politician) ticket(O) .(O)"}}
{"id":"71","dataset":"crossner_politics","split":"train","label_list":["country","election","political party","location","politician","person","organization","event"],"instance":{"id":"71","words":["In","the","1992","United","States","presidential","election",",","Bush","and","Quayle","were","challenged","in","their","bid","for","reelection","by","the","Democratic","ticket","of","Arkansas","Governor","Bill","Clinton","and","Tennessee","Senator","Al","Gore","and","the","independent","ticket","of","Texas","businessman","Ross","Perot","and","retired","Vice","Admiral","James","Stockdale","."],"labels":["O","O","B-election","I-election","I-election","I-election","I-election","O","B-politician","O","B-politician","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-politician","I-politician","O","B-location","O","B-politician","I-politician","O","O","O","O","O","B-location","O","B-politician","I-politician","O","O","O","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, political party, location, politician, person, organization, event and O.\nSentence: In the 1992 United States presidential election , Bush and Quayle were challenged in their bid for reelection by the Democratic ticket of Arkansas Governor Bill Clinton and Tennessee Senator Al Gore and the independent ticket of Texas businessman Ross Perot and retired Vice Admiral James Stockdale .","prompt_labels":"In(O) the(O) 1992(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) Bush(B-politician) and(O) Quayle(B-politician) were(O) challenged(O) in(O) their(O) bid(O) for(O) reelection(O) by(O) the(O) Democratic(O) ticket(O) of(O) Arkansas(B-location) Governor(O) Bill(B-politician) Clinton(I-politician) and(O) Tennessee(B-location) Senator(O) Al(B-politician) Gore(I-politician) and(O) the(O) independent(O) ticket(O) of(O) Texas(B-location) businessman(O) Ross(B-politician) Perot(I-politician) and(O) retired(O) Vice(O) Admiral(O) James(B-politician) Stockdale(I-politician) .(O)"}}
{"id":"72","dataset":"crossner_politics","split":"train","label_list":["organization","event","political party","country","election","politician","person","location"],"instance":{"id":"72","words":["His","wartime","leadership","established","him","as","Madison","'s","heir","apparent",",","and","he","easily","defeated","Federalist","Party","candidate","Rufus","King","in","the","1816","United","States","presidential","election","."],"labels":["O","O","O","O","O","O","B-politician","O","O","O","O","O","O","O","O","B-political party","I-political party","O","B-politician","I-politician","O","O","B-election","I-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, political party, country, election, politician, person, location and O.\nSentence: His wartime leadership established him as Madison 's heir apparent , and he easily defeated Federalist Party candidate Rufus King in the 1816 United States presidential election .","prompt_labels":"His(O) wartime(O) leadership(O) established(O) him(O) as(O) Madison(B-politician) 's(O) heir(O) apparent(O) ,(O) and(O) he(O) easily(O) defeated(O) Federalist(B-political party) Party(I-political party) candidate(O) Rufus(B-politician) King(I-politician) in(O) the(O) 1816(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) .(O)"}}
{"id":"73","dataset":"crossner_politics","split":"train","label_list":["person","country","event","election","political party","organization","location","politician"],"instance":{"id":"73","words":["Samuel","became","a","county","judge",",","and","the","guests","at","his","home","included","Andrew","Jackson",",","who","had","already","served","as","a","judge","and","in","Congress.Borneman",",","James","learned","from","the","political","talk","around","the","dinner","table",";","both","Samuel","and","Ezekiel","were","strong","supporters","of","President","Thomas","Jefferson","and","opponents","of","the","Federalist","Party","."],"labels":["B-politician","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","O","O","O","O","O","O","O","O","O","O","O","B-politician","O","B-politician","O","O","O","O","O","B-politician","I-politician","O","O","O","O","B-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, event, election, political party, organization, location, politician and O.\nSentence: Samuel became a county judge , and the guests at his home included Andrew Jackson , who had already served as a judge and in Congress.Borneman , James learned from the political talk around the dinner table ; both Samuel and Ezekiel were strong supporters of President Thomas Jefferson and opponents of the Federalist Party .","prompt_labels":"Samuel(B-politician) became(O) a(O) county(O) judge(O) ,(O) and(O) the(O) guests(O) at(O) his(O) home(O) included(O) Andrew(B-politician) Jackson(I-politician) ,(O) who(O) had(O) already(O) served(O) as(O) a(O) judge(O) and(O) in(O) Congress.Borneman(O) ,(O) James(B-politician) learned(O) from(O) the(O) political(O) talk(O) around(O) the(O) dinner(O) table(O) ;(O) both(O) Samuel(B-politician) and(O) Ezekiel(B-politician) were(O) strong(O) supporters(O) of(O) President(O) Thomas(B-politician) Jefferson(I-politician) and(O) opponents(O) of(O) the(O) Federalist(B-political party) Party(I-political party) .(O)"}}
{"id":"74","dataset":"crossner_politics","split":"train","label_list":["location","organization","election","politician","event","person","country","political party"],"instance":{"id":"74","words":["The","prestige","of","the","Speakership","caused","them","to","abandon","life","in","a","Washington","boarding","house","for","their","own","residence","on","Pennsylvania",".","In","the","1836","United","States","presidential","election",",","Vice","President","Martin","Van","Buren",",","Jackson","'s","chosen","successor",",","defeated","multiple","Whig","candidates",",","including","Tennessee","Senator","Hugh","Lawson","White","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","O","O","O","O","B-location","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","B-politician","I-politician","I-politician","O","B-politician","O","O","O","O","O","O","B-political party","O","O","O","B-location","O","B-politician","I-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, election, politician, event, person, country, political party and O.\nSentence: The prestige of the Speakership caused them to abandon life in a Washington boarding house for their own residence on Pennsylvania . In the 1836 United States presidential election , Vice President Martin Van Buren , Jackson 's chosen successor , defeated multiple Whig candidates , including Tennessee Senator Hugh Lawson White .","prompt_labels":"The(O) prestige(O) of(O) the(O) Speakership(O) caused(O) them(O) to(O) abandon(O) life(O) in(O) a(O) Washington(B-location) boarding(I-location) house(I-location) for(O) their(O) own(O) residence(O) on(O) Pennsylvania(B-location) .(O) In(O) the(O) 1836(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) Vice(O) President(O) Martin(B-politician) Van(I-politician) Buren(I-politician) ,(O) Jackson(B-politician) 's(O) chosen(O) successor(O) ,(O) defeated(O) multiple(O) Whig(B-political party) candidates(O) ,(O) including(O) Tennessee(B-location) Senator(O) Hugh(B-politician) Lawson(I-politician) White(I-politician) .(O)"}}
{"id":"75","dataset":"crossner_politics","split":"train","label_list":["location","person","country","organization","political party","event","politician","election"],"instance":{"id":"75","words":["He","faced","strong","challenges","from","the","right","(","Republican","Ronald","Reagan",")",",","the","center","(","independent","John","B.","Anderson",")",",","and","the","left","(","Democrat","Ted","Kennedy",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","B-politician","I-politician","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, country, organization, political party, event, politician, election and O.\nSentence: He faced strong challenges from the right ( Republican Ronald Reagan ) , the center ( independent John B. Anderson ) , and the left ( Democrat Ted Kennedy ) .","prompt_labels":"He(O) faced(O) strong(O) challenges(O) from(O) the(O) right(O) ((O) Republican(O) Ronald(B-politician) Reagan(I-politician) )(O) ,(O) the(O) center(O) ((O) independent(O) John(B-politician) B.(I-politician) Anderson(I-politician) )(O) ,(O) and(O) the(O) left(O) ((O) Democrat(O) Ted(B-politician) Kennedy(I-politician) )(O) .(O)"}}
{"id":"76","dataset":"crossner_politics","split":"train","label_list":["political party","person","country","election","event","politician","location","organization"],"instance":{"id":"76","words":["Amid","the","Democratic","presidential","primary","in","2008","United","States","presidential","election",",","Carter","was","speculated","to","endorse","Senator","Barack","Obama","over","his","main","primary","rival","Hillary","Clinton","amid","his","speaking","favorably","of","the","candidate",",","as","well","as","remarks","from","the","Carter","family","that","showed","their","support","for","Obama","."],"labels":["O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-politician","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","O","O","O","O","O","O","B-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, country, election, event, politician, location, organization and O.\nSentence: Amid the Democratic presidential primary in 2008 United States presidential election , Carter was speculated to endorse Senator Barack Obama over his main primary rival Hillary Clinton amid his speaking favorably of the candidate , as well as remarks from the Carter family that showed their support for Obama .","prompt_labels":"Amid(O) the(O) Democratic(O) presidential(O) primary(O) in(O) 2008(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) Carter(B-politician) was(O) speculated(O) to(O) endorse(O) Senator(O) Barack(B-politician) Obama(I-politician) over(O) his(O) main(O) primary(O) rival(O) Hillary(B-politician) Clinton(I-politician) amid(O) his(O) speaking(O) favorably(O) of(O) the(O) candidate(O) ,(O) as(O) well(O) as(O) remarks(O) from(O) the(O) Carter(B-politician) family(O) that(O) showed(O) their(O) support(O) for(O) Obama(B-politician) .(O)"}}
{"id":"77","dataset":"crossner_politics","split":"train","label_list":["person","country","event","location","election","organization","political party","politician"],"instance":{"id":"77","words":["Hagelin","stood","as","a","candidate","for","President","of","the","United","States","for","the","Natural","Law","Party",",","a","party","founded","by","the","TM","movement",",","in","the","1992","United","States","presidential","election",",","1996","United","States","presidential","election","and","2000","United","States","presidential","election","elections","."],"labels":["B-politician","O","O","O","O","O","O","O","O","B-country","I-country","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","B-event","I-event","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, event, location, election, organization, political party, politician and O.\nSentence: Hagelin stood as a candidate for President of the United States for the Natural Law Party , a party founded by the TM movement , in the 1992 United States presidential election , 1996 United States presidential election and 2000 United States presidential election elections .","prompt_labels":"Hagelin(B-politician) stood(O) as(O) a(O) candidate(O) for(O) President(O) of(O) the(O) United(B-country) States(I-country) for(O) the(O) Natural(B-political party) Law(I-political party) Party(I-political party) ,(O) a(O) party(O) founded(O) by(O) the(O) TM(B-event) movement(I-event) ,(O) in(O) the(O) 1992(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) 1996(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) 2000(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) elections(O) .(O)"}}
{"id":"78","dataset":"crossner_politics","split":"train","label_list":["election","event","location","organization","politician","political party","person","country"],"instance":{"id":"78","words":["In","the","weeks","leading","up","to","the","1852","United","States","presidential","election",",","he","campaigned","in","numerous","Southern","states","for","Democratic","candidates","Franklin","Pierce","and","William","R.","King","."],"labels":["O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, location, organization, politician, political party, person, country and O.\nSentence: In the weeks leading up to the 1852 United States presidential election , he campaigned in numerous Southern states for Democratic candidates Franklin Pierce and William R. King .","prompt_labels":"In(O) the(O) weeks(O) leading(O) up(O) to(O) the(O) 1852(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) he(O) campaigned(O) in(O) numerous(O) Southern(O) states(O) for(O) Democratic(O) candidates(O) Franklin(B-politician) Pierce(I-politician) and(O) William(B-politician) R.(I-politician) King(I-politician) .(O)"}}
{"id":"79","dataset":"crossner_politics","split":"train","label_list":["location","political party","country","politician","event","election","person","organization"],"instance":{"id":"79","words":["LeRoy","Pope","Walker","of","Alabama","was","made","Secretary","of","War",",","after","being","recommended","for","this","post","by","Clement","Claiborne","Clay","and","William","Lowndes","Yancey","(","both","of","whom","declined","to","accept","cabinet","positions","themselves",")","."],"labels":["B-politician","I-politician","I-politician","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, country, politician, event, election, person, organization and O.\nSentence: LeRoy Pope Walker of Alabama was made Secretary of War , after being recommended for this post by Clement Claiborne Clay and William Lowndes Yancey ( both of whom declined to accept cabinet positions themselves ) .","prompt_labels":"LeRoy(B-politician) Pope(I-politician) Walker(I-politician) of(O) Alabama(B-location) was(O) made(O) Secretary(O) of(O) War(O) ,(O) after(O) being(O) recommended(O) for(O) this(O) post(O) by(O) Clement(B-politician) Claiborne(I-politician) Clay(I-politician) and(O) William(B-politician) Lowndes(I-politician) Yancey(I-politician) ((O) both(O) of(O) whom(O) declined(O) to(O) accept(O) cabinet(O) positions(O) themselves(O) )(O) .(O)"}}
{"id":"80","dataset":"crossner_politics","split":"train","label_list":["person","event","organization","country","election","political party","politician","location"],"instance":{"id":"80","words":["His","arguments","concerning","liberty","and","the","social","contract","later","influenced","the","written","works","of","Alexander","Hamilton",",","James","Madison",",","Thomas","Jefferson",",","and","other","Founding","Fathers","of","the","United","States","."],"labels":["O","O","O","B-political party","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","O","O","B-country","I-country","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, organization, country, election, political party, politician, location and O.\nSentence: His arguments concerning liberty and the social contract later influenced the written works of Alexander Hamilton , James Madison , Thomas Jefferson , and other Founding Fathers of the United States .","prompt_labels":"His(O) arguments(O) concerning(O) liberty(B-political party) and(O) the(O) social(O) contract(O) later(O) influenced(O) the(O) written(O) works(O) of(O) Alexander(B-politician) Hamilton(I-politician) ,(O) James(B-politician) Madison(I-politician) ,(O) Thomas(B-politician) Jefferson(I-politician) ,(O) and(O) other(O) Founding(O) Fathers(O) of(O) the(O) United(B-country) States(I-country) .(O)"}}
{"id":"81","dataset":"crossner_politics","split":"train","label_list":["country","person","organization","politician","location","event","political party","election"],"instance":{"id":"81","words":["After","supporting","Richard","Nixon","in","his","1960","United","States","presidential","election","against","John","F.","Kennedy",",","Robinson","later","praised","Kennedy","effusively","for","his","stance","on","civil","rights","."],"labels":["O","O","B-politician","I-politician","O","O","B-election","I-election","I-election","I-election","I-election","O","B-politician","I-politician","I-politician","O","B-politician","O","O","B-politician","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, organization, politician, location, event, political party, election and O.\nSentence: After supporting Richard Nixon in his 1960 United States presidential election against John F. Kennedy , Robinson later praised Kennedy effusively for his stance on civil rights .","prompt_labels":"After(O) supporting(O) Richard(B-politician) Nixon(I-politician) in(O) his(O) 1960(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) against(O) John(B-politician) F.(I-politician) Kennedy(I-politician) ,(O) Robinson(B-politician) later(O) praised(O) Kennedy(B-politician) effusively(O) for(O) his(O) stance(O) on(O) civil(O) rights(O) .(O)"}}
{"id":"82","dataset":"crossner_politics","split":"train","label_list":["location","person","country","event","politician","election","political party","organization"],"instance":{"id":"82","words":["Gandhian","right-wing","Congressmen","Vallabhbhai","Patel",",","Rajendra","Prasad","and","C.","Rajagopalachari",".In","the","1930s","the","Congress","Socialist","Party","group","was","formed","within","the","INC","under","the","leadership","of","Jayaprakash","Narayan",",","Narendra","Deo","and","Nehru",",","however",",","never","joined","the","group","but","did","act","as","bridge","between","them","and","Gandhi","."],"labels":["B-politician","O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, country, event, politician, election, political party, organization and O.\nSentence: Gandhian right-wing Congressmen Vallabhbhai Patel , Rajendra Prasad and C. Rajagopalachari .In the 1930s the Congress Socialist Party group was formed within the INC under the leadership of Jayaprakash Narayan , Narendra Deo and Nehru , however , never joined the group but did act as bridge between them and Gandhi .","prompt_labels":"Gandhian(B-politician) right-wing(O) Congressmen(O) Vallabhbhai(B-politician) Patel(I-politician) ,(O) Rajendra(B-politician) Prasad(I-politician) and(O) C.(B-politician) Rajagopalachari(I-politician) .In(O) the(O) 1930s(O) the(O) Congress(B-political party) Socialist(I-political party) Party(I-political party) group(O) was(O) formed(O) within(O) the(O) INC(O) under(O) the(O) leadership(O) of(O) Jayaprakash(B-politician) Narayan(I-politician) ,(O) Narendra(B-politician) Deo(I-politician) and(O) Nehru(B-politician) ,(O) however(O) ,(O) never(O) joined(O) the(O) group(O) but(O) did(O) act(O) as(O) bridge(O) between(O) them(O) and(O) Gandhi(B-politician) .(O)"}}
{"id":"83","dataset":"crossner_politics","split":"train","label_list":["organization","politician","political party","person","country","election","event","location"],"instance":{"id":"83","words":["In","addition","to","the","Tydings-Butler","race",",","McCarthy","campaigned","for","several","other","Republicans","in","the","1950","United","States","Senate","elections",",","including","Everett","Dirksen","against","Democratic","incumbent","and","Senate","Majority","Leader","Scott","W.","Lucas","."],"labels":["O","O","O","O","B-event","I-event","O","B-politician","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-politician","I-politician","O","O","O","O","O","O","O","B-politician","I-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, political party, person, country, election, event, location and O.\nSentence: In addition to the Tydings-Butler race , McCarthy campaigned for several other Republicans in the 1950 United States Senate elections , including Everett Dirksen against Democratic incumbent and Senate Majority Leader Scott W. Lucas .","prompt_labels":"In(O) addition(O) to(O) the(O) Tydings-Butler(B-event) race(I-event) ,(O) McCarthy(B-politician) campaigned(O) for(O) several(O) other(O) Republicans(O) in(O) the(O) 1950(B-election) United(I-election) States(I-election) Senate(I-election) elections(I-election) ,(O) including(O) Everett(B-politician) Dirksen(I-politician) against(O) Democratic(O) incumbent(O) and(O) Senate(O) Majority(O) Leader(O) Scott(B-politician) W.(I-politician) Lucas(I-politician) .(O)"}}
{"id":"84","dataset":"crossner_politics","split":"train","label_list":["political party","location","organization","election","country","politician","event","person"],"instance":{"id":"84","words":["Six","other","Republican","senators","-","Wayne","Morse",",","Irving","Ives",",","Charles","W.","Tobey",",","Edward","John","Thye",",","George","Aiken",",","and","Robert","C.","Hendrickson","-","joined","her","in","condemning","McCarthy","'s","tactics","."],"labels":["O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","I-politician","O","B-politician","I-politician","I-politician","O","B-politician","I-politician","O","O","B-politician","I-politician","I-politician","O","O","O","O","O","B-politician","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, location, organization, election, country, politician, event, person and O.\nSentence: Six other Republican senators - Wayne Morse , Irving Ives , Charles W. Tobey , Edward John Thye , George Aiken , and Robert C. Hendrickson - joined her in condemning McCarthy 's tactics .","prompt_labels":"Six(O) other(O) Republican(O) senators(O) -(O) Wayne(B-politician) Morse(I-politician) ,(O) Irving(B-politician) Ives(I-politician) ,(O) Charles(B-politician) W.(I-politician) Tobey(I-politician) ,(O) Edward(B-politician) John(I-politician) Thye(I-politician) ,(O) George(B-politician) Aiken(I-politician) ,(O) and(O) Robert(B-politician) C.(I-politician) Hendrickson(I-politician) -(O) joined(O) her(O) in(O) condemning(O) McCarthy(B-politician) 's(O) tactics(O) .(O)"}}
{"id":"85","dataset":"crossner_politics","split":"train","label_list":["politician","organization","location","political party","person","election","country","event"],"instance":{"id":"85","words":["With","fellow","U.S.","senators","Trent","Lott",",","Larry","Craig",",","and","Jim","Jeffords",",","Ashcroft","formed","a","barbershop","quartet","called","The","Singing","Senators","."],"labels":["O","O","B-country","O","B-politician","I-politician","O","B-politician","I-politician","O","O","B-politician","I-politician","O","B-politician","O","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, location, political party, person, election, country, event and O.\nSentence: With fellow U.S. senators Trent Lott , Larry Craig , and Jim Jeffords , Ashcroft formed a barbershop quartet called The Singing Senators .","prompt_labels":"With(O) fellow(O) U.S.(B-country) senators(O) Trent(B-politician) Lott(I-politician) ,(O) Larry(B-politician) Craig(I-politician) ,(O) and(O) Jim(B-politician) Jeffords(I-politician) ,(O) Ashcroft(B-politician) formed(O) a(O) barbershop(B-organization) quartet(I-organization) called(O) The(B-organization) Singing(I-organization) Senators(I-organization) .(O)"}}
{"id":"86","dataset":"crossner_politics","split":"train","label_list":["organization","person","political party","country","politician","election","location","event"],"instance":{"id":"86","words":["He","gained","further","international","attention","as","the","chief","leader","of","the","Non-Aligned","Movement",",","alongside","Jawaharlal","Nehru","of","India",",","Gamal","Abdel","Nasser","of","Egypt",",","and","Kwame","Nkrumah","of","Ghana","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","B-politician","I-politician","O","B-country","O","B-politician","I-politician","I-politician","O","B-country","O","O","B-politician","I-politician","O","B-country","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, political party, country, politician, election, location, event and O.\nSentence: He gained further international attention as the chief leader of the Non-Aligned Movement , alongside Jawaharlal Nehru of India , Gamal Abdel Nasser of Egypt , and Kwame Nkrumah of Ghana .","prompt_labels":"He(O) gained(O) further(O) international(O) attention(O) as(O) the(O) chief(O) leader(O) of(O) the(O) Non-Aligned(B-organization) Movement(I-organization) ,(O) alongside(O) Jawaharlal(B-politician) Nehru(I-politician) of(O) India(B-country) ,(O) Gamal(B-politician) Abdel(I-politician) Nasser(I-politician) of(O) Egypt(B-country) ,(O) and(O) Kwame(B-politician) Nkrumah(I-politician) of(O) Ghana(B-country) .(O)"}}
{"id":"87","dataset":"crossner_politics","split":"train","label_list":["event","organization","location","election","country","person","political party","politician"],"instance":{"id":"87","words":["He","was","appointed","to","the","Committee","and","started","to","appoint","allies","to","him",",","among","them","Edvard","Kardelj",",","Milovan","Đilas",",","Aleksandar","Ranković","and","Boris","Kidrič","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, location, election, country, person, political party, politician and O.\nSentence: He was appointed to the Committee and started to appoint allies to him , among them Edvard Kardelj , Milovan Đilas , Aleksandar Ranković and Boris Kidrič .","prompt_labels":"He(O) was(O) appointed(O) to(O) the(O) Committee(O) and(O) started(O) to(O) appoint(O) allies(O) to(O) him(O) ,(O) among(O) them(O) Edvard(B-politician) Kardelj(I-politician) ,(O) Milovan(B-politician) Đilas(I-politician) ,(O) Aleksandar(B-politician) Ranković(I-politician) and(O) Boris(B-politician) Kidrič(I-politician) .(O)"}}
{"id":"88","dataset":"crossner_politics","split":"train","label_list":["political party","country","person","event","election","politician","location","organization"],"instance":{"id":"88","words":["In","1961",",","Tito","co-founded","the","movement","with","Egypt","'s","Gamal","Abdel","Nasser",",","India","'s","Jawaharlal","Nehru",",","Indonesia","'s","Sukarno","and","Ghana","'s","Kwame","Nkrumah",",","in","an","action","called","The","Initiative","of","Five","(","Tito",",","Nehru",",","Nasser",",","Sukarno",",","Nkrumah",")",",","thus","establishing","strong","ties","with","third","world","countries","."],"labels":["O","O","O","B-politician","O","O","O","O","B-country","O","B-politician","I-politician","I-politician","O","B-country","O","B-politician","I-politician","O","B-country","O","B-politician","O","B-country","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","B-politician","O","B-politician","O","B-politician","O","B-politician","O","B-politician","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, person, event, election, politician, location, organization and O.\nSentence: In 1961 , Tito co-founded the movement with Egypt 's Gamal Abdel Nasser , India 's Jawaharlal Nehru , Indonesia 's Sukarno and Ghana 's Kwame Nkrumah , in an action called The Initiative of Five ( Tito , Nehru , Nasser , Sukarno , Nkrumah ) , thus establishing strong ties with third world countries .","prompt_labels":"In(O) 1961(O) ,(O) Tito(B-politician) co-founded(O) the(O) movement(O) with(O) Egypt(B-country) 's(O) Gamal(B-politician) Abdel(I-politician) Nasser(I-politician) ,(O) India(B-country) 's(O) Jawaharlal(B-politician) Nehru(I-politician) ,(O) Indonesia(B-country) 's(O) Sukarno(B-politician) and(O) Ghana(B-country) 's(O) Kwame(B-politician) Nkrumah(I-politician) ,(O) in(O) an(O) action(O) called(O) The(O) Initiative(O) of(O) Five(O) ((O) Tito(B-politician) ,(O) Nehru(B-politician) ,(O) Nasser(B-politician) ,(O) Sukarno(B-politician) ,(O) Nkrumah(B-politician) )(O) ,(O) thus(O) establishing(O) strong(O) ties(O) with(O) third(O) world(O) countries(O) .(O)"}}
{"id":"89","dataset":"crossner_politics","split":"train","label_list":["event","election","organization","country","person","political party","location","politician"],"instance":{"id":"89","words":["Beginning","as","a","private","secretary","and","assistant","to","Abraham","Lincoln",",","Hay","'s","highest","office","was","United","States","Secretary","of","State","under","Presidents","William","McKinley","and","Theodore","Roosevelt","."],"labels":["O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, organization, country, person, political party, location, politician and O.\nSentence: Beginning as a private secretary and assistant to Abraham Lincoln , Hay 's highest office was United States Secretary of State under Presidents William McKinley and Theodore Roosevelt .","prompt_labels":"Beginning(O) as(O) a(O) private(O) secretary(O) and(O) assistant(O) to(O) Abraham(B-politician) Lincoln(I-politician) ,(O) Hay(B-politician) 's(O) highest(O) office(O) was(O) United(O) States(O) Secretary(O) of(O) State(O) under(O) Presidents(O) William(B-politician) McKinley(I-politician) and(O) Theodore(B-politician) Roosevelt(I-politician) .(O)"}}
{"id":"90","dataset":"crossner_politics","split":"train","label_list":["event","person","politician","location","political party","country","organization","election"],"instance":{"id":"90","words":["Between","2008","and","2013","Kenya","was","governed","by","a","Grand","coalition",",","established","by","a","power","sharing","agreement",",","signed","by","then","President","Mwai","Kibaki","and","Prime","Minister","Raila","Odinga","of","the","Orange","Democratic","Movement","."],"labels":["O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","B-politician","I-politician","O","O","B-political party","I-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, politician, location, political party, country, organization, election and O.\nSentence: Between 2008 and 2013 Kenya was governed by a Grand coalition , established by a power sharing agreement , signed by then President Mwai Kibaki and Prime Minister Raila Odinga of the Orange Democratic Movement .","prompt_labels":"Between(O) 2008(O) and(O) 2013(O) Kenya(B-country) was(O) governed(O) by(O) a(O) Grand(O) coalition(O) ,(O) established(O) by(O) a(O) power(O) sharing(O) agreement(O) ,(O) signed(O) by(O) then(O) President(O) Mwai(B-politician) Kibaki(I-politician) and(O) Prime(O) Minister(O) Raila(B-politician) Odinga(I-politician) of(O) the(O) Orange(B-political party) Democratic(I-political party) Movement(I-political party) .(O)"}}
{"id":"91","dataset":"crossner_politics","split":"train","label_list":["election","person","organization","political party","country","location","politician","event"],"instance":{"id":"91","words":["Friedman","had","hoped","to","follow","in","the","footsteps","of","other","entertainers-turned-governors",",","including","Jimmie","Davis",",","Jesse","Ventura",",","Arnold","Schwarzenegger",",","and","Ronald","Reagan","."],"labels":["B-politician","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, organization, political party, country, location, politician, event and O.\nSentence: Friedman had hoped to follow in the footsteps of other entertainers-turned-governors , including Jimmie Davis , Jesse Ventura , Arnold Schwarzenegger , and Ronald Reagan .","prompt_labels":"Friedman(B-politician) had(O) hoped(O) to(O) follow(O) in(O) the(O) footsteps(O) of(O) other(O) entertainers-turned-governors(O) ,(O) including(O) Jimmie(B-politician) Davis(I-politician) ,(O) Jesse(B-politician) Ventura(I-politician) ,(O) Arnold(B-politician) Schwarzenegger(I-politician) ,(O) and(O) Ronald(B-politician) Reagan(I-politician) .(O)"}}
{"id":"92","dataset":"crossner_politics","split":"train","label_list":["location","politician","political party","election","organization","event","person","country"],"instance":{"id":"92","words":["While","exiled","in","Japan","in","1914",",","Sun","established","the","Chinese","Revolutionary","Party","on","8","July","1914",",","but","many","of","his","old","revolutionary","comrades",",","including","Huang","Xing",",","Wang","Jingwei",",","Hu","Hanmin","and","Chen","Jiongming",",","refused","to","join","him","or","support","his","efforts","in","inciting","armed","uprising","against","Yuan","."],"labels":["O","O","O","B-country","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, political party, election, organization, event, person, country and O.\nSentence: While exiled in Japan in 1914 , Sun established the Chinese Revolutionary Party on 8 July 1914 , but many of his old revolutionary comrades , including Huang Xing , Wang Jingwei , Hu Hanmin and Chen Jiongming , refused to join him or support his efforts in inciting armed uprising against Yuan .","prompt_labels":"While(O) exiled(O) in(O) Japan(B-country) in(O) 1914(O) ,(O) Sun(O) established(O) the(O) Chinese(B-political party) Revolutionary(I-political party) Party(I-political party) on(O) 8(O) July(O) 1914(O) ,(O) but(O) many(O) of(O) his(O) old(O) revolutionary(O) comrades(O) ,(O) including(O) Huang(B-politician) Xing(I-politician) ,(O) Wang(B-politician) Jingwei(I-politician) ,(O) Hu(B-politician) Hanmin(I-politician) and(O) Chen(B-politician) Jiongming(I-politician) ,(O) refused(O) to(O) join(O) him(O) or(O) support(O) his(O) efforts(O) in(O) inciting(O) armed(O) uprising(O) against(O) Yuan(B-politician) .(O)"}}
{"id":"93","dataset":"crossner_politics","split":"train","label_list":["election","event","political party","politician","location","organization","country","person"],"instance":{"id":"93","words":["Masud","Sabri",",","a","Uyghur","was","appointed","as","Governor","of","Xinjiang","by","KMT",",","as","was","the","Tatar","Burhan","Shahidi","and","the","Uyghur","Yulbars","Khan","."],"labels":["B-politician","I-politician","O","O","O","O","O","O","O","O","B-location","O","B-political party","O","O","O","O","O","B-politician","I-politician","O","O","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, political party, politician, location, organization, country, person and O.\nSentence: Masud Sabri , a Uyghur was appointed as Governor of Xinjiang by KMT , as was the Tatar Burhan Shahidi and the Uyghur Yulbars Khan .","prompt_labels":"Masud(B-politician) Sabri(I-politician) ,(O) a(O) Uyghur(O) was(O) appointed(O) as(O) Governor(O) of(O) Xinjiang(B-location) by(O) KMT(B-political party) ,(O) as(O) was(O) the(O) Tatar(O) Burhan(B-politician) Shahidi(I-politician) and(O) the(O) Uyghur(O) Yulbars(B-politician) Khan(I-politician) .(O)"}}
{"id":"94","dataset":"crossner_politics","split":"train","label_list":["location","country","organization","political party","person","event","politician","election"],"instance":{"id":"94","words":["Following","their","victory",",","the","Khmer","Rouge","who","were","led","by","Pol","Pot",",","Nuon","Chea",",","Ieng","Sary",",","Son","Sen",",","and","Khieu","Samphan","immediately","set","about","forcibly","evacuating","the","country","'s","major","cities","and","in","1976","they","renamed","the","country","Democratic","Kampuchea","."],"labels":["O","O","O","O","O","B-organization","I-organization","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, organization, political party, person, event, politician, election and O.\nSentence: Following their victory , the Khmer Rouge who were led by Pol Pot , Nuon Chea , Ieng Sary , Son Sen , and Khieu Samphan immediately set about forcibly evacuating the country 's major cities and in 1976 they renamed the country Democratic Kampuchea .","prompt_labels":"Following(O) their(O) victory(O) ,(O) the(O) Khmer(B-organization) Rouge(I-organization) who(O) were(O) led(O) by(O) Pol(B-politician) Pot(I-politician) ,(O) Nuon(B-politician) Chea(I-politician) ,(O) Ieng(B-politician) Sary(I-politician) ,(O) Son(B-politician) Sen(I-politician) ,(O) and(O) Khieu(B-politician) Samphan(I-politician) immediately(O) set(O) about(O) forcibly(O) evacuating(O) the(O) country(O) 's(O) major(O) cities(O) and(O) in(O) 1976(O) they(O) renamed(O) the(O) country(O) Democratic(B-country) Kampuchea(I-country) .(O)"}}
{"id":"95","dataset":"crossner_politics","split":"train","label_list":["politician","location","organization","country","event","person","political party","election"],"instance":{"id":"95","words":["In","the","1932","German","presidential","election",",","the","SPD","supported","Paul","von","Hindenburg","for","President",",","fearing","that","a","split","vote","would","hand","the","job","to","the","National","Socialist","German","Workers","'","Party","(","NSDAP",")","candidate",",","Adolf","Hitler","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","O","B-political party","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-politician","I-politician","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, organization, country, event, person, political party, election and O.\nSentence: In the 1932 German presidential election , the SPD supported Paul von Hindenburg for President , fearing that a split vote would hand the job to the National Socialist German Workers ' Party ( NSDAP ) candidate , Adolf Hitler .","prompt_labels":"In(O) the(O) 1932(B-election) German(I-election) presidential(I-election) election(I-election) ,(O) the(O) SPD(B-political party) supported(O) Paul(B-politician) von(I-politician) Hindenburg(I-politician) for(O) President(O) ,(O) fearing(O) that(O) a(O) split(O) vote(O) would(O) hand(O) the(O) job(O) to(O) the(O) National(B-political party) Socialist(I-political party) German(I-political party) Workers(I-political party) '(I-political party) Party(I-political party) ((O) NSDAP(B-political party) )(O) candidate(O) ,(O) Adolf(B-politician) Hitler(I-politician) .(O)"}}
{"id":"96","dataset":"crossner_politics","split":"train","label_list":["political party","event","location","organization","person","politician","election","country"],"instance":{"id":"96","words":["In","almost","every","year",",","Wyden","has","maintained","a","100","percent","rating","or","close","to","it","with","pro-choice","groups",":","NARAL","Pro-Choice","America",",","Planned","Parenthood",",","and","National","Family","Planning","and","Reproductive","Health","Association",",","and","a","0","percent","rating","or","close","to","it","from","pro-life","group",":","the","National","Right","to","Life","Committee","."],"labels":["O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, location, organization, person, politician, election, country and O.\nSentence: In almost every year , Wyden has maintained a 100 percent rating or close to it with pro-choice groups : NARAL Pro-Choice America , Planned Parenthood , and National Family Planning and Reproductive Health Association , and a 0 percent rating or close to it from pro-life group : the National Right to Life Committee .","prompt_labels":"In(O) almost(O) every(O) year(O) ,(O) Wyden(B-person) has(O) maintained(O) a(O) 100(O) percent(O) rating(O) or(O) close(O) to(O) it(O) with(O) pro-choice(O) groups(O) :(O) NARAL(B-organization) Pro-Choice(I-organization) America(I-organization) ,(O) Planned(B-organization) Parenthood(I-organization) ,(O) and(O) National(B-organization) Family(I-organization) Planning(I-organization) and(I-organization) Reproductive(I-organization) Health(I-organization) Association(I-organization) ,(O) and(O) a(O) 0(O) percent(O) rating(O) or(O) close(O) to(O) it(O) from(O) pro-life(O) group(O) :(O) the(O) National(B-organization) Right(I-organization) to(I-organization) Life(I-organization) Committee(I-organization) .(O)"}}
{"id":"97","dataset":"crossner_politics","split":"train","label_list":["politician","election","organization","political party","country","event","person","location"],"instance":{"id":"97","words":["The","centre","cooperates","closely","with","the","Carter","Center","in","Atlanta",",","the","Kim","Dae","Jung","Library","in","Seoul","and","the","Crisis","Management","Initiative","in","Helsinki","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","O","B-location","O","O","B-organization","I-organization","I-organization","I-organization","O","B-location","O","O","B-organization","I-organization","I-organization","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, organization, political party, country, event, person, location and O.\nSentence: The centre cooperates closely with the Carter Center in Atlanta , the Kim Dae Jung Library in Seoul and the Crisis Management Initiative in Helsinki .","prompt_labels":"The(O) centre(O) cooperates(O) closely(O) with(O) the(O) Carter(B-organization) Center(I-organization) in(O) Atlanta(B-location) ,(O) the(O) Kim(B-organization) Dae(I-organization) Jung(I-organization) Library(I-organization) in(O) Seoul(B-location) and(O) the(O) Crisis(B-organization) Management(I-organization) Initiative(I-organization) in(O) Helsinki(B-location) .(O)"}}
{"id":"98","dataset":"crossner_politics","split":"train","label_list":["politician","country","political party","person","event","organization","election","location"],"instance":{"id":"98","words":["Successively",",","she","was","a","member","of","the","Social","Democracy","of","the","Kingdom","of","Poland","and","Lithuania","(","SDKPiL",")",",","the","Social","Democratic","Party","of","Germany","(","SPD",")",",","the","Independent","Social","Democratic","Party","of","Germany","(","USPD",")","and","the","Communist","Party","of","Germany","(","KPD",")","."],"labels":["O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, political party, person, event, organization, election, location and O.\nSentence: Successively , she was a member of the Social Democracy of the Kingdom of Poland and Lithuania ( SDKPiL ) , the Social Democratic Party of Germany ( SPD ) , the Independent Social Democratic Party of Germany ( USPD ) and the Communist Party of Germany ( KPD ) .","prompt_labels":"Successively(O) ,(O) she(O) was(O) a(O) member(O) of(O) the(O) Social(B-political party) Democracy(I-political party) of(I-political party) the(I-political party) Kingdom(I-political party) of(I-political party) Poland(I-political party) and(I-political party) Lithuania(I-political party) ((O) SDKPiL(B-political party) )(O) ,(O) the(O) Social(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Germany(I-political party) ((O) SPD(B-political party) )(O) ,(O) the(O) Independent(B-political party) Social(I-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Germany(I-political party) ((O) USPD(B-political party) )(O) and(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Germany(I-political party) ((O) KPD(B-political party) )(O) .(O)"}}
{"id":"99","dataset":"crossner_politics","split":"train","label_list":["person","location","organization","event","political party","politician","election","country"],"instance":{"id":"99","words":["The","devolved","administration","was","suspended","several","times","(","especially","between","15","October","2002","and","8","May","2007",")","because","the","Ulster","Unionist","Party","and","Democratic","Unionist","Party","were","uncomfortable","being","in","government","with","Sinn","Féin","when","the","Provisional","Irish","Republican","Army","had","failed","to","decommission","its","arms","fully","and","continued","its","criminal","activities","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, organization, event, political party, politician, election, country and O.\nSentence: The devolved administration was suspended several times ( especially between 15 October 2002 and 8 May 2007 ) because the Ulster Unionist Party and Democratic Unionist Party were uncomfortable being in government with Sinn Féin when the Provisional Irish Republican Army had failed to decommission its arms fully and continued its criminal activities .","prompt_labels":"The(O) devolved(O) administration(O) was(O) suspended(O) several(O) times(O) ((O) especially(O) between(O) 15(O) October(O) 2002(O) and(O) 8(O) May(O) 2007(O) )(O) because(O) the(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) and(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) were(O) uncomfortable(O) being(O) in(O) government(O) with(O) Sinn(B-political party) Féin(I-political party) when(O) the(O) Provisional(B-organization) Irish(I-organization) Republican(I-organization) Army(I-organization) had(O) failed(O) to(O) decommission(O) its(O) arms(O) fully(O) and(O) continued(O) its(O) criminal(O) activities(O) .(O)"}}
{"id":"100","dataset":"crossner_politics","split":"train","label_list":["organization","political party","election","country","person","event","politician","location"],"instance":{"id":"100","words":["Inukai","was","a","leading","figure","in","the","successors","to","the","Rikken","Kaishintō",",","the","Shimpotō",",","Kenseitō","and","the","Rikken","Kokumintō",",","which","eventually","toppled","the","government","of","Katsura","Tarō","in","1913","."],"labels":["B-politician","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","B-political party","O","B-political party","O","O","B-political party","I-political party","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, election, country, person, event, politician, location and O.\nSentence: Inukai was a leading figure in the successors to the Rikken Kaishintō , the Shimpotō , Kenseitō and the Rikken Kokumintō , which eventually toppled the government of Katsura Tarō in 1913 .","prompt_labels":"Inukai(B-politician) was(O) a(O) leading(O) figure(O) in(O) the(O) successors(O) to(O) the(O) Rikken(B-political party) Kaishintō(I-political party) ,(O) the(O) Shimpotō(B-political party) ,(O) Kenseitō(B-political party) and(O) the(O) Rikken(B-political party) Kokumintō(I-political party) ,(O) which(O) eventually(O) toppled(O) the(O) government(O) of(O) Katsura(B-politician) Tarō(I-politician) in(O) 1913(O) .(O)"}}
{"id":"101","dataset":"crossner_politics","split":"train","label_list":["location","organization","political party","person","country","election","politician","event"],"instance":{"id":"101","words":["He","attended","an","academy","run","by","the","crown","prince","'s","ethics","tutor","in","Tokyo",",","and","briefly","studied","at","Keio","University","and","the","Tokyo","Physics","School","(","now","the","Tokyo","University","of","Science",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, political party, person, country, election, politician, event and O.\nSentence: He attended an academy run by the crown prince 's ethics tutor in Tokyo , and briefly studied at Keio University and the Tokyo Physics School ( now the Tokyo University of Science ) .","prompt_labels":"He(O) attended(O) an(O) academy(O) run(O) by(O) the(O) crown(O) prince(O) 's(O) ethics(O) tutor(O) in(O) Tokyo(B-location) ,(O) and(O) briefly(O) studied(O) at(O) Keio(B-organization) University(I-organization) and(O) the(O) Tokyo(B-organization) Physics(I-organization) School(I-organization) ((O) now(O) the(O) Tokyo(B-organization) University(I-organization) of(I-organization) Science(I-organization) )(O) .(O)"}}
{"id":"102","dataset":"crossner_politics","split":"train","label_list":["organization","event","location","person","politician","country","election","political party"],"instance":{"id":"102","words":["Johnson","unsuccessfully","sought","the","Republican","presidential","nomination","in","1920","United","States","presidential","election","and","1924","United","States","presidential","election","and","supported","Democrat","Franklin","D.","Roosevelt","in","the","1932","United","States","presidential","election","."],"labels":["B-politician","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","O","B-politician","I-politician","I-politician","O","O","B-election","I-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, location, person, politician, country, election, political party and O.\nSentence: Johnson unsuccessfully sought the Republican presidential nomination in 1920 United States presidential election and 1924 United States presidential election and supported Democrat Franklin D. Roosevelt in the 1932 United States presidential election .","prompt_labels":"Johnson(B-politician) unsuccessfully(O) sought(O) the(O) Republican(O) presidential(O) nomination(O) in(O) 1920(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) 1924(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) supported(O) Democrat(O) Franklin(B-politician) D.(I-politician) Roosevelt(I-politician) in(O) the(O) 1932(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) .(O)"}}
{"id":"103","dataset":"crossner_politics","split":"train","label_list":["election","person","organization","political party","location","event","country","politician"],"instance":{"id":"103","words":["During","and","following","his","first","governorship",",","Brown","ran","as","a","candidate","for","the","Democratic","presidential","nomination","in","1976","United","States","presidential","election",",","1980","Democratic","Party","presidential","primaries","and","1992","Democratic","Party","presidential","primaries","."],"labels":["O","O","O","O","O","O","O","B-politician","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, organization, political party, location, event, country, politician and O.\nSentence: During and following his first governorship , Brown ran as a candidate for the Democratic presidential nomination in 1976 United States presidential election , 1980 Democratic Party presidential primaries and 1992 Democratic Party presidential primaries .","prompt_labels":"During(O) and(O) following(O) his(O) first(O) governorship(O) ,(O) Brown(B-politician) ran(O) as(O) a(O) candidate(O) for(O) the(O) Democratic(O) presidential(O) nomination(O) in(O) 1976(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) 1980(B-election) Democratic(I-election) Party(I-election) presidential(I-election) primaries(I-election) and(O) 1992(B-election) Democratic(I-election) Party(I-election) presidential(I-election) primaries(I-election) .(O)"}}
{"id":"104","dataset":"crossner_politics","split":"train","label_list":["organization","country","event","election","politician","location","person","political party"],"instance":{"id":"104","words":["At","its","peak","in","the","late","1980s","up","to","the","mid-1990s",",","Garuda","operated","an","extensive","network","of","flights","all","over","the","world",",","with","regularly","scheduled","services","to","Los","Angeles",",","Paris",",","Rome",",","Fukuoka",",","Adelaide",",","Johannesburg",",","Cairo","and","other","cities","in","Europe",",","Australia","and","Asia","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","O","O","O","B-location","O","B-location","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, event, election, politician, location, person, political party and O.\nSentence: At its peak in the late 1980s up to the mid-1990s , Garuda operated an extensive network of flights all over the world , with regularly scheduled services to Los Angeles , Paris , Rome , Fukuoka , Adelaide , Johannesburg , Cairo and other cities in Europe , Australia and Asia .","prompt_labels":"At(O) its(O) peak(O) in(O) the(O) late(O) 1980s(O) up(O) to(O) the(O) mid-1990s(O) ,(O) Garuda(B-organization) operated(O) an(O) extensive(O) network(O) of(O) flights(O) all(O) over(O) the(O) world(O) ,(O) with(O) regularly(O) scheduled(O) services(O) to(O) Los(B-location) Angeles(I-location) ,(O) Paris(B-location) ,(O) Rome(B-location) ,(O) Fukuoka(B-location) ,(O) Adelaide(B-location) ,(O) Johannesburg(B-location) ,(O) Cairo(B-location) and(O) other(O) cities(O) in(O) Europe(B-location) ,(O) Australia(B-location) and(O) Asia(B-location) .(O)"}}
{"id":"105","dataset":"crossner_politics","split":"train","label_list":["country","organization","political party","person","election","location","politician","event"],"instance":{"id":"105","words":["In","1965",",","the","airline","took","delivery","of","its","first","Douglas","DC-8",",","and","grew","beyond","the","Asian","market","it","was","focused","on",",","beginning","scheduled","flights","from","Kemayoran","Airport","to","Amsterdam","and","Frankfurt","via","Colombo",",","Mumbai",",","and","Prague","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","B-location","O","B-location","O","B-location","O","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, political party, person, election, location, politician, event and O.\nSentence: In 1965 , the airline took delivery of its first Douglas DC-8 , and grew beyond the Asian market it was focused on , beginning scheduled flights from Kemayoran Airport to Amsterdam and Frankfurt via Colombo , Mumbai , and Prague .","prompt_labels":"In(O) 1965(O) ,(O) the(O) airline(O) took(O) delivery(O) of(O) its(O) first(O) Douglas(O) DC-8(O) ,(O) and(O) grew(O) beyond(O) the(O) Asian(B-location) market(O) it(O) was(O) focused(O) on(O) ,(O) beginning(O) scheduled(O) flights(O) from(O) Kemayoran(B-location) Airport(I-location) to(O) Amsterdam(B-location) and(O) Frankfurt(B-location) via(O) Colombo(B-location) ,(O) Mumbai(B-location) ,(O) and(O) Prague(B-location) .(O)"}}
{"id":"106","dataset":"crossner_politics","split":"train","label_list":["politician","event","person","country","political party","location","election","organization"],"instance":{"id":"106","words":["Rome","and","Paris","became","the","airline","'s","third","and","fourth","European","destinations",",","with","flights","stopping","in","Mumbai","and","Cairo","to","refuel","."],"labels":["B-location","O","B-location","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","B-location","O","B-location","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, person, country, political party, location, election, organization and O.\nSentence: Rome and Paris became the airline 's third and fourth European destinations , with flights stopping in Mumbai and Cairo to refuel .","prompt_labels":"Rome(B-location) and(O) Paris(B-location) became(O) the(O) airline(O) 's(O) third(O) and(O) fourth(O) European(B-location) destinations(O) ,(O) with(O) flights(O) stopping(O) in(O) Mumbai(B-location) and(O) Cairo(B-location) to(O) refuel(O) .(O)"}}
{"id":"107","dataset":"crossner_politics","split":"train","label_list":["location","event","election","political party","organization","politician","person","country"],"instance":{"id":"107","words":["After","unsuccessfully","contesting","the","constituency","of","Harwich","at","the","1954","by-election","and","1955","United","Kingdom","general","election",",","as","well","as","the","constituency","of","Southampton","Test","at","the","1959","United","Kingdom","general","election",",","Williams","was","returned","in","the","1964","United","Kingdom","general","election","as","Labour","MP","for","the","constituency","of","Hitchin","in","Hertfordshire","."],"labels":["O","O","O","O","O","O","B-location","O","O","B-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-politician","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-political party","O","O","O","O","O","B-location","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, election, political party, organization, politician, person, country and O.\nSentence: After unsuccessfully contesting the constituency of Harwich at the 1954 by-election and 1955 United Kingdom general election , as well as the constituency of Southampton Test at the 1959 United Kingdom general election , Williams was returned in the 1964 United Kingdom general election as Labour MP for the constituency of Hitchin in Hertfordshire .","prompt_labels":"After(O) unsuccessfully(O) contesting(O) the(O) constituency(O) of(O) Harwich(B-location) at(O) the(O) 1954(B-election) by-election(I-election) and(O) 1955(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) as(O) well(O) as(O) the(O) constituency(O) of(O) Southampton(O) Test(O) at(O) the(O) 1959(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) Williams(B-politician) was(O) returned(O) in(O) the(O) 1964(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) as(O) Labour(B-political party) MP(O) for(O) the(O) constituency(O) of(O) Hitchin(B-location) in(O) Hertfordshire(B-location) .(O)"}}
{"id":"108","dataset":"crossner_politics","split":"train","label_list":["person","country","election","politician","political party","location","organization","event"],"instance":{"id":"108","words":["Between","1925","and","1930","Germany","was","the","site","of","innovative","and","extensive","municipal","public","housing","projects",",","mostly","in","Berlin",",","Hamburg",",","Cologne","and","Frankfurt","."],"labels":["O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, election, politician, political party, location, organization, event and O.\nSentence: Between 1925 and 1930 Germany was the site of innovative and extensive municipal public housing projects , mostly in Berlin , Hamburg , Cologne and Frankfurt .","prompt_labels":"Between(O) 1925(O) and(O) 1930(O) Germany(B-country) was(O) the(O) site(O) of(O) innovative(O) and(O) extensive(O) municipal(O) public(O) housing(O) projects(O) ,(O) mostly(O) in(O) Berlin(B-location) ,(O) Hamburg(B-location) ,(O) Cologne(B-location) and(O) Frankfurt(B-location) .(O)"}}
{"id":"109","dataset":"crossner_politics","split":"train","label_list":["political party","location","country","event","politician","election","person","organization"],"instance":{"id":"109","words":["In","this","same","period",",","such","organizations","as","the","Colonial","Dames","of","America",",","the","Mary","Ball","Washington","Memorial","Society",",","Preservation","Virginia",",","United","Daughters","of","the","Confederacy",",","and","Sons","of","Confederate","Veterans","were","also","founded","."],"labels":["O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, location, country, event, politician, election, person, organization and O.\nSentence: In this same period , such organizations as the Colonial Dames of America , the Mary Ball Washington Memorial Society , Preservation Virginia , United Daughters of the Confederacy , and Sons of Confederate Veterans were also founded .","prompt_labels":"In(O) this(O) same(O) period(O) ,(O) such(O) organizations(O) as(O) the(O) Colonial(B-organization) Dames(I-organization) of(I-organization) America(I-organization) ,(O) the(O) Mary(B-organization) Ball(I-organization) Washington(I-organization) Memorial(I-organization) Society(I-organization) ,(O) Preservation(B-organization) Virginia(I-organization) ,(O) United(B-organization) Daughters(I-organization) of(I-organization) the(I-organization) Confederacy(I-organization) ,(O) and(O) Sons(B-organization) of(I-organization) Confederate(I-organization) Veterans(I-organization) were(O) also(O) founded(O) .(O)"}}
{"id":"110","dataset":"crossner_politics","split":"train","label_list":["event","country","politician","location","political party","person","election","organization"],"instance":{"id":"110","words":["After","it","left","Gelsenkirchen",",","the","train","was","boarded","by","other","Jews","from","Münster",",","Dortmund","and","a","few","other","stops","along","the","way",",","and","mostly","by","the","Jews","of","Hanover",",","500","in","number","."],"labels":["O","O","O","B-location","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, politician, location, political party, person, election, organization and O.\nSentence: After it left Gelsenkirchen , the train was boarded by other Jews from Münster , Dortmund and a few other stops along the way , and mostly by the Jews of Hanover , 500 in number .","prompt_labels":"After(O) it(O) left(O) Gelsenkirchen(B-location) ,(O) the(O) train(O) was(O) boarded(O) by(O) other(O) Jews(O) from(O) Münster(B-location) ,(O) Dortmund(B-location) and(O) a(O) few(O) other(O) stops(O) along(O) the(O) way(O) ,(O) and(O) mostly(O) by(O) the(O) Jews(O) of(O) Hanover(B-location) ,(O) 500(O) in(O) number(O) .(O)"}}
{"id":"111","dataset":"crossner_politics","split":"train","label_list":["election","organization","location","person","event","political party","politician","country"],"instance":{"id":"111","words":["The","only","interruptions","occurred","in","1813-1814",",","occasioned","by","the","German","War","of","Liberation","(","War","of","the","Sixth","Coalition",")",",","during","which","the","university","was","closed",",","and","those","occasioned","by","two","prolonged","literary","tours",",","first","in","1820","to","Paris",",","London","and","Oxford","with","his","colleague","Johann","Karl","Thilo","(","1794-1853",")","for","the","examination","of","rare","oriental","manuscripts",",","and","in","1835","to","England","and","the","Netherlands","in","connection","with","his","Phoenician","studies","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","B-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","O","O","B-person","I-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","B-country","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, location, person, event, political party, politician, country and O.\nSentence: The only interruptions occurred in 1813-1814 , occasioned by the German War of Liberation ( War of the Sixth Coalition ) , during which the university was closed , and those occasioned by two prolonged literary tours , first in 1820 to Paris , London and Oxford with his colleague Johann Karl Thilo ( 1794-1853 ) for the examination of rare oriental manuscripts , and in 1835 to England and the Netherlands in connection with his Phoenician studies .","prompt_labels":"The(O) only(O) interruptions(O) occurred(O) in(O) 1813-1814(O) ,(O) occasioned(O) by(O) the(O) German(O) War(B-event) of(I-event) Liberation(I-event) ((O) War(B-event) of(I-event) the(I-event) Sixth(I-event) Coalition(I-event) )(O) ,(O) during(O) which(O) the(O) university(O) was(O) closed(O) ,(O) and(O) those(O) occasioned(O) by(O) two(O) prolonged(O) literary(O) tours(O) ,(O) first(O) in(O) 1820(O) to(O) Paris(B-location) ,(O) London(B-location) and(O) Oxford(B-location) with(O) his(O) colleague(O) Johann(B-person) Karl(I-person) Thilo(I-person) ((O) 1794-1853(O) )(O) for(O) the(O) examination(O) of(O) rare(O) oriental(O) manuscripts(O) ,(O) and(O) in(O) 1835(O) to(O) England(B-country) and(O) the(O) Netherlands(B-country) in(O) connection(O) with(O) his(O) Phoenician(O) studies(O) .(O)"}}
{"id":"112","dataset":"crossner_politics","split":"train","label_list":["location","event","organization","election","political party","person","country","politician"],"instance":{"id":"112","words":["Isaac","Butt","(","6","September","1813","-","5","May","1879",")",",","was","an","Irish","barrister",",","politician",",","Member","of","Parliament","(","M.P.",")","in","the","House","of","Commons","of","the","United","Kingdom",",","and","the","founder","and","first","leader","of","a","number","of","Irish","nationalist","parties","and","organisations",",","including","the","Irish","Metropolitan","Conservative","Society","in","1836",",","the","Home","Government","Association","in","1870","and","in","1873","the","Home","Rule","League","."],"labels":["B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, organization, election, political party, person, country, politician and O.\nSentence: Isaac Butt ( 6 September 1813 - 5 May 1879 ) , was an Irish barrister , politician , Member of Parliament ( M.P. ) in the House of Commons of the United Kingdom , and the founder and first leader of a number of Irish nationalist parties and organisations , including the Irish Metropolitan Conservative Society in 1836 , the Home Government Association in 1870 and in 1873 the Home Rule League .","prompt_labels":"Isaac(B-politician) Butt(I-politician) ((O) 6(O) September(O) 1813(O) -(O) 5(O) May(O) 1879(O) )(O) ,(O) was(O) an(O) Irish(O) barrister(O) ,(O) politician(O) ,(O) Member(O) of(O) Parliament(B-organization) ((O) M.P.(O) )(O) in(O) the(O) House(B-organization) of(I-organization) Commons(I-organization) of(I-organization) the(I-organization) United(I-organization) Kingdom(I-organization) ,(O) and(O) the(O) founder(O) and(O) first(O) leader(O) of(O) a(O) number(O) of(O) Irish(O) nationalist(O) parties(O) and(O) organisations(O) ,(O) including(O) the(O) Irish(B-political party) Metropolitan(I-political party) Conservative(I-political party) Society(I-political party) in(O) 1836(O) ,(O) the(O) Home(B-political party) Government(I-political party) Association(I-political party) in(O) 1870(O) and(O) in(O) 1873(O) the(O) Home(B-political party) Rule(I-political party) League(I-political party) .(O)"}}
{"id":"113","dataset":"crossner_politics","split":"train","label_list":["politician","organization","location","country","election","person","political party","event"],"instance":{"id":"113","words":["It","is","generally","estimated","that","over","3","million","people","marched","in","Rome",",","between","one","and","two","million","in","London",",","more","than","600,000","in","Madrid",",","300,000","in","Berlin",",","as","well","as","in","Damascus",",","Paris",",","New","York",",","Oslo",",","Stockholm",",","Brussels",",","Johannesburg",",","Montreal","-","more","than","600","cities","in","all",",","worldwide","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","B-country","O","O","O","O","O","B-location","O","O","O","B-location","O","O","O","O","O","B-location","O","B-location","O","B-location","I-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, location, country, election, person, political party, event and O.\nSentence: It is generally estimated that over 3 million people marched in Rome , between one and two million in London , more than 600,000 in Madrid , 300,000 in Berlin , as well as in Damascus , Paris , New York , Oslo , Stockholm , Brussels , Johannesburg , Montreal - more than 600 cities in all , worldwide .","prompt_labels":"It(O) is(O) generally(O) estimated(O) that(O) over(O) 3(O) million(O) people(O) marched(O) in(O) Rome(B-country) ,(O) between(O) one(O) and(O) two(O) million(O) in(O) London(B-country) ,(O) more(O) than(O) 600,000(O) in(O) Madrid(B-location) ,(O) 300,000(O) in(O) Berlin(B-location) ,(O) as(O) well(O) as(O) in(O) Damascus(B-location) ,(O) Paris(B-location) ,(O) New(B-location) York(I-location) ,(O) Oslo(B-location) ,(O) Stockholm(B-location) ,(O) Brussels(B-location) ,(O) Johannesburg(B-location) ,(O) Montreal(B-location) -(O) more(O) than(O) 600(O) cities(O) in(O) all(O) ,(O) worldwide(O) .(O)"}}
{"id":"114","dataset":"crossner_politics","split":"train","label_list":["election","political party","location","event","organization","country","person","politician"],"instance":{"id":"114","words":["During","the","1990s","and","2000s",",","NARAL","Pro-Choice","America","and","Planned","Parenthood","typically","gave","Kennedy","ratings","of","100","percent",",","while","the","National","Right","to","Life","Committee","typically","gave","him","a","rating","of","less","than","10","percent","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","O","B-politician","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, location, event, organization, country, person, politician and O.\nSentence: During the 1990s and 2000s , NARAL Pro-Choice America and Planned Parenthood typically gave Kennedy ratings of 100 percent , while the National Right to Life Committee typically gave him a rating of less than 10 percent .","prompt_labels":"During(O) the(O) 1990s(O) and(O) 2000s(O) ,(O) NARAL(B-organization) Pro-Choice(I-organization) America(I-organization) and(O) Planned(B-organization) Parenthood(I-organization) typically(O) gave(O) Kennedy(B-politician) ratings(O) of(O) 100(O) percent(O) ,(O) while(O) the(O) National(B-organization) Right(I-organization) to(I-organization) Life(I-organization) Committee(I-organization) typically(O) gave(O) him(O) a(O) rating(O) of(O) less(O) than(O) 10(O) percent(O) .(O)"}}
{"id":"115","dataset":"crossner_politics","split":"train","label_list":["election","politician","country","person","political party","location","event","organization"],"instance":{"id":"115","words":["As","the","Australia","Party","it","became","influential",",","particularly","in","the","landmark","1972","Australian","federal","election","when","its","preferences","assisted","the","Australian","Labor","Party","to","victory","-","ending","23","years","of","Liberal","Party","of","Australia","/","National","Party","of","Australia","Coalition","government","."],"labels":["O","O","B-country","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, country, person, political party, location, event, organization and O.\nSentence: As the Australia Party it became influential , particularly in the landmark 1972 Australian federal election when its preferences assisted the Australian Labor Party to victory - ending 23 years of Liberal Party of Australia / National Party of Australia Coalition government .","prompt_labels":"As(O) the(O) Australia(B-country) Party(O) it(O) became(O) influential(O) ,(O) particularly(O) in(O) the(O) landmark(O) 1972(B-election) Australian(I-election) federal(I-election) election(I-election) when(O) its(O) preferences(O) assisted(O) the(O) Australian(B-political party) Labor(I-political party) Party(I-political party) to(O) victory(O) -(O) ending(O) 23(O) years(O) of(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) /(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) Coalition(O) government(O) .(O)"}}
{"id":"116","dataset":"crossner_politics","split":"train","label_list":["politician","political party","person","organization","event","election","country","location"],"instance":{"id":"116","words":["Subsequently",",","the","party","allied","itself","with","the","New","Liberal","Movement","in","the","formation","of","the","Australian","Democrats","for","the","1977","Australian","federal","election","."],"labels":["O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","B-political party","I-political party","O","O","B-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, person, organization, event, election, country, location and O.\nSentence: Subsequently , the party allied itself with the New Liberal Movement in the formation of the Australian Democrats for the 1977 Australian federal election .","prompt_labels":"Subsequently(O) ,(O) the(O) party(O) allied(O) itself(O) with(O) the(O) New(B-political party) Liberal(I-political party) Movement(I-political party) in(O) the(O) formation(O) of(O) the(O) Australian(B-political party) Democrats(I-political party) for(O) the(O) 1977(B-election) Australian(I-election) federal(I-election) election(I-election) .(O)"}}
{"id":"117","dataset":"crossner_politics","split":"train","label_list":["person","election","event","politician","country","organization","political party","location"],"instance":{"id":"117","words":["The","Public","Voice","project","is","made","possible",",","in","part",",","by","support","from","the","Ford","Foundation",",","the","Markle","Foundation",",","the","Open","Society","Foundations",",","and","EPIC","."],"labels":["O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, event, politician, country, organization, political party, location and O.\nSentence: The Public Voice project is made possible , in part , by support from the Ford Foundation , the Markle Foundation , the Open Society Foundations , and EPIC .","prompt_labels":"The(O) Public(B-organization) Voice(I-organization) project(O) is(O) made(O) possible(O) ,(O) in(O) part(O) ,(O) by(O) support(O) from(O) the(O) Ford(B-organization) Foundation(I-organization) ,(O) the(O) Markle(B-organization) Foundation(I-organization) ,(O) the(O) Open(B-organization) Society(I-organization) Foundations(I-organization) ,(O) and(O) EPIC(B-organization) .(O)"}}
{"id":"118","dataset":"crossner_politics","split":"train","label_list":["location","person","event","politician","political party","organization","country","election"],"instance":{"id":"118","words":["In","June","1970",",","a","Dutch","group","called","Kabouters","won","5","of","the","45","seats","on","the","Amsterdam","Gemeenteraad","(","City","Council",")",",","as","well","as","two","seats","each","on","councils","in","The","Hague","and","Leeuwarden","and","one","seat","apiece","in","Arnhem",",","Alkmaar","and","Leiden","."],"labels":["O","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","O","O","O","O","B-location","O","B-location","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, event, politician, political party, organization, country, election and O.\nSentence: In June 1970 , a Dutch group called Kabouters won 5 of the 45 seats on the Amsterdam Gemeenteraad ( City Council ) , as well as two seats each on councils in The Hague and Leeuwarden and one seat apiece in Arnhem , Alkmaar and Leiden .","prompt_labels":"In(O) June(O) 1970(O) ,(O) a(O) Dutch(O) group(O) called(O) Kabouters(B-organization) won(O) 5(O) of(O) the(O) 45(O) seats(O) on(O) the(O) Amsterdam(B-location) Gemeenteraad(O) ((O) City(O) Council(O) )(O) ,(O) as(O) well(O) as(O) two(O) seats(O) each(O) on(O) councils(O) in(O) The(B-location) Hague(I-location) and(O) Leeuwarden(B-location) and(O) one(O) seat(O) apiece(O) in(O) Arnhem(B-location) ,(O) Alkmaar(B-location) and(O) Leiden(B-location) .(O)"}}
{"id":"119","dataset":"crossner_politics","split":"train","label_list":["event","politician","person","election","political party","country","location","organization"],"instance":{"id":"119","words":["It","was","not","registered","as","an","official","party",",","but","some","participants","in","that","effort","went","on","to","form","the","Green","Party","of","Canada","in","1983","(","the","Green","Party","of","Ontario","and","Green","Party","of","British","Columbia","were","also","formed","that","year",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, person, election, political party, country, location, organization and O.\nSentence: It was not registered as an official party , but some participants in that effort went on to form the Green Party of Canada in 1983 ( the Green Party of Ontario and Green Party of British Columbia were also formed that year ) .","prompt_labels":"It(O) was(O) not(O) registered(O) as(O) an(O) official(O) party(O) ,(O) but(O) some(O) participants(O) in(O) that(O) effort(O) went(O) on(O) to(O) form(O) the(O) Green(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) in(O) 1983(O) ((O) the(O) Green(B-political party) Party(I-political party) of(I-political party) Ontario(I-political party) and(O) Green(B-political party) Party(I-political party) of(I-political party) British(I-political party) Columbia(I-political party) were(O) also(O) formed(O) that(O) year(O) )(O) .(O)"}}
{"id":"120","dataset":"crossner_politics","split":"train","label_list":["country","political party","politician","election","event","organization","person","location"],"instance":{"id":"120","words":["She","won","a","close","race","for","the","U.S.","Senate","in","1996","United","States","Senate","election","in","Louisiana",";","she","was","re-elected","by","increasing","margins","in","competitive","races","in","2002","United","States","Senate","election","in","Louisiana","and","2008","United","States","Senate","election","in","Louisiana",",","but","lost","in","2014","United","States","Senate","election","in","Louisiana","."],"labels":["O","O","O","O","O","O","O","B-organization","I-organization","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, politician, election, event, organization, person, location and O.\nSentence: She won a close race for the U.S. Senate in 1996 United States Senate election in Louisiana ; she was re-elected by increasing margins in competitive races in 2002 United States Senate election in Louisiana and 2008 United States Senate election in Louisiana , but lost in 2014 United States Senate election in Louisiana .","prompt_labels":"She(O) won(O) a(O) close(O) race(O) for(O) the(O) U.S.(B-organization) Senate(I-organization) in(O) 1996(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Louisiana(I-election) ;(O) she(O) was(O) re-elected(O) by(O) increasing(O) margins(O) in(O) competitive(O) races(O) in(O) 2002(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Louisiana(I-election) and(O) 2008(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Louisiana(I-election) ,(O) but(O) lost(O) in(O) 2014(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Louisiana(I-election) .(O)"}}
{"id":"121","dataset":"crossner_politics","split":"train","label_list":["election","person","country","event","location","organization","politician","political party"],"instance":{"id":"121","words":["Its","continued","use","by","the","Southern","Army","'s","post-war","veterans","groups",",","the","United","Confederate","Veterans","(","U.C.V.",")","and","the","later","Sons","of","Confederate","Veterans",",","(","S.C.V.",")",",","and","elements","of","the","design","by","related","similar","female","descendants","organizations","of","the","United","Daughters","of","the","Confederacy",",","(","U.D.C.",")",",","led","to","the","assumption","that","it","was",",","as","it","has","been","termed",",","the","soldier","'s","flag","or","the","Confederate","battle","flag","."],"labels":["O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, country, event, location, organization, politician, political party and O.\nSentence: Its continued use by the Southern Army 's post-war veterans groups , the United Confederate Veterans ( U.C.V. ) and the later Sons of Confederate Veterans , ( S.C.V. ) , and elements of the design by related similar female descendants organizations of the United Daughters of the Confederacy , ( U.D.C. ) , led to the assumption that it was , as it has been termed , the soldier 's flag or the Confederate battle flag .","prompt_labels":"Its(O) continued(O) use(O) by(O) the(O) Southern(B-organization) Army(I-organization) 's(O) post-war(O) veterans(O) groups(O) ,(O) the(O) United(B-organization) Confederate(I-organization) Veterans(I-organization) ((O) U.C.V.(B-organization) )(O) and(O) the(O) later(O) Sons(B-organization) of(I-organization) Confederate(I-organization) Veterans(I-organization) ,(O) ((O) S.C.V.(B-organization) )(O) ,(O) and(O) elements(O) of(O) the(O) design(O) by(O) related(O) similar(O) female(O) descendants(O) organizations(O) of(O) the(O) United(B-organization) Daughters(I-organization) of(I-organization) the(I-organization) Confederacy(I-organization) ,(O) ((O) U.D.C.(B-organization) )(O) ,(O) led(O) to(O) the(O) assumption(O) that(O) it(O) was(O) ,(O) as(O) it(O) has(O) been(O) termed(O) ,(O) the(O) soldier(O) 's(O) flag(O) or(O) the(O) Confederate(O) battle(O) flag(O) .(O)"}}
{"id":"122","dataset":"crossner_politics","split":"train","label_list":["organization","political party","person","election","country","politician","event","location"],"instance":{"id":"122","words":["Rowse","was","elected","a","Fellow","of","the","British","Academy","(","FBA",")",",","of","the","Royal","Historical","Society","(","FRHistS",")","and","of","the","Royal","Society","of","Literature","(","FRSL",")","."],"labels":["B-person","O","O","O","O","O","O","B-organization","I-organization","O","B-organization","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, person, election, country, politician, event, location and O.\nSentence: Rowse was elected a Fellow of the British Academy ( FBA ) , of the Royal Historical Society ( FRHistS ) and of the Royal Society of Literature ( FRSL ) .","prompt_labels":"Rowse(B-person) was(O) elected(O) a(O) Fellow(O) of(O) the(O) British(B-organization) Academy(I-organization) ((O) FBA(B-organization) )(O) ,(O) of(O) the(O) Royal(B-organization) Historical(I-organization) Society(I-organization) ((O) FRHistS(B-organization) )(O) and(O) of(O) the(O) Royal(B-organization) Society(I-organization) of(I-organization) Literature(I-organization) ((O) FRSL(B-organization) )(O) .(O)"}}
{"id":"123","dataset":"crossner_politics","split":"train","label_list":["organization","country","person","event","political party","location","politician","election"],"instance":{"id":"123","words":["Duplessis","returned","as","premier","in","the","1944","Quebec","general","election",",","and","held","power","without","serious","opposition","for","the","next","fifteen","years",",","until","his","death",",","winning","elections","in","1948","Quebec","general","election",",","1952","Quebec","general","election","and","1956","Quebec","general","election","."],"labels":["B-politician","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, person, event, political party, location, politician, election and O.\nSentence: Duplessis returned as premier in the 1944 Quebec general election , and held power without serious opposition for the next fifteen years , until his death , winning elections in 1948 Quebec general election , 1952 Quebec general election and 1956 Quebec general election .","prompt_labels":"Duplessis(B-politician) returned(O) as(O) premier(O) in(O) the(O) 1944(B-election) Quebec(I-election) general(I-election) election(I-election) ,(O) and(O) held(O) power(O) without(O) serious(O) opposition(O) for(O) the(O) next(O) fifteen(O) years(O) ,(O) until(O) his(O) death(O) ,(O) winning(O) elections(O) in(O) 1948(B-election) Quebec(I-election) general(I-election) election(I-election) ,(O) 1952(B-election) Quebec(I-election) general(I-election) election(I-election) and(O) 1956(B-election) Quebec(I-election) general(I-election) election(I-election) .(O)"}}
{"id":"124","dataset":"crossner_politics","split":"train","label_list":["organization","country","event","location","political party","politician","election","person"],"instance":{"id":"124","words":["He","survived","the","Progressive","Conservative","Party","of","Canada","ascendancy","and","was","re-elected","in","both","1957","Canadian","federal","election","and","1958","Canadian","federal","election","."],"labels":["O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, event, location, political party, politician, election, person and O.\nSentence: He survived the Progressive Conservative Party of Canada ascendancy and was re-elected in both 1957 Canadian federal election and 1958 Canadian federal election .","prompt_labels":"He(O) survived(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) ascendancy(O) and(O) was(O) re-elected(O) in(O) both(O) 1957(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 1958(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)"}}
{"id":"125","dataset":"crossner_politics","split":"train","label_list":["event","organization","country","election","location","person","political party","politician"],"instance":{"id":"125","words":["Human","rights","groups",",","including","Amnesty","International",",","Human","Rights","Watch","and","the","Center","for","Constitutional","Rights",",","and","U.S.","military","defense","lawyers","have","criticised","the","military","commissions","for","lacking","due","process","for","a","fair","trial","."],"labels":["O","O","O","O","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, country, election, location, person, political party, politician and O.\nSentence: Human rights groups , including Amnesty International , Human Rights Watch and the Center for Constitutional Rights , and U.S. military defense lawyers have criticised the military commissions for lacking due process for a fair trial .","prompt_labels":"Human(O) rights(O) groups(O) ,(O) including(O) Amnesty(B-organization) International(I-organization) ,(O) Human(B-organization) Rights(I-organization) Watch(I-organization) and(O) the(O) Center(B-organization) for(I-organization) Constitutional(I-organization) Rights(I-organization) ,(O) and(O) U.S.(B-country) military(O) defense(O) lawyers(O) have(O) criticised(O) the(O) military(O) commissions(O) for(O) lacking(O) due(O) process(O) for(O) a(O) fair(O) trial(O) .(O)"}}
{"id":"126","dataset":"crossner_politics","split":"train","label_list":["person","event","organization","political party","country","location","politician","election"],"instance":{"id":"126","words":["As","of","February","2012","the","political","composition","of","the","council","was",":","11","Democratic","Unionist","Party","(","DUP",")",",","6","Alliance","Party","of","Northern","Ireland",",","3","Ulster","Unionist","Party","(","UUP",")",",","2","Social","Democratic","and","Labour","Party","(","SDLP",")","and","1","Green","Party","in","Northern","Ireland","councillor","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, organization, political party, country, location, politician, election and O.\nSentence: As of February 2012 the political composition of the council was : 11 Democratic Unionist Party ( DUP ) , 6 Alliance Party of Northern Ireland , 3 Ulster Unionist Party ( UUP ) , 2 Social Democratic and Labour Party ( SDLP ) and 1 Green Party in Northern Ireland councillor .","prompt_labels":"As(O) of(O) February(O) 2012(O) the(O) political(O) composition(O) of(O) the(O) council(O) was(O) :(O) 11(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) ((O) DUP(B-political party) )(O) ,(O) 6(O) Alliance(B-political party) Party(I-political party) of(I-political party) Northern(I-political party) Ireland(I-political party) ,(O) 3(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) ((O) UUP(B-political party) )(O) ,(O) 2(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) ((O) SDLP(B-political party) )(O) and(O) 1(O) Green(B-political party) Party(I-political party) in(I-political party) Northern(I-political party) Ireland(I-political party) councillor(O) .(O)"}}
{"id":"127","dataset":"crossner_politics","split":"train","label_list":["politician","organization","political party","person","country","location","election","event"],"instance":{"id":"127","words":["Jeddah",",","Medina","and","Dubai","are","the","major","focus","city","for","the","airline",",","with","flights","from","Islamabad",",","Karachi",",","Lahore",",","Peshawar",",","Multan",",","Sialkot","and","Faisalabad","."],"labels":["B-location","O","B-location","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, political party, person, country, location, election, event and O.\nSentence: Jeddah , Medina and Dubai are the major focus city for the airline , with flights from Islamabad , Karachi , Lahore , Peshawar , Multan , Sialkot and Faisalabad .","prompt_labels":"Jeddah(B-location) ,(O) Medina(B-location) and(O) Dubai(B-location) are(O) the(O) major(O) focus(O) city(O) for(O) the(O) airline(O) ,(O) with(O) flights(O) from(O) Islamabad(B-location) ,(O) Karachi(B-location) ,(O) Lahore(B-location) ,(O) Peshawar(B-location) ,(O) Multan(B-location) ,(O) Sialkot(B-location) and(O) Faisalabad(B-location) .(O)"}}
{"id":"128","dataset":"crossner_politics","split":"train","label_list":["election","event","politician","person","country","political party","organization","location"],"instance":{"id":"128","words":["PIA","currently","offers","cargo","/","service","for","these","international","destinations","Abu","Dhabi",",","Bangkok",",","Barcelona",",","Birmingham",",","China","-","Beijing",",","Copenhagen",",","Dammam",",","Delhi",",","Dhaka",",","Doha",",","Dubai",",","Jeddah",",","Kabul",",","Kuala","Lumpur",",","London",",","Manchester",",","Medina",",","Milan",",","Muscat",",","Najaf",",","Oslo",",","Paris",",","Riyadh",",","Sharjah",",","Tokyo","-","Narita","and","Toronto","-","Canada","."],"labels":["B-organization","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","B-location","O","B-location","O","B-country","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","I-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-country","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, politician, person, country, political party, organization, location and O.\nSentence: PIA currently offers cargo / service for these international destinations Abu Dhabi , Bangkok , Barcelona , Birmingham , China - Beijing , Copenhagen , Dammam , Delhi , Dhaka , Doha , Dubai , Jeddah , Kabul , Kuala Lumpur , London , Manchester , Medina , Milan , Muscat , Najaf , Oslo , Paris , Riyadh , Sharjah , Tokyo - Narita and Toronto - Canada .","prompt_labels":"PIA(B-organization) currently(O) offers(O) cargo(O) /(O) service(O) for(O) these(O) international(O) destinations(O) Abu(B-location) Dhabi(I-location) ,(O) Bangkok(B-location) ,(O) Barcelona(B-location) ,(O) Birmingham(B-location) ,(O) China(B-country) -(O) Beijing(B-location) ,(O) Copenhagen(B-location) ,(O) Dammam(B-location) ,(O) Delhi(B-location) ,(O) Dhaka(B-location) ,(O) Doha(B-location) ,(O) Dubai(B-location) ,(O) Jeddah(B-location) ,(O) Kabul(B-location) ,(O) Kuala(B-location) Lumpur(I-location) ,(O) London(B-location) ,(O) Manchester(B-location) ,(O) Medina(B-location) ,(O) Milan(B-location) ,(O) Muscat(B-location) ,(O) Najaf(B-location) ,(O) Oslo(B-location) ,(O) Paris(B-location) ,(O) Riyadh(B-location) ,(O) Sharjah(B-location) ,(O) Tokyo(B-location) -(O) Narita(B-location) and(O) Toronto(B-location) -(O) Canada(B-country) .(O)"}}
{"id":"129","dataset":"crossner_politics","split":"train","label_list":["political party","organization","country","politician","event","person","election","location"],"instance":{"id":"129","words":["The","contemporary","Québec","Liberal","Party","is","a","broad-based","federalist","coalition","including","among","its","members","some","supporters","of","the","federal","Liberal","Party","of","Canada",",","New","Democratic","Party",",","and","Conservative","Party","of","Canada","."],"labels":["O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, country, politician, event, person, election, location and O.\nSentence: The contemporary Québec Liberal Party is a broad-based federalist coalition including among its members some supporters of the federal Liberal Party of Canada , New Democratic Party , and Conservative Party of Canada .","prompt_labels":"The(O) contemporary(O) Québec(O) Liberal(B-political party) Party(I-political party) is(O) a(O) broad-based(O) federalist(O) coalition(O) including(O) among(O) its(O) members(O) some(O) supporters(O) of(O) the(O) federal(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) New(B-political party) Democratic(I-political party) Party(I-political party) ,(O) and(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) .(O)"}}
{"id":"130","dataset":"crossner_politics","split":"train","label_list":["political party","election","event","person","politician","country","location","organization"],"instance":{"id":"130","words":["Đinđić","was","voted","out","of","his","position","as","Belgrade","mayor","by","the","Serbian","Renewal","Movement",",","Socialist","Party","of","Serbia","and","Serbian","Radical","Party","."],"labels":["B-politician","O","O","O","O","O","O","O","B-location","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, event, person, politician, country, location, organization and O.\nSentence: Đinđić was voted out of his position as Belgrade mayor by the Serbian Renewal Movement , Socialist Party of Serbia and Serbian Radical Party .","prompt_labels":"Đinđić(B-politician) was(O) voted(O) out(O) of(O) his(O) position(O) as(O) Belgrade(B-location) mayor(O) by(O) the(O) Serbian(B-political party) Renewal(I-political party) Movement(I-political party) ,(O) Socialist(B-political party) Party(I-political party) of(I-political party) Serbia(I-political party) and(O) Serbian(B-political party) Radical(I-political party) Party(I-political party) .(O)"}}
{"id":"131","dataset":"crossner_politics","split":"train","label_list":["location","politician","country","political party","organization","person","election","event"],"instance":{"id":"131","words":["The","war","led","to","British","Hong","Kong",",","the","first","of","many","unequal","treaties","between","Western","powers","and","China",",","and","the","opening","of","five","treaty","ports","(","Shanghai",",","Guangzhou",",","Fuzhou",",","Ningbo",",","and","Xiamen",")","opened","to","trade","with","the","West","-","eschewing","China","'s","traditional","protectionism","."],"labels":["O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","O","B-location","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, country, political party, organization, person, election, event and O.\nSentence: The war led to British Hong Kong , the first of many unequal treaties between Western powers and China , and the opening of five treaty ports ( Shanghai , Guangzhou , Fuzhou , Ningbo , and Xiamen ) opened to trade with the West - eschewing China 's traditional protectionism .","prompt_labels":"The(O) war(O) led(O) to(O) British(O) Hong(B-location) Kong(I-location) ,(O) the(O) first(O) of(O) many(O) unequal(O) treaties(O) between(O) Western(O) powers(O) and(O) China(B-country) ,(O) and(O) the(O) opening(O) of(O) five(O) treaty(O) ports(O) ((O) Shanghai(B-location) ,(O) Guangzhou(B-location) ,(O) Fuzhou(B-location) ,(O) Ningbo(B-location) ,(O) and(O) Xiamen(B-location) )(O) opened(O) to(O) trade(O) with(O) the(O) West(O) -(O) eschewing(O) China(B-country) 's(O) traditional(O) protectionism(O) .(O)"}}
{"id":"132","dataset":"crossner_politics","split":"train","label_list":["politician","location","person","country","organization","election","political party","event"],"instance":{"id":"132","words":["There","are","Lord","Edward","Streets","named","in","his","honour","in","many","places","in","Ireland",",","such","as","Dublin",",","Limerick",",","Sligo",",","Kilkenny",",","Ballina",",","Ballymote",",","and","Ballycullenbeg","in","County","Laois","."],"labels":["O","O","B-location","I-location","I-location","O","O","O","O","O","O","O","O","B-country","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","O","B-location","O","B-location","I-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, person, country, organization, election, political party, event and O.\nSentence: There are Lord Edward Streets named in his honour in many places in Ireland , such as Dublin , Limerick , Sligo , Kilkenny , Ballina , Ballymote , and Ballycullenbeg in County Laois .","prompt_labels":"There(O) are(O) Lord(B-location) Edward(I-location) Streets(I-location) named(O) in(O) his(O) honour(O) in(O) many(O) places(O) in(O) Ireland(B-country) ,(O) such(O) as(O) Dublin(B-location) ,(O) Limerick(B-location) ,(O) Sligo(B-location) ,(O) Kilkenny(B-location) ,(O) Ballina(B-location) ,(O) Ballymote(B-location) ,(O) and(O) Ballycullenbeg(B-location) in(O) County(B-location) Laois(I-location) .(O)"}}
{"id":"133","dataset":"crossner_politics","split":"train","label_list":["politician","political party","location","organization","event","election","country","person"],"instance":{"id":"133","words":["He","was","a","candidate","in","the","Polish","presidential","election","in","1995","Polish","presidential","election",",","2000","Polish","presidential","election",",","2005","Polish","presidential","election","and","2010","Polish","presidential","election","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, location, organization, event, election, country, person and O.\nSentence: He was a candidate in the Polish presidential election in 1995 Polish presidential election , 2000 Polish presidential election , 2005 Polish presidential election and 2010 Polish presidential election .","prompt_labels":"He(O) was(O) a(O) candidate(O) in(O) the(O) Polish(O) presidential(O) election(O) in(O) 1995(B-election) Polish(I-election) presidential(I-election) election(I-election) ,(O) 2000(B-election) Polish(I-election) presidential(I-election) election(I-election) ,(O) 2005(B-election) Polish(I-election) presidential(I-election) election(I-election) and(O) 2010(B-election) Polish(I-election) presidential(I-election) election(I-election) .(O)"}}
{"id":"134","dataset":"crossner_politics","split":"train","label_list":["election","politician","political party","person","event","organization","country","location"],"instance":{"id":"134","words":["Brooks","was","a","member","of","the","Masonic","Lodge",",","the","Shriners",",","the","Benevolent","and","Protective","Order","of","Elks",",","American","Legion",",","Veterans","of","Foreign","Wars",",","and","the","Kiwanis","International","."],"labels":["B-person","O","O","O","O","O","B-organization","I-organization","O","O","B-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, political party, person, event, organization, country, location and O.\nSentence: Brooks was a member of the Masonic Lodge , the Shriners , the Benevolent and Protective Order of Elks , American Legion , Veterans of Foreign Wars , and the Kiwanis International .","prompt_labels":"Brooks(B-person) was(O) a(O) member(O) of(O) the(O) Masonic(B-organization) Lodge(I-organization) ,(O) the(O) Shriners(B-organization) ,(O) the(O) Benevolent(B-organization) and(I-organization) Protective(I-organization) Order(I-organization) of(I-organization) Elks(I-organization) ,(O) American(B-organization) Legion(I-organization) ,(O) Veterans(B-organization) of(I-organization) Foreign(I-organization) Wars(I-organization) ,(O) and(O) the(O) Kiwanis(B-organization) International(I-organization) .(O)"}}
{"id":"135","dataset":"crossner_politics","split":"train","label_list":["political party","organization","person","politician","election","country","location","event"],"instance":{"id":"135","words":["Amnesty","International","and","Human","Rights","Watch","as","well","as","B","'Tselem","and","Yesh","Din","criticized","the","military","investigation","."],"labels":["B-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","O","B-organization","I-organization","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, person, politician, election, country, location, event and O.\nSentence: Amnesty International and Human Rights Watch as well as B 'Tselem and Yesh Din criticized the military investigation .","prompt_labels":"Amnesty(B-organization) International(I-organization) and(O) Human(B-organization) Rights(I-organization) Watch(I-organization) as(O) well(O) as(O) B(B-organization) 'Tselem(I-organization) and(O) Yesh(B-organization) Din(I-organization) criticized(O) the(O) military(O) investigation(O) .(O)"}}
{"id":"136","dataset":"crossner_politics","split":"train","label_list":["country","election","politician","political party","organization","person","location","event"],"instance":{"id":"136","words":["The","People","'s","Party",",","led","by","Aznar",",","won","the","most","parliamentary","seats","at","the","1996","Spanish","general","election",",","but","he","failed","to","obtain","a","majority","in","the","Congress","of","Deputies",",","which","forced","the","PP","to","seek","the","support","of","Basque","(","Basque","Nationalist","Party",")",",","Catalan","(","Convergence","and","Union",")","and","Canarian","(","Canarian","Coalition",")","regionalists","."],"labels":["B-political party","I-political party","I-political party","I-political party","O","O","O","B-politician","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, politician, political party, organization, person, location, event and O.\nSentence: The People 's Party , led by Aznar , won the most parliamentary seats at the 1996 Spanish general election , but he failed to obtain a majority in the Congress of Deputies , which forced the PP to seek the support of Basque ( Basque Nationalist Party ) , Catalan ( Convergence and Union ) and Canarian ( Canarian Coalition ) regionalists .","prompt_labels":"The(B-political party) People(I-political party) 's(I-political party) Party(I-political party) ,(O) led(O) by(O) Aznar(B-politician) ,(O) won(O) the(O) most(O) parliamentary(O) seats(O) at(O) the(O) 1996(B-election) Spanish(I-election) general(I-election) election(I-election) ,(O) but(O) he(O) failed(O) to(O) obtain(O) a(O) majority(O) in(O) the(O) Congress(B-organization) of(I-organization) Deputies(I-organization) ,(O) which(O) forced(O) the(O) PP(O) to(O) seek(O) the(O) support(O) of(O) Basque(O) ((O) Basque(B-political party) Nationalist(I-political party) Party(I-political party) )(O) ,(O) Catalan(O) ((O) Convergence(B-political party) and(I-political party) Union(I-political party) )(O) and(O) Canarian(O) ((O) Canarian(B-political party) Coalition(I-political party) )(O) regionalists(O) .(O)"}}
{"id":"137","dataset":"crossner_politics","split":"train","label_list":["location","political party","person","politician","country","election","event","organization"],"instance":{"id":"137","words":["The","movement","was","immediately","supported","by","Mariotto","Segni",",","leader","of","the","centrist","Segni","Pact",";","after","few","weeks","the","post-communist","Democratic","Party","of","the","Left","of","Massimo","D","'Alema",",","the","PPI","and","the","Federation","of","the","Greens","also","joined","the","Olive","Tree","coalition","."],"labels":["O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-politician","I-politician","I-politician","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-organization","I-organization","I-organization","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, person, politician, country, election, event, organization and O.\nSentence: The movement was immediately supported by Mariotto Segni , leader of the centrist Segni Pact ; after few weeks the post-communist Democratic Party of the Left of Massimo D 'Alema , the PPI and the Federation of the Greens also joined the Olive Tree coalition .","prompt_labels":"The(O) movement(O) was(O) immediately(O) supported(O) by(O) Mariotto(B-politician) Segni(I-politician) ,(O) leader(O) of(O) the(O) centrist(O) Segni(B-political party) Pact(I-political party) ;(O) after(O) few(O) weeks(O) the(O) post-communist(O) Democratic(B-political party) Party(I-political party) of(I-political party) the(I-political party) Left(I-political party) of(O) Massimo(B-politician) D(I-politician) 'Alema(I-politician) ,(O) the(O) PPI(O) and(O) the(O) Federation(B-political party) of(I-political party) the(I-political party) Greens(I-political party) also(O) joined(O) the(B-organization) Olive(I-organization) Tree(I-organization) coalition(O) .(O)"}}
{"id":"138","dataset":"crossner_politics","split":"train","label_list":["organization","political party","country","location","election","person","event","politician"],"instance":{"id":"138","words":["After","some","time",",","more","candidates","were","presented",",","like","Union","of","Democrats","for","Europe","leader","Clemente","Mastella",",","Italy","of","Values","leader","and","former","magistrate","Antonio","Di","Pietro",",","Federation","of","the","Greens","leader","Alfonso","Pecoraro","Scanio","and","others","few","minor","candidates","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-politician","I-politician","O","B-political party","I-political party","I-political party","O","O","O","O","B-politician","I-politician","I-politician","O","B-political party","I-political party","I-political party","I-political party","O","B-politician","I-politician","I-politician","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, country, location, election, person, event, politician and O.\nSentence: After some time , more candidates were presented , like Union of Democrats for Europe leader Clemente Mastella , Italy of Values leader and former magistrate Antonio Di Pietro , Federation of the Greens leader Alfonso Pecoraro Scanio and others few minor candidates .","prompt_labels":"After(O) some(O) time(O) ,(O) more(O) candidates(O) were(O) presented(O) ,(O) like(O) Union(B-political party) of(I-political party) Democrats(I-political party) for(I-political party) Europe(I-political party) leader(O) Clemente(B-politician) Mastella(I-politician) ,(O) Italy(B-political party) of(I-political party) Values(I-political party) leader(O) and(O) former(O) magistrate(O) Antonio(B-politician) Di(I-politician) Pietro(I-politician) ,(O) Federation(B-political party) of(I-political party) the(I-political party) Greens(I-political party) leader(O) Alfonso(B-politician) Pecoraro(I-politician) Scanio(I-politician) and(O) others(O) few(O) minor(O) candidates(O) .(O)"}}
{"id":"139","dataset":"crossner_politics","split":"train","label_list":["country","event","organization","political party","politician","person","election","location"],"instance":{"id":"139","words":["Excluding","the","Japanese","Communist","Party",",","the","coalition","was","backed","by","all","of","the","former","opposition","parties",",","which","included","the","newly","formed","JNP",",","the","Japan","Socialist","Party",",","the","Japan","Renewal","Party","(","Shinseito",")",",","Komeito",",","the","Democratic","Socialist","Party",",","the","Socialist","Democratic","Federation",",","and","the","New","Party","Sakigake",",","who","together","controlled","243","seats","in","the","House","of","Representatives","."],"labels":["O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","B-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, organization, political party, politician, person, election, location and O.\nSentence: Excluding the Japanese Communist Party , the coalition was backed by all of the former opposition parties , which included the newly formed JNP , the Japan Socialist Party , the Japan Renewal Party ( Shinseito ) , Komeito , the Democratic Socialist Party , the Socialist Democratic Federation , and the New Party Sakigake , who together controlled 243 seats in the House of Representatives .","prompt_labels":"Excluding(O) the(O) Japanese(B-political party) Communist(I-political party) Party(I-political party) ,(O) the(O) coalition(O) was(O) backed(O) by(O) all(O) of(O) the(O) former(O) opposition(O) parties(O) ,(O) which(O) included(O) the(O) newly(O) formed(O) JNP(B-political party) ,(O) the(O) Japan(B-political party) Socialist(I-political party) Party(I-political party) ,(O) the(O) Japan(B-political party) Renewal(I-political party) Party(I-political party) ((O) Shinseito(B-political party) )(O) ,(O) Komeito(B-political party) ,(O) the(O) Democratic(B-political party) Socialist(I-political party) Party(I-political party) ,(O) the(O) Socialist(B-political party) Democratic(I-political party) Federation(I-political party) ,(O) and(O) the(O) New(B-political party) Party(I-political party) Sakigake(I-political party) ,(O) who(O) together(O) controlled(O) 243(O) seats(O) in(O) the(O) House(O) of(O) Representatives(O) .(O)"}}
{"id":"140","dataset":"crossner_politics","split":"train","label_list":["country","political party","person","politician","organization","event","location","election"],"instance":{"id":"140","words":["The","Theosophical","Society","built","several","Buddhist","schools","in","Ceylon",",","most","notably","Ananda","College","in","1886",",","Dharmaraja","College","Kandy","in","1887",",","Maliyadeva","College","Kurunegala","in","1888",",","Mahinda","College","Galle","in","1892",",","Nalanda","College",",","Colombo","in","1925",",","Musaeus","Girls","College","in","Colombo","and","Dharmasoka","College","in","Ambalangoda","."],"labels":["O","B-organization","I-organization","O","O","O","O","O","B-location","O","O","O","B-organization","I-organization","O","O","O","B-organization","I-organization","B-location","O","O","O","B-organization","I-organization","B-location","O","O","O","B-organization","I-organization","B-location","O","O","O","B-organization","I-organization","O","B-location","O","O","O","B-organization","I-organization","I-organization","O","B-location","O","B-organization","I-organization","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, person, politician, organization, event, location, election and O.\nSentence: The Theosophical Society built several Buddhist schools in Ceylon , most notably Ananda College in 1886 , Dharmaraja College Kandy in 1887 , Maliyadeva College Kurunegala in 1888 , Mahinda College Galle in 1892 , Nalanda College , Colombo in 1925 , Musaeus Girls College in Colombo and Dharmasoka College in Ambalangoda .","prompt_labels":"The(O) Theosophical(B-organization) Society(I-organization) built(O) several(O) Buddhist(O) schools(O) in(O) Ceylon(B-location) ,(O) most(O) notably(O) Ananda(B-organization) College(I-organization) in(O) 1886(O) ,(O) Dharmaraja(B-organization) College(I-organization) Kandy(B-location) in(O) 1887(O) ,(O) Maliyadeva(B-organization) College(I-organization) Kurunegala(B-location) in(O) 1888(O) ,(O) Mahinda(B-organization) College(I-organization) Galle(B-location) in(O) 1892(O) ,(O) Nalanda(B-organization) College(I-organization) ,(O) Colombo(B-location) in(O) 1925(O) ,(O) Musaeus(B-organization) Girls(I-organization) College(I-organization) in(O) Colombo(B-location) and(O) Dharmasoka(B-organization) College(I-organization) in(O) Ambalangoda(B-location) .(O)"}}
{"id":"141","dataset":"crossner_politics","split":"train","label_list":["politician","event","country","organization","person","location","political party","election"],"instance":{"id":"141","words":["The","carrier","traces","its","roots","back","to",",","initially","serving","Athens",",","Cairo",",","London",",","Malta",",","Paris",",","Rome","and","Tunis","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-country","O","B-location","O","B-location","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, country, organization, person, location, political party, election and O.\nSentence: The carrier traces its roots back to , initially serving Athens , Cairo , London , Malta , Paris , Rome and Tunis .","prompt_labels":"The(O) carrier(O) traces(O) its(O) roots(O) back(O) to(O) ,(O) initially(O) serving(O) Athens(B-location) ,(O) Cairo(B-location) ,(O) London(B-location) ,(O) Malta(B-country) ,(O) Paris(B-location) ,(O) Rome(B-location) and(O) Tunis(B-location) .(O)"}}
{"id":"142","dataset":"crossner_politics","split":"train","label_list":["event","political party","organization","location","politician","person","election","country"],"instance":{"id":"142","words":["Maharani","Laxmi","Bai","Medical","College","in","Jhansi","are","named","after","her","."],"labels":["B-organization","I-organization","I-organization","I-organization","I-organization","O","B-location","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, organization, location, politician, person, election, country and O.\nSentence: Maharani Laxmi Bai Medical College in Jhansi are named after her .","prompt_labels":"Maharani(B-organization) Laxmi(I-organization) Bai(I-organization) Medical(I-organization) College(I-organization) in(O) Jhansi(B-location) are(O) named(O) after(O) her(O) .(O)"}}
{"id":"143","dataset":"crossner_politics","split":"train","label_list":["person","election","political party","location","event","country","politician","organization"],"instance":{"id":"143","words":["The","major","cities","of","Lan","Xang","were","located","in","Luang","Prabang",",","Vientiane","including","the","towns","in","Nong","Khai",",","Muang","Phuan","."],"labels":["O","O","O","O","B-country","I-country","O","O","O","B-location","I-location","O","B-location","O","O","O","O","B-location","I-location","O","B-location","I-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, political party, location, event, country, politician, organization and O.\nSentence: The major cities of Lan Xang were located in Luang Prabang , Vientiane including the towns in Nong Khai , Muang Phuan .","prompt_labels":"The(O) major(O) cities(O) of(O) Lan(B-country) Xang(I-country) were(O) located(O) in(O) Luang(B-location) Prabang(I-location) ,(O) Vientiane(B-location) including(O) the(O) towns(O) in(O) Nong(B-location) Khai(I-location) ,(O) Muang(B-location) Phuan(I-location) .(O)"}}
{"id":"144","dataset":"crossner_politics","split":"train","label_list":["organization","country","election","political party","location","event","politician","person"],"instance":{"id":"144","words":["In","the","beginning",",","Anawrahta","'s","principality","was","a","small","area","-","barely","200","miles","north","to","south","and","about","80","miles","from","east","to","west",",","comprising","roughly","the","present","districts","of","Mandalay",",","Meiktila",",","Myingyan",",","Kyaukse",",","Yamethin",",","Magwe",",","Sagaing","and","Katha","east","of","the","Irrawaddy",",","and","the","riverine","portions","of","Minbu","and","Pakkoku","."],"labels":["O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","O","O","B-location","O","O","O","O","O","O","B-location","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, election, political party, location, event, politician, person and O.\nSentence: In the beginning , Anawrahta 's principality was a small area - barely 200 miles north to south and about 80 miles from east to west , comprising roughly the present districts of Mandalay , Meiktila , Myingyan , Kyaukse , Yamethin , Magwe , Sagaing and Katha east of the Irrawaddy , and the riverine portions of Minbu and Pakkoku .","prompt_labels":"In(O) the(O) beginning(O) ,(O) Anawrahta(B-person) 's(O) principality(O) was(O) a(O) small(O) area(O) -(O) barely(O) 200(O) miles(O) north(O) to(O) south(O) and(O) about(O) 80(O) miles(O) from(O) east(O) to(O) west(O) ,(O) comprising(O) roughly(O) the(O) present(O) districts(O) of(O) Mandalay(B-location) ,(O) Meiktila(B-location) ,(O) Myingyan(B-location) ,(O) Kyaukse(B-location) ,(O) Yamethin(B-location) ,(O) Magwe(B-location) ,(O) Sagaing(B-location) and(O) Katha(B-location) east(O) of(O) the(O) Irrawaddy(B-location) ,(O) and(O) the(O) riverine(O) portions(O) of(O) Minbu(B-location) and(O) Pakkoku(B-location) .(O)"}}
{"id":"145","dataset":"crossner_politics","split":"train","label_list":["country","political party","location","organization","election","event","person","politician"],"instance":{"id":"145","words":["This","tends","to","lead","to","the","chamber","being","dominated","by","two","major","parties",",","the","Liberal","Party","of","Australia","/","National","Party","of","Australia","Coalition","and","the","Australian","Labor","Party","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, location, organization, election, event, person, politician and O.\nSentence: This tends to lead to the chamber being dominated by two major parties , the Liberal Party of Australia / National Party of Australia Coalition and the Australian Labor Party .","prompt_labels":"This(O) tends(O) to(O) lead(O) to(O) the(O) chamber(O) being(O) dominated(O) by(O) two(O) major(O) parties(O) ,(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) /(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) Coalition(O) and(O) the(O) Australian(B-political party) Labor(I-political party) Party(I-political party) .(O)"}}
{"id":"146","dataset":"crossner_politics","split":"train","label_list":["organization","location","person","political party","country","election","politician","event"],"instance":{"id":"146","words":["Many","of","the","magazine","'s","articles","were","written","by","members","of","conservative","think","tanks","located","in","Washington",",","including","the","American","Enterprise","Institute",",","the","Ethics","and","Public","Policy","Center",",","the","Foundation","for","Defense","of","Democracies",",","the","Hudson","Institute",",","and","the","Foreign","Policy","Initiative","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, person, political party, country, election, politician, event and O.\nSentence: Many of the magazine 's articles were written by members of conservative think tanks located in Washington , including the American Enterprise Institute , the Ethics and Public Policy Center , the Foundation for Defense of Democracies , the Hudson Institute , and the Foreign Policy Initiative .","prompt_labels":"Many(O) of(O) the(O) magazine(O) 's(O) articles(O) were(O) written(O) by(O) members(O) of(O) conservative(O) think(O) tanks(O) located(O) in(O) Washington(B-location) ,(O) including(O) the(O) American(B-organization) Enterprise(I-organization) Institute(I-organization) ,(O) the(O) Ethics(B-organization) and(I-organization) Public(I-organization) Policy(I-organization) Center(I-organization) ,(O) the(O) Foundation(B-organization) for(I-organization) Defense(I-organization) of(I-organization) Democracies(I-organization) ,(O) the(O) Hudson(B-organization) Institute(I-organization) ,(O) and(O) the(O) Foreign(B-organization) Policy(I-organization) Initiative(I-organization) .(O)"}}
{"id":"147","dataset":"crossner_politics","split":"train","label_list":["location","country","person","politician","election","political party","organization","event"],"instance":{"id":"147","words":["At","the","turn","of","the","19th","century",",","nine","out","of","ten","workers","were","engaged","in","harbour","activities",",","but","as","the","century","progressed","ships","became","too","large","for","the","shallow","harbour","and","the","port","lost","business","to","the","deep","water","ports","at","Liverpool",",","Southampton","and","Plymouth","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, person, politician, election, political party, organization, event and O.\nSentence: At the turn of the 19th century , nine out of ten workers were engaged in harbour activities , but as the century progressed ships became too large for the shallow harbour and the port lost business to the deep water ports at Liverpool , Southampton and Plymouth .","prompt_labels":"At(O) the(O) turn(O) of(O) the(O) 19th(O) century(O) ,(O) nine(O) out(O) of(O) ten(O) workers(O) were(O) engaged(O) in(O) harbour(O) activities(O) ,(O) but(O) as(O) the(O) century(O) progressed(O) ships(O) became(O) too(O) large(O) for(O) the(O) shallow(O) harbour(O) and(O) the(O) port(O) lost(O) business(O) to(O) the(O) deep(O) water(O) ports(O) at(O) Liverpool(B-location) ,(O) Southampton(B-location) and(O) Plymouth(B-location) .(O)"}}
{"id":"148","dataset":"crossner_politics","split":"train","label_list":["politician","location","political party","election","organization","event","country","person"],"instance":{"id":"148","words":["Whereas","most","leaders","had","come","from","or","identified","with","the","independence","movement","Sinn","Féin","(","in","its","1917-22","phase",")",",","Bruton","identified","more","with","the","more","moderate","Irish","Parliamentary","Party","(","IPP",")","tradition","that","Sinn","Féin","had","eclipsed","at","the","1918","Irish","general","election","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","O","O","O","O","B-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, political party, election, organization, event, country, person and O.\nSentence: Whereas most leaders had come from or identified with the independence movement Sinn Féin ( in its 1917-22 phase ) , Bruton identified more with the more moderate Irish Parliamentary Party ( IPP ) tradition that Sinn Féin had eclipsed at the 1918 Irish general election .","prompt_labels":"Whereas(O) most(O) leaders(O) had(O) come(O) from(O) or(O) identified(O) with(O) the(O) independence(O) movement(O) Sinn(B-political party) Féin(I-political party) ((O) in(O) its(O) 1917-22(O) phase(O) )(O) ,(O) Bruton(B-location) identified(O) more(O) with(O) the(O) more(O) moderate(O) Irish(B-political party) Parliamentary(I-political party) Party(I-political party) ((O) IPP(B-political party) )(O) tradition(O) that(O) Sinn(B-political party) Féin(I-political party) had(O) eclipsed(O) at(O) the(O) 1918(B-election) Irish(I-election) general(I-election) election(I-election) .(O)"}}
{"id":"149","dataset":"crossner_politics","split":"train","label_list":["event","organization","politician","country","person","election","location","political party"],"instance":{"id":"149","words":["By","the","time","of","the","1997","Irish","general","election","Sinn","Féin","stated","that","they","would","prefer","a","Fianna","Fáil","led","government","and","the","IRA","resumed","their","ceasefire","soon","after","Fine","Gael","lost","the","1997","Irish","general","election","."],"labels":["O","O","O","O","O","B-election","I-election","I-election","I-election","B-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","B-organization","O","O","O","O","O","B-political party","I-political party","O","O","B-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, politician, country, person, election, location, political party and O.\nSentence: By the time of the 1997 Irish general election Sinn Féin stated that they would prefer a Fianna Fáil led government and the IRA resumed their ceasefire soon after Fine Gael lost the 1997 Irish general election .","prompt_labels":"By(O) the(O) time(O) of(O) the(O) 1997(B-election) Irish(I-election) general(I-election) election(I-election) Sinn(B-political party) Féin(I-political party) stated(O) that(O) they(O) would(O) prefer(O) a(O) Fianna(B-political party) Fáil(I-political party) led(O) government(O) and(O) the(O) IRA(B-organization) resumed(O) their(O) ceasefire(O) soon(O) after(O) Fine(B-political party) Gael(I-political party) lost(O) the(O) 1997(B-election) Irish(I-election) general(I-election) election(I-election) .(O)"}}
{"id":"150","dataset":"crossner_politics","split":"train","label_list":["election","event","location","politician","organization","person","country","political party"],"instance":{"id":"150","words":["Within","this","district","were","Barnsley",",","Batley",",","Bradford",",","Brighouse",",","Dewsbury",",","Doncaster",",","Halifax",",","Huddersfield",",","Keighley",",","Leeds",",","Morley",",","Ossett",",","Pontefract",",","Pudsey",",","Rotherham",",","Sheffield",",","Todmorden","(","partly","in","Lancashire","until","1888",",","when","fully","incorporated","into","Yorkshire",")","and","Wakefield","."],"labels":["O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","O","O","B-location","O","O","O","O","O","O","O","B-location","O","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, location, politician, organization, person, country, political party and O.\nSentence: Within this district were Barnsley , Batley , Bradford , Brighouse , Dewsbury , Doncaster , Halifax , Huddersfield , Keighley , Leeds , Morley , Ossett , Pontefract , Pudsey , Rotherham , Sheffield , Todmorden ( partly in Lancashire until 1888 , when fully incorporated into Yorkshire ) and Wakefield .","prompt_labels":"Within(O) this(O) district(O) were(O) Barnsley(B-location) ,(O) Batley(B-location) ,(O) Bradford(B-location) ,(O) Brighouse(B-location) ,(O) Dewsbury(B-location) ,(O) Doncaster(B-location) ,(O) Halifax(B-location) ,(O) Huddersfield(B-location) ,(O) Keighley(B-location) ,(O) Leeds(B-location) ,(O) Morley(B-location) ,(O) Ossett(B-location) ,(O) Pontefract(B-location) ,(O) Pudsey(B-location) ,(O) Rotherham(B-location) ,(O) Sheffield(B-location) ,(O) Todmorden(B-location) ((O) partly(O) in(O) Lancashire(B-location) until(O) 1888(O) ,(O) when(O) fully(O) incorporated(O) into(O) Yorkshire(B-location) )(O) and(O) Wakefield(B-location) .(O)"}}
{"id":"151","dataset":"crossner_politics","split":"train","label_list":["person","political party","country","organization","politician","event","election","location"],"instance":{"id":"151","words":["This","promise","was","seen","as","key","to","his","victory","in","many","ridings",",","such","as","those","in","the","suburbs","around","Longueuil","and","Quebec","City","and","the","continued","support","of","the","Anglophone","community","in","the","West","Island","of","Montreal","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","I-location","O","O","O","O","O","O","B-organization","O","O","O","O","O","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, country, organization, politician, event, election, location and O.\nSentence: This promise was seen as key to his victory in many ridings , such as those in the suburbs around Longueuil and Quebec City and the continued support of the Anglophone community in the West Island of Montreal .","prompt_labels":"This(O) promise(O) was(O) seen(O) as(O) key(O) to(O) his(O) victory(O) in(O) many(O) ridings(O) ,(O) such(O) as(O) those(O) in(O) the(O) suburbs(O) around(O) Longueuil(B-location) and(O) Quebec(B-location) City(I-location) and(O) the(O) continued(O) support(O) of(O) the(O) Anglophone(B-organization) community(O) in(O) the(O) West(O) Island(O) of(O) Montreal(B-location) .(O)"}}
{"id":"152","dataset":"crossner_politics","split":"train","label_list":["person","election","event","politician","location","country","political party","organization"],"instance":{"id":"152","words":["Lugar","was","reelected","in","1982","United","States","Senate","election","in","Indiana",",","1988","United","States","Senate","election","in","Indiana",",","1994","United","States","Senate","election","in","Indiana",",","2000","United","States","Senate","election","in","Indiana",",","and","2006","United","States","Senate","election","in","Indiana","."],"labels":["B-politician","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, event, politician, location, country, political party, organization and O.\nSentence: Lugar was reelected in 1982 United States Senate election in Indiana , 1988 United States Senate election in Indiana , 1994 United States Senate election in Indiana , 2000 United States Senate election in Indiana , and 2006 United States Senate election in Indiana .","prompt_labels":"Lugar(B-politician) was(O) reelected(O) in(O) 1982(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Indiana(I-election) ,(O) 1988(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Indiana(I-election) ,(O) 1994(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Indiana(I-election) ,(O) 2000(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Indiana(I-election) ,(O) and(O) 2006(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Indiana(I-election) .(O)"}}
{"id":"153","dataset":"crossner_politics","split":"train","label_list":["political party","event","person","election","location","country","organization","politician"],"instance":{"id":"153","words":["Formed","by","the","Social","Democratic","Party","(","SDP",")","and","the","Liberal","Party",",","the","SDP-Liberal","Alliance","was","established","in","1981",",","contesting","the","1983","United","Kingdom","general","election",",","1984","European","Parliament","election","in","the","United","Kingdom","and","1987","United","Kingdom","general","election","."],"labels":["O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","O","O","B-political party","I-political party","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, person, election, location, country, organization, politician and O.\nSentence: Formed by the Social Democratic Party ( SDP ) and the Liberal Party , the SDP-Liberal Alliance was established in 1981 , contesting the 1983 United Kingdom general election , 1984 European Parliament election in the United Kingdom and 1987 United Kingdom general election .","prompt_labels":"Formed(O) by(O) the(O) Social(B-political party) Democratic(I-political party) Party(I-political party) ((O) SDP(B-political party) )(O) and(O) the(O) Liberal(B-political party) Party(I-political party) ,(O) the(O) SDP-Liberal(B-political party) Alliance(I-political party) was(O) established(O) in(O) 1981(O) ,(O) contesting(O) the(O) 1983(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 1984(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) the(I-election) United(I-election) Kingdom(I-election) and(O) 1987(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)"}}
{"id":"154","dataset":"crossner_politics","split":"train","label_list":["election","event","location","country","person","politician","political party","organization"],"instance":{"id":"154","words":["In","1981",",","there","were","riots","in","London","'","s","Brixton",",","Birmingham","'","s","Handsworth",",","Leeds","'","Chapeltown",",","Liverpool","'","s","Toxteth","and","Manchester","'","s","Moss","Side","."],"labels":["O","O","O","O","O","O","O","B-location","O","O","B-location","O","B-location","O","O","B-location","O","B-location","O","B-location","O","B-location","O","O","B-location","O","B-location","O","O","B-location","I-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, location, country, person, politician, political party, organization and O.\nSentence: In 1981 , there were riots in London ' s Brixton , Birmingham ' s Handsworth , Leeds ' Chapeltown , Liverpool ' s Toxteth and Manchester ' s Moss Side .","prompt_labels":"In(O) 1981(O) ,(O) there(O) were(O) riots(O) in(O) London(B-location) '(O) s(O) Brixton(B-location) ,(O) Birmingham(B-location) '(O) s(O) Handsworth(B-location) ,(O) Leeds(B-location) '(O) Chapeltown(B-location) ,(O) Liverpool(B-location) '(O) s(O) Toxteth(B-location) and(O) Manchester(B-location) '(O) s(O) Moss(B-location) Side(I-location) .(O)"}}
{"id":"155","dataset":"crossner_politics","split":"train","label_list":["election","political party","event","organization","country","location","politician","person"],"instance":{"id":"155","words":["Šetalište","Lazaro","Kardenasa","(","Lázaro","Cárdenas","promenade",")","in","Belgrade",",","Serbia",",","is","also","named","after","him",",","as","is","a","street","in","Barcelona",",","Spain",",","and","a","monument","in","a","park","in","Madrid","dedicated","to","his","memory","for","his","role","in","admitting","defeated","Spanish","Republicans","to","Mexico","after","the","Civil","War","in","that","country","."],"labels":["B-location","I-location","I-location","O","B-location","I-location","I-location","O","O","B-location","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-country","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","B-event","I-event","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, event, organization, country, location, politician, person and O.\nSentence: Šetalište Lazaro Kardenasa ( Lázaro Cárdenas promenade ) in Belgrade , Serbia , is also named after him , as is a street in Barcelona , Spain , and a monument in a park in Madrid dedicated to his memory for his role in admitting defeated Spanish Republicans to Mexico after the Civil War in that country .","prompt_labels":"Šetalište(B-location) Lazaro(I-location) Kardenasa(I-location) ((O) Lázaro(B-location) Cárdenas(I-location) promenade(I-location) )(O) in(O) Belgrade(B-location) ,(O) Serbia(B-country) ,(O) is(O) also(O) named(O) after(O) him(O) ,(O) as(O) is(O) a(O) street(O) in(O) Barcelona(B-location) ,(O) Spain(B-country) ,(O) and(O) a(O) monument(O) in(O) a(O) park(O) in(O) Madrid(B-location) dedicated(O) to(O) his(O) memory(O) for(O) his(O) role(O) in(O) admitting(O) defeated(O) Spanish(O) Republicans(O) to(O) Mexico(B-country) after(O) the(O) Civil(B-event) War(I-event) in(O) that(O) country(O) .(O)"}}
{"id":"156","dataset":"crossner_politics","split":"train","label_list":["country","election","location","event","politician","organization","person","political party"],"instance":{"id":"156","words":["On","5","April","1960",",","BEA","introduced","de","Havilland","Comet","4B","aircraft","on","the","Nicosia",",","Athens",",","Rome",",","and","London","routes","."],"labels":["O","O","O","O","O","B-country","O","B-organization","I-organization","I-organization","O","O","O","O","B-location","O","B-location","O","B-location","O","O","B-location","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, location, event, politician, organization, person, political party and O.\nSentence: On 5 April 1960 , BEA introduced de Havilland Comet 4B aircraft on the Nicosia , Athens , Rome , and London routes .","prompt_labels":"On(O) 5(O) April(O) 1960(O) ,(O) BEA(B-country) introduced(O) de(B-organization) Havilland(I-organization) Comet(I-organization) 4B(O) aircraft(O) on(O) the(O) Nicosia(B-location) ,(O) Athens(B-location) ,(O) Rome(B-location) ,(O) and(O) London(B-location) routes(O) .(O)"}}
{"id":"157","dataset":"crossner_politics","split":"train","label_list":["country","person","political party","organization","event","election","location","politician"],"instance":{"id":"157","words":["Re-elected","as","MP","for","that","riding","in","the","1963","Canadian","federal","election","and","1965","Canadian","federal","election",",","Douglas","lost","the","redistricted","seat","of","Burnaby","-","Seymour","in","the","1968","Canadian","federal","election","."],"labels":["O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-politician","O","O","O","O","O","B-location","I-location","I-location","O","O","B-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, political party, organization, event, election, location, politician and O.\nSentence: Re-elected as MP for that riding in the 1963 Canadian federal election and 1965 Canadian federal election , Douglas lost the redistricted seat of Burnaby - Seymour in the 1968 Canadian federal election .","prompt_labels":"Re-elected(O) as(O) MP(O) for(O) that(O) riding(O) in(O) the(O) 1963(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 1965(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) Douglas(B-politician) lost(O) the(O) redistricted(O) seat(O) of(O) Burnaby(B-location) -(I-location) Seymour(I-location) in(O) the(O) 1968(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)"}}
{"id":"158","dataset":"crossner_politics","split":"train","label_list":["election","organization","person","country","political party","politician","location","event"],"instance":{"id":"158","words":["After","two","failed","attempts",",","New","Democratic","Party","candidate","Olivia","Chow","(","wife","of","NDP","leader","Jack","Layton",")",",","was","elected","in","the","2006","Canadian","federal","election",",","representing","the","riding","of","Trinity","-","Spadina",",","and","the","Bloc","Québécois","had","an","ethnic","Chinese","candidate",",","May","Chiu",",","running","in","the","riding","of","LaSalle","-","Émard","against","Liberal","Party","of","Canada","leader","Paul","Martin","during","the","2006","election","."],"labels":["O","O","O","O","O","B-political party","I-political party","I-political party","O","B-politician","I-politician","O","O","O","B-political party","O","B-politician","I-politician","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","B-location","I-location","I-location","O","O","O","B-political party","I-political party","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","B-location","I-location","I-location","O","B-political party","I-political party","I-political party","I-political party","O","B-politician","I-politician","O","O","B-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, person, country, political party, politician, location, event and O.\nSentence: After two failed attempts , New Democratic Party candidate Olivia Chow ( wife of NDP leader Jack Layton ) , was elected in the 2006 Canadian federal election , representing the riding of Trinity - Spadina , and the Bloc Québécois had an ethnic Chinese candidate , May Chiu , running in the riding of LaSalle - Émard against Liberal Party of Canada leader Paul Martin during the 2006 election .","prompt_labels":"After(O) two(O) failed(O) attempts(O) ,(O) New(B-political party) Democratic(I-political party) Party(I-political party) candidate(O) Olivia(B-politician) Chow(I-politician) ((O) wife(O) of(O) NDP(B-political party) leader(O) Jack(B-politician) Layton(I-politician) )(O) ,(O) was(O) elected(O) in(O) the(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) representing(O) the(O) riding(O) of(O) Trinity(B-location) -(I-location) Spadina(I-location) ,(O) and(O) the(O) Bloc(B-political party) Québécois(I-political party) had(O) an(O) ethnic(O) Chinese(O) candidate(O) ,(O) May(B-politician) Chiu(I-politician) ,(O) running(O) in(O) the(O) riding(O) of(O) LaSalle(B-location) -(I-location) Émard(I-location) against(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) leader(O) Paul(B-politician) Martin(I-politician) during(O) the(O) 2006(B-election) election(I-election) .(O)"}}
{"id":"159","dataset":"crossner_politics","split":"train","label_list":["election","location","political party","person","politician","event","country","organization"],"instance":{"id":"159","words":["He","was","the","former","president","of","the","American","Political","Science","Association","and","a","member","of","the","American","Academy","of","Arts","and","Sciences",",","American","Philosophical","Society","and","Human","Rights","Foundation","."],"labels":["O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, political party, person, politician, event, country, organization and O.\nSentence: He was the former president of the American Political Science Association and a member of the American Academy of Arts and Sciences , American Philosophical Society and Human Rights Foundation .","prompt_labels":"He(O) was(O) the(O) former(O) president(O) of(O) the(O) American(B-organization) Political(I-organization) Science(I-organization) Association(I-organization) and(O) a(O) member(O) of(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) ,(O) American(B-organization) Philosophical(I-organization) Society(I-organization) and(O) Human(B-organization) Rights(I-organization) Foundation(I-organization) .(O)"}}
{"id":"160","dataset":"crossner_politics","split":"train","label_list":["country","location","politician","political party","organization","event","election","person"],"instance":{"id":"160","words":["Marxism-Leninism","remains","the","ideology","of","several","communist","states","around","the","world","and","the","official","ideology","of","the","ruling","parties","of","Communist","Party","of","China",",","Communist","Party","of","Cuba",",","Laos","and","Communist","Party","of","Vietnam","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-country","O","B-political party","I-political party","I-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, politician, political party, organization, event, election, person and O.\nSentence: Marxism-Leninism remains the ideology of several communist states around the world and the official ideology of the ruling parties of Communist Party of China , Communist Party of Cuba , Laos and Communist Party of Vietnam .","prompt_labels":"Marxism-Leninism(O) remains(O) the(O) ideology(O) of(O) several(O) communist(O) states(O) around(O) the(O) world(O) and(O) the(O) official(O) ideology(O) of(O) the(O) ruling(O) parties(O) of(O) Communist(B-political party) Party(I-political party) of(I-political party) China(I-political party) ,(O) Communist(B-political party) Party(I-political party) of(I-political party) Cuba(I-political party) ,(O) Laos(B-country) and(O) Communist(B-political party) Party(I-political party) of(I-political party) Vietnam(I-political party) .(O)"}}
{"id":"161","dataset":"crossner_politics","split":"train","label_list":["country","event","political party","election","politician","location","organization","person"],"instance":{"id":"161","words":["The","party","was","directly","succeeded","by","the","Canadian","Alliance","in","2000",",","which","merged","with","the","Progressive","Conservative","Party","of","Canada","in","2003","to","form","the","modern-day","Conservative","Party","of","Canada","."],"labels":["O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, political party, election, politician, location, organization, person and O.\nSentence: The party was directly succeeded by the Canadian Alliance in 2000 , which merged with the Progressive Conservative Party of Canada in 2003 to form the modern-day Conservative Party of Canada .","prompt_labels":"The(O) party(O) was(O) directly(O) succeeded(O) by(O) the(O) Canadian(B-political party) Alliance(I-political party) in(O) 2000(O) ,(O) which(O) merged(O) with(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) in(O) 2003(O) to(O) form(O) the(O) modern-day(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) .(O)"}}
{"id":"162","dataset":"crossner_politics","split":"train","label_list":["organization","event","location","political party","politician","country","person","election"],"instance":{"id":"162","words":["This","resulted","in","significant","breakthroughs","at","the","2013","United","Kingdom","local","elections",",","2014","European","Parliament","election","in","the","United","Kingdom",",","and","2015","United","Kingdom","general","election","."],"labels":["O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, location, political party, politician, country, person, election and O.\nSentence: This resulted in significant breakthroughs at the 2013 United Kingdom local elections , 2014 European Parliament election in the United Kingdom , and 2015 United Kingdom general election .","prompt_labels":"This(O) resulted(O) in(O) significant(O) breakthroughs(O) at(O) the(O) 2013(B-election) United(I-election) Kingdom(I-election) local(I-election) elections(I-election) ,(O) 2014(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) the(I-election) United(I-election) Kingdom(I-election) ,(O) and(O) 2015(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)"}}
{"id":"163","dataset":"crossner_politics","split":"train","label_list":["country","location","political party","person","election","politician","event","organization"],"instance":{"id":"163","words":["Following","the","2004","European","Parliament","election","in","the","United","Kingdom",",","37","MEPs","from","the","UK",",","2004","European","Parliament","election","in","Poland",",","2004","European","Parliament","election","in","Denmark","and","Sweden","founded","a","new","European","Parliamentary","group","called","Independence","and","Democracy","as","a","direct","successor","to","the","EDD","group","."],"labels":["O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","B-country","O","B-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","O","B-country","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, political party, person, election, politician, event, organization and O.\nSentence: Following the 2004 European Parliament election in the United Kingdom , 37 MEPs from the UK , 2004 European Parliament election in Poland , 2004 European Parliament election in Denmark and Sweden founded a new European Parliamentary group called Independence and Democracy as a direct successor to the EDD group .","prompt_labels":"Following(O) the(O) 2004(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) the(I-election) United(I-election) Kingdom(I-election) ,(O) 37(O) MEPs(O) from(O) the(O) UK(B-country) ,(O) 2004(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) Poland(I-election) ,(O) 2004(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) Denmark(I-election) and(O) Sweden(B-country) founded(O) a(O) new(O) European(O) Parliamentary(O) group(O) called(O) Independence(B-organization) and(I-organization) Democracy(I-organization) as(O) a(O) direct(O) successor(O) to(O) the(O) EDD(B-organization) group(O) .(O)"}}
{"id":"164","dataset":"crossner_politics","split":"train","label_list":["event","political party","election","country","organization","politician","location","person"],"instance":{"id":"164","words":["During","his","time","as","governor",",","Thompson","served","as","chairman","of","the","National","Governors","Association","and","the","Education","Commission","of","the","States",",","in","addition","to","the","Council","of","State","Governments",",","the","Republican","Governors","Association",",","the","Council","of","Great","Lakes","Governors",",","and","the","Midwestern","Governors","Association","."],"labels":["O","O","O","O","O","O","B-politician","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-political party","I-political party","I-political party","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, election, country, organization, politician, location, person and O.\nSentence: During his time as governor , Thompson served as chairman of the National Governors Association and the Education Commission of the States , in addition to the Council of State Governments , the Republican Governors Association , the Council of Great Lakes Governors , and the Midwestern Governors Association .","prompt_labels":"During(O) his(O) time(O) as(O) governor(O) ,(O) Thompson(B-politician) served(O) as(O) chairman(O) of(O) the(O) National(B-political party) Governors(I-political party) Association(I-political party) and(O) the(O) Education(B-organization) Commission(I-organization) of(I-organization) the(I-organization) States(I-organization) ,(O) in(O) addition(O) to(O) the(O) Council(B-organization) of(I-organization) State(I-organization) Governments(I-organization) ,(O) the(O) Republican(B-political party) Governors(I-political party) Association(I-political party) ,(O) the(O) Council(B-organization) of(I-organization) Great(I-organization) Lakes(I-organization) Governors(I-organization) ,(O) and(O) the(O) Midwestern(B-organization) Governors(I-organization) Association(I-organization) .(O)"}}
{"id":"165","dataset":"crossner_politics","split":"train","label_list":["location","organization","political party","politician","event","country","election","person"],"instance":{"id":"165","words":["Nha","Trang",",","headquarters","of","the","U.S.","I","Field","Force",",","was","the","first","to","be","hit",",","followed","shortly","by","Ban","Mê","Thuột",",","Kon","Tum",",","Hội","An",",","Tuy","Hòa",",","Da","Nang",",","Qui","Nhơn",",","and","Pleiku","."],"labels":["B-location","I-location","O","O","O","O","B-country","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","O","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, political party, politician, event, country, election, person and O.\nSentence: Nha Trang , headquarters of the U.S. I Field Force , was the first to be hit , followed shortly by Ban Mê Thuột , Kon Tum , Hội An , Tuy Hòa , Da Nang , Qui Nhơn , and Pleiku .","prompt_labels":"Nha(B-location) Trang(I-location) ,(O) headquarters(O) of(O) the(O) U.S.(B-country) I(B-organization) Field(I-organization) Force(I-organization) ,(O) was(O) the(O) first(O) to(O) be(O) hit(O) ,(O) followed(O) shortly(O) by(O) Ban(B-location) Mê(I-location) Thuột(I-location) ,(O) Kon(B-location) Tum(I-location) ,(O) Hội(B-location) An(I-location) ,(O) Tuy(B-location) Hòa(I-location) ,(O) Da(B-location) Nang(I-location) ,(O) Qui(B-location) Nhơn(I-location) ,(O) and(O) Pleiku(B-location) .(O)"}}
{"id":"166","dataset":"crossner_politics","split":"train","label_list":["person","election","politician","event","organization","location","political party","country"],"instance":{"id":"166","words":["At","3",":","0","on","31","January","PAVN","/","VC","forces","attacked","Saigon",",","Cholon",",","and","Gia","Định","in","the","Capital","Military","District",";","Quảng","Trị","(","again",")",",","Huế",",","Quảng","Tín",",","Tam","Kỳ","and","Quảng","Ngãi","as","well","as","U.S.","bases","at","Phú","Bài","and","Chu","Lai","in","I","Corps",";","Phan","Thiết",",","Tuy","Hòa","and","U.S.","installations","at","Bong","Son","and","An","Khê","in","II","Corps",";","and","Cần","Thơ","and","Vĩnh","Long","in","IV","Corps","."],"labels":["O","O","O","O","O","O","O","B-organization","O","B-organization","O","O","B-location","O","B-location","O","O","B-location","I-location","O","O","B-location","I-location","I-location","O","B-location","I-location","O","O","O","O","B-location","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","O","O","O","B-country","O","O","B-location","I-location","O","B-location","I-location","O","O","O","O","B-location","I-location","O","B-location","I-location","O","B-country","O","O","B-location","I-location","O","B-location","I-location","O","O","O","O","O","B-location","I-location","O","B-location","I-location","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, politician, event, organization, location, political party, country and O.\nSentence: At 3 : 0 on 31 January PAVN / VC forces attacked Saigon , Cholon , and Gia Định in the Capital Military District ; Quảng Trị ( again ) , Huế , Quảng Tín , Tam Kỳ and Quảng Ngãi as well as U.S. bases at Phú Bài and Chu Lai in I Corps ; Phan Thiết , Tuy Hòa and U.S. installations at Bong Son and An Khê in II Corps ; and Cần Thơ and Vĩnh Long in IV Corps .","prompt_labels":"At(O) 3(O) :(O) 0(O) on(O) 31(O) January(O) PAVN(B-organization) /(O) VC(B-organization) forces(O) attacked(O) Saigon(B-location) ,(O) Cholon(B-location) ,(O) and(O) Gia(B-location) Định(I-location) in(O) the(O) Capital(B-location) Military(I-location) District(I-location) ;(O) Quảng(B-location) Trị(I-location) ((O) again(O) )(O) ,(O) Huế(B-location) ,(O) Quảng(B-location) Tín(I-location) ,(O) Tam(B-location) Kỳ(I-location) and(O) Quảng(B-location) Ngãi(I-location) as(O) well(O) as(O) U.S.(B-country) bases(O) at(O) Phú(B-location) Bài(I-location) and(O) Chu(B-location) Lai(I-location) in(O) I(O) Corps(O) ;(O) Phan(B-location) Thiết(I-location) ,(O) Tuy(B-location) Hòa(I-location) and(O) U.S.(B-country) installations(O) at(O) Bong(B-location) Son(I-location) and(O) An(B-location) Khê(I-location) in(O) II(O) Corps(O) ;(O) and(O) Cần(B-location) Thơ(I-location) and(O) Vĩnh(B-location) Long(I-location) in(O) IV(O) Corps(O) .(O)"}}
{"id":"167","dataset":"crossner_politics","split":"train","label_list":["organization","politician","location","political party","event","country","person","election"],"instance":{"id":"167","words":["The","January","1910","United","Kingdom","general","election","and","December","1910","United","Kingdom","general","election","elections","in","1910","destroyed","the","large","Liberal","majority",",","meaning","they","relied","on","the","Irish","Parliamentary","Party","to","maintain","a","government","."],"labels":["O","B-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","B-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, location, political party, event, country, person, election and O.\nSentence: The January 1910 United Kingdom general election and December 1910 United Kingdom general election elections in 1910 destroyed the large Liberal majority , meaning they relied on the Irish Parliamentary Party to maintain a government .","prompt_labels":"The(O) January(B-election) 1910(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) December(B-election) 1910(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) elections(O) in(O) 1910(O) destroyed(O) the(O) large(O) Liberal(B-political party) majority(O) ,(O) meaning(O) they(O) relied(O) on(O) the(O) Irish(B-political party) Parliamentary(I-political party) Party(I-political party) to(O) maintain(O) a(O) government(O) .(O)"}}
{"id":"168","dataset":"crossner_politics","split":"train","label_list":["organization","location","person","election","country","politician","political party","event"],"instance":{"id":"168","words":["Chief","towns","included","Kingston",",","Port","of","Spain",",","Bridgetown",",","Spanish","Town",",","Montego","Bay",",","Mandeville",",","Castries",",","Roseau",",","St.","George","'s",",","Kingstown",",","St.","John","'s",",","and","Basseterre","."],"labels":["O","O","O","B-location","O","B-location","I-location","I-location","O","B-location","O","B-location","I-location","O","B-location","I-location","O","B-location","O","B-location","O","B-location","O","B-location","I-location","I-location","O","B-location","O","B-location","I-location","I-location","O","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, person, election, country, politician, political party, event and O.\nSentence: Chief towns included Kingston , Port of Spain , Bridgetown , Spanish Town , Montego Bay , Mandeville , Castries , Roseau , St. George 's , Kingstown , St. John 's , and Basseterre .","prompt_labels":"Chief(O) towns(O) included(O) Kingston(B-location) ,(O) Port(B-location) of(I-location) Spain(I-location) ,(O) Bridgetown(B-location) ,(O) Spanish(B-location) Town(I-location) ,(O) Montego(B-location) Bay(I-location) ,(O) Mandeville(B-location) ,(O) Castries(B-location) ,(O) Roseau(B-location) ,(O) St.(B-location) George(I-location) 's(I-location) ,(O) Kingstown(B-location) ,(O) St.(B-location) John(I-location) 's(I-location) ,(O) and(O) Basseterre(B-location) .(O)"}}
{"id":"169","dataset":"crossner_politics","split":"train","label_list":["event","politician","political party","location","election","person","country","organization"],"instance":{"id":"169","words":["The","USPTO","was","expected","by","2014","to","open","its","first","ever","satellite","offices","in","Detroit",",","Dallas",",","Denver",",","and","Silicon","Valley","to","reduce","backlog","and","reflect","regional","industrial","strengths","."],"labels":["O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, political party, location, election, person, country, organization and O.\nSentence: The USPTO was expected by 2014 to open its first ever satellite offices in Detroit , Dallas , Denver , and Silicon Valley to reduce backlog and reflect regional industrial strengths .","prompt_labels":"The(O) USPTO(B-organization) was(O) expected(O) by(O) 2014(O) to(O) open(O) its(O) first(O) ever(O) satellite(O) offices(O) in(O) Detroit(B-location) ,(O) Dallas(B-location) ,(O) Denver(B-location) ,(O) and(O) Silicon(B-location) Valley(I-location) to(O) reduce(O) backlog(O) and(O) reflect(O) regional(O) industrial(O) strengths(O) .(O)"}}
{"id":"170","dataset":"crossner_politics","split":"train","label_list":["election","organization","location","event","politician","country","person","political party"],"instance":{"id":"170","words":["In","1875",",","a","lodge","was","established","in","Toronto",",","followed","soon","after","by","another","in","Montreal","and","in","1882","by","a","lodge","in","Berlin","."],"labels":["O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, location, event, politician, country, person, political party and O.\nSentence: In 1875 , a lodge was established in Toronto , followed soon after by another in Montreal and in 1882 by a lodge in Berlin .","prompt_labels":"In(O) 1875(O) ,(O) a(O) lodge(O) was(O) established(O) in(O) Toronto(B-location) ,(O) followed(O) soon(O) after(O) by(O) another(O) in(O) Montreal(B-location) and(O) in(O) 1882(O) by(O) a(O) lodge(O) in(O) Berlin(B-location) .(O)"}}
{"id":"171","dataset":"crossner_politics","split":"train","label_list":["politician","election","political party","location","person","country","organization","event"],"instance":{"id":"171","words":["It","is","also","the","home","of","Universities","at","Medway",",","a","tri-partite","collaboration","of","the","University","of","Greenwich",",","the","University","of","Kent","and","Canterbury","Christ","Church","University","on","a","single","campus","in","Chatham",",","together","with","the","University","for","the","Creative","Arts",",","which","has","a","campus","in","Rochester","."],"labels":["O","O","O","O","O","O","B-location","I-location","I-location","O","O","O","O","O","O","B-location","I-location","I-location","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, political party, location, person, country, organization, event and O.\nSentence: It is also the home of Universities at Medway , a tri-partite collaboration of the University of Greenwich , the University of Kent and Canterbury Christ Church University on a single campus in Chatham , together with the University for the Creative Arts , which has a campus in Rochester .","prompt_labels":"It(O) is(O) also(O) the(O) home(O) of(O) Universities(B-location) at(I-location) Medway(I-location) ,(O) a(O) tri-partite(O) collaboration(O) of(O) the(O) University(B-location) of(I-location) Greenwich(I-location) ,(O) the(O) University(B-organization) of(I-organization) Kent(I-organization) and(O) Canterbury(B-organization) Christ(I-organization) Church(I-organization) University(I-organization) on(O) a(O) single(O) campus(O) in(O) Chatham(O) ,(O) together(O) with(O) the(O) University(B-organization) for(I-organization) the(I-organization) Creative(I-organization) Arts(I-organization) ,(O) which(O) has(O) a(O) campus(O) in(O) Rochester(B-location) .(O)"}}
{"id":"172","dataset":"crossner_politics","split":"train","label_list":["political party","politician","election","country","organization","location","person","event"],"instance":{"id":"172","words":["Social","conservatives","often","felt","that","they","were","being","sidelined","by","officials","in","the","Progressive","Conservative","Party","of","Canada","and","its","leadership","of","so-called","Red","Tories","for","the","last","half","of","the","twentieth","century","and","therefore","many","eventually","made","their","political","home","with","parties","such","as","the","Social","Credit","Party","of","Canada","and","the","Reform","Party","of","Canada","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, election, country, organization, location, person, event and O.\nSentence: Social conservatives often felt that they were being sidelined by officials in the Progressive Conservative Party of Canada and its leadership of so-called Red Tories for the last half of the twentieth century and therefore many eventually made their political home with parties such as the Social Credit Party of Canada and the Reform Party of Canada .","prompt_labels":"Social(O) conservatives(O) often(O) felt(O) that(O) they(O) were(O) being(O) sidelined(O) by(O) officials(O) in(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) its(O) leadership(O) of(O) so-called(O) Red(O) Tories(O) for(O) the(O) last(O) half(O) of(O) the(O) twentieth(O) century(O) and(O) therefore(O) many(O) eventually(O) made(O) their(O) political(O) home(O) with(O) parties(O) such(O) as(O) the(O) Social(B-political party) Credit(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) the(O) Reform(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) .(O)"}}
{"id":"173","dataset":"crossner_politics","split":"train","label_list":["politician","country","election","person","location","political party","event","organization"],"instance":{"id":"173","words":["After","Vienna",",","he","studied","political","economy","and","social","science","at","the","universities","of","Heidelberg","University",",","Leipzig","University","and","University","of","Jena",","],"labels":["O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, election, person, location, political party, event, organization and O.\nSentence: After Vienna , he studied political economy and social science at the universities of Heidelberg University , Leipzig University and University of Jena ,","prompt_labels":"After(O) Vienna(B-location) ,(O) he(O) studied(O) political(O) economy(O) and(O) social(O) science(O) at(O) the(O) universities(O) of(O) Heidelberg(B-organization) University(I-organization) ,(O) Leipzig(B-organization) University(I-organization) and(O) University(B-organization) of(I-organization) Jena(I-organization) ,(O)"}}
{"id":"174","dataset":"crossner_politics","split":"train","label_list":["country","election","organization","political party","event","location","politician","person"],"instance":{"id":"174","words":["Brazil","'s","major","cities","began","to","resemble","1932-33","Berlin","with","its","street","battles","between","the","Communist","Party","of","Germany","and","the","Nazi","Party","."],"labels":["B-country","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, organization, political party, event, location, politician, person and O.\nSentence: Brazil 's major cities began to resemble 1932-33 Berlin with its street battles between the Communist Party of Germany and the Nazi Party .","prompt_labels":"Brazil(B-country) 's(O) major(O) cities(O) began(O) to(O) resemble(O) 1932-33(O) Berlin(B-location) with(O) its(O) street(O) battles(O) between(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Germany(I-political party) and(O) the(O) Nazi(B-political party) Party(I-political party) .(O)"}}
{"id":"175","dataset":"crossner_politics","split":"train","label_list":["country","event","political party","location","election","organization","politician","person"],"instance":{"id":"175","words":["These","include","the","TRUE","Blue","Crew",",","Antipodean","Resistance",",","the","Australian","Defence","League",",","National","Action","(","Australia",")",",","the","Q","Society",",","Reclaim","Australia","and","the","Lads","Society","(","formerly","United","Patriots","Front",")","."],"labels":["O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-country","O","O","O","B-organization","I-organization","O","B-organization","I-organization","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, political party, location, election, organization, politician, person and O.\nSentence: These include the TRUE Blue Crew , Antipodean Resistance , the Australian Defence League , National Action ( Australia ) , the Q Society , Reclaim Australia and the Lads Society ( formerly United Patriots Front ) .","prompt_labels":"These(O) include(O) the(O) TRUE(B-organization) Blue(I-organization) Crew(I-organization) ,(O) Antipodean(B-organization) Resistance(I-organization) ,(O) the(O) Australian(B-organization) Defence(I-organization) League(I-organization) ,(O) National(B-organization) Action(I-organization) ((O) Australia(B-country) )(O) ,(O) the(O) Q(B-organization) Society(I-organization) ,(O) Reclaim(B-organization) Australia(I-organization) and(O) the(O) Lads(B-organization) Society(I-organization) ((O) formerly(O) United(B-organization) Patriots(I-organization) Front(I-organization) )(O) .(O)"}}
{"id":"176","dataset":"crossner_politics","split":"train","label_list":["person","location","organization","event","country","politician","election","political party"],"instance":{"id":"176","words":["Representatives","of","the","Kuomintang",",","Communist","Party","of","China",",","Chinese","Youth","Party",",","and","China","Democratic","League",",","as","well","as","independent","delegates",",","attended","the","conference","in","Chongqing","."],"labels":["O","O","O","B-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, organization, event, country, politician, election, political party and O.\nSentence: Representatives of the Kuomintang , Communist Party of China , Chinese Youth Party , and China Democratic League , as well as independent delegates , attended the conference in Chongqing .","prompt_labels":"Representatives(O) of(O) the(O) Kuomintang(B-political party) ,(O) Communist(B-political party) Party(I-political party) of(I-political party) China(I-political party) ,(O) Chinese(B-political party) Youth(I-political party) Party(I-political party) ,(O) and(O) China(B-political party) Democratic(I-political party) League(I-political party) ,(O) as(O) well(O) as(O) independent(O) delegates(O) ,(O) attended(O) the(O) conference(O) in(O) Chongqing(B-location) .(O)"}}
{"id":"177","dataset":"crossner_politics","split":"train","label_list":["politician","person","organization","event","location","election","political party","country"],"instance":{"id":"177","words":["Like","Fidel",",","Raúl","later","attended","the","Jesuit","School","of","Colegio","Dolores","in","Santiago","de","Cuba","and","Belen","Jesuit","Preparatory","School","in","Havana","."],"labels":["O","B-person","O","B-person","O","O","O","B-location","I-location","I-location","I-location","I-location","O","B-location","I-location","I-location","O","B-location","I-location","I-location","I-location","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, organization, event, location, election, political party, country and O.\nSentence: Like Fidel , Raúl later attended the Jesuit School of Colegio Dolores in Santiago de Cuba and Belen Jesuit Preparatory School in Havana .","prompt_labels":"Like(O) Fidel(B-person) ,(O) Raúl(B-person) later(O) attended(O) the(O) Jesuit(B-location) School(I-location) of(I-location) Colegio(I-location) Dolores(I-location) in(O) Santiago(B-location) de(I-location) Cuba(I-location) and(O) Belen(B-location) Jesuit(I-location) Preparatory(I-location) School(I-location) in(O) Havana(B-location) .(O)"}}
{"id":"178","dataset":"crossner_politics","split":"train","label_list":["location","political party","person","country","organization","election","politician","event"],"instance":{"id":"178","words":["Wood","was","unopposed","in","the","general","elections","of","1918","United","Kingdom","general","election",",","1922","United","Kingdom","general","election",",","1923","United","Kingdom","general","election","and","1924","United","Kingdom","general","election","."],"labels":["O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, person, country, organization, election, politician, event and O.\nSentence: Wood was unopposed in the general elections of 1918 United Kingdom general election , 1922 United Kingdom general election , 1923 United Kingdom general election and 1924 United Kingdom general election .","prompt_labels":"Wood(O) was(O) unopposed(O) in(O) the(O) general(O) elections(O) of(O) 1918(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 1922(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 1923(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1924(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)"}}
{"id":"179","dataset":"crossner_politics","split":"train","label_list":["political party","politician","organization","person","event","location","election","country"],"instance":{"id":"179","words":["The","2001","Local","Government","Act","restyled","the","five","county","boroughs","of","Dublin",",","Cork",",","Galway",",","Waterford",",","and","Limerick","as","city","councils",",","with","the","same","status","in","law","as","county","councils","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, organization, person, event, location, election, country and O.\nSentence: The 2001 Local Government Act restyled the five county boroughs of Dublin , Cork , Galway , Waterford , and Limerick as city councils , with the same status in law as county councils .","prompt_labels":"The(O) 2001(O) Local(O) Government(O) Act(O) restyled(O) the(O) five(O) county(O) boroughs(O) of(O) Dublin(B-location) ,(O) Cork(B-location) ,(O) Galway(B-location) ,(O) Waterford(B-location) ,(O) and(O) Limerick(B-location) as(O) city(O) councils(O) ,(O) with(O) the(O) same(O) status(O) in(O) law(O) as(O) county(O) councils(O) .(O)"}}
{"id":"180","dataset":"crossner_politics","split":"train","label_list":["country","political party","politician","person","event","location","election","organization"],"instance":{"id":"180","words":["A","number","of","the","wealthiest","and","most","affluent","American","families","(","Old","Money",")","-","such","as","the","Vanderbilts",",","Astor","family",",","Rockefeller","family",",","Du","Pont","family",",","Roosevelt","family",",","Forbes",",","Whitneys",",","Junius","Spencer","Morgan",",","and","Harrimans","-","are","mostly","Mainline","Protestant","families","in","the","Episcopalian",",","Presbyterian",",","or","other","similar","traditions","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","B-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","O","B-organization","O","B-person","I-person","I-person","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","O","B-organization","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, politician, person, event, location, election, organization and O.\nSentence: A number of the wealthiest and most affluent American families ( Old Money ) - such as the Vanderbilts , Astor family , Rockefeller family , Du Pont family , Roosevelt family , Forbes , Whitneys , Junius Spencer Morgan , and Harrimans - are mostly Mainline Protestant families in the Episcopalian , Presbyterian , or other similar traditions .","prompt_labels":"A(O) number(O) of(O) the(O) wealthiest(O) and(O) most(O) affluent(O) American(O) families(O) ((O) Old(O) Money(O) )(O) -(O) such(O) as(O) the(O) Vanderbilts(B-organization) ,(O) Astor(B-organization) family(I-organization) ,(O) Rockefeller(B-organization) family(I-organization) ,(O) Du(B-organization) Pont(I-organization) family(I-organization) ,(O) Roosevelt(B-organization) family(I-organization) ,(O) Forbes(B-organization) ,(O) Whitneys(B-organization) ,(O) Junius(B-person) Spencer(I-person) Morgan(I-person) ,(O) and(O) Harrimans(O) -(O) are(O) mostly(O) Mainline(B-organization) Protestant(I-organization) families(I-organization) in(O) the(O) Episcopalian(B-organization) ,(O) Presbyterian(B-organization) ,(O) or(O) other(O) similar(O) traditions(O) .(O)"}}
{"id":"181","dataset":"crossner_politics","split":"train","label_list":["politician","political party","person","election","event","organization","location","country"],"instance":{"id":"181","words":["From","their","18","%","share","of","the","first","German","federal","elections","under","proportional","representation","in","1919","German","federal","election",",","they","dropped",",","for","example",",","to","4.9","%","in","the","1928","German","federal","election","and","to","1.0","%","in","the","November","1932","German","federal","election","."],"labels":["O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, person, election, event, organization, location, country and O.\nSentence: From their 18 % share of the first German federal elections under proportional representation in 1919 German federal election , they dropped , for example , to 4.9 % in the 1928 German federal election and to 1.0 % in the November 1932 German federal election .","prompt_labels":"From(O) their(O) 18(O) %(O) share(O) of(O) the(O) first(B-election) German(I-election) federal(I-election) elections(I-election) under(O) proportional(O) representation(O) in(O) 1919(B-election) German(I-election) federal(I-election) election(I-election) ,(O) they(O) dropped(O) ,(O) for(O) example(O) ,(O) to(O) 4.9(O) %(O) in(O) the(O) 1928(B-election) German(I-election) federal(I-election) election(I-election) and(O) to(O) 1.0(O) %(O) in(O) the(O) November(B-election) 1932(I-election) German(I-election) federal(I-election) election(I-election) .(O)"}}
{"id":"182","dataset":"crossner_politics","split":"train","label_list":["political party","country","organization","election","politician","person","location","event"],"instance":{"id":"182","words":["His","party","won","14","seats","in","the","1998","Nova","Scotia","general","election","and","held","the","balance","of","power","in","a","minority","government","where","both","the","Nova","Scotia","Liberal","Party","and","the","Nova","Scotia","New","Democratic","Party",",","led","by","Russell","MacLellan","and","Robert","Chisholm",",","respectively",",","each","had","nineteen","seats","."],"labels":["O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, organization, election, politician, person, location, event and O.\nSentence: His party won 14 seats in the 1998 Nova Scotia general election and held the balance of power in a minority government where both the Nova Scotia Liberal Party and the Nova Scotia New Democratic Party , led by Russell MacLellan and Robert Chisholm , respectively , each had nineteen seats .","prompt_labels":"His(O) party(O) won(O) 14(O) seats(O) in(O) the(O) 1998(B-election) Nova(I-election) Scotia(I-election) general(I-election) election(I-election) and(O) held(O) the(O) balance(O) of(O) power(O) in(O) a(O) minority(O) government(O) where(O) both(O) the(O) Nova(B-political party) Scotia(I-political party) Liberal(I-political party) Party(I-political party) and(O) the(O) Nova(B-political party) Scotia(I-political party) New(I-political party) Democratic(I-political party) Party(I-political party) ,(O) led(O) by(O) Russell(B-politician) MacLellan(I-politician) and(O) Robert(B-politician) Chisholm(I-politician) ,(O) respectively(O) ,(O) each(O) had(O) nineteen(O) seats(O) .(O)"}}
{"id":"183","dataset":"crossner_politics","split":"train","label_list":["country","politician","location","organization","event","political party","person","election"],"instance":{"id":"183","words":["OSHA","'s","regional","offices","are","located","in","Boston",",","New","York","City",",","Philadelphia",",","Atlanta",",","Chicago",",","Dallas",",","Kansas","City","metropolitan","area",",","Denver",",","San","Francisco",",","and","Seattle","."],"labels":["B-organization","O","O","O","O","O","O","B-location","O","B-location","I-location","I-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","I-location","I-location","I-location","O","B-location","O","B-location","I-location","O","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, politician, location, organization, event, political party, person, election and O.\nSentence: OSHA 's regional offices are located in Boston , New York City , Philadelphia , Atlanta , Chicago , Dallas , Kansas City metropolitan area , Denver , San Francisco , and Seattle .","prompt_labels":"OSHA(B-organization) 's(O) regional(O) offices(O) are(O) located(O) in(O) Boston(B-location) ,(O) New(B-location) York(I-location) City(I-location) ,(O) Philadelphia(B-location) ,(O) Atlanta(B-location) ,(O) Chicago(B-location) ,(O) Dallas(B-location) ,(O) Kansas(B-location) City(I-location) metropolitan(I-location) area(I-location) ,(O) Denver(B-location) ,(O) San(B-location) Francisco(I-location) ,(O) and(O) Seattle(B-location) .(O)"}}
{"id":"184","dataset":"crossner_politics","split":"train","label_list":["location","politician","person","country","political party","election","event","organization"],"instance":{"id":"184","words":["He","introduced","measures","to","combat","resistance",",","and","when","a","widespread","strike","took","place","in","Amsterdam",",","Arnhem","and","Hilversum","in","May","1943",",","special","summary","court-martial","procedures","were","brought","in",",","and","a","collective","fine","of","18","million","guilder","s","was","imposed","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, person, country, political party, election, event, organization and O.\nSentence: He introduced measures to combat resistance , and when a widespread strike took place in Amsterdam , Arnhem and Hilversum in May 1943 , special summary court-martial procedures were brought in , and a collective fine of 18 million guilder s was imposed .","prompt_labels":"He(O) introduced(O) measures(O) to(O) combat(O) resistance(O) ,(O) and(O) when(O) a(O) widespread(O) strike(O) took(O) place(O) in(O) Amsterdam(B-location) ,(O) Arnhem(B-location) and(O) Hilversum(B-location) in(O) May(O) 1943(O) ,(O) special(O) summary(O) court-martial(O) procedures(O) were(O) brought(O) in(O) ,(O) and(O) a(O) collective(O) fine(O) of(O) 18(O) million(O) guilder(O) s(O) was(O) imposed(O) .(O)"}}
{"id":"185","dataset":"crossner_politics","split":"train","label_list":["event","organization","person","politician","location","political party","election","country"],"instance":{"id":"185","words":["Hitchens","was","made","an","Honorary","Associate","of","the","Rationalist","International","and","the","National","Secular","Society","shortly","after","its","release",",","and","he","was","later","named","to","the","Honorary","Board","of","distinguished","achievers","of","the","Freedom","From","Religion","Foundation","..","Approximately","112","minutes","in",",","Hitchens","contends",",","'","The","moment","where","everything","went","wrong","is","the","moment","when","the","Jewish","hellenists","were","defeated","by","the","Jewish","messiahs",",","the","celebration","now","benignly","known","as","Hanukkah","."],"labels":["B-person","O","O","O","O","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, person, politician, location, political party, election, country and O.\nSentence: Hitchens was made an Honorary Associate of the Rationalist International and the National Secular Society shortly after its release , and he was later named to the Honorary Board of distinguished achievers of the Freedom From Religion Foundation .. Approximately 112 minutes in , Hitchens contends , ' The moment where everything went wrong is the moment when the Jewish hellenists were defeated by the Jewish messiahs , the celebration now benignly known as Hanukkah .","prompt_labels":"Hitchens(B-person) was(O) made(O) an(O) Honorary(O) Associate(O) of(O) the(O) Rationalist(B-organization) International(I-organization) and(O) the(O) National(B-organization) Secular(I-organization) Society(I-organization) shortly(O) after(O) its(O) release(O) ,(O) and(O) he(O) was(O) later(O) named(O) to(O) the(O) Honorary(O) Board(O) of(O) distinguished(O) achievers(O) of(O) the(O) Freedom(B-organization) From(I-organization) Religion(I-organization) Foundation(I-organization) ..(O) Approximately(O) 112(O) minutes(O) in(O) ,(O) Hitchens(O) contends(O) ,(O) '(O) The(O) moment(O) where(O) everything(O) went(O) wrong(O) is(O) the(O) moment(O) when(O) the(O) Jewish(O) hellenists(O) were(O) defeated(O) by(O) the(O) Jewish(O) messiahs(O) ,(O) the(O) celebration(O) now(O) benignly(O) known(O) as(O) Hanukkah(O) .(O)"}}
{"id":"186","dataset":"crossner_politics","split":"train","label_list":["political party","location","organization","election","politician","event","person","country"],"instance":{"id":"186","words":["He","was","also","the","Member","of","Parliament","for","Upper","Bann","from","1990","Upper","Bann","by-election","to","2005","United","Kingdom","general","election","in","Northern","Ireland","and","the","Member","of","the","Legislative","Assembly","(","MLA",")","for","Upper","Bann","from","1998","to","2007","Northern","Ireland","Assembly","election","."],"labels":["O","O","O","O","O","O","B-organization","O","B-organization","I-organization","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","B-organization","I-organization","O","B-organization","O","O","B-organization","I-organization","O","O","O","B-election","I-election","I-election","I-election","I-election","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, location, organization, election, politician, event, person, country and O.\nSentence: He was also the Member of Parliament for Upper Bann from 1990 Upper Bann by-election to 2005 United Kingdom general election in Northern Ireland and the Member of the Legislative Assembly ( MLA ) for Upper Bann from 1998 to 2007 Northern Ireland Assembly election .","prompt_labels":"He(O) was(O) also(O) the(O) Member(O) of(O) Parliament(B-organization) for(O) Upper(B-organization) Bann(I-organization) from(O) 1990(B-election) Upper(I-election) Bann(I-election) by-election(I-election) to(O) 2005(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) in(I-election) Northern(I-election) Ireland(I-election) and(O) the(O) Member(O) of(O) the(O) Legislative(B-organization) Assembly(I-organization) ((O) MLA(B-organization) )(O) for(O) Upper(B-organization) Bann(I-organization) from(O) 1998(O) to(O) 2007(B-election) Northern(I-election) Ireland(I-election) Assembly(I-election) election(I-election) .(O)"}}
{"id":"187","dataset":"crossner_politics","split":"train","label_list":["organization","election","country","event","location","person","political party","politician"],"instance":{"id":"187","words":["Kristol","was","a","fellow","of","the","American","Academy","of","Arts","and","Sciences",",","a","member","of","the","Council","on","Foreign","Relations","and","a","fellow","emeritus","at","the","American","Enterprise","Institute","(","having","been","an","associate","fellow","from","1972",",","a","senior","fellow","from","1977","and","the","John","M.","Olin","Distinguished","Fellow","from","1988","to","1999",")","."],"labels":["B-person","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, country, event, location, person, political party, politician and O.\nSentence: Kristol was a fellow of the American Academy of Arts and Sciences , a member of the Council on Foreign Relations and a fellow emeritus at the American Enterprise Institute ( having been an associate fellow from 1972 , a senior fellow from 1977 and the John M. Olin Distinguished Fellow from 1988 to 1999 ) .","prompt_labels":"Kristol(B-person) was(O) a(O) fellow(O) of(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) ,(O) a(O) member(O) of(O) the(O) Council(B-organization) on(I-organization) Foreign(I-organization) Relations(I-organization) and(O) a(O) fellow(O) emeritus(O) at(O) the(O) American(B-organization) Enterprise(I-organization) Institute(I-organization) ((O) having(O) been(O) an(O) associate(O) fellow(O) from(O) 1972(O) ,(O) a(O) senior(O) fellow(O) from(O) 1977(O) and(O) the(O) John(O) M.(O) Olin(O) Distinguished(O) Fellow(O) from(O) 1988(O) to(O) 1999(O) )(O) .(O)"}}
{"id":"188","dataset":"crossner_politics","split":"train","label_list":["politician","organization","person","election","political party","event","country","location"],"instance":{"id":"188","words":["The","party","won","two","additional","seats","in","the","1985","Belgian","general","election",",","two","additional","seats","in","1987","Belgian","general","election","and","one","in","1991","Belgian","general","election",":","in","that","year","it","won","seven","seats","in","parliament","."],"labels":["O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, person, election, political party, event, country, location and O.\nSentence: The party won two additional seats in the 1985 Belgian general election , two additional seats in 1987 Belgian general election and one in 1991 Belgian general election : in that year it won seven seats in parliament .","prompt_labels":"The(O) party(O) won(O) two(O) additional(O) seats(O) in(O) the(O) 1985(B-election) Belgian(I-election) general(I-election) election(I-election) ,(O) two(O) additional(O) seats(O) in(O) 1987(B-election) Belgian(I-election) general(I-election) election(I-election) and(O) one(O) in(O) 1991(B-election) Belgian(I-election) general(I-election) election(I-election) :(O) in(O) that(O) year(O) it(O) won(O) seven(O) seats(O) in(O) parliament(O) .(O)"}}
{"id":"189","dataset":"crossner_politics","split":"train","label_list":["person","politician","political party","location","organization","event","country","election"],"instance":{"id":"189","words":["Johnson","ran","for","governor","three","times",":","in","1947","Mississippi","gubernatorial","election",",","1951","Mississippi","gubernatorial","election",",","and","1955","Mississippi","gubernatorial","election",",","but","was","unsuccessful","."],"labels":["B-politician","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, political party, location, organization, event, country, election and O.\nSentence: Johnson ran for governor three times : in 1947 Mississippi gubernatorial election , 1951 Mississippi gubernatorial election , and 1955 Mississippi gubernatorial election , but was unsuccessful .","prompt_labels":"Johnson(B-politician) ran(O) for(O) governor(O) three(O) times(O) :(O) in(O) 1947(B-election) Mississippi(I-election) gubernatorial(I-election) election(I-election) ,(O) 1951(B-election) Mississippi(I-election) gubernatorial(I-election) election(I-election) ,(O) and(O) 1955(B-election) Mississippi(I-election) gubernatorial(I-election) election(I-election) ,(O) but(O) was(O) unsuccessful(O) .(O)"}}
{"id":"190","dataset":"crossner_politics","split":"train","label_list":["country","event","politician","election","location","person","organization","political party"],"instance":{"id":"190","words":["Following","the","launch","of","Yes","Scotland",",","other","campaigns","in","support","of","independence","were","launched",",","including","the","National","Collective","and","Radical","Independence","Campaign","."],"labels":["O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, politician, election, location, person, organization, political party and O.\nSentence: Following the launch of Yes Scotland , other campaigns in support of independence were launched , including the National Collective and Radical Independence Campaign .","prompt_labels":"Following(O) the(O) launch(O) of(O) Yes(B-organization) Scotland(I-organization) ,(O) other(O) campaigns(O) in(O) support(O) of(O) independence(O) were(O) launched(O) ,(O) including(O) the(O) National(B-organization) Collective(I-organization) and(O) Radical(B-organization) Independence(I-organization) Campaign(I-organization) .(O)"}}
{"id":"191","dataset":"crossner_politics","split":"train","label_list":["organization","event","political party","location","person","politician","election","country"],"instance":{"id":"191","words":["In","an","interview","for","Antena","3",",","the","leader","of","the","Social","Democratic","Party",",","Liviu","Dragnea",",","stated","that","even","though","his","party","agrees","with","the","referendum",",","there","are","many","voices","that","say","that","the","referendum","is","actually","useless","for","the","publicly","expressed","purpose","and","that","it","actually","is","a","way","for","Klaus","Iohannis","to","get","involved","in","the","electoral","campaign","so","that","he","can","help","parties","around","him","."],"labels":["O","O","O","O","B-organization","I-organization","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, political party, location, person, politician, election, country and O.\nSentence: In an interview for Antena 3 , the leader of the Social Democratic Party , Liviu Dragnea , stated that even though his party agrees with the referendum , there are many voices that say that the referendum is actually useless for the publicly expressed purpose and that it actually is a way for Klaus Iohannis to get involved in the electoral campaign so that he can help parties around him .","prompt_labels":"In(O) an(O) interview(O) for(O) Antena(B-organization) 3(I-organization) ,(O) the(O) leader(O) of(O) the(O) Social(B-political party) Democratic(I-political party) Party(I-political party) ,(O) Liviu(B-politician) Dragnea(I-politician) ,(O) stated(O) that(O) even(O) though(O) his(O) party(O) agrees(O) with(O) the(O) referendum(O) ,(O) there(O) are(O) many(O) voices(O) that(O) say(O) that(O) the(O) referendum(O) is(O) actually(O) useless(O) for(O) the(O) publicly(O) expressed(O) purpose(O) and(O) that(O) it(O) actually(O) is(O) a(O) way(O) for(O) Klaus(B-politician) Iohannis(I-politician) to(O) get(O) involved(O) in(O) the(O) electoral(O) campaign(O) so(O) that(O) he(O) can(O) help(O) parties(O) around(O) him(O) .(O)"}}
{"id":"192","dataset":"crossner_politics","split":"train","label_list":["organization","political party","election","event","person","politician","location","country"],"instance":{"id":"192","words":["The","III","CELAC","summit","or","2015","CELAC","summit","was","the","third","ordinary","heads","of","state","summit","of","the","Community","of","Latin","American","and","Caribbean","States","."],"labels":["O","B-event","I-event","I-event","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, election, event, person, politician, location, country and O.\nSentence: The III CELAC summit or 2015 CELAC summit was the third ordinary heads of state summit of the Community of Latin American and Caribbean States .","prompt_labels":"The(O) III(B-event) CELAC(I-event) summit(I-event) or(O) 2015(B-event) CELAC(I-event) summit(I-event) was(O) the(O) third(O) ordinary(O) heads(O) of(O) state(O) summit(O) of(O) the(O) Community(B-organization) of(I-organization) Latin(I-organization) American(I-organization) and(I-organization) Caribbean(I-organization) States(I-organization) .(O)"}}
{"id":"193","dataset":"crossner_politics","split":"train","label_list":["politician","country","person","event","location","election","organization","political party"],"instance":{"id":"193","words":["The","IV","CELAC","summit","or","2016","CELAC","summit","was","the","fourth","ordinary","heads","of","state","summit","of","the","Community","of","Latin","American","and","Caribbean","States","."],"labels":["O","B-event","I-event","I-event","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, person, event, location, election, organization, political party and O.\nSentence: The IV CELAC summit or 2016 CELAC summit was the fourth ordinary heads of state summit of the Community of Latin American and Caribbean States .","prompt_labels":"The(O) IV(B-event) CELAC(I-event) summit(I-event) or(O) 2016(B-event) CELAC(I-event) summit(I-event) was(O) the(O) fourth(O) ordinary(O) heads(O) of(O) state(O) summit(O) of(O) the(O) Community(B-organization) of(I-organization) Latin(I-organization) American(I-organization) and(I-organization) Caribbean(I-organization) States(I-organization) .(O)"}}
{"id":"194","dataset":"crossner_politics","split":"train","label_list":["politician","person","political party","location","election","country","organization","event"],"instance":{"id":"194","words":["The","heads","of","state","summit","was","followed","by","a","ministerial","summit","on","November","2009","in","Montego","Bay",",","Jamaica","to","prepare","an","action","plan","to","develop","the","agenda","agreed","in","the","I","CALC","summit","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, political party, location, election, country, organization, event and O.\nSentence: The heads of state summit was followed by a ministerial summit on November 2009 in Montego Bay , Jamaica to prepare an action plan to develop the agenda agreed in the I CALC summit .","prompt_labels":"The(O) heads(O) of(O) state(O) summit(O) was(O) followed(O) by(O) a(O) ministerial(O) summit(O) on(O) November(O) 2009(O) in(O) Montego(B-location) Bay(I-location) ,(O) Jamaica(B-country) to(O) prepare(O) an(O) action(O) plan(O) to(O) develop(O) the(O) agenda(O) agreed(O) in(O) the(O) I(B-event) CALC(I-event) summit(I-event) .(O)"}}
{"id":"195","dataset":"crossner_politics","split":"train","label_list":["political party","country","election","person","event","organization","politician","location"],"instance":{"id":"195","words":["As","agreed","with","the","Allies","of","World","War","II","at","the","Tehran","Conference","in","November","1943","and","the","Yalta","Conference","in","February","1945",",","the","Soviet","Union","entered","World","War","II","'s","Pacific","Theater","within","three","months","of","the","end","of","the","war","in","Europe","."],"labels":["O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-event","I-event","O","O","O","O","O","B-event","I-event","O","O","O","O","O","B-country","I-country","O","B-event","I-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, election, person, event, organization, politician, location and O.\nSentence: As agreed with the Allies of World War II at the Tehran Conference in November 1943 and the Yalta Conference in February 1945 , the Soviet Union entered World War II 's Pacific Theater within three months of the end of the war in Europe .","prompt_labels":"As(O) agreed(O) with(O) the(O) Allies(B-organization) of(I-organization) World(I-organization) War(I-organization) II(I-organization) at(O) the(O) Tehran(B-event) Conference(I-event) in(O) November(O) 1943(O) and(O) the(O) Yalta(B-event) Conference(I-event) in(O) February(O) 1945(O) ,(O) the(O) Soviet(B-country) Union(I-country) entered(O) World(B-event) War(I-event) II(I-event) 's(I-event) Pacific(I-event) Theater(I-event) within(O) three(O) months(O) of(O) the(O) end(O) of(O) the(O) war(O) in(O) Europe(B-location) .(O)"}}
{"id":"196","dataset":"crossner_politics","split":"train","label_list":["person","event","political party","location","politician","election","organization","country"],"instance":{"id":"196","words":["The","2019","BRICS","summit","was","the","eleventh","annual","BRICS","summit",",","an","international","relations","conference","to","be","attended","by","the","heads","of","state","or","heads","of","government","of","the","five","member","states","Brazil",",","Russia",",","India",",","China","and","South","Africa","."],"labels":["O","B-event","I-event","I-event","O","O","B-event","I-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","I-country","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, political party, location, politician, election, organization, country and O.\nSentence: The 2019 BRICS summit was the eleventh annual BRICS summit , an international relations conference to be attended by the heads of state or heads of government of the five member states Brazil , Russia , India , China and South Africa .","prompt_labels":"The(O) 2019(B-event) BRICS(I-event) summit(I-event) was(O) the(O) eleventh(B-event) annual(I-event) BRICS(I-event) summit(I-event) ,(O) an(O) international(O) relations(O) conference(O) to(O) be(O) attended(O) by(O) the(O) heads(O) of(O) state(O) or(O) heads(O) of(O) government(O) of(O) the(O) five(O) member(O) states(O) Brazil(B-country) ,(O) Russia(B-country) ,(O) India(B-country) ,(O) China(B-country) and(O) South(B-country) Africa(I-country) .(O)"}}
{"id":"197","dataset":"crossner_politics","split":"train","label_list":["election","organization","political party","location","event","politician","country","person"],"instance":{"id":"197","words":["On","21","June",",","Prime","Minister","Churchill","was","in","the","White","House","in","Washington","conferring","on","the","future","direction","of","the","war","with","President","Franklin","D.","Roosevelt",",","a","summit","meeting","known","as","the","Second","Washington","Conference","."],"labels":["O","O","O","O","O","O","B-politician","O","O","O","B-location","I-location","O","B-location","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","B-event","I-event","I-event","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, political party, location, event, politician, country, person and O.\nSentence: On 21 June , Prime Minister Churchill was in the White House in Washington conferring on the future direction of the war with President Franklin D. Roosevelt , a summit meeting known as the Second Washington Conference .","prompt_labels":"On(O) 21(O) June(O) ,(O) Prime(O) Minister(O) Churchill(B-politician) was(O) in(O) the(O) White(B-location) House(I-location) in(O) Washington(B-location) conferring(O) on(O) the(O) future(O) direction(O) of(O) the(O) war(O) with(O) President(O) Franklin(B-politician) D.(I-politician) Roosevelt(I-politician) ,(O) a(O) summit(O) meeting(O) known(O) as(O) the(O) Second(B-event) Washington(I-event) Conference(I-event) .(O)"}}
{"id":"198","dataset":"crossner_politics","split":"train","label_list":["country","politician","political party","election","organization","event","person","location"],"instance":{"id":"198","words":["She","worked","for","Christian","Aid","from","1994","to","1997",",","taking","on","roles","as","a","campaign","assistant","in","London",",","working","in","Serbia","during","the","war","and","as","Head","of","Country","Office","in","Bosnia","in","the","aftermath","of","the","Bosnian","War","."],"labels":["O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","B-event","I-event","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, politician, political party, election, organization, event, person, location and O.\nSentence: She worked for Christian Aid from 1994 to 1997 , taking on roles as a campaign assistant in London , working in Serbia during the war and as Head of Country Office in Bosnia in the aftermath of the Bosnian War .","prompt_labels":"She(O) worked(O) for(O) Christian(B-organization) Aid(I-organization) from(O) 1994(O) to(O) 1997(O) ,(O) taking(O) on(O) roles(O) as(O) a(O) campaign(O) assistant(O) in(O) London(B-location) ,(O) working(O) in(O) Serbia(B-country) during(O) the(O) war(O) and(O) as(O) Head(O) of(O) Country(O) Office(O) in(O) Bosnia(B-country) in(O) the(O) aftermath(O) of(O) the(O) Bosnian(B-event) War(I-event) .(O)"}}
{"id":"199","dataset":"crossner_politics","split":"train","label_list":["location","politician","election","event","political party","person","organization","country"],"instance":{"id":"199","words":["The","first","campaign","in","Illinois","was","not","by","Coolidge","or","Davis",",","but","by","radical","third-party","nominee",",","veteran","Wisconsin","Senator","Robert","M.","La","Follette",",","who","on","July","4","confirmed","his","previous","plan","to","run","a","third-party","campaign","when","nominated","by","the","Conference","for","Progressive","Political","Action","."],"labels":["O","O","O","O","B-location","O","O","O","B-politician","O","B-politician","O","O","O","O","O","O","O","O","B-location","O","B-politician","I-politician","I-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, election, event, political party, person, organization, country and O.\nSentence: The first campaign in Illinois was not by Coolidge or Davis , but by radical third-party nominee , veteran Wisconsin Senator Robert M. La Follette , who on July 4 confirmed his previous plan to run a third-party campaign when nominated by the Conference for Progressive Political Action .","prompt_labels":"The(O) first(O) campaign(O) in(O) Illinois(B-location) was(O) not(O) by(O) Coolidge(B-politician) or(O) Davis(B-politician) ,(O) but(O) by(O) radical(O) third-party(O) nominee(O) ,(O) veteran(O) Wisconsin(B-location) Senator(O) Robert(B-politician) M.(I-politician) La(I-politician) Follette(I-politician) ,(O) who(O) on(O) July(O) 4(O) confirmed(O) his(O) previous(O) plan(O) to(O) run(O) a(O) third-party(O) campaign(O) when(O) nominated(O) by(O) the(O) Conference(B-political party) for(I-political party) Progressive(I-political party) Political(I-political party) Action(I-political party) .(O)"}}
{"id":"0","dataset":"crossner_science","split":"train","label_list":["protein","enzyme","theory","university","discipline","chemical element","location","event","country","chemical compound","scientist","award","academic journal","astronomical object","organization","person"],"instance":{"id":"0","words":["They","may","also","use","Adenosine","triphosphate",",","Nitric","oxide",",","and","ROS","for","signaling","in","the","same","ways","that","animals","do","."],"labels":["O","O","O","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O","O","B-chemical compound","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, enzyme, theory, university, discipline, chemical element, location, event, country, chemical compound, scientist, award, academic journal, astronomical object, organization, person and O.\nSentence: They may also use Adenosine triphosphate , Nitric oxide , and ROS for signaling in the same ways that animals do .","prompt_labels":"They(O) may(O) also(O) use(O) Adenosine(B-chemical compound) triphosphate(I-chemical compound) ,(O) Nitric(B-chemical compound) oxide(I-chemical compound) ,(O) and(O) ROS(B-chemical compound) for(O) signaling(O) in(O) the(O) same(O) ways(O) that(O) animals(O) do(O) .(O)"}}
{"id":"1","dataset":"crossner_science","split":"train","label_list":["chemical compound","award","location","person","discipline","scientist","theory","university","academic journal","country","enzyme","chemical element","protein","astronomical object","event","organization"],"instance":{"id":"1","words":["August","Kopff",",","a","colleague","of","Wolf","at","Heidelberg",",","then","discovered","617","Patroclus","eight","months","after","Achilles",",","and",",","in","early","1907",",","he","discovered","the","largest","of","all","Jupiter","trojans",",","624","Hektor","."],"labels":["B-scientist","I-scientist","O","O","O","O","B-scientist","O","B-location","O","O","O","B-astronomical object","I-astronomical object","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, award, location, person, discipline, scientist, theory, university, academic journal, country, enzyme, chemical element, protein, astronomical object, event, organization and O.\nSentence: August Kopff , a colleague of Wolf at Heidelberg , then discovered 617 Patroclus eight months after Achilles , and , in early 1907 , he discovered the largest of all Jupiter trojans , 624 Hektor .","prompt_labels":"August(B-scientist) Kopff(I-scientist) ,(O) a(O) colleague(O) of(O) Wolf(B-scientist) at(O) Heidelberg(B-location) ,(O) then(O) discovered(O) 617(B-astronomical object) Patroclus(I-astronomical object) eight(O) months(O) after(O) Achilles(B-astronomical object) ,(O) and(O) ,(O) in(O) early(O) 1907(O) ,(O) he(O) discovered(O) the(O) largest(O) of(O) all(O) Jupiter(O) trojans(O) ,(O) 624(B-astronomical object) Hektor(I-astronomical object) .(O)"}}
{"id":"2","dataset":"crossner_science","split":"train","label_list":["enzyme","location","university","theory","person","country","discipline","academic journal","protein","event","chemical element","chemical compound","award","scientist","astronomical object","organization"],"instance":{"id":"2","words":["The","five","bodies","currently","called","planets","that","were","known","to","the","Greeks","were","those","visible","to","the","naked","eye",":","Mercury",",","Venus",",","Mars",",","Jupiter",",","and","Saturn","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","B-astronomical object","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, location, university, theory, person, country, discipline, academic journal, protein, event, chemical element, chemical compound, award, scientist, astronomical object, organization and O.\nSentence: The five bodies currently called planets that were known to the Greeks were those visible to the naked eye : Mercury , Venus , Mars , Jupiter , and Saturn .","prompt_labels":"The(O) five(O) bodies(O) currently(O) called(O) planets(O) that(O) were(O) known(O) to(O) the(O) Greeks(O) were(O) those(O) visible(O) to(O) the(O) naked(O) eye(O) :(O) Mercury(B-astronomical object) ,(O) Venus(B-astronomical object) ,(O) Mars(B-astronomical object) ,(O) Jupiter(B-astronomical object) ,(O) and(O) Saturn(B-astronomical object) .(O)"}}
{"id":"3","dataset":"crossner_science","split":"train","label_list":["country","scientist","academic journal","university","organization","astronomical object","chemical compound","enzyme","discipline","location","chemical element","event","person","theory","protein","award"],"instance":{"id":"3","words":["Several","genes","known","to","be","affected","by","differential","methylation","are","the","CYP1A1","xenobiotic","response","element",",","Aryl","hydrocarbon","receptor","repressor",",","and","F2RL3","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, scientist, academic journal, university, organization, astronomical object, chemical compound, enzyme, discipline, location, chemical element, event, person, theory, protein, award and O.\nSentence: Several genes known to be affected by differential methylation are the CYP1A1 xenobiotic response element , Aryl hydrocarbon receptor repressor , and F2RL3 .","prompt_labels":"Several(O) genes(O) known(O) to(O) be(O) affected(O) by(O) differential(O) methylation(O) are(O) the(O) CYP1A1(B-protein) xenobiotic(O) response(O) element(O) ,(O) Aryl(O) hydrocarbon(O) receptor(O) repressor(O) ,(O) and(O) F2RL3(O) .(O)"}}
{"id":"4","dataset":"crossner_science","split":"train","label_list":["academic journal","scientist","award","country","discipline","astronomical object","enzyme","university","person","organization","location","theory","protein","event","chemical compound","chemical element"],"instance":{"id":"4","words":["Hall","also","parodied","the","song","as","Cell","Black","Django","using","celebrities","NeNe","Leakes","(","as","the","master","of","ceremonies",")",",","Nicki","Minaj",",","Rihanna",",","Solange","Knowles",",","Beyoncé",",","internet","meme","Sharkeisha","(","which","he","portrayed",")",",","and","Mariah","Carey","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","O","B-person","I-person","O","B-person","O","O","O","B-person","O","O","O","O","O","O","O","B-person","I-person","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, scientist, award, country, discipline, astronomical object, enzyme, university, person, organization, location, theory, protein, event, chemical compound, chemical element and O.\nSentence: Hall also parodied the song as Cell Black Django using celebrities NeNe Leakes ( as the master of ceremonies ) , Nicki Minaj , Rihanna , Solange Knowles , Beyoncé , internet meme Sharkeisha ( which he portrayed ) , and Mariah Carey .","prompt_labels":"Hall(B-person) also(O) parodied(O) the(O) song(O) as(O) Cell(O) Black(O) Django(O) using(O) celebrities(O) NeNe(B-person) Leakes(I-person) ((O) as(O) the(O) master(O) of(O) ceremonies(O) )(O) ,(O) Nicki(B-person) Minaj(I-person) ,(O) Rihanna(B-person) ,(O) Solange(B-person) Knowles(I-person) ,(O) Beyoncé(B-person) ,(O) internet(O) meme(O) Sharkeisha(B-person) ((O) which(O) he(O) portrayed(O) )(O) ,(O) and(O) Mariah(B-person) Carey(I-person) .(O)"}}
{"id":"5","dataset":"crossner_science","split":"train","label_list":["theory","discipline","organization","location","country","chemical compound","university","award","scientist","person","event","chemical element","enzyme","academic journal","astronomical object","protein"],"instance":{"id":"5","words":["Also",",","Mercury","'s","fairly","eccentric","orbit","makes","it","much","easier","to","detect","the","perihelion","shift","than","is","the","case","for","the","nearly","circular","orbits","of","Venus","and","Earth","."],"labels":["O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, discipline, organization, location, country, chemical compound, university, award, scientist, person, event, chemical element, enzyme, academic journal, astronomical object, protein and O.\nSentence: Also , Mercury 's fairly eccentric orbit makes it much easier to detect the perihelion shift than is the case for the nearly circular orbits of Venus and Earth .","prompt_labels":"Also(O) ,(O) Mercury(B-astronomical object) 's(O) fairly(O) eccentric(O) orbit(O) makes(O) it(O) much(O) easier(O) to(O) detect(O) the(O) perihelion(O) shift(O) than(O) is(O) the(O) case(O) for(O) the(O) nearly(O) circular(O) orbits(O) of(O) Venus(B-astronomical object) and(O) Earth(B-astronomical object) .(O)"}}
{"id":"6","dataset":"crossner_science","split":"train","label_list":["academic journal","theory","discipline","enzyme","scientist","university","organization","chemical compound","event","chemical element","location","astronomical object","person","protein","award","country"],"instance":{"id":"6","words":["In","1926",",","Mark","C","Lidwill","of","the","Royal","Prince","Alfred","Hospital","of","Sydney",",","supported","by","physicist","Edgar","H.","Booth","of","the","University","of","Sydney",",","devised","a","portable","apparatus","which","plugged","into","a","lighting","point","and","in","which","One","pole","was","applied","to","a","skin","pad","soaked","in","strong","salt","solution","while","the","other","pole","consisted","of","a","needle","insulated","except","at","its","point",",","and","was","plunged","into","the","appropriate","cardiac","chamber","."],"labels":["O","O","O","B-person","I-person","I-person","O","O","B-location","I-location","I-location","I-location","O","B-location","O","O","O","O","B-scientist","I-scientist","I-scientist","O","O","B-university","I-university","I-university","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, theory, discipline, enzyme, scientist, university, organization, chemical compound, event, chemical element, location, astronomical object, person, protein, award, country and O.\nSentence: In 1926 , Mark C Lidwill of the Royal Prince Alfred Hospital of Sydney , supported by physicist Edgar H. Booth of the University of Sydney , devised a portable apparatus which plugged into a lighting point and in which One pole was applied to a skin pad soaked in strong salt solution while the other pole consisted of a needle insulated except at its point , and was plunged into the appropriate cardiac chamber .","prompt_labels":"In(O) 1926(O) ,(O) Mark(B-person) C(I-person) Lidwill(I-person) of(O) the(O) Royal(B-location) Prince(I-location) Alfred(I-location) Hospital(I-location) of(O) Sydney(B-location) ,(O) supported(O) by(O) physicist(O) Edgar(B-scientist) H.(I-scientist) Booth(I-scientist) of(O) the(O) University(B-university) of(I-university) Sydney(I-university) ,(O) devised(O) a(O) portable(O) apparatus(O) which(O) plugged(O) into(O) a(O) lighting(O) point(O) and(O) in(O) which(O) One(O) pole(O) was(O) applied(O) to(O) a(O) skin(O) pad(O) soaked(O) in(O) strong(O) salt(O) solution(O) while(O) the(O) other(O) pole(O) consisted(O) of(O) a(O) needle(O) insulated(O) except(O) at(O) its(O) point(O) ,(O) and(O) was(O) plunged(O) into(O) the(O) appropriate(O) cardiac(O) chamber(O) .(O)"}}
{"id":"7","dataset":"crossner_science","split":"train","label_list":["person","scientist","protein","award","astronomical object","enzyme","chemical element","university","discipline","academic journal","organization","country","theory","location","event","chemical compound"],"instance":{"id":"7","words":["Nüsslein-Volhard","was","educated","at","the","University","of","Tübingen","where","she","earned","a","PhD","in","1974","for","research","into","Protein-DNA","interaction","s","and","the","binding","of","RNA","polymerase","in","Escherichia","coli","."],"labels":["B-scientist","O","O","O","O","B-university","I-university","I-university","O","O","O","O","O","O","O","O","O","O","B-theory","I-theory","O","O","O","O","O","B-enzyme","I-enzyme","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, scientist, protein, award, astronomical object, enzyme, chemical element, university, discipline, academic journal, organization, country, theory, location, event, chemical compound and O.\nSentence: Nüsslein-Volhard was educated at the University of Tübingen where she earned a PhD in 1974 for research into Protein-DNA interaction s and the binding of RNA polymerase in Escherichia coli .","prompt_labels":"Nüsslein-Volhard(B-scientist) was(O) educated(O) at(O) the(O) University(B-university) of(I-university) Tübingen(I-university) where(O) she(O) earned(O) a(O) PhD(O) in(O) 1974(O) for(O) research(O) into(O) Protein-DNA(B-theory) interaction(I-theory) s(O) and(O) the(O) binding(O) of(O) RNA(B-enzyme) polymerase(I-enzyme) in(O) Escherichia(O) coli(O) .(O)"}}
{"id":"8","dataset":"crossner_science","split":"train","label_list":["country","discipline","location","academic journal","organization","chemical element","award","chemical compound","theory","protein","event","person","scientist","university","enzyme","astronomical object"],"instance":{"id":"8","words":["He","died",",","aged","78",",","on","August","5",",","2013",",","at","the","University","of","Maryland","Medical","Center","'","s","R","Adams","Cowley","Shock","Trauma","Center","from","complications","following","a","fall","at","his","home","in","Annapolis",",","MD","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, discipline, location, academic journal, organization, chemical element, award, chemical compound, theory, protein, event, person, scientist, university, enzyme, astronomical object and O.\nSentence: He died , aged 78 , on August 5 , 2013 , at the University of Maryland Medical Center ' s R Adams Cowley Shock Trauma Center from complications following a fall at his home in Annapolis , MD .","prompt_labels":"He(O) died(O) ,(O) aged(O) 78(O) ,(O) on(O) August(O) 5(O) ,(O) 2013(O) ,(O) at(O) the(O) University(B-organization) of(I-organization) Maryland(I-organization) Medical(I-organization) Center(I-organization) '(O) s(O) R(B-organization) Adams(I-organization) Cowley(I-organization) Shock(I-organization) Trauma(I-organization) Center(I-organization) from(O) complications(O) following(O) a(O) fall(O) at(O) his(O) home(O) in(O) Annapolis(B-location) ,(O) MD(B-location) .(O)"}}
{"id":"9","dataset":"crossner_science","split":"train","label_list":["event","award","scientist","location","academic journal","country","organization","astronomical object","discipline","protein","university","theory","chemical element","person","chemical compound","enzyme"],"instance":{"id":"9","words":["There","were","three","names","on","the","list",":","Werner","Heisenberg",",","who","received","the","Nobel","Prize","in","Physics","in","1932",",","Peter","Debye",",","who","would","receive","the","Nobel","Prize","in","Chemistry","in","1936",",","and","Richard","Becker","-","all","former","students","of","Sommerfeld","."],"labels":["O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","B-scientist","I-scientist","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","B-scientist","I-scientist","O","O","O","O","O","B-scientist","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, scientist, location, academic journal, country, organization, astronomical object, discipline, protein, university, theory, chemical element, person, chemical compound, enzyme and O.\nSentence: There were three names on the list : Werner Heisenberg , who received the Nobel Prize in Physics in 1932 , Peter Debye , who would receive the Nobel Prize in Chemistry in 1936 , and Richard Becker - all former students of Sommerfeld .","prompt_labels":"There(O) were(O) three(O) names(O) on(O) the(O) list(O) :(O) Werner(B-scientist) Heisenberg(I-scientist) ,(O) who(O) received(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Physics(I-award) in(O) 1932(O) ,(O) Peter(B-scientist) Debye(I-scientist) ,(O) who(O) would(O) receive(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Chemistry(I-award) in(O) 1936(O) ,(O) and(O) Richard(B-scientist) Becker(I-scientist) -(O) all(O) former(O) students(O) of(O) Sommerfeld(B-scientist) .(O)"}}
{"id":"10","dataset":"crossner_science","split":"train","label_list":["academic journal","country","scientist","chemical element","event","theory","chemical compound","award","university","discipline","protein","organization","location","enzyme","astronomical object","person"],"instance":{"id":"10","words":["DNA","methyltransferase","is","recruited","to","the","site","and","adds","methyl","groups","to","the","cytosine","of","the","CpG","dinucleotides","."],"labels":["B-enzyme","I-enzyme","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, country, scientist, chemical element, event, theory, chemical compound, award, university, discipline, protein, organization, location, enzyme, astronomical object, person and O.\nSentence: DNA methyltransferase is recruited to the site and adds methyl groups to the cytosine of the CpG dinucleotides .","prompt_labels":"DNA(B-enzyme) methyltransferase(I-enzyme) is(O) recruited(O) to(O) the(O) site(O) and(O) adds(O) methyl(O) groups(O) to(O) the(O) cytosine(B-chemical compound) of(O) the(O) CpG(O) dinucleotides(O) .(O)"}}
{"id":"11","dataset":"crossner_science","split":"train","label_list":["astronomical object","event","protein","chemical element","academic journal","person","theory","scientist","enzyme","location","chemical compound","award","university","organization","country","discipline"],"instance":{"id":"11","words":["He","also","studies","the","Kuiper","Belt","and","transitional","objects","such","as","2060","Chiron","and","5145","Pholus",",","as","well","as","the","occasional","comets","as","with","the","recent","Deep","impact","mission","that","travelled","to","Comet","Tempel","1",",","and","near-Earth","asteroids","with","the","occasional","use","of","the","Hubble","and","Spitzer","Space","Telescope","s","."],"labels":["O","O","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, event, protein, chemical element, academic journal, person, theory, scientist, enzyme, location, chemical compound, award, university, organization, country, discipline and O.\nSentence: He also studies the Kuiper Belt and transitional objects such as 2060 Chiron and 5145 Pholus , as well as the occasional comets as with the recent Deep impact mission that travelled to Comet Tempel 1 , and near-Earth asteroids with the occasional use of the Hubble and Spitzer Space Telescope s .","prompt_labels":"He(O) also(O) studies(O) the(O) Kuiper(B-astronomical object) Belt(I-astronomical object) and(O) transitional(O) objects(O) such(O) as(O) 2060(B-astronomical object) Chiron(I-astronomical object) and(O) 5145(B-astronomical object) Pholus(I-astronomical object) ,(O) as(O) well(O) as(O) the(O) occasional(O) comets(O) as(O) with(O) the(O) recent(O) Deep(O) impact(O) mission(O) that(O) travelled(O) to(O) Comet(B-astronomical object) Tempel(I-astronomical object) 1(I-astronomical object) ,(O) and(O) near-Earth(O) asteroids(O) with(O) the(O) occasional(O) use(O) of(O) the(O) Hubble(O) and(O) Spitzer(O) Space(O) Telescope(O) s(O) .(O)"}}
{"id":"12","dataset":"crossner_science","split":"train","label_list":["protein","academic journal","chemical compound","country","location","event","university","chemical element","organization","theory","enzyme","astronomical object","award","scientist","person","discipline"],"instance":{"id":"12","words":["The","widely","used","iGluSnFR","consists","of","a","circularly","permuted","Green","fluorescent","protein","fused","to","a","glutamate","binding","protein","(","GluBP",")","from","a","bacterium","."],"labels":["O","O","O","O","O","O","O","O","O","B-protein","I-protein","I-protein","O","O","O","B-protein","I-protein","I-protein","O","B-protein","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, academic journal, chemical compound, country, location, event, university, chemical element, organization, theory, enzyme, astronomical object, award, scientist, person, discipline and O.\nSentence: The widely used iGluSnFR consists of a circularly permuted Green fluorescent protein fused to a glutamate binding protein ( GluBP ) from a bacterium .","prompt_labels":"The(O) widely(O) used(O) iGluSnFR(O) consists(O) of(O) a(O) circularly(O) permuted(O) Green(B-protein) fluorescent(I-protein) protein(I-protein) fused(O) to(O) a(O) glutamate(B-protein) binding(I-protein) protein(I-protein) ((O) GluBP(B-protein) )(O) from(O) a(O) bacterium(O) .(O)"}}
{"id":"13","dataset":"crossner_science","split":"train","label_list":["protein","enzyme","person","academic journal","theory","location","country","chemical element","organization","discipline","astronomical object","event","university","chemical compound","award","scientist"],"instance":{"id":"13","words":["He","was","research","assistant","to","Richard","Harrison","at","the","Royal","London","Hospital",",","then","worked","with","J.","D.","H.","Slater","at","the","Middlesex","Hospital","from","1973","to","1974","and","with","A.","Grabham","at","Kettering","General","Hospital","for","a","year",",","before","being","appointed","as","assistant","lecturer","and","honorary","senior","house","orderly","registrar","in","the","School","of","Pathology","at","Middlesex","Hospital","Medical","School","(","1975","to","1976",")","."],"labels":["O","O","O","O","O","B-person","I-person","O","O","B-organization","I-organization","I-organization","O","O","O","O","B-person","I-person","I-person","I-person","O","O","B-organization","I-organization","O","O","O","O","O","O","B-person","I-person","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, enzyme, person, academic journal, theory, location, country, chemical element, organization, discipline, astronomical object, event, university, chemical compound, award, scientist and O.\nSentence: He was research assistant to Richard Harrison at the Royal London Hospital , then worked with J. D. H. Slater at the Middlesex Hospital from 1973 to 1974 and with A. Grabham at Kettering General Hospital for a year , before being appointed as assistant lecturer and honorary senior house orderly registrar in the School of Pathology at Middlesex Hospital Medical School ( 1975 to 1976 ) .","prompt_labels":"He(O) was(O) research(O) assistant(O) to(O) Richard(B-person) Harrison(I-person) at(O) the(O) Royal(B-organization) London(I-organization) Hospital(I-organization) ,(O) then(O) worked(O) with(O) J.(B-person) D.(I-person) H.(I-person) Slater(I-person) at(O) the(O) Middlesex(B-organization) Hospital(I-organization) from(O) 1973(O) to(O) 1974(O) and(O) with(O) A.(B-person) Grabham(I-person) at(O) Kettering(B-organization) General(I-organization) Hospital(I-organization) for(O) a(O) year(O) ,(O) before(O) being(O) appointed(O) as(O) assistant(O) lecturer(O) and(O) honorary(O) senior(O) house(O) orderly(O) registrar(O) in(O) the(B-organization) School(I-organization) of(I-organization) Pathology(I-organization) at(I-organization) Middlesex(I-organization) Hospital(I-organization) Medical(I-organization) School(I-organization) ((O) 1975(O) to(O) 1976(O) )(O) .(O)"}}
{"id":"14","dataset":"crossner_science","split":"train","label_list":["enzyme","event","award","astronomical object","protein","theory","scientist","country","university","chemical compound","academic journal","chemical element","location","discipline","organization","person"],"instance":{"id":"14","words":["In","the","20th","century",",","DuPont","developed","many","polymer","s","such","as","Vespel",",","neoprene",",","nylon",",","Corian",",","Polytetrafluoroethylene",",","Mylar",",","Kapton",",","Kevlar",",","Zemdrain",",","M5","fiber",",","Nomex",",","Tyvek",",","Sorona",",","Corfam",",","and","Lycra","."],"labels":["O","O","O","O","O","B-organization","O","O","O","O","O","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","I-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","O","B-chemical compound","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, event, award, astronomical object, protein, theory, scientist, country, university, chemical compound, academic journal, chemical element, location, discipline, organization, person and O.\nSentence: In the 20th century , DuPont developed many polymer s such as Vespel , neoprene , nylon , Corian , Polytetrafluoroethylene , Mylar , Kapton , Kevlar , Zemdrain , M5 fiber , Nomex , Tyvek , Sorona , Corfam , and Lycra .","prompt_labels":"In(O) the(O) 20th(O) century(O) ,(O) DuPont(B-organization) developed(O) many(O) polymer(O) s(O) such(O) as(O) Vespel(B-chemical compound) ,(O) neoprene(B-chemical compound) ,(O) nylon(B-chemical compound) ,(O) Corian(B-chemical compound) ,(O) Polytetrafluoroethylene(B-chemical compound) ,(O) Mylar(B-chemical compound) ,(O) Kapton(B-chemical compound) ,(O) Kevlar(B-chemical compound) ,(O) Zemdrain(B-chemical compound) ,(O) M5(B-chemical compound) fiber(I-chemical compound) ,(O) Nomex(B-chemical compound) ,(O) Tyvek(B-chemical compound) ,(O) Sorona(B-chemical compound) ,(O) Corfam(B-chemical compound) ,(O) and(O) Lycra(B-chemical compound) .(O)"}}
{"id":"15","dataset":"crossner_science","split":"train","label_list":["scientist","location","person","protein","award","academic journal","chemical compound","organization","university","enzyme","chemical element","astronomical object","event","country","theory","discipline"],"instance":{"id":"15","words":["Under","the","influence","of","the","new","empirical","methods","propounded","by","Sir","Francis","Bacon","and","others",",","a","group","of","chemists","at","Oxford",",","Robert","Boyle",",","Robert","Hooke","and","John","Mayow","began","to","reshape","the","old","alchemical","traditions","into","a","scientific","discipline","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","B-university","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, location, person, protein, award, academic journal, chemical compound, organization, university, enzyme, chemical element, astronomical object, event, country, theory, discipline and O.\nSentence: Under the influence of the new empirical methods propounded by Sir Francis Bacon and others , a group of chemists at Oxford , Robert Boyle , Robert Hooke and John Mayow began to reshape the old alchemical traditions into a scientific discipline .","prompt_labels":"Under(O) the(O) influence(O) of(O) the(O) new(O) empirical(O) methods(O) propounded(O) by(O) Sir(O) Francis(B-scientist) Bacon(I-scientist) and(O) others(O) ,(O) a(O) group(O) of(O) chemists(O) at(O) Oxford(B-university) ,(O) Robert(B-scientist) Boyle(I-scientist) ,(O) Robert(B-scientist) Hooke(I-scientist) and(O) John(B-scientist) Mayow(I-scientist) began(O) to(O) reshape(O) the(O) old(O) alchemical(O) traditions(O) into(O) a(O) scientific(O) discipline(O) .(O)"}}
{"id":"16","dataset":"crossner_science","split":"train","label_list":["chemical compound","event","academic journal","scientist","theory","enzyme","country","astronomical object","person","chemical element","discipline","university","organization","location","award","protein"],"instance":{"id":"16","words":["The","Thimble","Tickle","squid","was","found","aground","offshore",",","alive",",","on","2","November","1878",",","near","Little","Bay","Copper","Mine",",","Thimble","Tickle","Bay",",","Notre","Dame","Bay",",","Newfoundland","Colony","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","B-location","I-location","I-location","O","B-location","I-location","I-location","O","B-country","I-country","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, event, academic journal, scientist, theory, enzyme, country, astronomical object, person, chemical element, discipline, university, organization, location, award, protein and O.\nSentence: The Thimble Tickle squid was found aground offshore , alive , on 2 November 1878 , near Little Bay Copper Mine , Thimble Tickle Bay , Notre Dame Bay , Newfoundland Colony .","prompt_labels":"The(O) Thimble(O) Tickle(O) squid(O) was(O) found(O) aground(O) offshore(O) ,(O) alive(O) ,(O) on(O) 2(O) November(O) 1878(O) ,(O) near(O) Little(B-location) Bay(I-location) Copper(I-location) Mine(I-location) ,(O) Thimble(B-location) Tickle(I-location) Bay(I-location) ,(O) Notre(B-location) Dame(I-location) Bay(I-location) ,(O) Newfoundland(B-country) Colony(I-country) .(O)"}}
{"id":"17","dataset":"crossner_science","split":"train","label_list":["country","award","protein","chemical compound","organization","enzyme","location","person","event","scientist","academic journal","chemical element","discipline","theory","astronomical object","university"],"instance":{"id":"17","words":["Spermidine","synthase","uses","putrescine","and","S-Adenosylmethioninamine","(","decarboxylated","S-Adenosyl","methionine",")","to","produce","spermidine","."],"labels":["B-enzyme","I-enzyme","O","B-chemical compound","O","B-chemical compound","O","O","B-chemical compound","I-chemical compound","O","O","O","B-chemical compound","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, protein, chemical compound, organization, enzyme, location, person, event, scientist, academic journal, chemical element, discipline, theory, astronomical object, university and O.\nSentence: Spermidine synthase uses putrescine and S-Adenosylmethioninamine ( decarboxylated S-Adenosyl methionine ) to produce spermidine .","prompt_labels":"Spermidine(B-enzyme) synthase(I-enzyme) uses(O) putrescine(B-chemical compound) and(O) S-Adenosylmethioninamine(B-chemical compound) ((O) decarboxylated(O) S-Adenosyl(B-chemical compound) methionine(I-chemical compound) )(O) to(O) produce(O) spermidine(B-chemical compound) .(O)"}}
{"id":"18","dataset":"crossner_science","split":"train","label_list":["astronomical object","academic journal","university","discipline","person","country","protein","location","chemical element","theory","scientist","chemical compound","event","enzyme","award","organization"],"instance":{"id":"18","words":["Organizations","such","as","Community","Forestry","International",",","Cool","Earth",",","The","Nature","Conservancy",",","World","Wide","Fund","for","Nature",",","Conservation","International",",","African","Conservation","Foundation","and","Greenpeace","also","focus","on","preserving","forest","habitats","."],"labels":["O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, academic journal, university, discipline, person, country, protein, location, chemical element, theory, scientist, chemical compound, event, enzyme, award, organization and O.\nSentence: Organizations such as Community Forestry International , Cool Earth , The Nature Conservancy , World Wide Fund for Nature , Conservation International , African Conservation Foundation and Greenpeace also focus on preserving forest habitats .","prompt_labels":"Organizations(O) such(O) as(O) Community(B-organization) Forestry(I-organization) International(I-organization) ,(O) Cool(B-organization) Earth(I-organization) ,(O) The(B-organization) Nature(I-organization) Conservancy(I-organization) ,(O) World(B-organization) Wide(I-organization) Fund(I-organization) for(I-organization) Nature(I-organization) ,(O) Conservation(B-organization) International(I-organization) ,(O) African(B-organization) Conservation(I-organization) Foundation(I-organization) and(O) Greenpeace(B-organization) also(O) focus(O) on(O) preserving(O) forest(O) habitats(O) .(O)"}}
{"id":"19","dataset":"crossner_science","split":"train","label_list":["award","organization","chemical compound","academic journal","discipline","protein","enzyme","event","chemical element","university","astronomical object","location","person","country","scientist","theory"],"instance":{"id":"19","words":["Tyas","'","5","recent","consecutive","high","rankings","within","the","highly","coveted","DJ","Mag","Top","100","poll","are","a","result","of","his","dynamic","DJ","performances","at","some","of","the","largest","clubs","and","festivals","in","the","world",",","including","Tomorrowland",",","A","State","Of","Trance",",","Ultra","Music","Festival",",","Electric","Daisy","Carnival",",","Electric","Zoo",",","Godskitchen",",","Gatecrasher",",","Ministry","Of","Sound",",","Beyond","Wonderland",",","Avalon","Hollywood",",","Privilege","Ibiza",",","and","many","more","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","O","B-event","I-event","I-event","I-event","O","B-event","I-event","I-event","O","B-event","I-event","I-event","O","B-event","I-event","O","B-organization","O","B-event","O","B-organization","I-organization","I-organization","O","B-event","I-event","O","B-organization","I-organization","O","B-organization","I-organization","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, chemical compound, academic journal, discipline, protein, enzyme, event, chemical element, university, astronomical object, location, person, country, scientist, theory and O.\nSentence: Tyas ' 5 recent consecutive high rankings within the highly coveted DJ Mag Top 100 poll are a result of his dynamic DJ performances at some of the largest clubs and festivals in the world , including Tomorrowland , A State Of Trance , Ultra Music Festival , Electric Daisy Carnival , Electric Zoo , Godskitchen , Gatecrasher , Ministry Of Sound , Beyond Wonderland , Avalon Hollywood , Privilege Ibiza , and many more .","prompt_labels":"Tyas(B-person) '(O) 5(O) recent(O) consecutive(O) high(O) rankings(O) within(O) the(O) highly(O) coveted(O) DJ(O) Mag(O) Top(O) 100(O) poll(O) are(O) a(O) result(O) of(O) his(O) dynamic(O) DJ(O) performances(O) at(O) some(O) of(O) the(O) largest(O) clubs(O) and(O) festivals(O) in(O) the(O) world(O) ,(O) including(O) Tomorrowland(B-event) ,(O) A(B-event) State(I-event) Of(I-event) Trance(I-event) ,(O) Ultra(B-event) Music(I-event) Festival(I-event) ,(O) Electric(B-event) Daisy(I-event) Carnival(I-event) ,(O) Electric(B-event) Zoo(I-event) ,(O) Godskitchen(B-organization) ,(O) Gatecrasher(B-event) ,(O) Ministry(B-organization) Of(I-organization) Sound(I-organization) ,(O) Beyond(B-event) Wonderland(I-event) ,(O) Avalon(B-organization) Hollywood(I-organization) ,(O) Privilege(B-organization) Ibiza(I-organization) ,(O) and(O) many(O) more(O) .(O)"}}
{"id":"20","dataset":"crossner_science","split":"train","label_list":["enzyme","country","person","organization","protein","event","discipline","theory","astronomical object","chemical element","location","chemical compound","university","award","scientist","academic journal"],"instance":{"id":"20","words":["He","has","been","elected","a","Fellow","of","the","American","Physical","Society",",","the","Chinese","Academy","of","Sciences",",","the","Academia","Sinica",",","the","Russian","Academy","of","Sciences",",","and","the","Royal","Society","."],"labels":["O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, country, person, organization, protein, event, discipline, theory, astronomical object, chemical element, location, chemical compound, university, award, scientist, academic journal and O.\nSentence: He has been elected a Fellow of the American Physical Society , the Chinese Academy of Sciences , the Academia Sinica , the Russian Academy of Sciences , and the Royal Society .","prompt_labels":"He(O) has(O) been(O) elected(O) a(O) Fellow(B-award) of(I-award) the(I-award) American(I-award) Physical(I-award) Society(I-award) ,(O) the(O) Chinese(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) the(O) Academia(B-organization) Sinica(I-organization) ,(O) the(O) Russian(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) and(O) the(O) Royal(B-organization) Society(I-organization) .(O)"}}
{"id":"21","dataset":"crossner_science","split":"train","label_list":["scientist","theory","academic journal","university","organization","person","country","chemical compound","protein","event","award","astronomical object","discipline","enzyme","location","chemical element"],"instance":{"id":"21","words":["The","author","of","the","Dictionary","of","Minor","Planet","Names",",","Lutz","D.","Schmadel",",","contacted","Italian","astronomer","Paul","G.","Comba",",","who","confirmed","that","this","naming","was","another","clear","instance","of","homage","to","him","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","O","O","O","O","B-scientist","I-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, theory, academic journal, university, organization, person, country, chemical compound, protein, event, award, astronomical object, discipline, enzyme, location, chemical element and O.\nSentence: The author of the Dictionary of Minor Planet Names , Lutz D. Schmadel , contacted Italian astronomer Paul G. Comba , who confirmed that this naming was another clear instance of homage to him .","prompt_labels":"The(O) author(O) of(O) the(O) Dictionary(O) of(O) Minor(O) Planet(O) Names(O) ,(O) Lutz(B-person) D.(I-person) Schmadel(I-person) ,(O) contacted(O) Italian(O) astronomer(O) Paul(B-scientist) G.(I-scientist) Comba(I-scientist) ,(O) who(O) confirmed(O) that(O) this(O) naming(O) was(O) another(O) clear(O) instance(O) of(O) homage(O) to(O) him(O) .(O)"}}
{"id":"22","dataset":"crossner_science","split":"train","label_list":["discipline","protein","location","astronomical object","person","academic journal","university","country","enzyme","chemical element","chemical compound","scientist","event","theory","award","organization"],"instance":{"id":"22","words":["Liver","function","test","s","may","be","elevated",",","particularly","involving","Gamma-glutamyltransferase","and","Alkaline","phosphatase",",","with","ultrasound","and","CT","scans","being","considered","medical","imaging","investigations","of","choice","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-protein","O","B-enzyme","I-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, protein, location, astronomical object, person, academic journal, university, country, enzyme, chemical element, chemical compound, scientist, event, theory, award, organization and O.\nSentence: Liver function test s may be elevated , particularly involving Gamma-glutamyltransferase and Alkaline phosphatase , with ultrasound and CT scans being considered medical imaging investigations of choice .","prompt_labels":"Liver(O) function(O) test(O) s(O) may(O) be(O) elevated(O) ,(O) particularly(O) involving(O) Gamma-glutamyltransferase(B-protein) and(O) Alkaline(B-enzyme) phosphatase(I-enzyme) ,(O) with(O) ultrasound(O) and(O) CT(O) scans(O) being(O) considered(O) medical(O) imaging(O) investigations(O) of(O) choice(O) .(O)"}}
{"id":"23","dataset":"crossner_science","split":"train","label_list":["enzyme","theory","academic journal","location","discipline","award","astronomical object","university","scientist","person","chemical element","protein","organization","chemical compound","country","event"],"instance":{"id":"23","words":["Eigen","received","his","Ph.D.","at","the","University","of","Göttingen","in","1951","under","supervision","of","Arnold","Eucken","."],"labels":["B-scientist","O","O","O","O","O","B-university","I-university","I-university","O","O","O","O","O","B-scientist","I-scientist","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, theory, academic journal, location, discipline, award, astronomical object, university, scientist, person, chemical element, protein, organization, chemical compound, country, event and O.\nSentence: Eigen received his Ph.D. at the University of Göttingen in 1951 under supervision of Arnold Eucken .","prompt_labels":"Eigen(B-scientist) received(O) his(O) Ph.D.(O) at(O) the(O) University(B-university) of(I-university) Göttingen(I-university) in(O) 1951(O) under(O) supervision(O) of(O) Arnold(B-scientist) Eucken(I-scientist) .(O)"}}
{"id":"24","dataset":"crossner_science","split":"train","label_list":["scientist","event","country","protein","theory","organization","chemical element","enzyme","discipline","chemical compound","person","academic journal","astronomical object","location","university","award"],"instance":{"id":"24","words":["In","1831",",","Michael","Faraday","made","the","seminal","observation","that","time-varying","magnetic","fields","could","induce","electric","currents","and","then",",","in","1864",",","James","Clerk","Maxwell","published","his","famous","paper","A","Dynamical","Theory","of","the","Electromagnetic","Field",".","Maxwell","1864","5",",","page","499",";","also","David","J.","Griffiths","(","1999",")",",","Introduction","to","electrodynamics",",","third","Edition",",","ed","."],"labels":["O","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","O","O","O","O","O","B-discipline","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, event, country, protein, theory, organization, chemical element, enzyme, discipline, chemical compound, person, academic journal, astronomical object, location, university, award and O.\nSentence: In 1831 , Michael Faraday made the seminal observation that time-varying magnetic fields could induce electric currents and then , in 1864 , James Clerk Maxwell published his famous paper A Dynamical Theory of the Electromagnetic Field . Maxwell 1864 5 , page 499 ; also David J. Griffiths ( 1999 ) , Introduction to electrodynamics , third Edition , ed .","prompt_labels":"In(O) 1831(O) ,(O) Michael(B-scientist) Faraday(I-scientist) made(O) the(O) seminal(O) observation(O) that(O) time-varying(O) magnetic(O) fields(O) could(O) induce(O) electric(O) currents(O) and(O) then(O) ,(O) in(O) 1864(O) ,(O) James(B-scientist) Clerk(I-scientist) Maxwell(I-scientist) published(O) his(O) famous(O) paper(O) A(O) Dynamical(O) Theory(O) of(O) the(O) Electromagnetic(O) Field(O) .(O) Maxwell(B-scientist) 1864(O) 5(O) ,(O) page(O) 499(O) ;(O) also(O) David(B-scientist) J.(I-scientist) Griffiths(I-scientist) ((O) 1999(O) )(O) ,(O) Introduction(O) to(O) electrodynamics(B-discipline) ,(O) third(O) Edition(O) ,(O) ed(O) .(O)"}}
{"id":"25","dataset":"crossner_science","split":"train","label_list":["theory","university","country","chemical compound","location","protein","enzyme","award","chemical element","scientist","academic journal","event","astronomical object","organization","person","discipline"],"instance":{"id":"25","words":["Still","within","the","same","PCR","tube",",","overnight","IVT","reaction","is","assembled",",","including","standard","IVT","buffer",",","NTPs",",","T7","RNA","polymerase",",","RNase","inhibitor",",","DMSO",",","etc","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-enzyme","I-enzyme","I-enzyme","O","B-protein","I-protein","O","B-chemical compound","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, university, country, chemical compound, location, protein, enzyme, award, chemical element, scientist, academic journal, event, astronomical object, organization, person, discipline and O.\nSentence: Still within the same PCR tube , overnight IVT reaction is assembled , including standard IVT buffer , NTPs , T7 RNA polymerase , RNase inhibitor , DMSO , etc .","prompt_labels":"Still(O) within(O) the(O) same(O) PCR(O) tube(O) ,(O) overnight(O) IVT(O) reaction(O) is(O) assembled(O) ,(O) including(O) standard(O) IVT(O) buffer(O) ,(O) NTPs(O) ,(O) T7(B-enzyme) RNA(I-enzyme) polymerase(I-enzyme) ,(O) RNase(B-protein) inhibitor(I-protein) ,(O) DMSO(B-chemical compound) ,(O) etc(O) .(O)"}}
{"id":"26","dataset":"crossner_science","split":"train","label_list":["protein","country","discipline","person","chemical element","organization","university","theory","location","chemical compound","scientist","event","enzyme","award","academic journal","astronomical object"],"instance":{"id":"26","words":["Based","on","spectroscopy",",","Saturn","is","thought","to","be","similar","in","composition","to","Jupiter",",","but","the","other","giant","planets","Uranus","and","Neptune","have","relatively","less","hydrogen","and","helium","and","relatively","more","ices","and","are","thus","now","termed","ice","giant","s","."],"labels":["O","O","B-discipline","O","B-astronomical object","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","B-chemical element","O","B-chemical element","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, country, discipline, person, chemical element, organization, university, theory, location, chemical compound, scientist, event, enzyme, award, academic journal, astronomical object and O.\nSentence: Based on spectroscopy , Saturn is thought to be similar in composition to Jupiter , but the other giant planets Uranus and Neptune have relatively less hydrogen and helium and relatively more ices and are thus now termed ice giant s .","prompt_labels":"Based(O) on(O) spectroscopy(B-discipline) ,(O) Saturn(B-astronomical object) is(O) thought(O) to(O) be(O) similar(O) in(O) composition(O) to(O) Jupiter(B-astronomical object) ,(O) but(O) the(O) other(O) giant(O) planets(O) Uranus(B-astronomical object) and(O) Neptune(B-astronomical object) have(O) relatively(O) less(O) hydrogen(B-chemical element) and(O) helium(B-chemical element) and(O) relatively(O) more(O) ices(O) and(O) are(O) thus(O) now(O) termed(O) ice(O) giant(O) s(O) .(O)"}}
{"id":"27","dataset":"crossner_science","split":"train","label_list":["enzyme","country","university","protein","scientist","chemical compound","theory","discipline","event","location","person","organization","astronomical object","chemical element","award","academic journal"],"instance":{"id":"27","words":["Julius","Tafel","discovered","that","hydroxylamine","hydrochloride","or","sulfate","salts","can","be","produced","by","electrolytic","reduction","of","nitric","acid","with","Hydrochloric","acid","or","Sulfuric","acid","respectively",":"],"labels":["B-scientist","I-scientist","O","O","B-chemical compound","I-chemical compound","O","O","O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, country, university, protein, scientist, chemical compound, theory, discipline, event, location, person, organization, astronomical object, chemical element, award, academic journal and O.\nSentence: Julius Tafel discovered that hydroxylamine hydrochloride or sulfate salts can be produced by electrolytic reduction of nitric acid with Hydrochloric acid or Sulfuric acid respectively :","prompt_labels":"Julius(B-scientist) Tafel(I-scientist) discovered(O) that(O) hydroxylamine(B-chemical compound) hydrochloride(I-chemical compound) or(O) sulfate(O) salts(O) can(O) be(O) produced(O) by(O) electrolytic(O) reduction(O) of(O) nitric(B-chemical compound) acid(I-chemical compound) with(O) Hydrochloric(B-chemical compound) acid(I-chemical compound) or(O) Sulfuric(B-chemical compound) acid(I-chemical compound) respectively(O) :(O)"}}
{"id":"28","dataset":"crossner_science","split":"train","label_list":["academic journal","person","enzyme","scientist","theory","chemical element","event","location","chemical compound","university","award","protein","organization","country","discipline","astronomical object"],"instance":{"id":"28","words":["Lindsay","and","his","wife","Rachel","translated","Hans","Kramers","book",",","The","Atom","and","the","Bohr","Theory","of","its","Structure",",","in","1923",",","Hans","Kramers","&","amp",";","Helge","Holst",",","The","Atom","and","the","Bohr","Theory","of","its","Structure",":","An","Elementary","Presentation",",","Gyldendal",",","210pp","."],"labels":["B-person","O","O","O","B-person","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, person, enzyme, scientist, theory, chemical element, event, location, chemical compound, university, award, protein, organization, country, discipline, astronomical object and O.\nSentence: Lindsay and his wife Rachel translated Hans Kramers book , The Atom and the Bohr Theory of its Structure , in 1923 , Hans Kramers & amp ; Helge Holst , The Atom and the Bohr Theory of its Structure : An Elementary Presentation , Gyldendal , 210pp .","prompt_labels":"Lindsay(B-person) and(O) his(O) wife(O) Rachel(B-person) translated(O) Hans(B-scientist) Kramers(I-scientist) book(O) ,(O) The(O) Atom(O) and(O) the(O) Bohr(O) Theory(O) of(O) its(O) Structure(O) ,(O) in(O) 1923(O) ,(O) Hans(B-scientist) Kramers(I-scientist) &(O) amp(O) ;(O) Helge(B-scientist) Holst(I-scientist) ,(O) The(O) Atom(O) and(O) the(O) Bohr(O) Theory(O) of(O) its(O) Structure(O) :(O) An(O) Elementary(O) Presentation(O) ,(O) Gyldendal(B-organization) ,(O) 210pp(O) .(O)"}}
{"id":"29","dataset":"crossner_science","split":"train","label_list":["award","chemical element","university","scientist","chemical compound","enzyme","organization","country","astronomical object","location","theory","academic journal","protein","discipline","person","event"],"instance":{"id":"29","words":["Nicknames","(","e.g.","51","Pegasi","b",",","HD","209458","b",",","2018","VG18",")","should","not","be","used","as","article","titles",",","unless","they","have","become","the","common","name","in","the","scientific","literature","."],"labels":["O","O","O","B-astronomical object","I-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, chemical element, university, scientist, chemical compound, enzyme, organization, country, astronomical object, location, theory, academic journal, protein, discipline, person, event and O.\nSentence: Nicknames ( e.g. 51 Pegasi b , HD 209458 b , 2018 VG18 ) should not be used as article titles , unless they have become the common name in the scientific literature .","prompt_labels":"Nicknames(O) ((O) e.g.(O) 51(B-astronomical object) Pegasi(I-astronomical object) b(I-astronomical object) ,(O) HD(B-astronomical object) 209458(I-astronomical object) b(I-astronomical object) ,(O) 2018(B-astronomical object) VG18(I-astronomical object) )(O) should(O) not(O) be(O) used(O) as(O) article(O) titles(O) ,(O) unless(O) they(O) have(O) become(O) the(O) common(O) name(O) in(O) the(O) scientific(O) literature(O) .(O)"}}
{"id":"30","dataset":"crossner_science","split":"train","label_list":["scientist","protein","discipline","university","academic journal","location","chemical compound","event","organization","astronomical object","enzyme","person","theory","award","country","chemical element"],"instance":{"id":"30","words":["He","has","written","several","invited","review","articles","and","book","chapters","for","a","number","of","prestigious","journals","and","books","including","Chemical","Reviews",",","Accounts","of","Chemical","Research",",","Angewandte","Chemie","and","the","main","textbook","in","the","field","of","circular","dichroism",",","Comprehensive","Chiroptical","Spectroscopy","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","O","O","O","O","O","O","O","B-theory","I-theory","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, protein, discipline, university, academic journal, location, chemical compound, event, organization, astronomical object, enzyme, person, theory, award, country, chemical element and O.\nSentence: He has written several invited review articles and book chapters for a number of prestigious journals and books including Chemical Reviews , Accounts of Chemical Research , Angewandte Chemie and the main textbook in the field of circular dichroism , Comprehensive Chiroptical Spectroscopy .","prompt_labels":"He(O) has(O) written(O) several(O) invited(O) review(O) articles(O) and(O) book(O) chapters(O) for(O) a(O) number(O) of(O) prestigious(O) journals(O) and(O) books(O) including(O) Chemical(B-academic journal) Reviews(I-academic journal) ,(O) Accounts(B-academic journal) of(I-academic journal) Chemical(I-academic journal) Research(I-academic journal) ,(O) Angewandte(B-academic journal) Chemie(I-academic journal) and(O) the(O) main(O) textbook(O) in(O) the(O) field(O) of(O) circular(B-theory) dichroism(I-theory) ,(O) Comprehensive(O) Chiroptical(O) Spectroscopy(O) .(O)"}}
{"id":"31","dataset":"crossner_science","split":"train","label_list":["theory","country","university","astronomical object","scientist","academic journal","chemical element","enzyme","award","location","person","event","chemical compound","protein","discipline","organization"],"instance":{"id":"31","words":["In","order","to","get","a","meaningful","sulfur","signal","from","the","analysis",",","the","buffer","should","not","contain","sulfur","(","i.e.","no","BES",",","DDT",",","HEPES",",","MES",",","MOPS",",","or","PIPES","compounds",")","."],"labels":["O","O","O","O","O","O","B-chemical element","O","O","O","O","O","O","O","O","O","O","B-chemical element","O","O","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","O","B-chemical compound","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, country, university, astronomical object, scientist, academic journal, chemical element, enzyme, award, location, person, event, chemical compound, protein, discipline, organization and O.\nSentence: In order to get a meaningful sulfur signal from the analysis , the buffer should not contain sulfur ( i.e. no BES , DDT , HEPES , MES , MOPS , or PIPES compounds ) .","prompt_labels":"In(O) order(O) to(O) get(O) a(O) meaningful(O) sulfur(B-chemical element) signal(O) from(O) the(O) analysis(O) ,(O) the(O) buffer(O) should(O) not(O) contain(O) sulfur(B-chemical element) ((O) i.e.(O) no(O) BES(B-chemical compound) ,(O) DDT(B-chemical compound) ,(O) HEPES(B-chemical compound) ,(O) MES(B-chemical compound) ,(O) MOPS(B-chemical compound) ,(O) or(O) PIPES(B-chemical compound) compounds(O) )(O) .(O)"}}
{"id":"32","dataset":"crossner_science","split":"train","label_list":["astronomical object","theory","event","organization","chemical element","enzyme","award","academic journal","country","protein","discipline","scientist","person","chemical compound","university","location"],"instance":{"id":"32","words":["The","Oak","Ridge","Institute","for","Science","and","Education",",","operated","by","Oak","Ridge","Associated","Universities",",","conducts","research","and","education","programs","for","the","DOE",",","Department","of","Homeland","Security",",","and","other","federal","agencies","."],"labels":["O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-university","I-university","I-university","I-university","O","O","O","O","O","O","O","O","B-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, theory, event, organization, chemical element, enzyme, award, academic journal, country, protein, discipline, scientist, person, chemical compound, university, location and O.\nSentence: The Oak Ridge Institute for Science and Education , operated by Oak Ridge Associated Universities , conducts research and education programs for the DOE , Department of Homeland Security , and other federal agencies .","prompt_labels":"The(O) Oak(B-organization) Ridge(I-organization) Institute(I-organization) for(I-organization) Science(I-organization) and(I-organization) Education(I-organization) ,(O) operated(O) by(O) Oak(B-university) Ridge(I-university) Associated(I-university) Universities(I-university) ,(O) conducts(O) research(O) and(O) education(O) programs(O) for(O) the(O) DOE(B-organization) ,(O) Department(B-organization) of(I-organization) Homeland(I-organization) Security(I-organization) ,(O) and(O) other(O) federal(O) agencies(O) .(O)"}}
{"id":"33","dataset":"crossner_science","split":"train","label_list":["location","protein","organization","person","country","scientist","astronomical object","theory","university","chemical compound","chemical element","event","academic journal","enzyme","discipline","award"],"instance":{"id":"33","words":["After","students","at","Uppsala","University","celebrated","Bonaparte","'s","return","to","France","from","his","Italian","campaigns",",","Gustav","IV","Adolf","appointed","von","Fersen","as","Uppsala","University","."],"labels":["O","O","O","B-university","I-university","O","B-person","O","O","O","B-country","O","O","O","O","O","B-person","I-person","I-person","O","B-person","I-person","O","B-university","I-university","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, protein, organization, person, country, scientist, astronomical object, theory, university, chemical compound, chemical element, event, academic journal, enzyme, discipline, award and O.\nSentence: After students at Uppsala University celebrated Bonaparte 's return to France from his Italian campaigns , Gustav IV Adolf appointed von Fersen as Uppsala University .","prompt_labels":"After(O) students(O) at(O) Uppsala(B-university) University(I-university) celebrated(O) Bonaparte(B-person) 's(O) return(O) to(O) France(B-country) from(O) his(O) Italian(O) campaigns(O) ,(O) Gustav(B-person) IV(I-person) Adolf(I-person) appointed(O) von(B-person) Fersen(I-person) as(O) Uppsala(B-university) University(I-university) .(O)"}}
{"id":"34","dataset":"crossner_science","split":"train","label_list":["theory","organization","award","country","location","enzyme","university","scientist","protein","academic journal","person","event","discipline","astronomical object","chemical element","chemical compound"],"instance":{"id":"34","words":["Flexner","had","studied","European","schools","such","as","Heidelberg","University",",","All","Souls","College",",","Oxford",",","and","the","Collège","de","France","-and","he","wanted","to","establish","a","similar","advanced","research","center","in","the","United","States","."],"labels":["B-person","O","O","O","O","O","O","B-university","I-university","O","B-university","I-university","I-university","I-university","I-university","O","O","O","B-university","I-university","I-university","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, organization, award, country, location, enzyme, university, scientist, protein, academic journal, person, event, discipline, astronomical object, chemical element, chemical compound and O.\nSentence: Flexner had studied European schools such as Heidelberg University , All Souls College , Oxford , and the Collège de France -and he wanted to establish a similar advanced research center in the United States .","prompt_labels":"Flexner(B-person) had(O) studied(O) European(O) schools(O) such(O) as(O) Heidelberg(B-university) University(I-university) ,(O) All(B-university) Souls(I-university) College(I-university) ,(I-university) Oxford(I-university) ,(O) and(O) the(O) Collège(B-university) de(I-university) France(I-university) -and(O) he(O) wanted(O) to(O) establish(O) a(O) similar(O) advanced(O) research(O) center(O) in(O) the(O) United(B-country) States(I-country) .(O)"}}
{"id":"35","dataset":"crossner_science","split":"train","label_list":["chemical compound","event","scientist","person","theory","discipline","university","country","chemical element","protein","organization","location","award","academic journal","enzyme","astronomical object"],"instance":{"id":"35","words":["This","mechanism","is","evident","on","Jupiter","and","Saturn","and","on","brown","dwarf","s","whose","central","temperatures","are","not","high","enough","to","undergo","nuclear","fusion","."],"labels":["O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, event, scientist, person, theory, discipline, university, country, chemical element, protein, organization, location, award, academic journal, enzyme, astronomical object and O.\nSentence: This mechanism is evident on Jupiter and Saturn and on brown dwarf s whose central temperatures are not high enough to undergo nuclear fusion .","prompt_labels":"This(O) mechanism(O) is(O) evident(O) on(O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) and(O) on(O) brown(O) dwarf(O) s(O) whose(O) central(O) temperatures(O) are(O) not(O) high(O) enough(O) to(O) undergo(O) nuclear(O) fusion(O) .(O)"}}
{"id":"36","dataset":"crossner_science","split":"train","label_list":["academic journal","scientist","person","university","protein","enzyme","award","chemical element","theory","organization","country","event","location","astronomical object","discipline","chemical compound"],"instance":{"id":"36","words":["Oxon",")","Royal","College","of","Physicians",",","the","son","of","Sir","Henry","Acland",",","1st","Baronet","MD","Royal","Society","."],"labels":["B-location","O","B-university","I-university","I-university","I-university","O","O","O","O","O","B-scientist","I-scientist","O","O","B-award","I-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, scientist, person, university, protein, enzyme, award, chemical element, theory, organization, country, event, location, astronomical object, discipline, chemical compound and O.\nSentence: Oxon ) Royal College of Physicians , the son of Sir Henry Acland , 1st Baronet MD Royal Society .","prompt_labels":"Oxon(B-location) )(O) Royal(B-university) College(I-university) of(I-university) Physicians(I-university) ,(O) the(O) son(O) of(O) Sir(O) Henry(B-scientist) Acland(I-scientist) ,(O) 1st(O) Baronet(B-award) MD(I-award) Royal(I-award) Society(I-award) .(O)"}}
{"id":"37","dataset":"crossner_science","split":"train","label_list":["enzyme","chemical compound","country","protein","event","award","theory","chemical element","discipline","person","location","scientist","organization","astronomical object","university","academic journal"],"instance":{"id":"37","words":["The","area","is","also","served","by","two","hospitals","with","Merlin","Park","University","Hospital","to","the","west","and","the","private","Galway","Clinic","to","the","east","of","Doughiska","."],"labels":["O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical compound, country, protein, event, award, theory, chemical element, discipline, person, location, scientist, organization, astronomical object, university, academic journal and O.\nSentence: The area is also served by two hospitals with Merlin Park University Hospital to the west and the private Galway Clinic to the east of Doughiska .","prompt_labels":"The(O) area(O) is(O) also(O) served(O) by(O) two(O) hospitals(O) with(O) Merlin(B-organization) Park(I-organization) University(I-organization) Hospital(I-organization) to(O) the(O) west(O) and(O) the(O) private(O) Galway(B-organization) Clinic(I-organization) to(O) the(O) east(O) of(O) Doughiska(B-location) .(O)"}}
{"id":"38","dataset":"crossner_science","split":"train","label_list":["astronomical object","enzyme","university","chemical element","person","chemical compound","academic journal","country","discipline","location","protein","event","organization","theory","scientist","award"],"instance":{"id":"38","words":["Pacific","Union","College",",","classified","as","a","National","Liberal","Arts","College","by","the","Carnegie","Foundation","for","the","Advancement","of","Teaching",",","is","the","county","'s","only","four-year","college","and","serves","roughly","1,500","students","."],"labels":["B-university","I-university","I-university","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, enzyme, university, chemical element, person, chemical compound, academic journal, country, discipline, location, protein, event, organization, theory, scientist, award and O.\nSentence: Pacific Union College , classified as a National Liberal Arts College by the Carnegie Foundation for the Advancement of Teaching , is the county 's only four-year college and serves roughly 1,500 students .","prompt_labels":"Pacific(B-university) Union(I-university) College(I-university) ,(O) classified(O) as(O) a(O) National(O) Liberal(O) Arts(O) College(O) by(O) the(O) Carnegie(B-organization) Foundation(I-organization) for(I-organization) the(I-organization) Advancement(I-organization) of(I-organization) Teaching(I-organization) ,(O) is(O) the(O) county(O) 's(O) only(O) four-year(O) college(O) and(O) serves(O) roughly(O) 1,500(O) students(O) .(O)"}}
{"id":"39","dataset":"crossner_science","split":"train","label_list":["organization","chemical element","enzyme","chemical compound","astronomical object","location","discipline","university","protein","theory","scientist","award","academic journal","event","person","country"],"instance":{"id":"39","words":["The","five","bodies","currently","called","planets","that","were","known","to","the","Greeks","were","those","visible","to","the","naked","eye",":","Mercury",",","Venus",",","Mars",",","Jupiter",",","and","Saturn","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","B-astronomical object","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, chemical element, enzyme, chemical compound, astronomical object, location, discipline, university, protein, theory, scientist, award, academic journal, event, person, country and O.\nSentence: The five bodies currently called planets that were known to the Greeks were those visible to the naked eye : Mercury , Venus , Mars , Jupiter , and Saturn .","prompt_labels":"The(O) five(O) bodies(O) currently(O) called(O) planets(O) that(O) were(O) known(O) to(O) the(O) Greeks(O) were(O) those(O) visible(O) to(O) the(O) naked(O) eye(O) :(O) Mercury(B-astronomical object) ,(O) Venus(B-astronomical object) ,(O) Mars(B-astronomical object) ,(O) Jupiter(B-astronomical object) ,(O) and(O) Saturn(B-astronomical object) .(O)"}}
{"id":"40","dataset":"crossner_science","split":"train","label_list":["astronomical object","award","event","chemical element","chemical compound","academic journal","person","organization","location","theory","country","enzyme","university","scientist","discipline","protein"],"instance":{"id":"40","words":["In","1871","he","was","elected","a","fellow","of","the","Linnean","Society","of","London",",","and","in","1881","he","was","elected","a","fellow","of","the","Royal","Society","."],"labels":["O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, award, event, chemical element, chemical compound, academic journal, person, organization, location, theory, country, enzyme, university, scientist, discipline, protein and O.\nSentence: In 1871 he was elected a fellow of the Linnean Society of London , and in 1881 he was elected a fellow of the Royal Society .","prompt_labels":"In(O) 1871(O) he(O) was(O) elected(O) a(O) fellow(B-award) of(I-award) the(I-award) Linnean(I-award) Society(I-award) of(I-award) London(I-award) ,(O) and(O) in(O) 1881(O) he(O) was(O) elected(O) a(O) fellow(B-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) .(O)"}}
{"id":"41","dataset":"crossner_science","split":"train","label_list":["chemical element","protein","enzyme","award","country","scientist","event","organization","academic journal","person","chemical compound","astronomical object","theory","discipline","location","university"],"instance":{"id":"41","words":["The","planets","have","been","designated","Pr0201","b","and","Pr0211","b","."],"labels":["O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, protein, enzyme, award, country, scientist, event, organization, academic journal, person, chemical compound, astronomical object, theory, discipline, location, university and O.\nSentence: The planets have been designated Pr0201 b and Pr0211 b .","prompt_labels":"The(O) planets(O) have(O) been(O) designated(O) Pr0201(B-astronomical object) b(I-astronomical object) and(O) Pr0211(B-astronomical object) b(I-astronomical object) .(O)"}}
{"id":"42","dataset":"crossner_science","split":"train","label_list":["person","chemical compound","protein","award","discipline","chemical element","academic journal","scientist","enzyme","astronomical object","theory","country","organization","event","university","location"],"instance":{"id":"42","words":["The","two-part","finale","'s","epic","scale","and","underlying","plot","was","first","conceived","in","early","2007","as","the","last","regular-series","story","for","departing","producer","s","Russell","T","Davies",",","Julie","Gardner",",","and","Phil","Collinson",":","the","fourth","series","finale","is","the","last","story","produced","by","Collinson",";","and","Steven","Moffat","and","Piers","Wenger","replaced","Davies","and","Gardner","as","show","runner","and","executive","producer","respectively","in","2010","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","O","B-person","I-person","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","B-person","O","O","B-person","I-person","O","B-person","I-person","O","B-person","O","B-person","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, chemical compound, protein, award, discipline, chemical element, academic journal, scientist, enzyme, astronomical object, theory, country, organization, event, university, location and O.\nSentence: The two-part finale 's epic scale and underlying plot was first conceived in early 2007 as the last regular-series story for departing producer s Russell T Davies , Julie Gardner , and Phil Collinson : the fourth series finale is the last story produced by Collinson ; and Steven Moffat and Piers Wenger replaced Davies and Gardner as show runner and executive producer respectively in 2010 .","prompt_labels":"The(O) two-part(O) finale(O) 's(O) epic(O) scale(O) and(O) underlying(O) plot(O) was(O) first(O) conceived(O) in(O) early(O) 2007(O) as(O) the(O) last(O) regular-series(O) story(O) for(O) departing(O) producer(O) s(O) Russell(B-person) T(I-person) Davies(I-person) ,(O) Julie(B-person) Gardner(I-person) ,(O) and(O) Phil(B-person) Collinson(I-person) :(O) the(O) fourth(O) series(O) finale(O) is(O) the(O) last(O) story(O) produced(O) by(O) Collinson(B-person) ;(O) and(O) Steven(B-person) Moffat(I-person) and(O) Piers(B-person) Wenger(I-person) replaced(O) Davies(B-person) and(O) Gardner(B-person) as(O) show(O) runner(O) and(O) executive(O) producer(O) respectively(O) in(O) 2010(O) .(O)"}}
{"id":"43","dataset":"crossner_science","split":"train","label_list":["discipline","scientist","astronomical object","chemical compound","theory","university","academic journal","location","chemical element","person","award","event","country","organization","enzyme","protein"],"instance":{"id":"43","words":["Lost","was","created","by","Jeffrey","Lieber",",","J.","J.","Abrams","and","Damon","Lindelof",",","who","share","story","writing","credits","for","the","pilot","episode",",","which","Abrams","directed","."],"labels":["O","O","O","O","B-person","I-person","O","B-person","I-person","I-person","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, scientist, astronomical object, chemical compound, theory, university, academic journal, location, chemical element, person, award, event, country, organization, enzyme, protein and O.\nSentence: Lost was created by Jeffrey Lieber , J. J. Abrams and Damon Lindelof , who share story writing credits for the pilot episode , which Abrams directed .","prompt_labels":"Lost(O) was(O) created(O) by(O) Jeffrey(B-person) Lieber(I-person) ,(O) J.(B-person) J.(I-person) Abrams(I-person) and(O) Damon(B-person) Lindelof(I-person) ,(O) who(O) share(O) story(O) writing(O) credits(O) for(O) the(O) pilot(O) episode(O) ,(O) which(O) Abrams(B-person) directed(O) .(O)"}}
{"id":"44","dataset":"crossner_science","split":"train","label_list":["organization","country","chemical element","university","discipline","enzyme","astronomical object","academic journal","protein","theory","chemical compound","scientist","location","event","person","award"],"instance":{"id":"44","words":["The","study","claimed","precise","CRISPR","and","homology-directed","repair","response","with","high","accuracy","and","percision","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, chemical element, university, discipline, enzyme, astronomical object, academic journal, protein, theory, chemical compound, scientist, location, event, person, award and O.\nSentence: The study claimed precise CRISPR and homology-directed repair response with high accuracy and percision .","prompt_labels":"The(O) study(O) claimed(O) precise(O) CRISPR(O) and(O) homology-directed(O) repair(O) response(O) with(O) high(O) accuracy(O) and(O) percision(O) .(O)"}}
{"id":"45","dataset":"crossner_science","split":"train","label_list":["astronomical object","university","award","protein","theory","location","enzyme","chemical compound","academic journal","country","chemical element","person","scientist","event","organization","discipline"],"instance":{"id":"45","words":["In","1917",",","he","was","appointed","as","the","first","Palit","Professor","of","Physics","by","Ashutosh","Mukherjee","at","the","Rajabazar","Science","College",",","University","of","Calcutta","."],"labels":["O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","B-scientist","I-scientist","O","O","B-university","I-university","I-university","O","B-university","I-university","I-university","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, university, award, protein, theory, location, enzyme, chemical compound, academic journal, country, chemical element, person, scientist, event, organization, discipline and O.\nSentence: In 1917 , he was appointed as the first Palit Professor of Physics by Ashutosh Mukherjee at the Rajabazar Science College , University of Calcutta .","prompt_labels":"In(O) 1917(O) ,(O) he(O) was(O) appointed(O) as(O) the(O) first(O) Palit(B-award) Professor(I-award) of(I-award) Physics(I-award) by(O) Ashutosh(B-scientist) Mukherjee(I-scientist) at(O) the(O) Rajabazar(B-university) Science(I-university) College(I-university) ,(O) University(B-university) of(I-university) Calcutta(I-university) .(O)"}}
{"id":"46","dataset":"crossner_science","split":"train","label_list":["chemical compound","country","academic journal","protein","enzyme","event","theory","location","chemical element","person","university","award","scientist","astronomical object","organization","discipline"],"instance":{"id":"46","words":["It","has","one","of","the","six","Indian","Institute","of","Management","Indore","and","one","of","sixteen","Indian","Institute","of","Technology","Indore","."],"labels":["O","O","O","O","O","O","B-university","I-university","I-university","I-university","I-university","O","O","O","O","B-university","I-university","I-university","I-university","I-university","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, country, academic journal, protein, enzyme, event, theory, location, chemical element, person, university, award, scientist, astronomical object, organization, discipline and O.\nSentence: It has one of the six Indian Institute of Management Indore and one of sixteen Indian Institute of Technology Indore .","prompt_labels":"It(O) has(O) one(O) of(O) the(O) six(O) Indian(B-university) Institute(I-university) of(I-university) Management(I-university) Indore(I-university) and(O) one(O) of(O) sixteen(O) Indian(B-university) Institute(I-university) of(I-university) Technology(I-university) Indore(I-university) .(O)"}}
{"id":"47","dataset":"crossner_science","split":"train","label_list":["discipline","event","theory","person","location","astronomical object","organization","protein","country","chemical compound","scientist","chemical element","academic journal","enzyme","award","university"],"instance":{"id":"47","words":["From","the","16th","century",",","researchers","including","Jan","Baptist","van","Helmont",",","Robert","Boyle",",","and","Isaac","Newton","tried","to","establish","theories","of","the","experimentally","observed","chemical","transformations","."],"labels":["O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, event, theory, person, location, astronomical object, organization, protein, country, chemical compound, scientist, chemical element, academic journal, enzyme, award, university and O.\nSentence: From the 16th century , researchers including Jan Baptist van Helmont , Robert Boyle , and Isaac Newton tried to establish theories of the experimentally observed chemical transformations .","prompt_labels":"From(O) the(O) 16th(O) century(O) ,(O) researchers(O) including(O) Jan(B-scientist) Baptist(I-scientist) van(I-scientist) Helmont(I-scientist) ,(O) Robert(B-scientist) Boyle(I-scientist) ,(O) and(O) Isaac(B-scientist) Newton(I-scientist) tried(O) to(O) establish(O) theories(O) of(O) the(O) experimentally(O) observed(O) chemical(O) transformations(O) .(O)"}}
{"id":"48","dataset":"crossner_science","split":"train","label_list":["country","protein","discipline","university","academic journal","astronomical object","theory","award","organization","scientist","chemical element","person","location","chemical compound","event","enzyme"],"instance":{"id":"48","words":["Weisskopf","was","awarded","the","Max","Planck","Medal","in","1956","and","the","Prix","mondial","Cino","Del","Duca","in","1972",",","the","National","Medal","of","Science","(","1980",")",",","the","Wolf","Prize","(","1981",")","and","the","Public","Welfare","Medal","from","the","National","Academy","of","Sciences","(","1991",")","."],"labels":["B-scientist","O","O","O","B-award","I-award","I-award","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","B-award","I-award","O","O","O","O","O","B-award","I-award","I-award","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, protein, discipline, university, academic journal, astronomical object, theory, award, organization, scientist, chemical element, person, location, chemical compound, event, enzyme and O.\nSentence: Weisskopf was awarded the Max Planck Medal in 1956 and the Prix mondial Cino Del Duca in 1972 , the National Medal of Science ( 1980 ) , the Wolf Prize ( 1981 ) and the Public Welfare Medal from the National Academy of Sciences ( 1991 ) .","prompt_labels":"Weisskopf(B-scientist) was(O) awarded(O) the(O) Max(B-award) Planck(I-award) Medal(I-award) in(O) 1956(O) and(O) the(O) Prix(B-award) mondial(I-award) Cino(I-award) Del(I-award) Duca(I-award) in(O) 1972(O) ,(O) the(O) National(B-award) Medal(I-award) of(I-award) Science(I-award) ((O) 1980(O) )(O) ,(O) the(O) Wolf(B-award) Prize(I-award) ((O) 1981(O) )(O) and(O) the(O) Public(B-award) Welfare(I-award) Medal(I-award) from(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ((O) 1991(O) )(O) .(O)"}}
{"id":"49","dataset":"crossner_science","split":"train","label_list":["location","award","theory","chemical element","organization","scientist","discipline","country","university","astronomical object","chemical compound","academic journal","protein","event","person","enzyme"],"instance":{"id":"49","words":["In","2005",",","Tania","Singer","and","Chris","Frith","of","the","UCL","Institute","of","Cognitive","Neuroscience","and","the","Functional","Imaging","Laboratory","published","the","results","of","a","study","using","transcranial","magnetic","stimulation","which","showed","for","the","first","time","the","role","of","sensorimotor","components","in","empathy","for","pain","in","other","people","."],"labels":["O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, theory, chemical element, organization, scientist, discipline, country, university, astronomical object, chemical compound, academic journal, protein, event, person, enzyme and O.\nSentence: In 2005 , Tania Singer and Chris Frith of the UCL Institute of Cognitive Neuroscience and the Functional Imaging Laboratory published the results of a study using transcranial magnetic stimulation which showed for the first time the role of sensorimotor components in empathy for pain in other people .","prompt_labels":"In(O) 2005(O) ,(O) Tania(B-scientist) Singer(I-scientist) and(O) Chris(B-scientist) Frith(I-scientist) of(O) the(O) UCL(B-organization) Institute(I-organization) of(I-organization) Cognitive(I-organization) Neuroscience(I-organization) and(O) the(O) Functional(B-organization) Imaging(I-organization) Laboratory(I-organization) published(O) the(O) results(O) of(O) a(O) study(O) using(O) transcranial(O) magnetic(O) stimulation(O) which(O) showed(O) for(O) the(O) first(O) time(O) the(O) role(O) of(O) sensorimotor(O) components(O) in(O) empathy(O) for(O) pain(O) in(O) other(O) people(O) .(O)"}}
{"id":"50","dataset":"crossner_science","split":"train","label_list":["person","enzyme","discipline","university","theory","academic journal","protein","scientist","event","organization","location","chemical element","chemical compound","award","astronomical object","country"],"instance":{"id":"50","words":["In","August","1955",",","Grissom","was","reassigned","to","the","U.S.","Air","Force","Institute","of","Technology","at","Wright-Patterson","Air","Force","Base","near","Dayton",",","Ohio","."],"labels":["O","O","O","O","B-person","O","O","O","O","B-university","I-university","I-university","I-university","I-university","I-university","O","B-organization","I-organization","I-organization","I-organization","O","B-location","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, enzyme, discipline, university, theory, academic journal, protein, scientist, event, organization, location, chemical element, chemical compound, award, astronomical object, country and O.\nSentence: In August 1955 , Grissom was reassigned to the U.S. Air Force Institute of Technology at Wright-Patterson Air Force Base near Dayton , Ohio .","prompt_labels":"In(O) August(O) 1955(O) ,(O) Grissom(B-person) was(O) reassigned(O) to(O) the(O) U.S.(B-university) Air(I-university) Force(I-university) Institute(I-university) of(I-university) Technology(I-university) at(O) Wright-Patterson(B-organization) Air(I-organization) Force(I-organization) Base(I-organization) near(O) Dayton(B-location) ,(O) Ohio(B-location) .(O)"}}
{"id":"51","dataset":"crossner_science","split":"train","label_list":["discipline","chemical element","country","scientist","academic journal","location","university","person","protein","chemical compound","event","astronomical object","enzyme","theory","award","organization"],"instance":{"id":"51","words":["Joly","was","elected","a","Fellow","of","the","Royal","Society","of","London","in","1892",",","was","awarded","the","Boyle","Medal","of","the","Royal","Dublin","Society","in","1911",",",",","a","student","geological","association","established","in","1960","."],"labels":["B-scientist","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","B-award","I-award","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical element, country, scientist, academic journal, location, university, person, protein, chemical compound, event, astronomical object, enzyme, theory, award, organization and O.\nSentence: Joly was elected a Fellow of the Royal Society of London in 1892 , was awarded the Boyle Medal of the Royal Dublin Society in 1911 , , a student geological association established in 1960 .","prompt_labels":"Joly(B-scientist) was(O) elected(O) a(O) Fellow(B-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) of(I-award) London(I-award) in(O) 1892(O) ,(O) was(O) awarded(O) the(O) Boyle(B-award) Medal(I-award) of(O) the(O) Royal(B-organization) Dublin(I-organization) Society(I-organization) in(O) 1911(O) ,(O) ,(O) a(O) student(O) geological(O) association(O) established(O) in(O) 1960(O) .(O)"}}
{"id":"52","dataset":"crossner_science","split":"train","label_list":["astronomical object","academic journal","scientist","protein","location","enzyme","chemical compound","person","award","chemical element","country","university","organization","event","discipline","theory"],"instance":{"id":"52","words":["Nihonium","was","first","reported","to","have","been","created","in","2003","by","a","Russian-American","collaboration","at","the","Joint","Institute","for","Nuclear","Research","(","JINR",")","in","Dubna",",","Russia",",","and","in","2004","by","a","team","of","Japanese","scientists","at","Riken","in","Wakō",",","Japan","."],"labels":["B-chemical element","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","B-location","O","B-country","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","B-location","O","B-country","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, academic journal, scientist, protein, location, enzyme, chemical compound, person, award, chemical element, country, university, organization, event, discipline, theory and O.\nSentence: Nihonium was first reported to have been created in 2003 by a Russian-American collaboration at the Joint Institute for Nuclear Research ( JINR ) in Dubna , Russia , and in 2004 by a team of Japanese scientists at Riken in Wakō , Japan .","prompt_labels":"Nihonium(B-chemical element) was(O) first(O) reported(O) to(O) have(O) been(O) created(O) in(O) 2003(O) by(O) a(O) Russian-American(O) collaboration(O) at(O) the(O) Joint(B-organization) Institute(I-organization) for(I-organization) Nuclear(I-organization) Research(I-organization) ((O) JINR(B-organization) )(O) in(O) Dubna(B-location) ,(O) Russia(B-country) ,(O) and(O) in(O) 2004(O) by(O) a(O) team(O) of(O) Japanese(O) scientists(O) at(O) Riken(B-organization) in(O) Wakō(B-location) ,(O) Japan(B-country) .(O)"}}
{"id":"53","dataset":"crossner_science","split":"train","label_list":["astronomical object","person","protein","organization","country","discipline","chemical compound","academic journal","enzyme","theory","university","scientist","event","award","location","chemical element"],"instance":{"id":"53","words":["The","album","received","a","Grammy","Award","nomination","for","Grammy","Award","for","Best","Pop","Vocal","Album","at","the","45th","Annual","Grammy","Awards","(","2003",")","."],"labels":["O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, person, protein, organization, country, discipline, chemical compound, academic journal, enzyme, theory, university, scientist, event, award, location, chemical element and O.\nSentence: The album received a Grammy Award nomination for Grammy Award for Best Pop Vocal Album at the 45th Annual Grammy Awards ( 2003 ) .","prompt_labels":"The(O) album(O) received(O) a(O) Grammy(B-award) Award(I-award) nomination(O) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Pop(I-award) Vocal(I-award) Album(I-award) at(O) the(O) 45th(B-award) Annual(I-award) Grammy(I-award) Awards(I-award) ((O) 2003(O) )(O) .(O)"}}
{"id":"54","dataset":"crossner_science","split":"train","label_list":["discipline","theory","country","scientist","chemical compound","person","enzyme","organization","astronomical object","academic journal","event","protein","chemical element","award","location","university"],"instance":{"id":"54","words":["The","exceptions","are","Venus","and","Uranus","."],"labels":["O","O","O","B-astronomical object","O","B-astronomical object","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, theory, country, scientist, chemical compound, person, enzyme, organization, astronomical object, academic journal, event, protein, chemical element, award, location, university and O.\nSentence: The exceptions are Venus and Uranus .","prompt_labels":"The(O) exceptions(O) are(O) Venus(B-astronomical object) and(O) Uranus(B-astronomical object) .(O)"}}
{"id":"55","dataset":"crossner_science","split":"train","label_list":["event","person","enzyme","astronomical object","academic journal","location","award","protein","scientist","chemical compound","university","discipline","theory","organization","chemical element","country"],"instance":{"id":"55","words":["Triple","conjunctions","between","the","inferior","planets","Mercury","and","Venus","and","the","superior","planets","Jupiter",",","Saturn",",","Uranus",",","Neptune",",","dwarf","planet","Pluto","or","with","star","s","take","place","when","these","objects","are","at","the","same","time","in","conjunction","to","Sun","while","Mercury","or","Venus","are","at","inferior","conjunction","."],"labels":["O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, enzyme, astronomical object, academic journal, location, award, protein, scientist, chemical compound, university, discipline, theory, organization, chemical element, country and O.\nSentence: Triple conjunctions between the inferior planets Mercury and Venus and the superior planets Jupiter , Saturn , Uranus , Neptune , dwarf planet Pluto or with star s take place when these objects are at the same time in conjunction to Sun while Mercury or Venus are at inferior conjunction .","prompt_labels":"Triple(O) conjunctions(O) between(O) the(O) inferior(O) planets(O) Mercury(B-astronomical object) and(O) Venus(B-astronomical object) and(O) the(O) superior(O) planets(O) Jupiter(B-astronomical object) ,(O) Saturn(B-astronomical object) ,(O) Uranus(B-astronomical object) ,(O) Neptune(B-astronomical object) ,(O) dwarf(O) planet(O) Pluto(B-astronomical object) or(O) with(O) star(O) s(O) take(O) place(O) when(O) these(O) objects(O) are(O) at(O) the(O) same(O) time(O) in(O) conjunction(O) to(O) Sun(B-astronomical object) while(O) Mercury(B-astronomical object) or(O) Venus(B-astronomical object) are(O) at(O) inferior(O) conjunction(O) .(O)"}}
{"id":"56","dataset":"crossner_science","split":"train","label_list":["astronomical object","chemical element","university","location","enzyme","event","organization","theory","discipline","country","award","person","chemical compound","scientist","protein","academic journal"],"instance":{"id":"56","words":["In","1991",",","John","Preskill","and","Kip","Thorne","bet","against","Stephen","Hawking","that","the","hypothesis","was","FALSE","."],"labels":["O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, chemical element, university, location, enzyme, event, organization, theory, discipline, country, award, person, chemical compound, scientist, protein, academic journal and O.\nSentence: In 1991 , John Preskill and Kip Thorne bet against Stephen Hawking that the hypothesis was FALSE .","prompt_labels":"In(O) 1991(O) ,(O) John(B-scientist) Preskill(I-scientist) and(O) Kip(B-scientist) Thorne(I-scientist) bet(O) against(O) Stephen(B-scientist) Hawking(I-scientist) that(O) the(O) hypothesis(O) was(O) FALSE(O) .(O)"}}
{"id":"57","dataset":"crossner_science","split":"train","label_list":["discipline","chemical compound","location","university","event","chemical element","country","theory","academic journal","award","enzyme","organization","protein","astronomical object","person","scientist"],"instance":{"id":"57","words":["She","represented","her","country","at","the","2017","World","Championships","in","Athletics","without","advancing","from","the","first","round","."],"labels":["O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical compound, location, university, event, chemical element, country, theory, academic journal, award, enzyme, organization, protein, astronomical object, person, scientist and O.\nSentence: She represented her country at the 2017 World Championships in Athletics without advancing from the first round .","prompt_labels":"She(O) represented(O) her(O) country(O) at(O) the(O) 2017(B-event) World(I-event) Championships(I-event) in(I-event) Athletics(I-event) without(O) advancing(O) from(O) the(O) first(O) round(O) .(O)"}}
{"id":"58","dataset":"crossner_science","split":"train","label_list":["chemical element","scientist","country","chemical compound","location","university","theory","enzyme","organization","award","discipline","academic journal","person","event","protein","astronomical object"],"instance":{"id":"58","words":["Auroras","have","been","observed","on","both","gas","planets",",","most","clearly","using","the","Hubble","Space","Telescope",",","and","the","Cassini","and","Galileo","spacecraft",",","as","well","as","on","Uranus","and","Neptune","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, scientist, country, chemical compound, location, university, theory, enzyme, organization, award, discipline, academic journal, person, event, protein, astronomical object and O.\nSentence: Auroras have been observed on both gas planets , most clearly using the Hubble Space Telescope , and the Cassini and Galileo spacecraft , as well as on Uranus and Neptune .","prompt_labels":"Auroras(O) have(O) been(O) observed(O) on(O) both(O) gas(O) planets(O) ,(O) most(O) clearly(O) using(O) the(O) Hubble(O) Space(O) Telescope(O) ,(O) and(O) the(O) Cassini(O) and(O) Galileo(O) spacecraft(O) ,(O) as(O) well(O) as(O) on(O) Uranus(B-astronomical object) and(O) Neptune(B-astronomical object) .(O)"}}
{"id":"59","dataset":"crossner_science","split":"train","label_list":["organization","chemical compound","theory","university","enzyme","event","award","chemical element","protein","country","person","scientist","location","academic journal","astronomical object","discipline"],"instance":{"id":"59","words":["The","kinetic","isotope","effect","(","KIE",")","of","ribulose-1,5-bisphosphate","carboxylase","oxygenase","(","RuBisCO",")","is","the","isotopic","fractionation","associated","solely","with","the","step","in","the","Calvin-Benson","Cycle","where","a","molecule","of","carbon","dioxide","("],"labels":["O","O","O","O","O","O","O","O","B-enzyme","I-enzyme","I-enzyme","O","B-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, chemical compound, theory, university, enzyme, event, award, chemical element, protein, country, person, scientist, location, academic journal, astronomical object, discipline and O.\nSentence: The kinetic isotope effect ( KIE ) of ribulose-1,5-bisphosphate carboxylase oxygenase ( RuBisCO ) is the isotopic fractionation associated solely with the step in the Calvin-Benson Cycle where a molecule of carbon dioxide (","prompt_labels":"The(O) kinetic(O) isotope(O) effect(O) ((O) KIE(O) )(O) of(O) ribulose-1,5-bisphosphate(B-enzyme) carboxylase(I-enzyme) oxygenase(I-enzyme) ((O) RuBisCO(B-enzyme) )(O) is(O) the(O) isotopic(O) fractionation(O) associated(O) solely(O) with(O) the(O) step(O) in(O) the(O) Calvin-Benson(O) Cycle(O) where(O) a(O) molecule(O) of(O) carbon(B-chemical compound) dioxide(I-chemical compound) ((O)"}}
{"id":"60","dataset":"crossner_science","split":"train","label_list":["chemical compound","scientist","location","country","person","academic journal","event","award","university","astronomical object","discipline","chemical element","organization","theory","enzyme","protein"],"instance":{"id":"60","words":["Glyceraldehyde","3-phosphate","has","also","been","shown","to","inhibit","the","lsr","operon","through","Cyclic","adenosine","monophosphate","-CAPK-mediated","inhibition","."],"labels":["B-chemical compound","I-chemical compound","O","O","O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","I-chemical compound","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, scientist, location, country, person, academic journal, event, award, university, astronomical object, discipline, chemical element, organization, theory, enzyme, protein and O.\nSentence: Glyceraldehyde 3-phosphate has also been shown to inhibit the lsr operon through Cyclic adenosine monophosphate -CAPK-mediated inhibition .","prompt_labels":"Glyceraldehyde(B-chemical compound) 3-phosphate(I-chemical compound) has(O) also(O) been(O) shown(O) to(O) inhibit(O) the(O) lsr(O) operon(O) through(O) Cyclic(B-chemical compound) adenosine(I-chemical compound) monophosphate(I-chemical compound) -CAPK-mediated(O) inhibition(O) .(O)"}}
{"id":"61","dataset":"crossner_science","split":"train","label_list":["award","chemical element","discipline","theory","event","chemical compound","location","enzyme","academic journal","person","scientist","astronomical object","organization","country","protein","university"],"instance":{"id":"61","words":["While","geodesy","and","geophysics","are","separate","fields",",","the","two","are","so","closely","connected","that","many","scientific","organizations","such","as","the","American","Geophysical","Union",",","the","Canadian","Geophysical","Union","and","the","International","Union","of","Geodesy","and","Geophysics","encompass","both","."],"labels":["O","B-discipline","O","B-discipline","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, chemical element, discipline, theory, event, chemical compound, location, enzyme, academic journal, person, scientist, astronomical object, organization, country, protein, university and O.\nSentence: While geodesy and geophysics are separate fields , the two are so closely connected that many scientific organizations such as the American Geophysical Union , the Canadian Geophysical Union and the International Union of Geodesy and Geophysics encompass both .","prompt_labels":"While(O) geodesy(B-discipline) and(O) geophysics(B-discipline) are(O) separate(O) fields(O) ,(O) the(O) two(O) are(O) so(O) closely(O) connected(O) that(O) many(O) scientific(O) organizations(O) such(O) as(O) the(O) American(B-organization) Geophysical(I-organization) Union(I-organization) ,(O) the(O) Canadian(B-organization) Geophysical(I-organization) Union(I-organization) and(O) the(O) International(B-organization) Union(I-organization) of(I-organization) Geodesy(I-organization) and(I-organization) Geophysics(I-organization) encompass(O) both(O) .(O)"}}
{"id":"62","dataset":"crossner_science","split":"train","label_list":["astronomical object","scientist","chemical compound","enzyme","university","protein","organization","chemical element","country","theory","location","academic journal","award","discipline","event","person"],"instance":{"id":"62","words":["The","relative","positions","of","five","planets","(","Saturn",",","Jupiter",",","Mars",",","Venus",",","and","Mercury",")","were","shown",",","as","were","the","moon","'s","phases","and","the","position","of","the","Sun","in","the","zodiac","."],"labels":["O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","B-astronomical object","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, scientist, chemical compound, enzyme, university, protein, organization, chemical element, country, theory, location, academic journal, award, discipline, event, person and O.\nSentence: The relative positions of five planets ( Saturn , Jupiter , Mars , Venus , and Mercury ) were shown , as were the moon 's phases and the position of the Sun in the zodiac .","prompt_labels":"The(O) relative(O) positions(O) of(O) five(O) planets(O) ((O) Saturn(B-astronomical object) ,(O) Jupiter(B-astronomical object) ,(O) Mars(B-astronomical object) ,(O) Venus(B-astronomical object) ,(O) and(O) Mercury(B-astronomical object) )(O) were(O) shown(O) ,(O) as(O) were(O) the(O) moon(B-astronomical object) 's(O) phases(O) and(O) the(O) position(O) of(O) the(O) Sun(B-astronomical object) in(O) the(O) zodiac(O) .(O)"}}
{"id":"63","dataset":"crossner_science","split":"train","label_list":["award","scientist","chemical compound","chemical element","location","country","university","person","event","discipline","protein","organization","astronomical object","enzyme","academic journal","theory"],"instance":{"id":"63","words":["He","was","also","a","member","of","the","Royal","Academies","of","Medicine","and","the","Royal","Academy","of","Sciences",",","Arts",",","and","of","Literature","of","Belgium",";","the","Pontifical","Academy","of","Sciences","of","the","Vatican",";","the","American","Academy","of","Arts","and","Sciences",";","the","French","National","Academy","of","Medicine",";","the","Academy","of","Sciences","of","Paris",";","the","Deutsche","Akademie","der","Naturforscher","Leopoldina",";","the","American","Philosophical","Society","."],"labels":["O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","B-discipline","O","O","O","B-discipline","I-discipline","I-discipline","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-location","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","B-location","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, scientist, chemical compound, chemical element, location, country, university, person, event, discipline, protein, organization, astronomical object, enzyme, academic journal, theory and O.\nSentence: He was also a member of the Royal Academies of Medicine and the Royal Academy of Sciences , Arts , and of Literature of Belgium ; the Pontifical Academy of Sciences of the Vatican ; the American Academy of Arts and Sciences ; the French National Academy of Medicine ; the Academy of Sciences of Paris ; the Deutsche Akademie der Naturforscher Leopoldina ; the American Philosophical Society .","prompt_labels":"He(O) was(O) also(O) a(O) member(O) of(O) the(O) Royal(B-organization) Academies(I-organization) of(I-organization) Medicine(I-organization) and(O) the(O) Royal(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) Arts(B-discipline) ,(O) and(O) of(O) Literature(B-discipline) of(I-discipline) Belgium(I-discipline) ;(O) the(O) Pontifical(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) of(O) the(O) Vatican(B-location) ;(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) ;(O) the(O) French(O) National(B-organization) Academy(I-organization) of(I-organization) Medicine(I-organization) ;(O) the(O) Academy(B-organization) of(I-organization) Sciences(I-organization) of(O) Paris(B-location) ;(O) the(O) Deutsche(B-organization) Akademie(I-organization) der(I-organization) Naturforscher(I-organization) Leopoldina(I-organization) ;(O) the(O) American(B-organization) Philosophical(I-organization) Society(I-organization) .(O)"}}
{"id":"64","dataset":"crossner_science","split":"train","label_list":["academic journal","organization","award","discipline","chemical compound","university","location","enzyme","country","astronomical object","protein","chemical element","event","theory","scientist","person"],"instance":{"id":"64","words":["Ragtime","was","nominated","for","twelve","Tony","Awards","and","won","the","Tony","Award","for","Best","Original","Score","for","Lynn","Ahrens","and","Stephen","Flahertyin","addition","to","the","Drama","Desk","Award","for","Outstanding","Lyrics","."],"labels":["O","O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-person","I-person","O","B-person","I-person","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, organization, award, discipline, chemical compound, university, location, enzyme, country, astronomical object, protein, chemical element, event, theory, scientist, person and O.\nSentence: Ragtime was nominated for twelve Tony Awards and won the Tony Award for Best Original Score for Lynn Ahrens and Stephen Flahertyin addition to the Drama Desk Award for Outstanding Lyrics .","prompt_labels":"Ragtime(O) was(O) nominated(O) for(O) twelve(O) Tony(B-award) Awards(I-award) and(O) won(O) the(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) for(O) Lynn(B-person) Ahrens(I-person) and(O) Stephen(B-person) Flahertyin(I-person) addition(O) to(O) the(O) Drama(B-award) Desk(I-award) Award(I-award) for(I-award) Outstanding(I-award) Lyrics(I-award) .(O)"}}
{"id":"65","dataset":"crossner_science","split":"train","label_list":["event","location","university","country","enzyme","protein","academic journal","chemical compound","scientist","discipline","astronomical object","organization","person","award","chemical element","theory"],"instance":{"id":"65","words":["The","Apollo","7","mission","is","dramatized","in","the","1998","miniseries","From","the","Earth","to","the","Moon","episode","We","Have","Cleared","the","Tower",",","with","Mark","Harmon","as","Schirra",",","John","Mese","as","Eisele",",","Fredric","Lehne","as","Cunningham",",","and","Max","Wright","as","Wendt","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","O","B-person","I-person","O","B-person","O","B-person","I-person","O","B-person","O","O","B-person","I-person","O","B-person","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, university, country, enzyme, protein, academic journal, chemical compound, scientist, discipline, astronomical object, organization, person, award, chemical element, theory and O.\nSentence: The Apollo 7 mission is dramatized in the 1998 miniseries From the Earth to the Moon episode We Have Cleared the Tower , with Mark Harmon as Schirra , John Mese as Eisele , Fredric Lehne as Cunningham , and Max Wright as Wendt .","prompt_labels":"The(O) Apollo(O) 7(O) mission(O) is(O) dramatized(O) in(O) the(O) 1998(O) miniseries(O) From(O) the(O) Earth(O) to(O) the(O) Moon(O) episode(O) We(O) Have(O) Cleared(O) the(O) Tower(O) ,(O) with(O) Mark(B-person) Harmon(I-person) as(O) Schirra(B-person) ,(O) John(B-person) Mese(I-person) as(O) Eisele(B-person) ,(O) Fredric(B-person) Lehne(I-person) as(O) Cunningham(B-person) ,(O) and(O) Max(B-person) Wright(I-person) as(O) Wendt(B-person) .(O)"}}
{"id":"66","dataset":"crossner_science","split":"train","label_list":["chemical element","chemical compound","protein","enzyme","person","location","award","country","discipline","organization","event","astronomical object","academic journal","scientist","theory","university"],"instance":{"id":"66","words":["The","journal","establishment","was","similar","to","the","starting","of","The","Astrophysical","Journal","and","The","Astronomical","Journal","by","George","Ellery","Hale","."],"labels":["O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-scientist","I-scientist","I-scientist","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, chemical compound, protein, enzyme, person, location, award, country, discipline, organization, event, astronomical object, academic journal, scientist, theory, university and O.\nSentence: The journal establishment was similar to the starting of The Astrophysical Journal and The Astronomical Journal by George Ellery Hale .","prompt_labels":"The(O) journal(O) establishment(O) was(O) similar(O) to(O) the(O) starting(O) of(O) The(B-academic journal) Astrophysical(I-academic journal) Journal(I-academic journal) and(O) The(B-academic journal) Astronomical(I-academic journal) Journal(I-academic journal) by(O) George(B-scientist) Ellery(I-scientist) Hale(I-scientist) .(O)"}}
{"id":"67","dataset":"crossner_science","split":"train","label_list":["theory","scientist","chemical element","university","academic journal","country","astronomical object","event","protein","organization","award","enzyme","person","location","discipline","chemical compound"],"instance":{"id":"67","words":["In","1969",",","Shenton","persuaded","Ellis","Hillman",",","a","University","of","East","London","lecturer",",","to","become","president","of","the","Flat","Earth","Society",";","but","there","is","little","evidence","of","any","activity","on","his","part","until","after","Shenton","'s","death",",","when","he","added","most","of","Shenton","'s","library","to","the","archives","of","the","Science","Fiction","Foundation","he","helped","to","establish","."],"labels":["O","O","O","B-person","O","B-person","I-person","O","O","B-university","I-university","I-university","I-university","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, scientist, chemical element, university, academic journal, country, astronomical object, event, protein, organization, award, enzyme, person, location, discipline, chemical compound and O.\nSentence: In 1969 , Shenton persuaded Ellis Hillman , a University of East London lecturer , to become president of the Flat Earth Society ; but there is little evidence of any activity on his part until after Shenton 's death , when he added most of Shenton 's library to the archives of the Science Fiction Foundation he helped to establish .","prompt_labels":"In(O) 1969(O) ,(O) Shenton(B-person) persuaded(O) Ellis(B-person) Hillman(I-person) ,(O) a(O) University(B-university) of(I-university) East(I-university) London(I-university) lecturer(O) ,(O) to(O) become(O) president(O) of(O) the(O) Flat(B-organization) Earth(I-organization) Society(I-organization) ;(O) but(O) there(O) is(O) little(O) evidence(O) of(O) any(O) activity(O) on(O) his(O) part(O) until(O) after(O) Shenton(B-person) 's(O) death(O) ,(O) when(O) he(O) added(O) most(O) of(O) Shenton(B-person) 's(O) library(O) to(O) the(O) archives(O) of(O) the(O) Science(B-organization) Fiction(I-organization) Foundation(I-organization) he(O) helped(O) to(O) establish(O) .(O)"}}
{"id":"68","dataset":"crossner_science","split":"train","label_list":["award","protein","academic journal","person","theory","location","astronomical object","chemical compound","scientist","enzyme","chemical element","organization","country","discipline","event","university"],"instance":{"id":"68","words":["Mechnikov","was","appointed","docent","at","the","newly","established","Odessa","University","(","now","Odessa","University",")","."],"labels":["B-scientist","O","O","O","O","O","O","O","B-university","I-university","O","O","B-university","I-university","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, protein, academic journal, person, theory, location, astronomical object, chemical compound, scientist, enzyme, chemical element, organization, country, discipline, event, university and O.\nSentence: Mechnikov was appointed docent at the newly established Odessa University ( now Odessa University ) .","prompt_labels":"Mechnikov(B-scientist) was(O) appointed(O) docent(O) at(O) the(O) newly(O) established(O) Odessa(B-university) University(I-university) ((O) now(O) Odessa(B-university) University(I-university) )(O) .(O)"}}
{"id":"69","dataset":"crossner_science","split":"train","label_list":["scientist","event","chemical element","theory","country","organization","enzyme","award","chemical compound","protein","person","location","astronomical object","academic journal","discipline","university"],"instance":{"id":"69","words":["Hideki","Shirakawa","(","白川","英樹","Shirakawa","Hideki",",","born","August","20",",","1936",")","is","a","Japanese","chemist",",","engineer",",","and","Professor","Emeritus","at","the","University","of","Tsukuba","and","Zhejiang","University","."],"labels":["B-scientist","I-scientist","O","B-scientist","I-scientist","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","O","B-university","I-university","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, event, chemical element, theory, country, organization, enzyme, award, chemical compound, protein, person, location, astronomical object, academic journal, discipline, university and O.\nSentence: Hideki Shirakawa ( 白川 英樹 Shirakawa Hideki , born August 20 , 1936 ) is a Japanese chemist , engineer , and Professor Emeritus at the University of Tsukuba and Zhejiang University .","prompt_labels":"Hideki(B-scientist) Shirakawa(I-scientist) ((O) 白川(B-scientist) 英樹(I-scientist) Shirakawa(B-scientist) Hideki(I-scientist) ,(O) born(O) August(O) 20(O) ,(O) 1936(O) )(O) is(O) a(O) Japanese(O) chemist(O) ,(O) engineer(O) ,(O) and(O) Professor(O) Emeritus(O) at(O) the(O) University(B-university) of(I-university) Tsukuba(I-university) and(O) Zhejiang(B-university) University(I-university) .(O)"}}
{"id":"70","dataset":"crossner_science","split":"train","label_list":["organization","university","person","academic journal","chemical compound","protein","astronomical object","scientist","chemical element","country","enzyme","award","discipline","event","theory","location"],"instance":{"id":"70","words":["The","Scottish","chemist","Joseph","Black","(","the","first","experimental","chemist",")","and","the","Dutchman","J.B.","van","Helmont","discovered","carbon","dioxide",",","or","what","Black","called","'","fixed","air","'","in","1754",";","Henry","Cavendish","discovered","hydrogen","and","elucidated","its","properties","and","Joseph","Priestley","and",",","independently",",","Carl","Wilhelm","Scheele","isolated","pure","oxygen","."],"labels":["O","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-chemical compound","I-chemical compound","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-chemical compound","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","B-scientist","I-scientist","I-scientist","O","O","B-chemical compound","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, university, person, academic journal, chemical compound, protein, astronomical object, scientist, chemical element, country, enzyme, award, discipline, event, theory, location and O.\nSentence: The Scottish chemist Joseph Black ( the first experimental chemist ) and the Dutchman J.B. van Helmont discovered carbon dioxide , or what Black called ' fixed air ' in 1754 ; Henry Cavendish discovered hydrogen and elucidated its properties and Joseph Priestley and , independently , Carl Wilhelm Scheele isolated pure oxygen .","prompt_labels":"The(O) Scottish(O) chemist(O) Joseph(B-scientist) Black(I-scientist) ((O) the(O) first(O) experimental(O) chemist(O) )(O) and(O) the(O) Dutchman(O) J.B.(B-scientist) van(I-scientist) Helmont(I-scientist) discovered(O) carbon(B-chemical compound) dioxide(I-chemical compound) ,(O) or(O) what(O) Black(O) called(O) '(O) fixed(O) air(O) '(O) in(O) 1754(O) ;(O) Henry(B-scientist) Cavendish(I-scientist) discovered(O) hydrogen(B-chemical compound) and(O) elucidated(O) its(O) properties(O) and(O) Joseph(B-scientist) Priestley(I-scientist) and(O) ,(O) independently(O) ,(O) Carl(B-scientist) Wilhelm(I-scientist) Scheele(I-scientist) isolated(O) pure(O) oxygen(B-chemical compound) .(O)"}}
{"id":"71","dataset":"crossner_science","split":"train","label_list":["academic journal","university","organization","location","chemical compound","scientist","discipline","event","theory","protein","astronomical object","person","chemical element","country","enzyme","award"],"instance":{"id":"71","words":["Poly","(","ADP-ribosyl",")","ation","is","an","immediate","DNA","damage-dependent","post-translational","modification","of","Histone","and","other","nuclear","proteins","that","contributes","to","the","survival","of","injured","proliferating","cells","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","O","O","B-protein","I-protein","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, university, organization, location, chemical compound, scientist, discipline, event, theory, protein, astronomical object, person, chemical element, country, enzyme, award and O.\nSentence: Poly ( ADP-ribosyl ) ation is an immediate DNA damage-dependent post-translational modification of Histone and other nuclear proteins that contributes to the survival of injured proliferating cells .","prompt_labels":"Poly(O) ((O) ADP-ribosyl(O) )(O) ation(O) is(O) an(O) immediate(O) DNA(O) damage-dependent(O) post-translational(O) modification(O) of(O) Histone(B-protein) and(O) other(O) nuclear(B-protein) proteins(I-protein) that(O) contributes(O) to(O) the(O) survival(O) of(O) injured(O) proliferating(O) cells(O) .(O)"}}
{"id":"72","dataset":"crossner_science","split":"train","label_list":["location","university","enzyme","scientist","country","astronomical object","protein","person","award","theory","organization","event","chemical element","discipline","academic journal","chemical compound"],"instance":{"id":"72","words":["During","her","time","at","the","Fred","Hutchinson","Cancer","Research","Center",",","Zakian","published","or","co-published","around","sixty","articles","in","peer-reviewed","journals","like","Nature","(","journal",")",",","Cell","(","journal",")",",","Proceedings","of","the","National","Academy","of","Sciences","of","the","United","States","of","America","and","the","Journal","of","Molecular","Biology","."],"labels":["O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-scientist","O","O","O","O","O","O","O","O","O","O","B-academic journal","O","O","O","O","B-academic journal","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, university, enzyme, scientist, country, astronomical object, protein, person, award, theory, organization, event, chemical element, discipline, academic journal, chemical compound and O.\nSentence: During her time at the Fred Hutchinson Cancer Research Center , Zakian published or co-published around sixty articles in peer-reviewed journals like Nature ( journal ) , Cell ( journal ) , Proceedings of the National Academy of Sciences of the United States of America and the Journal of Molecular Biology .","prompt_labels":"During(O) her(O) time(O) at(O) the(O) Fred(B-organization) Hutchinson(I-organization) Cancer(I-organization) Research(I-organization) Center(I-organization) ,(O) Zakian(B-scientist) published(O) or(O) co-published(O) around(O) sixty(O) articles(O) in(O) peer-reviewed(O) journals(O) like(O) Nature(B-academic journal) ((O) journal(O) )(O) ,(O) Cell(B-academic journal) ((O) journal(O) )(O) ,(O) Proceedings(B-academic journal) of(I-academic journal) the(I-academic journal) National(I-academic journal) Academy(I-academic journal) of(I-academic journal) Sciences(I-academic journal) of(I-academic journal) the(I-academic journal) United(I-academic journal) States(I-academic journal) of(I-academic journal) America(I-academic journal) and(O) the(O) Journal(B-academic journal) of(I-academic journal) Molecular(I-academic journal) Biology(I-academic journal) .(O)"}}
{"id":"73","dataset":"crossner_science","split":"train","label_list":["location","academic journal","organization","protein","country","astronomical object","person","scientist","chemical compound","enzyme","event","university","award","discipline","chemical element","theory"],"instance":{"id":"73","words":["Between","1855","and","1881","it","had","two","parts",",","the","first","for","the","meridian","of","Greenwich","contained","data","on","the","Sun",",","Moon",",","lunar","distances",",","Venus",",","Mars",",","Jupiter",",","and","Saturn",",","which","was","published","separately","as","The","American","Nautical","Almanac","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, academic journal, organization, protein, country, astronomical object, person, scientist, chemical compound, enzyme, event, university, award, discipline, chemical element, theory and O.\nSentence: Between 1855 and 1881 it had two parts , the first for the meridian of Greenwich contained data on the Sun , Moon , lunar distances , Venus , Mars , Jupiter , and Saturn , which was published separately as The American Nautical Almanac .","prompt_labels":"Between(O) 1855(O) and(O) 1881(O) it(O) had(O) two(O) parts(O) ,(O) the(O) first(O) for(O) the(O) meridian(B-location) of(I-location) Greenwich(I-location) contained(O) data(O) on(O) the(O) Sun(B-astronomical object) ,(O) Moon(B-astronomical object) ,(O) lunar(O) distances(O) ,(O) Venus(B-astronomical object) ,(O) Mars(B-astronomical object) ,(O) Jupiter(B-astronomical object) ,(O) and(O) Saturn(B-astronomical object) ,(O) which(O) was(O) published(O) separately(O) as(O) The(O) American(O) Nautical(O) Almanac(O) .(O)"}}
{"id":"74","dataset":"crossner_science","split":"train","label_list":["scientist","location","discipline","enzyme","protein","person","academic journal","theory","event","chemical element","astronomical object","award","chemical compound","organization","country","university"],"instance":{"id":"74","words":["DNA","methyltransferase","is","recruited","to","DNA","during","its","replication",",","or","during","DNA","repair","."],"labels":["B-enzyme","I-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, location, discipline, enzyme, protein, person, academic journal, theory, event, chemical element, astronomical object, award, chemical compound, organization, country, university and O.\nSentence: DNA methyltransferase is recruited to DNA during its replication , or during DNA repair .","prompt_labels":"DNA(B-enzyme) methyltransferase(I-enzyme) is(O) recruited(O) to(O) DNA(O) during(O) its(O) replication(O) ,(O) or(O) during(O) DNA(O) repair(O) .(O)"}}
{"id":"75","dataset":"crossner_science","split":"train","label_list":["university","chemical compound","enzyme","location","organization","discipline","award","theory","protein","event","astronomical object","country","chemical element","person","scientist","academic journal"],"instance":{"id":"75","words":["Primary","structure","of","calponin","consists","of","a","conserved","N-terminal","Calponin","homology","domain",",","a","conserved","middle","region","containing","two","actin-binding","sites",",","and","a","C-terminal","variable","region","that","contributes","to","the","differences","among","there","isoforms","."],"labels":["O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, chemical compound, enzyme, location, organization, discipline, award, theory, protein, event, astronomical object, country, chemical element, person, scientist, academic journal and O.\nSentence: Primary structure of calponin consists of a conserved N-terminal Calponin homology domain , a conserved middle region containing two actin-binding sites , and a C-terminal variable region that contributes to the differences among there isoforms .","prompt_labels":"Primary(O) structure(O) of(O) calponin(B-protein) consists(O) of(O) a(O) conserved(O) N-terminal(O) Calponin(O) homology(O) domain(O) ,(O) a(O) conserved(O) middle(O) region(O) containing(O) two(O) actin-binding(O) sites(O) ,(O) and(O) a(O) C-terminal(O) variable(O) region(O) that(O) contributes(O) to(O) the(O) differences(O) among(O) there(O) isoforms(O) .(O)"}}
{"id":"76","dataset":"crossner_science","split":"train","label_list":["chemical element","scientist","country","astronomical object","discipline","academic journal","organization","event","theory","protein","enzyme","location","university","person","award","chemical compound"],"instance":{"id":"76","words":["Jeffreys","received","the","Gold","Medal","of","the","Royal","Astronomical","Society","in","1937",",","the","Royal","Society","'","s","Copley","Medal","in","1960",",","and","the","Royal","Statistical","Society","'","s","Guy","Medal","in","Gold","in","1962","."],"labels":["B-scientist","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, scientist, country, astronomical object, discipline, academic journal, organization, event, theory, protein, enzyme, location, university, person, award, chemical compound and O.\nSentence: Jeffreys received the Gold Medal of the Royal Astronomical Society in 1937 , the Royal Society ' s Copley Medal in 1960 , and the Royal Statistical Society ' s Guy Medal in Gold in 1962 .","prompt_labels":"Jeffreys(B-scientist) received(O) the(O) Gold(B-award) Medal(I-award) of(I-award) the(I-award) Royal(I-award) Astronomical(I-award) Society(I-award) in(O) 1937(O) ,(O) the(O) Royal(B-award) Society(I-award) '(I-award) s(I-award) Copley(I-award) Medal(I-award) in(O) 1960(O) ,(O) and(O) the(O) Royal(B-award) Statistical(I-award) Society(I-award) '(I-award) s(I-award) Guy(I-award) Medal(I-award) in(I-award) Gold(I-award) in(O) 1962(O) .(O)"}}
{"id":"77","dataset":"crossner_science","split":"train","label_list":["enzyme","protein","theory","scientist","academic journal","event","astronomical object","country","chemical compound","person","university","award","chemical element","location","discipline","organization"],"instance":{"id":"77","words":["Lin","is","recognized","for","his","contributions","to","stem","cell","research",",","especially","for","his","discoveries","of","the","Piwi","/","Argonaute","(","AGO",")","gene","family","and","the","Piwi-interacting","RNA","s","(","piRNAs",")",",","and","for","proving","the","stem","cell","niche","theory","."],"labels":["B-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","O","B-protein","O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-theory","I-theory","I-theory","I-theory","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, protein, theory, scientist, academic journal, event, astronomical object, country, chemical compound, person, university, award, chemical element, location, discipline, organization and O.\nSentence: Lin is recognized for his contributions to stem cell research , especially for his discoveries of the Piwi / Argonaute ( AGO ) gene family and the Piwi-interacting RNA s ( piRNAs ) , and for proving the stem cell niche theory .","prompt_labels":"Lin(B-scientist) is(O) recognized(O) for(O) his(O) contributions(O) to(O) stem(O) cell(O) research(O) ,(O) especially(O) for(O) his(O) discoveries(O) of(O) the(O) Piwi(B-protein) /(O) Argonaute(B-protein) ((O) AGO(B-protein) )(O) gene(O) family(O) and(O) the(O) Piwi-interacting(O) RNA(O) s(O) ((O) piRNAs(O) )(O) ,(O) and(O) for(O) proving(O) the(O) stem(B-theory) cell(I-theory) niche(I-theory) theory(I-theory) .(O)"}}
{"id":"78","dataset":"crossner_science","split":"train","label_list":["organization","discipline","location","chemical element","person","award","event","academic journal","enzyme","chemical compound","country","astronomical object","university","theory","protein","scientist"],"instance":{"id":"78","words":["It","was","discovered","during","the","Palomar-Leiden","Trojan","survey","on","26","March","1971",",","by","Ingrid","van","Houten-Groeneveld","and","Cornelis","van","Houten","at","Leiden",",","and","Tom","Gehrels","at","Palomar","Observatory","in","California","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-location","O","O","B-scientist","I-scientist","O","B-location","I-location","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, discipline, location, chemical element, person, award, event, academic journal, enzyme, chemical compound, country, astronomical object, university, theory, protein, scientist and O.\nSentence: It was discovered during the Palomar-Leiden Trojan survey on 26 March 1971 , by Ingrid van Houten-Groeneveld and Cornelis van Houten at Leiden , and Tom Gehrels at Palomar Observatory in California .","prompt_labels":"It(O) was(O) discovered(O) during(O) the(O) Palomar-Leiden(O) Trojan(O) survey(O) on(O) 26(O) March(O) 1971(O) ,(O) by(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Cornelis(B-scientist) van(I-scientist) Houten(I-scientist) at(O) Leiden(B-location) ,(O) and(O) Tom(B-scientist) Gehrels(I-scientist) at(O) Palomar(B-location) Observatory(I-location) in(O) California(B-location) .(O)"}}
{"id":"79","dataset":"crossner_science","split":"train","label_list":["university","location","discipline","chemical compound","organization","theory","country","scientist","chemical element","academic journal","event","enzyme","astronomical object","award","person","protein"],"instance":{"id":"79","words":["From","1916","to","1921",",","he","was","a","lecturer","in","the","physics","department","of","the","Rajabazar","Science","College","under","University","of","Calcutta","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-university","I-university","I-university","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, location, discipline, chemical compound, organization, theory, country, scientist, chemical element, academic journal, event, enzyme, astronomical object, award, person, protein and O.\nSentence: From 1916 to 1921 , he was a lecturer in the physics department of the Rajabazar Science College under University of Calcutta .","prompt_labels":"From(O) 1916(O) to(O) 1921(O) ,(O) he(O) was(O) a(O) lecturer(O) in(O) the(O) physics(B-organization) department(I-organization) of(I-organization) the(I-organization) Rajabazar(I-organization) Science(I-organization) College(I-organization) under(O) University(B-university) of(I-university) Calcutta(I-university) .(O)"}}
{"id":"80","dataset":"crossner_science","split":"train","label_list":["enzyme","chemical element","person","academic journal","chemical compound","organization","scientist","theory","discipline","university","event","location","protein","astronomical object","country","award"],"instance":{"id":"80","words":["Previously",",","americium","was","named","after","a","continent","as","its","analogue","europium",",","and","curium","honored","scientists","Marie","Curie","and","Pierre","Curie","as","the","lanthanide","above","it",",","gadolinium",",","was","named","after","the","explorer","of","the","rare","earth","element","s","Johan","Gadolin","."],"labels":["O","O","B-chemical element","O","O","O","O","O","O","O","O","B-chemical element","O","O","B-chemical element","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O","B-chemical element","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical element, person, academic journal, chemical compound, organization, scientist, theory, discipline, university, event, location, protein, astronomical object, country, award and O.\nSentence: Previously , americium was named after a continent as its analogue europium , and curium honored scientists Marie Curie and Pierre Curie as the lanthanide above it , gadolinium , was named after the explorer of the rare earth element s Johan Gadolin .","prompt_labels":"Previously(O) ,(O) americium(B-chemical element) was(O) named(O) after(O) a(O) continent(O) as(O) its(O) analogue(O) europium(B-chemical element) ,(O) and(O) curium(B-chemical element) honored(O) scientists(O) Marie(B-scientist) Curie(I-scientist) and(O) Pierre(B-scientist) Curie(I-scientist) as(O) the(O) lanthanide(O) above(O) it(O) ,(O) gadolinium(B-chemical element) ,(O) was(O) named(O) after(O) the(O) explorer(O) of(O) the(O) rare(O) earth(O) element(O) s(O) Johan(B-scientist) Gadolin(I-scientist) .(O)"}}
{"id":"81","dataset":"crossner_science","split":"train","label_list":["astronomical object","discipline","enzyme","theory","academic journal","country","organization","chemical element","scientist","person","university","protein","location","event","chemical compound","award"],"instance":{"id":"81","words":["They","found","seven","universally","clustered","pathways",":","glycolysis",",","aminoacyl-tRNA","biosynthesis",",","ATP","synthase",",","DNA","polymerase",",","hexachlorocyclohexane","degradation",",","cyanoamino","acid","metabolism",",","and","photosynthesis","(","Adenosine","diphosphate","synthesis","in","non","plant","species",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-chemical compound","O","O","B-enzyme","I-enzyme","O","B-enzyme","I-enzyme","O","B-chemical compound","O","O","B-chemical compound","I-chemical compound","O","O","O","O","O","B-chemical compound","I-chemical compound","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, discipline, enzyme, theory, academic journal, country, organization, chemical element, scientist, person, university, protein, location, event, chemical compound, award and O.\nSentence: They found seven universally clustered pathways : glycolysis , aminoacyl-tRNA biosynthesis , ATP synthase , DNA polymerase , hexachlorocyclohexane degradation , cyanoamino acid metabolism , and photosynthesis ( Adenosine diphosphate synthesis in non plant species ) .","prompt_labels":"They(O) found(O) seven(O) universally(O) clustered(O) pathways(O) :(O) glycolysis(O) ,(O) aminoacyl-tRNA(B-chemical compound) biosynthesis(O) ,(O) ATP(B-enzyme) synthase(I-enzyme) ,(O) DNA(B-enzyme) polymerase(I-enzyme) ,(O) hexachlorocyclohexane(B-chemical compound) degradation(O) ,(O) cyanoamino(B-chemical compound) acid(I-chemical compound) metabolism(O) ,(O) and(O) photosynthesis(O) ((O) Adenosine(B-chemical compound) diphosphate(I-chemical compound) synthesis(O) in(O) non(O) plant(O) species(O) )(O) .(O)"}}
{"id":"82","dataset":"crossner_science","split":"train","label_list":["scientist","location","protein","chemical element","theory","country","university","organization","person","enzyme","astronomical object","discipline","chemical compound","event","award","academic journal"],"instance":{"id":"82","words":["The","Nicolaus","Copernicus","heliocentric","theory","of","the","Solar","System","had","received","confirmation","by","the","observations","of","Galileo","Galilei","and","Tycho","Brahe","and","the","mathematical","investigations","of","Johannes","Kepler","and","Isaac","Newton","."],"labels":["O","B-theory","I-theory","I-theory","I-theory","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, location, protein, chemical element, theory, country, university, organization, person, enzyme, astronomical object, discipline, chemical compound, event, award, academic journal and O.\nSentence: The Nicolaus Copernicus heliocentric theory of the Solar System had received confirmation by the observations of Galileo Galilei and Tycho Brahe and the mathematical investigations of Johannes Kepler and Isaac Newton .","prompt_labels":"The(O) Nicolaus(B-theory) Copernicus(I-theory) heliocentric(I-theory) theory(I-theory) of(O) the(O) Solar(O) System(O) had(O) received(O) confirmation(O) by(O) the(O) observations(O) of(O) Galileo(B-scientist) Galilei(I-scientist) and(O) Tycho(B-scientist) Brahe(I-scientist) and(O) the(O) mathematical(O) investigations(O) of(O) Johannes(B-scientist) Kepler(I-scientist) and(O) Isaac(B-scientist) Newton(I-scientist) .(O)"}}
{"id":"83","dataset":"crossner_science","split":"train","label_list":["discipline","award","enzyme","university","theory","person","academic journal","organization","chemical compound","scientist","astronomical object","event","location","protein","country","chemical element"],"instance":{"id":"83","words":["Other","silencing","mechanisms","include","the","recruitment","of","specialized","proteins","that","methylate","DNA","such","that","the","core","promoter","element","is","inaccessible","to","transcription","factor","s","and","RNA","polymerase","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-enzyme","I-enzyme","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, award, enzyme, university, theory, person, academic journal, organization, chemical compound, scientist, astronomical object, event, location, protein, country, chemical element and O.\nSentence: Other silencing mechanisms include the recruitment of specialized proteins that methylate DNA such that the core promoter element is inaccessible to transcription factor s and RNA polymerase .","prompt_labels":"Other(O) silencing(O) mechanisms(O) include(O) the(O) recruitment(O) of(O) specialized(O) proteins(O) that(O) methylate(O) DNA(O) such(O) that(O) the(O) core(O) promoter(O) element(O) is(O) inaccessible(O) to(O) transcription(O) factor(O) s(O) and(O) RNA(B-enzyme) polymerase(I-enzyme) .(O)"}}
{"id":"84","dataset":"crossner_science","split":"train","label_list":["location","organization","enzyme","protein","academic journal","discipline","country","astronomical object","event","university","person","award","scientist","chemical compound","chemical element","theory"],"instance":{"id":"84","words":["He","received","his","PhD","in","1953","and","began","postdoctoral","work","at","the","University","of","London",",","Cambridge","University",",","and","the","University","of","Amsterdam","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","O","B-university","I-university","O","O","O","B-university","I-university","I-university","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, enzyme, protein, academic journal, discipline, country, astronomical object, event, university, person, award, scientist, chemical compound, chemical element, theory and O.\nSentence: He received his PhD in 1953 and began postdoctoral work at the University of London , Cambridge University , and the University of Amsterdam .","prompt_labels":"He(O) received(O) his(O) PhD(O) in(O) 1953(O) and(O) began(O) postdoctoral(O) work(O) at(O) the(O) University(B-university) of(I-university) London(I-university) ,(O) Cambridge(B-university) University(I-university) ,(O) and(O) the(O) University(B-university) of(I-university) Amsterdam(I-university) .(O)"}}
{"id":"85","dataset":"crossner_science","split":"train","label_list":["organization","person","astronomical object","theory","university","chemical element","enzyme","award","discipline","event","chemical compound","protein","location","academic journal","scientist","country"],"instance":{"id":"85","words":["For","example",",","H3K14","and","H4K12","acetylation","was","found","to","be","decreased",",","as","well","as","general","acetylation","across","histones","H2B","and","Histone","H3","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","I-protein","O","B-protein","I-protein","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, astronomical object, theory, university, chemical element, enzyme, award, discipline, event, chemical compound, protein, location, academic journal, scientist, country and O.\nSentence: For example , H3K14 and H4K12 acetylation was found to be decreased , as well as general acetylation across histones H2B and Histone H3 .","prompt_labels":"For(O) example(O) ,(O) H3K14(O) and(O) H4K12(O) acetylation(O) was(O) found(O) to(O) be(O) decreased(O) ,(O) as(O) well(O) as(O) general(O) acetylation(O) across(O) histones(B-protein) H2B(I-protein) and(O) Histone(B-protein) H3(I-protein) .(O)"}}
{"id":"86","dataset":"crossner_science","split":"train","label_list":["scientist","theory","event","award","university","academic journal","location","country","enzyme","astronomical object","chemical element","protein","person","organization","chemical compound","discipline"],"instance":{"id":"86","words":["Histone","deacetylase","(","HDACs",")","are","a","class","of","enzymes","that","remove","acetyl","groups","from","histones","."],"labels":["B-enzyme","I-enzyme","O","B-enzyme","O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","B-protein","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, theory, event, award, university, academic journal, location, country, enzyme, astronomical object, chemical element, protein, person, organization, chemical compound, discipline and O.\nSentence: Histone deacetylase ( HDACs ) are a class of enzymes that remove acetyl groups from histones .","prompt_labels":"Histone(B-enzyme) deacetylase(I-enzyme) ((O) HDACs(B-enzyme) )(O) are(O) a(O) class(O) of(O) enzymes(O) that(O) remove(O) acetyl(B-chemical compound) groups(I-chemical compound) from(O) histones(B-protein) .(O)"}}
{"id":"87","dataset":"crossner_science","split":"train","label_list":["award","event","location","chemical element","country","person","discipline","university","chemical compound","scientist","academic journal","protein","theory","enzyme","astronomical object","organization"],"instance":{"id":"87","words":["It","is","bounded","in","the","north","by","the","A30","road","and","runs","from","Retew","and","Treviscoe","in","the","west","to","Redmoor","and","Penpillick","in","the","east","."],"labels":["O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","B-location","O","B-location","O","O","O","O","B-location","O","B-location","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, location, chemical element, country, person, discipline, university, chemical compound, scientist, academic journal, protein, theory, enzyme, astronomical object, organization and O.\nSentence: It is bounded in the north by the A30 road and runs from Retew and Treviscoe in the west to Redmoor and Penpillick in the east .","prompt_labels":"It(O) is(O) bounded(O) in(O) the(O) north(O) by(O) the(O) A30(B-location) road(I-location) and(O) runs(O) from(O) Retew(B-location) and(O) Treviscoe(B-location) in(O) the(O) west(O) to(O) Redmoor(B-location) and(O) Penpillick(B-location) in(O) the(O) east(O) .(O)"}}
{"id":"88","dataset":"crossner_science","split":"train","label_list":["location","scientist","country","chemical compound","event","protein","university","astronomical object","chemical element","academic journal","enzyme","theory","discipline","person","organization","award"],"instance":{"id":"88","words":["This","increased","DctA","expression",",","they","found",",","permitted","Citsup","+","/","sup","cells","to","re-uptake","Succinic","acid",",","Malic","acid",",","and","Fumaric","acid","released","into","the","medium","by","the","CitT","transporter","during","import","of","citrate","."],"labels":["O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O","O","B-chemical compound","I-chemical compound","O","O","O","O","O","O","B-protein","O","O","O","O","B-chemical compound","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, scientist, country, chemical compound, event, protein, university, astronomical object, chemical element, academic journal, enzyme, theory, discipline, person, organization, award and O.\nSentence: This increased DctA expression , they found , permitted Citsup + / sup cells to re-uptake Succinic acid , Malic acid , and Fumaric acid released into the medium by the CitT transporter during import of citrate .","prompt_labels":"This(O) increased(O) DctA(B-protein) expression(O) ,(O) they(O) found(O) ,(O) permitted(O) Citsup(O) +(O) /(O) sup(O) cells(O) to(O) re-uptake(O) Succinic(B-chemical compound) acid(I-chemical compound) ,(O) Malic(B-chemical compound) acid(I-chemical compound) ,(O) and(O) Fumaric(B-chemical compound) acid(I-chemical compound) released(O) into(O) the(O) medium(O) by(O) the(O) CitT(B-protein) transporter(O) during(O) import(O) of(O) citrate(B-chemical compound) .(O)"}}
{"id":"89","dataset":"crossner_science","split":"train","label_list":["university","academic journal","chemical compound","astronomical object","scientist","protein","enzyme","chemical element","discipline","award","event","country","person","theory","location","organization"],"instance":{"id":"89","words":["The","observatory","made","headlines","briefly","with","the","announcement","on","10","December","1983","of","the","discovery","of","an","unknown","object","at","first","described","as","possibly","as","large","as","the","giant","planet","Jupiter","and","possibly","so","close","to","Earth","that","it","would","be","part","of","this","solar","system","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, academic journal, chemical compound, astronomical object, scientist, protein, enzyme, chemical element, discipline, award, event, country, person, theory, location, organization and O.\nSentence: The observatory made headlines briefly with the announcement on 10 December 1983 of the discovery of an unknown object at first described as possibly as large as the giant planet Jupiter and possibly so close to Earth that it would be part of this solar system .","prompt_labels":"The(O) observatory(O) made(O) headlines(O) briefly(O) with(O) the(O) announcement(O) on(O) 10(O) December(O) 1983(O) of(O) the(O) discovery(O) of(O) an(O) unknown(O) object(O) at(O) first(O) described(O) as(O) possibly(O) as(O) large(O) as(O) the(O) giant(O) planet(O) Jupiter(B-astronomical object) and(O) possibly(O) so(O) close(O) to(O) Earth(B-astronomical object) that(O) it(O) would(O) be(O) part(O) of(O) this(O) solar(O) system(O) .(O)"}}
{"id":"90","dataset":"crossner_science","split":"train","label_list":["scientist","country","academic journal","discipline","university","chemical compound","location","organization","person","event","theory","chemical element","protein","award","enzyme","astronomical object"],"instance":{"id":"90","words":["This","discovery",",","which","earned","him","the","Copley","Medal","of","the","Royal","Society","in","1825",",","was","followed","by","another",",","that","a","rotating","plate","of","copper","tends","to","communicate","its","motion","to","a","magnetic","needle","suspended","over","it",",","which","he","called","magnetism","of","rotation","Annales","de","chimie","et","de","physique","(","1824",")",",","vol","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical element","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, country, academic journal, discipline, university, chemical compound, location, organization, person, event, theory, chemical element, protein, award, enzyme, astronomical object and O.\nSentence: This discovery , which earned him the Copley Medal of the Royal Society in 1825 , was followed by another , that a rotating plate of copper tends to communicate its motion to a magnetic needle suspended over it , which he called magnetism of rotation Annales de chimie et de physique ( 1824 ) , vol .","prompt_labels":"This(O) discovery(O) ,(O) which(O) earned(O) him(O) the(O) Copley(B-award) Medal(I-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) in(O) 1825(O) ,(O) was(O) followed(O) by(O) another(O) ,(O) that(O) a(O) rotating(O) plate(O) of(O) copper(B-chemical element) tends(O) to(O) communicate(O) its(O) motion(O) to(O) a(O) magnetic(O) needle(O) suspended(O) over(O) it(O) ,(O) which(O) he(O) called(O) magnetism(O) of(O) rotation(O) Annales(B-academic journal) de(I-academic journal) chimie(I-academic journal) et(I-academic journal) de(I-academic journal) physique(I-academic journal) ((O) 1824(O) )(O) ,(O) vol(O) .(O)"}}
{"id":"91","dataset":"crossner_science","split":"train","label_list":["university","chemical element","protein","astronomical object","award","enzyme","scientist","organization","location","academic journal","event","chemical compound","country","person","theory","discipline"],"instance":{"id":"91","words":["In","the","early","days","of","his","laboratory","he","discovered","a","new","mechanism","of","microRNA","processing","independent","of","Dicer","that","requires","the","catalytic","activity","of","Argonaute","2",",","a","type","of","Argonaute","protein","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-enzyme","O","O","O","O","O","O","B-protein","I-protein","O","O","O","O","B-protein","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, chemical element, protein, astronomical object, award, enzyme, scientist, organization, location, academic journal, event, chemical compound, country, person, theory, discipline and O.\nSentence: In the early days of his laboratory he discovered a new mechanism of microRNA processing independent of Dicer that requires the catalytic activity of Argonaute 2 , a type of Argonaute protein .","prompt_labels":"In(O) the(O) early(O) days(O) of(O) his(O) laboratory(O) he(O) discovered(O) a(O) new(O) mechanism(O) of(O) microRNA(O) processing(O) independent(O) of(O) Dicer(B-enzyme) that(O) requires(O) the(O) catalytic(O) activity(O) of(O) Argonaute(B-protein) 2(I-protein) ,(O) a(O) type(O) of(O) Argonaute(B-protein) protein(O) .(O)"}}
{"id":"92","dataset":"crossner_science","split":"train","label_list":["academic journal","theory","enzyme","university","event","chemical compound","award","protein","chemical element","astronomical object","location","country","discipline","person","scientist","organization"],"instance":{"id":"92","words":["He","was","awarded","the","Copley","Medal","by","the","Royal","Society","in","1850",",","and","his","Solar","Tables",",","compiled","with","the","assistance","of","Christian","Olufsen",",","appeared","in","1854","."],"labels":["O","O","O","O","B-award","I-award","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, theory, enzyme, university, event, chemical compound, award, protein, chemical element, astronomical object, location, country, discipline, person, scientist, organization and O.\nSentence: He was awarded the Copley Medal by the Royal Society in 1850 , and his Solar Tables , compiled with the assistance of Christian Olufsen , appeared in 1854 .","prompt_labels":"He(O) was(O) awarded(O) the(O) Copley(B-award) Medal(I-award) by(O) the(O) Royal(B-organization) Society(I-organization) in(O) 1850(O) ,(O) and(O) his(O) Solar(O) Tables(O) ,(O) compiled(O) with(O) the(O) assistance(O) of(O) Christian(B-scientist) Olufsen(I-scientist) ,(O) appeared(O) in(O) 1854(O) .(O)"}}
{"id":"93","dataset":"crossner_science","split":"train","label_list":["person","academic journal","organization","chemical element","chemical compound","discipline","location","enzyme","event","protein","theory","astronomical object","country","award","scientist","university"],"instance":{"id":"93","words":["The","men","'s","+","100","kg","judo","event","at","the","2015","European","Games","in","Baku","took","place","on","27","June","."],"labels":["B-event","I-event","I-event","I-event","I-event","I-event","I-event","I-event","O","O","B-event","I-event","I-event","O","B-location","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, academic journal, organization, chemical element, chemical compound, discipline, location, enzyme, event, protein, theory, astronomical object, country, award, scientist, university and O.\nSentence: The men 's + 100 kg judo event at the 2015 European Games in Baku took place on 27 June .","prompt_labels":"The(B-event) men(I-event) 's(I-event) +(I-event) 100(I-event) kg(I-event) judo(I-event) event(I-event) at(O) the(O) 2015(B-event) European(I-event) Games(I-event) in(O) Baku(B-location) took(O) place(O) on(O) 27(O) June(O) .(O)"}}
{"id":"94","dataset":"crossner_science","split":"train","label_list":["award","chemical compound","astronomical object","academic journal","chemical element","organization","protein","university","theory","enzyme","discipline","event","location","person","scientist","country"],"instance":{"id":"94","words":["One","of","the","first","significant","contributions","of","Maheswari","was","the","discovery","of","RNA","polymerase","activity","in","chloroplasts","which","he","accomplished","during","his","early","stint","at","California","Institute","of","Technology","while","working","with","Robert","S.","Bandurski","and","their","researches","revealed","the","presence","of","DNA","in","organelle","."],"labels":["O","O","O","O","O","O","O","B-scientist","O","O","O","O","B-enzyme","I-enzyme","O","O","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","O","O","B-scientist","I-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, chemical compound, astronomical object, academic journal, chemical element, organization, protein, university, theory, enzyme, discipline, event, location, person, scientist, country and O.\nSentence: One of the first significant contributions of Maheswari was the discovery of RNA polymerase activity in chloroplasts which he accomplished during his early stint at California Institute of Technology while working with Robert S. Bandurski and their researches revealed the presence of DNA in organelle .","prompt_labels":"One(O) of(O) the(O) first(O) significant(O) contributions(O) of(O) Maheswari(B-scientist) was(O) the(O) discovery(O) of(O) RNA(B-enzyme) polymerase(I-enzyme) activity(O) in(O) chloroplasts(O) which(O) he(O) accomplished(O) during(O) his(O) early(O) stint(O) at(O) California(B-university) Institute(I-university) of(I-university) Technology(I-university) while(O) working(O) with(O) Robert(B-scientist) S.(I-scientist) Bandurski(I-scientist) and(O) their(O) researches(O) revealed(O) the(O) presence(O) of(O) DNA(O) in(O) organelle(O) .(O)"}}
{"id":"95","dataset":"crossner_science","split":"train","label_list":["astronomical object","university","protein","scientist","chemical element","location","organization","theory","discipline","enzyme","award","academic journal","chemical compound","country","event","person"],"instance":{"id":"95","words":["In","the","2002","film",",","this","musical","number","is","performed","by","Catherine","Zeta-Jones","(","as","Velma","Kelly",")",",","Susan","Misner","(","as","Liz",")",",","Denise","Faye","(","as","Annie",")",",","Deidre","Goodwin","(","as","June",")",",","Ekaterina","Shchelkanova","(","as","Katalin","Helinszki","nicknamed","the","Hunyak",")","and","Mýa","(","as","Mona",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","B-person","I-person","O","O","B-person","I-person","O","O","B-person","O","O","B-person","I-person","O","O","B-person","O","O","B-person","I-person","O","O","B-person","O","O","B-person","I-person","O","O","B-person","I-person","O","O","B-person","O","O","B-person","O","O","B-person","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, university, protein, scientist, chemical element, location, organization, theory, discipline, enzyme, award, academic journal, chemical compound, country, event, person and O.\nSentence: In the 2002 film , this musical number is performed by Catherine Zeta-Jones ( as Velma Kelly ) , Susan Misner ( as Liz ) , Denise Faye ( as Annie ) , Deidre Goodwin ( as June ) , Ekaterina Shchelkanova ( as Katalin Helinszki nicknamed the Hunyak ) and Mýa ( as Mona ) .","prompt_labels":"In(O) the(O) 2002(O) film(O) ,(O) this(O) musical(O) number(O) is(O) performed(O) by(O) Catherine(B-person) Zeta-Jones(I-person) ((O) as(O) Velma(B-person) Kelly(I-person) )(O) ,(O) Susan(B-person) Misner(I-person) ((O) as(O) Liz(B-person) )(O) ,(O) Denise(B-person) Faye(I-person) ((O) as(O) Annie(B-person) )(O) ,(O) Deidre(B-person) Goodwin(I-person) ((O) as(O) June(B-person) )(O) ,(O) Ekaterina(B-person) Shchelkanova(I-person) ((O) as(O) Katalin(B-person) Helinszki(I-person) nicknamed(O) the(O) Hunyak(B-person) )(O) and(O) Mýa(B-person) ((O) as(O) Mona(B-person) )(O) .(O)"}}
{"id":"96","dataset":"crossner_science","split":"train","label_list":["discipline","person","protein","award","event","chemical element","university","astronomical object","chemical compound","organization","country","enzyme","scientist","location","theory","academic journal"],"instance":{"id":"96","words":["It","contains","abundant","speculation","about","technological","invention",",","including","descriptions","of","a","worldwide","telephone","network",",","solar","power",",","air","travel",",","space","travel","to","the","planets","Saturn","and","Jupiter",",","and","terraforming","engineering","projects","damming","the","Arctic","Ocean",",","and","adjusting","the","Earth","'s","(","Terra",")","axial","tilt","(","by","the","Terrestrial","Axis","Straightening","Company",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, person, protein, award, event, chemical element, university, astronomical object, chemical compound, organization, country, enzyme, scientist, location, theory, academic journal and O.\nSentence: It contains abundant speculation about technological invention , including descriptions of a worldwide telephone network , solar power , air travel , space travel to the planets Saturn and Jupiter , and terraforming engineering projects damming the Arctic Ocean , and adjusting the Earth 's ( Terra ) axial tilt ( by the Terrestrial Axis Straightening Company ) .","prompt_labels":"It(O) contains(O) abundant(O) speculation(O) about(O) technological(O) invention(O) ,(O) including(O) descriptions(O) of(O) a(O) worldwide(O) telephone(O) network(O) ,(O) solar(O) power(O) ,(O) air(O) travel(O) ,(O) space(O) travel(O) to(O) the(O) planets(O) Saturn(B-astronomical object) and(O) Jupiter(B-astronomical object) ,(O) and(O) terraforming(O) engineering(O) projects(O) damming(O) the(O) Arctic(O) Ocean(O) ,(O) and(O) adjusting(O) the(O) Earth(B-astronomical object) 's(O) ((O) Terra(O) )(O) axial(O) tilt(O) ((O) by(O) the(O) Terrestrial(O) Axis(O) Straightening(O) Company(O) )(O) .(O)"}}
{"id":"97","dataset":"crossner_science","split":"train","label_list":["discipline","person","theory","chemical compound","enzyme","country","academic journal","organization","protein","university","scientist","chemical element","astronomical object","award","location","event"],"instance":{"id":"97","words":["Minimum","distances","from","the","Earth",",","Venus",",","and","Jupiter",",","are","0.5",",","0.8",",","and","3.5","AU",",","respectively","."],"labels":["O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, person, theory, chemical compound, enzyme, country, academic journal, organization, protein, university, scientist, chemical element, astronomical object, award, location, event and O.\nSentence: Minimum distances from the Earth , Venus , and Jupiter , are 0.5 , 0.8 , and 3.5 AU , respectively .","prompt_labels":"Minimum(O) distances(O) from(O) the(O) Earth(B-astronomical object) ,(O) Venus(B-astronomical object) ,(O) and(O) Jupiter(B-astronomical object) ,(O) are(O) 0.5(O) ,(O) 0.8(O) ,(O) and(O) 3.5(O) AU(O) ,(O) respectively(O) .(O)"}}
{"id":"98","dataset":"crossner_science","split":"train","label_list":["university","person","award","astronomical object","chemical element","scientist","location","country","discipline","theory","chemical compound","event","enzyme","protein","academic journal","organization"],"instance":{"id":"98","words":["He","visited","some","European","universities","and","institutions",",","including","Cavendish","Laboratory",",","Georg","August","University","of","Göttingen",",","and","University","of","Copenhagen","."],"labels":["O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-university","I-university","I-university","I-university","I-university","O","O","B-university","I-university","I-university","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, person, award, astronomical object, chemical element, scientist, location, country, discipline, theory, chemical compound, event, enzyme, protein, academic journal, organization and O.\nSentence: He visited some European universities and institutions , including Cavendish Laboratory , Georg August University of Göttingen , and University of Copenhagen .","prompt_labels":"He(O) visited(O) some(O) European(O) universities(O) and(O) institutions(O) ,(O) including(O) Cavendish(B-organization) Laboratory(I-organization) ,(O) Georg(B-university) August(I-university) University(I-university) of(I-university) Göttingen(I-university) ,(O) and(O) University(B-university) of(I-university) Copenhagen(I-university) .(O)"}}
{"id":"99","dataset":"crossner_science","split":"train","label_list":["country","location","university","discipline","protein","chemical compound","person","organization","event","theory","scientist","academic journal","enzyme","award","astronomical object","chemical element"],"instance":{"id":"99","words":["One","of","these","was","in","the","category","of","Best","Performance","by","an","Actor","in","a","Leading","Role","in","a","Dramatic","Program","or","Mini-Series","for","Beau","Bridges","'","leading","role","as","Simon","Kress",",","which","also","garnered","nominations","for","the","Primetime","Emmy","Award","for","Outstanding","Guest","Actor","in","a","Drama","Series","and","the","CableACE","Award","for","Best","Actor","in","a","Dramatic","Series","."],"labels":["O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, university, discipline, protein, chemical compound, person, organization, event, theory, scientist, academic journal, enzyme, award, astronomical object, chemical element and O.\nSentence: One of these was in the category of Best Performance by an Actor in a Leading Role in a Dramatic Program or Mini-Series for Beau Bridges ' leading role as Simon Kress , which also garnered nominations for the Primetime Emmy Award for Outstanding Guest Actor in a Drama Series and the CableACE Award for Best Actor in a Dramatic Series .","prompt_labels":"One(O) of(O) these(O) was(O) in(O) the(O) category(O) of(O) Best(B-award) Performance(I-award) by(O) an(O) Actor(O) in(O) a(O) Leading(O) Role(O) in(O) a(O) Dramatic(O) Program(O) or(O) Mini-Series(O) for(O) Beau(O) Bridges(O) '(O) leading(O) role(O) as(O) Simon(B-person) Kress(I-person) ,(O) which(O) also(O) garnered(O) nominations(O) for(O) the(O) Primetime(B-award) Emmy(I-award) Award(I-award) for(I-award) Outstanding(I-award) Guest(I-award) Actor(I-award) in(O) a(O) Drama(O) Series(O) and(O) the(O) CableACE(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) in(O) a(O) Dramatic(O) Series(O) .(O)"}}
{"id":"100","dataset":"crossner_science","split":"train","label_list":["protein","country","university","award","chemical compound","theory","event","academic journal","scientist","discipline","chemical element","enzyme","astronomical object","organization","location","person"],"instance":{"id":"100","words":["The","Fall","of","Singapore","in","February","1942","led","him","to","offer","his","services","to","John","Madsen",",","the","Professor","of","Electrical","Engineering","at","the","University","of","Sydney",",","and","the","head","of","the","Radiophysics","Laboratory","at","the","Council","for","Scientific","and","Industrial","Research",",","which","was","responsible","for","developing","radar","."],"labels":["O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","B-discipline","I-discipline","O","O","B-university","I-university","I-university","O","O","O","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, country, university, award, chemical compound, theory, event, academic journal, scientist, discipline, chemical element, enzyme, astronomical object, organization, location, person and O.\nSentence: The Fall of Singapore in February 1942 led him to offer his services to John Madsen , the Professor of Electrical Engineering at the University of Sydney , and the head of the Radiophysics Laboratory at the Council for Scientific and Industrial Research , which was responsible for developing radar .","prompt_labels":"The(O) Fall(B-event) of(I-event) Singapore(I-event) in(O) February(O) 1942(O) led(O) him(O) to(O) offer(O) his(O) services(O) to(O) John(B-scientist) Madsen(I-scientist) ,(O) the(O) Professor(O) of(O) Electrical(B-discipline) Engineering(I-discipline) at(O) the(O) University(B-university) of(I-university) Sydney(I-university) ,(O) and(O) the(O) head(O) of(O) the(O) Radiophysics(B-organization) Laboratory(I-organization) at(O) the(O) Council(B-organization) for(I-organization) Scientific(I-organization) and(I-organization) Industrial(I-organization) Research(I-organization) ,(O) which(O) was(O) responsible(O) for(O) developing(O) radar(O) .(O)"}}
{"id":"101","dataset":"crossner_science","split":"train","label_list":["enzyme","astronomical object","chemical compound","protein","chemical element","event","scientist","country","academic journal","award","organization","discipline","location","theory","person","university"],"instance":{"id":"101","words":["After","that",",","they","present","these","peptides","in","complexes","together","with","their","Major","histocompatibility","complex","molecules","on","their","cell","surface","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-protein","I-protein","I-protein","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, astronomical object, chemical compound, protein, chemical element, event, scientist, country, academic journal, award, organization, discipline, location, theory, person, university and O.\nSentence: After that , they present these peptides in complexes together with their Major histocompatibility complex molecules on their cell surface .","prompt_labels":"After(O) that(O) ,(O) they(O) present(O) these(O) peptides(O) in(O) complexes(O) together(O) with(O) their(O) Major(B-protein) histocompatibility(I-protein) complex(I-protein) molecules(O) on(O) their(O) cell(O) surface(O) .(O)"}}
{"id":"102","dataset":"crossner_science","split":"train","label_list":["enzyme","discipline","location","astronomical object","academic journal","person","scientist","chemical compound","country","chemical element","theory","university","event","award","organization","protein"],"instance":{"id":"102","words":["She","represented","her","country","at","the","2017","World","Championships","in","Athletics","without","reaching","the","semifinals","."],"labels":["O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, discipline, location, astronomical object, academic journal, person, scientist, chemical compound, country, chemical element, theory, university, event, award, organization, protein and O.\nSentence: She represented her country at the 2017 World Championships in Athletics without reaching the semifinals .","prompt_labels":"She(O) represented(O) her(O) country(O) at(O) the(O) 2017(B-event) World(I-event) Championships(I-event) in(I-event) Athletics(I-event) without(O) reaching(O) the(O) semifinals(O) .(O)"}}
{"id":"103","dataset":"crossner_science","split":"train","label_list":["chemical compound","enzyme","scientist","person","academic journal","award","event","chemical element","protein","location","theory","university","discipline","astronomical object","country","organization"],"instance":{"id":"103","words":["He","was","awarded","honorary","degree","from","the","University","of","Cambridge","in","Cambridge",",","UK",",","and","the","Copley","Medal","of","the","Royal","Society","in","1906","."],"labels":["O","O","O","O","O","O","O","B-university","I-university","I-university","O","B-location","O","B-country","O","O","O","B-award","I-award","O","O","B-organization","I-organization","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, enzyme, scientist, person, academic journal, award, event, chemical element, protein, location, theory, university, discipline, astronomical object, country, organization and O.\nSentence: He was awarded honorary degree from the University of Cambridge in Cambridge , UK , and the Copley Medal of the Royal Society in 1906 .","prompt_labels":"He(O) was(O) awarded(O) honorary(O) degree(O) from(O) the(O) University(B-university) of(I-university) Cambridge(I-university) in(O) Cambridge(B-location) ,(O) UK(B-country) ,(O) and(O) the(O) Copley(B-award) Medal(I-award) of(O) the(O) Royal(B-organization) Society(I-organization) in(O) 1906(O) .(O)"}}
{"id":"104","dataset":"crossner_science","split":"train","label_list":["protein","chemical compound","chemical element","university","astronomical object","enzyme","location","person","country","discipline","event","award","theory","scientist","organization","academic journal"],"instance":{"id":"104","words":["The","Big","Bend","Country","is","part","of","the","larger","Columbia","Country",",","which","includes","the","Columbia","Valley","and","upper","Arrow","Lakes","."],"labels":["O","B-location","I-location","I-location","O","O","O","O","O","B-location","I-location","O","O","O","O","B-location","I-location","O","O","B-location","I-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, chemical compound, chemical element, university, astronomical object, enzyme, location, person, country, discipline, event, award, theory, scientist, organization, academic journal and O.\nSentence: The Big Bend Country is part of the larger Columbia Country , which includes the Columbia Valley and upper Arrow Lakes .","prompt_labels":"The(O) Big(B-location) Bend(I-location) Country(I-location) is(O) part(O) of(O) the(O) larger(O) Columbia(B-location) Country(I-location) ,(O) which(O) includes(O) the(O) Columbia(B-location) Valley(I-location) and(O) upper(O) Arrow(B-location) Lakes(I-location) .(O)"}}
{"id":"105","dataset":"crossner_science","split":"train","label_list":["chemical compound","theory","award","university","enzyme","location","country","protein","event","chemical element","academic journal","scientist","person","discipline","organization","astronomical object"],"instance":{"id":"105","words":["Xu","noted","that","under","the","near-null","magnetic","field",",","Arabidopsis","thaliana","delays","the","flowering","time","by","altering","the","transcription","level","of","three","cryptochrome","related","florigen","genes",":","Phytochrome",",","CO",",","and","FT",";","Arabidopsis","thaliana","also","induced","longer","hypocotyl","length","under","white","light","in","the","Near-Null","magnetic","field","compared","to","standard","geomagnetic","field","and","either","dark","or","white","light","conditions","."],"labels":["B-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, theory, award, university, enzyme, location, country, protein, event, chemical element, academic journal, scientist, person, discipline, organization, astronomical object and O.\nSentence: Xu noted that under the near-null magnetic field , Arabidopsis thaliana delays the flowering time by altering the transcription level of three cryptochrome related florigen genes : Phytochrome , CO , and FT ; Arabidopsis thaliana also induced longer hypocotyl length under white light in the Near-Null magnetic field compared to standard geomagnetic field and either dark or white light conditions .","prompt_labels":"Xu(B-scientist) noted(O) that(O) under(O) the(O) near-null(O) magnetic(O) field(O) ,(O) Arabidopsis(O) thaliana(O) delays(O) the(O) flowering(O) time(O) by(O) altering(O) the(O) transcription(O) level(O) of(O) three(O) cryptochrome(O) related(O) florigen(O) genes(O) :(O) Phytochrome(O) ,(O) CO(O) ,(O) and(O) FT(O) ;(O) Arabidopsis(O) thaliana(O) also(O) induced(O) longer(O) hypocotyl(O) length(O) under(O) white(O) light(O) in(O) the(O) Near-Null(O) magnetic(O) field(O) compared(O) to(O) standard(O) geomagnetic(O) field(O) and(O) either(O) dark(O) or(O) white(O) light(O) conditions(O) .(O)"}}
{"id":"106","dataset":"crossner_science","split":"train","label_list":["enzyme","chemical compound","chemical element","scientist","university","event","protein","theory","country","person","discipline","astronomical object","award","academic journal","location","organization"],"instance":{"id":"106","words":["It","will","use","the","gravity","assist","technique","with","Earth","once",",","with","Venus","twice",",","and","six","times","with","Mercury","."],"labels":["O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","B-astronomical object","O","O","O","O","O","O","B-astronomical object","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical compound, chemical element, scientist, university, event, protein, theory, country, person, discipline, astronomical object, award, academic journal, location, organization and O.\nSentence: It will use the gravity assist technique with Earth once , with Venus twice , and six times with Mercury .","prompt_labels":"It(O) will(O) use(O) the(O) gravity(O) assist(O) technique(O) with(O) Earth(B-astronomical object) once(O) ,(O) with(O) Venus(B-astronomical object) twice(O) ,(O) and(O) six(O) times(O) with(O) Mercury(B-astronomical object) .(O)"}}
{"id":"107","dataset":"crossner_science","split":"train","label_list":["country","discipline","award","scientist","protein","event","enzyme","university","organization","chemical compound","location","theory","chemical element","astronomical object","academic journal","person"],"instance":{"id":"107","words":["Other","toxins","with","a","similar","mode","of","action","to","ATX-II","are","Scorpion","toxin","."],"labels":["O","O","O","O","O","O","O","O","O","B-chemical compound","O","B-protein","I-protein","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, discipline, award, scientist, protein, event, enzyme, university, organization, chemical compound, location, theory, chemical element, astronomical object, academic journal, person and O.\nSentence: Other toxins with a similar mode of action to ATX-II are Scorpion toxin .","prompt_labels":"Other(O) toxins(O) with(O) a(O) similar(O) mode(O) of(O) action(O) to(O) ATX-II(B-chemical compound) are(O) Scorpion(B-protein) toxin(I-protein) .(O)"}}
{"id":"108","dataset":"crossner_science","split":"train","label_list":["country","award","event","chemical element","organization","scientist","university","astronomical object","location","protein","person","enzyme","theory","discipline","academic journal","chemical compound"],"instance":{"id":"108","words":["Harold","Washington","College","is","a","City","Colleges","of","Chicago","community","college","located","in","the","Loop","."],"labels":["B-university","I-university","I-university","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, event, chemical element, organization, scientist, university, astronomical object, location, protein, person, enzyme, theory, discipline, academic journal, chemical compound and O.\nSentence: Harold Washington College is a City Colleges of Chicago community college located in the Loop .","prompt_labels":"Harold(B-university) Washington(I-university) College(I-university) is(O) a(O) City(O) Colleges(O) of(O) Chicago(B-organization) community(I-organization) college(I-organization) located(O) in(O) the(O) Loop(B-location) .(O)"}}
{"id":"109","dataset":"crossner_science","split":"train","label_list":["astronomical object","organization","enzyme","university","academic journal","award","location","chemical compound","discipline","country","person","scientist","theory","protein","chemical element","event"],"instance":{"id":"109","words":["This","is","because","those","with","larger","semi-major","axes","have","larger","libration","amplitudes",",","with","amplitudes","~","70","°","and","above","being","destabilized","by","secondary","resonances","between","the","trojan","motion","and","the","dynamics","of","at","least","Saturn",",","Uranus",",","and","Neptune","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","B-astronomical object","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, organization, enzyme, university, academic journal, award, location, chemical compound, discipline, country, person, scientist, theory, protein, chemical element, event and O.\nSentence: This is because those with larger semi-major axes have larger libration amplitudes , with amplitudes ~ 70 ° and above being destabilized by secondary resonances between the trojan motion and the dynamics of at least Saturn , Uranus , and Neptune .","prompt_labels":"This(O) is(O) because(O) those(O) with(O) larger(O) semi-major(O) axes(O) have(O) larger(O) libration(O) amplitudes(O) ,(O) with(O) amplitudes(O) ~(O) 70(O) °(O) and(O) above(O) being(O) destabilized(O) by(O) secondary(O) resonances(O) between(O) the(O) trojan(O) motion(O) and(O) the(O) dynamics(O) of(O) at(O) least(O) Saturn(B-astronomical object) ,(O) Uranus(B-astronomical object) ,(O) and(O) Neptune(B-astronomical object) .(O)"}}
{"id":"110","dataset":"crossner_science","split":"train","label_list":["discipline","country","scientist","location","university","academic journal","event","chemical compound","award","chemical element","person","theory","protein","enzyme","organization","astronomical object"],"instance":{"id":"110","words":["The","Patriot","was","nominated","for","three","Academy","Awards",":","Academy","Award","for","Best","Sound","Mixing","(","Kevin","O","'Connell",",","Greg","P.","Russell","and","Lee","Orloff",")",",","Academy","Award","for","Best","Cinematography",",","and","Academy","Award","for","Best","Original","Score","."],"labels":["O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-person","I-person","I-person","O","B-person","I-person","I-person","O","B-person","I-person","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, country, scientist, location, university, academic journal, event, chemical compound, award, chemical element, person, theory, protein, enzyme, organization, astronomical object and O.\nSentence: The Patriot was nominated for three Academy Awards : Academy Award for Best Sound Mixing ( Kevin O 'Connell , Greg P. Russell and Lee Orloff ) , Academy Award for Best Cinematography , and Academy Award for Best Original Score .","prompt_labels":"The(O) Patriot(O) was(O) nominated(O) for(O) three(O) Academy(O) Awards(O) :(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Sound(I-award) Mixing(I-award) ((O) Kevin(B-person) O(I-person) 'Connell(I-person) ,(O) Greg(B-person) P.(I-person) Russell(I-person) and(O) Lee(B-person) Orloff(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Cinematography(I-award) ,(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) .(O)"}}
{"id":"111","dataset":"crossner_science","split":"train","label_list":["theory","protein","enzyme","organization","astronomical object","academic journal","scientist","chemical element","person","country","discipline","event","chemical compound","location","award","university"],"instance":{"id":"111","words":["She","is","one","of","only","six","female","artists","(","including","Reba","McEntire",",","Barbara","Mandrell",",","Shania","Twain",",","Loretta","Lynn",",","and","Taylor","Swift",")",",","to","win","the","Country","Music","Association","'s","highest","honor",",","Entertainer","of","the","Year","(","1978",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","B-person","I-person","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, protein, enzyme, organization, astronomical object, academic journal, scientist, chemical element, person, country, discipline, event, chemical compound, location, award, university and O.\nSentence: She is one of only six female artists ( including Reba McEntire , Barbara Mandrell , Shania Twain , Loretta Lynn , and Taylor Swift ) , to win the Country Music Association 's highest honor , Entertainer of the Year ( 1978 ) .","prompt_labels":"She(O) is(O) one(O) of(O) only(O) six(O) female(O) artists(O) ((O) including(O) Reba(B-person) McEntire(I-person) ,(O) Barbara(B-person) Mandrell(I-person) ,(O) Shania(B-person) Twain(I-person) ,(O) Loretta(B-person) Lynn(I-person) ,(O) and(O) Taylor(B-person) Swift(I-person) )(O) ,(O) to(O) win(O) the(O) Country(B-organization) Music(I-organization) Association(I-organization) 's(O) highest(O) honor(O) ,(O) Entertainer(O) of(O) the(O) Year(O) ((O) 1978(O) )(O) .(O)"}}
{"id":"112","dataset":"crossner_science","split":"train","label_list":["university","event","protein","theory","organization","discipline","academic journal","astronomical object","enzyme","location","country","person","chemical compound","award","scientist","chemical element"],"instance":{"id":"112","words":["This","minor","planet","was","named","by","the","discoverer","in","memory","of","English","astronomers","Charles","Mason","(","1728-1786",")","and","Jeremiah","Dixon","(","1733-1779",")",",","who","observed","the","1761","transit","of","Venus","from","the","Cape","of","Good","Hope","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O","O","B-location","I-location","I-location","I-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, event, protein, theory, organization, discipline, academic journal, astronomical object, enzyme, location, country, person, chemical compound, award, scientist, chemical element and O.\nSentence: This minor planet was named by the discoverer in memory of English astronomers Charles Mason ( 1728-1786 ) and Jeremiah Dixon ( 1733-1779 ) , who observed the 1761 transit of Venus from the Cape of Good Hope .","prompt_labels":"This(O) minor(O) planet(O) was(O) named(O) by(O) the(O) discoverer(O) in(O) memory(O) of(O) English(O) astronomers(O) Charles(B-scientist) Mason(I-scientist) ((O) 1728-1786(O) )(O) and(O) Jeremiah(B-scientist) Dixon(I-scientist) ((O) 1733-1779(O) )(O) ,(O) who(O) observed(O) the(O) 1761(B-event) transit(I-event) of(I-event) Venus(I-event) from(O) the(O) Cape(B-location) of(I-location) Good(I-location) Hope(I-location) .(O)"}}
{"id":"113","dataset":"crossner_science","split":"train","label_list":["person","event","chemical compound","award","country","chemical element","scientist","astronomical object","protein","academic journal","university","theory","enzyme","location","organization","discipline"],"instance":{"id":"113","words":["Among","those","in","attendance","were","Werner","Heisenberg",",","Carl","Ramsauer",",","Wolfgang","Finkelnburg",",","Carl","Friedrich","von","Weizsäcker",",","Juilfs",",","as","well","as","supporters","of","the","declining","deutsche","Physik","movement","."],"labels":["O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","I-scientist","O","B-scientist","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, chemical compound, award, country, chemical element, scientist, astronomical object, protein, academic journal, university, theory, enzyme, location, organization, discipline and O.\nSentence: Among those in attendance were Werner Heisenberg , Carl Ramsauer , Wolfgang Finkelnburg , Carl Friedrich von Weizsäcker , Juilfs , as well as supporters of the declining deutsche Physik movement .","prompt_labels":"Among(O) those(O) in(O) attendance(O) were(O) Werner(B-scientist) Heisenberg(I-scientist) ,(O) Carl(B-scientist) Ramsauer(I-scientist) ,(O) Wolfgang(B-scientist) Finkelnburg(I-scientist) ,(O) Carl(B-scientist) Friedrich(I-scientist) von(I-scientist) Weizsäcker(I-scientist) ,(O) Juilfs(B-scientist) ,(O) as(O) well(O) as(O) supporters(O) of(O) the(O) declining(O) deutsche(B-event) Physik(I-event) movement(I-event) .(O)"}}
{"id":"114","dataset":"crossner_science","split":"train","label_list":["enzyme","organization","country","theory","scientist","chemical element","university","discipline","chemical compound","person","event","astronomical object","location","protein","academic journal","award"],"instance":{"id":"114","words":["CoRoT-7b","and","CoRoT-9b","have","already","been","observed","by","Spitzer","."],"labels":["B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, organization, country, theory, scientist, chemical element, university, discipline, chemical compound, person, event, astronomical object, location, protein, academic journal, award and O.\nSentence: CoRoT-7b and CoRoT-9b have already been observed by Spitzer .","prompt_labels":"CoRoT-7b(B-astronomical object) and(O) CoRoT-9b(B-astronomical object) have(O) already(O) been(O) observed(O) by(O) Spitzer(O) .(O)"}}
{"id":"115","dataset":"crossner_science","split":"train","label_list":["scientist","chemical element","organization","discipline","chemical compound","university","award","country","theory","academic journal","event","enzyme","protein","location","astronomical object","person"],"instance":{"id":"115","words":["Stockport","played","for","eight","seasons","from","the","1895-96","season","to","the","end","of","1902-1903","season",",","the","latter","two","seasons","played","at","Edgeley","Park",",","the","club","finished","17th","of","22","in","the","initial","combined","league",",","then","5th",",","11th",",","11th",",","9th",",","12th",",","6th",",","in","the","14-club","Lancashire","Senior","Competition",",","and","then","18th","of","18","in","Division","2","of","the","recombined","league",",","after","which","it","withdrew","from","the","Northern","Rugby","Football","Union","."],"labels":["B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, chemical element, organization, discipline, chemical compound, university, award, country, theory, academic journal, event, enzyme, protein, location, astronomical object, person and O.\nSentence: Stockport played for eight seasons from the 1895-96 season to the end of 1902-1903 season , the latter two seasons played at Edgeley Park , the club finished 17th of 22 in the initial combined league , then 5th , 11th , 11th , 9th , 12th , 6th , in the 14-club Lancashire Senior Competition , and then 18th of 18 in Division 2 of the recombined league , after which it withdrew from the Northern Rugby Football Union .","prompt_labels":"Stockport(B-location) played(O) for(O) eight(O) seasons(O) from(O) the(O) 1895-96(O) season(O) to(O) the(O) end(O) of(O) 1902-1903(O) season(O) ,(O) the(O) latter(O) two(O) seasons(O) played(O) at(O) Edgeley(B-location) Park(I-location) ,(O) the(O) club(O) finished(O) 17th(O) of(O) 22(O) in(O) the(O) initial(O) combined(O) league(O) ,(O) then(O) 5th(O) ,(O) 11th(O) ,(O) 11th(O) ,(O) 9th(O) ,(O) 12th(O) ,(O) 6th(O) ,(O) in(O) the(O) 14-club(O) Lancashire(B-event) Senior(I-event) Competition(I-event) ,(O) and(O) then(O) 18th(O) of(O) 18(O) in(O) Division(O) 2(O) of(O) the(O) recombined(O) league(O) ,(O) after(O) which(O) it(O) withdrew(O) from(O) the(O) Northern(B-organization) Rugby(I-organization) Football(I-organization) Union(I-organization) .(O)"}}
{"id":"116","dataset":"crossner_science","split":"train","label_list":["chemical compound","protein","chemical element","location","theory","organization","discipline","person","event","astronomical object","academic journal","enzyme","award","country","university","scientist"],"instance":{"id":"116","words":["The","gaseous","outer","atmospheres","of","Jupiter","and","Saturn","transition","smoothly","into","the","dense","liquid","interior",",","while","the","nature","of","the","transition","zones","of","Neptune","and","Uranus","is","unknown","."],"labels":["O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, protein, chemical element, location, theory, organization, discipline, person, event, astronomical object, academic journal, enzyme, award, country, university, scientist and O.\nSentence: The gaseous outer atmospheres of Jupiter and Saturn transition smoothly into the dense liquid interior , while the nature of the transition zones of Neptune and Uranus is unknown .","prompt_labels":"The(O) gaseous(O) outer(O) atmospheres(O) of(O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) transition(O) smoothly(O) into(O) the(O) dense(O) liquid(O) interior(O) ,(O) while(O) the(O) nature(O) of(O) the(O) transition(O) zones(O) of(O) Neptune(B-astronomical object) and(O) Uranus(B-astronomical object) is(O) unknown(O) .(O)"}}
{"id":"117","dataset":"crossner_science","split":"train","label_list":["theory","organization","location","university","discipline","academic journal","award","enzyme","protein","chemical element","person","astronomical object","country","event","chemical compound","scientist"],"instance":{"id":"117","words":["The","outer","gas","giant","planets","are","Jupiter",",","Saturn",",","Uranus",",","and","Neptune","."],"labels":["O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","B-astronomical object","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, organization, location, university, discipline, academic journal, award, enzyme, protein, chemical element, person, astronomical object, country, event, chemical compound, scientist and O.\nSentence: The outer gas giant planets are Jupiter , Saturn , Uranus , and Neptune .","prompt_labels":"The(O) outer(O) gas(O) giant(O) planets(O) are(O) Jupiter(B-astronomical object) ,(O) Saturn(B-astronomical object) ,(O) Uranus(B-astronomical object) ,(O) and(O) Neptune(B-astronomical object) .(O)"}}
{"id":"118","dataset":"crossner_science","split":"train","label_list":["protein","location","chemical element","scientist","enzyme","person","academic journal","award","discipline","organization","country","chemical compound","astronomical object","theory","event","university"],"instance":{"id":"118","words":["In","1970",",","he","was","awarded","the","Louisa","Gross","Horwitz","Prize","from","Columbia","University","together","with","Renato","Dulbecco","winner","of","1975","Nobel","Prize","in","Physiology","or","Medicine","for","discoveries","concerning","the","functional","organization","of","the","cell","that","were","seminal","events","in","the","development","of","modern","cell","biology",",","In","1988","he","was","also","elected","an","Honorary","Member","of","the","American-Romanian","Academy","of","Arts","and","Sciences","(","ARA",")","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","B-university","I-university","O","O","B-scientist","I-scientist","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-discipline","I-discipline","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, location, chemical element, scientist, enzyme, person, academic journal, award, discipline, organization, country, chemical compound, astronomical object, theory, event, university and O.\nSentence: In 1970 , he was awarded the Louisa Gross Horwitz Prize from Columbia University together with Renato Dulbecco winner of 1975 Nobel Prize in Physiology or Medicine for discoveries concerning the functional organization of the cell that were seminal events in the development of modern cell biology , In 1988 he was also elected an Honorary Member of the American-Romanian Academy of Arts and Sciences ( ARA ) .","prompt_labels":"In(O) 1970(O) ,(O) he(O) was(O) awarded(O) the(O) Louisa(B-award) Gross(I-award) Horwitz(I-award) Prize(I-award) from(O) Columbia(B-university) University(I-university) together(O) with(O) Renato(B-scientist) Dulbecco(I-scientist) winner(O) of(O) 1975(O) Nobel(B-award) Prize(I-award) in(I-award) Physiology(I-award) or(I-award) Medicine(I-award) for(O) discoveries(O) concerning(O) the(O) functional(O) organization(O) of(O) the(O) cell(O) that(O) were(O) seminal(O) events(O) in(O) the(O) development(O) of(O) modern(O) cell(B-discipline) biology(I-discipline) ,(O) In(O) 1988(O) he(O) was(O) also(O) elected(O) an(O) Honorary(O) Member(O) of(O) the(O) American-Romanian(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) ((O) ARA(B-organization) )(O) .(O)"}}
{"id":"119","dataset":"crossner_science","split":"train","label_list":["astronomical object","event","discipline","location","organization","person","protein","scientist","university","chemical compound","theory","enzyme","award","country","chemical element","academic journal"],"instance":{"id":"119","words":["Al-Farabi","'s","school","of","philosophy","breaks","with","the","philosophy","of","Plato","and","Aristotle","...","and","...","moves","from","metaphysics","to","methodology",",","a","move","that","anticipates","modernity",",","and","at","the","level","of","philosophy",",","Alfarabi","unites","theory","and","practice","...","and","in","the","sphere","of","the","Politics","he","liberates","practice","from","theory","."],"labels":["B-person","O","B-organization","I-organization","I-organization","O","O","O","B-discipline","O","B-person","O","B-person","O","O","O","O","O","B-theory","O","B-theory","O","O","O","O","O","O","O","O","O","O","O","O","B-discipline","O","B-person","O","O","O","O","O","O","O","O","O","O","O","B-discipline","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, event, discipline, location, organization, person, protein, scientist, university, chemical compound, theory, enzyme, award, country, chemical element, academic journal and O.\nSentence: Al-Farabi 's school of philosophy breaks with the philosophy of Plato and Aristotle ... and ... moves from metaphysics to methodology , a move that anticipates modernity , and at the level of philosophy , Alfarabi unites theory and practice ... and in the sphere of the Politics he liberates practice from theory .","prompt_labels":"Al-Farabi(B-person) 's(O) school(B-organization) of(I-organization) philosophy(I-organization) breaks(O) with(O) the(O) philosophy(B-discipline) of(O) Plato(B-person) and(O) Aristotle(B-person) ...(O) and(O) ...(O) moves(O) from(O) metaphysics(B-theory) to(O) methodology(B-theory) ,(O) a(O) move(O) that(O) anticipates(O) modernity(O) ,(O) and(O) at(O) the(O) level(O) of(O) philosophy(B-discipline) ,(O) Alfarabi(B-person) unites(O) theory(O) and(O) practice(O) ...(O) and(O) in(O) the(O) sphere(O) of(O) the(O) Politics(B-discipline) he(O) liberates(O) practice(O) from(O) theory(O) .(O)"}}
{"id":"120","dataset":"crossner_science","split":"train","label_list":["award","event","astronomical object","protein","university","location","chemical element","country","enzyme","person","scientist","discipline","academic journal","organization","chemical compound","theory"],"instance":{"id":"120","words":["The","American","Breeders","Association","held","its","first","meeting","in","1903","to","discuss","the","new","science","of","genetics","that","arose","from","Charles","Darwin","s","theory","of","evolution","and","Gregor","Mendel","s","discoveries","of","the","laws","of","inheritance","."],"labels":["O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-theory","I-theory","I-theory","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, astronomical object, protein, university, location, chemical element, country, enzyme, person, scientist, discipline, academic journal, organization, chemical compound, theory and O.\nSentence: The American Breeders Association held its first meeting in 1903 to discuss the new science of genetics that arose from Charles Darwin s theory of evolution and Gregor Mendel s discoveries of the laws of inheritance .","prompt_labels":"The(O) American(B-organization) Breeders(I-organization) Association(I-organization) held(O) its(O) first(O) meeting(O) in(O) 1903(O) to(O) discuss(O) the(O) new(O) science(O) of(O) genetics(O) that(O) arose(O) from(O) Charles(B-scientist) Darwin(I-scientist) s(O) theory(B-theory) of(I-theory) evolution(I-theory) and(O) Gregor(B-scientist) Mendel(I-scientist) s(O) discoveries(O) of(O) the(O) laws(O) of(O) inheritance(O) .(O)"}}
{"id":"121","dataset":"crossner_science","split":"train","label_list":["protein","country","organization","university","chemical compound","award","scientist","discipline","astronomical object","academic journal","location","chemical element","person","enzyme","event","theory"],"instance":{"id":"121","words":["Rhea","was","not","named","until","1847",",","when","John","Herschel","(","son","of","William","Herschel",",","discoverer","of","the","planet","Uranus",",","and","two","other","moons","of","Saturn",",","Mimas","and","Enceladus",")","suggested","in","Results","of","Astronomical","Observations","made","at","the","Cape","of","Good","Hope","that","the","names","of","the","Titans",",","sisters","and","brothers","of","Kronos","(","Saturn",",","in","Roman","mythology",")",",","be","used","."],"labels":["B-astronomical object","O","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O","B-scientist","I-scientist","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","O","O","O","O","B-astronomical object","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, country, organization, university, chemical compound, award, scientist, discipline, astronomical object, academic journal, location, chemical element, person, enzyme, event, theory and O.\nSentence: Rhea was not named until 1847 , when John Herschel ( son of William Herschel , discoverer of the planet Uranus , and two other moons of Saturn , Mimas and Enceladus ) suggested in Results of Astronomical Observations made at the Cape of Good Hope that the names of the Titans , sisters and brothers of Kronos ( Saturn , in Roman mythology ) , be used .","prompt_labels":"Rhea(B-astronomical object) was(O) not(O) named(O) until(O) 1847(O) ,(O) when(O) John(B-scientist) Herschel(I-scientist) ((O) son(O) of(O) William(B-scientist) Herschel(I-scientist) ,(O) discoverer(O) of(O) the(O) planet(O) Uranus(B-astronomical object) ,(O) and(O) two(O) other(O) moons(O) of(O) Saturn(B-astronomical object) ,(O) Mimas(B-astronomical object) and(O) Enceladus(B-astronomical object) )(O) suggested(O) in(O) Results(O) of(O) Astronomical(O) Observations(O) made(O) at(O) the(O) Cape(B-location) of(I-location) Good(I-location) Hope(I-location) that(O) the(O) names(O) of(O) the(O) Titans(B-astronomical object) ,(O) sisters(O) and(O) brothers(O) of(O) Kronos(B-astronomical object) ((O) Saturn(B-astronomical object) ,(O) in(O) Roman(O) mythology(O) )(O) ,(O) be(O) used(O) .(O)"}}
{"id":"122","dataset":"crossner_science","split":"train","label_list":["protein","organization","discipline","chemical element","scientist","astronomical object","event","enzyme","person","award","chemical compound","location","university","academic journal","theory","country"],"instance":{"id":"122","words":["ChemBioChem","is","a","sister","publication","to","other","scientific","journal","s","published","by","Wiley-VCH",",","including","Angewandte","Chemie",",","ChemMedChem",",","ChemPhysChem",",","ChemSusChem",",","ChemCatChem",",","and","ChemistryViews","."],"labels":["B-academic journal","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","B-academic journal","I-academic journal","O","B-academic journal","O","B-academic journal","O","B-academic journal","O","B-academic journal","O","O","B-academic journal","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, organization, discipline, chemical element, scientist, astronomical object, event, enzyme, person, award, chemical compound, location, university, academic journal, theory, country and O.\nSentence: ChemBioChem is a sister publication to other scientific journal s published by Wiley-VCH , including Angewandte Chemie , ChemMedChem , ChemPhysChem , ChemSusChem , ChemCatChem , and ChemistryViews .","prompt_labels":"ChemBioChem(B-academic journal) is(O) a(O) sister(O) publication(O) to(O) other(O) scientific(O) journal(O) s(O) published(O) by(O) Wiley-VCH(B-organization) ,(O) including(O) Angewandte(B-academic journal) Chemie(I-academic journal) ,(O) ChemMedChem(B-academic journal) ,(O) ChemPhysChem(B-academic journal) ,(O) ChemSusChem(B-academic journal) ,(O) ChemCatChem(B-academic journal) ,(O) and(O) ChemistryViews(B-academic journal) .(O)"}}
{"id":"123","dataset":"crossner_science","split":"train","label_list":["country","scientist","person","theory","award","event","protein","chemical element","organization","location","academic journal","university","enzyme","chemical compound","discipline","astronomical object"],"instance":{"id":"123","words":["In","the","Early","Modern","period",",","scientists","such","as","William","Harvey","in","England","and","Galileo","Galilei","in","Italy","reacted","against","the","theories","of","Aristotle","and","other","classical","era","thinkers","like","Galen",",","establishing","new","theories","based","to","some","degree","on","observation","and","experiment","."],"labels":["O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-country","O","B-scientist","I-scientist","O","B-country","O","O","O","O","O","B-person","O","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, scientist, person, theory, award, event, protein, chemical element, organization, location, academic journal, university, enzyme, chemical compound, discipline, astronomical object and O.\nSentence: In the Early Modern period , scientists such as William Harvey in England and Galileo Galilei in Italy reacted against the theories of Aristotle and other classical era thinkers like Galen , establishing new theories based to some degree on observation and experiment .","prompt_labels":"In(O) the(O) Early(O) Modern(O) period(O) ,(O) scientists(O) such(O) as(O) William(B-scientist) Harvey(I-scientist) in(O) England(B-country) and(O) Galileo(B-scientist) Galilei(I-scientist) in(O) Italy(B-country) reacted(O) against(O) the(O) theories(O) of(O) Aristotle(B-person) and(O) other(O) classical(O) era(O) thinkers(O) like(O) Galen(B-person) ,(O) establishing(O) new(O) theories(O) based(O) to(O) some(O) degree(O) on(O) observation(O) and(O) experiment(O) .(O)"}}
{"id":"124","dataset":"crossner_science","split":"train","label_list":["chemical element","university","astronomical object","scientist","country","discipline","academic journal","award","person","enzyme","protein","theory","event","chemical compound","organization","location"],"instance":{"id":"124","words":["Ubiquitin","ligase","are","proteins","that","assist","in","tagging","their","targets","with","an","epigenetic","mark","known","as","ubiquitin","."],"labels":["B-enzyme","I-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, university, astronomical object, scientist, country, discipline, academic journal, award, person, enzyme, protein, theory, event, chemical compound, organization, location and O.\nSentence: Ubiquitin ligase are proteins that assist in tagging their targets with an epigenetic mark known as ubiquitin .","prompt_labels":"Ubiquitin(B-enzyme) ligase(I-enzyme) are(O) proteins(O) that(O) assist(O) in(O) tagging(O) their(O) targets(O) with(O) an(O) epigenetic(O) mark(O) known(O) as(O) ubiquitin(B-protein) .(O)"}}
{"id":"125","dataset":"crossner_science","split":"train","label_list":["organization","theory","scientist","location","award","enzyme","discipline","academic journal","country","university","event","person","astronomical object","protein","chemical element","chemical compound"],"instance":{"id":"125","words":["He","then","began","working","at","the","Institute","of","Tropical","Medicine","Antwerp","while","pursuing","a","graduate","degree","in","clinical","microbiology","from","the","University","of","Antwerp","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","B-discipline","I-discipline","O","O","B-university","I-university","I-university","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, theory, scientist, location, award, enzyme, discipline, academic journal, country, university, event, person, astronomical object, protein, chemical element, chemical compound and O.\nSentence: He then began working at the Institute of Tropical Medicine Antwerp while pursuing a graduate degree in clinical microbiology from the University of Antwerp .","prompt_labels":"He(O) then(O) began(O) working(O) at(O) the(O) Institute(B-organization) of(I-organization) Tropical(I-organization) Medicine(I-organization) Antwerp(I-organization) while(O) pursuing(O) a(O) graduate(O) degree(O) in(O) clinical(B-discipline) microbiology(I-discipline) from(O) the(O) University(B-university) of(I-university) Antwerp(I-university) .(O)"}}
{"id":"126","dataset":"crossner_science","split":"train","label_list":["university","chemical compound","award","organization","scientist","protein","location","enzyme","academic journal","chemical element","event","person","country","discipline","astronomical object","theory"],"instance":{"id":"126","words":["5145","Pholus","in","diameter",",","that","crosses","the","orbit","of","both","Saturn","and","Neptune","."],"labels":["B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, chemical compound, award, organization, scientist, protein, location, enzyme, academic journal, chemical element, event, person, country, discipline, astronomical object, theory and O.\nSentence: 5145 Pholus in diameter , that crosses the orbit of both Saturn and Neptune .","prompt_labels":"5145(B-astronomical object) Pholus(I-astronomical object) in(O) diameter(O) ,(O) that(O) crosses(O) the(O) orbit(O) of(O) both(O) Saturn(B-astronomical object) and(O) Neptune(B-astronomical object) .(O)"}}
{"id":"127","dataset":"crossner_science","split":"train","label_list":["enzyme","organization","protein","chemical compound","event","academic journal","location","astronomical object","person","award","country","theory","scientist","university","chemical element","discipline"],"instance":{"id":"127","words":["Prominent","anatomists","and","geologists","such","as","Georges","Cuvier",",","Richard","Owen",",","Adam","Sedgwick",",","and","Charles","Lyell","attacked","it","vigorously","."],"labels":["O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, organization, protein, chemical compound, event, academic journal, location, astronomical object, person, award, country, theory, scientist, university, chemical element, discipline and O.\nSentence: Prominent anatomists and geologists such as Georges Cuvier , Richard Owen , Adam Sedgwick , and Charles Lyell attacked it vigorously .","prompt_labels":"Prominent(O) anatomists(O) and(O) geologists(O) such(O) as(O) Georges(B-scientist) Cuvier(I-scientist) ,(O) Richard(B-scientist) Owen(I-scientist) ,(O) Adam(B-scientist) Sedgwick(I-scientist) ,(O) and(O) Charles(B-scientist) Lyell(I-scientist) attacked(O) it(O) vigorously(O) .(O)"}}
{"id":"128","dataset":"crossner_science","split":"train","label_list":["event","academic journal","protein","organization","award","discipline","theory","chemical element","country","university","astronomical object","person","chemical compound","enzyme","scientist","location"],"instance":{"id":"128","words":["The","U.S.","Navy","has","three","other","facilities","on","the","bay",":","Naval","Station","San","Diego",",","Naval","Base","Point","Loma","at","Ballast","Point",",","which","is","a","Nuclear","Submarine","base",",","and","Naval","Amphibious","Base","Coronado","."],"labels":["B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","B-location","I-location","I-location","I-location","O","B-location","I-location","O","O","O","O","B-location","I-location","I-location","O","O","B-location","I-location","I-location","I-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, academic journal, protein, organization, award, discipline, theory, chemical element, country, university, astronomical object, person, chemical compound, enzyme, scientist, location and O.\nSentence: The U.S. Navy has three other facilities on the bay : Naval Station San Diego , Naval Base Point Loma at Ballast Point , which is a Nuclear Submarine base , and Naval Amphibious Base Coronado .","prompt_labels":"The(B-organization) U.S.(I-organization) Navy(I-organization) has(O) three(O) other(O) facilities(O) on(O) the(O) bay(O) :(O) Naval(B-location) Station(I-location) San(I-location) Diego(I-location) ,(O) Naval(B-location) Base(I-location) Point(I-location) Loma(I-location) at(O) Ballast(B-location) Point(I-location) ,(O) which(O) is(O) a(O) Nuclear(B-location) Submarine(I-location) base(I-location) ,(O) and(O) Naval(B-location) Amphibious(I-location) Base(I-location) Coronado(I-location) .(O)"}}
{"id":"129","dataset":"crossner_science","split":"train","label_list":["award","enzyme","astronomical object","organization","academic journal","event","chemical compound","discipline","location","country","person","university","theory","scientist","protein","chemical element"],"instance":{"id":"129","words":["Comparisons","were","frequently","drawn","between","the","second","cyclotron","at","the","Harvard","Cyclotron","Laboratory","and","the","Harwell","Synchrocyclotron",",","and","in","1974","clinicians","from","Oxford","'","s","Radcliffe","Infirmary","led","by","Dr","T","Hockaday","floated","plans","to","replicate","the","proton","therapy","work","carried","out","at","Massachusetts","General","Hospital","with","the","accelerator","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","B-university","O","O","B-location","I-location","O","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, enzyme, astronomical object, organization, academic journal, event, chemical compound, discipline, location, country, person, university, theory, scientist, protein, chemical element and O.\nSentence: Comparisons were frequently drawn between the second cyclotron at the Harvard Cyclotron Laboratory and the Harwell Synchrocyclotron , and in 1974 clinicians from Oxford ' s Radcliffe Infirmary led by Dr T Hockaday floated plans to replicate the proton therapy work carried out at Massachusetts General Hospital with the accelerator .","prompt_labels":"Comparisons(O) were(O) frequently(O) drawn(O) between(O) the(O) second(O) cyclotron(O) at(O) the(O) Harvard(B-organization) Cyclotron(I-organization) Laboratory(I-organization) and(O) the(O) Harwell(O) Synchrocyclotron(O) ,(O) and(O) in(O) 1974(O) clinicians(O) from(O) Oxford(B-university) '(O) s(O) Radcliffe(B-location) Infirmary(I-location) led(O) by(O) Dr(O) T(B-scientist) Hockaday(I-scientist) floated(O) plans(O) to(O) replicate(O) the(O) proton(O) therapy(O) work(O) carried(O) out(O) at(O) Massachusetts(B-organization) General(I-organization) Hospital(I-organization) with(O) the(O) accelerator(O) .(O)"}}
{"id":"130","dataset":"crossner_science","split":"train","label_list":["person","scientist","protein","chemical compound","chemical element","university","country","theory","discipline","award","event","enzyme","location","academic journal","organization","astronomical object"],"instance":{"id":"130","words":["Ong","Valley","(",")","is","a","mainly","ice-free","valley","5","nautical","miles","(","9","km",")","long",",","just","west","of","Kreiling","Mesa","in","the","Miller","Range","."],"labels":["B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","B-location","I-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, scientist, protein, chemical compound, chemical element, university, country, theory, discipline, award, event, enzyme, location, academic journal, organization, astronomical object and O.\nSentence: Ong Valley ( ) is a mainly ice-free valley 5 nautical miles ( 9 km ) long , just west of Kreiling Mesa in the Miller Range .","prompt_labels":"Ong(B-location) Valley(I-location) ((O) )(O) is(O) a(O) mainly(O) ice-free(O) valley(O) 5(O) nautical(O) miles(O) ((O) 9(O) km(O) )(O) long(O) ,(O) just(O) west(O) of(O) Kreiling(B-location) Mesa(I-location) in(O) the(O) Miller(B-location) Range(I-location) .(O)"}}
{"id":"131","dataset":"crossner_science","split":"train","label_list":["chemical compound","chemical element","award","discipline","academic journal","location","event","country","organization","university","theory","scientist","enzyme","person","astronomical object","protein"],"instance":{"id":"131","words":["Detergents","are","key","reagents","to","extract","protein","by","lysis","of","the","cells","and","tissues",":","They","disorganize","the","membrane","'s","lipid","bilayer","(","SDS",",","Triton","X-100",",","X-114",",","CHAPS","detergent",",","DOC",",","and","NP-40",")",",","and","solubilize","proteins","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","O","B-chemical compound","I-chemical compound","O","B-chemical compound","O","B-chemical compound","I-chemical compound","O","B-chemical compound","O","O","B-chemical compound","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, chemical element, award, discipline, academic journal, location, event, country, organization, university, theory, scientist, enzyme, person, astronomical object, protein and O.\nSentence: Detergents are key reagents to extract protein by lysis of the cells and tissues : They disorganize the membrane 's lipid bilayer ( SDS , Triton X-100 , X-114 , CHAPS detergent , DOC , and NP-40 ) , and solubilize proteins .","prompt_labels":"Detergents(O) are(O) key(O) reagents(O) to(O) extract(O) protein(O) by(O) lysis(O) of(O) the(O) cells(O) and(O) tissues(O) :(O) They(O) disorganize(O) the(O) membrane(O) 's(O) lipid(O) bilayer(O) ((O) SDS(B-chemical compound) ,(O) Triton(B-chemical compound) X-100(I-chemical compound) ,(O) X-114(B-chemical compound) ,(O) CHAPS(B-chemical compound) detergent(I-chemical compound) ,(O) DOC(B-chemical compound) ,(O) and(O) NP-40(B-chemical compound) )(O) ,(O) and(O) solubilize(O) proteins(O) .(O)"}}
{"id":"132","dataset":"crossner_science","split":"train","label_list":["protein","scientist","university","country","chemical element","chemical compound","theory","award","academic journal","organization","event","location","person","enzyme","astronomical object","discipline"],"instance":{"id":"132","words":["Such","experiments","were","independently","performed","by","Walther","Bothe","and","Hans","Geiger",","],"labels":["O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, scientist, university, country, chemical element, chemical compound, theory, award, academic journal, organization, event, location, person, enzyme, astronomical object, discipline and O.\nSentence: Such experiments were independently performed by Walther Bothe and Hans Geiger ,","prompt_labels":"Such(O) experiments(O) were(O) independently(O) performed(O) by(O) Walther(B-scientist) Bothe(I-scientist) and(O) Hans(B-scientist) Geiger(I-scientist) ,(O)"}}
{"id":"133","dataset":"crossner_science","split":"train","label_list":["country","location","event","university","award","chemical compound","theory","academic journal","enzyme","scientist","chemical element","protein","person","astronomical object","organization","discipline"],"instance":{"id":"133","words":["The","WHO",",","the","American","Academy","of","Pediatrics",",","the","Advisory","Committee","on","Immunization","Practices","of","the","Centers","for","Disease","Control","and","Prevention",",","the","American","Academy","of","Family","Physicians",",","the","British","Medical","Association",",","and","the","Royal","Pharmaceutical","Society","of","Great","Britain","recommend","routine","vaccination","of","children","against","mumps","."],"labels":["O","B-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, event, university, award, chemical compound, theory, academic journal, enzyme, scientist, chemical element, protein, person, astronomical object, organization, discipline and O.\nSentence: The WHO , the American Academy of Pediatrics , the Advisory Committee on Immunization Practices of the Centers for Disease Control and Prevention , the American Academy of Family Physicians , the British Medical Association , and the Royal Pharmaceutical Society of Great Britain recommend routine vaccination of children against mumps .","prompt_labels":"The(O) WHO(B-organization) ,(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Pediatrics(I-organization) ,(O) the(O) Advisory(B-organization) Committee(I-organization) on(I-organization) Immunization(I-organization) Practices(I-organization) of(I-organization) the(I-organization) Centers(I-organization) for(I-organization) Disease(I-organization) Control(I-organization) and(I-organization) Prevention(I-organization) ,(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Family(I-organization) Physicians(I-organization) ,(O) the(O) British(B-organization) Medical(I-organization) Association(I-organization) ,(O) and(O) the(O) Royal(B-organization) Pharmaceutical(I-organization) Society(I-organization) of(I-organization) Great(I-organization) Britain(I-organization) recommend(O) routine(O) vaccination(O) of(O) children(O) against(O) mumps(O) .(O)"}}
{"id":"134","dataset":"crossner_science","split":"train","label_list":["person","theory","location","chemical element","chemical compound","discipline","enzyme","scientist","country","protein","event","award","astronomical object","university","academic journal","organization"],"instance":{"id":"134","words":["According","to","writer","Daniel","Loxton","and","paleontologist","Donald","Prothero",",","Cryptozoologists","have","often","promoted","'","Professor","Roy","Mackal",",","PhD","."],"labels":["O","O","O","B-person","I-person","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, theory, location, chemical element, chemical compound, discipline, enzyme, scientist, country, protein, event, award, astronomical object, university, academic journal, organization and O.\nSentence: According to writer Daniel Loxton and paleontologist Donald Prothero , Cryptozoologists have often promoted ' Professor Roy Mackal , PhD .","prompt_labels":"According(O) to(O) writer(O) Daniel(B-person) Loxton(I-person) and(O) paleontologist(O) Donald(B-scientist) Prothero(I-scientist) ,(O) Cryptozoologists(O) have(O) often(O) promoted(O) '(O) Professor(O) Roy(B-scientist) Mackal(I-scientist) ,(O) PhD(O) .(O)"}}
{"id":"135","dataset":"crossner_science","split":"train","label_list":["country","organization","location","theory","discipline","scientist","person","astronomical object","event","chemical element","protein","academic journal","chemical compound","award","enzyme","university"],"instance":{"id":"135","words":["A","great","conjunction","is","a","conjunction","of","the","planets","Jupiter","and","Saturn","."],"labels":["O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, location, theory, discipline, scientist, person, astronomical object, event, chemical element, protein, academic journal, chemical compound, award, enzyme, university and O.\nSentence: A great conjunction is a conjunction of the planets Jupiter and Saturn .","prompt_labels":"A(O) great(O) conjunction(O) is(O) a(O) conjunction(O) of(O) the(O) planets(O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) .(O)"}}
{"id":"136","dataset":"crossner_science","split":"train","label_list":["chemical compound","university","scientist","chemical element","protein","country","astronomical object","enzyme","discipline","theory","award","location","event","academic journal","person","organization"],"instance":{"id":"136","words":["Four","COSMIC","mutational","signatures","have","been","associated","with","DNA","mismatch","repair","deficiency","and","found","in","tumors","with","microsatellite","instability",":","Signature","6",",","15",",","20","and","26","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, university, scientist, chemical element, protein, country, astronomical object, enzyme, discipline, theory, award, location, event, academic journal, person, organization and O.\nSentence: Four COSMIC mutational signatures have been associated with DNA mismatch repair deficiency and found in tumors with microsatellite instability : Signature 6 , 15 , 20 and 26 .","prompt_labels":"Four(O) COSMIC(O) mutational(O) signatures(O) have(O) been(O) associated(O) with(O) DNA(O) mismatch(O) repair(O) deficiency(O) and(O) found(O) in(O) tumors(O) with(O) microsatellite(O) instability(O) :(O) Signature(O) 6(O) ,(O) 15(O) ,(O) 20(O) and(O) 26(O) .(O)"}}
{"id":"137","dataset":"crossner_science","split":"train","label_list":["country","astronomical object","theory","protein","discipline","event","scientist","chemical compound","enzyme","location","award","chemical element","academic journal","university","organization","person"],"instance":{"id":"137","words":["The","telescope","is","owned","by","Liverpool","John","Moores","University",",","and","operated","by","the","Astrophysics","Research","Institute","with","operational","funding","partly","from","Science","and","Technology","Facilities","Council","."],"labels":["O","O","O","O","O","B-university","I-university","I-university","I-university","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, astronomical object, theory, protein, discipline, event, scientist, chemical compound, enzyme, location, award, chemical element, academic journal, university, organization, person and O.\nSentence: The telescope is owned by Liverpool John Moores University , and operated by the Astrophysics Research Institute with operational funding partly from Science and Technology Facilities Council .","prompt_labels":"The(O) telescope(O) is(O) owned(O) by(O) Liverpool(B-university) John(I-university) Moores(I-university) University(I-university) ,(O) and(O) operated(O) by(O) the(O) Astrophysics(B-organization) Research(I-organization) Institute(I-organization) with(O) operational(O) funding(O) partly(O) from(O) Science(B-organization) and(I-organization) Technology(I-organization) Facilities(I-organization) Council(I-organization) .(O)"}}
{"id":"138","dataset":"crossner_science","split":"train","label_list":["chemical compound","theory","person","enzyme","organization","chemical element","discipline","country","location","award","university","scientist","event","protein","academic journal","astronomical object"],"instance":{"id":"138","words":["6,7-Dibromo-1,4-epoxy-1,4-dihydronaphthalene","(","6,7-Dibromonaphthalene-1,4-endoxide",",","accessible","after","debromination","from","1,2,4,5-Tetrabromobenzene","using","an","equivalent","of","N-Butyllithium","and","Diels-Alder","reaction","in","furan","in","70","%","yield","{","{","cite","journal","|","author","=","H.","Hart",",","A.","Bashir-Hashemi",",","J.","Luo",",","M.","A.","Meador","|","journal","=","Tetrahedron","|","title","=","Iptycenes",":","Extended","Triptycenes","|","volume","=","42","|","year","=","1986","|","page","=","1641-1654","|","doi","=","10.1016","/","S0040-4020","(","1",")","87581-5"],"labels":["B-chemical compound","O","B-chemical compound","O","O","O","O","O","B-chemical compound","O","O","O","O","B-chemical compound","O","O","O","O","B-chemical compound","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","O","O","B-academic journal","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, theory, person, enzyme, organization, chemical element, discipline, country, location, award, university, scientist, event, protein, academic journal, astronomical object and O.\nSentence: 6,7-Dibromo-1,4-epoxy-1,4-dihydronaphthalene ( 6,7-Dibromonaphthalene-1,4-endoxide , accessible after debromination from 1,2,4,5-Tetrabromobenzene using an equivalent of N-Butyllithium and Diels-Alder reaction in furan in 70 % yield { { cite journal | author = H. Hart , A. Bashir-Hashemi , J. Luo , M. A. Meador | journal = Tetrahedron | title = Iptycenes : Extended Triptycenes | volume = 42 | year = 1986 | page = 1641-1654 | doi = 10.1016 / S0040-4020 ( 1 ) 87581-5","prompt_labels":"6,7-Dibromo-1,4-epoxy-1,4-dihydronaphthalene(B-chemical compound) ((O) 6,7-Dibromonaphthalene-1,4-endoxide(B-chemical compound) ,(O) accessible(O) after(O) debromination(O) from(O) 1,2,4,5-Tetrabromobenzene(B-chemical compound) using(O) an(O) equivalent(O) of(O) N-Butyllithium(B-chemical compound) and(O) Diels-Alder(O) reaction(O) in(O) furan(B-chemical compound) in(O) 70(O) %(O) yield(O) {(O) {(O) cite(O) journal(O) |(O) author(O) =(O) H.(B-scientist) Hart(I-scientist) ,(O) A.(B-scientist) Bashir-Hashemi(I-scientist) ,(O) J.(B-scientist) Luo(I-scientist) ,(O) M.(B-scientist) A.(I-scientist) Meador(I-scientist) |(O) journal(O) =(O) Tetrahedron(B-academic journal) |(O) title(O) =(O) Iptycenes(O) :(O) Extended(O) Triptycenes(O) |(O) volume(O) =(O) 42(O) |(O) year(O) =(O) 1986(O) |(O) page(O) =(O) 1641-1654(O) |(O) doi(O) =(O) 10.1016(O) /(O) S0040-4020(O) ((O) 1(O) )(O) 87581-5(O)"}}
{"id":"139","dataset":"crossner_science","split":"train","label_list":["theory","person","academic journal","university","chemical compound","scientist","chemical element","location","organization","protein","country","discipline","event","astronomical object","enzyme","award"],"instance":{"id":"139","words":["Other","members","of","the","Nuclear","Physics","Working","Group","in","both","1956","and","1957","were",":","Werner","Heisenberg","(","chairman",")",",","Hans","Kopfermann","(","vice-chairman",")",",","Walther","Bothe",",","Wolfgang","Gentner",",","Otto","Haxel",",","Willibald","Jentschke",",","Heinz","Maier-Leibnitz",",","Josef","Mattauch",",","Wolfgang","Riezler",",","Wilhelm","Walcher",",","and","Carl","Friedrich","von","Weizsäcker","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","B-scientist","I-scientist","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","I-scientist","I-scientist","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, person, academic journal, university, chemical compound, scientist, chemical element, location, organization, protein, country, discipline, event, astronomical object, enzyme, award and O.\nSentence: Other members of the Nuclear Physics Working Group in both 1956 and 1957 were : Werner Heisenberg ( chairman ) , Hans Kopfermann ( vice-chairman ) , Walther Bothe , Wolfgang Gentner , Otto Haxel , Willibald Jentschke , Heinz Maier-Leibnitz , Josef Mattauch , Wolfgang Riezler , Wilhelm Walcher , and Carl Friedrich von Weizsäcker .","prompt_labels":"Other(O) members(O) of(O) the(O) Nuclear(O) Physics(O) Working(O) Group(O) in(O) both(O) 1956(O) and(O) 1957(O) were(O) :(O) Werner(B-scientist) Heisenberg(I-scientist) ((O) chairman(O) )(O) ,(O) Hans(B-scientist) Kopfermann(I-scientist) ((O) vice-chairman(O) )(O) ,(O) Walther(B-scientist) Bothe(I-scientist) ,(O) Wolfgang(B-scientist) Gentner(I-scientist) ,(O) Otto(B-scientist) Haxel(I-scientist) ,(O) Willibald(B-scientist) Jentschke(I-scientist) ,(O) Heinz(B-scientist) Maier-Leibnitz(I-scientist) ,(O) Josef(B-scientist) Mattauch(I-scientist) ,(O) Wolfgang(B-scientist) Riezler(I-scientist) ,(O) Wilhelm(B-scientist) Walcher(I-scientist) ,(O) and(O) Carl(B-scientist) Friedrich(I-scientist) von(I-scientist) Weizsäcker(I-scientist) .(O)"}}
{"id":"140","dataset":"crossner_science","split":"train","label_list":["astronomical object","award","academic journal","enzyme","country","discipline","protein","scientist","organization","event","chemical compound","chemical element","university","person","theory","location"],"instance":{"id":"140","words":["Using","single-molecule","force","spectroscopy",",","his","lab","showed","how","intracellular","molecules","such","as","Alpha","catenin","and","b-catenin","modulate","cell-cell","adhesion","mediated","by","E-cadherin","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","I-protein","O","B-protein","O","O","O","O","O","B-protein","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, award, academic journal, enzyme, country, discipline, protein, scientist, organization, event, chemical compound, chemical element, university, person, theory, location and O.\nSentence: Using single-molecule force spectroscopy , his lab showed how intracellular molecules such as Alpha catenin and b-catenin modulate cell-cell adhesion mediated by E-cadherin .","prompt_labels":"Using(O) single-molecule(O) force(O) spectroscopy(O) ,(O) his(O) lab(O) showed(O) how(O) intracellular(O) molecules(O) such(O) as(O) Alpha(B-protein) catenin(I-protein) and(O) b-catenin(B-protein) modulate(O) cell-cell(O) adhesion(O) mediated(O) by(O) E-cadherin(B-protein) .(O)"}}
{"id":"141","dataset":"crossner_science","split":"train","label_list":["enzyme","discipline","academic journal","university","astronomical object","organization","chemical element","theory","chemical compound","person","scientist","protein","event","country","location","award"],"instance":{"id":"141","words":["The","show","has","received","recognition","as","one","of","Britain","'s","finest","television","programmes",",","winning","the","2006","British","Academy","Television","Award","for","Best","Drama","Series","and","five","consecutive","(","2005-2010",")","awards","at","the","National","Television","Awards","during","Russell","T","Davies","'","tenure","as","executive","producer","."],"labels":["O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","O","B-person","I-person","I-person","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, discipline, academic journal, university, astronomical object, organization, chemical element, theory, chemical compound, person, scientist, protein, event, country, location, award and O.\nSentence: The show has received recognition as one of Britain 's finest television programmes , winning the 2006 British Academy Television Award for Best Drama Series and five consecutive ( 2005-2010 ) awards at the National Television Awards during Russell T Davies ' tenure as executive producer .","prompt_labels":"The(O) show(O) has(O) received(O) recognition(O) as(O) one(O) of(O) Britain(B-country) 's(O) finest(O) television(O) programmes(O) ,(O) winning(O) the(O) 2006(O) British(B-award) Academy(I-award) Television(I-award) Award(I-award) for(I-award) Best(I-award) Drama(I-award) Series(I-award) and(O) five(O) consecutive(O) ((O) 2005-2010(O) )(O) awards(O) at(O) the(O) National(B-award) Television(I-award) Awards(I-award) during(O) Russell(B-person) T(I-person) Davies(I-person) '(O) tenure(O) as(O) executive(O) producer(O) .(O)"}}
{"id":"142","dataset":"crossner_science","split":"train","label_list":["person","event","country","scientist","astronomical object","academic journal","discipline","enzyme","location","university","organization","theory","protein","chemical element","chemical compound","award"],"instance":{"id":"142","words":["He","was","the","first","to","identify","the","biological","role","of","the","non-structural","NS1","Influenza","Protein","during","infection",",","the","first","to","describe","and","provide","a","molecular","analysis","of","a","viral-encoded","Interferon","antagonist","among","negative","strand","RNA","viruses",",","and","the","first","to","demonstrate","that","the","M1","protein","of","the","influenza","virus","determines","its","morphology","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-protein","I-protein","I-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","I-protein","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, country, scientist, astronomical object, academic journal, discipline, enzyme, location, university, organization, theory, protein, chemical element, chemical compound, award and O.\nSentence: He was the first to identify the biological role of the non-structural NS1 Influenza Protein during infection , the first to describe and provide a molecular analysis of a viral-encoded Interferon antagonist among negative strand RNA viruses , and the first to demonstrate that the M1 protein of the influenza virus determines its morphology .","prompt_labels":"He(O) was(O) the(O) first(O) to(O) identify(O) the(O) biological(O) role(O) of(O) the(O) non-structural(O) NS1(B-protein) Influenza(I-protein) Protein(I-protein) during(O) infection(O) ,(O) the(O) first(O) to(O) describe(O) and(O) provide(O) a(O) molecular(O) analysis(O) of(O) a(O) viral-encoded(O) Interferon(O) antagonist(O) among(O) negative(O) strand(O) RNA(O) viruses(O) ,(O) and(O) the(O) first(O) to(O) demonstrate(O) that(O) the(O) M1(B-protein) protein(I-protein) of(O) the(O) influenza(O) virus(O) determines(O) its(O) morphology(O) .(O)"}}
{"id":"143","dataset":"crossner_science","split":"train","label_list":["university","theory","scientist","chemical element","country","enzyme","discipline","location","event","protein","organization","person","award","academic journal","astronomical object","chemical compound"],"instance":{"id":"143","words":["Alberts","has","also","served","as","an","editor","for","numerous","peer-reviewed","journals","in","a","variety","of","fields",",","including","Behavioral","Ecology","(","journal",")",",","the","American","Journal","of","Primatology",",","and","PeerJ","."],"labels":["B-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","B-academic journal","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, theory, scientist, chemical element, country, enzyme, discipline, location, event, protein, organization, person, award, academic journal, astronomical object, chemical compound and O.\nSentence: Alberts has also served as an editor for numerous peer-reviewed journals in a variety of fields , including Behavioral Ecology ( journal ) , the American Journal of Primatology , and PeerJ .","prompt_labels":"Alberts(B-scientist) has(O) also(O) served(O) as(O) an(O) editor(O) for(O) numerous(O) peer-reviewed(O) journals(O) in(O) a(O) variety(O) of(O) fields(O) ,(O) including(O) Behavioral(B-academic journal) Ecology(I-academic journal) ((O) journal(O) )(O) ,(O) the(O) American(B-academic journal) Journal(I-academic journal) of(I-academic journal) Primatology(I-academic journal) ,(O) and(O) PeerJ(B-academic journal) .(O)"}}
{"id":"144","dataset":"crossner_science","split":"train","label_list":["chemical compound","location","astronomical object","enzyme","country","theory","award","organization","academic journal","person","chemical element","discipline","event","scientist","protein","university"],"instance":{"id":"144","words":["He","is","well","known","for","his","structural","enzymology","work","on","Rieske","protein","non-heme","iron","oxygenases","and","Alcohol","dehydrogenase"],"labels":["O","O","O","O","O","O","O","B-discipline","O","O","B-enzyme","I-enzyme","I-enzyme","I-enzyme","I-enzyme","O","B-enzyme","I-enzyme"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, location, astronomical object, enzyme, country, theory, award, organization, academic journal, person, chemical element, discipline, event, scientist, protein, university and O.\nSentence: He is well known for his structural enzymology work on Rieske protein non-heme iron oxygenases and Alcohol dehydrogenase","prompt_labels":"He(O) is(O) well(O) known(O) for(O) his(O) structural(O) enzymology(B-discipline) work(O) on(O) Rieske(B-enzyme) protein(I-enzyme) non-heme(I-enzyme) iron(I-enzyme) oxygenases(I-enzyme) and(O) Alcohol(B-enzyme) dehydrogenase(I-enzyme)"}}
{"id":"145","dataset":"crossner_science","split":"train","label_list":["award","event","country","person","academic journal","organization","astronomical object","university","chemical compound","location","protein","discipline","enzyme","chemical element","scientist","theory"],"instance":{"id":"145","words":["This","is","due","to","the","gravitation","influence","of","Saturn","and","Jupiter","."],"labels":["O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, country, person, academic journal, organization, astronomical object, university, chemical compound, location, protein, discipline, enzyme, chemical element, scientist, theory and O.\nSentence: This is due to the gravitation influence of Saturn and Jupiter .","prompt_labels":"This(O) is(O) due(O) to(O) the(O) gravitation(O) influence(O) of(O) Saturn(B-astronomical object) and(O) Jupiter(B-astronomical object) .(O)"}}
{"id":"146","dataset":"crossner_science","split":"train","label_list":["award","person","discipline","location","astronomical object","theory","enzyme","event","university","chemical element","organization","protein","chemical compound","academic journal","scientist","country"],"instance":{"id":"146","words":["Boom","Town",",","a","1940","film","about","wildcatting","in","the","early","Oklahoma","oil","industry",",","starred","Clark","Gable","and","Spencer","Tracy","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","B-person","I-person","O","B-person","I-person","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, discipline, location, astronomical object, theory, enzyme, event, university, chemical element, organization, protein, chemical compound, academic journal, scientist, country and O.\nSentence: Boom Town , a 1940 film about wildcatting in the early Oklahoma oil industry , starred Clark Gable and Spencer Tracy .","prompt_labels":"Boom(O) Town(O) ,(O) a(O) 1940(O) film(O) about(O) wildcatting(O) in(O) the(O) early(O) Oklahoma(B-location) oil(O) industry(O) ,(O) starred(O) Clark(B-person) Gable(I-person) and(O) Spencer(B-person) Tracy(I-person) .(O)"}}
{"id":"147","dataset":"crossner_science","split":"train","label_list":["theory","university","chemical compound","location","award","chemical element","country","academic journal","scientist","protein","organization","discipline","astronomical object","person","enzyme","event"],"instance":{"id":"147","words":["6239","Minos","(","1989","QF",")","is","an","Apollo","asteroid","classified","as","a","PHA","discovered","on","31","August","1989","by","Carolyn","S.","Shoemaker","and","Eugene","Merle","Shoemaker","at","Palomar","."],"labels":["B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, university, chemical compound, location, award, chemical element, country, academic journal, scientist, protein, organization, discipline, astronomical object, person, enzyme, event and O.\nSentence: 6239 Minos ( 1989 QF ) is an Apollo asteroid classified as a PHA discovered on 31 August 1989 by Carolyn S. Shoemaker and Eugene Merle Shoemaker at Palomar .","prompt_labels":"6239(B-astronomical object) Minos(I-astronomical object) ((O) 1989(B-astronomical object) QF(I-astronomical object) )(O) is(O) an(O) Apollo(B-astronomical object) asteroid(I-astronomical object) classified(O) as(O) a(O) PHA(O) discovered(O) on(O) 31(O) August(O) 1989(O) by(O) Carolyn(B-scientist) S.(I-scientist) Shoemaker(I-scientist) and(O) Eugene(B-scientist) Merle(I-scientist) Shoemaker(I-scientist) at(O) Palomar(B-location) .(O)"}}
{"id":"148","dataset":"crossner_science","split":"train","label_list":["chemical compound","astronomical object","chemical element","award","country","discipline","protein","event","location","enzyme","organization","academic journal","scientist","person","theory","university"],"instance":{"id":"148","words":["Human","leukocyte","antigen","constitutes","a","group","of","cell","surface","antigens","also","known","as","the","Major","histocompatibility","complex","of","humans","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, astronomical object, chemical element, award, country, discipline, protein, event, location, enzyme, organization, academic journal, scientist, person, theory, university and O.\nSentence: Human leukocyte antigen constitutes a group of cell surface antigens also known as the Major histocompatibility complex of humans .","prompt_labels":"Human(O) leukocyte(O) antigen(O) constitutes(O) a(O) group(O) of(O) cell(O) surface(O) antigens(O) also(O) known(O) as(O) the(O) Major(O) histocompatibility(O) complex(O) of(O) humans(O) .(O)"}}
{"id":"149","dataset":"crossner_science","split":"train","label_list":["scientist","discipline","protein","chemical compound","organization","person","theory","country","university","academic journal","chemical element","event","enzyme","award","astronomical object","location"],"instance":{"id":"149","words":["For","example",",","when","there","is","a","mistake","in","base","pairing",",","DNA","mismatch","repair","has","a","bias","favoring","GC","pairs","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, discipline, protein, chemical compound, organization, person, theory, country, university, academic journal, chemical element, event, enzyme, award, astronomical object, location and O.\nSentence: For example , when there is a mistake in base pairing , DNA mismatch repair has a bias favoring GC pairs .","prompt_labels":"For(O) example(O) ,(O) when(O) there(O) is(O) a(O) mistake(O) in(O) base(O) pairing(O) ,(O) DNA(O) mismatch(O) repair(O) has(O) a(O) bias(O) favoring(O) GC(O) pairs(O) .(O)"}}
{"id":"150","dataset":"crossner_science","split":"train","label_list":["award","chemical element","university","academic journal","protein","event","organization","enzyme","astronomical object","chemical compound","scientist","theory","person","discipline","location","country"],"instance":{"id":"150","words":["He","also","is","one","of","the","two","Shaw","Prize","Founding","Members","and","is","a","Distinguished","Professor-at-Large","at","the","Chinese","University","of","Hong","Kong","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","I-university","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, chemical element, university, academic journal, protein, event, organization, enzyme, astronomical object, chemical compound, scientist, theory, person, discipline, location, country and O.\nSentence: He also is one of the two Shaw Prize Founding Members and is a Distinguished Professor-at-Large at the Chinese University of Hong Kong .","prompt_labels":"He(O) also(O) is(O) one(O) of(O) the(O) two(O) Shaw(B-award) Prize(I-award) Founding(O) Members(O) and(O) is(O) a(O) Distinguished(O) Professor-at-Large(O) at(O) the(O) Chinese(B-university) University(I-university) of(I-university) Hong(I-university) Kong(I-university) .(O)"}}
{"id":"151","dataset":"crossner_science","split":"train","label_list":["enzyme","scientist","university","astronomical object","theory","protein","event","academic journal","chemical compound","country","chemical element","award","discipline","organization","person","location"],"instance":{"id":"151","words":["DNA","methyltransferase","nowiki","/","s",")","are","involved","in","regulation","of","the","electrophysiological","landscape","of","the","brain","through","methylation","of","CpG","nowiki","/","s","."],"labels":["B-enzyme","I-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, scientist, university, astronomical object, theory, protein, event, academic journal, chemical compound, country, chemical element, award, discipline, organization, person, location and O.\nSentence: DNA methyltransferase nowiki / s ) are involved in regulation of the electrophysiological landscape of the brain through methylation of CpG nowiki / s .","prompt_labels":"DNA(B-enzyme) methyltransferase(I-enzyme) nowiki(O) /(O) s(O) )(O) are(O) involved(O) in(O) regulation(O) of(O) the(O) electrophysiological(O) landscape(O) of(O) the(O) brain(O) through(O) methylation(O) of(O) CpG(O) nowiki(O) /(O) s(O) .(O)"}}
{"id":"152","dataset":"crossner_science","split":"train","label_list":["academic journal","location","country","person","enzyme","organization","scientist","event","theory","award","chemical compound","university","astronomical object","discipline","chemical element","protein"],"instance":{"id":"152","words":["The","existence","of","the","antiproton","was","experimentally","confirmed","in","1955","by","University","of","California",",","Berkeley","physicist","s","Emilio","Segrè","and","Owen","Chamberlain",",","for","which","they","were","awarded","the","1959","Nobel","Prize","in","Physics","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","I-university","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, location, country, person, enzyme, organization, scientist, event, theory, award, chemical compound, university, astronomical object, discipline, chemical element, protein and O.\nSentence: The existence of the antiproton was experimentally confirmed in 1955 by University of California , Berkeley physicist s Emilio Segrè and Owen Chamberlain , for which they were awarded the 1959 Nobel Prize in Physics .","prompt_labels":"The(O) existence(O) of(O) the(O) antiproton(O) was(O) experimentally(O) confirmed(O) in(O) 1955(O) by(O) University(B-university) of(I-university) California(I-university) ,(I-university) Berkeley(I-university) physicist(O) s(O) Emilio(B-scientist) Segrè(I-scientist) and(O) Owen(B-scientist) Chamberlain(I-scientist) ,(O) for(O) which(O) they(O) were(O) awarded(O) the(O) 1959(O) Nobel(B-award) Prize(I-award) in(I-award) Physics(I-award) .(O)"}}
{"id":"153","dataset":"crossner_science","split":"train","label_list":["award","chemical compound","scientist","enzyme","location","university","astronomical object","event","theory","person","protein","chemical element","academic journal","discipline","country","organization"],"instance":{"id":"153","words":["Bio-active","constituents","of","human","milk","that","have","been","cataloged","to","possess","immune-modulating","capabilities","include","immunoglobulins",",","Lactoferrin",",","Lysozyme",",","oligosaccharide","s",",","lipid","s",",","cytokine","s",",","hormone","s",",","and","growth","factor","s","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","O","B-protein","O","B-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, chemical compound, scientist, enzyme, location, university, astronomical object, event, theory, person, protein, chemical element, academic journal, discipline, country, organization and O.\nSentence: Bio-active constituents of human milk that have been cataloged to possess immune-modulating capabilities include immunoglobulins , Lactoferrin , Lysozyme , oligosaccharide s , lipid s , cytokine s , hormone s , and growth factor s .","prompt_labels":"Bio-active(O) constituents(O) of(O) human(O) milk(O) that(O) have(O) been(O) cataloged(O) to(O) possess(O) immune-modulating(O) capabilities(O) include(O) immunoglobulins(B-protein) ,(O) Lactoferrin(B-protein) ,(O) Lysozyme(B-enzyme) ,(O) oligosaccharide(O) s(O) ,(O) lipid(O) s(O) ,(O) cytokine(O) s(O) ,(O) hormone(O) s(O) ,(O) and(O) growth(O) factor(O) s(O) .(O)"}}
{"id":"154","dataset":"crossner_science","split":"train","label_list":["chemical element","protein","person","chemical compound","enzyme","country","scientist","organization","university","event","academic journal","location","discipline","theory","award","astronomical object"],"instance":{"id":"154","words":["The","orbit","of","70","Panopaea","places","it","in","a","mean","motion","resonance","with","the","planets","Jupiter","and","Saturn","."],"labels":["O","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, protein, person, chemical compound, enzyme, country, scientist, organization, university, event, academic journal, location, discipline, theory, award, astronomical object and O.\nSentence: The orbit of 70 Panopaea places it in a mean motion resonance with the planets Jupiter and Saturn .","prompt_labels":"The(O) orbit(O) of(O) 70(B-astronomical object) Panopaea(I-astronomical object) places(O) it(O) in(O) a(O) mean(O) motion(O) resonance(O) with(O) the(O) planets(O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) .(O)"}}
{"id":"155","dataset":"crossner_science","split":"train","label_list":["enzyme","astronomical object","chemical element","university","location","theory","scientist","person","chemical compound","award","event","academic journal","protein","organization","discipline","country"],"instance":{"id":"155","words":["Several","scholars","including","Steven","Pinker",",","Chomsky",",","Gerald","Edelman",",","and","Alexander","Luria","have","indicated","the","importance","of","the","emergence","of","human","language","as","an","important","regulative","mechanism","of","learning","and","memory","in","the","context","of","the","development","of","higher-order","consciousness","."],"labels":["O","O","O","B-scientist","I-scientist","O","B-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-theory","I-theory","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, astronomical object, chemical element, university, location, theory, scientist, person, chemical compound, award, event, academic journal, protein, organization, discipline, country and O.\nSentence: Several scholars including Steven Pinker , Chomsky , Gerald Edelman , and Alexander Luria have indicated the importance of the emergence of human language as an important regulative mechanism of learning and memory in the context of the development of higher-order consciousness .","prompt_labels":"Several(O) scholars(O) including(O) Steven(B-scientist) Pinker(I-scientist) ,(O) Chomsky(B-scientist) ,(O) Gerald(B-scientist) Edelman(I-scientist) ,(O) and(O) Alexander(B-scientist) Luria(I-scientist) have(O) indicated(O) the(O) importance(O) of(O) the(O) emergence(O) of(O) human(O) language(O) as(O) an(O) important(O) regulative(O) mechanism(O) of(O) learning(O) and(O) memory(O) in(O) the(O) context(O) of(O) the(O) development(O) of(O) higher-order(B-theory) consciousness(I-theory) .(O)"}}
{"id":"156","dataset":"crossner_science","split":"train","label_list":["theory","chemical element","award","organization","country","enzyme","astronomical object","location","chemical compound","protein","scientist","university","event","academic journal","discipline","person"],"instance":{"id":"156","words":["During","that","year",",","he","moved","to","University","of","Würzburg","and","two","years","later",",","in","1869","to","University","of","Bonn","."],"labels":["O","O","O","O","O","O","O","B-university","I-university","I-university","O","O","O","O","O","O","O","O","B-university","I-university","I-university","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, chemical element, award, organization, country, enzyme, astronomical object, location, chemical compound, protein, scientist, university, event, academic journal, discipline, person and O.\nSentence: During that year , he moved to University of Würzburg and two years later , in 1869 to University of Bonn .","prompt_labels":"During(O) that(O) year(O) ,(O) he(O) moved(O) to(O) University(B-university) of(I-university) Würzburg(I-university) and(O) two(O) years(O) later(O) ,(O) in(O) 1869(O) to(O) University(B-university) of(I-university) Bonn(I-university) .(O)"}}
{"id":"157","dataset":"crossner_science","split":"train","label_list":["discipline","theory","chemical compound","location","event","protein","person","scientist","university","organization","astronomical object","chemical element","country","enzyme","academic journal","award"],"instance":{"id":"157","words":["These","included","the","Elliott","Cresson","Medal","from","the","Franklin","Institute","in","1942",",","He","was","a","Fellow","of","the","American","Physical","Society",",","serving","as","its","president","in","1950",",","and","a","member","of","the","National","Academy","of","Sciences",",","the","American","Philosophical","Society",",","and","the","American","Academy","of","Arts","and","Sciences","."],"labels":["O","O","O","B-award","I-award","I-award","O","O","B-organization","I-organization","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, theory, chemical compound, location, event, protein, person, scientist, university, organization, astronomical object, chemical element, country, enzyme, academic journal, award and O.\nSentence: These included the Elliott Cresson Medal from the Franklin Institute in 1942 , He was a Fellow of the American Physical Society , serving as its president in 1950 , and a member of the National Academy of Sciences , the American Philosophical Society , and the American Academy of Arts and Sciences .","prompt_labels":"These(O) included(O) the(O) Elliott(B-award) Cresson(I-award) Medal(I-award) from(O) the(O) Franklin(B-organization) Institute(I-organization) in(O) 1942(O) ,(O) He(O) was(O) a(O) Fellow(B-award) of(I-award) the(I-award) American(I-award) Physical(I-award) Society(I-award) ,(O) serving(O) as(O) its(O) president(O) in(O) 1950(O) ,(O) and(O) a(O) member(O) of(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) the(O) American(B-organization) Philosophical(I-organization) Society(I-organization) ,(O) and(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) .(O)"}}
{"id":"158","dataset":"crossner_science","split":"train","label_list":["location","university","organization","award","academic journal","event","scientist","theory","chemical element","astronomical object","discipline","enzyme","person","chemical compound","country","protein"],"instance":{"id":"158","words":["Antonio","de","Ulloa","y","de","la","Torre-Giralt",",","Fellow","of","the","Royal","Society",",","Royal","Swedish","Academy","of","Sciences",",","KOS","(","12","January","1716","-","3","July","1795",")","was","a","Spanish","general","of","the","navy",",","explorer",",","scientist",",","author",",","astronomer",",","colonial","administrator","and","the","first","Spanish","governor","of","Louisiana","."],"labels":["B-scientist","I-scientist","I-scientist","I-scientist","I-scientist","I-scientist","I-scientist","O","B-award","I-award","I-award","I-award","I-award","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, university, organization, award, academic journal, event, scientist, theory, chemical element, astronomical object, discipline, enzyme, person, chemical compound, country, protein and O.\nSentence: Antonio de Ulloa y de la Torre-Giralt , Fellow of the Royal Society , Royal Swedish Academy of Sciences , KOS ( 12 January 1716 - 3 July 1795 ) was a Spanish general of the navy , explorer , scientist , author , astronomer , colonial administrator and the first Spanish governor of Louisiana .","prompt_labels":"Antonio(B-scientist) de(I-scientist) Ulloa(I-scientist) y(I-scientist) de(I-scientist) la(I-scientist) Torre-Giralt(I-scientist) ,(O) Fellow(B-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) ,(O) Royal(B-organization) Swedish(I-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) KOS(B-organization) ((O) 12(O) January(O) 1716(O) -(O) 3(O) July(O) 1795(O) )(O) was(O) a(O) Spanish(O) general(O) of(O) the(O) navy(O) ,(O) explorer(O) ,(O) scientist(O) ,(O) author(O) ,(O) astronomer(O) ,(O) colonial(O) administrator(O) and(O) the(O) first(O) Spanish(O) governor(O) of(O) Louisiana(B-location) .(O)"}}
{"id":"159","dataset":"crossner_science","split":"train","label_list":["chemical compound","academic journal","university","discipline","award","person","theory","event","enzyme","protein","scientist","chemical element","astronomical object","organization","location","country"],"instance":{"id":"159","words":["He","studied","at","CMS","College","Kottayam",",","and","graduated","with","honors","from","the","Madras","Christian","College","in","1951","."],"labels":["O","O","O","B-university","I-university","I-university","O","O","O","O","O","O","O","B-university","I-university","I-university","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, academic journal, university, discipline, award, person, theory, event, enzyme, protein, scientist, chemical element, astronomical object, organization, location, country and O.\nSentence: He studied at CMS College Kottayam , and graduated with honors from the Madras Christian College in 1951 .","prompt_labels":"He(O) studied(O) at(O) CMS(B-university) College(I-university) Kottayam(I-university) ,(O) and(O) graduated(O) with(O) honors(O) from(O) the(O) Madras(B-university) Christian(I-university) College(I-university) in(O) 1951(O) .(O)"}}
{"id":"160","dataset":"crossner_science","split":"train","label_list":["discipline","academic journal","scientist","organization","protein","astronomical object","person","chemical compound","location","award","theory","country","event","chemical element","university","enzyme"],"instance":{"id":"160","words":["The","Cricket","Club","of","India","(","CCI",")","and","Mumbai","Cricket","Association","(","MCA",")","are","located","at","Mumbai","'s","two","cricket","stadiums",",","namely","Brabourne","Stadium","and","Wankhede","Stadium","respectively","."],"labels":["O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","O","B-location","O","O","O","O","O","O","B-location","I-location","O","B-location","I-location","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, academic journal, scientist, organization, protein, astronomical object, person, chemical compound, location, award, theory, country, event, chemical element, university, enzyme and O.\nSentence: The Cricket Club of India ( CCI ) and Mumbai Cricket Association ( MCA ) are located at Mumbai 's two cricket stadiums , namely Brabourne Stadium and Wankhede Stadium respectively .","prompt_labels":"The(O) Cricket(B-organization) Club(I-organization) of(I-organization) India(I-organization) ((O) CCI(B-organization) )(O) and(O) Mumbai(B-organization) Cricket(I-organization) Association(I-organization) ((O) MCA(B-organization) )(O) are(O) located(O) at(O) Mumbai(B-location) 's(O) two(O) cricket(O) stadiums(O) ,(O) namely(O) Brabourne(B-location) Stadium(I-location) and(O) Wankhede(B-location) Stadium(I-location) respectively(O) .(O)"}}
{"id":"161","dataset":"crossner_science","split":"train","label_list":["person","scientist","chemical element","protein","astronomical object","discipline","organization","university","country","award","theory","academic journal","event","chemical compound","enzyme","location"],"instance":{"id":"161","words":["Keelung","City","houses","several","universities","and","colleges",",","such","as","the","National","Taiwan","Ocean","University",",","Ching","Kuo","Institute","of","Management","and","Health","and","Chungyu","Institute","of","Technology","."],"labels":["B-location","I-location","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","B-university","I-university","I-university","I-university","I-university","I-university","I-university","O","B-university","I-university","I-university","I-university","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, scientist, chemical element, protein, astronomical object, discipline, organization, university, country, award, theory, academic journal, event, chemical compound, enzyme, location and O.\nSentence: Keelung City houses several universities and colleges , such as the National Taiwan Ocean University , Ching Kuo Institute of Management and Health and Chungyu Institute of Technology .","prompt_labels":"Keelung(B-location) City(I-location) houses(O) several(O) universities(O) and(O) colleges(O) ,(O) such(O) as(O) the(O) National(B-university) Taiwan(I-university) Ocean(I-university) University(I-university) ,(O) Ching(B-university) Kuo(I-university) Institute(I-university) of(I-university) Management(I-university) and(I-university) Health(I-university) and(O) Chungyu(B-university) Institute(I-university) of(I-university) Technology(I-university) .(O)"}}
{"id":"162","dataset":"crossner_science","split":"train","label_list":["chemical element","organization","country","theory","protein","astronomical object","location","event","academic journal","university","chemical compound","scientist","person","enzyme","award","discipline"],"instance":{"id":"162","words":["Yunjin","Kim","played","Sun-Hwa","Kwon",",","the","daughter","of","a","powerful","and","incredibly","wealthy","Korean","businessman","and","mobster",",","with","Daniel","Dae","Kim","as","her","husband","and","father","'s","enforcer","Jin-Soo","Kwon","."],"labels":["B-person","I-person","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","O","O","O","O","O","O","O","B-person","I-person","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, organization, country, theory, protein, astronomical object, location, event, academic journal, university, chemical compound, scientist, person, enzyme, award, discipline and O.\nSentence: Yunjin Kim played Sun-Hwa Kwon , the daughter of a powerful and incredibly wealthy Korean businessman and mobster , with Daniel Dae Kim as her husband and father 's enforcer Jin-Soo Kwon .","prompt_labels":"Yunjin(B-person) Kim(I-person) played(O) Sun-Hwa(B-person) Kwon(I-person) ,(O) the(O) daughter(O) of(O) a(O) powerful(O) and(O) incredibly(O) wealthy(O) Korean(O) businessman(O) and(O) mobster(O) ,(O) with(O) Daniel(B-person) Dae(I-person) Kim(I-person) as(O) her(O) husband(O) and(O) father(O) 's(O) enforcer(O) Jin-Soo(B-person) Kwon(I-person) .(O)"}}
{"id":"163","dataset":"crossner_science","split":"train","label_list":["award","organization","chemical compound","theory","discipline","location","chemical element","country","event","enzyme","academic journal","astronomical object","university","person","protein","scientist"],"instance":{"id":"163","words":["The","other","side","of","the","road","houses","several","historical","buildings","and","institutions","including","the","M.","A.","Chidambaram","Stadium",",","the","University","of","Madras",",","the","Presidency","College",",","Vivekananda","House",",","Queen","Mary","'s","College",",","Inspector","General","of","Police","Headquarters",",","All","India","Radio","-","Chennai",",","Dr.","Annie","Besant","Park","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","O","B-university","I-university","I-university","O","O","B-university","I-university","O","B-location","I-location","O","B-university","I-university","I-university","I-university","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-location","O","B-location","I-location","I-location","I-location","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, chemical compound, theory, discipline, location, chemical element, country, event, enzyme, academic journal, astronomical object, university, person, protein, scientist and O.\nSentence: The other side of the road houses several historical buildings and institutions including the M. A. Chidambaram Stadium , the University of Madras , the Presidency College , Vivekananda House , Queen Mary 's College , Inspector General of Police Headquarters , All India Radio - Chennai , Dr. Annie Besant Park .","prompt_labels":"The(O) other(O) side(O) of(O) the(O) road(O) houses(O) several(O) historical(O) buildings(O) and(O) institutions(O) including(O) the(O) M.(B-location) A.(I-location) Chidambaram(I-location) Stadium(I-location) ,(O) the(O) University(B-university) of(I-university) Madras(I-university) ,(O) the(O) Presidency(B-university) College(I-university) ,(O) Vivekananda(B-location) House(I-location) ,(O) Queen(B-university) Mary(I-university) 's(I-university) College(I-university) ,(O) Inspector(B-organization) General(I-organization) of(I-organization) Police(I-organization) Headquarters(I-organization) ,(O) All(B-organization) India(I-organization) Radio(I-organization) -(O) Chennai(B-location) ,(O) Dr.(B-location) Annie(I-location) Besant(I-location) Park(I-location) .(O)"}}
{"id":"164","dataset":"crossner_science","split":"train","label_list":["country","enzyme","award","theory","organization","location","astronomical object","chemical element","person","chemical compound","academic journal","university","discipline","protein","scientist","event"],"instance":{"id":"164","words":["South","Mumbai","has","reputed","and","leading","educational","institutions","such","as","Jamnalal","Bajaj","Institute","of","Management","Studies",",","St.","Xavier","'s","College",",","Government","Law","College",",","Jai","Hind","College",",","Grant","Medical","College","and","Sir","Jamshedjee","Jeejeebhoy","Group","of","Hospitals",",","Sydenham","Institute","of","Management","Studies",",","Research","and","Entrepreneurship","Education","to","name","a","few","."],"labels":["B-location","B-location","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-university","I-university","I-university","I-university","O","B-university","I-university","I-university","O","B-university","I-university","I-university","O","B-university","I-university","I-university","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, enzyme, award, theory, organization, location, astronomical object, chemical element, person, chemical compound, academic journal, university, discipline, protein, scientist, event and O.\nSentence: South Mumbai has reputed and leading educational institutions such as Jamnalal Bajaj Institute of Management Studies , St. Xavier 's College , Government Law College , Jai Hind College , Grant Medical College and Sir Jamshedjee Jeejeebhoy Group of Hospitals , Sydenham Institute of Management Studies , Research and Entrepreneurship Education to name a few .","prompt_labels":"South(B-location) Mumbai(B-location) has(O) reputed(O) and(O) leading(O) educational(O) institutions(O) such(O) as(O) Jamnalal(B-organization) Bajaj(I-organization) Institute(I-organization) of(I-organization) Management(I-organization) Studies(I-organization) ,(O) St.(B-university) Xavier(I-university) 's(I-university) College(I-university) ,(O) Government(B-university) Law(I-university) College(I-university) ,(O) Jai(B-university) Hind(I-university) College(I-university) ,(O) Grant(B-university) Medical(I-university) College(I-university) and(O) Sir(B-organization) Jamshedjee(I-organization) Jeejeebhoy(I-organization) Group(I-organization) of(I-organization) Hospitals(I-organization) ,(O) Sydenham(B-organization) Institute(I-organization) of(I-organization) Management(I-organization) Studies(I-organization) ,(O) Research(B-organization) and(I-organization) Entrepreneurship(I-organization) Education(I-organization) to(O) name(O) a(O) few(O) .(O)"}}
{"id":"165","dataset":"crossner_science","split":"train","label_list":["location","university","protein","academic journal","discipline","person","scientist","enzyme","theory","chemical element","event","award","chemical compound","country","organization","astronomical object"],"instance":{"id":"165","words":["Hewish","has","Honorary","degrees","from","six","universities","including","Manchester",",","Exeter","and","Cambridge",",","is","a","Foreign","Member","of","the","Belgian","Royal","Academy","and","the","American","Academy","of","Arts","and","Sciences","and","the","Indian","National","Science","Academy","."],"labels":["B-scientist","O","O","O","O","O","O","O","B-university","O","B-university","O","B-university","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, university, protein, academic journal, discipline, person, scientist, enzyme, theory, chemical element, event, award, chemical compound, country, organization, astronomical object and O.\nSentence: Hewish has Honorary degrees from six universities including Manchester , Exeter and Cambridge , is a Foreign Member of the Belgian Royal Academy and the American Academy of Arts and Sciences and the Indian National Science Academy .","prompt_labels":"Hewish(B-scientist) has(O) Honorary(O) degrees(O) from(O) six(O) universities(O) including(O) Manchester(B-university) ,(O) Exeter(B-university) and(O) Cambridge(B-university) ,(O) is(O) a(O) Foreign(O) Member(O) of(O) the(O) Belgian(B-organization) Royal(I-organization) Academy(I-organization) and(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) and(O) the(O) Indian(B-organization) National(I-organization) Science(I-organization) Academy(I-organization) .(O)"}}
{"id":"166","dataset":"crossner_science","split":"train","label_list":["location","country","event","chemical compound","person","award","scientist","university","enzyme","academic journal","theory","discipline","organization","protein","astronomical object","chemical element"],"instance":{"id":"166","words":["In","1977",",","the","UK-APC","named","a","series","of","peaks","in","Palmer","Land",",","Antarctica","the","Sverdrup","Nunataks","after","him","."],"labels":["O","O","O","O","B-organization","O","O","O","O","O","O","B-location","I-location","O","B-location","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, event, chemical compound, person, award, scientist, university, enzyme, academic journal, theory, discipline, organization, protein, astronomical object, chemical element and O.\nSentence: In 1977 , the UK-APC named a series of peaks in Palmer Land , Antarctica the Sverdrup Nunataks after him .","prompt_labels":"In(O) 1977(O) ,(O) the(O) UK-APC(B-organization) named(O) a(O) series(O) of(O) peaks(O) in(O) Palmer(B-location) Land(I-location) ,(O) Antarctica(B-location) the(O) Sverdrup(O) Nunataks(O) after(O) him(O) .(O)"}}
{"id":"167","dataset":"crossner_science","split":"train","label_list":["protein","person","enzyme","theory","university","scientist","astronomical object","academic journal","country","discipline","location","chemical compound","chemical element","event","organization","award"],"instance":{"id":"167","words":["After","the","war",",","he","returned","to","research","at","the","Cavendish","Laboratory","in","Cambridge","and","completed","building","his","first","mass","spectrograph","that","he","reported","on","in","1919","and","a","fellow","of","the","Royal","Society","and","received","the","Nobel","Prize","in","Chemistry","the","following","year","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-university","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, person, enzyme, theory, university, scientist, astronomical object, academic journal, country, discipline, location, chemical compound, chemical element, event, organization, award and O.\nSentence: After the war , he returned to research at the Cavendish Laboratory in Cambridge and completed building his first mass spectrograph that he reported on in 1919 and a fellow of the Royal Society and received the Nobel Prize in Chemistry the following year .","prompt_labels":"After(O) the(O) war(O) ,(O) he(O) returned(O) to(O) research(O) at(O) the(O) Cavendish(B-organization) Laboratory(I-organization) in(O) Cambridge(B-university) and(O) completed(O) building(O) his(O) first(O) mass(O) spectrograph(O) that(O) he(O) reported(O) on(O) in(O) 1919(O) and(O) a(O) fellow(B-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) and(O) received(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Chemistry(I-award) the(O) following(O) year(O) .(O)"}}
{"id":"168","dataset":"crossner_science","split":"train","label_list":["academic journal","chemical element","person","theory","award","scientist","location","discipline","astronomical object","country","enzyme","organization","university","chemical compound","event","protein"],"instance":{"id":"168","words":["The","IERS","was","established","in","its","present","form","in","1987","by","the","International","Astronomical","Union","and","the","International","Union","of","Geodesy","and","Geophysics",",","replacing","the","earlier","International","Polar","Motion","Service","(","IPMS",")","and","the","Earth","rotation","section","of","the","Bureau","International","de","l","'Heure","(","BIH",")","."],"labels":["O","B-organization","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, chemical element, person, theory, award, scientist, location, discipline, astronomical object, country, enzyme, organization, university, chemical compound, event, protein and O.\nSentence: The IERS was established in its present form in 1987 by the International Astronomical Union and the International Union of Geodesy and Geophysics , replacing the earlier International Polar Motion Service ( IPMS ) and the Earth rotation section of the Bureau International de l 'Heure ( BIH ) .","prompt_labels":"The(O) IERS(B-organization) was(O) established(O) in(O) its(O) present(O) form(O) in(O) 1987(O) by(O) the(O) International(B-organization) Astronomical(I-organization) Union(I-organization) and(O) the(O) International(B-organization) Union(I-organization) of(I-organization) Geodesy(I-organization) and(I-organization) Geophysics(I-organization) ,(O) replacing(O) the(O) earlier(O) International(B-organization) Polar(I-organization) Motion(I-organization) Service(I-organization) ((O) IPMS(B-organization) )(O) and(O) the(O) Earth(B-organization) rotation(I-organization) section(I-organization) of(I-organization) the(I-organization) Bureau(I-organization) International(I-organization) de(I-organization) l(I-organization) 'Heure(I-organization) ((O) BIH(B-organization) )(O) .(O)"}}
{"id":"169","dataset":"crossner_science","split":"train","label_list":["organization","academic journal","protein","astronomical object","enzyme","award","chemical compound","scientist","discipline","university","event","chemical element","country","person","location","theory"],"instance":{"id":"169","words":["196","(","1976",")","His","research","articles","have","appeared","in","Physical","Review",",","The","Astrophysical","Journal",",","Reviews","of","Modern","Physics",",","Nature",",","and","other","journals","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, academic journal, protein, astronomical object, enzyme, award, chemical compound, scientist, discipline, university, event, chemical element, country, person, location, theory and O.\nSentence: 196 ( 1976 ) His research articles have appeared in Physical Review , The Astrophysical Journal , Reviews of Modern Physics , Nature , and other journals .","prompt_labels":"196(O) ((O) 1976(O) )(O) His(O) research(O) articles(O) have(O) appeared(O) in(O) Physical(B-academic journal) Review(I-academic journal) ,(O) The(B-academic journal) Astrophysical(I-academic journal) Journal(I-academic journal) ,(O) Reviews(B-academic journal) of(I-academic journal) Modern(I-academic journal) Physics(I-academic journal) ,(O) Nature(B-academic journal) ,(O) and(O) other(O) journals(O) .(O)"}}
{"id":"170","dataset":"crossner_science","split":"train","label_list":["location","chemical compound","astronomical object","theory","enzyme","scientist","discipline","event","chemical element","university","academic journal","award","person","organization","protein","country"],"instance":{"id":"170","words":["The","keratins","include","the","following","proteins","of","which","KRT23",",","KRT24",",","KRT25",",","KRT26",",","KRT27",",","KRT28",",","KRT31",",","KRT32",",","KRT33A",",","KRT33B",",","KRT34",",","KRT35",",","KRT36",",","KRT37",",","KRT38",",","KRT39",",","KRT40",",","KRT71",",","KRT72",",","KRT73",",","KRT74",",","KRT75",",","KRT76",",","KRT77",",","KRT78",",","KRT79",",","KRT8",",","KRT80",",","KRT81",",","KRT82",",","KRT83",",","KRT84",",","KRT85","and","KRT86","have","been","used","to","describe","keratins","past","20","."],"labels":["O","O","O","O","O","O","O","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, chemical compound, astronomical object, theory, enzyme, scientist, discipline, event, chemical element, university, academic journal, award, person, organization, protein, country and O.\nSentence: The keratins include the following proteins of which KRT23 , KRT24 , KRT25 , KRT26 , KRT27 , KRT28 , KRT31 , KRT32 , KRT33A , KRT33B , KRT34 , KRT35 , KRT36 , KRT37 , KRT38 , KRT39 , KRT40 , KRT71 , KRT72 , KRT73 , KRT74 , KRT75 , KRT76 , KRT77 , KRT78 , KRT79 , KRT8 , KRT80 , KRT81 , KRT82 , KRT83 , KRT84 , KRT85 and KRT86 have been used to describe keratins past 20 .","prompt_labels":"The(O) keratins(O) include(O) the(O) following(O) proteins(O) of(O) which(O) KRT23(B-protein) ,(O) KRT24(B-protein) ,(O) KRT25(B-protein) ,(O) KRT26(B-protein) ,(O) KRT27(B-protein) ,(O) KRT28(B-protein) ,(O) KRT31(B-protein) ,(O) KRT32(B-protein) ,(O) KRT33A(B-protein) ,(O) KRT33B(B-protein) ,(O) KRT34(B-protein) ,(O) KRT35(B-protein) ,(O) KRT36(B-protein) ,(O) KRT37(B-protein) ,(O) KRT38(B-protein) ,(O) KRT39(B-protein) ,(O) KRT40(B-protein) ,(O) KRT71(B-protein) ,(O) KRT72(B-protein) ,(O) KRT73(B-protein) ,(O) KRT74(B-protein) ,(O) KRT75(B-protein) ,(O) KRT76(B-protein) ,(O) KRT77(B-protein) ,(O) KRT78(B-protein) ,(O) KRT79(B-protein) ,(O) KRT8(B-protein) ,(O) KRT80(B-protein) ,(O) KRT81(B-protein) ,(O) KRT82(B-protein) ,(O) KRT83(B-protein) ,(O) KRT84(B-protein) ,(O) KRT85(B-protein) and(O) KRT86(B-protein) have(O) been(O) used(O) to(O) describe(O) keratins(O) past(O) 20(O) .(O)"}}
{"id":"171","dataset":"crossner_science","split":"train","label_list":["location","theory","chemical element","award","organization","event","university","chemical compound","enzyme","astronomical object","protein","scientist","country","person","academic journal","discipline"],"instance":{"id":"171","words":["Aryl","hydrocarbon","receptor","repressor","is","known","to","inhibit","the","aryl","hydrocarbon","receptor",",","which","is","important","to","metabolizing","harmful","chemicals","."],"labels":["O","O","O","O","O","O","O","O","O","B-protein","I-protein","I-protein","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, theory, chemical element, award, organization, event, university, chemical compound, enzyme, astronomical object, protein, scientist, country, person, academic journal, discipline and O.\nSentence: Aryl hydrocarbon receptor repressor is known to inhibit the aryl hydrocarbon receptor , which is important to metabolizing harmful chemicals .","prompt_labels":"Aryl(O) hydrocarbon(O) receptor(O) repressor(O) is(O) known(O) to(O) inhibit(O) the(O) aryl(B-protein) hydrocarbon(I-protein) receptor(I-protein) ,(O) which(O) is(O) important(O) to(O) metabolizing(O) harmful(O) chemicals(O) .(O)"}}
{"id":"172","dataset":"crossner_science","split":"train","label_list":["country","theory","protein","scientist","discipline","person","organization","award","academic journal","astronomical object","event","chemical compound","chemical element","enzyme","location","university"],"instance":{"id":"172","words":["Two","pairs","of","fathers","and","sons","have","won","Nobel","Prizes","in","other","fields",":","Hans","von","Euler-Chelpin","(","chemistry",",","1929",")","and","Ulf","von","Euler","(","medicine",",","1970",")",";","and","Arthur","Kornberg","(","medicine",",","1969",")","and","Roger","D.","Kornberg","(","chemistry",",","2006",")","."],"labels":["O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-discipline","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-discipline","O","O","O","O","O","B-scientist","I-scientist","O","B-discipline","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-discipline","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, theory, protein, scientist, discipline, person, organization, award, academic journal, astronomical object, event, chemical compound, chemical element, enzyme, location, university and O.\nSentence: Two pairs of fathers and sons have won Nobel Prizes in other fields : Hans von Euler-Chelpin ( chemistry , 1929 ) and Ulf von Euler ( medicine , 1970 ) ; and Arthur Kornberg ( medicine , 1969 ) and Roger D. Kornberg ( chemistry , 2006 ) .","prompt_labels":"Two(O) pairs(O) of(O) fathers(O) and(O) sons(O) have(O) won(O) Nobel(B-award) Prizes(I-award) in(O) other(O) fields(O) :(O) Hans(B-scientist) von(I-scientist) Euler-Chelpin(I-scientist) ((O) chemistry(B-discipline) ,(O) 1929(O) )(O) and(O) Ulf(B-scientist) von(I-scientist) Euler(I-scientist) ((O) medicine(B-discipline) ,(O) 1970(O) )(O) ;(O) and(O) Arthur(B-scientist) Kornberg(I-scientist) ((O) medicine(B-discipline) ,(O) 1969(O) )(O) and(O) Roger(B-scientist) D.(I-scientist) Kornberg(I-scientist) ((O) chemistry(B-discipline) ,(O) 2006(O) )(O) .(O)"}}
{"id":"173","dataset":"crossner_science","split":"train","label_list":["protein","theory","person","discipline","enzyme","organization","academic journal","award","chemical element","event","location","country","scientist","university","astronomical object","chemical compound"],"instance":{"id":"173","words":["However",",","Health","Canada",",","the","American","Association","for","the","Advancement","of","Science",",","the","American","Medical","Association",",","the","National","Academies","of","Sciences",",","the","World","Health","Organization",",","and","several","other","scientific","and","medical","institutions","from","around","the","world","have","all","concluded","that","genetically","modified","foods","are","safe","to","eat","."],"labels":["O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, theory, person, discipline, enzyme, organization, academic journal, award, chemical element, event, location, country, scientist, university, astronomical object, chemical compound and O.\nSentence: However , Health Canada , the American Association for the Advancement of Science , the American Medical Association , the National Academies of Sciences , the World Health Organization , and several other scientific and medical institutions from around the world have all concluded that genetically modified foods are safe to eat .","prompt_labels":"However(O) ,(O) Health(B-organization) Canada(I-organization) ,(O) the(O) American(B-organization) Association(I-organization) for(I-organization) the(I-organization) Advancement(I-organization) of(I-organization) Science(I-organization) ,(O) the(O) American(B-organization) Medical(I-organization) Association(I-organization) ,(O) the(O) National(B-organization) Academies(I-organization) of(I-organization) Sciences(I-organization) ,(O) the(O) World(B-organization) Health(I-organization) Organization(I-organization) ,(O) and(O) several(O) other(O) scientific(O) and(O) medical(O) institutions(O) from(O) around(O) the(O) world(O) have(O) all(O) concluded(O) that(O) genetically(O) modified(O) foods(O) are(O) safe(O) to(O) eat(O) .(O)"}}
{"id":"174","dataset":"crossner_science","split":"train","label_list":["protein","theory","academic journal","event","enzyme","country","university","chemical element","person","location","scientist","award","discipline","organization","chemical compound","astronomical object"],"instance":{"id":"174","words":["Alexander","Marson","is","an","American","Biologist",",","specializing","in","genetics",",","human","immunology",",","and","CRISPR","genome","engineering","."],"labels":["B-scientist","I-scientist","O","O","O","O","O","O","O","B-discipline","O","B-discipline","I-discipline","O","O","B-discipline","I-discipline","I-discipline","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, theory, academic journal, event, enzyme, country, university, chemical element, person, location, scientist, award, discipline, organization, chemical compound, astronomical object and O.\nSentence: Alexander Marson is an American Biologist , specializing in genetics , human immunology , and CRISPR genome engineering .","prompt_labels":"Alexander(B-scientist) Marson(I-scientist) is(O) an(O) American(O) Biologist(O) ,(O) specializing(O) in(O) genetics(B-discipline) ,(O) human(B-discipline) immunology(I-discipline) ,(O) and(O) CRISPR(B-discipline) genome(I-discipline) engineering(I-discipline) .(O)"}}
{"id":"175","dataset":"crossner_science","split":"train","label_list":["enzyme","astronomical object","academic journal","scientist","theory","chemical element","country","university","person","protein","location","award","event","organization","discipline","chemical compound"],"instance":{"id":"175","words":["He","represented","his","country","at","the","2013","World","Championships","in","Athletics","and","2015","World","Championships","in","Athletics","without","qualifying","for","the","final","."],"labels":["O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, astronomical object, academic journal, scientist, theory, chemical element, country, university, person, protein, location, award, event, organization, discipline, chemical compound and O.\nSentence: He represented his country at the 2013 World Championships in Athletics and 2015 World Championships in Athletics without qualifying for the final .","prompt_labels":"He(O) represented(O) his(O) country(O) at(O) the(O) 2013(B-event) World(I-event) Championships(I-event) in(O) Athletics(O) and(O) 2015(B-event) World(I-event) Championships(I-event) in(O) Athletics(O) without(O) qualifying(O) for(O) the(O) final(O) .(O)"}}
{"id":"176","dataset":"crossner_science","split":"train","label_list":["chemical element","enzyme","organization","protein","country","event","theory","location","academic journal","astronomical object","discipline","person","scientist","award","university","chemical compound"],"instance":{"id":"176","words":["In","dimension","2","(","i.e.","surface","s",")",",","the","mapping","class","group","is","a","finitely","presented","group","generated","by","Dehn","twist","s","(","Max","Dehn",",","W.","B.","R.","Lickorish",",","Allen","Hatcher",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, enzyme, organization, protein, country, event, theory, location, academic journal, astronomical object, discipline, person, scientist, award, university, chemical compound and O.\nSentence: In dimension 2 ( i.e. surface s ) , the mapping class group is a finitely presented group generated by Dehn twist s ( Max Dehn , W. B. R. Lickorish , Allen Hatcher ) .","prompt_labels":"In(O) dimension(O) 2(O) ((O) i.e.(O) surface(O) s(O) )(O) ,(O) the(O) mapping(O) class(O) group(O) is(O) a(O) finitely(O) presented(O) group(O) generated(O) by(O) Dehn(O) twist(O) s(O) ((O) Max(B-scientist) Dehn(I-scientist) ,(O) W.(B-scientist) B.(I-scientist) R.(I-scientist) Lickorish(I-scientist) ,(O) Allen(B-scientist) Hatcher(I-scientist) )(O) .(O)"}}
{"id":"177","dataset":"crossner_science","split":"train","label_list":["university","location","discipline","protein","person","astronomical object","organization","event","enzyme","theory","academic journal","scientist","country","chemical compound","chemical element","award"],"instance":{"id":"177","words":["Parsons","'","obituary","listed","him","as","a","member","of","the","National","Defense","Industrial","Association",",","the","American","Institute","of","Aeronautics","and","Astronautics",",","the","American","Chemical","Society",",","the","American","Association","for","the","Advancement","of","Science","."],"labels":["B-scientist","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, location, discipline, protein, person, astronomical object, organization, event, enzyme, theory, academic journal, scientist, country, chemical compound, chemical element, award and O.\nSentence: Parsons ' obituary listed him as a member of the National Defense Industrial Association , the American Institute of Aeronautics and Astronautics , the American Chemical Society , the American Association for the Advancement of Science .","prompt_labels":"Parsons(B-scientist) '(O) obituary(O) listed(O) him(O) as(O) a(O) member(O) of(O) the(O) National(B-organization) Defense(I-organization) Industrial(I-organization) Association(I-organization) ,(O) the(O) American(B-organization) Institute(I-organization) of(I-organization) Aeronautics(I-organization) and(I-organization) Astronautics(I-organization) ,(O) the(O) American(B-organization) Chemical(I-organization) Society(I-organization) ,(O) the(O) American(B-organization) Association(I-organization) for(I-organization) the(I-organization) Advancement(I-organization) of(I-organization) Science(I-organization) .(O)"}}
{"id":"178","dataset":"crossner_science","split":"train","label_list":["academic journal","location","astronomical object","university","award","scientist","discipline","chemical element","person","event","organization","protein","chemical compound","country","enzyme","theory"],"instance":{"id":"178","words":["He","described","Bleach","'s","portrayal","as","a","halfway","house","between","the","original","version","as","played","by","Michael","Wisher","and","the","more","exuberant","...","turn","by","Terry","Molloy","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","B-person","I-person","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, location, astronomical object, university, award, scientist, discipline, chemical element, person, event, organization, protein, chemical compound, country, enzyme, theory and O.\nSentence: He described Bleach 's portrayal as a halfway house between the original version as played by Michael Wisher and the more exuberant ... turn by Terry Molloy .","prompt_labels":"He(O) described(O) Bleach(O) 's(O) portrayal(O) as(O) a(O) halfway(O) house(O) between(O) the(O) original(O) version(O) as(O) played(O) by(O) Michael(B-person) Wisher(I-person) and(O) the(O) more(O) exuberant(O) ...(O) turn(O) by(O) Terry(B-person) Molloy(I-person) .(O)"}}
{"id":"179","dataset":"crossner_science","split":"train","label_list":["theory","scientist","location","award","country","organization","university","chemical element","enzyme","discipline","astronomical object","person","event","chemical compound","academic journal","protein"],"instance":{"id":"179","words":["Knowles","was","heavily","influenced","by","the","Motown","Sound","of","the","1960s","and","1970s","prior","to","the","album","'s","recording",",","prompting","her","to","work","with","several","like-minded","producers","and","songwriters","such","as","Jack","Splash",",","CeeLo","Green",",","Mark","Ronson",",","and","former","Holland-Dozier-Holland","member","Lamont","Dozier","."],"labels":["B-person","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","O","B-organization","O","B-person","I-person","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, scientist, location, award, country, organization, university, chemical element, enzyme, discipline, astronomical object, person, event, chemical compound, academic journal, protein and O.\nSentence: Knowles was heavily influenced by the Motown Sound of the 1960s and 1970s prior to the album 's recording , prompting her to work with several like-minded producers and songwriters such as Jack Splash , CeeLo Green , Mark Ronson , and former Holland-Dozier-Holland member Lamont Dozier .","prompt_labels":"Knowles(B-person) was(O) heavily(O) influenced(O) by(O) the(O) Motown(B-organization) Sound(I-organization) of(O) the(O) 1960s(O) and(O) 1970s(O) prior(O) to(O) the(O) album(O) 's(O) recording(O) ,(O) prompting(O) her(O) to(O) work(O) with(O) several(O) like-minded(O) producers(O) and(O) songwriters(O) such(O) as(O) Jack(B-person) Splash(I-person) ,(O) CeeLo(B-person) Green(I-person) ,(O) Mark(B-person) Ronson(I-person) ,(O) and(O) former(O) Holland-Dozier-Holland(B-organization) member(O) Lamont(B-person) Dozier(I-person) .(O)"}}
{"id":"180","dataset":"crossner_science","split":"train","label_list":["organization","person","scientist","enzyme","country","astronomical object","academic journal","chemical compound","chemical element","protein","event","location","university","award","discipline","theory"],"instance":{"id":"180","words":["H3K9me3","is","an","epigenetic","modification","to","the","DNA","packaging","protein","Histone","H3","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-protein","I-protein","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, scientist, enzyme, country, astronomical object, academic journal, chemical compound, chemical element, protein, event, location, university, award, discipline, theory and O.\nSentence: H3K9me3 is an epigenetic modification to the DNA packaging protein Histone H3 .","prompt_labels":"H3K9me3(O) is(O) an(O) epigenetic(O) modification(O) to(O) the(O) DNA(O) packaging(O) protein(O) Histone(B-protein) H3(I-protein) .(O)"}}
{"id":"181","dataset":"crossner_science","split":"train","label_list":["discipline","academic journal","scientist","event","organization","theory","enzyme","chemical element","protein","country","person","chemical compound","location","award","university","astronomical object"],"instance":{"id":"181","words":["On","migrating","to","the","blood","stream",",","nitrogen","dioxide","poisoning","results","in","an","irreversible","inhibition","of","the","Acetylcholinesterase","which","may","lead","to","muscular","paralysis",",","convulsions",",","bronchoconstriction",",","the","narrowing","of","the","airways","in","the","lungs","(","bronchi","and","bronchioles",")","and","death","by","asphyxia","tion","."],"labels":["O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","O","O","O","O","O","O","O","B-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, academic journal, scientist, event, organization, theory, enzyme, chemical element, protein, country, person, chemical compound, location, award, university, astronomical object and O.\nSentence: On migrating to the blood stream , nitrogen dioxide poisoning results in an irreversible inhibition of the Acetylcholinesterase which may lead to muscular paralysis , convulsions , bronchoconstriction , the narrowing of the airways in the lungs ( bronchi and bronchioles ) and death by asphyxia tion .","prompt_labels":"On(O) migrating(O) to(O) the(O) blood(O) stream(O) ,(O) nitrogen(B-chemical compound) dioxide(I-chemical compound) poisoning(O) results(O) in(O) an(O) irreversible(O) inhibition(O) of(O) the(O) Acetylcholinesterase(B-enzyme) which(O) may(O) lead(O) to(O) muscular(O) paralysis(O) ,(O) convulsions(O) ,(O) bronchoconstriction(O) ,(O) the(O) narrowing(O) of(O) the(O) airways(O) in(O) the(O) lungs(O) ((O) bronchi(O) and(O) bronchioles(O) )(O) and(O) death(O) by(O) asphyxia(O) tion(O) .(O)"}}
{"id":"182","dataset":"crossner_science","split":"train","label_list":["chemical compound","university","country","protein","award","enzyme","scientist","organization","astronomical object","location","event","chemical element","discipline","academic journal","theory","person"],"instance":{"id":"182","words":["Holmes","has","served","on","the","editorial","or","advisory","boards","of","numerous","learned","scientific","journals",",","including","Organic","Letters",",","Chemical","Communications","and","Angewandte","Chemie","."],"labels":["B-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, university, country, protein, award, enzyme, scientist, organization, astronomical object, location, event, chemical element, discipline, academic journal, theory, person and O.\nSentence: Holmes has served on the editorial or advisory boards of numerous learned scientific journals , including Organic Letters , Chemical Communications and Angewandte Chemie .","prompt_labels":"Holmes(B-scientist) has(O) served(O) on(O) the(O) editorial(O) or(O) advisory(O) boards(O) of(O) numerous(O) learned(O) scientific(O) journals(O) ,(O) including(O) Organic(B-academic journal) Letters(I-academic journal) ,(O) Chemical(B-academic journal) Communications(I-academic journal) and(O) Angewandte(B-academic journal) Chemie(I-academic journal) .(O)"}}
{"id":"183","dataset":"crossner_science","split":"train","label_list":["protein","discipline","person","country","theory","university","chemical element","scientist","academic journal","organization","award","chemical compound","astronomical object","event","location","enzyme"],"instance":{"id":"183","words":["Her","laboratory","has","determined","the","structures","of","numerous","large","protein","complexes","that","play","critically","important","roles","in","immune","signaling",",","including","structures","of","TRAFs",",","Myddosome","and","IKK-beta","in","Toll-like","receptor",",","the","Inflammasome",",","and","the","synaptic","Recombination-activating","gene","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","O","O","O","B-protein","O","B-protein","I-protein","O","O","O","O","O","O","B-protein","I-protein","I-protein","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, discipline, person, country, theory, university, chemical element, scientist, academic journal, organization, award, chemical compound, astronomical object, event, location, enzyme and O.\nSentence: Her laboratory has determined the structures of numerous large protein complexes that play critically important roles in immune signaling , including structures of TRAFs , Myddosome and IKK-beta in Toll-like receptor , the Inflammasome , and the synaptic Recombination-activating gene .","prompt_labels":"Her(O) laboratory(O) has(O) determined(O) the(O) structures(O) of(O) numerous(O) large(O) protein(O) complexes(O) that(O) play(O) critically(O) important(O) roles(O) in(O) immune(O) signaling(O) ,(O) including(O) structures(O) of(O) TRAFs(B-protein) ,(O) Myddosome(O) and(O) IKK-beta(B-protein) in(O) Toll-like(B-protein) receptor(I-protein) ,(O) the(O) Inflammasome(O) ,(O) and(O) the(O) synaptic(B-protein) Recombination-activating(I-protein) gene(I-protein) .(O)"}}
{"id":"184","dataset":"crossner_science","split":"train","label_list":["organization","discipline","country","university","person","enzyme","award","academic journal","protein","astronomical object","theory","scientist","location","event","chemical element","chemical compound"],"instance":{"id":"184","words":["On","21","December","2012",",","the","Cassini-Huygens","probe",",","in","orbit","around","Saturn",",","observed","the","planet","Venus","transiting","the","Sun","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","B-astronomical object","O","O","B-astronomical object","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, discipline, country, university, person, enzyme, award, academic journal, protein, astronomical object, theory, scientist, location, event, chemical element, chemical compound and O.\nSentence: On 21 December 2012 , the Cassini-Huygens probe , in orbit around Saturn , observed the planet Venus transiting the Sun .","prompt_labels":"On(O) 21(O) December(O) 2012(O) ,(O) the(O) Cassini-Huygens(O) probe(O) ,(O) in(O) orbit(O) around(O) Saturn(B-astronomical object) ,(O) observed(O) the(O) planet(O) Venus(B-astronomical object) transiting(O) the(O) Sun(B-astronomical object) .(O)"}}
{"id":"185","dataset":"crossner_science","split":"train","label_list":["scientist","chemical compound","protein","award","discipline","location","organization","person","event","academic journal","astronomical object","enzyme","country","university","theory","chemical element"],"instance":{"id":"185","words":["During","his","life",",","in","addition","to","the","Nobel","Prize",",","Hopkins","was","awarded","the","Royal","Medal","of","the","Royal","Society","in","1918",",","the","Cameron","Prize","for","Therapeutics","of","the","University","of","Edinburgh","in","1922",",","and","the","Copley","Medal","of","the","Royal","Society","in","1926","."],"labels":["O","O","O","O","O","O","O","O","B-award","I-award","O","B-person","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, chemical compound, protein, award, discipline, location, organization, person, event, academic journal, astronomical object, enzyme, country, university, theory, chemical element and O.\nSentence: During his life , in addition to the Nobel Prize , Hopkins was awarded the Royal Medal of the Royal Society in 1918 , the Cameron Prize for Therapeutics of the University of Edinburgh in 1922 , and the Copley Medal of the Royal Society in 1926 .","prompt_labels":"During(O) his(O) life(O) ,(O) in(O) addition(O) to(O) the(O) Nobel(B-award) Prize(I-award) ,(O) Hopkins(B-person) was(O) awarded(O) the(O) Royal(B-award) Medal(I-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) in(O) 1918(O) ,(O) the(O) Cameron(B-award) Prize(I-award) for(I-award) Therapeutics(I-award) of(I-award) the(I-award) University(I-award) of(I-award) Edinburgh(I-award) in(O) 1922(O) ,(O) and(O) the(O) Copley(B-award) Medal(I-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) in(O) 1926(O) .(O)"}}
{"id":"186","dataset":"crossner_science","split":"train","label_list":["chemical compound","university","event","location","chemical element","protein","astronomical object","country","enzyme","person","award","organization","academic journal","scientist","theory","discipline"],"instance":{"id":"186","words":["She","is","awakened","as","a","Soldier","by","the","white","cat","Artemis","when","she","is","thirteen","years","old","and","instructed","that","she","has","a","duty","to","become","the","beautiful","warrior",",","Sailor","V.","Artemis","explains","that","Venus","and","Earth","are","twin","planets","of","about","the","same","size","and","weight",",","that","Venus","is","her","mother","star",",","and","that","she","must","protect","Earth","from","its","enemies","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, university, event, location, chemical element, protein, astronomical object, country, enzyme, person, award, organization, academic journal, scientist, theory, discipline and O.\nSentence: She is awakened as a Soldier by the white cat Artemis when she is thirteen years old and instructed that she has a duty to become the beautiful warrior , Sailor V. Artemis explains that Venus and Earth are twin planets of about the same size and weight , that Venus is her mother star , and that she must protect Earth from its enemies .","prompt_labels":"She(O) is(O) awakened(O) as(O) a(O) Soldier(O) by(O) the(O) white(O) cat(O) Artemis(O) when(O) she(O) is(O) thirteen(O) years(O) old(O) and(O) instructed(O) that(O) she(O) has(O) a(O) duty(O) to(O) become(O) the(O) beautiful(O) warrior(O) ,(O) Sailor(B-person) V.(I-person) Artemis(I-person) explains(O) that(O) Venus(B-astronomical object) and(O) Earth(B-astronomical object) are(O) twin(O) planets(O) of(O) about(O) the(O) same(O) size(O) and(O) weight(O) ,(O) that(O) Venus(B-astronomical object) is(O) her(O) mother(O) star(O) ,(O) and(O) that(O) she(O) must(O) protect(O) Earth(B-astronomical object) from(O) its(O) enemies(O) .(O)"}}
{"id":"187","dataset":"crossner_science","split":"train","label_list":["location","theory","astronomical object","protein","person","chemical element","organization","event","scientist","academic journal","university","discipline","enzyme","award","chemical compound","country"],"instance":{"id":"187","words":["In","1931",",","an","expedition","led","by","Hubert","Wilkins","and","Lincoln","Ellsworth","and","partly","financed","by","William","Randolph","Hearst",",","attempted","to","reach","the","North","Pole","with","a","leased","US","Navy","submarine","named","Nautilus","."],"labels":["O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","O","O","O","B-person","I-person","I-person","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, theory, astronomical object, protein, person, chemical element, organization, event, scientist, academic journal, university, discipline, enzyme, award, chemical compound, country and O.\nSentence: In 1931 , an expedition led by Hubert Wilkins and Lincoln Ellsworth and partly financed by William Randolph Hearst , attempted to reach the North Pole with a leased US Navy submarine named Nautilus .","prompt_labels":"In(O) 1931(O) ,(O) an(O) expedition(O) led(O) by(O) Hubert(B-person) Wilkins(I-person) and(O) Lincoln(B-person) Ellsworth(I-person) and(O) partly(O) financed(O) by(O) William(B-person) Randolph(I-person) Hearst(I-person) ,(O) attempted(O) to(O) reach(O) the(O) North(B-location) Pole(I-location) with(O) a(O) leased(O) US(O) Navy(O) submarine(O) named(O) Nautilus(O) .(O)"}}
{"id":"188","dataset":"crossner_science","split":"train","label_list":["enzyme","chemical compound","organization","country","university","academic journal","astronomical object","chemical element","scientist","location","theory","person","discipline","award","protein","event"],"instance":{"id":"188","words":["Sagan","and","his","works","received","numerous","awards","and","honors",",","including","the","NASA","Distinguished","Public","Service","Medal",",","the","National","Academy","of","Sciences","Public","Welfare","Medal",",","the","Pulitzer","Prize","for","General","Non-Fiction","for","his","book","The","Dragons","of","Eden",",","and",",","regarding","Cosmos",":","A","Personal","Voyage",",","two","Emmy","Award","s",",","the","Peabody","Award",",","and","the","Hugo","Award","."],"labels":["B-scientist","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","O","O","O","B-award","I-award","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical compound, organization, country, university, academic journal, astronomical object, chemical element, scientist, location, theory, person, discipline, award, protein, event and O.\nSentence: Sagan and his works received numerous awards and honors , including the NASA Distinguished Public Service Medal , the National Academy of Sciences Public Welfare Medal , the Pulitzer Prize for General Non-Fiction for his book The Dragons of Eden , and , regarding Cosmos : A Personal Voyage , two Emmy Award s , the Peabody Award , and the Hugo Award .","prompt_labels":"Sagan(B-scientist) and(O) his(O) works(O) received(O) numerous(O) awards(O) and(O) honors(O) ,(O) including(O) the(O) NASA(B-award) Distinguished(I-award) Public(I-award) Service(I-award) Medal(I-award) ,(O) the(O) National(B-award) Academy(I-award) of(I-award) Sciences(I-award) Public(I-award) Welfare(I-award) Medal(I-award) ,(O) the(O) Pulitzer(B-award) Prize(I-award) for(I-award) General(I-award) Non-Fiction(I-award) for(O) his(O) book(O) The(O) Dragons(O) of(O) Eden(O) ,(O) and(O) ,(O) regarding(O) Cosmos(O) :(O) A(O) Personal(O) Voyage(O) ,(O) two(O) Emmy(B-award) Award(I-award) s(O) ,(O) the(O) Peabody(B-award) Award(I-award) ,(O) and(O) the(O) Hugo(B-award) Award(I-award) .(O)"}}
{"id":"189","dataset":"crossner_science","split":"train","label_list":["organization","theory","country","university","event","scientist","award","enzyme","location","astronomical object","person","chemical compound","protein","academic journal","discipline","chemical element"],"instance":{"id":"189","words":["Both","Jupiter","and","Saturn","have","magnetic","fields","that","are","stronger","than","Earth","'s","(","Jupiter","'s","equatorial","field","strength","is","4.3","gauss",",","compared","to","0.3","gauss","for","Earth",")",",","and","both","have","extensive","radiation","belts","."],"labels":["O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","B-astronomical object","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, theory, country, university, event, scientist, award, enzyme, location, astronomical object, person, chemical compound, protein, academic journal, discipline, chemical element and O.\nSentence: Both Jupiter and Saturn have magnetic fields that are stronger than Earth 's ( Jupiter 's equatorial field strength is 4.3 gauss , compared to 0.3 gauss for Earth ) , and both have extensive radiation belts .","prompt_labels":"Both(O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) have(O) magnetic(O) fields(O) that(O) are(O) stronger(O) than(O) Earth(B-astronomical object) 's(O) ((O) Jupiter(B-astronomical object) 's(O) equatorial(O) field(O) strength(O) is(O) 4.3(O) gauss(O) ,(O) compared(O) to(O) 0.3(O) gauss(O) for(O) Earth(B-astronomical object) )(O) ,(O) and(O) both(O) have(O) extensive(O) radiation(O) belts(O) .(O)"}}
{"id":"190","dataset":"crossner_science","split":"train","label_list":["event","astronomical object","enzyme","person","location","award","scientist","country","theory","discipline","protein","chemical compound","organization","academic journal","chemical element","university"],"instance":{"id":"190","words":["Most","of","metal","contribution",",","copper",",","lead",",","and","zinc",",","comes","from","Hammarbybacken","where","high","levels","of","arsenic",",","chromium",",","phthalates",",","Polychlorinated","biphenyl",",","and","Carbon","tetrachloride","have","also","been","recorded","."],"labels":["O","O","O","O","O","B-chemical element","O","B-chemical element","O","O","B-chemical element","O","O","O","B-location","O","O","O","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical compound","I-chemical compound","O","O","B-chemical compound","I-chemical compound","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, astronomical object, enzyme, person, location, award, scientist, country, theory, discipline, protein, chemical compound, organization, academic journal, chemical element, university and O.\nSentence: Most of metal contribution , copper , lead , and zinc , comes from Hammarbybacken where high levels of arsenic , chromium , phthalates , Polychlorinated biphenyl , and Carbon tetrachloride have also been recorded .","prompt_labels":"Most(O) of(O) metal(O) contribution(O) ,(O) copper(B-chemical element) ,(O) lead(B-chemical element) ,(O) and(O) zinc(B-chemical element) ,(O) comes(O) from(O) Hammarbybacken(B-location) where(O) high(O) levels(O) of(O) arsenic(B-chemical element) ,(O) chromium(B-chemical element) ,(O) phthalates(B-chemical element) ,(O) Polychlorinated(B-chemical compound) biphenyl(I-chemical compound) ,(O) and(O) Carbon(B-chemical compound) tetrachloride(I-chemical compound) have(O) also(O) been(O) recorded(O) .(O)"}}
{"id":"191","dataset":"crossner_science","split":"train","label_list":["organization","enzyme","discipline","scientist","university","award","theory","event","country","protein","location","chemical element","chemical compound","academic journal","astronomical object","person"],"instance":{"id":"191","words":["Such","celebrities","as","Hermann","von","Helmholtz",",","Gustav","Kirchhoff",",","William","Thomson","(","Lord","Kelvin",")",",","the","Siemens","brothers",",","and","the","Marquis","of","Salisbury","visited","the","small","Norwegian","exhibit","booth","and","watched","with","amazement","as","a","system","of","pulsating","spheres","and","similar","devices","appeared","to","reproduce","well-known","electric","and","magnetic","phenomena","."],"labels":["O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","B-person","I-person","O","O","O","B-person","I-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, enzyme, discipline, scientist, university, award, theory, event, country, protein, location, chemical element, chemical compound, academic journal, astronomical object, person and O.\nSentence: Such celebrities as Hermann von Helmholtz , Gustav Kirchhoff , William Thomson ( Lord Kelvin ) , the Siemens brothers , and the Marquis of Salisbury visited the small Norwegian exhibit booth and watched with amazement as a system of pulsating spheres and similar devices appeared to reproduce well-known electric and magnetic phenomena .","prompt_labels":"Such(O) celebrities(O) as(O) Hermann(B-scientist) von(I-scientist) Helmholtz(I-scientist) ,(O) Gustav(B-scientist) Kirchhoff(I-scientist) ,(O) William(B-scientist) Thomson(I-scientist) ((O) Lord(B-scientist) Kelvin(I-scientist) )(O) ,(O) the(O) Siemens(B-person) brothers(I-person) ,(O) and(O) the(O) Marquis(B-person) of(I-person) Salisbury(I-person) visited(O) the(O) small(O) Norwegian(O) exhibit(O) booth(O) and(O) watched(O) with(O) amazement(O) as(O) a(O) system(O) of(O) pulsating(O) spheres(O) and(O) similar(O) devices(O) appeared(O) to(O) reproduce(O) well-known(O) electric(O) and(O) magnetic(O) phenomena(O) .(O)"}}
{"id":"192","dataset":"crossner_science","split":"train","label_list":["country","protein","chemical compound","astronomical object","person","award","chemical element","event","enzyme","location","theory","scientist","academic journal","discipline","university","organization"],"instance":{"id":"192","words":["On","October","7",",","2008",",","the","Royal","Swedish","Academy","of","Sciences","awarded","the","2008","Nobel","Prize","in","Physics","to","three","scientists","for","their","work","in","subatomic","physics","symmetry","breaking","."],"labels":["O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","O","B-discipline","I-discipline","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, protein, chemical compound, astronomical object, person, award, chemical element, event, enzyme, location, theory, scientist, academic journal, discipline, university, organization and O.\nSentence: On October 7 , 2008 , the Royal Swedish Academy of Sciences awarded the 2008 Nobel Prize in Physics to three scientists for their work in subatomic physics symmetry breaking .","prompt_labels":"On(O) October(O) 7(O) ,(O) 2008(O) ,(O) the(O) Royal(B-organization) Swedish(I-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) awarded(O) the(O) 2008(O) Nobel(B-award) Prize(I-award) in(I-award) Physics(I-award) to(O) three(O) scientists(O) for(O) their(O) work(O) in(O) subatomic(B-discipline) physics(I-discipline) symmetry(O) breaking(O) .(O)"}}
{"id":"193","dataset":"crossner_science","split":"train","label_list":["scientist","academic journal","location","enzyme","chemical element","country","university","discipline","event","person","organization","theory","astronomical object","award","protein","chemical compound"],"instance":{"id":"193","words":["The","Hannes","Alfvén","Prize",",","awarded","annually","by","the","European","Physical","Society","for","outstanding","contributions","in","plasma","physics",",","is","named","after","him","."],"labels":["O","B-award","I-award","I-award","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","B-discipline","I-discipline","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, academic journal, location, enzyme, chemical element, country, university, discipline, event, person, organization, theory, astronomical object, award, protein, chemical compound and O.\nSentence: The Hannes Alfvén Prize , awarded annually by the European Physical Society for outstanding contributions in plasma physics , is named after him .","prompt_labels":"The(O) Hannes(B-award) Alfvén(I-award) Prize(I-award) ,(O) awarded(O) annually(O) by(O) the(O) European(B-organization) Physical(I-organization) Society(I-organization) for(O) outstanding(O) contributions(O) in(O) plasma(B-discipline) physics(I-discipline) ,(O) is(O) named(O) after(O) him(O) .(O)"}}
{"id":"194","dataset":"crossner_science","split":"train","label_list":["award","scientist","protein","location","organization","chemical compound","university","enzyme","person","theory","event","academic journal","astronomical object","country","discipline","chemical element"],"instance":{"id":"194","words":["Since","the","middle","of","the","20th","century",",","the","region","has","come","to","play","host","to","numerous","international","sports","and","entertainment","festivals",",","such","as","the","Lisbon","&","amp",";","Estoril","Film","Festival","(","in","Estoril",")",",","the","4","Hours","of","Estoril","endurance","race","(","in","Cascais",")",",","and","NOS","Alive","music","festival","(","in","Algés",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","I-event","O","O","B-location","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","O","B-location","O","O","O","B-event","I-event","I-event","I-event","O","O","B-location","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, scientist, protein, location, organization, chemical compound, university, enzyme, person, theory, event, academic journal, astronomical object, country, discipline, chemical element and O.\nSentence: Since the middle of the 20th century , the region has come to play host to numerous international sports and entertainment festivals , such as the Lisbon & amp ; Estoril Film Festival ( in Estoril ) , the 4 Hours of Estoril endurance race ( in Cascais ) , and NOS Alive music festival ( in Algés ) .","prompt_labels":"Since(O) the(O) middle(O) of(O) the(O) 20th(O) century(O) ,(O) the(O) region(O) has(O) come(O) to(O) play(O) host(O) to(O) numerous(O) international(O) sports(O) and(O) entertainment(O) festivals(O) ,(O) such(O) as(O) the(O) Lisbon(B-event) &(I-event) amp(I-event) ;(I-event) Estoril(I-event) Film(I-event) Festival(I-event) ((O) in(O) Estoril(B-location) )(O) ,(O) the(O) 4(B-event) Hours(I-event) of(I-event) Estoril(I-event) endurance(I-event) race(I-event) ((O) in(O) Cascais(B-location) )(O) ,(O) and(O) NOS(B-event) Alive(I-event) music(I-event) festival(I-event) ((O) in(O) Algés(B-location) )(O) .(O)"}}
{"id":"195","dataset":"crossner_science","split":"train","label_list":["award","enzyme","university","scientist","astronomical object","academic journal","chemical element","event","chemical compound","country","discipline","person","protein","organization","location","theory"],"instance":{"id":"195","words":["However",",","in","November","2015",",","a","group","of","Chinese","scientists","used","the","gene","editing","technique","CRISPR","/","Cas9","to","edit","single-celled",",","non-viable","embryos","to","see","the","effectiveness","of","this","technique","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, enzyme, university, scientist, astronomical object, academic journal, chemical element, event, chemical compound, country, discipline, person, protein, organization, location, theory and O.\nSentence: However , in November 2015 , a group of Chinese scientists used the gene editing technique CRISPR / Cas9 to edit single-celled , non-viable embryos to see the effectiveness of this technique .","prompt_labels":"However(O) ,(O) in(O) November(O) 2015(O) ,(O) a(O) group(O) of(O) Chinese(O) scientists(O) used(O) the(O) gene(O) editing(O) technique(O) CRISPR(O) /(O) Cas9(O) to(O) edit(O) single-celled(O) ,(O) non-viable(O) embryos(O) to(O) see(O) the(O) effectiveness(O) of(O) this(O) technique(O) .(O)"}}
{"id":"196","dataset":"crossner_science","split":"train","label_list":["chemical compound","location","chemical element","university","organization","discipline","enzyme","person","scientist","theory","academic journal","award","astronomical object","event","country","protein"],"instance":{"id":"196","words":["For","example",",","Saturn","'","s","moons","Titan","and","Enceladus","and","Jupiter","'","s","moons","Europa","and","Ganymede",",","all","of","which","are","outside","the","habitable","zone",",","may","hold","large","volumes","of","liquid","water","in","subsurface","ocean","s","."],"labels":["O","O","O","B-astronomical object","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, location, chemical element, university, organization, discipline, enzyme, person, scientist, theory, academic journal, award, astronomical object, event, country, protein and O.\nSentence: For example , Saturn ' s moons Titan and Enceladus and Jupiter ' s moons Europa and Ganymede , all of which are outside the habitable zone , may hold large volumes of liquid water in subsurface ocean s .","prompt_labels":"For(O) example(O) ,(O) Saturn(B-astronomical object) '(O) s(O) moons(O) Titan(B-astronomical object) and(O) Enceladus(B-astronomical object) and(O) Jupiter(B-astronomical object) '(O) s(O) moons(O) Europa(B-astronomical object) and(O) Ganymede(B-astronomical object) ,(O) all(O) of(O) which(O) are(O) outside(O) the(O) habitable(O) zone(O) ,(O) may(O) hold(O) large(O) volumes(O) of(O) liquid(O) water(O) in(O) subsurface(O) ocean(O) s(O) .(O)"}}
{"id":"197","dataset":"crossner_science","split":"train","label_list":["university","enzyme","scientist","academic journal","event","protein","person","discipline","chemical element","location","award","country","astronomical object","chemical compound","theory","organization"],"instance":{"id":"197","words":["The","most","common","trialkyl","aluminum","reagents","for","this","transformation","are","Trimethylaluminium",",","Triethylaluminium",",","and","sometimes","Triisobutylaluminium","."],"labels":["O","O","O","B-chemical compound","I-chemical compound","O","O","O","O","O","B-chemical compound","O","B-chemical compound","O","O","O","B-chemical compound","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, enzyme, scientist, academic journal, event, protein, person, discipline, chemical element, location, award, country, astronomical object, chemical compound, theory, organization and O.\nSentence: The most common trialkyl aluminum reagents for this transformation are Trimethylaluminium , Triethylaluminium , and sometimes Triisobutylaluminium .","prompt_labels":"The(O) most(O) common(O) trialkyl(B-chemical compound) aluminum(I-chemical compound) reagents(O) for(O) this(O) transformation(O) are(O) Trimethylaluminium(B-chemical compound) ,(O) Triethylaluminium(B-chemical compound) ,(O) and(O) sometimes(O) Triisobutylaluminium(B-chemical compound) .(O)"}}
{"id":"198","dataset":"crossner_science","split":"train","label_list":["organization","chemical compound","astronomical object","discipline","scientist","protein","event","location","academic journal","enzyme","country","theory","person","university","award","chemical element"],"instance":{"id":"198","words":["In","DNA","replication",",","RNA","primers","must","be","inserted","along","the","lagging","strand","so","that","DNA","polymerase","is","able","to","synthesize","the","strand","in","the","5","to","3","direction","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-enzyme","I-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, chemical compound, astronomical object, discipline, scientist, protein, event, location, academic journal, enzyme, country, theory, person, university, award, chemical element and O.\nSentence: In DNA replication , RNA primers must be inserted along the lagging strand so that DNA polymerase is able to synthesize the strand in the 5 to 3 direction .","prompt_labels":"In(O) DNA(O) replication(O) ,(O) RNA(O) primers(O) must(O) be(O) inserted(O) along(O) the(O) lagging(O) strand(O) so(O) that(O) DNA(B-enzyme) polymerase(I-enzyme) is(O) able(O) to(O) synthesize(O) the(O) strand(O) in(O) the(O) 5(O) to(O) 3(O) direction(O) .(O)"}}
{"id":"199","dataset":"crossner_science","split":"train","label_list":["discipline","chemical element","university","theory","academic journal","enzyme","chemical compound","protein","organization","astronomical object","award","person","event","location","scientist","country"],"instance":{"id":"199","words":["Examples","are","5-O-Methylgenistein",",","5-O-Methylmyricetin","or","5-O-methylquercetin",",","also","known","as","azaleatin","."],"labels":["O","O","B-chemical compound","O","B-chemical compound","O","B-chemical compound","O","O","O","O","B-chemical compound","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical element, university, theory, academic journal, enzyme, chemical compound, protein, organization, astronomical object, award, person, event, location, scientist, country and O.\nSentence: Examples are 5-O-Methylgenistein , 5-O-Methylmyricetin or 5-O-methylquercetin , also known as azaleatin .","prompt_labels":"Examples(O) are(O) 5-O-Methylgenistein(B-chemical compound) ,(O) 5-O-Methylmyricetin(B-chemical compound) or(O) 5-O-methylquercetin(B-chemical compound) ,(O) also(O) known(O) as(O) azaleatin(B-chemical compound) .(O)"}}
{"id":"0","dataset":"mit-movie","split":"train","label_list":["plot","title","rating","director","year","genre","song","trailer","actor","character","average ratings","review"],"instance":{"id":"0","words":["what","movies","star","bruce","willis"],"labels":["O","O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, title, rating, director, year, genre, song, trailer, actor, character, average ratings, review and O.\nSentence: what movies star bruce willis","prompt_labels":"what(O) movies(O) star(O) bruce(B-actor) willis(I-actor)"}}
{"id":"1","dataset":"mit-movie","split":"train","label_list":["character","rating","year","plot","title","genre","song","director","actor","average ratings","review","trailer"],"instance":{"id":"1","words":["show","me","films","with","drew","barrymore","from","the","1980s"],"labels":["O","O","O","O","B-actor","I-actor","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, rating, year, plot, title, genre, song, director, actor, average ratings, review, trailer and O.\nSentence: show me films with drew barrymore from the 1980s","prompt_labels":"show(O) me(O) films(O) with(O) drew(B-actor) barrymore(I-actor) from(O) the(O) 1980s(B-year)"}}
{"id":"2","dataset":"mit-movie","split":"train","label_list":["rating","director","review","genre","title","average ratings","character","trailer","actor","song","year","plot"],"instance":{"id":"2","words":["what","movies","starred","both","al","pacino","and","robert","deniro"],"labels":["O","O","O","O","B-actor","I-actor","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, review, genre, title, average ratings, character, trailer, actor, song, year, plot and O.\nSentence: what movies starred both al pacino and robert deniro","prompt_labels":"what(O) movies(O) starred(O) both(O) al(B-actor) pacino(I-actor) and(O) robert(B-actor) deniro(I-actor)"}}
{"id":"3","dataset":"mit-movie","split":"train","label_list":["rating","actor","average ratings","plot","director","song","title","character","genre","trailer","review","year"],"instance":{"id":"3","words":["find","me","all","of","the","movies","that","starred","harold","ramis","and","bill","murray"],"labels":["O","O","O","O","O","O","O","O","B-actor","I-actor","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, actor, average ratings, plot, director, song, title, character, genre, trailer, review, year and O.\nSentence: find me all of the movies that starred harold ramis and bill murray","prompt_labels":"find(O) me(O) all(O) of(O) the(O) movies(O) that(O) starred(O) harold(B-actor) ramis(I-actor) and(O) bill(B-actor) murray(I-actor)"}}
{"id":"4","dataset":"mit-movie","split":"train","label_list":["trailer","genre","actor","plot","year","song","rating","title","director","average ratings","character","review"],"instance":{"id":"4","words":["find","me","a","movie","with","a","quote","about","baseball","in","it"],"labels":["O","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, genre, actor, plot, year, song, rating, title, director, average ratings, character, review and O.\nSentence: find me a movie with a quote about baseball in it","prompt_labels":"find(O) me(O) a(O) movie(O) with(O) a(O) quote(O) about(O) baseball(O) in(O) it(O)"}}
{"id":"5","dataset":"mit-movie","split":"train","label_list":["character","rating","review","genre","actor","trailer","song","year","plot","director","title","average ratings"],"instance":{"id":"5","words":["what","movies","have","mississippi","in","the","title"],"labels":["O","O","O","B-title","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, rating, review, genre, actor, trailer, song, year, plot, director, title, average ratings and O.\nSentence: what movies have mississippi in the title","prompt_labels":"what(O) movies(O) have(O) mississippi(B-title) in(O) the(O) title(O)"}}
{"id":"6","dataset":"mit-movie","split":"train","label_list":["actor","plot","song","character","title","trailer","genre","review","average ratings","rating","director","year"],"instance":{"id":"6","words":["show","me","science","fiction","films","directed","by","steven","spielberg"],"labels":["O","O","B-genre","I-genre","I-genre","O","O","B-director","I-director"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, plot, song, character, title, trailer, genre, review, average ratings, rating, director, year and O.\nSentence: show me science fiction films directed by steven spielberg","prompt_labels":"show(O) me(O) science(B-genre) fiction(I-genre) films(I-genre) directed(O) by(O) steven(B-director) spielberg(I-director)"}}
{"id":"7","dataset":"mit-movie","split":"train","label_list":["trailer","year","genre","review","director","song","title","average ratings","rating","character","actor","plot"],"instance":{"id":"7","words":["do","you","have","any","thrillers","directed","by","sofia","coppola"],"labels":["O","O","O","O","B-genre","O","O","B-director","I-director"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, year, genre, review, director, song, title, average ratings, rating, character, actor, plot and O.\nSentence: do you have any thrillers directed by sofia coppola","prompt_labels":"do(O) you(O) have(O) any(O) thrillers(B-genre) directed(O) by(O) sofia(B-director) coppola(I-director)"}}
{"id":"8","dataset":"mit-movie","split":"train","label_list":["genre","review","director","actor","year","title","average ratings","trailer","plot","song","rating","character"],"instance":{"id":"8","words":["what","leonard","cohen","songs","have","been","used","in","a","movie"],"labels":["O","B-song","I-song","I-song","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, review, director, actor, year, title, average ratings, trailer, plot, song, rating, character and O.\nSentence: what leonard cohen songs have been used in a movie","prompt_labels":"what(O) leonard(B-song) cohen(I-song) songs(I-song) have(O) been(O) used(O) in(O) a(O) movie(O)"}}
{"id":"9","dataset":"mit-movie","split":"train","label_list":["review","rating","title","character","song","plot","actor","trailer","year","director","average ratings","genre"],"instance":{"id":"9","words":["show","me","films","elvis","films","set","in","hawaii"],"labels":["O","O","O","B-actor","O","B-plot","I-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, rating, title, character, song, plot, actor, trailer, year, director, average ratings, genre and O.\nSentence: show me films elvis films set in hawaii","prompt_labels":"show(O) me(O) films(O) elvis(B-actor) films(O) set(B-plot) in(I-plot) hawaii(I-plot)"}}
{"id":"10","dataset":"mit-movie","split":"train","label_list":["year","review","actor","trailer","title","character","rating","average ratings","director","genre","song","plot"],"instance":{"id":"10","words":["what","movie","is","references","zydrate"],"labels":["O","O","O","O","B-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, review, actor, trailer, title, character, rating, average ratings, director, genre, song, plot and O.\nSentence: what movie is references zydrate","prompt_labels":"what(O) movie(O) is(O) references(O) zydrate(B-plot)"}}
{"id":"11","dataset":"mit-movie","split":"train","label_list":["character","plot","genre","year","rating","review","director","average ratings","song","actor","title","trailer"],"instance":{"id":"11","words":["are","there","any","musical","films","with","patrick","dempsey"],"labels":["O","O","O","B-genre","I-genre","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, plot, genre, year, rating, review, director, average ratings, song, actor, title, trailer and O.\nSentence: are there any musical films with patrick dempsey","prompt_labels":"are(O) there(O) any(O) musical(B-genre) films(I-genre) with(O) patrick(B-actor) dempsey(I-actor)"}}
{"id":"12","dataset":"mit-movie","split":"train","label_list":["review","year","actor","trailer","plot","rating","average ratings","song","character","genre","director","title"],"instance":{"id":"12","words":["list","westerns","starring","john","wayne"],"labels":["O","B-genre","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, year, actor, trailer, plot, rating, average ratings, song, character, genre, director, title and O.\nSentence: list westerns starring john wayne","prompt_labels":"list(O) westerns(B-genre) starring(O) john(B-actor) wayne(I-actor)"}}
{"id":"13","dataset":"mit-movie","split":"train","label_list":["rating","song","character","title","director","average ratings","actor","genre","plot","year","trailer","review"],"instance":{"id":"13","words":["show","me","military","related","movies","with","demi","moore"],"labels":["O","O","B-genre","O","O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, song, character, title, director, average ratings, actor, genre, plot, year, trailer, review and O.\nSentence: show me military related movies with demi moore","prompt_labels":"show(O) me(O) military(B-genre) related(O) movies(O) with(O) demi(B-actor) moore(I-actor)"}}
{"id":"14","dataset":"mit-movie","split":"train","label_list":["trailer","actor","review","director","year","character","song","plot","average ratings","genre","title","rating"],"instance":{"id":"14","words":["did","stephen","made","any","sex","or","horror","movie","that","must","be","have","to","watch"],"labels":["O","B-director","O","O","B-genre","O","B-genre","O","O","B-review","O","O","O","B-review"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, actor, review, director, year, character, song, plot, average ratings, genre, title, rating and O.\nSentence: did stephen made any sex or horror movie that must be have to watch","prompt_labels":"did(O) stephen(B-director) made(O) any(O) sex(B-genre) or(O) horror(B-genre) movie(O) that(O) must(B-review) be(O) have(O) to(O) watch(B-review)"}}
{"id":"15","dataset":"mit-movie","split":"train","label_list":["actor","song","rating","average ratings","plot","character","director","trailer","title","review","year","genre"],"instance":{"id":"15","words":["did","ang","lee","direct","a","costume","drama","movie","set","in","the","1800s"],"labels":["O","B-director","I-director","O","O","B-genre","I-genre","O","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, song, rating, average ratings, plot, character, director, trailer, title, review, year, genre and O.\nSentence: did ang lee direct a costume drama movie set in the 1800s","prompt_labels":"did(O) ang(B-director) lee(I-director) direct(O) a(O) costume(B-genre) drama(I-genre) movie(O) set(O) in(O) the(O) 1800s(B-year)"}}
{"id":"16","dataset":"mit-movie","split":"train","label_list":["director","plot","average ratings","trailer","rating","genre","title","review","song","character","year","actor"],"instance":{"id":"16","words":["who","directed","the","first","james","bond","movies"],"labels":["O","O","O","O","B-character","I-character","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, plot, average ratings, trailer, rating, genre, title, review, song, character, year, actor and O.\nSentence: who directed the first james bond movies","prompt_labels":"who(O) directed(O) the(O) first(O) james(B-character) bond(I-character) movies(O)"}}
{"id":"17","dataset":"mit-movie","split":"train","label_list":["rating","plot","director","genre","average ratings","actor","character","title","trailer","song","review","year"],"instance":{"id":"17","words":["list","the","r","rated","best","picture","oscar","winners"],"labels":["O","O","B-rating","O","B-average ratings","I-average ratings","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, plot, director, genre, average ratings, actor, character, title, trailer, song, review, year and O.\nSentence: list the r rated best picture oscar winners","prompt_labels":"list(O) the(O) r(B-rating) rated(O) best(B-average ratings) picture(I-average ratings) oscar(O) winners(O)"}}
{"id":"18","dataset":"mit-movie","split":"train","label_list":["year","song","character","director","plot","average ratings","rating","genre","title","review","trailer","actor"],"instance":{"id":"18","words":["where","can","i","buy","the","sucker","punch","soundtrack"],"labels":["O","O","O","O","O","B-title","I-title","B-song"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, song, character, director, plot, average ratings, rating, genre, title, review, trailer, actor and O.\nSentence: where can i buy the sucker punch soundtrack","prompt_labels":"where(O) can(O) i(O) buy(O) the(O) sucker(B-title) punch(I-title) soundtrack(B-song)"}}
{"id":"19","dataset":"mit-movie","split":"train","label_list":["trailer","year","review","song","plot","director","genre","rating","character","actor","title","average ratings"],"instance":{"id":"19","words":["what","british","movies","came","out","in","1990s","about","the","royal","family"],"labels":["O","B-genre","O","O","O","O","B-year","O","O","B-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, year, review, song, plot, director, genre, rating, character, actor, title, average ratings and O.\nSentence: what british movies came out in 1990s about the royal family","prompt_labels":"what(O) british(B-genre) movies(O) came(O) out(O) in(O) 1990s(B-year) about(O) the(O) royal(B-plot) family(I-plot)"}}
{"id":"20","dataset":"mit-movie","split":"train","label_list":["director","average ratings","song","rating","review","plot","year","actor","trailer","character","genre","title"],"instance":{"id":"20","words":["who","did","the","soundtrack","to","james","camerons","titanic"],"labels":["O","O","O","B-song","O","B-director","I-director","B-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, average ratings, song, rating, review, plot, year, actor, trailer, character, genre, title and O.\nSentence: who did the soundtrack to james camerons titanic","prompt_labels":"who(O) did(O) the(O) soundtrack(B-song) to(O) james(B-director) camerons(I-director) titanic(B-title)"}}
{"id":"21","dataset":"mit-movie","split":"train","label_list":["director","actor","genre","review","title","rating","year","plot","average ratings","song","character","trailer"],"instance":{"id":"21","words":["what","was","the","last","movie","judy","garland","starred","in"],"labels":["O","O","O","B-year","O","B-actor","I-actor","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, genre, review, title, rating, year, plot, average ratings, song, character, trailer and O.\nSentence: what was the last movie judy garland starred in","prompt_labels":"what(O) was(O) the(O) last(B-year) movie(O) judy(B-actor) garland(I-actor) starred(O) in(O)"}}
{"id":"22","dataset":"mit-movie","split":"train","label_list":["title","director","trailer","rating","genre","review","average ratings","character","song","year","plot","actor"],"instance":{"id":"22","words":["what","military","spoof","movie","did","charlie","sheen","do"],"labels":["O","B-genre","I-genre","O","O","B-actor","I-actor","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, director, trailer, rating, genre, review, average ratings, character, song, year, plot, actor and O.\nSentence: what military spoof movie did charlie sheen do","prompt_labels":"what(O) military(B-genre) spoof(I-genre) movie(O) did(O) charlie(B-actor) sheen(I-actor) do(O)"}}
{"id":"23","dataset":"mit-movie","split":"train","label_list":["director","average ratings","actor","genre","rating","song","character","plot","trailer","title","year","review"],"instance":{"id":"23","words":["id","like","a","documentary","about","doctors","in","a","chicago","hospital","in","the","1980s"],"labels":["O","O","O","B-genre","O","B-plot","O","O","B-plot","I-plot","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, average ratings, actor, genre, rating, song, character, plot, trailer, title, year, review and O.\nSentence: id like a documentary about doctors in a chicago hospital in the 1980s","prompt_labels":"id(O) like(O) a(O) documentary(B-genre) about(O) doctors(B-plot) in(O) a(O) chicago(B-plot) hospital(I-plot) in(O) the(O) 1980s(B-year)"}}
{"id":"24","dataset":"mit-movie","split":"train","label_list":["rating","genre","average ratings","plot","character","review","trailer","title","song","director","actor","year"],"instance":{"id":"24","words":["find","the","movies","action","movies","directed","by","john","woo","from","the","1990s"],"labels":["O","O","O","B-genre","O","O","O","B-director","I-director","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, genre, average ratings, plot, character, review, trailer, title, song, director, actor, year and O.\nSentence: find the movies action movies directed by john woo from the 1990s","prompt_labels":"find(O) the(O) movies(O) action(B-genre) movies(O) directed(O) by(O) john(B-director) woo(I-director) from(O) the(O) 1990s(B-year)"}}
{"id":"25","dataset":"mit-movie","split":"train","label_list":["genre","plot","rating","title","director","review","average ratings","trailer","character","year","song","actor"],"instance":{"id":"25","words":["what","is","a","romantic","film","where","the","protagonist","doesnt","fall","in","love"],"labels":["O","O","O","B-genre","I-genre","O","O","B-plot","I-plot","I-plot","I-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, plot, rating, title, director, review, average ratings, trailer, character, year, song, actor and O.\nSentence: what is a romantic film where the protagonist doesnt fall in love","prompt_labels":"what(O) is(O) a(O) romantic(B-genre) film(I-genre) where(O) the(O) protagonist(B-plot) doesnt(I-plot) fall(I-plot) in(I-plot) love(I-plot)"}}
{"id":"26","dataset":"mit-movie","split":"train","label_list":["year","director","song","genre","review","rating","plot","title","character","actor","trailer","average ratings"],"instance":{"id":"26","words":["does","the","original","little","mermaid","by","hans","christian","anderson","come","on","dvd"],"labels":["O","O","B-title","I-title","I-title","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, song, genre, review, rating, plot, title, character, actor, trailer, average ratings and O.\nSentence: does the original little mermaid by hans christian anderson come on dvd","prompt_labels":"does(O) the(O) original(B-title) little(I-title) mermaid(I-title) by(O) hans(O) christian(O) anderson(O) come(O) on(O) dvd(O)"}}
{"id":"27","dataset":"mit-movie","split":"train","label_list":["year","average ratings","trailer","song","genre","plot","actor","director","review","character","title","rating"],"instance":{"id":"27","words":["is","there","a","disney","movie","where","beyonce","sang","on","the","soundtrack"],"labels":["O","O","O","B-genre","I-genre","O","B-song","O","O","O","B-song"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, average ratings, trailer, song, genre, plot, actor, director, review, character, title, rating and O.\nSentence: is there a disney movie where beyonce sang on the soundtrack","prompt_labels":"is(O) there(O) a(O) disney(B-genre) movie(I-genre) where(O) beyonce(B-song) sang(O) on(O) the(O) soundtrack(B-song)"}}
{"id":"28","dataset":"mit-movie","split":"train","label_list":["actor","song","year","genre","plot","character","rating","trailer","director","average ratings","title","review"],"instance":{"id":"28","words":["did","shawn","levy","direct","any","science","fiction","movies","in","the","2010s"],"labels":["O","B-director","I-director","O","O","B-genre","I-genre","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, song, year, genre, plot, character, rating, trailer, director, average ratings, title, review and O.\nSentence: did shawn levy direct any science fiction movies in the 2010s","prompt_labels":"did(O) shawn(B-director) levy(I-director) direct(O) any(O) science(B-genre) fiction(I-genre) movies(O) in(O) the(O) 2010s(B-year)"}}
{"id":"29","dataset":"mit-movie","split":"train","label_list":["title","genre","song","trailer","plot","review","character","year","director","average ratings","actor","rating"],"instance":{"id":"29","words":["any","new","movie","of","david","dhawan"],"labels":["O","O","O","O","B-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, genre, song, trailer, plot, review, character, year, director, average ratings, actor, rating and O.\nSentence: any new movie of david dhawan","prompt_labels":"any(O) new(O) movie(O) of(O) david(B-plot) dhawan(I-plot)"}}
{"id":"30","dataset":"mit-movie","split":"train","label_list":["director","title","average ratings","rating","actor","review","character","genre","song","trailer","year","plot"],"instance":{"id":"30","words":["find","me","all","the","hannibal","lector","movies"],"labels":["O","O","O","O","B-character","I-character","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, title, average ratings, rating, actor, review, character, genre, song, trailer, year, plot and O.\nSentence: find me all the hannibal lector movies","prompt_labels":"find(O) me(O) all(O) the(O) hannibal(B-character) lector(I-character) movies(O)"}}
{"id":"31","dataset":"mit-movie","split":"train","label_list":["song","trailer","actor","character","year","review","average ratings","director","genre","title","rating","plot"],"instance":{"id":"31","words":["list","childrens","movies","with","billy","crystal"],"labels":["O","B-genre","O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, trailer, actor, character, year, review, average ratings, director, genre, title, rating, plot and O.\nSentence: list childrens movies with billy crystal","prompt_labels":"list(O) childrens(B-genre) movies(O) with(O) billy(B-actor) crystal(I-actor)"}}
{"id":"32","dataset":"mit-movie","split":"train","label_list":["actor","review","average ratings","song","director","character","genre","title","year","plot","trailer","rating"],"instance":{"id":"32","words":["what","are","the","best","reviewed","movie","rock","ballads","from","the","1980s"],"labels":["O","O","O","B-average ratings","I-average ratings","O","B-song","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, review, average ratings, song, director, character, genre, title, year, plot, trailer, rating and O.\nSentence: what are the best reviewed movie rock ballads from the 1980s","prompt_labels":"what(O) are(O) the(O) best(B-average ratings) reviewed(I-average ratings) movie(O) rock(B-song) ballads(O) from(O) the(O) 1980s(B-year)"}}
{"id":"33","dataset":"mit-movie","split":"train","label_list":["title","plot","actor","year","rating","character","genre","average ratings","review","trailer","song","director"],"instance":{"id":"33","words":["where","were","the","free","willy","movies","filmed"],"labels":["O","O","O","B-title","I-title","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, plot, actor, year, rating, character, genre, average ratings, review, trailer, song, director and O.\nSentence: where were the free willy movies filmed","prompt_labels":"where(O) were(O) the(O) free(B-title) willy(I-title) movies(O) filmed(O)"}}
{"id":"34","dataset":"mit-movie","split":"train","label_list":["character","title","year","plot","actor","trailer","genre","director","review","average ratings","song","rating"],"instance":{"id":"34","words":["is","there","a","movie","with","jennifer","aniston","that","is","r","rated"],"labels":["O","O","O","O","O","B-actor","I-actor","O","O","B-rating","I-rating"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, title, year, plot, actor, trailer, genre, director, review, average ratings, song, rating and O.\nSentence: is there a movie with jennifer aniston that is r rated","prompt_labels":"is(O) there(O) a(O) movie(O) with(O) jennifer(B-actor) aniston(I-actor) that(O) is(O) r(B-rating) rated(I-rating)"}}
{"id":"35","dataset":"mit-movie","split":"train","label_list":["rating","average ratings","title","director","year","character","actor","trailer","genre","plot","review","song"],"instance":{"id":"35","words":["who","was","the","main","character","in","star","wars"],"labels":["O","O","O","B-character","I-character","O","B-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, average ratings, title, director, year, character, actor, trailer, genre, plot, review, song and O.\nSentence: who was the main character in star wars","prompt_labels":"who(O) was(O) the(O) main(B-character) character(I-character) in(O) star(B-title) wars(I-title)"}}
{"id":"36","dataset":"mit-movie","split":"train","label_list":["actor","song","genre","review","plot","character","year","average ratings","rating","trailer","director","title"],"instance":{"id":"36","words":["im","looking","for","a","musical","with","the","song","day","by","day"],"labels":["O","O","O","O","B-genre","O","O","B-song","I-song","I-song","I-song"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, song, genre, review, plot, character, year, average ratings, rating, trailer, director, title and O.\nSentence: im looking for a musical with the song day by day","prompt_labels":"im(O) looking(O) for(O) a(O) musical(B-genre) with(O) the(O) song(B-song) day(I-song) by(I-song) day(I-song)"}}
{"id":"37","dataset":"mit-movie","split":"train","label_list":["director","title","rating","trailer","year","review","plot","genre","actor","song","average ratings","character"],"instance":{"id":"37","words":["what","movie","has","the","shortest","trailer"],"labels":["O","O","O","O","B-trailer","I-trailer"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, title, rating, trailer, year, review, plot, genre, actor, song, average ratings, character and O.\nSentence: what movie has the shortest trailer","prompt_labels":"what(O) movie(O) has(O) the(O) shortest(B-trailer) trailer(I-trailer)"}}
{"id":"38","dataset":"mit-movie","split":"train","label_list":["director","review","average ratings","character","title","actor","rating","genre","plot","trailer","song","year"],"instance":{"id":"38","words":["what","movie","has","the","song","i","will","always","love","you"],"labels":["O","O","O","O","O","B-song","I-song","I-song","I-song","I-song"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, review, average ratings, character, title, actor, rating, genre, plot, trailer, song, year and O.\nSentence: what movie has the song i will always love you","prompt_labels":"what(O) movie(O) has(O) the(O) song(O) i(B-song) will(I-song) always(I-song) love(I-song) you(I-song)"}}
{"id":"39","dataset":"mit-movie","split":"train","label_list":["plot","character","trailer","year","title","actor","review","song","director","average ratings","rating","genre"],"instance":{"id":"39","words":["i","want","to","see","a","clint","eastwood","comedy"],"labels":["O","O","O","O","O","B-actor","I-actor","B-genre"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, character, trailer, year, title, actor, review, song, director, average ratings, rating, genre and O.\nSentence: i want to see a clint eastwood comedy","prompt_labels":"i(O) want(O) to(O) see(O) a(O) clint(B-actor) eastwood(I-actor) comedy(B-genre)"}}
{"id":"40","dataset":"mit-movie","split":"train","label_list":["plot","title","actor","rating","song","genre","average ratings","character","year","trailer","director","review"],"instance":{"id":"40","words":["what","was","whitney","houstons","first","movie"],"labels":["O","O","B-actor","I-actor","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, title, actor, rating, song, genre, average ratings, character, year, trailer, director, review and O.\nSentence: what was whitney houstons first movie","prompt_labels":"what(O) was(O) whitney(B-actor) houstons(I-actor) first(O) movie(O)"}}
{"id":"41","dataset":"mit-movie","split":"train","label_list":["song","director","genre","trailer","year","average ratings","actor","review","rating","title","character","plot"],"instance":{"id":"41","words":["have","u","had","the","new","trailer","of","fast","and","furious"],"labels":["O","O","O","O","B-year","B-trailer","O","B-title","I-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, director, genre, trailer, year, average ratings, actor, review, rating, title, character, plot and O.\nSentence: have u had the new trailer of fast and furious","prompt_labels":"have(O) u(O) had(O) the(O) new(B-year) trailer(B-trailer) of(O) fast(B-title) and(I-title) furious(I-title)"}}
{"id":"42","dataset":"mit-movie","split":"train","label_list":["year","character","actor","review","director","genre","song","average ratings","title","rating","trailer","plot"],"instance":{"id":"42","words":["in","what","movie","is","jack","nicholson","in","an","insane","asylum"],"labels":["O","O","O","O","B-actor","I-actor","O","O","B-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, character, actor, review, director, genre, song, average ratings, title, rating, trailer, plot and O.\nSentence: in what movie is jack nicholson in an insane asylum","prompt_labels":"in(O) what(O) movie(O) is(O) jack(B-actor) nicholson(I-actor) in(O) an(O) insane(B-plot) asylum(I-plot)"}}
{"id":"43","dataset":"mit-movie","split":"train","label_list":["plot","year","trailer","director","genre","character","rating","song","review","average ratings","actor","title"],"instance":{"id":"43","words":["list","the","dramatic","movies","from","the","1980s","starring","donald","sutherland"],"labels":["O","O","B-genre","O","O","O","B-year","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, year, trailer, director, genre, character, rating, song, review, average ratings, actor, title and O.\nSentence: list the dramatic movies from the 1980s starring donald sutherland","prompt_labels":"list(O) the(O) dramatic(B-genre) movies(O) from(O) the(O) 1980s(B-year) starring(O) donald(B-actor) sutherland(I-actor)"}}
{"id":"44","dataset":"mit-movie","split":"train","label_list":["average ratings","title","actor","plot","genre","year","director","character","review","rating","song","trailer"],"instance":{"id":"44","words":["show","me","a","list","of","rocky","movies"],"labels":["O","O","O","O","O","B-title","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, title, actor, plot, genre, year, director, character, review, rating, song, trailer and O.\nSentence: show me a list of rocky movies","prompt_labels":"show(O) me(O) a(O) list(O) of(O) rocky(B-title) movies(O)"}}
{"id":"45","dataset":"mit-movie","split":"train","label_list":["review","plot","character","year","title","rating","genre","trailer","song","average ratings","director","actor"],"instance":{"id":"45","words":["what","is","the","aaron","sorkin","movie","that","trent","reznor","did","the","soundtrack","for"],"labels":["O","O","O","B-director","I-director","O","O","B-song","I-song","O","O","B-song","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, character, year, title, rating, genre, trailer, song, average ratings, director, actor and O.\nSentence: what is the aaron sorkin movie that trent reznor did the soundtrack for","prompt_labels":"what(O) is(O) the(O) aaron(B-director) sorkin(I-director) movie(O) that(O) trent(B-song) reznor(I-song) did(O) the(O) soundtrack(B-song) for(O)"}}
{"id":"46","dataset":"mit-movie","split":"train","label_list":["average ratings","title","rating","trailer","review","genre","character","plot","director","song","actor","year"],"instance":{"id":"46","words":["do","you","have","any","films","made","starring","michael","jackson","in","the","1970s"],"labels":["O","O","O","O","O","O","O","B-actor","I-actor","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, title, rating, trailer, review, genre, character, plot, director, song, actor, year and O.\nSentence: do you have any films made starring michael jackson in the 1970s","prompt_labels":"do(O) you(O) have(O) any(O) films(O) made(O) starring(O) michael(B-actor) jackson(I-actor) in(O) the(O) 1970s(B-year)"}}
{"id":"47","dataset":"mit-movie","split":"train","label_list":["trailer","year","average ratings","director","title","genre","rating","character","actor","plot","review","song"],"instance":{"id":"47","words":["are","there","political","drama","movies","with","alec","baldwin"],"labels":["O","O","B-genre","I-genre","O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, year, average ratings, director, title, genre, rating, character, actor, plot, review, song and O.\nSentence: are there political drama movies with alec baldwin","prompt_labels":"are(O) there(O) political(B-genre) drama(I-genre) movies(O) with(O) alec(B-actor) baldwin(I-actor)"}}
{"id":"48","dataset":"mit-movie","split":"train","label_list":["trailer","character","rating","plot","director","genre","actor","year","song","title","average ratings","review"],"instance":{"id":"48","words":["in","what","movie","did","michael","keaton","play","batman"],"labels":["O","O","O","O","B-actor","I-actor","O","B-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, character, rating, plot, director, genre, actor, year, song, title, average ratings, review and O.\nSentence: in what movie did michael keaton play batman","prompt_labels":"in(O) what(O) movie(O) did(O) michael(B-actor) keaton(I-actor) play(O) batman(B-title)"}}
{"id":"49","dataset":"mit-movie","split":"train","label_list":["song","title","character","trailer","director","actor","genre","average ratings","review","rating","plot","year"],"instance":{"id":"49","words":["show","me","movies","with","ewan","macgregor","singing"],"labels":["O","O","O","O","B-actor","I-actor","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, title, character, trailer, director, actor, genre, average ratings, review, rating, plot, year and O.\nSentence: show me movies with ewan macgregor singing","prompt_labels":"show(O) me(O) movies(O) with(O) ewan(B-actor) macgregor(I-actor) singing(O)"}}
{"id":"50","dataset":"mit-movie","split":"train","label_list":["actor","rating","director","song","plot","review","character","average ratings","year","title","genre","trailer"],"instance":{"id":"50","words":["show","me","a","list","of","superman","movies"],"labels":["O","O","O","O","O","B-title","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, rating, director, song, plot, review, character, average ratings, year, title, genre, trailer and O.\nSentence: show me a list of superman movies","prompt_labels":"show(O) me(O) a(O) list(O) of(O) superman(B-title) movies(O)"}}
{"id":"51","dataset":"mit-movie","split":"train","label_list":["actor","review","song","character","rating","trailer","plot","average ratings","director","year","genre","title"],"instance":{"id":"51","words":["what","romance","movies","were","popular","in","2002"],"labels":["O","B-genre","I-genre","O","B-review","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, review, song, character, rating, trailer, plot, average ratings, director, year, genre, title and O.\nSentence: what romance movies were popular in 2002","prompt_labels":"what(O) romance(B-genre) movies(I-genre) were(O) popular(B-review) in(O) 2002(B-year)"}}
{"id":"52","dataset":"mit-movie","split":"train","label_list":["review","song","trailer","director","title","character","genre","average ratings","year","rating","actor","plot"],"instance":{"id":"52","words":["list","the","movies","directed","by","kevin","smith","with","jason","mews"],"labels":["O","O","O","B-director","O","B-director","I-director","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, trailer, director, title, character, genre, average ratings, year, rating, actor, plot and O.\nSentence: list the movies directed by kevin smith with jason mews","prompt_labels":"list(O) the(O) movies(O) directed(B-director) by(O) kevin(B-director) smith(I-director) with(O) jason(B-actor) mews(I-actor)"}}
{"id":"53","dataset":"mit-movie","split":"train","label_list":["average ratings","song","plot","title","year","trailer","director","actor","genre","rating","character","review"],"instance":{"id":"53","words":["find","me","the","movie","with","the","song","suicide","is","painless"],"labels":["O","O","O","O","O","O","O","B-song","I-song","I-song"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, song, plot, title, year, trailer, director, actor, genre, rating, character, review and O.\nSentence: find me the movie with the song suicide is painless","prompt_labels":"find(O) me(O) the(O) movie(O) with(O) the(O) song(O) suicide(B-song) is(I-song) painless(I-song)"}}
{"id":"54","dataset":"mit-movie","split":"train","label_list":["director","actor","character","average ratings","rating","title","genre","review","trailer","plot","song","year"],"instance":{"id":"54","words":["will","their","be","a","movie","made","about","world","of","warcraft"],"labels":["O","O","O","O","O","O","O","B-plot","I-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, character, average ratings, rating, title, genre, review, trailer, plot, song, year and O.\nSentence: will their be a movie made about world of warcraft","prompt_labels":"will(O) their(O) be(O) a(O) movie(O) made(O) about(O) world(B-plot) of(I-plot) warcraft(I-plot)"}}
{"id":"55","dataset":"mit-movie","split":"train","label_list":["director","review","trailer","genre","song","average ratings","rating","plot","character","actor","title","year"],"instance":{"id":"55","words":["are","there","any","wes","anderson","films","with","jason","schwartzman"],"labels":["O","O","O","B-director","I-director","O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, review, trailer, genre, song, average ratings, rating, plot, character, actor, title, year and O.\nSentence: are there any wes anderson films with jason schwartzman","prompt_labels":"are(O) there(O) any(O) wes(B-director) anderson(I-director) films(O) with(O) jason(B-actor) schwartzman(I-actor)"}}
{"id":"56","dataset":"mit-movie","split":"train","label_list":["character","average ratings","review","actor","plot","title","song","director","trailer","genre","rating","year"],"instance":{"id":"56","words":["who","directed","savannah","smiles"],"labels":["O","B-director","B-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, average ratings, review, actor, plot, title, song, director, trailer, genre, rating, year and O.\nSentence: who directed savannah smiles","prompt_labels":"who(O) directed(B-director) savannah(B-title) smiles(I-title)"}}
{"id":"57","dataset":"mit-movie","split":"train","label_list":["genre","title","average ratings","director","song","character","plot","trailer","year","rating","actor","review"],"instance":{"id":"57","words":["show","me","films","with","owen","wilson","movies","that","were","considered","must","see"],"labels":["O","O","O","O","B-actor","I-actor","O","O","O","O","B-average ratings","I-average ratings"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, title, average ratings, director, song, character, plot, trailer, year, rating, actor, review and O.\nSentence: show me films with owen wilson movies that were considered must see","prompt_labels":"show(O) me(O) films(O) with(O) owen(B-actor) wilson(I-actor) movies(O) that(O) were(O) considered(O) must(B-average ratings) see(I-average ratings)"}}
{"id":"58","dataset":"mit-movie","split":"train","label_list":["year","review","rating","character","director","genre","actor","song","average ratings","trailer","plot","title"],"instance":{"id":"58","words":["find","me","a","movie","which","features","the","song","blaze","of","glory"],"labels":["O","O","O","O","O","O","O","O","B-song","I-song","I-song"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, review, rating, character, director, genre, actor, song, average ratings, trailer, plot, title and O.\nSentence: find me a movie which features the song blaze of glory","prompt_labels":"find(O) me(O) a(O) movie(O) which(O) features(O) the(O) song(O) blaze(B-song) of(I-song) glory(I-song)"}}
{"id":"59","dataset":"mit-movie","split":"train","label_list":["character","song","trailer","actor","average ratings","review","director","plot","genre","rating","title","year"],"instance":{"id":"59","words":["is","any","film","maker","planning","to","produce","a","world","of","warcraft","movie"],"labels":["O","O","O","O","O","O","O","O","B-plot","I-plot","I-plot","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, song, trailer, actor, average ratings, review, director, plot, genre, rating, title, year and O.\nSentence: is any film maker planning to produce a world of warcraft movie","prompt_labels":"is(O) any(O) film(O) maker(O) planning(O) to(O) produce(O) a(O) world(B-plot) of(I-plot) warcraft(I-plot) movie(O)"}}
{"id":"60","dataset":"mit-movie","split":"train","label_list":["trailer","actor","title","character","genre","director","review","year","plot","song","average ratings","rating"],"instance":{"id":"60","words":["find","all","films","directed","by","the","farely","brothers"],"labels":["O","O","O","O","O","O","B-director","I-director"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, actor, title, character, genre, director, review, year, plot, song, average ratings, rating and O.\nSentence: find all films directed by the farely brothers","prompt_labels":"find(O) all(O) films(O) directed(O) by(O) the(O) farely(B-director) brothers(I-director)"}}
{"id":"61","dataset":"mit-movie","split":"train","label_list":["average ratings","actor","plot","title","year","character","director","song","review","rating","genre","trailer"],"instance":{"id":"61","words":["what","was","the","last","move","that","frank","sinatra","was","in"],"labels":["O","O","O","O","O","O","B-actor","I-actor","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, actor, plot, title, year, character, director, song, review, rating, genre, trailer and O.\nSentence: what was the last move that frank sinatra was in","prompt_labels":"what(O) was(O) the(O) last(O) move(O) that(O) frank(B-actor) sinatra(I-actor) was(O) in(O)"}}
{"id":"62","dataset":"mit-movie","split":"train","label_list":["genre","year","trailer","actor","title","review","song","plot","rating","director","average ratings","character"],"instance":{"id":"62","words":["what","film","had","something","to","do","with","a","one","armed","man"],"labels":["O","O","O","O","O","O","O","O","B-plot","I-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, year, trailer, actor, title, review, song, plot, rating, director, average ratings, character and O.\nSentence: what film had something to do with a one armed man","prompt_labels":"what(O) film(O) had(O) something(O) to(O) do(O) with(O) a(O) one(B-plot) armed(I-plot) man(I-plot)"}}
{"id":"63","dataset":"mit-movie","split":"train","label_list":["director","actor","rating","genre","average ratings","title","character","year","trailer","plot","song","review"],"instance":{"id":"63","words":["find","me","films","starring","rick","springfield"],"labels":["O","O","O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, rating, genre, average ratings, title, character, year, trailer, plot, song, review and O.\nSentence: find me films starring rick springfield","prompt_labels":"find(O) me(O) films(O) starring(O) rick(B-actor) springfield(I-actor)"}}
{"id":"64","dataset":"mit-movie","split":"train","label_list":["plot","song","character","average ratings","trailer","director","actor","title","genre","review","rating","year"],"instance":{"id":"64","words":["list","the","movies","directed","by","shawn","levy","starring","steve","martin"],"labels":["O","O","O","O","O","B-director","I-director","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, song, character, average ratings, trailer, director, actor, title, genre, review, rating, year and O.\nSentence: list the movies directed by shawn levy starring steve martin","prompt_labels":"list(O) the(O) movies(O) directed(O) by(O) shawn(B-director) levy(I-director) starring(O) steve(B-actor) martin(I-actor)"}}
{"id":"65","dataset":"mit-movie","split":"train","label_list":["actor","rating","title","plot","character","director","review","year","trailer","song","average ratings","genre"],"instance":{"id":"65","words":["did","james","cameron","direct","any","action","films","that","were","rated","just","two","stars","or","less"],"labels":["O","B-director","I-director","O","O","B-genre","O","O","O","O","O","B-average ratings","I-average ratings","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, rating, title, plot, character, director, review, year, trailer, song, average ratings, genre and O.\nSentence: did james cameron direct any action films that were rated just two stars or less","prompt_labels":"did(O) james(B-director) cameron(I-director) direct(O) any(O) action(B-genre) films(O) that(O) were(O) rated(O) just(O) two(B-average ratings) stars(I-average ratings) or(O) less(O)"}}
{"id":"66","dataset":"mit-movie","split":"train","label_list":["song","character","plot","title","director","genre","trailer","average ratings","year","review","rating","actor"],"instance":{"id":"66","words":["find","me","dramatic","movies","with","angelina","jolie","released","in","the","1990s"],"labels":["O","O","B-genre","O","O","B-actor","I-actor","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, character, plot, title, director, genre, trailer, average ratings, year, review, rating, actor and O.\nSentence: find me dramatic movies with angelina jolie released in the 1990s","prompt_labels":"find(O) me(O) dramatic(B-genre) movies(O) with(O) angelina(B-actor) jolie(I-actor) released(O) in(O) the(O) 1990s(B-year)"}}
{"id":"67","dataset":"mit-movie","split":"train","label_list":["rating","plot","average ratings","year","review","song","trailer","director","actor","title","genre","character"],"instance":{"id":"67","words":["who","directed","the","most","horror","movies"],"labels":["O","O","O","O","B-genre","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, plot, average ratings, year, review, song, trailer, director, actor, title, genre, character and O.\nSentence: who directed the most horror movies","prompt_labels":"who(O) directed(O) the(O) most(O) horror(B-genre) movies(O)"}}
{"id":"68","dataset":"mit-movie","split":"train","label_list":["review","year","rating","plot","trailer","genre","title","character","average ratings","director","actor","song"],"instance":{"id":"68","words":["who","directed","rocky","4"],"labels":["O","O","B-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, year, rating, plot, trailer, genre, title, character, average ratings, director, actor, song and O.\nSentence: who directed rocky 4","prompt_labels":"who(O) directed(O) rocky(B-title) 4(I-title)"}}
{"id":"69","dataset":"mit-movie","split":"train","label_list":["director","rating","trailer","plot","actor","average ratings","song","genre","title","year","review","character"],"instance":{"id":"69","words":["what","was","the","oscar","winning","movie","of","1998"],"labels":["O","O","O","B-review","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, rating, trailer, plot, actor, average ratings, song, genre, title, year, review, character and O.\nSentence: what was the oscar winning movie of 1998","prompt_labels":"what(O) was(O) the(O) oscar(B-review) winning(O) movie(O) of(O) 1998(B-year)"}}
{"id":"70","dataset":"mit-movie","split":"train","label_list":["trailer","rating","review","plot","title","genre","song","year","average ratings","character","actor","director"],"instance":{"id":"70","words":["is","there","are","soundtrack","for","the","movie","roadtrip"],"labels":["O","O","O","B-song","O","O","O","B-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, rating, review, plot, title, genre, song, year, average ratings, character, actor, director and O.\nSentence: is there are soundtrack for the movie roadtrip","prompt_labels":"is(O) there(O) are(O) soundtrack(B-song) for(O) the(O) movie(O) roadtrip(B-title)"}}
{"id":"71","dataset":"mit-movie","split":"train","label_list":["year","review","title","genre","average ratings","song","actor","director","trailer","character","plot","rating"],"instance":{"id":"71","words":["list","films","with","ben","stiller","and","owen","wilson"],"labels":["O","O","O","B-actor","I-actor","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, review, title, genre, average ratings, song, actor, director, trailer, character, plot, rating and O.\nSentence: list films with ben stiller and owen wilson","prompt_labels":"list(O) films(O) with(O) ben(B-actor) stiller(I-actor) and(O) owen(B-actor) wilson(I-actor)"}}
{"id":"72","dataset":"mit-movie","split":"train","label_list":["trailer","average ratings","review","rating","director","title","genre","actor","plot","character","song","year"],"instance":{"id":"72","words":["how","many","movies","are","based","on","the","texas","chainsaw","massacre"],"labels":["O","O","O","O","O","O","O","B-title","I-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, average ratings, review, rating, director, title, genre, actor, plot, character, song, year and O.\nSentence: how many movies are based on the texas chainsaw massacre","prompt_labels":"how(O) many(O) movies(O) are(O) based(O) on(O) the(O) texas(B-title) chainsaw(I-title) massacre(I-title)"}}
{"id":"73","dataset":"mit-movie","split":"train","label_list":["director","genre","plot","title","year","song","actor","trailer","review","average ratings","character","rating"],"instance":{"id":"73","words":["did","jimmy","stewart","ever","direct","a","movie"],"labels":["O","B-actor","I-actor","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, genre, plot, title, year, song, actor, trailer, review, average ratings, character, rating and O.\nSentence: did jimmy stewart ever direct a movie","prompt_labels":"did(O) jimmy(B-actor) stewart(I-actor) ever(O) direct(O) a(O) movie(O)"}}
{"id":"74","dataset":"mit-movie","split":"train","label_list":["title","plot","year","rating","actor","character","trailer","average ratings","director","review","genre","song"],"instance":{"id":"74","words":["who","was","the","starring","actor","in","the","film","called","matchstick","men"],"labels":["O","O","O","B-actor","I-actor","O","O","O","O","B-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, plot, year, rating, actor, character, trailer, average ratings, director, review, genre, song and O.\nSentence: who was the starring actor in the film called matchstick men","prompt_labels":"who(O) was(O) the(O) starring(B-actor) actor(I-actor) in(O) the(O) film(O) called(O) matchstick(B-title) men(I-title)"}}
{"id":"75","dataset":"mit-movie","split":"train","label_list":["review","director","rating","title","plot","trailer","song","year","character","genre","average ratings","actor"],"instance":{"id":"75","words":["are","there","any","good","nc","17","rated","movies"],"labels":["O","O","O","B-review","B-rating","I-rating","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, director, rating, title, plot, trailer, song, year, character, genre, average ratings, actor and O.\nSentence: are there any good nc 17 rated movies","prompt_labels":"are(O) there(O) any(O) good(B-review) nc(B-rating) 17(I-rating) rated(O) movies(O)"}}
{"id":"76","dataset":"mit-movie","split":"train","label_list":["average ratings","title","genre","year","director","trailer","rating","character","actor","song","plot","review"],"instance":{"id":"76","words":["what","action","movies","are","based","in","texas"],"labels":["O","B-genre","I-genre","O","B-plot","I-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, title, genre, year, director, trailer, rating, character, actor, song, plot, review and O.\nSentence: what action movies are based in texas","prompt_labels":"what(O) action(B-genre) movies(I-genre) are(O) based(B-plot) in(I-plot) texas(I-plot)"}}
{"id":"77","dataset":"mit-movie","split":"train","label_list":["average ratings","actor","director","character","review","title","plot","trailer","genre","rating","year","song"],"instance":{"id":"77","words":["what","are","disney","movies","that","do","not","have","a","princess","in","them"],"labels":["O","O","B-genre","I-genre","O","O","O","O","O","B-plot","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, actor, director, character, review, title, plot, trailer, genre, rating, year, song and O.\nSentence: what are disney movies that do not have a princess in them","prompt_labels":"what(O) are(O) disney(B-genre) movies(I-genre) that(O) do(O) not(O) have(O) a(O) princess(B-plot) in(O) them(O)"}}
{"id":"78","dataset":"mit-movie","split":"train","label_list":["year","song","trailer","plot","rating","director","character","average ratings","genre","review","title","actor"],"instance":{"id":"78","words":["show","me","comedies","with","brad","pitt","released","in","the","2000s"],"labels":["O","O","B-genre","O","B-actor","I-actor","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, song, trailer, plot, rating, director, character, average ratings, genre, review, title, actor and O.\nSentence: show me comedies with brad pitt released in the 2000s","prompt_labels":"show(O) me(O) comedies(B-genre) with(O) brad(B-actor) pitt(I-actor) released(O) in(O) the(O) 2000s(B-year)"}}
{"id":"79","dataset":"mit-movie","split":"train","label_list":["rating","title","year","actor","director","genre","average ratings","trailer","character","review","song","plot"],"instance":{"id":"79","words":["who","is","the","main","character","of","the","godfather"],"labels":["O","O","O","B-character","I-character","O","O","B-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, title, year, actor, director, genre, average ratings, trailer, character, review, song, plot and O.\nSentence: who is the main character of the godfather","prompt_labels":"who(O) is(O) the(O) main(B-character) character(I-character) of(O) the(O) godfather(B-title)"}}
{"id":"80","dataset":"mit-movie","split":"train","label_list":["year","review","song","character","actor","average ratings","trailer","title","plot","rating","director","genre"],"instance":{"id":"80","words":["what","movie","featured","a","character","called","the","dude"],"labels":["O","O","O","O","B-character","O","B-character","I-character"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, review, song, character, actor, average ratings, trailer, title, plot, rating, director, genre and O.\nSentence: what movie featured a character called the dude","prompt_labels":"what(O) movie(O) featured(O) a(O) character(B-character) called(O) the(B-character) dude(I-character)"}}
{"id":"81","dataset":"mit-movie","split":"train","label_list":["rating","trailer","actor","review","plot","average ratings","character","director","song","title","genre","year"],"instance":{"id":"81","words":["are","there","any","rated","g","movies","about","world","war","ii"],"labels":["O","O","O","B-rating","I-rating","O","O","B-plot","I-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, trailer, actor, review, plot, average ratings, character, director, song, title, genre, year and O.\nSentence: are there any rated g movies about world war ii","prompt_labels":"are(O) there(O) any(O) rated(B-rating) g(I-rating) movies(O) about(O) world(B-plot) war(I-plot) ii(I-plot)"}}
{"id":"82","dataset":"mit-movie","split":"train","label_list":["actor","trailer","character","song","year","average ratings","plot","review","genre","rating","title","director"],"instance":{"id":"82","words":["please","locate","the","george","lucas","film","from","1996"],"labels":["O","O","O","B-director","I-director","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, trailer, character, song, year, average ratings, plot, review, genre, rating, title, director and O.\nSentence: please locate the george lucas film from 1996","prompt_labels":"please(O) locate(O) the(O) george(B-director) lucas(I-director) film(O) from(O) 1996(B-year)"}}
{"id":"83","dataset":"mit-movie","split":"train","label_list":["genre","character","director","plot","year","average ratings","trailer","actor","title","song","rating","review"],"instance":{"id":"83","words":["what","actors","are","in","the","original","fast","and","the","furious"],"labels":["O","O","O","O","O","O","B-title","I-title","I-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, character, director, plot, year, average ratings, trailer, actor, title, song, rating, review and O.\nSentence: what actors are in the original fast and the furious","prompt_labels":"what(O) actors(O) are(O) in(O) the(O) original(O) fast(B-title) and(I-title) the(I-title) furious(I-title)"}}
{"id":"84","dataset":"mit-movie","split":"train","label_list":["genre","actor","song","plot","title","review","rating","trailer","director","year","character","average ratings"],"instance":{"id":"84","words":["what","actor","does","the","cameo","in","the","spongebob","movie"],"labels":["O","O","O","O","B-plot","O","O","B-plot","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, actor, song, plot, title, review, rating, trailer, director, year, character, average ratings and O.\nSentence: what actor does the cameo in the spongebob movie","prompt_labels":"what(O) actor(O) does(O) the(O) cameo(B-plot) in(O) the(O) spongebob(B-plot) movie(O)"}}
{"id":"85","dataset":"mit-movie","split":"train","label_list":["rating","song","trailer","genre","average ratings","review","plot","actor","year","title","character","director"],"instance":{"id":"85","words":["what","year","was","the","social","network","made"],"labels":["O","B-year","O","B-title","I-title","I-title","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, song, trailer, genre, average ratings, review, plot, actor, year, title, character, director and O.\nSentence: what year was the social network made","prompt_labels":"what(O) year(B-year) was(O) the(B-title) social(I-title) network(I-title) made(O)"}}
{"id":"86","dataset":"mit-movie","split":"train","label_list":["year","director","character","song","trailer","actor","genre","average ratings","plot","rating","review","title"],"instance":{"id":"86","words":["what","is","the","plot","of","rocky","1"],"labels":["O","O","O","O","O","B-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, character, song, trailer, actor, genre, average ratings, plot, rating, review, title and O.\nSentence: what is the plot of rocky 1","prompt_labels":"what(O) is(O) the(O) plot(O) of(O) rocky(B-title) 1(I-title)"}}
{"id":"87","dataset":"mit-movie","split":"train","label_list":["year","title","plot","trailer","actor","genre","rating","average ratings","review","song","director","character"],"instance":{"id":"87","words":["movies","released","from","1990","to","1991"],"labels":["O","O","O","B-year","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, title, plot, trailer, actor, genre, rating, average ratings, review, song, director, character and O.\nSentence: movies released from 1990 to 1991","prompt_labels":"movies(O) released(O) from(O) 1990(B-year) to(O) 1991(B-year)"}}
{"id":"88","dataset":"mit-movie","split":"train","label_list":["average ratings","plot","actor","trailer","director","rating","character","year","review","song","genre","title"],"instance":{"id":"88","words":["did","garry","marshall","direct","a","drama","movie","released","in","the","1980s"],"labels":["O","B-director","I-director","O","O","B-genre","O","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, plot, actor, trailer, director, rating, character, year, review, song, genre, title and O.\nSentence: did garry marshall direct a drama movie released in the 1980s","prompt_labels":"did(O) garry(B-director) marshall(I-director) direct(O) a(O) drama(B-genre) movie(O) released(O) in(O) the(O) 1980s(B-year)"}}
{"id":"89","dataset":"mit-movie","split":"train","label_list":["director","rating","song","character","trailer","plot","title","review","genre","average ratings","actor","year"],"instance":{"id":"89","words":["show","me","a","list","of","the","best","movies","of","2011"],"labels":["O","O","O","O","O","O","B-review","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, rating, song, character, trailer, plot, title, review, genre, average ratings, actor, year and O.\nSentence: show me a list of the best movies of 2011","prompt_labels":"show(O) me(O) a(O) list(O) of(O) the(O) best(B-review) movies(O) of(O) 2011(B-year)"}}
{"id":"90","dataset":"mit-movie","split":"train","label_list":["song","director","genre","average ratings","review","rating","title","year","plot","character","trailer","actor"],"instance":{"id":"90","words":["find","me","star","wars","movies","with","natalie","portman"],"labels":["O","O","B-title","I-title","O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, director, genre, average ratings, review, rating, title, year, plot, character, trailer, actor and O.\nSentence: find me star wars movies with natalie portman","prompt_labels":"find(O) me(O) star(B-title) wars(I-title) movies(O) with(O) natalie(B-actor) portman(I-actor)"}}
{"id":"91","dataset":"mit-movie","split":"train","label_list":["song","character","title","actor","average ratings","rating","director","genre","review","plot","trailer","year"],"instance":{"id":"91","words":["what","is","the","first","movie","martin","sheen","starred","in"],"labels":["O","O","O","O","O","B-actor","I-actor","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, character, title, actor, average ratings, rating, director, genre, review, plot, trailer, year and O.\nSentence: what is the first movie martin sheen starred in","prompt_labels":"what(O) is(O) the(O) first(O) movie(O) martin(B-actor) sheen(I-actor) starred(O) in(O)"}}
{"id":"92","dataset":"mit-movie","split":"train","label_list":["plot","song","director","title","review","rating","year","actor","average ratings","genre","trailer","character"],"instance":{"id":"92","words":["did","george","lucas","direct","any","dramas"],"labels":["O","B-director","I-director","O","O","B-genre"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, song, director, title, review, rating, year, actor, average ratings, genre, trailer, character and O.\nSentence: did george lucas direct any dramas","prompt_labels":"did(O) george(B-director) lucas(I-director) direct(O) any(O) dramas(B-genre)"}}
{"id":"93","dataset":"mit-movie","split":"train","label_list":["actor","trailer","title","song","character","rating","average ratings","review","genre","director","year","plot"],"instance":{"id":"93","words":["what","movie","has","the","latest","john","williams","soundtrack"],"labels":["O","O","O","O","B-year","B-song","I-song","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, trailer, title, song, character, rating, average ratings, review, genre, director, year, plot and O.\nSentence: what movie has the latest john williams soundtrack","prompt_labels":"what(O) movie(O) has(O) the(O) latest(B-year) john(B-song) williams(I-song) soundtrack(O)"}}
{"id":"94","dataset":"mit-movie","split":"train","label_list":["review","title","rating","year","director","actor","average ratings","trailer","character","song","genre","plot"],"instance":{"id":"94","words":["find","me","the","most","popular","westerns","of","all","time"],"labels":["O","O","O","B-review","I-review","B-genre","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, title, rating, year, director, actor, average ratings, trailer, character, song, genre, plot and O.\nSentence: find me the most popular westerns of all time","prompt_labels":"find(O) me(O) the(O) most(B-review) popular(I-review) westerns(B-genre) of(O) all(O) time(O)"}}
{"id":"95","dataset":"mit-movie","split":"train","label_list":["title","actor","average ratings","genre","year","review","plot","character","rating","director","song","trailer"],"instance":{"id":"95","words":["what","movies","from","the","1970s","had","car","chases"],"labels":["O","O","O","O","B-year","O","B-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, actor, average ratings, genre, year, review, plot, character, rating, director, song, trailer and O.\nSentence: what movies from the 1970s had car chases","prompt_labels":"what(O) movies(O) from(O) the(O) 1970s(B-year) had(O) car(B-plot) chases(I-plot)"}}
{"id":"96","dataset":"mit-movie","split":"train","label_list":["actor","title","genre","rating","director","review","song","year","average ratings","character","plot","trailer"],"instance":{"id":"96","words":["what","was","the","worst","reviewed","scary","movie","in","2010"],"labels":["O","O","O","B-average ratings","I-average ratings","B-genre","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, title, genre, rating, director, review, song, year, average ratings, character, plot, trailer and O.\nSentence: what was the worst reviewed scary movie in 2010","prompt_labels":"what(O) was(O) the(O) worst(B-average ratings) reviewed(I-average ratings) scary(B-genre) movie(O) in(O) 2010(B-year)"}}
{"id":"97","dataset":"mit-movie","split":"train","label_list":["year","director","rating","song","average ratings","title","trailer","review","plot","character","actor","genre"],"instance":{"id":"97","words":["please","find","me","the","thriller","about","an","art","heist","from","the","late","1990s"],"labels":["O","O","O","O","B-genre","O","O","B-plot","I-plot","O","O","O","B-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, rating, song, average ratings, title, trailer, review, plot, character, actor, genre and O.\nSentence: please find me the thriller about an art heist from the late 1990s","prompt_labels":"please(O) find(O) me(O) the(O) thriller(B-genre) about(O) an(O) art(B-plot) heist(I-plot) from(O) the(O) late(O) 1990s(B-plot)"}}
{"id":"98","dataset":"mit-movie","split":"train","label_list":["average ratings","actor","title","genre","director","plot","character","rating","trailer","song","year","review"],"instance":{"id":"98","words":["did","james","earl","jones","star","in","any","horror","films"],"labels":["O","B-actor","I-actor","I-actor","O","O","O","B-genre","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, actor, title, genre, director, plot, character, rating, trailer, song, year, review and O.\nSentence: did james earl jones star in any horror films","prompt_labels":"did(O) james(B-actor) earl(I-actor) jones(I-actor) star(O) in(O) any(O) horror(B-genre) films(O)"}}
{"id":"99","dataset":"mit-movie","split":"train","label_list":["title","director","song","review","actor","character","genre","trailer","year","plot","average ratings","rating"],"instance":{"id":"99","words":["what","is","the","quote","at","the","end","of","a","bronx","tale"],"labels":["O","O","O","O","O","O","O","O","B-title","I-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, director, song, review, actor, character, genre, trailer, year, plot, average ratings, rating and O.\nSentence: what is the quote at the end of a bronx tale","prompt_labels":"what(O) is(O) the(O) quote(O) at(O) the(O) end(O) of(O) a(B-title) bronx(I-title) tale(I-title)"}}
{"id":"100","dataset":"mit-movie","split":"train","label_list":["review","actor","rating","title","genre","trailer","average ratings","year","plot","song","director","character"],"instance":{"id":"100","words":["what","is","the","top","rated","martin","scorsesy","moive"],"labels":["O","O","O","B-average ratings","I-average ratings","B-director","I-director","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, actor, rating, title, genre, trailer, average ratings, year, plot, song, director, character and O.\nSentence: what is the top rated martin scorsesy moive","prompt_labels":"what(O) is(O) the(O) top(B-average ratings) rated(I-average ratings) martin(B-director) scorsesy(I-director) moive(O)"}}
{"id":"101","dataset":"mit-movie","split":"train","label_list":["rating","genre","review","trailer","director","average ratings","song","plot","year","actor","title","character"],"instance":{"id":"101","words":["show","me","a","movie","rated","pg","13","that","is","about","ghosts"],"labels":["O","O","O","O","O","B-rating","I-rating","O","O","O","B-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, genre, review, trailer, director, average ratings, song, plot, year, actor, title, character and O.\nSentence: show me a movie rated pg 13 that is about ghosts","prompt_labels":"show(O) me(O) a(O) movie(O) rated(O) pg(B-rating) 13(I-rating) that(O) is(O) about(O) ghosts(B-plot)"}}
{"id":"102","dataset":"mit-movie","split":"train","label_list":["review","title","character","trailer","genre","song","year","average ratings","director","rating","actor","plot"],"instance":{"id":"102","words":["what","year","did","the","godfather","come","out"],"labels":["O","B-year","O","B-title","I-title","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, title, character, trailer, genre, song, year, average ratings, director, rating, actor, plot and O.\nSentence: what year did the godfather come out","prompt_labels":"what(O) year(B-year) did(O) the(B-title) godfather(I-title) come(O) out(O)"}}
{"id":"103","dataset":"mit-movie","split":"train","label_list":["genre","plot","year","actor","review","song","character","director","trailer","rating","title","average ratings"],"instance":{"id":"103","words":["show","me","romantic","comedy","movies","directed","by","shawn","levy"],"labels":["O","O","B-genre","I-genre","O","O","O","B-director","I-director"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, plot, year, actor, review, song, character, director, trailer, rating, title, average ratings and O.\nSentence: show me romantic comedy movies directed by shawn levy","prompt_labels":"show(O) me(O) romantic(B-genre) comedy(I-genre) movies(O) directed(O) by(O) shawn(B-director) levy(I-director)"}}
{"id":"104","dataset":"mit-movie","split":"train","label_list":["actor","plot","director","title","character","trailer","genre","average ratings","review","song","year","rating"],"instance":{"id":"104","words":["show","me","r","rated","films","with","jennifer","aniston"],"labels":["O","O","B-rating","O","O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, plot, director, title, character, trailer, genre, average ratings, review, song, year, rating and O.\nSentence: show me r rated films with jennifer aniston","prompt_labels":"show(O) me(O) r(B-rating) rated(O) films(O) with(O) jennifer(B-actor) aniston(I-actor)"}}
{"id":"105","dataset":"mit-movie","split":"train","label_list":["rating","average ratings","plot","genre","trailer","actor","character","year","title","review","director","song"],"instance":{"id":"105","words":["show","me","g","rated","films","about","dogs","rated","must","see"],"labels":["O","O","B-rating","I-rating","O","O","B-plot","O","B-review","I-review"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, average ratings, plot, genre, trailer, actor, character, year, title, review, director, song and O.\nSentence: show me g rated films about dogs rated must see","prompt_labels":"show(O) me(O) g(B-rating) rated(I-rating) films(O) about(O) dogs(B-plot) rated(O) must(B-review) see(I-review)"}}
{"id":"106","dataset":"mit-movie","split":"train","label_list":["genre","year","average ratings","song","plot","title","actor","director","review","character","rating","trailer"],"instance":{"id":"106","words":["show","me","musicals","starring","antonio","banderas"],"labels":["O","O","B-genre","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, year, average ratings, song, plot, title, actor, director, review, character, rating, trailer and O.\nSentence: show me musicals starring antonio banderas","prompt_labels":"show(O) me(O) musicals(B-genre) starring(O) antonio(B-actor) banderas(I-actor)"}}
{"id":"107","dataset":"mit-movie","split":"train","label_list":["average ratings","director","title","rating","plot","trailer","character","song","review","actor","year","genre"],"instance":{"id":"107","words":["are","there","hitchcock","color","movies"],"labels":["O","O","B-director","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, director, title, rating, plot, trailer, character, song, review, actor, year, genre and O.\nSentence: are there hitchcock color movies","prompt_labels":"are(O) there(O) hitchcock(B-director) color(O) movies(O)"}}
{"id":"108","dataset":"mit-movie","split":"train","label_list":["plot","average ratings","review","character","title","year","song","director","trailer","rating","genre","actor"],"instance":{"id":"108","words":["pg","13","movies","about","animals"],"labels":["B-year","I-year","O","O","B-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, average ratings, review, character, title, year, song, director, trailer, rating, genre, actor and O.\nSentence: pg 13 movies about animals","prompt_labels":"pg(B-year) 13(I-year) movies(O) about(O) animals(B-plot)"}}
{"id":"109","dataset":"mit-movie","split":"train","label_list":["genre","title","year","rating","average ratings","trailer","song","actor","review","plot","director","character"],"instance":{"id":"109","words":["is","there","a","five","star","movie","with","the","song","love","the","one","youre","with"],"labels":["O","O","O","B-average ratings","I-average ratings","O","O","O","O","B-song","I-song","I-song","I-song","I-song"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, title, year, rating, average ratings, trailer, song, actor, review, plot, director, character and O.\nSentence: is there a five star movie with the song love the one youre with","prompt_labels":"is(O) there(O) a(O) five(B-average ratings) star(I-average ratings) movie(O) with(O) the(O) song(O) love(B-song) the(I-song) one(I-song) youre(I-song) with(I-song)"}}
{"id":"110","dataset":"mit-movie","split":"train","label_list":["plot","rating","title","genre","actor","trailer","average ratings","song","director","character","review","year"],"instance":{"id":"110","words":["how","many","love","stories","has","amanda","seyfried","starred","in"],"labels":["O","O","B-genre","I-genre","O","B-actor","I-actor","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, rating, title, genre, actor, trailer, average ratings, song, director, character, review, year and O.\nSentence: how many love stories has amanda seyfried starred in","prompt_labels":"how(O) many(O) love(B-genre) stories(I-genre) has(O) amanda(B-actor) seyfried(I-actor) starred(O) in(O)"}}
{"id":"111","dataset":"mit-movie","split":"train","label_list":["rating","plot","review","song","trailer","actor","genre","director","title","average ratings","year","character"],"instance":{"id":"111","words":["what","movie","is","ill","be","back","from"],"labels":["O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, plot, review, song, trailer, actor, genre, director, title, average ratings, year, character and O.\nSentence: what movie is ill be back from","prompt_labels":"what(O) movie(O) is(O) ill(O) be(O) back(O) from(O)"}}
{"id":"112","dataset":"mit-movie","split":"train","label_list":["rating","trailer","plot","song","average ratings","title","review","genre","actor","character","year","director"],"instance":{"id":"112","words":["show","me","foreign","romance","movies"],"labels":["O","O","B-genre","I-genre","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, trailer, plot, song, average ratings, title, review, genre, actor, character, year, director and O.\nSentence: show me foreign romance movies","prompt_labels":"show(O) me(O) foreign(B-genre) romance(I-genre) movies(O)"}}
{"id":"113","dataset":"mit-movie","split":"train","label_list":["director","genre","song","rating","trailer","actor","character","plot","year","review","title","average ratings"],"instance":{"id":"113","words":["are","there","any","scary","movies","that","are","not","rated","r"],"labels":["O","O","O","B-genre","O","O","O","B-rating","I-rating","I-rating"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, genre, song, rating, trailer, actor, character, plot, year, review, title, average ratings and O.\nSentence: are there any scary movies that are not rated r","prompt_labels":"are(O) there(O) any(O) scary(B-genre) movies(O) that(O) are(O) not(B-rating) rated(I-rating) r(I-rating)"}}
{"id":"114","dataset":"mit-movie","split":"train","label_list":["genre","average ratings","character","year","trailer","rating","director","actor","song","review","plot","title"],"instance":{"id":"114","words":["what","is","the","most","popular","martin","sheen","film"],"labels":["O","O","O","B-review","I-review","B-director","I-director","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, average ratings, character, year, trailer, rating, director, actor, song, review, plot, title and O.\nSentence: what is the most popular martin sheen film","prompt_labels":"what(O) is(O) the(O) most(B-review) popular(I-review) martin(B-director) sheen(I-director) film(O)"}}
{"id":"115","dataset":"mit-movie","split":"train","label_list":["average ratings","genre","actor","rating","character","plot","year","trailer","director","song","title","review"],"instance":{"id":"115","words":["find","me","the","movie","with","the","song","blue","hawaii"],"labels":["O","O","O","O","O","O","O","B-song","I-song"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, genre, actor, rating, character, plot, year, trailer, director, song, title, review and O.\nSentence: find me the movie with the song blue hawaii","prompt_labels":"find(O) me(O) the(O) movie(O) with(O) the(O) song(O) blue(B-song) hawaii(I-song)"}}
{"id":"116","dataset":"mit-movie","split":"train","label_list":["average ratings","trailer","review","title","genre","director","year","actor","song","character","plot","rating"],"instance":{"id":"116","words":["did","rob","reiner","direct","a","movie","released","in","1992","that","had","a","military","drill","team"],"labels":["O","B-director","I-director","O","O","O","O","O","B-year","O","O","O","B-plot","I-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, trailer, review, title, genre, director, year, actor, song, character, plot, rating and O.\nSentence: did rob reiner direct a movie released in 1992 that had a military drill team","prompt_labels":"did(O) rob(B-director) reiner(I-director) direct(O) a(O) movie(O) released(O) in(O) 1992(B-year) that(O) had(O) a(O) military(B-plot) drill(I-plot) team(I-plot)"}}
{"id":"117","dataset":"mit-movie","split":"train","label_list":["plot","actor","average ratings","year","title","director","genre","song","trailer","character","review","rating"],"instance":{"id":"117","words":["give","me","a","list","of","all","movies","that","have","the","song","ice","ice","baby","by","vanilla","ice"],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-song","I-song","I-song","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, actor, average ratings, year, title, director, genre, song, trailer, character, review, rating and O.\nSentence: give me a list of all movies that have the song ice ice baby by vanilla ice","prompt_labels":"give(O) me(O) a(O) list(O) of(O) all(O) movies(O) that(O) have(O) the(O) song(O) ice(B-song) ice(I-song) baby(I-song) by(O) vanilla(O) ice(O)"}}
{"id":"118","dataset":"mit-movie","split":"train","label_list":["year","director","title","trailer","character","actor","rating","song","genre","plot","review","average ratings"],"instance":{"id":"118","words":["i","want","to","buy","the","soundtrack","from","an","atmospheric","movie"],"labels":["O","O","O","O","O","B-song","O","O","B-plot","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, title, trailer, character, actor, rating, song, genre, plot, review, average ratings and O.\nSentence: i want to buy the soundtrack from an atmospheric movie","prompt_labels":"i(O) want(O) to(O) buy(O) the(O) soundtrack(B-song) from(O) an(O) atmospheric(B-plot) movie(O)"}}
{"id":"119","dataset":"mit-movie","split":"train","label_list":["review","year","genre","trailer","song","character","average ratings","title","plot","actor","rating","director"],"instance":{"id":"119","words":["show","me","the","top","rated","movie","of","2011"],"labels":["O","O","O","B-average ratings","I-average ratings","I-average ratings","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, year, genre, trailer, song, character, average ratings, title, plot, actor, rating, director and O.\nSentence: show me the top rated movie of 2011","prompt_labels":"show(O) me(O) the(O) top(B-average ratings) rated(I-average ratings) movie(I-average ratings) of(O) 2011(B-year)"}}
{"id":"120","dataset":"mit-movie","split":"train","label_list":["song","director","average ratings","trailer","genre","character","rating","actor","year","plot","title","review"],"instance":{"id":"120","words":["what","is","bambi","about"],"labels":["O","O","B-title","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, director, average ratings, trailer, genre, character, rating, actor, year, plot, title, review and O.\nSentence: what is bambi about","prompt_labels":"what(O) is(O) bambi(B-title) about(O)"}}
{"id":"121","dataset":"mit-movie","split":"train","label_list":["director","plot","actor","title","review","genre","rating","year","trailer","average ratings","character","song"],"instance":{"id":"121","words":["find","me","teen","films","from","the","1990s"],"labels":["O","O","B-genre","I-genre","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, plot, actor, title, review, genre, rating, year, trailer, average ratings, character, song and O.\nSentence: find me teen films from the 1990s","prompt_labels":"find(O) me(O) teen(B-genre) films(I-genre) from(O) the(O) 1990s(B-year)"}}
{"id":"122","dataset":"mit-movie","split":"train","label_list":["average ratings","review","plot","director","song","rating","year","genre","title","actor","trailer","character"],"instance":{"id":"122","words":["have","you","got","any","rock","operas"],"labels":["O","O","O","O","B-genre","I-genre"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, review, plot, director, song, rating, year, genre, title, actor, trailer, character and O.\nSentence: have you got any rock operas","prompt_labels":"have(O) you(O) got(O) any(O) rock(B-genre) operas(I-genre)"}}
{"id":"123","dataset":"mit-movie","split":"train","label_list":["trailer","actor","genre","year","song","plot","rating","average ratings","review","director","character","title"],"instance":{"id":"123","words":["list","the","science","fiction","movies","with","selma","blair"],"labels":["O","O","B-genre","I-genre","I-genre","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, actor, genre, year, song, plot, rating, average ratings, review, director, character, title and O.\nSentence: list the science fiction movies with selma blair","prompt_labels":"list(O) the(O) science(B-genre) fiction(I-genre) movies(I-genre) with(O) selma(B-actor) blair(I-actor)"}}
{"id":"124","dataset":"mit-movie","split":"train","label_list":["title","character","actor","trailer","plot","song","average ratings","review","director","genre","year","rating"],"instance":{"id":"124","words":["who","directed","cars"],"labels":["O","B-director","B-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, character, actor, trailer, plot, song, average ratings, review, director, genre, year, rating and O.\nSentence: who directed cars","prompt_labels":"who(O) directed(B-director) cars(B-title)"}}
{"id":"125","dataset":"mit-movie","split":"train","label_list":["average ratings","trailer","rating","genre","year","title","song","director","plot","actor","review","character"],"instance":{"id":"125","words":["find","me","movies","about","dragons"],"labels":["O","O","O","O","B-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, trailer, rating, genre, year, title, song, director, plot, actor, review, character and O.\nSentence: find me movies about dragons","prompt_labels":"find(O) me(O) movies(O) about(O) dragons(B-plot)"}}
{"id":"126","dataset":"mit-movie","split":"train","label_list":["title","year","character","genre","actor","song","rating","plot","director","trailer","average ratings","review"],"instance":{"id":"126","words":["can","you","find","me","a","steven","spielberg","movie"],"labels":["O","O","O","O","O","B-director","I-director","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, year, character, genre, actor, song, rating, plot, director, trailer, average ratings, review and O.\nSentence: can you find me a steven spielberg movie","prompt_labels":"can(O) you(O) find(O) me(O) a(O) steven(B-director) spielberg(I-director) movie(O)"}}
{"id":"127","dataset":"mit-movie","split":"train","label_list":["average ratings","rating","trailer","year","review","director","genre","song","plot","actor","character","title"],"instance":{"id":"127","words":["did","george","clooney","direct","a","political","drama","movie"],"labels":["O","B-director","I-director","O","O","B-genre","I-genre","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, rating, trailer, year, review, director, genre, song, plot, actor, character, title and O.\nSentence: did george clooney direct a political drama movie","prompt_labels":"did(O) george(B-director) clooney(I-director) direct(O) a(O) political(B-genre) drama(I-genre) movie(O)"}}
{"id":"128","dataset":"mit-movie","split":"train","label_list":["actor","trailer","character","director","genre","plot","song","average ratings","title","rating","year","review"],"instance":{"id":"128","words":["show","me","musicals","starring","judy","garland"],"labels":["O","O","B-genre","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, trailer, character, director, genre, plot, song, average ratings, title, rating, year, review and O.\nSentence: show me musicals starring judy garland","prompt_labels":"show(O) me(O) musicals(B-genre) starring(O) judy(B-actor) garland(I-actor)"}}
{"id":"129","dataset":"mit-movie","split":"train","label_list":["year","title","director","trailer","plot","average ratings","song","review","rating","actor","character","genre"],"instance":{"id":"129","words":["what","was","the","first","movie","leonardo","dicaprio","starred","in"],"labels":["O","O","O","B-year","O","B-actor","I-actor","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, title, director, trailer, plot, average ratings, song, review, rating, actor, character, genre and O.\nSentence: what was the first movie leonardo dicaprio starred in","prompt_labels":"what(O) was(O) the(O) first(B-year) movie(O) leonardo(B-actor) dicaprio(I-actor) starred(O) in(O)"}}
{"id":"130","dataset":"mit-movie","split":"train","label_list":["song","character","title","director","rating","genre","year","average ratings","review","plot","actor","trailer"],"instance":{"id":"130","words":["are","there","any","childrens","movies","with","danny","devito"],"labels":["O","O","O","B-genre","O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, character, title, director, rating, genre, year, average ratings, review, plot, actor, trailer and O.\nSentence: are there any childrens movies with danny devito","prompt_labels":"are(O) there(O) any(O) childrens(B-genre) movies(O) with(O) danny(B-actor) devito(I-actor)"}}
{"id":"131","dataset":"mit-movie","split":"train","label_list":["rating","title","plot","review","genre","year","director","actor","character","song","trailer","average ratings"],"instance":{"id":"131","words":["movie","starring","john","krasinski"],"labels":["O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, title, plot, review, genre, year, director, actor, character, song, trailer, average ratings and O.\nSentence: movie starring john krasinski","prompt_labels":"movie(O) starring(O) john(B-actor) krasinski(I-actor)"}}
{"id":"132","dataset":"mit-movie","split":"train","label_list":["director","genre","song","rating","average ratings","review","plot","actor","trailer","character","year","title"],"instance":{"id":"132","words":["look","up","the","best","british","films"],"labels":["O","O","O","O","B-plot","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, genre, song, rating, average ratings, review, plot, actor, trailer, character, year, title and O.\nSentence: look up the best british films","prompt_labels":"look(O) up(O) the(O) best(O) british(B-plot) films(O)"}}
{"id":"133","dataset":"mit-movie","split":"train","label_list":["director","average ratings","plot","actor","trailer","rating","song","genre","review","year","character","title"],"instance":{"id":"133","words":["could","you","locate","a","mobster","film","starring","will","smith"],"labels":["O","O","O","O","B-plot","O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, average ratings, plot, actor, trailer, rating, song, genre, review, year, character, title and O.\nSentence: could you locate a mobster film starring will smith","prompt_labels":"could(O) you(O) locate(O) a(O) mobster(B-plot) film(O) starring(O) will(B-actor) smith(I-actor)"}}
{"id":"134","dataset":"mit-movie","split":"train","label_list":["trailer","title","review","song","director","character","rating","plot","actor","year","genre","average ratings"],"instance":{"id":"134","words":["how","many","friday","the","13th","movies","were","made","in","the","1980s"],"labels":["O","O","B-title","I-title","I-title","O","O","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, title, review, song, director, character, rating, plot, actor, year, genre, average ratings and O.\nSentence: how many friday the 13th movies were made in the 1980s","prompt_labels":"how(O) many(O) friday(B-title) the(I-title) 13th(I-title) movies(O) were(O) made(O) in(O) the(O) 1980s(B-year)"}}
{"id":"135","dataset":"mit-movie","split":"train","label_list":["actor","year","average ratings","character","review","title","plot","genre","rating","trailer","song","director"],"instance":{"id":"135","words":["i","loved","angelina","jolie","in","tomb","raider"],"labels":["O","O","B-actor","I-actor","O","B-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, year, average ratings, character, review, title, plot, genre, rating, trailer, song, director and O.\nSentence: i loved angelina jolie in tomb raider","prompt_labels":"i(O) loved(O) angelina(B-actor) jolie(I-actor) in(O) tomb(B-title) raider(I-title)"}}
{"id":"136","dataset":"mit-movie","split":"train","label_list":["review","director","plot","trailer","title","actor","rating","song","character","average ratings","year","genre"],"instance":{"id":"136","words":["list","the","movies","with","steve","carrel","and","ben","stiller"],"labels":["O","O","O","O","B-actor","I-actor","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, director, plot, trailer, title, actor, rating, song, character, average ratings, year, genre and O.\nSentence: list the movies with steve carrel and ben stiller","prompt_labels":"list(O) the(O) movies(O) with(O) steve(B-actor) carrel(I-actor) and(O) ben(B-actor) stiller(I-actor)"}}
{"id":"137","dataset":"mit-movie","split":"train","label_list":["character","actor","plot","average ratings","song","trailer","genre","year","review","director","title","rating"],"instance":{"id":"137","words":["who","stars","in","the","remake","of","the","movie","dirty","dancing"],"labels":["O","O","O","O","O","O","O","O","B-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, actor, plot, average ratings, song, trailer, genre, year, review, director, title, rating and O.\nSentence: who stars in the remake of the movie dirty dancing","prompt_labels":"who(O) stars(O) in(O) the(O) remake(O) of(O) the(O) movie(O) dirty(B-title) dancing(I-title)"}}
{"id":"138","dataset":"mit-movie","split":"train","label_list":["character","actor","director","average ratings","song","title","plot","review","year","genre","trailer","rating"],"instance":{"id":"138","words":["what","movies","are","similar","to","gnomeo","and","juliet"],"labels":["O","O","O","B-plot","O","B-title","I-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, actor, director, average ratings, song, title, plot, review, year, genre, trailer, rating and O.\nSentence: what movies are similar to gnomeo and juliet","prompt_labels":"what(O) movies(O) are(O) similar(B-plot) to(O) gnomeo(B-title) and(I-title) juliet(I-title)"}}
{"id":"139","dataset":"mit-movie","split":"train","label_list":["year","director","review","actor","character","average ratings","song","genre","trailer","rating","title","plot"],"instance":{"id":"139","words":["where","does","you","talking","to","me","come","from"],"labels":["O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, review, actor, character, average ratings, song, genre, trailer, rating, title, plot and O.\nSentence: where does you talking to me come from","prompt_labels":"where(O) does(O) you(O) talking(O) to(O) me(O) come(O) from(O)"}}
{"id":"140","dataset":"mit-movie","split":"train","label_list":["rating","plot","character","director","actor","year","song","review","trailer","average ratings","title","genre"],"instance":{"id":"140","words":["did","bill","murray","star","in","any","action","movies"],"labels":["O","B-actor","I-actor","O","O","O","B-genre","I-genre"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, plot, character, director, actor, year, song, review, trailer, average ratings, title, genre and O.\nSentence: did bill murray star in any action movies","prompt_labels":"did(O) bill(B-actor) murray(I-actor) star(O) in(O) any(O) action(B-genre) movies(I-genre)"}}
{"id":"141","dataset":"mit-movie","split":"train","label_list":["song","character","plot","title","average ratings","trailer","director","rating","review","genre","year","actor"],"instance":{"id":"141","words":["what","was","the","first","movie","martin","scorsese","directed"],"labels":["O","O","O","O","O","B-director","I-director","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, character, plot, title, average ratings, trailer, director, rating, review, genre, year, actor and O.\nSentence: what was the first movie martin scorsese directed","prompt_labels":"what(O) was(O) the(O) first(O) movie(O) martin(B-director) scorsese(I-director) directed(O)"}}
{"id":"142","dataset":"mit-movie","split":"train","label_list":["trailer","song","average ratings","actor","genre","character","plot","review","director","title","rating","year"],"instance":{"id":"142","words":["do","you","have","any","documentaries","about","pollution","in","china"],"labels":["O","O","O","O","B-genre","O","B-plot","I-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, song, average ratings, actor, genre, character, plot, review, director, title, rating, year and O.\nSentence: do you have any documentaries about pollution in china","prompt_labels":"do(O) you(O) have(O) any(O) documentaries(B-genre) about(O) pollution(B-plot) in(I-plot) china(I-plot)"}}
{"id":"143","dataset":"mit-movie","split":"train","label_list":["average ratings","trailer","song","review","actor","rating","title","year","genre","director","plot","character"],"instance":{"id":"143","words":["what","movie","has","the","most","famous","soundtrack"],"labels":["O","O","O","O","B-review","I-review","B-song"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, trailer, song, review, actor, rating, title, year, genre, director, plot, character and O.\nSentence: what movie has the most famous soundtrack","prompt_labels":"what(O) movie(O) has(O) the(O) most(B-review) famous(I-review) soundtrack(B-song)"}}
{"id":"144","dataset":"mit-movie","split":"train","label_list":["song","average ratings","character","title","year","trailer","rating","director","genre","actor","review","plot"],"instance":{"id":"144","words":["what","was","the","name","of","the","film","starring","cary","grant","and","eva","marie","saint"],"labels":["O","O","O","O","O","O","O","O","B-actor","I-actor","O","B-actor","I-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, average ratings, character, title, year, trailer, rating, director, genre, actor, review, plot and O.\nSentence: what was the name of the film starring cary grant and eva marie saint","prompt_labels":"what(O) was(O) the(O) name(O) of(O) the(O) film(O) starring(O) cary(B-actor) grant(I-actor) and(O) eva(B-actor) marie(I-actor) saint(I-actor)"}}
{"id":"145","dataset":"mit-movie","split":"train","label_list":["year","plot","song","character","genre","trailer","title","actor","review","average ratings","director","rating"],"instance":{"id":"145","words":["who","has","top","billing","for","backdraft"],"labels":["O","O","O","O","O","B-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, plot, song, character, genre, trailer, title, actor, review, average ratings, director, rating and O.\nSentence: who has top billing for backdraft","prompt_labels":"who(O) has(O) top(O) billing(O) for(O) backdraft(B-title)"}}
{"id":"146","dataset":"mit-movie","split":"train","label_list":["trailer","actor","song","year","review","character","plot","average ratings","director","genre","rating","title"],"instance":{"id":"146","words":["what","was","the","theme","song","to","8","mile"],"labels":["O","O","O","B-song","I-song","O","B-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, actor, song, year, review, character, plot, average ratings, director, genre, rating, title and O.\nSentence: what was the theme song to 8 mile","prompt_labels":"what(O) was(O) the(O) theme(B-song) song(I-song) to(O) 8(B-title) mile(I-title)"}}
{"id":"147","dataset":"mit-movie","split":"train","label_list":["title","average ratings","genre","review","year","song","plot","character","director","actor","trailer","rating"],"instance":{"id":"147","words":["show","me","drama","movies","with","america","fererra"],"labels":["O","O","B-genre","O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, average ratings, genre, review, year, song, plot, character, director, actor, trailer, rating and O.\nSentence: show me drama movies with america fererra","prompt_labels":"show(O) me(O) drama(B-genre) movies(O) with(O) america(B-actor) fererra(I-actor)"}}
{"id":"148","dataset":"mit-movie","split":"train","label_list":["review","plot","average ratings","actor","song","title","rating","character","genre","year","director","trailer"],"instance":{"id":"148","words":["are","there","any","movies","with","susan","sarandon","and","jenna","elfman"],"labels":["O","O","O","O","O","B-actor","I-actor","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, average ratings, actor, song, title, rating, character, genre, year, director, trailer and O.\nSentence: are there any movies with susan sarandon and jenna elfman","prompt_labels":"are(O) there(O) any(O) movies(O) with(O) susan(B-actor) sarandon(I-actor) and(O) jenna(B-actor) elfman(I-actor)"}}
{"id":"149","dataset":"mit-movie","split":"train","label_list":["director","actor","plot","review","average ratings","character","title","year","trailer","genre","song","rating"],"instance":{"id":"149","words":["show","me","musicals","starring","julie","andrews","that","were","rated","five","stars"],"labels":["O","O","B-genre","O","B-actor","I-actor","O","O","B-average ratings","I-average ratings","I-average ratings"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, plot, review, average ratings, character, title, year, trailer, genre, song, rating and O.\nSentence: show me musicals starring julie andrews that were rated five stars","prompt_labels":"show(O) me(O) musicals(B-genre) starring(O) julie(B-actor) andrews(I-actor) that(O) were(O) rated(B-average ratings) five(I-average ratings) stars(I-average ratings)"}}
{"id":"150","dataset":"mit-movie","split":"train","label_list":["genre","plot","trailer","actor","song","year","character","director","title","review","rating","average ratings"],"instance":{"id":"150","words":["find","the","name","of","the","actor","who","play","danny","in","grease"],"labels":["O","O","O","O","O","B-actor","O","O","B-character","O","B-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, plot, trailer, actor, song, year, character, director, title, review, rating, average ratings and O.\nSentence: find the name of the actor who play danny in grease","prompt_labels":"find(O) the(O) name(O) of(O) the(O) actor(B-actor) who(O) play(O) danny(B-character) in(O) grease(B-title)"}}
{"id":"151","dataset":"mit-movie","split":"train","label_list":["rating","title","trailer","character","actor","average ratings","song","director","review","plot","year","genre"],"instance":{"id":"151","words":["did","russel","crowe","star","in","a","movie","about","a","mathematician","released","in","2001"],"labels":["O","B-actor","I-actor","O","O","O","O","O","O","B-plot","O","O","B-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, title, trailer, character, actor, average ratings, song, director, review, plot, year, genre and O.\nSentence: did russel crowe star in a movie about a mathematician released in 2001","prompt_labels":"did(O) russel(B-actor) crowe(I-actor) star(O) in(O) a(O) movie(O) about(O) a(O) mathematician(B-plot) released(O) in(O) 2001(B-plot)"}}
{"id":"152","dataset":"mit-movie","split":"train","label_list":["year","genre","plot","actor","title","character","song","review","average ratings","rating","director","trailer"],"instance":{"id":"152","words":["who","played","ron","weasley","in","harry","potter"],"labels":["O","O","B-character","I-character","O","B-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, genre, plot, actor, title, character, song, review, average ratings, rating, director, trailer and O.\nSentence: who played ron weasley in harry potter","prompt_labels":"who(O) played(O) ron(B-character) weasley(I-character) in(O) harry(B-title) potter(I-title)"}}
{"id":"153","dataset":"mit-movie","split":"train","label_list":["director","rating","year","title","song","review","character","plot","trailer","average ratings","genre","actor"],"instance":{"id":"153","words":["movie","about","two","groups","of","robbers","robbing","the","same","bank"],"labels":["O","O","B-plot","I-plot","I-plot","I-plot","I-plot","I-plot","I-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, rating, year, title, song, review, character, plot, trailer, average ratings, genre, actor and O.\nSentence: movie about two groups of robbers robbing the same bank","prompt_labels":"movie(O) about(O) two(B-plot) groups(I-plot) of(I-plot) robbers(I-plot) robbing(I-plot) the(I-plot) same(I-plot) bank(I-plot)"}}
{"id":"154","dataset":"mit-movie","split":"train","label_list":["rating","director","year","character","review","plot","genre","average ratings","song","trailer","title","actor"],"instance":{"id":"154","words":["are","there","any","new","movies","from","2011","directed","by","steven","spielberg"],"labels":["O","O","O","O","O","O","B-year","O","O","B-director","I-director"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, year, character, review, plot, genre, average ratings, song, trailer, title, actor and O.\nSentence: are there any new movies from 2011 directed by steven spielberg","prompt_labels":"are(O) there(O) any(O) new(O) movies(O) from(O) 2011(B-year) directed(O) by(O) steven(B-director) spielberg(I-director)"}}
{"id":"155","dataset":"mit-movie","split":"train","label_list":["title","actor","song","character","plot","rating","review","year","genre","average ratings","director","trailer"],"instance":{"id":"155","words":["whats","the","movie","with","a","song","by","coolio"],"labels":["O","O","O","O","O","B-song","I-song","I-song"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, actor, song, character, plot, rating, review, year, genre, average ratings, director, trailer and O.\nSentence: whats the movie with a song by coolio","prompt_labels":"whats(O) the(O) movie(O) with(O) a(O) song(B-song) by(I-song) coolio(I-song)"}}
{"id":"156","dataset":"mit-movie","split":"train","label_list":["actor","title","director","review","plot","average ratings","genre","rating","trailer","year","character","song"],"instance":{"id":"156","words":["funniest","trailer","for","new","movie"],"labels":["B-review","B-trailer","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, title, director, review, plot, average ratings, genre, rating, trailer, year, character, song and O.\nSentence: funniest trailer for new movie","prompt_labels":"funniest(B-review) trailer(B-trailer) for(O) new(O) movie(O)"}}
{"id":"157","dataset":"mit-movie","split":"train","label_list":["year","actor","plot","genre","review","director","trailer","song","character","title","rating","average ratings"],"instance":{"id":"157","words":["show","me","robert","deniro","movies","about","mobsters"],"labels":["O","O","B-actor","I-actor","O","O","B-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, actor, plot, genre, review, director, trailer, song, character, title, rating, average ratings and O.\nSentence: show me robert deniro movies about mobsters","prompt_labels":"show(O) me(O) robert(B-actor) deniro(I-actor) movies(O) about(O) mobsters(B-plot)"}}
{"id":"158","dataset":"mit-movie","split":"train","label_list":["average ratings","year","director","genre","rating","song","title","trailer","character","review","actor","plot"],"instance":{"id":"158","words":["what","are","recent","clint","eastwood","movies"],"labels":["O","O","O","B-actor","I-actor","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, year, director, genre, rating, song, title, trailer, character, review, actor, plot and O.\nSentence: what are recent clint eastwood movies","prompt_labels":"what(O) are(O) recent(O) clint(B-actor) eastwood(I-actor) movies(O)"}}
{"id":"159","dataset":"mit-movie","split":"train","label_list":["plot","trailer","genre","average ratings","director","review","song","character","title","rating","actor","year"],"instance":{"id":"159","words":["how","many","rocky","movies","are","out"],"labels":["O","O","B-title","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, trailer, genre, average ratings, director, review, song, character, title, rating, actor, year and O.\nSentence: how many rocky movies are out","prompt_labels":"how(O) many(O) rocky(B-title) movies(O) are(O) out(O)"}}
{"id":"160","dataset":"mit-movie","split":"train","label_list":["character","rating","genre","plot","year","average ratings","director","review","actor","trailer","song","title"],"instance":{"id":"160","words":["who","is","the","main","character","of","a","bronx","tale"],"labels":["O","O","O","B-character","O","O","B-title","I-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, rating, genre, plot, year, average ratings, director, review, actor, trailer, song, title and O.\nSentence: who is the main character of a bronx tale","prompt_labels":"who(O) is(O) the(O) main(B-character) character(O) of(O) a(B-title) bronx(I-title) tale(I-title)"}}
{"id":"161","dataset":"mit-movie","split":"train","label_list":["character","rating","review","year","actor","genre","director","plot","trailer","song","title","average ratings"],"instance":{"id":"161","words":["is","there","a","sequel","to","the","movie","speed"],"labels":["O","O","O","B-title","O","O","O","B-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, rating, review, year, actor, genre, director, plot, trailer, song, title, average ratings and O.\nSentence: is there a sequel to the movie speed","prompt_labels":"is(O) there(O) a(O) sequel(B-title) to(O) the(O) movie(O) speed(B-title)"}}
{"id":"162","dataset":"mit-movie","split":"train","label_list":["review","trailer","character","year","song","genre","average ratings","plot","actor","director","rating","title"],"instance":{"id":"162","words":["are","there","any","movies","directed","by","anthony","minghella","starring","matt","damon"],"labels":["O","O","O","O","O","O","B-director","I-director","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, trailer, character, year, song, genre, average ratings, plot, actor, director, rating, title and O.\nSentence: are there any movies directed by anthony minghella starring matt damon","prompt_labels":"are(O) there(O) any(O) movies(O) directed(O) by(O) anthony(B-director) minghella(I-director) starring(O) matt(B-actor) damon(I-actor)"}}
{"id":"163","dataset":"mit-movie","split":"train","label_list":["average ratings","title","plot","rating","review","year","song","director","actor","trailer","genre","character"],"instance":{"id":"163","words":["list","the","costume","drama","movies","directed","by","milos","forman"],"labels":["O","O","B-genre","I-genre","O","O","O","B-director","I-director"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, title, plot, rating, review, year, song, director, actor, trailer, genre, character and O.\nSentence: list the costume drama movies directed by milos forman","prompt_labels":"list(O) the(O) costume(B-genre) drama(I-genre) movies(O) directed(O) by(O) milos(B-director) forman(I-director)"}}
{"id":"164","dataset":"mit-movie","split":"train","label_list":["review","character","year","director","genre","trailer","title","actor","plot","rating","song","average ratings"],"instance":{"id":"164","words":["show","me","a","movie","about","life","in","the","1980s"],"labels":["O","O","O","O","O","B-plot","I-plot","I-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, character, year, director, genre, trailer, title, actor, plot, rating, song, average ratings and O.\nSentence: show me a movie about life in the 1980s","prompt_labels":"show(O) me(O) a(O) movie(O) about(O) life(B-plot) in(I-plot) the(I-plot) 1980s(I-plot)"}}
{"id":"165","dataset":"mit-movie","split":"train","label_list":["genre","average ratings","rating","year","plot","title","character","actor","review","song","trailer","director"],"instance":{"id":"165","words":["which","movie","did","brad","pitt","and","angelina","jolie","star","in","together"],"labels":["O","O","O","B-actor","I-actor","O","B-actor","I-actor","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, average ratings, rating, year, plot, title, character, actor, review, song, trailer, director and O.\nSentence: which movie did brad pitt and angelina jolie star in together","prompt_labels":"which(O) movie(O) did(O) brad(B-actor) pitt(I-actor) and(O) angelina(B-actor) jolie(I-actor) star(O) in(O) together(O)"}}
{"id":"166","dataset":"mit-movie","split":"train","label_list":["review","actor","song","rating","title","plot","trailer","genre","director","character","average ratings","year"],"instance":{"id":"166","words":["when","does","the","new","hobbit","movie","come","out"],"labels":["O","O","O","O","B-plot","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, actor, song, rating, title, plot, trailer, genre, director, character, average ratings, year and O.\nSentence: when does the new hobbit movie come out","prompt_labels":"when(O) does(O) the(O) new(O) hobbit(B-plot) movie(O) come(O) out(O)"}}
{"id":"167","dataset":"mit-movie","split":"train","label_list":["rating","average ratings","genre","actor","trailer","character","title","director","year","song","review","plot"],"instance":{"id":"167","words":["what","movies","were","directed","by","woody","allen","in","the","1980s"],"labels":["O","O","O","O","O","B-director","I-director","O","O","B-average ratings"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, average ratings, genre, actor, trailer, character, title, director, year, song, review, plot and O.\nSentence: what movies were directed by woody allen in the 1980s","prompt_labels":"what(O) movies(O) were(O) directed(O) by(O) woody(B-director) allen(I-director) in(O) the(O) 1980s(B-average ratings)"}}
{"id":"168","dataset":"mit-movie","split":"train","label_list":["genre","rating","title","actor","review","song","plot","director","year","character","trailer","average ratings"],"instance":{"id":"168","words":["do","other","people","think","that","godfather","iii","was","just","terrible"],"labels":["O","O","O","O","O","B-title","I-title","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, rating, title, actor, review, song, plot, director, year, character, trailer, average ratings and O.\nSentence: do other people think that godfather iii was just terrible","prompt_labels":"do(O) other(O) people(O) think(O) that(O) godfather(B-title) iii(I-title) was(O) just(O) terrible(O)"}}
{"id":"169","dataset":"mit-movie","split":"train","label_list":["genre","trailer","title","rating","director","plot","average ratings","review","character","actor","year","song"],"instance":{"id":"169","words":["what","is","the","highest","rated","kids","new","release","involving","cartoons"],"labels":["O","O","O","B-average ratings","I-average ratings","B-genre","O","O","O","B-genre"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, trailer, title, rating, director, plot, average ratings, review, character, actor, year, song and O.\nSentence: what is the highest rated kids new release involving cartoons","prompt_labels":"what(O) is(O) the(O) highest(B-average ratings) rated(I-average ratings) kids(B-genre) new(O) release(O) involving(O) cartoons(B-genre)"}}
{"id":"170","dataset":"mit-movie","split":"train","label_list":["plot","title","trailer","actor","genre","rating","director","song","review","year","average ratings","character"],"instance":{"id":"170","words":["list","war","movies","about","world","war","ii","released","in","the","1990s"],"labels":["O","B-plot","O","O","B-plot","I-plot","I-plot","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, title, trailer, actor, genre, rating, director, song, review, year, average ratings, character and O.\nSentence: list war movies about world war ii released in the 1990s","prompt_labels":"list(O) war(B-plot) movies(O) about(O) world(B-plot) war(I-plot) ii(I-plot) released(O) in(O) the(O) 1990s(B-year)"}}
{"id":"171","dataset":"mit-movie","split":"train","label_list":["title","average ratings","actor","genre","review","plot","year","trailer","director","character","rating","song"],"instance":{"id":"171","words":["get","me","all","biography","movies","where","a","real","us","president","was","portrayed"],"labels":["O","O","O","B-genre","O","O","O","O","B-plot","I-plot","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, average ratings, actor, genre, review, plot, year, trailer, director, character, rating, song and O.\nSentence: get me all biography movies where a real us president was portrayed","prompt_labels":"get(O) me(O) all(O) biography(B-genre) movies(O) where(O) a(O) real(O) us(B-plot) president(I-plot) was(O) portrayed(O)"}}
{"id":"172","dataset":"mit-movie","split":"train","label_list":["director","song","title","average ratings","character","trailer","actor","genre","year","review","rating","plot"],"instance":{"id":"172","words":["what","movies","have","funny","wedding","toasts"],"labels":["O","O","O","B-genre","B-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, song, title, average ratings, character, trailer, actor, genre, year, review, rating, plot and O.\nSentence: what movies have funny wedding toasts","prompt_labels":"what(O) movies(O) have(O) funny(B-genre) wedding(B-plot) toasts(I-plot)"}}
{"id":"173","dataset":"mit-movie","split":"train","label_list":["song","rating","genre","director","year","average ratings","review","actor","character","plot","trailer","title"],"instance":{"id":"173","words":["what","movie","was","britney","spears","in","where","she","sang","satisfaction"],"labels":["O","O","O","B-actor","I-actor","O","O","O","O","B-song"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, rating, genre, director, year, average ratings, review, actor, character, plot, trailer, title and O.\nSentence: what movie was britney spears in where she sang satisfaction","prompt_labels":"what(O) movie(O) was(O) britney(B-actor) spears(I-actor) in(O) where(O) she(O) sang(O) satisfaction(B-song)"}}
{"id":"174","dataset":"mit-movie","split":"train","label_list":["year","genre","plot","review","director","actor","rating","title","song","average ratings","trailer","character"],"instance":{"id":"174","words":["find","movies","set","in","chicago"],"labels":["O","O","O","O","B-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, genre, plot, review, director, actor, rating, title, song, average ratings, trailer, character and O.\nSentence: find movies set in chicago","prompt_labels":"find(O) movies(O) set(O) in(O) chicago(B-plot)"}}
{"id":"175","dataset":"mit-movie","split":"train","label_list":["rating","review","plot","genre","director","trailer","character","title","year","average ratings","actor","song"],"instance":{"id":"175","words":["who","directed","indiana","jones"],"labels":["O","O","B-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, review, plot, genre, director, trailer, character, title, year, average ratings, actor, song and O.\nSentence: who directed indiana jones","prompt_labels":"who(O) directed(O) indiana(B-title) jones(I-title)"}}
{"id":"176","dataset":"mit-movie","split":"train","label_list":["character","rating","song","plot","average ratings","year","title","trailer","review","director","actor","genre"],"instance":{"id":"176","words":["show","me","all","horror","films","from","the","2000s"],"labels":["O","O","O","B-genre","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, rating, song, plot, average ratings, year, title, trailer, review, director, actor, genre and O.\nSentence: show me all horror films from the 2000s","prompt_labels":"show(O) me(O) all(O) horror(B-genre) films(O) from(O) the(O) 2000s(B-year)"}}
{"id":"177","dataset":"mit-movie","split":"train","label_list":["year","plot","title","genre","character","rating","trailer","review","actor","average ratings","song","director"],"instance":{"id":"177","words":["find","the","judd","apatow","movies","released","in","the","2000s"],"labels":["O","O","B-director","I-director","O","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, plot, title, genre, character, rating, trailer, review, actor, average ratings, song, director and O.\nSentence: find the judd apatow movies released in the 2000s","prompt_labels":"find(O) the(O) judd(B-director) apatow(I-director) movies(O) released(O) in(O) the(O) 2000s(B-year)"}}
{"id":"178","dataset":"mit-movie","split":"train","label_list":["trailer","character","director","year","title","average ratings","actor","song","rating","review","plot","genre"],"instance":{"id":"178","words":["what","pg","13","movies","are","there","with","ghosts"],"labels":["O","B-rating","I-rating","O","O","O","O","B-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, character, director, year, title, average ratings, actor, song, rating, review, plot, genre and O.\nSentence: what pg 13 movies are there with ghosts","prompt_labels":"what(O) pg(B-rating) 13(I-rating) movies(O) are(O) there(O) with(O) ghosts(B-plot)"}}
{"id":"179","dataset":"mit-movie","split":"train","label_list":["year","director","average ratings","plot","rating","trailer","title","review","character","actor","genre","song"],"instance":{"id":"179","words":["do","most","people","think","that","godfather","iii","was","terrible"],"labels":["O","O","O","O","O","B-title","I-title","O","B-rating"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, average ratings, plot, rating, trailer, title, review, character, actor, genre, song and O.\nSentence: do most people think that godfather iii was terrible","prompt_labels":"do(O) most(O) people(O) think(O) that(O) godfather(B-title) iii(I-title) was(O) terrible(B-rating)"}}
{"id":"180","dataset":"mit-movie","split":"train","label_list":["character","genre","song","review","rating","average ratings","title","actor","trailer","year","plot","director"],"instance":{"id":"180","words":["find","me","movies","since","2002","about","a","dog"],"labels":["O","O","O","B-year","I-year","O","O","B-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, genre, song, review, rating, average ratings, title, actor, trailer, year, plot, director and O.\nSentence: find me movies since 2002 about a dog","prompt_labels":"find(O) me(O) movies(O) since(B-year) 2002(I-year) about(O) a(O) dog(B-plot)"}}
{"id":"181","dataset":"mit-movie","split":"train","label_list":["review","plot","character","genre","song","year","director","title","average ratings","actor","rating","trailer"],"instance":{"id":"181","words":["find","car","chase","movie","with","jason","statum","in","it"],"labels":["O","B-plot","I-plot","O","O","B-actor","I-actor","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, character, genre, song, year, director, title, average ratings, actor, rating, trailer and O.\nSentence: find car chase movie with jason statum in it","prompt_labels":"find(O) car(B-plot) chase(I-plot) movie(O) with(O) jason(B-actor) statum(I-actor) in(O) it(O)"}}
{"id":"182","dataset":"mit-movie","split":"train","label_list":["rating","director","plot","genre","year","title","trailer","character","actor","song","review","average ratings"],"instance":{"id":"182","words":["what","year","did","godfather","1","come","out"],"labels":["O","B-year","O","B-title","I-title","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, plot, genre, year, title, trailer, character, actor, song, review, average ratings and O.\nSentence: what year did godfather 1 come out","prompt_labels":"what(O) year(B-year) did(O) godfather(B-title) 1(I-title) come(O) out(O)"}}
{"id":"183","dataset":"mit-movie","split":"train","label_list":["character","review","title","year","genre","trailer","average ratings","plot","rating","actor","director","song"],"instance":{"id":"183","words":["what","movies","did","nora","ephron","write"],"labels":["O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, review, title, year, genre, trailer, average ratings, plot, rating, actor, director, song and O.\nSentence: what movies did nora ephron write","prompt_labels":"what(O) movies(O) did(O) nora(O) ephron(O) write(O)"}}
{"id":"184","dataset":"mit-movie","split":"train","label_list":["character","title","plot","actor","song","trailer","rating","director","genre","year","review","average ratings"],"instance":{"id":"184","words":["what","was","steven","spielbergs","last","award","winning","movie"],"labels":["O","O","B-director","I-director","O","B-average ratings","I-average ratings","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, title, plot, actor, song, trailer, rating, director, genre, year, review, average ratings and O.\nSentence: what was steven spielbergs last award winning movie","prompt_labels":"what(O) was(O) steven(B-director) spielbergs(I-director) last(O) award(B-average ratings) winning(I-average ratings) movie(O)"}}
{"id":"185","dataset":"mit-movie","split":"train","label_list":["review","song","title","character","rating","plot","year","genre","director","actor","average ratings","trailer"],"instance":{"id":"185","words":["im","looking","for","the","well","known","film","made","by","julia","roberts","in","1993"],"labels":["O","O","O","O","B-review","I-review","O","O","O","B-actor","I-actor","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, title, character, rating, plot, year, genre, director, actor, average ratings, trailer and O.\nSentence: im looking for the well known film made by julia roberts in 1993","prompt_labels":"im(O) looking(O) for(O) the(O) well(B-review) known(I-review) film(O) made(O) by(O) julia(B-actor) roberts(I-actor) in(O) 1993(B-year)"}}
{"id":"186","dataset":"mit-movie","split":"train","label_list":["plot","genre","trailer","year","actor","song","character","review","average ratings","rating","title","director"],"instance":{"id":"186","words":["what","is","the","best","movie","of","2012"],"labels":["O","O","O","B-review","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, genre, trailer, year, actor, song, character, review, average ratings, rating, title, director and O.\nSentence: what is the best movie of 2012","prompt_labels":"what(O) is(O) the(O) best(B-review) movie(O) of(O) 2012(B-year)"}}
{"id":"187","dataset":"mit-movie","split":"train","label_list":["genre","song","character","actor","rating","plot","average ratings","trailer","review","year","title","director"],"instance":{"id":"187","words":["im","looking","for","an","awful","movie","about","teenagers","lost","in","the","woods"],"labels":["O","O","O","O","B-review","I-review","O","B-plot","I-plot","I-plot","I-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, song, character, actor, rating, plot, average ratings, trailer, review, year, title, director and O.\nSentence: im looking for an awful movie about teenagers lost in the woods","prompt_labels":"im(O) looking(O) for(O) an(O) awful(B-review) movie(I-review) about(O) teenagers(B-plot) lost(I-plot) in(I-plot) the(I-plot) woods(I-plot)"}}
{"id":"188","dataset":"mit-movie","split":"train","label_list":["review","character","year","title","song","genre","average ratings","rating","trailer","director","plot","actor"],"instance":{"id":"188","words":["which","film","was","it","that","contained","the","line","phone","home"],"labels":["O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, character, year, title, song, genre, average ratings, rating, trailer, director, plot, actor and O.\nSentence: which film was it that contained the line phone home","prompt_labels":"which(O) film(O) was(O) it(O) that(O) contained(O) the(O) line(O) phone(O) home(O)"}}
{"id":"189","dataset":"mit-movie","split":"train","label_list":["year","rating","song","title","average ratings","genre","review","character","plot","director","trailer","actor"],"instance":{"id":"189","words":["do","you","have","any","police","dramas","rated","pg13"],"labels":["O","O","O","O","B-plot","B-genre","O","B-rating"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, rating, song, title, average ratings, genre, review, character, plot, director, trailer, actor and O.\nSentence: do you have any police dramas rated pg13","prompt_labels":"do(O) you(O) have(O) any(O) police(B-plot) dramas(B-genre) rated(O) pg13(B-rating)"}}
{"id":"190","dataset":"mit-movie","split":"train","label_list":["year","review","song","character","director","plot","trailer","actor","average ratings","genre","rating","title"],"instance":{"id":"190","words":["what","are","the","movies","darren","aranofsky","has","directed"],"labels":["O","O","O","O","B-director","I-director","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, review, song, character, director, plot, trailer, actor, average ratings, genre, rating, title and O.\nSentence: what are the movies darren aranofsky has directed","prompt_labels":"what(O) are(O) the(O) movies(O) darren(B-director) aranofsky(I-director) has(O) directed(O)"}}
{"id":"191","dataset":"mit-movie","split":"train","label_list":["title","plot","genre","song","review","rating","trailer","average ratings","actor","director","character","year"],"instance":{"id":"191","words":["show","me","the","best","reviews","of","2011"],"labels":["O","O","O","B-average ratings","I-average ratings","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, plot, genre, song, review, rating, trailer, average ratings, actor, director, character, year and O.\nSentence: show me the best reviews of 2011","prompt_labels":"show(O) me(O) the(O) best(B-average ratings) reviews(I-average ratings) of(O) 2011(B-year)"}}
{"id":"192","dataset":"mit-movie","split":"train","label_list":["plot","song","average ratings","title","rating","review","genre","actor","trailer","character","director","year"],"instance":{"id":"192","words":["which","movies","are","similar","to","gnomeo","and","juliet"],"labels":["O","O","O","B-review","O","B-title","I-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, song, average ratings, title, rating, review, genre, actor, trailer, character, director, year and O.\nSentence: which movies are similar to gnomeo and juliet","prompt_labels":"which(O) movies(O) are(O) similar(B-review) to(O) gnomeo(B-title) and(I-title) juliet(I-title)"}}
{"id":"193","dataset":"mit-movie","split":"train","label_list":["actor","character","plot","director","rating","review","title","trailer","genre","average ratings","year","song"],"instance":{"id":"193","words":["how","many","james","bond","movies","were","made","before","the","year","2000"],"labels":["O","O","B-character","I-character","O","O","O","B-year","I-year","I-year","I-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, character, plot, director, rating, review, title, trailer, genre, average ratings, year, song and O.\nSentence: how many james bond movies were made before the year 2000","prompt_labels":"how(O) many(O) james(B-character) bond(I-character) movies(O) were(O) made(O) before(B-year) the(I-year) year(I-year) 2000(I-year)"}}
{"id":"194","dataset":"mit-movie","split":"train","label_list":["genre","director","plot","actor","song","average ratings","trailer","title","year","review","rating","character"],"instance":{"id":"194","words":["are","there","any","g","rated","movies","with","a","giraffe"],"labels":["O","O","O","B-rating","O","O","O","O","B-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, director, plot, actor, song, average ratings, trailer, title, year, review, rating, character and O.\nSentence: are there any g rated movies with a giraffe","prompt_labels":"are(O) there(O) any(O) g(B-rating) rated(O) movies(O) with(O) a(O) giraffe(B-plot)"}}
{"id":"195","dataset":"mit-movie","split":"train","label_list":["actor","character","average ratings","director","title","plot","trailer","genre","rating","year","song","review"],"instance":{"id":"195","words":["can","you","get","me","a","documentary","directed","by","michael","moore","about","health","care"],"labels":["O","O","O","O","O","B-genre","O","O","B-director","I-director","O","B-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, character, average ratings, director, title, plot, trailer, genre, rating, year, song, review and O.\nSentence: can you get me a documentary directed by michael moore about health care","prompt_labels":"can(O) you(O) get(O) me(O) a(O) documentary(B-genre) directed(O) by(O) michael(B-director) moore(I-director) about(O) health(B-plot) care(I-plot)"}}
{"id":"196","dataset":"mit-movie","split":"train","label_list":["trailer","plot","director","review","title","song","rating","genre","actor","character","average ratings","year"],"instance":{"id":"196","words":["show","me","the","lists","of","braddock","films"],"labels":["O","O","O","O","O","B-director","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, plot, director, review, title, song, rating, genre, actor, character, average ratings, year and O.\nSentence: show me the lists of braddock films","prompt_labels":"show(O) me(O) the(O) lists(O) of(O) braddock(B-director) films(O)"}}
{"id":"197","dataset":"mit-movie","split":"train","label_list":["review","song","year","actor","rating","director","title","genre","plot","character","average ratings","trailer"],"instance":{"id":"197","words":["which","movie","includes","your","touch","by","the","black","keys"],"labels":["O","O","O","B-song","I-song","O","O","B-song","I-song"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, year, actor, rating, director, title, genre, plot, character, average ratings, trailer and O.\nSentence: which movie includes your touch by the black keys","prompt_labels":"which(O) movie(O) includes(O) your(B-song) touch(I-song) by(O) the(O) black(B-song) keys(I-song)"}}
{"id":"198","dataset":"mit-movie","split":"train","label_list":["director","character","actor","rating","plot","year","average ratings","genre","title","trailer","review","song"],"instance":{"id":"198","words":["what","movies","is","considered","the","worst","by","critics"],"labels":["O","O","O","O","O","B-average ratings","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, character, actor, rating, plot, year, average ratings, genre, title, trailer, review, song and O.\nSentence: what movies is considered the worst by critics","prompt_labels":"what(O) movies(O) is(O) considered(O) the(O) worst(B-average ratings) by(O) critics(O)"}}
{"id":"199","dataset":"mit-movie","split":"train","label_list":["rating","director","trailer","plot","actor","character","title","year","average ratings","genre","review","song"],"instance":{"id":"199","words":["show","me","non","disney","cartoons","with","a","g","rating"],"labels":["O","O","B-plot","I-plot","B-genre","O","O","B-rating","I-rating"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, trailer, plot, actor, character, title, year, average ratings, genre, review, song and O.\nSentence: show me non disney cartoons with a g rating","prompt_labels":"show(O) me(O) non(B-plot) disney(I-plot) cartoons(B-genre) with(O) a(O) g(B-rating) rating(I-rating)"}}
{"id":"200","dataset":"mit-movie","split":"train","label_list":["rating","actor","title","song","plot","average ratings","trailer","director","review","character","year","genre"],"instance":{"id":"200","words":["when","is","the","next","star","wars","movie","coming","out"],"labels":["O","O","O","O","B-title","I-title","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, actor, title, song, plot, average ratings, trailer, director, review, character, year, genre and O.\nSentence: when is the next star wars movie coming out","prompt_labels":"when(O) is(O) the(O) next(O) star(B-title) wars(I-title) movie(O) coming(O) out(O)"}}
{"id":"201","dataset":"mit-movie","split":"train","label_list":["song","rating","actor","review","character","director","average ratings","plot","genre","trailer","year","title"],"instance":{"id":"201","words":["courtney","foxx","looks","great","in","this","movie"],"labels":["B-actor","I-actor","B-plot","I-plot","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, rating, actor, review, character, director, average ratings, plot, genre, trailer, year, title and O.\nSentence: courtney foxx looks great in this movie","prompt_labels":"courtney(B-actor) foxx(I-actor) looks(B-plot) great(I-plot) in(O) this(O) movie(O)"}}
{"id":"202","dataset":"mit-movie","split":"train","label_list":["review","plot","year","average ratings","rating","genre","actor","title","song","director","character","trailer"],"instance":{"id":"202","words":["show","me","films","set","in","world","war","i","released","in","the","1970s"],"labels":["O","O","O","O","O","B-plot","I-plot","I-plot","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, year, average ratings, rating, genre, actor, title, song, director, character, trailer and O.\nSentence: show me films set in world war i released in the 1970s","prompt_labels":"show(O) me(O) films(O) set(O) in(O) world(B-plot) war(I-plot) i(I-plot) released(O) in(O) the(O) 1970s(B-year)"}}
{"id":"203","dataset":"mit-movie","split":"train","label_list":["year","character","rating","actor","genre","director","review","plot","average ratings","song","trailer","title"],"instance":{"id":"203","words":["who","directed","the","shining"],"labels":["O","B-director","B-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, character, rating, actor, genre, director, review, plot, average ratings, song, trailer, title and O.\nSentence: who directed the shining","prompt_labels":"who(O) directed(B-director) the(B-title) shining(I-title)"}}
{"id":"204","dataset":"mit-movie","split":"train","label_list":["director","genre","title","song","average ratings","character","year","actor","rating","plot","review","trailer"],"instance":{"id":"204","words":["show","me","movies","directed","by","drew","barrymore"],"labels":["O","O","O","B-director","O","B-director","I-director"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, genre, title, song, average ratings, character, year, actor, rating, plot, review, trailer and O.\nSentence: show me movies directed by drew barrymore","prompt_labels":"show(O) me(O) movies(O) directed(B-director) by(O) drew(B-director) barrymore(I-director)"}}
{"id":"205","dataset":"mit-movie","split":"train","label_list":["review","rating","plot","song","actor","trailer","director","title","genre","character","average ratings","year"],"instance":{"id":"205","words":["who","directed","the","harry","potter","movies"],"labels":["O","B-director","O","B-title","I-title","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, rating, plot, song, actor, trailer, director, title, genre, character, average ratings, year and O.\nSentence: who directed the harry potter movies","prompt_labels":"who(O) directed(B-director) the(O) harry(B-title) potter(I-title) movies(O)"}}
{"id":"206","dataset":"mit-movie","split":"train","label_list":["title","plot","year","genre","review","average ratings","song","director","trailer","character","actor","rating"],"instance":{"id":"206","words":["top","rated","action","movies"],"labels":["B-review","I-review","B-genre","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, plot, year, genre, review, average ratings, song, director, trailer, character, actor, rating and O.\nSentence: top rated action movies","prompt_labels":"top(B-review) rated(I-review) action(B-genre) movies(O)"}}
{"id":"207","dataset":"mit-movie","split":"train","label_list":["year","title","actor","song","director","average ratings","rating","plot","trailer","genre","character","review"],"instance":{"id":"207","words":["did","the","original","little","mermaid","made","by","hans","christian","anderson","ever","come","out","on","dvd"],"labels":["O","O","B-title","I-title","I-title","O","O","O","O","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, title, actor, song, director, average ratings, rating, plot, trailer, genre, character, review and O.\nSentence: did the original little mermaid made by hans christian anderson ever come out on dvd","prompt_labels":"did(O) the(O) original(B-title) little(I-title) mermaid(I-title) made(O) by(O) hans(O) christian(O) anderson(O) ever(O) come(O) out(O) on(O) dvd(O)"}}
{"id":"208","dataset":"mit-movie","split":"train","label_list":["character","title","genre","rating","average ratings","review","plot","trailer","year","song","director","actor"],"instance":{"id":"208","words":["what","r","rated","horror","movies","are","about","boats"],"labels":["O","B-rating","I-rating","B-genre","I-genre","O","O","B-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, title, genre, rating, average ratings, review, plot, trailer, year, song, director, actor and O.\nSentence: what r rated horror movies are about boats","prompt_labels":"what(O) r(B-rating) rated(I-rating) horror(B-genre) movies(I-genre) are(O) about(O) boats(B-plot)"}}
{"id":"209","dataset":"mit-movie","split":"train","label_list":["song","director","trailer","review","year","actor","genre","rating","title","average ratings","plot","character"],"instance":{"id":"209","words":["was","a","beautiful","mind","an","original","score"],"labels":["O","B-title","I-title","I-title","O","B-song","I-song"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, director, trailer, review, year, actor, genre, rating, title, average ratings, plot, character and O.\nSentence: was a beautiful mind an original score","prompt_labels":"was(O) a(B-title) beautiful(I-title) mind(I-title) an(O) original(B-song) score(I-song)"}}
{"id":"210","dataset":"mit-movie","split":"train","label_list":["average ratings","trailer","year","actor","review","character","song","director","title","plot","rating","genre"],"instance":{"id":"210","words":["what","is","shierly","temples","greatest","movie"],"labels":["O","O","B-actor","I-actor","B-review","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, trailer, year, actor, review, character, song, director, title, plot, rating, genre and O.\nSentence: what is shierly temples greatest movie","prompt_labels":"what(O) is(O) shierly(B-actor) temples(I-actor) greatest(B-review) movie(O)"}}
{"id":"211","dataset":"mit-movie","split":"train","label_list":["character","review","average ratings","director","song","year","genre","title","plot","trailer","rating","actor"],"instance":{"id":"211","words":["find","me","movies","with","cameron","diaz","from","the","2000s"],"labels":["O","O","O","O","B-actor","I-actor","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, review, average ratings, director, song, year, genre, title, plot, trailer, rating, actor and O.\nSentence: find me movies with cameron diaz from the 2000s","prompt_labels":"find(O) me(O) movies(O) with(O) cameron(B-actor) diaz(I-actor) from(O) the(O) 2000s(B-year)"}}
{"id":"212","dataset":"mit-movie","split":"train","label_list":["review","song","plot","average ratings","character","actor","title","rating","trailer","director","year","genre"],"instance":{"id":"212","words":["show","me","the","movie","named","hugo"],"labels":["O","O","O","O","O","B-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, plot, average ratings, character, actor, title, rating, trailer, director, year, genre and O.\nSentence: show me the movie named hugo","prompt_labels":"show(O) me(O) the(O) movie(O) named(O) hugo(B-title)"}}
{"id":"213","dataset":"mit-movie","split":"train","label_list":["song","rating","trailer","genre","year","director","plot","actor","review","title","average ratings","character"],"instance":{"id":"213","words":["could","you","locate","the","1990s","film","about","a","high","school","starring","sean","penn"],"labels":["O","O","O","O","B-year","O","B-plot","I-plot","I-plot","I-plot","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, rating, trailer, genre, year, director, plot, actor, review, title, average ratings, character and O.\nSentence: could you locate the 1990s film about a high school starring sean penn","prompt_labels":"could(O) you(O) locate(O) the(O) 1990s(B-year) film(O) about(B-plot) a(I-plot) high(I-plot) school(I-plot) starring(O) sean(B-actor) penn(I-actor)"}}
{"id":"214","dataset":"mit-movie","split":"train","label_list":["song","plot","year","character","director","rating","trailer","review","genre","actor","average ratings","title"],"instance":{"id":"214","words":["are","there","any","r","rated","movies","about","horses","from","the","1980s"],"labels":["O","O","O","B-rating","I-rating","O","O","B-plot","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, plot, year, character, director, rating, trailer, review, genre, actor, average ratings, title and O.\nSentence: are there any r rated movies about horses from the 1980s","prompt_labels":"are(O) there(O) any(O) r(B-rating) rated(I-rating) movies(O) about(O) horses(B-plot) from(O) the(O) 1980s(B-year)"}}
{"id":"215","dataset":"mit-movie","split":"train","label_list":["title","genre","character","song","rating","actor","year","review","trailer","director","average ratings","plot"],"instance":{"id":"215","words":["did","billy","crystal","ever","star","in","a","sci","fi","film"],"labels":["O","B-actor","I-actor","O","O","O","O","B-genre","I-genre","I-genre"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, genre, character, song, rating, actor, year, review, trailer, director, average ratings, plot and O.\nSentence: did billy crystal ever star in a sci fi film","prompt_labels":"did(O) billy(B-actor) crystal(I-actor) ever(O) star(O) in(O) a(O) sci(B-genre) fi(I-genre) film(I-genre)"}}
{"id":"216","dataset":"mit-movie","split":"train","label_list":["character","average ratings","review","title","trailer","director","actor","year","plot","song","genre","rating"],"instance":{"id":"216","words":["how","many","films","has","disney","released","since","2001"],"labels":["O","O","O","O","B-genre","B-year","I-year","I-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, average ratings, review, title, trailer, director, actor, year, plot, song, genre, rating and O.\nSentence: how many films has disney released since 2001","prompt_labels":"how(O) many(O) films(O) has(O) disney(B-genre) released(B-year) since(I-year) 2001(I-year)"}}
{"id":"217","dataset":"mit-movie","split":"train","label_list":["year","character","average ratings","title","actor","genre","trailer","song","review","rating","director","plot"],"instance":{"id":"217","words":["find","me","movies","with","laurence","olivier","released","in","the","1940s"],"labels":["O","O","O","O","B-actor","I-actor","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, character, average ratings, title, actor, genre, trailer, song, review, rating, director, plot and O.\nSentence: find me movies with laurence olivier released in the 1940s","prompt_labels":"find(O) me(O) movies(O) with(O) laurence(B-actor) olivier(I-actor) released(O) in(O) the(O) 1940s(B-year)"}}
{"id":"218","dataset":"mit-movie","split":"train","label_list":["director","genre","year","title","song","character","rating","plot","average ratings","trailer","review","actor"],"instance":{"id":"218","words":["show","me","the","jason","bourne","movies","from","the","1990s"],"labels":["O","O","O","B-character","I-character","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, genre, year, title, song, character, rating, plot, average ratings, trailer, review, actor and O.\nSentence: show me the jason bourne movies from the 1990s","prompt_labels":"show(O) me(O) the(O) jason(B-character) bourne(I-character) movies(O) from(O) the(O) 1990s(B-year)"}}
{"id":"219","dataset":"mit-movie","split":"train","label_list":["average ratings","trailer","director","character","title","plot","genre","actor","rating","review","year","song"],"instance":{"id":"219","words":["do","you","have","any","r","rated","movies","featuring","alien","invasion"],"labels":["O","O","O","O","B-rating","O","O","O","B-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, trailer, director, character, title, plot, genre, actor, rating, review, year, song and O.\nSentence: do you have any r rated movies featuring alien invasion","prompt_labels":"do(O) you(O) have(O) any(O) r(B-rating) rated(O) movies(O) featuring(O) alien(B-plot) invasion(I-plot)"}}
{"id":"220","dataset":"mit-movie","split":"train","label_list":["plot","average ratings","rating","year","title","director","actor","genre","trailer","song","character","review"],"instance":{"id":"220","words":["find","some","martin","and","lewis","movies"],"labels":["O","O","B-character","I-character","I-character","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, average ratings, rating, year, title, director, actor, genre, trailer, song, character, review and O.\nSentence: find some martin and lewis movies","prompt_labels":"find(O) some(O) martin(B-character) and(I-character) lewis(I-character) movies(O)"}}
{"id":"221","dataset":"mit-movie","split":"train","label_list":["genre","rating","trailer","plot","title","review","character","average ratings","actor","director","year","song"],"instance":{"id":"221","words":["find","me","3d","animated","movies"],"labels":["O","O","B-genre","I-genre","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, rating, trailer, plot, title, review, character, average ratings, actor, director, year, song and O.\nSentence: find me 3d animated movies","prompt_labels":"find(O) me(O) 3d(B-genre) animated(I-genre) movies(O)"}}
{"id":"222","dataset":"mit-movie","split":"train","label_list":["character","rating","title","genre","year","review","plot","trailer","average ratings","song","director","actor"],"instance":{"id":"222","words":["is","monsters","vs","aliens","appropriate","for","toddlers"],"labels":["O","B-title","I-title","I-title","B-review","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, rating, title, genre, year, review, plot, trailer, average ratings, song, director, actor and O.\nSentence: is monsters vs aliens appropriate for toddlers","prompt_labels":"is(O) monsters(B-title) vs(I-title) aliens(I-title) appropriate(B-review) for(O) toddlers(O)"}}
{"id":"223","dataset":"mit-movie","split":"train","label_list":["rating","character","plot","trailer","year","actor","genre","review","song","average ratings","title","director"],"instance":{"id":"223","words":["what","movie","was","the","song","somewhere","out","there","featured","in"],"labels":["O","O","O","O","O","B-song","I-song","I-song","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, character, plot, trailer, year, actor, genre, review, song, average ratings, title, director and O.\nSentence: what movie was the song somewhere out there featured in","prompt_labels":"what(O) movie(O) was(O) the(O) song(O) somewhere(B-song) out(I-song) there(I-song) featured(O) in(O)"}}
{"id":"224","dataset":"mit-movie","split":"train","label_list":["trailer","genre","song","review","average ratings","director","character","rating","actor","year","title","plot"],"instance":{"id":"224","words":["find","me","comedy","movies","with","tom","hanks","released","in","the","1990s"],"labels":["O","O","B-genre","I-genre","O","B-actor","I-actor","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, genre, song, review, average ratings, director, character, rating, actor, year, title, plot and O.\nSentence: find me comedy movies with tom hanks released in the 1990s","prompt_labels":"find(O) me(O) comedy(B-genre) movies(I-genre) with(O) tom(B-actor) hanks(I-actor) released(O) in(O) the(O) 1990s(B-year)"}}
{"id":"225","dataset":"mit-movie","split":"train","label_list":["song","plot","character","year","actor","trailer","rating","director","genre","average ratings","review","title"],"instance":{"id":"225","words":["what","movie","was","keanu","reeves","in","with","al","pacino"],"labels":["O","O","O","B-actor","I-actor","O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, plot, character, year, actor, trailer, rating, director, genre, average ratings, review, title and O.\nSentence: what movie was keanu reeves in with al pacino","prompt_labels":"what(O) movie(O) was(O) keanu(B-actor) reeves(I-actor) in(O) with(O) al(B-actor) pacino(I-actor)"}}
{"id":"226","dataset":"mit-movie","split":"train","label_list":["director","trailer","average ratings","plot","actor","review","rating","song","year","genre","title","character"],"instance":{"id":"226","words":["i","need","to","see","quotes","and","trailers","of","the","latest","r","rated","lesbian","action","movies"],"labels":["O","O","O","O","O","O","B-trailer","O","O","O","B-rating","I-rating","B-plot","B-genre","I-genre"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, trailer, average ratings, plot, actor, review, rating, song, year, genre, title, character and O.\nSentence: i need to see quotes and trailers of the latest r rated lesbian action movies","prompt_labels":"i(O) need(O) to(O) see(O) quotes(O) and(O) trailers(B-trailer) of(O) the(O) latest(O) r(B-rating) rated(I-rating) lesbian(B-plot) action(B-genre) movies(I-genre)"}}
{"id":"227","dataset":"mit-movie","split":"train","label_list":["trailer","director","genre","average ratings","review","title","rating","actor","year","song","character","plot"],"instance":{"id":"227","words":["id","like","a","pg","science","fiction","movie","please"],"labels":["O","O","O","B-rating","B-genre","I-genre","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, director, genre, average ratings, review, title, rating, actor, year, song, character, plot and O.\nSentence: id like a pg science fiction movie please","prompt_labels":"id(O) like(O) a(O) pg(B-rating) science(B-genre) fiction(I-genre) movie(O) please(O)"}}
{"id":"228","dataset":"mit-movie","split":"train","label_list":["review","actor","song","plot","rating","character","average ratings","genre","title","year","director","trailer"],"instance":{"id":"228","words":["find","me","movies","based","on","a","rock","concert"],"labels":["O","O","O","O","O","O","B-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, actor, song, plot, rating, character, average ratings, genre, title, year, director, trailer and O.\nSentence: find me movies based on a rock concert","prompt_labels":"find(O) me(O) movies(O) based(O) on(O) a(O) rock(B-plot) concert(I-plot)"}}
{"id":"229","dataset":"mit-movie","split":"train","label_list":["genre","title","average ratings","actor","character","song","plot","trailer","director","rating","review","year"],"instance":{"id":"229","words":["what","are","some","of","the","must","see","movies","of","1973"],"labels":["O","O","O","O","O","B-review","I-review","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, title, average ratings, actor, character, song, plot, trailer, director, rating, review, year and O.\nSentence: what are some of the must see movies of 1973","prompt_labels":"what(O) are(O) some(O) of(O) the(O) must(B-review) see(I-review) movies(O) of(O) 1973(B-year)"}}
{"id":"230","dataset":"mit-movie","split":"train","label_list":["rating","genre","character","review","plot","trailer","song","year","title","director","actor","average ratings"],"instance":{"id":"230","words":["what","was","the","first","movie","to","receive","a","pg","13","rating"],"labels":["O","O","O","B-year","O","O","O","O","B-rating","I-rating","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, genre, character, review, plot, trailer, song, year, title, director, actor, average ratings and O.\nSentence: what was the first movie to receive a pg 13 rating","prompt_labels":"what(O) was(O) the(O) first(B-year) movie(O) to(O) receive(O) a(O) pg(B-rating) 13(I-rating) rating(O)"}}
{"id":"231","dataset":"mit-movie","split":"train","label_list":["plot","average ratings","character","rating","trailer","song","review","title","director","actor","year","genre"],"instance":{"id":"231","words":["what","genre","is","the","movie","a","walk","to","remember","a","part","of"],"labels":["O","B-genre","O","O","O","B-title","I-title","I-title","I-title","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, average ratings, character, rating, trailer, song, review, title, director, actor, year, genre and O.\nSentence: what genre is the movie a walk to remember a part of","prompt_labels":"what(O) genre(B-genre) is(O) the(O) movie(O) a(B-title) walk(I-title) to(I-title) remember(I-title) a(O) part(O) of(O)"}}
{"id":"232","dataset":"mit-movie","split":"train","label_list":["rating","year","plot","character","trailer","title","actor","review","song","genre","average ratings","director"],"instance":{"id":"232","words":["find","me","movies","with","the","beatles","on","the","soundtrack"],"labels":["O","O","O","O","O","B-song","O","O","B-song"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, year, plot, character, trailer, title, actor, review, song, genre, average ratings, director and O.\nSentence: find me movies with the beatles on the soundtrack","prompt_labels":"find(O) me(O) movies(O) with(O) the(O) beatles(B-song) on(O) the(O) soundtrack(B-song)"}}
{"id":"233","dataset":"mit-movie","split":"train","label_list":["average ratings","rating","trailer","director","review","plot","actor","title","character","genre","year","song"],"instance":{"id":"233","words":["are","there","any","musicals","with","an","r","rating"],"labels":["O","O","O","B-genre","O","O","B-rating","I-rating"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, rating, trailer, director, review, plot, actor, title, character, genre, year, song and O.\nSentence: are there any musicals with an r rating","prompt_labels":"are(O) there(O) any(O) musicals(B-genre) with(O) an(O) r(B-rating) rating(I-rating)"}}
{"id":"234","dataset":"mit-movie","split":"train","label_list":["character","year","title","director","review","plot","actor","genre","rating","average ratings","trailer","song"],"instance":{"id":"234","words":["did","simon","pegg","do","any","alien","movies"],"labels":["O","B-director","I-director","O","O","B-title","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, year, title, director, review, plot, actor, genre, rating, average ratings, trailer, song and O.\nSentence: did simon pegg do any alien movies","prompt_labels":"did(O) simon(B-director) pegg(I-director) do(O) any(O) alien(B-title) movies(O)"}}
{"id":"235","dataset":"mit-movie","split":"train","label_list":["year","director","trailer","title","song","rating","plot","review","actor","genre","average ratings","character"],"instance":{"id":"235","words":["what","movie","uses","abba","songs"],"labels":["O","O","O","B-song","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, trailer, title, song, rating, plot, review, actor, genre, average ratings, character and O.\nSentence: what movie uses abba songs","prompt_labels":"what(O) movie(O) uses(O) abba(B-song) songs(O)"}}
{"id":"236","dataset":"mit-movie","split":"train","label_list":["review","director","trailer","average ratings","plot","actor","title","genre","song","character","year","rating"],"instance":{"id":"236","words":["has","ridley","scott","directed","any","comedy","movies"],"labels":["O","B-director","I-director","O","O","B-genre","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, director, trailer, average ratings, plot, actor, title, genre, song, character, year, rating and O.\nSentence: has ridley scott directed any comedy movies","prompt_labels":"has(O) ridley(B-director) scott(I-director) directed(O) any(O) comedy(B-genre) movies(O)"}}
{"id":"237","dataset":"mit-movie","split":"train","label_list":["year","director","actor","plot","character","trailer","rating","review","average ratings","title","song","genre"],"instance":{"id":"237","words":["are","there","any","g","rated","thrillers"],"labels":["O","O","O","B-rating","I-rating","B-genre"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, actor, plot, character, trailer, rating, review, average ratings, title, song, genre and O.\nSentence: are there any g rated thrillers","prompt_labels":"are(O) there(O) any(O) g(B-rating) rated(I-rating) thrillers(B-genre)"}}
{"id":"238","dataset":"mit-movie","split":"train","label_list":["review","year","genre","director","average ratings","plot","character","trailer","actor","title","rating","song"],"instance":{"id":"238","words":["list","films","with","music","by","rick","springfield"],"labels":["O","O","O","O","O","B-song","I-song"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, year, genre, director, average ratings, plot, character, trailer, actor, title, rating, song and O.\nSentence: list films with music by rick springfield","prompt_labels":"list(O) films(O) with(O) music(O) by(O) rick(B-song) springfield(I-song)"}}
{"id":"239","dataset":"mit-movie","split":"train","label_list":["genre","actor","average ratings","rating","trailer","song","director","year","title","review","plot","character"],"instance":{"id":"239","words":["show","me","romantic","comedy","movies","starring","luke","wilson"],"labels":["O","O","B-genre","I-genre","O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, actor, average ratings, rating, trailer, song, director, year, title, review, plot, character and O.\nSentence: show me romantic comedy movies starring luke wilson","prompt_labels":"show(O) me(O) romantic(B-genre) comedy(I-genre) movies(O) starring(O) luke(B-actor) wilson(I-actor)"}}
{"id":"240","dataset":"mit-movie","split":"train","label_list":["genre","song","title","review","director","actor","character","average ratings","rating","year","trailer","plot"],"instance":{"id":"240","words":["find","me","the","movie","with","the","song","my","heart","will","go","on"],"labels":["O","O","O","O","O","O","O","B-song","I-song","I-song","I-song","I-song"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, song, title, review, director, actor, character, average ratings, rating, year, trailer, plot and O.\nSentence: find me the movie with the song my heart will go on","prompt_labels":"find(O) me(O) the(O) movie(O) with(O) the(O) song(O) my(B-song) heart(I-song) will(I-song) go(I-song) on(I-song)"}}
{"id":"241","dataset":"mit-movie","split":"train","label_list":["character","year","title","plot","rating","song","review","director","genre","trailer","average ratings","actor"],"instance":{"id":"241","words":["which","film","won","best","picture","in","1999"],"labels":["O","O","O","B-average ratings","I-average ratings","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, year, title, plot, rating, song, review, director, genre, trailer, average ratings, actor and O.\nSentence: which film won best picture in 1999","prompt_labels":"which(O) film(O) won(O) best(B-average ratings) picture(I-average ratings) in(O) 1999(B-year)"}}
{"id":"242","dataset":"mit-movie","split":"train","label_list":["director","title","year","rating","actor","trailer","average ratings","song","character","plot","review","genre"],"instance":{"id":"242","words":["when","is","the","next","twilight","movie","expected","to","appear","in","theatres"],"labels":["O","O","O","O","B-title","O","O","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, title, year, rating, actor, trailer, average ratings, song, character, plot, review, genre and O.\nSentence: when is the next twilight movie expected to appear in theatres","prompt_labels":"when(O) is(O) the(O) next(O) twilight(B-title) movie(O) expected(O) to(O) appear(O) in(O) theatres(O)"}}
{"id":"243","dataset":"mit-movie","split":"train","label_list":["plot","year","average ratings","director","genre","character","song","title","rating","review","trailer","actor"],"instance":{"id":"243","words":["are","there","any","movies","directed","by","steven","spielberg","rated","four","stars","or","higher"],"labels":["O","O","O","O","O","O","B-director","I-director","B-average ratings","I-average ratings","I-average ratings","I-average ratings","I-average ratings"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, year, average ratings, director, genre, character, song, title, rating, review, trailer, actor and O.\nSentence: are there any movies directed by steven spielberg rated four stars or higher","prompt_labels":"are(O) there(O) any(O) movies(O) directed(O) by(O) steven(B-director) spielberg(I-director) rated(B-average ratings) four(I-average ratings) stars(I-average ratings) or(I-average ratings) higher(I-average ratings)"}}
{"id":"244","dataset":"mit-movie","split":"train","label_list":["character","song","review","rating","director","actor","plot","year","trailer","genre","title","average ratings"],"instance":{"id":"244","words":["show","me","baseball","movies","from","the","1950s"],"labels":["O","O","B-plot","O","O","O","B-genre"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, song, review, rating, director, actor, plot, year, trailer, genre, title, average ratings and O.\nSentence: show me baseball movies from the 1950s","prompt_labels":"show(O) me(O) baseball(B-plot) movies(O) from(O) the(O) 1950s(B-genre)"}}
{"id":"245","dataset":"mit-movie","split":"train","label_list":["genre","director","average ratings","trailer","plot","rating","title","song","actor","review","year","character"],"instance":{"id":"245","words":["list","the","westerns","starring","clint","eastwood"],"labels":["O","O","B-genre","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, director, average ratings, trailer, plot, rating, title, song, actor, review, year, character and O.\nSentence: list the westerns starring clint eastwood","prompt_labels":"list(O) the(O) westerns(B-genre) starring(O) clint(B-actor) eastwood(I-actor)"}}
{"id":"246","dataset":"mit-movie","split":"train","label_list":["plot","director","character","review","actor","year","song","genre","average ratings","rating","trailer","title"],"instance":{"id":"246","words":["id","like","a","boxing","film","from","the","1940s"],"labels":["O","O","O","B-plot","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, director, character, review, actor, year, song, genre, average ratings, rating, trailer, title and O.\nSentence: id like a boxing film from the 1940s","prompt_labels":"id(O) like(O) a(O) boxing(B-plot) film(O) from(O) the(O) 1940s(B-year)"}}
{"id":"247","dataset":"mit-movie","split":"train","label_list":["trailer","song","genre","director","review","character","rating","plot","title","year","actor","average ratings"],"instance":{"id":"247","words":["has","anthony","hopkins","directed","any","movies"],"labels":["O","B-director","I-director","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, song, genre, director, review, character, rating, plot, title, year, actor, average ratings and O.\nSentence: has anthony hopkins directed any movies","prompt_labels":"has(O) anthony(B-director) hopkins(I-director) directed(O) any(O) movies(O)"}}
{"id":"248","dataset":"mit-movie","split":"train","label_list":["average ratings","song","actor","character","year","rating","trailer","director","plot","review","genre","title"],"instance":{"id":"248","words":["find","me","the","movie","with","the","song","everythings","coming","up","roses"],"labels":["O","O","O","O","O","O","O","B-song","I-song","I-song","I-song"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, song, actor, character, year, rating, trailer, director, plot, review, genre, title and O.\nSentence: find me the movie with the song everythings coming up roses","prompt_labels":"find(O) me(O) the(O) movie(O) with(O) the(O) song(O) everythings(B-song) coming(I-song) up(I-song) roses(I-song)"}}
{"id":"249","dataset":"mit-movie","split":"train","label_list":["year","genre","rating","song","average ratings","title","review","trailer","plot","actor","character","director"],"instance":{"id":"249","words":["show","me","dramatic","movies","directed","by","ron","howard"],"labels":["O","O","B-genre","O","O","O","B-director","I-director"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, genre, rating, song, average ratings, title, review, trailer, plot, actor, character, director and O.\nSentence: show me dramatic movies directed by ron howard","prompt_labels":"show(O) me(O) dramatic(B-genre) movies(O) directed(O) by(O) ron(B-director) howard(I-director)"}}
{"id":"250","dataset":"mit-movie","split":"train","label_list":["title","year","trailer","actor","director","plot","average ratings","review","rating","genre","character","song"],"instance":{"id":"250","words":["im","looking","for","a","family","drama","starring","meryl","streep"],"labels":["O","O","O","O","B-genre","I-genre","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, year, trailer, actor, director, plot, average ratings, review, rating, genre, character, song and O.\nSentence: im looking for a family drama starring meryl streep","prompt_labels":"im(O) looking(O) for(O) a(O) family(B-genre) drama(I-genre) starring(O) meryl(B-actor) streep(I-actor)"}}
{"id":"251","dataset":"mit-movie","split":"train","label_list":["rating","director","character","review","year","genre","actor","title","average ratings","plot","song","trailer"],"instance":{"id":"251","words":["what","year","was","mad","mad","mad","mad","mad","world","filmed"],"labels":["O","O","O","B-title","I-title","I-title","I-title","I-title","I-title","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, character, review, year, genre, actor, title, average ratings, plot, song, trailer and O.\nSentence: what year was mad mad mad mad mad world filmed","prompt_labels":"what(O) year(O) was(O) mad(B-title) mad(I-title) mad(I-title) mad(I-title) mad(I-title) world(I-title) filmed(O)"}}
{"id":"252","dataset":"mit-movie","split":"train","label_list":["average ratings","review","actor","title","rating","trailer","genre","plot","song","year","character","director"],"instance":{"id":"252","words":["which","film","won","best","picture","in","1969"],"labels":["O","O","O","B-average ratings","I-average ratings","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, review, actor, title, rating, trailer, genre, plot, song, year, character, director and O.\nSentence: which film won best picture in 1969","prompt_labels":"which(O) film(O) won(O) best(B-average ratings) picture(I-average ratings) in(O) 1969(B-year)"}}
{"id":"253","dataset":"mit-movie","split":"train","label_list":["title","character","review","rating","actor","director","year","trailer","song","genre","average ratings","plot"],"instance":{"id":"253","words":["top","disney","movies"],"labels":["O","B-plot","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, character, review, rating, actor, director, year, trailer, song, genre, average ratings, plot and O.\nSentence: top disney movies","prompt_labels":"top(O) disney(B-plot) movies(O)"}}
{"id":"254","dataset":"mit-movie","split":"train","label_list":["character","genre","average ratings","title","actor","song","trailer","plot","year","review","rating","director"],"instance":{"id":"254","words":["show","me","all","romantic","movies","released","in","1996"],"labels":["O","O","O","B-genre","I-genre","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, genre, average ratings, title, actor, song, trailer, plot, year, review, rating, director and O.\nSentence: show me all romantic movies released in 1996","prompt_labels":"show(O) me(O) all(O) romantic(B-genre) movies(I-genre) released(O) in(O) 1996(B-year)"}}
{"id":"255","dataset":"mit-movie","split":"train","label_list":["average ratings","year","plot","rating","trailer","review","genre","song","actor","director","character","title"],"instance":{"id":"255","words":["has","scarlett","johansson","ever","been","in","a","comedy"],"labels":["O","B-actor","I-actor","O","O","O","O","B-genre"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, year, plot, rating, trailer, review, genre, song, actor, director, character, title and O.\nSentence: has scarlett johansson ever been in a comedy","prompt_labels":"has(O) scarlett(B-actor) johansson(I-actor) ever(O) been(O) in(O) a(O) comedy(B-genre)"}}
{"id":"256","dataset":"mit-movie","split":"train","label_list":["average ratings","character","review","year","title","plot","actor","trailer","rating","genre","song","director"],"instance":{"id":"256","words":["who","directed","the","first","james","bond","movie"],"labels":["O","B-director","O","O","B-title","I-title","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, character, review, year, title, plot, actor, trailer, rating, genre, song, director and O.\nSentence: who directed the first james bond movie","prompt_labels":"who(O) directed(B-director) the(O) first(O) james(B-title) bond(I-title) movie(O)"}}
{"id":"257","dataset":"mit-movie","split":"train","label_list":["average ratings","rating","song","character","year","genre","actor","director","title","plot","trailer","review"],"instance":{"id":"257","words":["find","me","the","romantic","comedy","movies","directed","by","shawn","levy"],"labels":["O","O","O","B-genre","I-genre","O","B-director","O","B-director","I-director"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, rating, song, character, year, genre, actor, director, title, plot, trailer, review and O.\nSentence: find me the romantic comedy movies directed by shawn levy","prompt_labels":"find(O) me(O) the(O) romantic(B-genre) comedy(I-genre) movies(O) directed(B-director) by(O) shawn(B-director) levy(I-director)"}}
{"id":"258","dataset":"mit-movie","split":"train","label_list":["plot","actor","trailer","year","title","song","average ratings","genre","rating","review","director","character"],"instance":{"id":"258","words":["did","milos","forman","direct","a","musical","film","in","the","1970s"],"labels":["O","B-director","I-director","O","O","B-genre","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, actor, trailer, year, title, song, average ratings, genre, rating, review, director, character and O.\nSentence: did milos forman direct a musical film in the 1970s","prompt_labels":"did(O) milos(B-director) forman(I-director) direct(O) a(O) musical(B-genre) film(O) in(O) the(O) 1970s(B-year)"}}
{"id":"259","dataset":"mit-movie","split":"train","label_list":["song","actor","director","title","plot","rating","review","average ratings","year","character","genre","trailer"],"instance":{"id":"259","words":["show","me","a","movie","that","features","spaceships"],"labels":["O","O","O","O","O","O","B-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, actor, director, title, plot, rating, review, average ratings, year, character, genre, trailer and O.\nSentence: show me a movie that features spaceships","prompt_labels":"show(O) me(O) a(O) movie(O) that(O) features(O) spaceships(B-plot)"}}
{"id":"260","dataset":"mit-movie","split":"train","label_list":["average ratings","plot","rating","trailer","title","actor","review","year","director","genre","song","character"],"instance":{"id":"260","words":["best","movies","of","all","time"],"labels":["B-review","O","B-year","I-year","I-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, plot, rating, trailer, title, actor, review, year, director, genre, song, character and O.\nSentence: best movies of all time","prompt_labels":"best(B-review) movies(O) of(B-year) all(I-year) time(I-year)"}}
{"id":"261","dataset":"mit-movie","split":"train","label_list":["year","title","trailer","actor","director","character","song","rating","genre","average ratings","plot","review"],"instance":{"id":"261","words":["find","me","a","movie","that","starred","steve","martin","and","john","candy"],"labels":["O","O","O","O","O","O","B-actor","I-actor","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, title, trailer, actor, director, character, song, rating, genre, average ratings, plot, review and O.\nSentence: find me a movie that starred steve martin and john candy","prompt_labels":"find(O) me(O) a(O) movie(O) that(O) starred(O) steve(B-actor) martin(I-actor) and(O) john(B-actor) candy(I-actor)"}}
{"id":"262","dataset":"mit-movie","split":"train","label_list":["song","review","director","title","average ratings","plot","character","actor","trailer","year","genre","rating"],"instance":{"id":"262","words":["show","me","helen","hunt","movies","from","the","1990s"],"labels":["O","O","B-actor","I-actor","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, review, director, title, average ratings, plot, character, actor, trailer, year, genre, rating and O.\nSentence: show me helen hunt movies from the 1990s","prompt_labels":"show(O) me(O) helen(B-actor) hunt(I-actor) movies(O) from(O) the(O) 1990s(B-year)"}}
{"id":"263","dataset":"mit-movie","split":"train","label_list":["rating","trailer","review","plot","actor","year","character","song","title","genre","director","average ratings"],"instance":{"id":"263","words":["list","musicals","starring","julie","andrews","from","the","1960s"],"labels":["O","B-genre","O","B-actor","I-actor","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, trailer, review, plot, actor, year, character, song, title, genre, director, average ratings and O.\nSentence: list musicals starring julie andrews from the 1960s","prompt_labels":"list(O) musicals(B-genre) starring(O) julie(B-actor) andrews(I-actor) from(O) the(O) 1960s(B-year)"}}
{"id":"264","dataset":"mit-movie","split":"train","label_list":["character","genre","year","rating","director","review","title","song","actor","average ratings","trailer","plot"],"instance":{"id":"264","words":["show","me","the","names","of","the","best","reviewed","movies","based","on","books"],"labels":["O","O","O","O","O","O","B-average ratings","I-average ratings","O","B-plot","I-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, genre, year, rating, director, review, title, song, actor, average ratings, trailer, plot and O.\nSentence: show me the names of the best reviewed movies based on books","prompt_labels":"show(O) me(O) the(O) names(O) of(O) the(O) best(B-average ratings) reviewed(I-average ratings) movies(O) based(B-plot) on(I-plot) books(I-plot)"}}
{"id":"265","dataset":"mit-movie","split":"train","label_list":["rating","title","plot","director","review","actor","year","song","character","average ratings","genre","trailer"],"instance":{"id":"265","words":["can","you","get","me","the","fifth","horror","film","made","with","freddy","krueger"],"labels":["O","O","O","O","O","O","B-genre","O","O","O","B-character","I-character"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, title, plot, director, review, actor, year, song, character, average ratings, genre, trailer and O.\nSentence: can you get me the fifth horror film made with freddy krueger","prompt_labels":"can(O) you(O) get(O) me(O) the(O) fifth(O) horror(B-genre) film(O) made(O) with(O) freddy(B-character) krueger(I-character)"}}
{"id":"266","dataset":"mit-movie","split":"train","label_list":["trailer","average ratings","review","actor","song","genre","rating","title","year","director","character","plot"],"instance":{"id":"266","words":["show","me","films","directed","by","gary","ross","starring","tobey","maguire"],"labels":["O","O","O","O","O","B-director","I-director","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, average ratings, review, actor, song, genre, rating, title, year, director, character, plot and O.\nSentence: show me films directed by gary ross starring tobey maguire","prompt_labels":"show(O) me(O) films(O) directed(O) by(O) gary(B-director) ross(I-director) starring(O) tobey(B-actor) maguire(I-actor)"}}
{"id":"267","dataset":"mit-movie","split":"train","label_list":["review","year","genre","director","plot","character","rating","average ratings","actor","title","song","trailer"],"instance":{"id":"267","words":["has","peter","jackson","directed","any","movies","other","than","lord","of","the","rings"],"labels":["O","B-director","I-director","O","O","O","O","O","B-title","I-title","I-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, year, genre, director, plot, character, rating, average ratings, actor, title, song, trailer and O.\nSentence: has peter jackson directed any movies other than lord of the rings","prompt_labels":"has(O) peter(B-director) jackson(I-director) directed(O) any(O) movies(O) other(O) than(O) lord(B-title) of(I-title) the(I-title) rings(I-title)"}}
{"id":"268","dataset":"mit-movie","split":"train","label_list":["rating","review","title","director","plot","average ratings","song","actor","trailer","year","character","genre"],"instance":{"id":"268","words":["are","there","any","pg","rated","historical","dramas","starring","russel","crowe"],"labels":["O","O","O","B-rating","I-rating","B-genre","I-genre","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, review, title, director, plot, average ratings, song, actor, trailer, year, character, genre and O.\nSentence: are there any pg rated historical dramas starring russel crowe","prompt_labels":"are(O) there(O) any(O) pg(B-rating) rated(I-rating) historical(B-genre) dramas(I-genre) starring(O) russel(B-actor) crowe(I-actor)"}}
{"id":"269","dataset":"mit-movie","split":"train","label_list":["average ratings","character","director","plot","genre","year","song","rating","review","trailer","title","actor"],"instance":{"id":"269","words":["show","me","political","drama","movies","with","jeff","daniels"],"labels":["O","O","B-genre","I-genre","O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, character, director, plot, genre, year, song, rating, review, trailer, title, actor and O.\nSentence: show me political drama movies with jeff daniels","prompt_labels":"show(O) me(O) political(B-genre) drama(I-genre) movies(O) with(O) jeff(B-actor) daniels(I-actor)"}}
{"id":"270","dataset":"mit-movie","split":"train","label_list":["character","average ratings","actor","review","genre","rating","year","trailer","plot","song","director","title"],"instance":{"id":"270","words":["show","me","boxing","movies"],"labels":["O","O","B-plot","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, average ratings, actor, review, genre, rating, year, trailer, plot, song, director, title and O.\nSentence: show me boxing movies","prompt_labels":"show(O) me(O) boxing(B-plot) movies(O)"}}
{"id":"271","dataset":"mit-movie","split":"train","label_list":["plot","trailer","average ratings","review","year","rating","song","genre","actor","character","director","title"],"instance":{"id":"271","words":["find","me","all","movies","with","the","title","a","midsummer","nights","dream"],"labels":["O","O","O","O","O","O","O","B-title","I-title","I-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, trailer, average ratings, review, year, rating, song, genre, actor, character, director, title and O.\nSentence: find me all movies with the title a midsummer nights dream","prompt_labels":"find(O) me(O) all(O) movies(O) with(O) the(O) title(O) a(B-title) midsummer(I-title) nights(I-title) dream(I-title)"}}
{"id":"272","dataset":"mit-movie","split":"train","label_list":["review","plot","actor","trailer","song","genre","year","average ratings","character","director","title","rating"],"instance":{"id":"272","words":["what","is","a","good","romatic","movie","that","is","rated","r"],"labels":["O","O","O","O","B-genre","O","O","O","O","B-rating"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, actor, trailer, song, genre, year, average ratings, character, director, title, rating and O.\nSentence: what is a good romatic movie that is rated r","prompt_labels":"what(O) is(O) a(O) good(O) romatic(B-genre) movie(O) that(O) is(O) rated(O) r(B-rating)"}}
{"id":"273","dataset":"mit-movie","split":"train","label_list":["trailer","director","review","title","rating","character","genre","song","plot","year","actor","average ratings"],"instance":{"id":"273","words":["list","the","musicals","directed","by","rob","marshall"],"labels":["O","O","B-genre","B-director","O","B-director","I-director"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, director, review, title, rating, character, genre, song, plot, year, actor, average ratings and O.\nSentence: list the musicals directed by rob marshall","prompt_labels":"list(O) the(O) musicals(B-genre) directed(B-director) by(O) rob(B-director) marshall(I-director)"}}
{"id":"274","dataset":"mit-movie","split":"train","label_list":["song","genre","director","character","title","year","rating","plot","trailer","average ratings","review","actor"],"instance":{"id":"274","words":["i","love","johnny","depp","movies"],"labels":["O","O","B-actor","I-actor","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, genre, director, character, title, year, rating, plot, trailer, average ratings, review, actor and O.\nSentence: i love johnny depp movies","prompt_labels":"i(O) love(O) johnny(B-actor) depp(I-actor) movies(O)"}}
{"id":"275","dataset":"mit-movie","split":"train","label_list":["actor","plot","song","trailer","year","title","genre","average ratings","character","rating","director","review"],"instance":{"id":"275","words":["what","movie","did","christopher","plummer","win","an","oscar","for"],"labels":["O","O","O","B-actor","I-actor","O","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, plot, song, trailer, year, title, genre, average ratings, character, rating, director, review and O.\nSentence: what movie did christopher plummer win an oscar for","prompt_labels":"what(O) movie(O) did(O) christopher(B-actor) plummer(I-actor) win(O) an(O) oscar(O) for(O)"}}
{"id":"276","dataset":"mit-movie","split":"train","label_list":["character","song","year","average ratings","rating","review","title","genre","actor","director","trailer","plot"],"instance":{"id":"276","words":["best","selling","novel","about","vampires","made","in","to","a","movie"],"labels":["O","O","B-plot","O","B-plot","O","O","O","O","B-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, song, year, average ratings, rating, review, title, genre, actor, director, trailer, plot and O.\nSentence: best selling novel about vampires made in to a movie","prompt_labels":"best(O) selling(O) novel(B-plot) about(O) vampires(B-plot) made(O) in(O) to(O) a(O) movie(B-plot)"}}
{"id":"277","dataset":"mit-movie","split":"train","label_list":["actor","character","title","review","song","plot","director","year","rating","genre","trailer","average ratings"],"instance":{"id":"277","words":["show","me","movies","directed","by","michael","bay"],"labels":["O","O","O","B-director","O","B-director","I-director"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, character, title, review, song, plot, director, year, rating, genre, trailer, average ratings and O.\nSentence: show me movies directed by michael bay","prompt_labels":"show(O) me(O) movies(O) directed(B-director) by(O) michael(B-director) bay(I-director)"}}
{"id":"278","dataset":"mit-movie","split":"train","label_list":["trailer","year","title","director","plot","actor","review","character","average ratings","song","rating","genre"],"instance":{"id":"278","words":["list","pg","rated","film","set","in","psychiatric","hospitals"],"labels":["O","B-rating","O","O","O","O","B-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, year, title, director, plot, actor, review, character, average ratings, song, rating, genre and O.\nSentence: list pg rated film set in psychiatric hospitals","prompt_labels":"list(O) pg(B-rating) rated(O) film(O) set(O) in(O) psychiatric(B-plot) hospitals(I-plot)"}}
{"id":"279","dataset":"mit-movie","split":"train","label_list":["character","genre","title","director","rating","review","actor","year","song","plot","trailer","average ratings"],"instance":{"id":"279","words":["what","is","a","popular","movie","soundtrack","from","the","late","1990s"],"labels":["O","O","O","B-song","I-song","I-song","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, genre, title, director, rating, review, actor, year, song, plot, trailer, average ratings and O.\nSentence: what is a popular movie soundtrack from the late 1990s","prompt_labels":"what(O) is(O) a(O) popular(B-song) movie(I-song) soundtrack(I-song) from(O) the(O) late(O) 1990s(B-year)"}}
{"id":"280","dataset":"mit-movie","split":"train","label_list":["director","title","average ratings","year","plot","genre","actor","rating","song","character","review","trailer"],"instance":{"id":"280","words":["who","stars","in","pretty","in","pink"],"labels":["O","O","O","B-title","I-title","I-title"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, title, average ratings, year, plot, genre, actor, rating, song, character, review, trailer and O.\nSentence: who stars in pretty in pink","prompt_labels":"who(O) stars(O) in(O) pretty(B-title) in(I-title) pink(I-title)"}}
{"id":"281","dataset":"mit-movie","split":"train","label_list":["song","average ratings","trailer","director","actor","genre","character","plot","review","year","rating","title"],"instance":{"id":"281","words":["show","me","a","comedy","made","in","the","1930s"],"labels":["O","O","O","B-genre","O","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, average ratings, trailer, director, actor, genre, character, plot, review, year, rating, title and O.\nSentence: show me a comedy made in the 1930s","prompt_labels":"show(O) me(O) a(O) comedy(B-genre) made(O) in(O) the(O) 1930s(B-year)"}}
{"id":"282","dataset":"mit-movie","split":"train","label_list":["review","plot","director","average ratings","actor","trailer","rating","song","character","title","year","genre"],"instance":{"id":"282","words":["are","there","any","scifi","films","where","the","villain","wins"],"labels":["O","O","O","B-genre","O","O","O","B-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, director, average ratings, actor, trailer, rating, song, character, title, year, genre and O.\nSentence: are there any scifi films where the villain wins","prompt_labels":"are(O) there(O) any(O) scifi(B-genre) films(O) where(O) the(O) villain(B-plot) wins(I-plot)"}}
{"id":"283","dataset":"mit-movie","split":"train","label_list":["rating","average ratings","year","actor","trailer","plot","character","director","review","title","genre","song"],"instance":{"id":"283","words":["show","me","the","action","adventure","movies","directed","by","steven","spielberg"],"labels":["O","O","O","B-genre","I-genre","O","B-director","O","B-director","I-director"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, average ratings, year, actor, trailer, plot, character, director, review, title, genre, song and O.\nSentence: show me the action adventure movies directed by steven spielberg","prompt_labels":"show(O) me(O) the(O) action(B-genre) adventure(I-genre) movies(O) directed(B-director) by(O) steven(B-director) spielberg(I-director)"}}
{"id":"284","dataset":"mit-movie","split":"train","label_list":["rating","title","average ratings","genre","song","trailer","character","plot","review","actor","year","director"],"instance":{"id":"284","words":["find","me","movies","with","the","rolling","stones"],"labels":["O","O","O","O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, title, average ratings, genre, song, trailer, character, plot, review, actor, year, director and O.\nSentence: find me movies with the rolling stones","prompt_labels":"find(O) me(O) movies(O) with(O) the(O) rolling(B-actor) stones(I-actor)"}}
{"id":"285","dataset":"mit-movie","split":"train","label_list":["actor","review","song","average ratings","year","title","genre","plot","director","rating","character","trailer"],"instance":{"id":"285","words":["show","me","the","new","lone","ranger","trailer"],"labels":["O","O","O","B-year","B-title","I-title","B-trailer"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, review, song, average ratings, year, title, genre, plot, director, rating, character, trailer and O.\nSentence: show me the new lone ranger trailer","prompt_labels":"show(O) me(O) the(O) new(B-year) lone(B-title) ranger(I-title) trailer(B-trailer)"}}
{"id":"286","dataset":"mit-movie","split":"train","label_list":["character","trailer","year","plot","review","song","genre","title","average ratings","actor","director","rating"],"instance":{"id":"286","words":["did","the","soundtrack","for","moulin","rouge","win","any","awards"],"labels":["O","O","B-song","O","B-title","I-title","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, trailer, year, plot, review, song, genre, title, average ratings, actor, director, rating and O.\nSentence: did the soundtrack for moulin rouge win any awards","prompt_labels":"did(O) the(O) soundtrack(B-song) for(O) moulin(B-title) rouge(I-title) win(O) any(O) awards(O)"}}
{"id":"287","dataset":"mit-movie","split":"train","label_list":["director","actor","year","plot","average ratings","character","trailer","title","genre","song","review","rating"],"instance":{"id":"287","words":["do","you","have","any","silent","movies","starring","harpo","marx"],"labels":["O","O","O","O","B-genre","I-genre","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, year, plot, average ratings, character, trailer, title, genre, song, review, rating and O.\nSentence: do you have any silent movies starring harpo marx","prompt_labels":"do(O) you(O) have(O) any(O) silent(B-genre) movies(I-genre) starring(O) harpo(B-actor) marx(I-actor)"}}
{"id":"288","dataset":"mit-movie","split":"train","label_list":["character","title","year","review","average ratings","song","trailer","actor","genre","rating","plot","director"],"instance":{"id":"288","words":["when","did","the","first","scream","movie","come","out"],"labels":["O","O","O","O","B-title","O","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, title, year, review, average ratings, song, trailer, actor, genre, rating, plot, director and O.\nSentence: when did the first scream movie come out","prompt_labels":"when(O) did(O) the(O) first(O) scream(B-title) movie(O) come(O) out(O)"}}
{"id":"289","dataset":"mit-movie","split":"train","label_list":["trailer","title","song","average ratings","review","actor","character","year","director","rating","genre","plot"],"instance":{"id":"289","words":["are","there","any","movies","about","ice","skating"],"labels":["O","O","O","O","O","B-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, title, song, average ratings, review, actor, character, year, director, rating, genre, plot and O.\nSentence: are there any movies about ice skating","prompt_labels":"are(O) there(O) any(O) movies(O) about(O) ice(B-plot) skating(I-plot)"}}
{"id":"290","dataset":"mit-movie","split":"train","label_list":["actor","director","trailer","genre","year","title","plot","review","song","average ratings","character","rating"],"instance":{"id":"290","words":["what","james","bond","movies","have","roger","moore","in","them"],"labels":["O","B-character","I-character","O","O","B-actor","I-actor","O","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, director, trailer, genre, year, title, plot, review, song, average ratings, character, rating and O.\nSentence: what james bond movies have roger moore in them","prompt_labels":"what(O) james(B-character) bond(I-character) movies(O) have(O) roger(B-actor) moore(I-actor) in(O) them(O)"}}
{"id":"291","dataset":"mit-movie","split":"train","label_list":["trailer","title","year","song","genre","rating","plot","average ratings","director","actor","review","character"],"instance":{"id":"291","words":["what","is","an","mpaa","rating"],"labels":["O","O","O","B-rating","I-rating"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, title, year, song, genre, rating, plot, average ratings, director, actor, review, character and O.\nSentence: what is an mpaa rating","prompt_labels":"what(O) is(O) an(O) mpaa(B-rating) rating(I-rating)"}}
{"id":"292","dataset":"mit-movie","split":"train","label_list":["title","genre","trailer","character","year","average ratings","director","plot","review","song","actor","rating"],"instance":{"id":"292","words":["who","starred","in","the","most","romantic","comedies","in","the","2000s"],"labels":["O","O","O","O","O","B-genre","I-genre","O","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, genre, trailer, character, year, average ratings, director, plot, review, song, actor, rating and O.\nSentence: who starred in the most romantic comedies in the 2000s","prompt_labels":"who(O) starred(O) in(O) the(O) most(O) romantic(B-genre) comedies(I-genre) in(O) the(O) 2000s(B-year)"}}
{"id":"293","dataset":"mit-movie","split":"train","label_list":["song","year","title","plot","rating","review","director","trailer","genre","average ratings","character","actor"],"instance":{"id":"293","words":["show","me","science","fiction","movies","starring","henry","thomas"],"labels":["O","O","B-genre","I-genre","O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, year, title, plot, rating, review, director, trailer, genre, average ratings, character, actor and O.\nSentence: show me science fiction movies starring henry thomas","prompt_labels":"show(O) me(O) science(B-genre) fiction(I-genre) movies(O) starring(O) henry(B-actor) thomas(I-actor)"}}
{"id":"294","dataset":"mit-movie","split":"train","label_list":["review","actor","trailer","plot","song","rating","year","average ratings","director","character","title","genre"],"instance":{"id":"294","words":["what","is","the","best","quentin","tarentino","movie"],"labels":["O","O","O","O","B-actor","I-actor","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, actor, trailer, plot, song, rating, year, average ratings, director, character, title, genre and O.\nSentence: what is the best quentin tarentino movie","prompt_labels":"what(O) is(O) the(O) best(O) quentin(B-actor) tarentino(I-actor) movie(O)"}}
{"id":"295","dataset":"mit-movie","split":"train","label_list":["genre","plot","trailer","character","actor","director","rating","title","year","review","average ratings","song"],"instance":{"id":"295","words":["are","there","any","directors","who","have","only","directed","r","rated","comedies"],"labels":["O","O","O","O","O","O","O","O","B-rating","I-rating","B-genre"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, plot, trailer, character, actor, director, rating, title, year, review, average ratings, song and O.\nSentence: are there any directors who have only directed r rated comedies","prompt_labels":"are(O) there(O) any(O) directors(O) who(O) have(O) only(O) directed(O) r(B-rating) rated(I-rating) comedies(B-genre)"}}
{"id":"296","dataset":"mit-movie","split":"train","label_list":["trailer","review","average ratings","director","year","character","rating","actor","title","genre","plot","song"],"instance":{"id":"296","words":["psychological","movie","by","stanly","kubrick","in","1971"],"labels":["B-genre","I-genre","O","B-director","I-director","O","B-year"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, review, average ratings, director, year, character, rating, actor, title, genre, plot, song and O.\nSentence: psychological movie by stanly kubrick in 1971","prompt_labels":"psychological(B-genre) movie(I-genre) by(O) stanly(B-director) kubrick(I-director) in(O) 1971(B-year)"}}
{"id":"297","dataset":"mit-movie","split":"train","label_list":["plot","average ratings","trailer","character","actor","review","song","director","title","year","rating","genre"],"instance":{"id":"297","words":["what","is","the","movie","starring","brad","pitt","and","ed","norton","called"],"labels":["O","O","O","O","O","B-actor","I-actor","O","B-actor","I-actor","O"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, average ratings, trailer, character, actor, review, song, director, title, year, rating, genre and O.\nSentence: what is the movie starring brad pitt and ed norton called","prompt_labels":"what(O) is(O) the(O) movie(O) starring(O) brad(B-actor) pitt(I-actor) and(O) ed(B-actor) norton(I-actor) called(O)"}}
{"id":"298","dataset":"mit-movie","split":"train","label_list":["genre","character","actor","trailer","plot","year","director","review","title","rating","song","average ratings"],"instance":{"id":"298","words":["do","you","have","the","recently","made","documentary","about","babies","around","the","world"],"labels":["O","O","O","O","O","O","B-genre","O","B-plot","I-plot","I-plot","I-plot"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, character, actor, trailer, plot, year, director, review, title, rating, song, average ratings and O.\nSentence: do you have the recently made documentary about babies around the world","prompt_labels":"do(O) you(O) have(O) the(O) recently(O) made(O) documentary(B-genre) about(O) babies(B-plot) around(I-plot) the(I-plot) world(I-plot)"}}
{"id":"299","dataset":"mit-movie","split":"train","label_list":["rating","plot","average ratings","director","song","trailer","title","year","genre","character","actor","review"],"instance":{"id":"299","words":["show","me","a","list","of","movies","starring","elvis","presley"],"labels":["O","O","O","O","O","O","O","B-actor","I-actor"],"instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, plot, average ratings, director, song, trailer, title, year, genre, character, actor, review and O.\nSentence: show me a list of movies starring elvis presley","prompt_labels":"show(O) me(O) a(O) list(O) of(O) movies(O) starring(O) elvis(B-actor) presley(I-actor)"}}
