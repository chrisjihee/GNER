{"id": "258", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "organization", "country", "metric", "person", "conference", "researcher", "field", "programming language", "university", "algorithm", "location", "task"], "instance": {"id": "258", "words": ["In", "the", "academic", "community", ",", "the", "major", "forums", "for", "research", "started", "in", "1995", "when", "the", "First", "International", "Conference", "Data", "Mining", "and", "Knowledge", "Discovery", "(", "KDD-95", ")", "was", "started", "in", "Montreal", "under", "AAAI", "sponsorship", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O", "O", "O", "B-location", "O", "B-conference", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, organization, country, metric, person, conference, researcher, field, programming language, university, algorithm, location, task and O.\nSentence: In the academic community , the major forums for research started in 1995 when the First International Conference Data Mining and Knowledge Discovery ( KDD-95 ) was started in Montreal under AAAI sponsorship .", "prompt_labels": "In(O) the(O) academic(O) community(O) ,(O) the(O) major(O) forums(O) for(O) research(O) started(O) in(O) 1995(O) when(O) the(O) First(B-conference) International(I-conference) Conference(I-conference) Data(I-conference) Mining(I-conference) and(I-conference) Knowledge(I-conference) Discovery(I-conference) ((O) KDD-95(B-conference) )(O) was(O) started(O) in(O) Montreal(B-location) under(O) AAAI(B-conference) sponsorship(O) .(O)"}}
{"id": "156", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "field", "programming language", "country", "task", "location", "organization", "conference", "university", "algorithm", "researcher", "person", "product"], "instance": {"id": "156", "words": ["The", "Newcomb", "Awards", "are", "announced", "in", "the", "AI", "Magazine", "published", "by", "AAAI", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, field, programming language, country, task, location, organization, conference, university, algorithm, researcher, person, product and O.\nSentence: The Newcomb Awards are announced in the AI Magazine published by AAAI .", "prompt_labels": "The(O) Newcomb(O) Awards(O) are(O) announced(O) in(O) the(O) AI(O) Magazine(O) published(O) by(O) AAAI(B-conference) .(O)"}}
{"id": "67", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "researcher", "organization", "university", "metric", "field", "person", "task", "country", "algorithm", "product", "programming language", "conference"], "instance": {"id": "67", "words": ["Conferences", "in", "the", "field", "of", "natural", "language", "processing", ",", "such", "as", "Association", "for", "Computational", "Linguistics", ",", "North", "American", "Chapter", "of", "the", "Association", "for", "Computational", "Linguistics", ",", "EMNLP", ",", "and", "HLT", ",", "are", "beginning", "to", "include", "papers", "on", "speech", "processing", "."], "labels": ["O", "O", "O", "O", "O", "B-field", "I-field", "I-field", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O", "B-conference", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, researcher, organization, university, metric, field, person, task, country, algorithm, product, programming language, conference and O.\nSentence: Conferences in the field of natural language processing , such as Association for Computational Linguistics , North American Chapter of the Association for Computational Linguistics , EMNLP , and HLT , are beginning to include papers on speech processing .", "prompt_labels": "Conferences(O) in(O) the(O) field(O) of(O) natural(B-field) language(I-field) processing(I-field) ,(O) such(O) as(O) Association(B-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) ,(O) North(B-conference) American(I-conference) Chapter(I-conference) of(I-conference) the(I-conference) Association(I-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) ,(O) EMNLP(B-conference) ,(O) and(O) HLT(B-conference) ,(O) are(O) beginning(O) to(O) include(O) papers(O) on(O) speech(B-field) processing(I-field) .(O)"}}
{"id": "326", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "metric", "researcher", "programming language", "conference", "person", "algorithm", "country", "field", "task", "university", "location", "product"], "instance": {"id": "326", "words": ["Some", "successful", "applications", "of", "deep", "learning", "are", "computer", "vision", "and", "speech", "recognition", ".", "Honglak", "Lee", ",", "Roger", "Grosse", ",", "Rajesh", "Ranganath", ",", "Andrew", "Y.", "Ng", "."], "labels": ["O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-task", "I-task", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, metric, researcher, programming language, conference, person, algorithm, country, field, task, university, location, product and O.\nSentence: Some successful applications of deep learning are computer vision and speech recognition . Honglak Lee , Roger Grosse , Rajesh Ranganath , Andrew Y. Ng .", "prompt_labels": "Some(O) successful(O) applications(O) of(O) deep(B-field) learning(I-field) are(O) computer(B-field) vision(I-field) and(O) speech(B-task) recognition(I-task) .(O) Honglak(B-researcher) Lee(I-researcher) ,(O) Roger(B-researcher) Grosse(I-researcher) ,(O) Rajesh(B-researcher) Ranganath(I-researcher) ,(O) Andrew(B-researcher) Y.(I-researcher) Ng(I-researcher) .(O)"}}
{"id": "370", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "location", "task", "programming language", "algorithm", "metric", "conference", "organization", "product", "researcher", "country", "university", "person"], "instance": {"id": "370", "words": ["Hyponymy", "is", "the", "most", "frequently", "encoded", "relation", "among", "synsets", "used", "in", "lexical", "databases", "such", "as", "WordNet", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, location, task, programming language, algorithm, metric, conference, organization, product, researcher, country, university, person and O.\nSentence: Hyponymy is the most frequently encoded relation among synsets used in lexical databases such as WordNet .", "prompt_labels": "Hyponymy(O) is(O) the(O) most(O) frequently(O) encoded(O) relation(O) among(O) synsets(O) used(O) in(O) lexical(O) databases(O) such(O) as(O) WordNet(B-product) .(O)"}}
{"id": "235", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "researcher", "location", "country", "programming language", "product", "organization", "algorithm", "task", "conference", "metric", "field", "university"], "instance": {"id": "235", "words": ["Other", "programming", "options", "include", "an", "embedded", "Python", "environment", ",", "and", "an", "R", "Console", "plus", "support", "for", "Rserve", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-programming language", "O", "O", "O", "O", "B-programming language", "O", "O", "O", "O", "B-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, researcher, location, country, programming language, product, organization, algorithm, task, conference, metric, field, university and O.\nSentence: Other programming options include an embedded Python environment , and an R Console plus support for Rserve .", "prompt_labels": "Other(O) programming(O) options(O) include(O) an(O) embedded(O) Python(B-programming language) environment(O) ,(O) and(O) an(O) R(B-programming language) Console(O) plus(O) support(O) for(O) Rserve(B-product) .(O)"}}
{"id": "182", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "algorithm", "location", "programming language", "product", "conference", "organization", "person", "country", "metric", "task", "field", "university"], "instance": {"id": "182", "words": ["Such", "a", "sequence", "(", "which", "depends", "on", "the", "outcome", "of", "the", "investigation", "of", "previous", "attributes", "at", "each", "stage", ")", "is", "called", "a", "decision", "tree", "and", "applied", "in", "the", "area", "of", "machine", "learning", "known", "as", "decision", "tree", "learning", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, algorithm, location, programming language, product, conference, organization, person, country, metric, task, field, university and O.\nSentence: Such a sequence ( which depends on the outcome of the investigation of previous attributes at each stage ) is called a decision tree and applied in the area of machine learning known as decision tree learning .", "prompt_labels": "Such(O) a(O) sequence(O) ((O) which(O) depends(O) on(O) the(O) outcome(O) of(O) the(O) investigation(O) of(O) previous(O) attributes(O) at(O) each(O) stage(O) )(O) is(O) called(O) a(O) decision(B-algorithm) tree(I-algorithm) and(O) applied(O) in(O) the(O) area(O) of(O) machine(B-field) learning(I-field) known(O) as(O) decision(B-algorithm) tree(I-algorithm) learning(I-algorithm) .(O)"}}
{"id": "96", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "university", "task", "product", "conference", "person", "organization", "programming language", "country", "location", "field", "metric", "algorithm"], "instance": {"id": "96", "words": ["The", "website", "was", "originally", "Perl", "-based", ",", "but", "IMDb", "no", "longer", "discloses", "what", "software", "it", "uses", "for", "reasons", "of", "security", "."], "labels": ["O", "O", "O", "O", "B-programming language", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, university, task, product, conference, person, organization, programming language, country, location, field, metric, algorithm and O.\nSentence: The website was originally Perl -based , but IMDb no longer discloses what software it uses for reasons of security .", "prompt_labels": "The(O) website(O) was(O) originally(O) Perl(B-programming language) -based(O) ,(O) but(O) IMDb(B-organization) no(O) longer(O) discloses(O) what(O) software(O) it(O) uses(O) for(O) reasons(O) of(O) security(O) .(O)"}}
{"id": "303", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "algorithm", "metric", "task", "organization", "person", "conference", "location", "university", "product", "country", "programming language", "researcher"], "instance": {"id": "303", "words": ["Edge", "detection", "is", "a", "fundamental", "tool", "in", "image", "processing", ",", "machine", "vision", "and", "computer", "vision", ",", "particularly", "in", "the", "areas", "of", "feature", "detection", "and", "feature", "extraction", "."], "labels": ["B-task", "I-task", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, algorithm, metric, task, organization, person, conference, location, university, product, country, programming language, researcher and O.\nSentence: Edge detection is a fundamental tool in image processing , machine vision and computer vision , particularly in the areas of feature detection and feature extraction .", "prompt_labels": "Edge(B-task) detection(I-task) is(O) a(O) fundamental(O) tool(O) in(O) image(B-field) processing(I-field) ,(O) machine(B-field) vision(I-field) and(O) computer(B-field) vision(I-field) ,(O) particularly(O) in(O) the(O) areas(O) of(O) feature(B-task) detection(I-task) and(O) feature(B-task) extraction(I-task) .(O)"}}
{"id": "363", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "metric", "programming language", "product", "field", "person", "conference", "researcher", "task", "organization", "location", "country", "university"], "instance": {"id": "363", "words": ["With", "the", "help", "of", "advanced", "AR", "technologies", "(", "e.g.", "adding", "computer", "vision", ",", "incorporating", "AR", "cameras", "into", "smartphone", "and", "object", "recognition", ")", "the", "information", "about", "the", "surrounding", "real", "world", "of", "the", "user", "becomes", "interactive", "and", "digitally", "manipulated", "."], "labels": ["O", "O", "O", "O", "O", "B-field", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "B-field", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, metric, programming language, product, field, person, conference, researcher, task, organization, location, country, university and O.\nSentence: With the help of advanced AR technologies ( e.g. adding computer vision , incorporating AR cameras into smartphone and object recognition ) the information about the surrounding real world of the user becomes interactive and digitally manipulated .", "prompt_labels": "With(O) the(O) help(O) of(O) advanced(O) AR(B-field) technologies(O) ((O) e.g.(O) adding(O) computer(B-field) vision(I-field) ,(O) incorporating(O) AR(B-field) cameras(O) into(O) smartphone(O) and(O) object(B-task) recognition(I-task) )(O) the(O) information(O) about(O) the(O) surrounding(O) real(O) world(O) of(O) the(O) user(O) becomes(O) interactive(O) and(O) digitally(O) manipulated(O) .(O)"}}
{"id": "361", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "university", "programming language", "field", "algorithm", "person", "researcher", "task", "product", "location", "metric", "organization", "conference"], "instance": {"id": "361", "words": ["In", "the", "context", "of", "machine", "learning", ",", "where", "it", "is", "most", "widely", "applied", "today", ",", "LDA", "was", "rediscovered", "independently", "by", "David", "Blei", ",", "Andrew", "Ng", "and", "Michael", "I.", "Jordan", "in", "2003", ",", "and", "presented", "as", "a", "graphical", "model", "for", "topic", "discovery", "."], "labels": ["O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, programming language, field, algorithm, person, researcher, task, product, location, metric, organization, conference and O.\nSentence: In the context of machine learning , where it is most widely applied today , LDA was rediscovered independently by David Blei , Andrew Ng and Michael I. Jordan in 2003 , and presented as a graphical model for topic discovery .", "prompt_labels": "In(O) the(O) context(O) of(O) machine(B-field) learning(I-field) ,(O) where(O) it(O) is(O) most(O) widely(O) applied(O) today(O) ,(O) LDA(B-algorithm) was(O) rediscovered(O) independently(O) by(O) David(B-researcher) Blei(I-researcher) ,(O) Andrew(B-researcher) Ng(I-researcher) and(O) Michael(B-researcher) I.(I-researcher) Jordan(I-researcher) in(O) 2003(O) ,(O) and(O) presented(O) as(O) a(O) graphical(B-algorithm) model(I-algorithm) for(O) topic(B-task) discovery(I-task) .(O)"}}
{"id": "252", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "country", "organization", "algorithm", "person", "task", "field", "programming language", "university", "researcher", "conference", "metric", "location"], "instance": {"id": "252", "words": ["He", "received", "a", "B.E.", "in", "electronics", "engineering", "from", "B.M.S.", "College", "of", "Engineering", "in", "Bangalore", ",", "India", "in", "1982", ",", "when", "it", "was", "affiliated", "with", "Bangalore", "University", ",", "an", "M.S.", "in", "electrical", "and", "computer", "engineering", "in", "1984", "from", "Drexel", "University", ",", "and", "an", "M.S.", "in", "computer", "science", "in", "1989", ",", "and", "a", "Ph.D.", "in", "1990", ",", "respectively", ",", "from", "the", "University", "of", "Wisconsin-Madison", ",", "where", "he", "studied", "Artificial", "Intelligence", "and", "worked", "with", "Leonard", "Uhr", "."], "labels": ["O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-university", "I-university", "I-university", "I-university", "O", "B-location", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "O", "O", "O", "O", "B-field", "I-field", "I-field", "I-field", "O", "O", "O", "B-university", "I-university", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, country, organization, algorithm, person, task, field, programming language, university, researcher, conference, metric, location and O.\nSentence: He received a B.E. in electronics engineering from B.M.S. College of Engineering in Bangalore , India in 1982 , when it was affiliated with Bangalore University , an M.S. in electrical and computer engineering in 1984 from Drexel University , and an M.S. in computer science in 1989 , and a Ph.D. in 1990 , respectively , from the University of Wisconsin-Madison , where he studied Artificial Intelligence and worked with Leonard Uhr .", "prompt_labels": "He(O) received(O) a(O) B.E.(O) in(O) electronics(B-field) engineering(I-field) from(O) B.M.S.(B-university) College(I-university) of(I-university) Engineering(I-university) in(O) Bangalore(B-location) ,(O) India(B-country) in(O) 1982(O) ,(O) when(O) it(O) was(O) affiliated(O) with(O) Bangalore(B-university) University(I-university) ,(O) an(O) M.S.(O) in(O) electrical(B-field) and(I-field) computer(I-field) engineering(I-field) in(O) 1984(O) from(O) Drexel(B-university) University(I-university) ,(O) and(O) an(O) M.S.(O) in(O) computer(B-field) science(I-field) in(O) 1989(O) ,(O) and(O) a(O) Ph.D.(O) in(O) 1990(O) ,(O) respectively(O) ,(O) from(O) the(O) University(B-university) of(I-university) Wisconsin-Madison(I-university) ,(O) where(O) he(O) studied(O) Artificial(B-field) Intelligence(I-field) and(O) worked(O) with(O) Leonard(B-researcher) Uhr(I-researcher) .(O)"}}
{"id": "215", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "organization", "task", "location", "person", "programming language", "country", "algorithm", "metric", "researcher", "product", "university", "conference"], "instance": {"id": "215", "words": ["The", "first", "alpha", "version", "of", "OpenCV", "was", "released", "to", "the", "public", "at", "the", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "in", "2000", ",", "and", "five", "betas", "were", "released", "between", "2001", "and", "2005", "."], "labels": ["O", "O", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, organization, task, location, person, programming language, country, algorithm, metric, researcher, product, university, conference and O.\nSentence: The first alpha version of OpenCV was released to the public at the Conference on Computer Vision and Pattern Recognition in 2000 , and five betas were released between 2001 and 2005 .", "prompt_labels": "The(O) first(O) alpha(O) version(O) of(O) OpenCV(B-product) was(O) released(O) to(O) the(O) public(O) at(O) the(O) Conference(O) on(O) Computer(B-conference) Vision(I-conference) and(I-conference) Pattern(I-conference) Recognition(I-conference) in(O) 2000(O) ,(O) and(O) five(O) betas(O) were(O) released(O) between(O) 2001(O) and(O) 2005(O) .(O)"}}
{"id": "45", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "researcher", "task", "person", "conference", "location", "algorithm", "organization", "product", "university", "programming language", "metric", "country"], "instance": {"id": "45", "words": ["Bengio", ",", "together", "with", "Geoffrey", "Hinton", "and", "Yann", "LeCun", ",", "are", "referred", "to", "by", "some", "as", "the", "Godfathers", "of", "AI", "and", "Godfathers", "of", "Deep", "Learning", "."], "labels": ["B-researcher", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, researcher, task, person, conference, location, algorithm, organization, product, university, programming language, metric, country and O.\nSentence: Bengio , together with Geoffrey Hinton and Yann LeCun , are referred to by some as the Godfathers of AI and Godfathers of Deep Learning .", "prompt_labels": "Bengio(B-researcher) ,(O) together(O) with(O) Geoffrey(B-researcher) Hinton(I-researcher) and(O) Yann(B-researcher) LeCun(I-researcher) ,(O) are(O) referred(O) to(O) by(O) some(O) as(O) the(O) Godfathers(O) of(O) AI(O) and(O) Godfathers(O) of(O) Deep(O) Learning(O) .(O)"}}
{"id": "5", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "field", "task", "conference", "metric", "algorithm", "product", "researcher", "university", "country", "programming language", "organization", "location"], "instance": {"id": "5", "words": ["A", "frame", "language", "is", "a", "technology", "used", "for", "knowledge", "representation", "in", "artificial", "intelligence", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, field, task, conference, metric, algorithm, product, researcher, university, country, programming language, organization, location and O.\nSentence: A frame language is a technology used for knowledge representation in artificial intelligence .", "prompt_labels": "A(O) frame(O) language(O) is(O) a(O) technology(O) used(O) for(O) knowledge(B-task) representation(I-task) in(O) artificial(B-field) intelligence(I-field) .(O)"}}
{"id": "118", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "location", "field", "metric", "researcher", "organization", "task", "country", "algorithm", "person", "product", "conference", "university"], "instance": {"id": "118", "words": ["Local", "search", "algorithms", "are", "widely", "applied", "to", "numerous", "hard", "computational", "problems", ",", "including", "problems", "from", "computer", "science", "(", "particularly", "artificial", "intelligence", ")", ",", "mathematics", ",", "operations", "research", ",", "engineering", ",", "and", "bioinformatics", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "B-field", "I-field", "O", "O", "B-field", "O", "B-field", "I-field", "O", "B-field", "O", "O", "B-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, location, field, metric, researcher, organization, task, country, algorithm, person, product, conference, university and O.\nSentence: Local search algorithms are widely applied to numerous hard computational problems , including problems from computer science ( particularly artificial intelligence ) , mathematics , operations research , engineering , and bioinformatics .", "prompt_labels": "Local(O) search(O) algorithms(O) are(O) widely(O) applied(O) to(O) numerous(O) hard(O) computational(O) problems(O) ,(O) including(O) problems(O) from(O) computer(B-field) science(I-field) ((O) particularly(O) artificial(B-field) intelligence(I-field) )(O) ,(O) mathematics(B-field) ,(O) operations(B-field) research(I-field) ,(O) engineering(B-field) ,(O) and(O) bioinformatics(B-field) .(O)"}}
{"id": "350", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "algorithm", "metric", "product", "person", "researcher", "organization", "programming language", "country", "university", "field", "task", "location"], "instance": {"id": "350", "words": ["Various", "methods", "for", "doing", "so", "were", "developed", "in", "the", "1980s", "and", "early", "1990s", "by", "Werbos", ",", "Williams", ",", "Robinson", ",", "Jürgen", "Schmidhuber", ",", "Sepp", "Hochreiter", ",", "Pearlmutter", "and", "others", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "O", "B-researcher", "O", "B-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, algorithm, metric, product, person, researcher, organization, programming language, country, university, field, task, location and O.\nSentence: Various methods for doing so were developed in the 1980s and early 1990s by Werbos , Williams , Robinson , Jürgen Schmidhuber , Sepp Hochreiter , Pearlmutter and others .", "prompt_labels": "Various(O) methods(O) for(O) doing(O) so(O) were(O) developed(O) in(O) the(O) 1980s(O) and(O) early(O) 1990s(O) by(O) Werbos(B-researcher) ,(O) Williams(B-researcher) ,(O) Robinson(B-researcher) ,(O) Jürgen(B-researcher) Schmidhuber(I-researcher) ,(O) Sepp(B-researcher) Hochreiter(I-researcher) ,(O) Pearlmutter(B-researcher) and(O) others(O) .(O)"}}
{"id": "56", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "country", "conference", "person", "metric", "programming language", "organization", "algorithm", "field", "researcher", "task", "university", "location"], "instance": {"id": "56", "words": ["In", "computerised", "Facial", "recognition", "system", ",", "each", "face", "is", "represented", "by", "a", "large", "number", "of", "pixel", "values", "."], "labels": ["O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, country, conference, person, metric, programming language, organization, algorithm, field, researcher, task, university, location and O.\nSentence: In computerised Facial recognition system , each face is represented by a large number of pixel values .", "prompt_labels": "In(O) computerised(O) Facial(B-product) recognition(I-product) system(I-product) ,(O) each(O) face(O) is(O) represented(O) by(O) a(O) large(O) number(O) of(O) pixel(O) values(O) .(O)"}}
{"id": "402", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "algorithm", "field", "metric", "university", "organization", "programming language", "task", "location", "person", "researcher", "conference", "product"], "instance": {"id": "402", "words": ["The", "encoder", "and", "decoder", "are", "trained", "to", "take", "a", "phrase", "and", "reproduce", "the", "one-hot", "distribution", "of", "a", "corresponding", "paraphrase", "by", "minimizing", "perplexity", "using", "simple", "stochastic", "gradient", "descent", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, algorithm, field, metric, university, organization, programming language, task, location, person, researcher, conference, product and O.\nSentence: The encoder and decoder are trained to take a phrase and reproduce the one-hot distribution of a corresponding paraphrase by minimizing perplexity using simple stochastic gradient descent .", "prompt_labels": "The(O) encoder(O) and(O) decoder(O) are(O) trained(O) to(O) take(O) a(O) phrase(O) and(O) reproduce(O) the(O) one-hot(O) distribution(O) of(O) a(O) corresponding(O) paraphrase(O) by(O) minimizing(O) perplexity(B-metric) using(O) simple(O) stochastic(B-algorithm) gradient(I-algorithm) descent(I-algorithm) .(O)"}}
{"id": "163", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "country", "product", "metric", "conference", "field", "researcher", "person", "programming language", "task", "organization", "location", "university"], "instance": {"id": "163", "words": ["As", "data", "set", "s", "have", "grown", "in", "size", "and", "complexity", ",", "direct", "hands-on", "data", "analysis", "has", "been", "augmented", "with", "indirect", ",", "automated", "data", "processing", ",", "aided", "by", "other", "discoveries", "in", "computer", "science", ",", "specially", "in", "the", "field", "of", "machine", "learning", ",", "such", "as", "neural", "networks", ",", "cluster", "analysis", ",", "genetic", "algorithms", "(", "1950s", ")", ",", "decision", "tree", "learning", "and", "decision", "rules", "(", "1960s", ")", ",", "and", "support", "vector", "machines", "(", "1990s", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "B-task", "I-task", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, country, product, metric, conference, field, researcher, person, programming language, task, organization, location, university and O.\nSentence: As data set s have grown in size and complexity , direct hands-on data analysis has been augmented with indirect , automated data processing , aided by other discoveries in computer science , specially in the field of machine learning , such as neural networks , cluster analysis , genetic algorithms ( 1950s ) , decision tree learning and decision rules ( 1960s ) , and support vector machines ( 1990s ) .", "prompt_labels": "As(O) data(O) set(O) s(O) have(O) grown(O) in(O) size(O) and(O) complexity(O) ,(O) direct(O) hands-on(O) data(B-field) analysis(I-field) has(O) been(O) augmented(O) with(O) indirect(O) ,(O) automated(O) data(O) processing(O) ,(O) aided(O) by(O) other(O) discoveries(O) in(O) computer(B-field) science(I-field) ,(O) specially(O) in(O) the(O) field(O) of(O) machine(B-field) learning(I-field) ,(O) such(O) as(O) neural(B-algorithm) networks(I-algorithm) ,(O) cluster(B-task) analysis(I-task) ,(O) genetic(B-algorithm) algorithms(I-algorithm) ((O) 1950s(O) )(O) ,(O) decision(B-algorithm) tree(I-algorithm) learning(I-algorithm) and(O) decision(B-algorithm) rules(I-algorithm) ((O) 1960s(O) )(O) ,(O) and(O) support(B-algorithm) vector(I-algorithm) machines(I-algorithm) ((O) 1990s(O) )(O) .(O)"}}
{"id": "203", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "conference", "organization", "researcher", "person", "university", "algorithm", "product", "location", "metric", "country", "field", "task"], "instance": {"id": "203", "words": ["In", "July", "2016", ",", "Nvidia", "demonstrated", "during", "SIGGRAPH", "a", "new", "method", "of", "foveated", "rendering", "claimed", "to", "be", "invisible", "to", "users", "."], "labels": ["O", "O", "O", "O", "B-organization", "O", "O", "B-conference", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, conference, organization, researcher, person, university, algorithm, product, location, metric, country, field, task and O.\nSentence: In July 2016 , Nvidia demonstrated during SIGGRAPH a new method of foveated rendering claimed to be invisible to users .", "prompt_labels": "In(O) July(O) 2016(O) ,(O) Nvidia(B-organization) demonstrated(O) during(O) SIGGRAPH(B-conference) a(O) new(O) method(O) of(O) foveated(O) rendering(O) claimed(O) to(O) be(O) invisible(O) to(O) users(O) .(O)"}}
{"id": "423", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "algorithm", "university", "product", "country", "person", "researcher", "metric", "field", "conference", "task", "organization", "location"], "instance": {"id": "423", "words": ["The", "Institute", "has", "collaborated", "closely", "with", "the", "Janelia", "Farm", "Campus", "of", "Howard", "Hughes", "Medical", "Institute", ",", "the", "Allen", "Institute", "for", "Brain", "Science", "and", "the", "National", "Institutes", "of", "Health", "to", "develop", "better", "methods", "of", "reconstructing", "neuronal", "architectures", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, algorithm, university, product, country, person, researcher, metric, field, conference, task, organization, location and O.\nSentence: The Institute has collaborated closely with the Janelia Farm Campus of Howard Hughes Medical Institute , the Allen Institute for Brain Science and the National Institutes of Health to develop better methods of reconstructing neuronal architectures .", "prompt_labels": "The(O) Institute(O) has(O) collaborated(O) closely(O) with(O) the(O) Janelia(B-organization) Farm(I-organization) Campus(I-organization) of(I-organization) Howard(I-organization) Hughes(I-organization) Medical(I-organization) Institute(I-organization) ,(O) the(O) Allen(B-organization) Institute(I-organization) for(I-organization) Brain(I-organization) Science(I-organization) and(O) the(O) National(B-organization) Institutes(I-organization) of(I-organization) Health(I-organization) to(O) develop(O) better(O) methods(O) of(O) reconstructing(O) neuronal(O) architectures(O) .(O)"}}
{"id": "282", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "metric", "programming language", "researcher", "country", "person", "university", "location", "organization", "conference", "task", "algorithm", "product"], "instance": {"id": "282", "words": ["R", "functionality", "is", "accessible", "from", "several", "scripting", "languages", "such", "as", "Python", ",", "are", "available", "as", "well", "."], "labels": ["B-programming language", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-programming language", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, metric, programming language, researcher, country, person, university, location, organization, conference, task, algorithm, product and O.\nSentence: R functionality is accessible from several scripting languages such as Python , are available as well .", "prompt_labels": "R(B-programming language) functionality(O) is(O) accessible(O) from(O) several(O) scripting(O) languages(O) such(O) as(O) Python(B-programming language) ,(O) are(O) available(O) as(O) well(O) .(O)"}}
{"id": "161", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "location", "conference", "programming language", "organization", "metric", "product", "algorithm", "person", "country", "task", "field", "researcher"], "instance": {"id": "161", "words": ["EM", "is", "frequently", "used", "for", "data", "clustering", "in", "machine", "learning", "and", "computer", "vision", "."], "labels": ["B-algorithm", "O", "O", "O", "O", "B-task", "I-task", "O", "B-field", "I-field", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, location, conference, programming language, organization, metric, product, algorithm, person, country, task, field, researcher and O.\nSentence: EM is frequently used for data clustering in machine learning and computer vision .", "prompt_labels": "EM(B-algorithm) is(O) frequently(O) used(O) for(O) data(B-task) clustering(I-task) in(O) machine(B-field) learning(I-field) and(O) computer(B-field) vision(I-field) .(O)"}}
{"id": "193", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "person", "country", "location", "programming language", "product", "university", "organization", "metric", "task", "conference", "researcher", "field"], "instance": {"id": "193", "words": ["Milner", "has", "received", "numerous", "awards", "for", "her", "contributions", "to", "neuroscience", "and", "psychology", "including", "memberships", "in", "the", "Royal", "Society", "of", "London", ",", "the", "Royal", "Society", "of", "Canada", "and", "the", "National", "Academy", "of", "Sciences", "."], "labels": ["B-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "O", "B-field", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, person, country, location, programming language, product, university, organization, metric, task, conference, researcher, field and O.\nSentence: Milner has received numerous awards for her contributions to neuroscience and psychology including memberships in the Royal Society of London , the Royal Society of Canada and the National Academy of Sciences .", "prompt_labels": "Milner(B-researcher) has(O) received(O) numerous(O) awards(O) for(O) her(O) contributions(O) to(O) neuroscience(B-field) and(O) psychology(B-field) including(O) memberships(O) in(O) the(O) Royal(B-organization) Society(I-organization) of(I-organization) London(I-organization) ,(O) the(O) Royal(B-organization) Society(I-organization) of(I-organization) Canada(I-organization) and(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) .(O)"}}
{"id": "349", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "task", "researcher", "metric", "university", "field", "algorithm", "programming language", "location", "conference", "organization", "country", "product"], "instance": {"id": "349", "words": ["Google", "Translate", "'s", "neural", "machine", "translation", "system", "uses", "a", "large", "end-to-end", "artificial", "neural", "network", "that", "attempts", "to", "perform", "deep", "learning", ",", "in", "particular", ",", "long", "short-term", "memory", "networks", "."], "labels": ["B-product", "I-product", "O", "B-product", "I-product", "I-product", "I-product", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, task, researcher, metric, university, field, algorithm, programming language, location, conference, organization, country, product and O.\nSentence: Google Translate 's neural machine translation system uses a large end-to-end artificial neural network that attempts to perform deep learning , in particular , long short-term memory networks .", "prompt_labels": "Google(B-product) Translate(I-product) 's(O) neural(B-product) machine(I-product) translation(I-product) system(I-product) uses(O) a(O) large(O) end-to-end(B-algorithm) artificial(I-algorithm) neural(I-algorithm) network(I-algorithm) that(O) attempts(O) to(O) perform(O) deep(B-field) learning(I-field) ,(O) in(O) particular(O) ,(O) long(B-algorithm) short-term(I-algorithm) memory(I-algorithm) networks(I-algorithm) .(O)"}}
{"id": "306", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "researcher", "metric", "task", "country", "product", "university", "location", "person", "programming language", "organization", "field", "algorithm"], "instance": {"id": "306", "words": ["But", "these", "methods", "never", "won", "over", "the", "non-uniform", "internal-handcrafting", "Gaussian", "mixture", "model", "/", "Hidden", "Markov", "model", "(", "GMM-HMM", ")", "technology", "based", "on", "generative", "models", "of", "speech", "trained", "discriminatively", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, researcher, metric, task, country, product, university, location, person, programming language, organization, field, algorithm and O.\nSentence: But these methods never won over the non-uniform internal-handcrafting Gaussian mixture model / Hidden Markov model ( GMM-HMM ) technology based on generative models of speech trained discriminatively .", "prompt_labels": "But(O) these(O) methods(O) never(O) won(O) over(O) the(O) non-uniform(O) internal-handcrafting(O) Gaussian(B-algorithm) mixture(I-algorithm) model(I-algorithm) /(O) Hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) ((O) GMM-HMM(B-algorithm) )(O) technology(O) based(O) on(O) generative(O) models(O) of(O) speech(O) trained(O) discriminatively(O) .(O)"}}
{"id": "92", "dataset": "crossner_ai", "split": "test", "label_list": ["task", "person", "algorithm", "researcher", "location", "programming language", "metric", "conference", "product", "university", "organization", "field", "country"], "instance": {"id": "92", "words": ["Neuroevolution", ",", "or", "neuro-evolution", ",", "is", "a", "form", "of", "artificial", "intelligence", "that", "uses", "evolutionary", "algorithm", "s", "to", "generate", "artificial", "neural", "network", "s", "(", "ANN", ")", ",", "parameters", ",", "topology", "and", "rules.", "and", "evolutionary", "robotics", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "B-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, person, algorithm, researcher, location, programming language, metric, conference, product, university, organization, field, country and O.\nSentence: Neuroevolution , or neuro-evolution , is a form of artificial intelligence that uses evolutionary algorithm s to generate artificial neural network s ( ANN ) , parameters , topology and rules. and evolutionary robotics .", "prompt_labels": "Neuroevolution(O) ,(O) or(O) neuro-evolution(O) ,(O) is(O) a(O) form(O) of(O) artificial(B-field) intelligence(I-field) that(O) uses(O) evolutionary(B-algorithm) algorithm(I-algorithm) s(O) to(O) generate(O) artificial(B-algorithm) neural(I-algorithm) network(I-algorithm) s(O) ((O) ANN(B-algorithm) )(O) ,(O) parameters(O) ,(O) topology(O) and(O) rules.(O) and(O) evolutionary(B-algorithm) robotics(I-algorithm) .(O)"}}
{"id": "135", "dataset": "crossner_ai", "split": "test", "label_list": ["task", "algorithm", "organization", "location", "metric", "person", "conference", "country", "field", "product", "researcher", "university", "programming language"], "instance": {"id": "135", "words": ["Two", "shallow", "approaches", "used", "to", "train", "and", "then", "disambiguate", "are", "Naive", "Bayes", "classifier", "and", "decision", "trees", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, algorithm, organization, location, metric, person, conference, country, field, product, researcher, university, programming language and O.\nSentence: Two shallow approaches used to train and then disambiguate are Naive Bayes classifier and decision trees .", "prompt_labels": "Two(O) shallow(O) approaches(O) used(O) to(O) train(O) and(O) then(O) disambiguate(O) are(O) Naive(B-algorithm) Bayes(I-algorithm) classifier(I-algorithm) and(O) decision(B-algorithm) trees(I-algorithm) .(O)"}}
{"id": "104", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "researcher", "task", "programming language", "algorithm", "metric", "conference", "country", "person", "product", "location", "organization", "field"], "instance": {"id": "104", "words": [",", "Ltd.", "in", "Thailand", ";", "Komatsu", "(", "Shanghai", ")", "Ltd.", "in", "1996", "in", "Shanghai", ",", "China", ";", "Industrial", "Power", "Alliance", "Ltd.", "in", "Japan", ",", "a", "joint", "venture", "with", "Cummins", ",", "in", "1998", ";", "L", "&", "T-Komatsu", "Limited", "in", "India", "in", "1998", "(", "shares", "sold", "in", "2013", ")", ";", "and", "Komatsu", "Brasil", "International", "Ltda.", "in", "Brazil", "in", "1998", "."], "labels": ["O", "O", "O", "B-country", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-location", "O", "B-country", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-country", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-country", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, researcher, task, programming language, algorithm, metric, conference, country, person, product, location, organization, field and O.\nSentence: , Ltd. in Thailand ; Komatsu ( Shanghai ) Ltd. in 1996 in Shanghai , China ; Industrial Power Alliance Ltd. in Japan , a joint venture with Cummins , in 1998 ; L & T-Komatsu Limited in India in 1998 ( shares sold in 2013 ) ; and Komatsu Brasil International Ltda. in Brazil in 1998 .", "prompt_labels": ",(O) Ltd.(O) in(O) Thailand(B-country) ;(O) Komatsu(B-organization) ((I-organization) Shanghai(I-organization) )(I-organization) Ltd.(I-organization) in(O) 1996(O) in(O) Shanghai(B-location) ,(O) China(B-country) ;(O) Industrial(B-organization) Power(I-organization) Alliance(I-organization) Ltd.(I-organization) in(O) Japan(B-country) ,(O) a(O) joint(O) venture(O) with(O) Cummins(B-organization) ,(O) in(O) 1998(O) ;(O) L(B-organization) &(I-organization) T-Komatsu(I-organization) Limited(I-organization) in(O) India(B-country) in(O) 1998(O) ((O) shares(O) sold(O) in(O) 2013(O) )(O) ;(O) and(O) Komatsu(B-organization) Brasil(I-organization) International(I-organization) Ltda.(I-organization) in(O) Brazil(B-country) in(O) 1998(O) .(O)"}}
{"id": "94", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "conference", "task", "country", "location", "person", "field", "product", "algorithm", "university", "programming language", "metric", "organization"], "instance": {"id": "94", "words": ["In", "2009", ",", "experts", "attended", "a", "conference", "hosted", "by", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "(", "AAAI", ")", "to", "discuss", "whether", "computers", "and", "robots", "might", "be", "able", "to", "acquire", "any", "autonomy", ",", "and", "how", "much", "these", "abilities", "might", "pose", "a", "threat", "or", "hazard", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, conference, task, country, location, person, field, product, algorithm, university, programming language, metric, organization and O.\nSentence: In 2009 , experts attended a conference hosted by the Association for the Advancement of Artificial Intelligence ( AAAI ) to discuss whether computers and robots might be able to acquire any autonomy , and how much these abilities might pose a threat or hazard .", "prompt_labels": "In(O) 2009(O) ,(O) experts(O) attended(O) a(O) conference(O) hosted(O) by(O) the(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) ((O) AAAI(B-conference) )(O) to(O) discuss(O) whether(O) computers(O) and(O) robots(O) might(O) be(O) able(O) to(O) acquire(O) any(O) autonomy(O) ,(O) and(O) how(O) much(O) these(O) abilities(O) might(O) pose(O) a(O) threat(O) or(O) hazard(O) .(O)"}}
{"id": "368", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "university", "country", "programming language", "algorithm", "conference", "product", "task", "organization", "researcher", "field", "metric", "person"], "instance": {"id": "368", "words": ["An", "eigenface", "(", "The", "approach", "of", "using", "eigenfaces", "for", "Facial", "recognition", "system", "was", "developed", "by", "Sirovich", "and", "Kirby", "(", "1987", ")", "and", "used", "by", "Matthew", "Turk", "and", "Alex", "Pentland", "in", "face", "classification", ".", "Turk", ",", "Matthew", "A", "and", "Pentland", ",", "Alex", "P.", "Face", "recognition", "using", "eigenfaces", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "B-researcher", "O", "B-researcher", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-task", "I-task", "O", "B-researcher", "I-researcher", "I-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "I-researcher", "I-researcher", "B-task", "I-task", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, university, country, programming language, algorithm, conference, product, task, organization, researcher, field, metric, person and O.\nSentence: An eigenface ( The approach of using eigenfaces for Facial recognition system was developed by Sirovich and Kirby ( 1987 ) and used by Matthew Turk and Alex Pentland in face classification . Turk , Matthew A and Pentland , Alex P. Face recognition using eigenfaces .", "prompt_labels": "An(O) eigenface(O) ((O) The(O) approach(O) of(O) using(O) eigenfaces(O) for(O) Facial(B-product) recognition(I-product) system(I-product) was(O) developed(O) by(O) Sirovich(B-researcher) and(O) Kirby(B-researcher) ((O) 1987(O) )(O) and(O) used(O) by(O) Matthew(B-researcher) Turk(I-researcher) and(O) Alex(B-researcher) Pentland(I-researcher) in(O) face(B-task) classification(I-task) .(O) Turk(B-researcher) ,(I-researcher) Matthew(I-researcher) A(I-researcher) and(O) Pentland(B-researcher) ,(I-researcher) Alex(I-researcher) P.(I-researcher) Face(B-task) recognition(I-task) using(O) eigenfaces(O) .(O)"}}
{"id": "78", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "metric", "product", "university", "field", "person", "task", "programming language", "researcher", "conference", "country", "organization", "algorithm"], "instance": {"id": "78", "words": ["Similarly", ",", "investigators", "sometimes", "report", "the", "FALSE", "Positive", "Rate", "(", "FPR", ")", "as", "well", "as", "the", "FALSE", "Negative", "Rate", "(", "FNR", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, metric, product, university, field, person, task, programming language, researcher, conference, country, organization, algorithm and O.\nSentence: Similarly , investigators sometimes report the FALSE Positive Rate ( FPR ) as well as the FALSE Negative Rate ( FNR ) .", "prompt_labels": "Similarly(O) ,(O) investigators(O) sometimes(O) report(O) the(O) FALSE(B-metric) Positive(I-metric) Rate(I-metric) ((O) FPR(B-metric) )(O) as(O) well(O) as(O) the(O) FALSE(B-metric) Negative(I-metric) Rate(I-metric) ((O) FNR(B-metric) )(O) .(O)"}}
{"id": "401", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "conference", "location", "researcher", "person", "metric", "field", "algorithm", "programming language", "task", "product", "university", "organization"], "instance": {"id": "401", "words": ["It", "is", "covered", "by", "American", "National", "Standards", "Institute", "/", "NISO", "standard", "Z39.50", ",", "and", "International", "Organization", "for", "Standardization", "standard", "23950", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, conference, location, researcher, person, metric, field, algorithm, programming language, task, product, university, organization and O.\nSentence: It is covered by American National Standards Institute / NISO standard Z39.50 , and International Organization for Standardization standard 23950 .", "prompt_labels": "It(O) is(O) covered(O) by(O) American(O) National(O) Standards(O) Institute(O) /(O) NISO(O) standard(O) Z39.50(O) ,(O) and(O) International(O) Organization(O) for(O) Standardization(O) standard(O) 23950(O) .(O)"}}
{"id": "377", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "location", "conference", "person", "organization", "product", "country", "programming language", "field", "algorithm", "researcher", "metric", "task"], "instance": {"id": "377", "words": ["Such", "applications", "should", "streamline", "the", "call", "flows", ",", "minimize", "prompts", ",", "eliminate", "unnecessary", "iterations", "and", "allow", "elaborate", "mixed", "initiative", "dialog", "system", ",", "which", "enable", "callers", "to", "enter", "several", "pieces", "of", "information", "in", "a", "single", "utterance", "and", "in", "any", "order", "or", "combination", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, location, conference, person, organization, product, country, programming language, field, algorithm, researcher, metric, task and O.\nSentence: Such applications should streamline the call flows , minimize prompts , eliminate unnecessary iterations and allow elaborate mixed initiative dialog system , which enable callers to enter several pieces of information in a single utterance and in any order or combination .", "prompt_labels": "Such(O) applications(O) should(O) streamline(O) the(O) call(O) flows(O) ,(O) minimize(O) prompts(O) ,(O) eliminate(O) unnecessary(O) iterations(O) and(O) allow(O) elaborate(O) mixed(B-product) initiative(I-product) dialog(I-product) system(I-product) ,(O) which(O) enable(O) callers(O) to(O) enter(O) several(O) pieces(O) of(O) information(O) in(O) a(O) single(O) utterance(O) and(O) in(O) any(O) order(O) or(O) combination(O) .(O)"}}
{"id": "237", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "metric", "programming language", "location", "task", "country", "researcher", "algorithm", "university", "conference", "product", "organization", "person"], "instance": {"id": "237", "words": ["The", "first", "USA", "edition", "of", "Campus", "Party", "will", "take", "place", "from", "20", "to", "22", "of", "August", "at", "TCF", "Center", "in", "Detroit", ",", "Michigan", "."], "labels": ["O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, metric, programming language, location, task, country, researcher, algorithm, university, conference, product, organization, person and O.\nSentence: The first USA edition of Campus Party will take place from 20 to 22 of August at TCF Center in Detroit , Michigan .", "prompt_labels": "The(O) first(O) USA(B-conference) edition(I-conference) of(I-conference) Campus(I-conference) Party(I-conference) will(O) take(O) place(O) from(O) 20(O) to(O) 22(O) of(O) August(O) at(O) TCF(B-location) Center(I-location) in(O) Detroit(B-location) ,(O) Michigan(B-location) .(O)"}}
{"id": "340", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "researcher", "university", "organization", "metric", "person", "country", "programming language", "location", "conference", "task", "field", "algorithm"], "instance": {"id": "340", "words": ["An", "alternative", "to", "the", "use", "of", "the", "definitions", "is", "to", "consider", "general", "word-sense", "relatedness", "and", "to", "compute", "the", "similarity", "of", "each", "pair", "of", "word", "senses", "based", "on", "a", "given", "lexical", "knowledge", "base", "such", "as", "WordNet", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, researcher, university, organization, metric, person, country, programming language, location, conference, task, field, algorithm and O.\nSentence: An alternative to the use of the definitions is to consider general word-sense relatedness and to compute the similarity of each pair of word senses based on a given lexical knowledge base such as WordNet .", "prompt_labels": "An(O) alternative(O) to(O) the(O) use(O) of(O) the(O) definitions(O) is(O) to(O) consider(O) general(O) word-sense(O) relatedness(O) and(O) to(O) compute(O) the(O) similarity(O) of(O) each(O) pair(O) of(O) word(O) senses(O) based(O) on(O) a(O) given(O) lexical(O) knowledge(O) base(O) such(O) as(O) WordNet(B-product) .(O)"}}
{"id": "210", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "university", "product", "conference", "field", "organization", "programming language", "task", "country", "person", "location", "researcher", "algorithm"], "instance": {"id": "210", "words": ["This", "field", "of", "computer", "science", "developed", "in", "the", "1950s", "at", "academic", "institutions", "such", "as", "the", "MIT", "A.I.", "Lab", ",", "originally", "as", "a", "branch", "of", "artificial", "intelligence", "and", "robotics", "."], "labels": ["O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, university, product, conference, field, organization, programming language, task, country, person, location, researcher, algorithm and O.\nSentence: This field of computer science developed in the 1950s at academic institutions such as the MIT A.I. Lab , originally as a branch of artificial intelligence and robotics .", "prompt_labels": "This(O) field(O) of(O) computer(B-field) science(I-field) developed(O) in(O) the(O) 1950s(O) at(O) academic(O) institutions(O) such(O) as(O) the(O) MIT(B-organization) A.I.(I-organization) Lab(I-organization) ,(O) originally(O) as(O) a(O) branch(O) of(O) artificial(B-field) intelligence(I-field) and(O) robotics(B-field) .(O)"}}
{"id": "33", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "task", "field", "product", "metric", "conference", "algorithm", "researcher", "country", "programming language", "university", "location", "person"], "instance": {"id": "33", "words": ["Johnson-Laird", "is", "a", "Fellow", "of", "the", "American", "Philosophical", "Society", ",", "a", "Fellow", "of", "the", "Royal", "Society", ",", "a", "Fellow", "of", "the", "British", "Academy", ",", "a", "William", "James", "Fellow", "of", "the", "Association", "for", "Psychological", "Science", ",", "and", "a", "Fellow", "of", "the", "Cognitive", "Science", "Society", "."], "labels": ["B-researcher", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, task, field, product, metric, conference, algorithm, researcher, country, programming language, university, location, person and O.\nSentence: Johnson-Laird is a Fellow of the American Philosophical Society , a Fellow of the Royal Society , a Fellow of the British Academy , a William James Fellow of the Association for Psychological Science , and a Fellow of the Cognitive Science Society .", "prompt_labels": "Johnson-Laird(B-researcher) is(O) a(O) Fellow(O) of(O) the(O) American(B-organization) Philosophical(I-organization) Society(I-organization) ,(O) a(O) Fellow(O) of(O) the(O) Royal(B-organization) Society(I-organization) ,(O) a(O) Fellow(O) of(O) the(O) British(B-organization) Academy(I-organization) ,(O) a(O) William(B-researcher) James(I-researcher) Fellow(O) of(O) the(O) Association(B-organization) for(I-organization) Psychological(I-organization) Science(I-organization) ,(O) and(O) a(O) Fellow(O) of(O) the(O) Cognitive(B-organization) Science(I-organization) Society(I-organization) .(O)"}}
{"id": "268", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "task", "product", "person", "metric", "organization", "field", "programming language", "researcher", "country", "university", "algorithm", "location"], "instance": {"id": "268", "words": ["They", "demonstrated", "its", "performance", "on", "a", "number", "of", "problems", "of", "interest", "to", "the", "machine", "learning", "community", ",", "including", "handwriting", "recognition", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, task, product, person, metric, organization, field, programming language, researcher, country, university, algorithm, location and O.\nSentence: They demonstrated its performance on a number of problems of interest to the machine learning community , including handwriting recognition .", "prompt_labels": "They(O) demonstrated(O) its(O) performance(O) on(O) a(O) number(O) of(O) problems(O) of(O) interest(O) to(O) the(O) machine(B-field) learning(I-field) community(O) ,(O) including(O) handwriting(B-task) recognition(I-task) .(O)"}}
{"id": "1", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "algorithm", "university", "programming language", "organization", "country", "person", "product", "location", "researcher", "field", "task", "conference"], "instance": {"id": "1", "words": ["Finally", ",", "every", "other", "year", ",", "ELRA", "organizes", "a", "major", "conference", "LREC", ",", "the", "International", "Language", "Resources", "and", "Evaluation", "Conference", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-conference", "O", "O", "O", "O", "B-conference", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, algorithm, university, programming language, organization, country, person, product, location, researcher, field, task, conference and O.\nSentence: Finally , every other year , ELRA organizes a major conference LREC , the International Language Resources and Evaluation Conference .", "prompt_labels": "Finally(O) ,(O) every(O) other(O) year(O) ,(O) ELRA(B-conference) organizes(O) a(O) major(O) conference(O) LREC(B-conference) ,(O) the(O) International(B-conference) Language(I-conference) Resources(I-conference) and(I-conference) Evaluation(I-conference) Conference(I-conference) .(O)"}}
{"id": "283", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "university", "researcher", "product", "programming language", "location", "task", "field", "metric", "country", "conference", "organization", "person"], "instance": {"id": "283", "words": ["VAL", "was", "one", "of", "the", "first", "robot", "languages", "and", "was", "used", "in", "Unimate", "robots", "."], "labels": ["B-programming language", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, university, researcher, product, programming language, location, task, field, metric, country, conference, organization, person and O.\nSentence: VAL was one of the first robot languages and was used in Unimate robots .", "prompt_labels": "VAL(B-programming language) was(O) one(O) of(O) the(O) first(O) robot(O) languages(O) and(O) was(O) used(O) in(O) Unimate(B-product) robots(I-product) .(O)"}}
{"id": "311", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "researcher", "conference", "task", "algorithm", "country", "metric", "product", "location", "organization", "person", "programming language", "field"], "instance": {"id": "311", "words": ["To", "extend", "SVM", "to", "cases", "in", "which", "the", "data", "are", "not", "linearly", "separable", ",", "we", "introduce", "the", "loss", "function", ","], "labels": ["O", "O", "B-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, researcher, conference, task, algorithm, country, metric, product, location, organization, person, programming language, field and O.\nSentence: To extend SVM to cases in which the data are not linearly separable , we introduce the loss function ,", "prompt_labels": "To(O) extend(O) SVM(B-algorithm) to(O) cases(O) in(O) which(O) the(O) data(O) are(O) not(O) linearly(O) separable(O) ,(O) we(O) introduce(O) the(O) loss(O) function(O) ,(O)"}}
{"id": "202", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "university", "organization", "person", "field", "researcher", "algorithm", "country", "location", "programming language", "metric", "task", "conference"], "instance": {"id": "202", "words": ["Time-inhomogeneous", "hidden", "Bernoulli", "model", "(", "TI-HBM", ")", "is", "an", "alternative", "to", "hidden", "Markov", "model", "(", "HMM", ")", "for", "automatic", "speech", "recognition", "."], "labels": ["B-algorithm", "I-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "B-task", "I-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, university, organization, person, field, researcher, algorithm, country, location, programming language, metric, task, conference and O.\nSentence: Time-inhomogeneous hidden Bernoulli model ( TI-HBM ) is an alternative to hidden Markov model ( HMM ) for automatic speech recognition .", "prompt_labels": "Time-inhomogeneous(B-algorithm) hidden(I-algorithm) Bernoulli(I-algorithm) model(I-algorithm) ((O) TI-HBM(B-algorithm) )(O) is(O) an(O) alternative(O) to(O) hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) ((O) HMM(B-algorithm) )(O) for(O) automatic(B-task) speech(I-task) recognition(I-task) .(O)"}}
{"id": "16", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "location", "person", "field", "product", "metric", "task", "university", "country", "researcher", "algorithm", "programming language", "organization"], "instance": {"id": "16", "words": ["Engelberger", "'s", "most", "famous", "co-invention", ",", "the", "Unimate", "industrial", "robotic", "arm", ",", "was", "among", "the", "first", "inductees", "into", "the", "Robot", "Hall", "of", "Fame", "in", "2003", "."], "labels": ["B-researcher", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, location, person, field, product, metric, task, university, country, researcher, algorithm, programming language, organization and O.\nSentence: Engelberger 's most famous co-invention , the Unimate industrial robotic arm , was among the first inductees into the Robot Hall of Fame in 2003 .", "prompt_labels": "Engelberger(B-researcher) 's(O) most(O) famous(O) co-invention(O) ,(O) the(O) Unimate(B-product) industrial(I-product) robotic(I-product) arm(I-product) ,(O) was(O) among(O) the(O) first(O) inductees(O) into(O) the(O) Robot(B-location) Hall(I-location) of(I-location) Fame(I-location) in(O) 2003(O) .(O)"}}
{"id": "170", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "field", "researcher", "conference", "task", "product", "university", "country", "organization", "person", "programming language", "location", "algorithm"], "instance": {"id": "170", "words": ["LSI", "helps", "overcome", "synonymy", "by", "increasing", "recall", ",", "one", "of", "the", "most", "problematic", "constraints", "of", "Boolean", "keyword", "queries", "and", "vector", "space", "models", "."], "labels": ["B-task", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, field, researcher, conference, task, product, university, country, organization, person, programming language, location, algorithm and O.\nSentence: LSI helps overcome synonymy by increasing recall , one of the most problematic constraints of Boolean keyword queries and vector space models .", "prompt_labels": "LSI(B-task) helps(O) overcome(O) synonymy(O) by(O) increasing(O) recall(B-metric) ,(O) one(O) of(O) the(O) most(O) problematic(O) constraints(O) of(O) Boolean(B-algorithm) keyword(I-algorithm) queries(I-algorithm) and(O) vector(B-algorithm) space(I-algorithm) models(I-algorithm) .(O)"}}
{"id": "320", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "field", "metric", "researcher", "conference", "product", "country", "location", "task", "university", "algorithm", "programming language", "organization"], "instance": {"id": "320", "words": ["Miller", "wrote", "several", "books", "and", "directed", "the", "development", "of", "WordNet", ",", "an", "online", "word-linkage", "database", "usable", "by", "computer", "programs", "."], "labels": ["B-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, field, metric, researcher, conference, product, country, location, task, university, algorithm, programming language, organization and O.\nSentence: Miller wrote several books and directed the development of WordNet , an online word-linkage database usable by computer programs .", "prompt_labels": "Miller(B-researcher) wrote(O) several(O) books(O) and(O) directed(O) the(O) development(O) of(O) WordNet(B-product) ,(O) an(O) online(O) word-linkage(O) database(O) usable(O) by(O) computer(O) programs(O) .(O)"}}
{"id": "119", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "product", "programming language", "country", "conference", "organization", "university", "person", "field", "task", "researcher", "metric", "algorithm"], "instance": {"id": "119", "words": ["Gerd", "Gigerenzer", "(", "born", "September", "3", ",", "1947", ",", "Wallersdorf", ",", "Germany", ")", "is", "a", "Germany", "psychologist", "who", "has", "studied", "the", "use", "of", "bounded", "rationality", "and", "heuristic", "s", "in", "decision", "making", "."], "labels": ["B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-country", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, product, programming language, country, conference, organization, university, person, field, task, researcher, metric, algorithm and O.\nSentence: Gerd Gigerenzer ( born September 3 , 1947 , Wallersdorf , Germany ) is a Germany psychologist who has studied the use of bounded rationality and heuristic s in decision making .", "prompt_labels": "Gerd(B-researcher) Gigerenzer(I-researcher) ((O) born(O) September(O) 3(O) ,(O) 1947(O) ,(O) Wallersdorf(B-location) ,(O) Germany(B-country) )(O) is(O) a(O) Germany(B-country) psychologist(O) who(O) has(O) studied(O) the(O) use(O) of(O) bounded(B-algorithm) rationality(I-algorithm) and(O) heuristic(B-algorithm) s(O) in(O) decision(B-task) making(I-task) .(O)"}}
{"id": "2", "dataset": "crossner_ai", "split": "test", "label_list": ["task", "organization", "field", "country", "conference", "researcher", "person", "product", "metric", "location", "programming language", "university", "algorithm"], "instance": {"id": "2", "words": ["The", "task", "is", "usually", "to", "derive", "the", "maximum", "likelihood", "estimate", "of", "the", "parameters", "of", "the", "HMM", "given", "the", "of", "output", "sequences", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "B-algorithm", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, organization, field, country, conference, researcher, person, product, metric, location, programming language, university, algorithm and O.\nSentence: The task is usually to derive the maximum likelihood estimate of the parameters of the HMM given the of output sequences .", "prompt_labels": "The(O) task(O) is(O) usually(O) to(O) derive(O) the(O) maximum(B-algorithm) likelihood(I-algorithm) estimate(I-algorithm) of(O) the(O) parameters(O) of(O) the(O) HMM(B-algorithm) given(O) the(O) of(O) output(O) sequences(O) .(O)"}}
{"id": "272", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "field", "metric", "person", "programming language", "location", "organization", "university", "task", "researcher", "algorithm", "conference", "country"], "instance": {"id": "272", "words": ["Much", "of", "the", "confusion", "between", "these", "two", "research", "communities", "(", "which", "do", "often", "have", "separate", "conferences", "and", "separate", "journals", ",", "ECML", "PKDD", "being", "a", "major", "exception", ")", "comes", "from", "the", "basic", "assumptions", "they", "work", "with", ":", "in", "machine", "learning", ",", "performance", "is", "usually", "evaluated", "with", "respect", "to", "the", "ability", "to", "reproduce", "known", "knowledge", ",", "while", "in", "knowledge", "discovery", "and", "data", "mining", "(", "KDD", ")", "the", "key", "task", "is", "the", "discovery", "of", "previously", "unknown", "knowledge", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, field, metric, person, programming language, location, organization, university, task, researcher, algorithm, conference, country and O.\nSentence: Much of the confusion between these two research communities ( which do often have separate conferences and separate journals , ECML PKDD being a major exception ) comes from the basic assumptions they work with : in machine learning , performance is usually evaluated with respect to the ability to reproduce known knowledge , while in knowledge discovery and data mining ( KDD ) the key task is the discovery of previously unknown knowledge .", "prompt_labels": "Much(O) of(O) the(O) confusion(O) between(O) these(O) two(O) research(O) communities(O) ((O) which(O) do(O) often(O) have(O) separate(O) conferences(O) and(O) separate(O) journals(O) ,(O) ECML(B-conference) PKDD(I-conference) being(O) a(O) major(O) exception(O) )(O) comes(O) from(O) the(O) basic(O) assumptions(O) they(O) work(O) with(O) :(O) in(O) machine(B-field) learning(I-field) ,(O) performance(O) is(O) usually(O) evaluated(O) with(O) respect(O) to(O) the(O) ability(O) to(O) reproduce(O) known(O) knowledge(O) ,(O) while(O) in(O) knowledge(B-conference) discovery(I-conference) and(I-conference) data(I-conference) mining(I-conference) ((O) KDD(B-conference) )(O) the(O) key(O) task(O) is(O) the(O) discovery(O) of(O) previously(O) unknown(O) knowledge(O) .(O)"}}
{"id": "142", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "location", "task", "country", "person", "conference", "university", "researcher", "algorithm", "metric", "product", "programming language", "organization"], "instance": {"id": "142", "words": ["An", "unrelated", "but", "commonly", "used", "combination", "of", "basic", "statistics", "from", "information", "retrieval", "is", "the", "F-score", ",", "being", "a", "(", "possibly", "weighted", ")", "harmonic", "mean", "of", "recall", "and", "precision", "where", "recall", "=", "sensitivity", "=", "TRUE", "positive", "rate", ",", "but", "specificity", "and", "precision", "are", "totally", "different", "measures", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "B-metric", "O", "B-metric", "O", "B-metric", "O", "B-metric", "I-metric", "I-metric", "O", "O", "B-metric", "O", "B-metric", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, location, task, country, person, conference, university, researcher, algorithm, metric, product, programming language, organization and O.\nSentence: An unrelated but commonly used combination of basic statistics from information retrieval is the F-score , being a ( possibly weighted ) harmonic mean of recall and precision where recall = sensitivity = TRUE positive rate , but specificity and precision are totally different measures .", "prompt_labels": "An(O) unrelated(O) but(O) commonly(O) used(O) combination(O) of(O) basic(O) statistics(O) from(O) information(B-task) retrieval(I-task) is(O) the(O) F-score(B-metric) ,(O) being(O) a(O) ((O) possibly(O) weighted(O) )(O) harmonic(O) mean(O) of(O) recall(B-metric) and(O) precision(B-metric) where(O) recall(B-metric) =(O) sensitivity(B-metric) =(O) TRUE(B-metric) positive(I-metric) rate(I-metric) ,(O) but(O) specificity(B-metric) and(O) precision(B-metric) are(O) totally(O) different(O) measures(O) .(O)"}}
{"id": "160", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "algorithm", "product", "field", "country", "researcher", "location", "conference", "organization", "task", "university", "metric", "person"], "instance": {"id": "160", "words": ["Important", "journals", "include", "the", "IEEE", "Transactions", "on", "Speech", "and", "Audio", "Processing", "(", "later", "renamed", "IEEE", "Transactions", "on", "Audio", ",", "Speech", "and", "Language", "Processing", "and", "since", "Sept", "2014", "renamed", "IEEE", "/", "ACM", "Transactions", "on", "Audio", ",", "Speech", "and", "Language", "Processing", "-", "after", "merging", "with", "an", "ACM", "publication", ")", ",", "Computer", "Speech", "and", "Language", ",", "and", "Speech", "Communication", "."], "labels": ["O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "B-conference", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "O", "O", "B-conference", "I-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, algorithm, product, field, country, researcher, location, conference, organization, task, university, metric, person and O.\nSentence: Important journals include the IEEE Transactions on Speech and Audio Processing ( later renamed IEEE Transactions on Audio , Speech and Language Processing and since Sept 2014 renamed IEEE / ACM Transactions on Audio , Speech and Language Processing - after merging with an ACM publication ) , Computer Speech and Language , and Speech Communication .", "prompt_labels": "Important(O) journals(O) include(O) the(O) IEEE(B-conference) Transactions(I-conference) on(I-conference) Speech(I-conference) and(I-conference) Audio(I-conference) Processing(I-conference) ((O) later(O) renamed(O) IEEE(B-conference) Transactions(I-conference) on(I-conference) Audio(I-conference) ,(I-conference) Speech(I-conference) and(I-conference) Language(I-conference) Processing(I-conference) and(O) since(O) Sept(O) 2014(O) renamed(O) IEEE(B-conference) /(I-conference) ACM(I-conference) Transactions(I-conference) on(I-conference) Audio(I-conference) ,(I-conference) Speech(I-conference) and(I-conference) Language(I-conference) Processing(I-conference) -(O) after(O) merging(O) with(O) an(O) ACM(B-conference) publication(O) )(O) ,(O) Computer(B-conference) Speech(I-conference) and(I-conference) Language(I-conference) ,(O) and(O) Speech(B-conference) Communication(I-conference) .(O)"}}
{"id": "324", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "metric", "algorithm", "university", "person", "product", "field", "task", "country", "organization", "location", "researcher", "programming language"], "instance": {"id": "324", "words": ["In", "1960", ",", "Devol", "personally", "sold", "the", "first", "Unimate", "robot", ",", "which", "was", "shipped", "in", "1961", "to", "General", "Motors", "."], "labels": ["O", "O", "O", "B-person", "O", "O", "O", "O", "B-product", "B-product", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, metric, algorithm, university, person, product, field, task, country, organization, location, researcher, programming language and O.\nSentence: In 1960 , Devol personally sold the first Unimate robot , which was shipped in 1961 to General Motors .", "prompt_labels": "In(O) 1960(O) ,(O) Devol(B-person) personally(O) sold(O) the(O) first(O) Unimate(B-product) robot(B-product) ,(O) which(O) was(O) shipped(O) in(O) 1961(O) to(O) General(B-organization) Motors(I-organization) .(O)"}}
{"id": "184", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "person", "university", "task", "product", "organization", "metric", "programming language", "field", "researcher", "conference", "location", "algorithm"], "instance": {"id": "184", "words": ["Supervised", "neural", "networks", "that", "use", "a", "mean", "squared", "error", "(", "MSE", ")", "cost", "function", "can", "use", "formal", "statistical", "methods", "to", "determine", "the", "confidence", "of", "the", "trained", "model", "."], "labels": ["B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, university, task, product, organization, metric, programming language, field, researcher, conference, location, algorithm and O.\nSentence: Supervised neural networks that use a mean squared error ( MSE ) cost function can use formal statistical methods to determine the confidence of the trained model .", "prompt_labels": "Supervised(B-algorithm) neural(I-algorithm) networks(I-algorithm) that(O) use(O) a(O) mean(B-metric) squared(I-metric) error(I-metric) ((O) MSE(B-metric) )(O) cost(O) function(O) can(O) use(O) formal(O) statistical(O) methods(O) to(O) determine(O) the(O) confidence(O) of(O) the(O) trained(O) model(O) .(O)"}}
{"id": "285", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "metric", "programming language", "organization", "location", "person", "country", "product", "task", "field", "university", "algorithm", "researcher"], "instance": {"id": "285", "words": ["Categorization", "tasks", "in", "which", "no", "labels", "are", "supplied", "are", "referred", "to", "as", "unsupervised", "classification", ",", "unsupervised", "learning", ",", "Cluster", "analysis", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "B-field", "I-field", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, metric, programming language, organization, location, person, country, product, task, field, university, algorithm, researcher and O.\nSentence: Categorization tasks in which no labels are supplied are referred to as unsupervised classification , unsupervised learning , Cluster analysis .", "prompt_labels": "Categorization(O) tasks(O) in(O) which(O) no(O) labels(O) are(O) supplied(O) are(O) referred(O) to(O) as(O) unsupervised(B-task) classification(I-task) ,(O) unsupervised(B-field) learning(I-field) ,(O) Cluster(B-task) analysis(I-task) .(O)"}}
{"id": "356", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "conference", "algorithm", "university", "product", "researcher", "programming language", "location", "task", "metric", "person", "country", "organization"], "instance": {"id": "356", "words": ["Research", "stagnated", "after", "machine", "learning", "research", "by", "Marvin", "Minsky", "and", "Seymour", "Papert", "(", "1969", ")", ","], "labels": ["O", "O", "O", "B-field", "I-field", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, conference, algorithm, university, product, researcher, programming language, location, task, metric, person, country, organization and O.\nSentence: Research stagnated after machine learning research by Marvin Minsky and Seymour Papert ( 1969 ) ,", "prompt_labels": "Research(O) stagnated(O) after(O) machine(B-field) learning(I-field) research(O) by(O) Marvin(B-researcher) Minsky(I-researcher) and(O) Seymour(B-researcher) Papert(I-researcher) ((O) 1969(O) )(O) ,(O)"}}
{"id": "409", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "programming language", "location", "conference", "task", "researcher", "country", "metric", "organization", "field", "university", "product", "person"], "instance": {"id": "409", "words": ["The", "special", "case", "of", "linear", "support-vector", "machines", "can", "be", "solved", "more", "efficiently", "by", "the", "same", "kind", "of", "algorithms", "to", "optimize", "its", "close", "cousin", ",", "logistic", "regression", ";", "this", "class", "of", "algorithms", "includes", "Stochastic", "gradient", "descent", "(", "e.g.", ",", "PEGASOS", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "B-algorithm", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, programming language, location, conference, task, researcher, country, metric, organization, field, university, product, person and O.\nSentence: The special case of linear support-vector machines can be solved more efficiently by the same kind of algorithms to optimize its close cousin , logistic regression ; this class of algorithms includes Stochastic gradient descent ( e.g. , PEGASOS ) .", "prompt_labels": "The(O) special(O) case(O) of(O) linear(O) support-vector(B-algorithm) machines(I-algorithm) can(O) be(O) solved(O) more(O) efficiently(O) by(O) the(O) same(O) kind(O) of(O) algorithms(O) to(O) optimize(O) its(O) close(O) cousin(O) ,(O) logistic(B-algorithm) regression(I-algorithm) ;(O) this(O) class(O) of(O) algorithms(O) includes(O) Stochastic(B-algorithm) gradient(I-algorithm) descent(I-algorithm) ((O) e.g.(O) ,(O) PEGASOS(B-algorithm) )(O) .(O)"}}
{"id": "19", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "person", "university", "task", "researcher", "country", "programming language", "algorithm", "conference", "location", "metric", "organization", "product"], "instance": {"id": "19", "words": ["A", "confusion", "matrix", "or", "matching", "matrix", "is", "often", "used", "as", "a", "tool", "to", "validate", "the", "accuracy", "of", "k", "-NN", "classification", "."], "labels": ["O", "B-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, person, university, task, researcher, country, programming language, algorithm, conference, location, metric, organization, product and O.\nSentence: A confusion matrix or matching matrix is often used as a tool to validate the accuracy of k -NN classification .", "prompt_labels": "A(O) confusion(B-metric) matrix(I-metric) or(O) matching(O) matrix(O) is(O) often(O) used(O) as(O) a(O) tool(O) to(O) validate(O) the(O) accuracy(B-metric) of(O) k(B-algorithm) -NN(I-algorithm) classification(I-algorithm) .(O)"}}
{"id": "345", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "person", "algorithm", "metric", "conference", "task", "product", "researcher", "field", "university", "programming language", "organization", "country"], "instance": {"id": "345", "words": [",", "typically", "providing", "bindings", "to", "languages", "such", "as", "Python", ",", "C", "+", "+", ",", "Java", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-programming language", "O", "B-programming language", "I-programming language", "I-programming language", "O", "B-programming language", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, algorithm, metric, conference, task, product, researcher, field, university, programming language, organization, country and O.\nSentence: , typically providing bindings to languages such as Python , C + + , Java ) .", "prompt_labels": ",(O) typically(O) providing(O) bindings(O) to(O) languages(O) such(O) as(O) Python(B-programming language) ,(O) C(B-programming language) +(I-programming language) +(I-programming language) ,(O) Java(B-programming language) )(O) .(O)"}}
{"id": "100", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "university", "person", "metric", "algorithm", "task", "organization", "conference", "field", "researcher", "product", "programming language", "country"], "instance": {"id": "100", "words": ["A", "deep", "learning", "based", "approach", "to", "MT", ",", "neural", "machine", "translation", "has", "made", "rapid", "progress", "in", "recent", "years", ",", "and", "Google", "has", "announced", "its", "translation", "services", "are", "now", "using", "this", "technology", "in", "preference", "to", "its", "previous", "statistical", "methods", "."], "labels": ["O", "B-field", "I-field", "O", "O", "O", "B-task", "O", "B-task", "I-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, university, person, metric, algorithm, task, organization, conference, field, researcher, product, programming language, country and O.\nSentence: A deep learning based approach to MT , neural machine translation has made rapid progress in recent years , and Google has announced its translation services are now using this technology in preference to its previous statistical methods .", "prompt_labels": "A(O) deep(B-field) learning(I-field) based(O) approach(O) to(O) MT(B-task) ,(O) neural(B-task) machine(I-task) translation(I-task) has(O) made(O) rapid(O) progress(O) in(O) recent(O) years(O) ,(O) and(O) Google(B-organization) has(O) announced(O) its(O) translation(O) services(O) are(O) now(O) using(O) this(O) technology(O) in(O) preference(O) to(O) its(O) previous(O) statistical(O) methods(O) .(O)"}}
{"id": "207", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "programming language", "task", "algorithm", "product", "field", "researcher", "person", "location", "university", "country", "organization", "conference"], "instance": {"id": "207", "words": ["However", ",", "usage", "only", "became", "widespread", "in", "2005", "when", "Navneet", "Dalal", "and", "Bill", "Triggs", ",", "researchers", "for", "the", "French", "National", "Institute", "for", "Research", "in", "Computer", "Science", "and", "Automation", "(", "INRIA", ")", ",", "presented", "their", "supplementary", "work", "on", "HOG", "descriptors", "at", "the", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, programming language, task, algorithm, product, field, researcher, person, location, university, country, organization, conference and O.\nSentence: However , usage only became widespread in 2005 when Navneet Dalal and Bill Triggs , researchers for the French National Institute for Research in Computer Science and Automation ( INRIA ) , presented their supplementary work on HOG descriptors at the Conference on Computer Vision and Pattern Recognition ( CVPR ) .", "prompt_labels": "However(O) ,(O) usage(O) only(O) became(O) widespread(O) in(O) 2005(O) when(O) Navneet(B-researcher) Dalal(I-researcher) and(O) Bill(B-researcher) Triggs(I-researcher) ,(O) researchers(O) for(O) the(O) French(B-organization) National(I-organization) Institute(I-organization) for(I-organization) Research(I-organization) in(I-organization) Computer(I-organization) Science(I-organization) and(I-organization) Automation(I-organization) ((O) INRIA(B-organization) )(O) ,(O) presented(O) their(O) supplementary(O) work(O) on(O) HOG(B-algorithm) descriptors(I-algorithm) at(O) the(O) Conference(B-conference) on(I-conference) Computer(I-conference) Vision(I-conference) and(I-conference) Pattern(I-conference) Recognition(I-conference) ((O) CVPR(B-conference) )(O) .(O)"}}
{"id": "299", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "location", "metric", "programming language", "country", "product", "university", "task", "researcher", "field", "person", "conference", "algorithm"], "instance": {"id": "299", "words": ["However", ",", "floating-point", "numbers", "have", "only", "a", "certain", "amount", "of", "mathematical", "precision", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, metric, programming language, country, product, university, task, researcher, field, person, conference, algorithm and O.\nSentence: However , floating-point numbers have only a certain amount of mathematical precision .", "prompt_labels": "However(O) ,(O) floating-point(O) numbers(O) have(O) only(O) a(O) certain(O) amount(O) of(O) mathematical(O) precision(O) .(O)"}}
{"id": "265", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "task", "university", "country", "programming language", "organization", "product", "person", "conference", "field", "location", "researcher", "metric"], "instance": {"id": "265", "words": ["A", "widely", "used", "type", "of", "composition", "is", "the", "nonlinear", "weighted", "sum", ",", "where", "math", "\\", "textstyle", "f", "(", "x", ")", "=", "K", "\\", "left", "(", "\\", "sum", "_", "i", "w", "_", "i", "g", "_", "i", "(", "x", ")", "\\", "right", ")", "/", "math", ",", "where", "math", "\\", "textstyle", "K", "/", "math", "(", "commonly", "referred", "to", "as", "the", "activation", "function", ")", "is", "some", "predefined", "function", ",", "such", "as", "the", "hyperbolic", "tangent", ",", "sigmoid", "function", ",", "softmax", "function", ",", "or", "rectifier", "function", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "O", "O", "B-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, task, university, country, programming language, organization, product, person, conference, field, location, researcher, metric and O.\nSentence: A widely used type of composition is the nonlinear weighted sum , where math \\ textstyle f ( x ) = K \\ left ( \\ sum _ i w _ i g _ i ( x ) \\ right ) / math , where math \\ textstyle K / math ( commonly referred to as the activation function ) is some predefined function , such as the hyperbolic tangent , sigmoid function , softmax function , or rectifier function .", "prompt_labels": "A(O) widely(O) used(O) type(O) of(O) composition(O) is(O) the(O) nonlinear(B-algorithm) weighted(I-algorithm) sum(I-algorithm) ,(O) where(O) math(O) \\(O) textstyle(O) f(O) ((O) x(O) )(O) =(O) K(O) \\(O) left(O) ((O) \\(O) sum(O) _(O) i(O) w(O) _(O) i(O) g(O) _(O) i(O) ((O) x(O) )(O) \\(O) right(O) )(O) /(O) math(O) ,(O) where(O) math(O) \\(O) textstyle(O) K(O) /(O) math(O) ((O) commonly(O) referred(O) to(O) as(O) the(O) activation(O) function(O) )(O) is(O) some(O) predefined(O) function(O) ,(O) such(O) as(O) the(O) hyperbolic(B-algorithm) tangent(I-algorithm) ,(O) sigmoid(B-algorithm) function(I-algorithm) ,(O) softmax(B-algorithm) function(I-algorithm) ,(O) or(O) rectifier(B-algorithm) function(I-algorithm) .(O)"}}
{"id": "342", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "country", "task", "metric", "university", "person", "programming language", "conference", "field", "researcher", "organization", "location", "algorithm"], "instance": {"id": "342", "words": ["In", "data", "mining", "and", "statistics", ",", "hierarchical", "clustering", "(", "also", "called", "hierarchical", "cluster", "analysis", "or", "HCA", ")", "is", "a", "method", "of", "cluster", "analysis", "which", "seeks", "to", "build", "a", "hierarchy", "of", "clusters", "."], "labels": ["O", "B-field", "I-field", "O", "B-field", "O", "B-task", "I-task", "O", "O", "O", "B-task", "I-task", "I-task", "O", "B-task", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, country, task, metric, university, person, programming language, conference, field, researcher, organization, location, algorithm and O.\nSentence: In data mining and statistics , hierarchical clustering ( also called hierarchical cluster analysis or HCA ) is a method of cluster analysis which seeks to build a hierarchy of clusters .", "prompt_labels": "In(O) data(B-field) mining(I-field) and(O) statistics(B-field) ,(O) hierarchical(B-task) clustering(I-task) ((O) also(O) called(O) hierarchical(B-task) cluster(I-task) analysis(I-task) or(O) HCA(B-task) )(O) is(O) a(O) method(O) of(O) cluster(B-task) analysis(I-task) which(O) seeks(O) to(O) build(O) a(O) hierarchy(O) of(O) clusters(O) .(O)"}}
{"id": "174", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "task", "field", "algorithm", "country", "product", "university", "programming language", "person", "location", "researcher", "organization", "metric"], "instance": {"id": "174", "words": ["Expectation-maximization", "algorithm", "s", "may", "be", "employed", "to", "calculate", "approximate", "maximum", "likelihood", "estimates", "of", "unknown", "state-space", "parameters", "within", "minimum-variance", "filters", "and", "smoothers", "."], "labels": ["B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, task, field, algorithm, country, product, university, programming language, person, location, researcher, organization, metric and O.\nSentence: Expectation-maximization algorithm s may be employed to calculate approximate maximum likelihood estimates of unknown state-space parameters within minimum-variance filters and smoothers .", "prompt_labels": "Expectation-maximization(B-algorithm) algorithm(I-algorithm) s(O) may(O) be(O) employed(O) to(O) calculate(O) approximate(O) maximum(B-algorithm) likelihood(I-algorithm) estimates(I-algorithm) of(O) unknown(O) state-space(O) parameters(O) within(O) minimum-variance(O) filters(O) and(O) smoothers(O) .(O)"}}
{"id": "337", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "product", "location", "person", "country", "algorithm", "conference", "task", "university", "organization", "metric", "researcher", "field"], "instance": {"id": "337", "words": ["The", "IJCAI", "Award", "for", "Research", "Excellence", "is", "a", "biannual", "award", "given", "at", "the", "IJCAI", "conference", "to", "researcher", "in", "artificial", "intelligence", "as", "a", "recognition", "of", "excellence", "of", "their", "career", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, product, location, person, country, algorithm, conference, task, university, organization, metric, researcher, field and O.\nSentence: The IJCAI Award for Research Excellence is a biannual award given at the IJCAI conference to researcher in artificial intelligence as a recognition of excellence of their career .", "prompt_labels": "The(O) IJCAI(O) Award(O) for(O) Research(O) Excellence(O) is(O) a(O) biannual(O) award(O) given(O) at(O) the(O) IJCAI(B-conference) conference(O) to(O) researcher(O) in(O) artificial(B-field) intelligence(I-field) as(O) a(O) recognition(O) of(O) excellence(O) of(O) their(O) career(O) .(O)"}}
{"id": "393", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "university", "person", "organization", "country", "conference", "location", "product", "researcher", "task", "field", "programming language", "metric"], "instance": {"id": "393", "words": ["Denso", "Wave", "is", "a", "subsidiary", "that", "produces", "automatic", "identification", "products", "(", "bar-code", "reader", "s", "and", "related", "products", ")", ",", "industrial", "robot", "s", "and", "programmable", "logic", "controller", "s", "."], "labels": ["B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O", "B-product", "I-product", "I-product", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, university, person, organization, country, conference, location, product, researcher, task, field, programming language, metric and O.\nSentence: Denso Wave is a subsidiary that produces automatic identification products ( bar-code reader s and related products ) , industrial robot s and programmable logic controller s .", "prompt_labels": "Denso(B-organization) Wave(I-organization) is(O) a(O) subsidiary(O) that(O) produces(O) automatic(O) identification(O) products(O) ((O) bar-code(B-product) reader(I-product) s(O) and(O) related(O) products(O) )(O) ,(O) industrial(B-product) robot(I-product) s(O) and(O) programmable(B-product) logic(I-product) controller(I-product) s(O) .(O)"}}
{"id": "392", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "task", "conference", "field", "country", "product", "person", "programming language", "university", "organization", "algorithm", "location", "metric"], "instance": {"id": "392", "words": ["To", "illustrate", "the", "basic", "principles", "of", "bagging", ",", "below", "is", "an", "analysis", "on", "the", "relationship", "between", "ozone", "and", "temperature", "(", "data", "from", "Rousseeuw", "and", "Leroy", "(", "1986", ")", ",", "analysis", "done", "in", "R", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "O", "B-researcher", "O", "O", "O", "O", "O", "O", "O", "B-programming language", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, task, conference, field, country, product, person, programming language, university, organization, algorithm, location, metric and O.\nSentence: To illustrate the basic principles of bagging , below is an analysis on the relationship between ozone and temperature ( data from Rousseeuw and Leroy ( 1986 ) , analysis done in R ) .", "prompt_labels": "To(O) illustrate(O) the(O) basic(O) principles(O) of(O) bagging(O) ,(O) below(O) is(O) an(O) analysis(O) on(O) the(O) relationship(O) between(O) ozone(O) and(O) temperature(O) ((O) data(O) from(O) Rousseeuw(B-researcher) and(O) Leroy(B-researcher) ((O) 1986(O) )(O) ,(O) analysis(O) done(O) in(O) R(B-programming language) )(O) .(O)"}}
{"id": "188", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "programming language", "task", "product", "person", "organization", "location", "algorithm", "university", "metric", "conference", "researcher", "country"], "instance": {"id": "188", "words": ["John", "Ireland", ",", "Joanne", "Dru", "and", "Macdonald", "Carey", "starred", "in", "the", "Jack", "Broder", "color", "production", "Hannah", "Lee", ",", "which", "premiered", "June", "19", ",", "1953", "."], "labels": ["B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, programming language, task, product, person, organization, location, algorithm, university, metric, conference, researcher, country and O.\nSentence: John Ireland , Joanne Dru and Macdonald Carey starred in the Jack Broder color production Hannah Lee , which premiered June 19 , 1953 .", "prompt_labels": "John(B-person) Ireland(I-person) ,(O) Joanne(B-person) Dru(I-person) and(O) Macdonald(B-person) Carey(I-person) starred(O) in(O) the(O) Jack(B-person) Broder(I-person) color(O) production(O) Hannah(O) Lee(O) ,(O) which(O) premiered(O) June(O) 19(O) ,(O) 1953(O) .(O)"}}
{"id": "400", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "organization", "country", "location", "university", "field", "conference", "task", "product", "programming language", "researcher", "metric", "algorithm"], "instance": {"id": "400", "words": ["Crawler-friendly", "Web", "Servers", ",", "and", "it", "integrates", "the", "features", "of", "sitemaps", "and", "RSS", "feeds", "into", "a", "decentralized", "mechanism", "for", "computational", "biologists", "and", "bio-informaticians", "to", "openly", "broadcast", "and", "retrieve", "meta-data", "about", "biomedical", "resources", "."], "labels": ["B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, country, location, university, field, conference, task, product, programming language, researcher, metric, algorithm and O.\nSentence: Crawler-friendly Web Servers , and it integrates the features of sitemaps and RSS feeds into a decentralized mechanism for computational biologists and bio-informaticians to openly broadcast and retrieve meta-data about biomedical resources .", "prompt_labels": "Crawler-friendly(B-product) Web(I-product) Servers(I-product) ,(O) and(O) it(O) integrates(O) the(O) features(O) of(O) sitemaps(O) and(O) RSS(O) feeds(O) into(O) a(O) decentralized(O) mechanism(O) for(O) computational(O) biologists(O) and(O) bio-informaticians(O) to(O) openly(O) broadcast(O) and(O) retrieve(O) meta-data(O) about(O) biomedical(O) resources(O) .(O)"}}
{"id": "307", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "conference", "location", "person", "programming language", "algorithm", "country", "organization", "field", "university", "product", "task", "researcher"], "instance": {"id": "307", "words": ["Software", "packages", "like", "MATLAB", ",", "GNU", "Octave", ",", "Scilab", ",", "and", "SciPy", "provide", "convenient", "ways", "to", "apply", "these", "different", "methods", "."], "labels": ["O", "O", "O", "B-product", "O", "B-programming language", "I-programming language", "O", "B-programming language", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, conference, location, person, programming language, algorithm, country, organization, field, university, product, task, researcher and O.\nSentence: Software packages like MATLAB , GNU Octave , Scilab , and SciPy provide convenient ways to apply these different methods .", "prompt_labels": "Software(O) packages(O) like(O) MATLAB(B-product) ,(O) GNU(B-programming language) Octave(I-programming language) ,(O) Scilab(B-programming language) ,(O) and(O) SciPy(B-product) provide(O) convenient(O) ways(O) to(O) apply(O) these(O) different(O) methods(O) .(O)"}}
{"id": "70", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "programming language", "person", "metric", "organization", "task", "researcher", "field", "location", "country", "university", "algorithm", "conference"], "instance": {"id": "70", "words": ["This", "is", "an", "example", "implementation", "in", "Python", ":"], "labels": ["O", "O", "O", "O", "O", "O", "B-programming language", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, programming language, person, metric, organization, task, researcher, field, location, country, university, algorithm, conference and O.\nSentence: This is an example implementation in Python :", "prompt_labels": "This(O) is(O) an(O) example(O) implementation(O) in(O) Python(B-programming language) :(O)"}}
{"id": "102", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "programming language", "organization", "algorithm", "researcher", "conference", "task", "country", "field", "product", "university", "person", "location"], "instance": {"id": "102", "words": ["Face", "detection", "is", "used", "in", "biometrics", ",", "often", "as", "a", "part", "of", "(", "or", "together", "with", ")", "a", "facial", "recognition", "system", "."], "labels": ["B-task", "I-task", "O", "O", "O", "B-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, programming language, organization, algorithm, researcher, conference, task, country, field, product, university, person, location and O.\nSentence: Face detection is used in biometrics , often as a part of ( or together with ) a facial recognition system .", "prompt_labels": "Face(B-task) detection(I-task) is(O) used(O) in(O) biometrics(B-field) ,(O) often(O) as(O) a(O) part(O) of(O) ((O) or(O) together(O) with(O) )(O) a(O) facial(B-product) recognition(I-product) system(I-product) .(O)"}}
{"id": "90", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "organization", "metric", "algorithm", "researcher", "conference", "country", "programming language", "person", "field", "task", "product", "university"], "instance": {"id": "90", "words": ["Classification", "can", "be", "thought", "of", "as", "two", "separate", "problems", "-", "binary", "classification", "and", "multiclass", "classification", "."], "labels": ["B-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, metric, algorithm, researcher, conference, country, programming language, person, field, task, product, university and O.\nSentence: Classification can be thought of as two separate problems - binary classification and multiclass classification .", "prompt_labels": "Classification(B-task) can(O) be(O) thought(O) of(O) as(O) two(O) separate(O) problems(O) -(O) binary(B-task) classification(I-task) and(O) multiclass(B-task) classification(I-task) .(O)"}}
{"id": "396", "dataset": "crossner_ai", "split": "test", "label_list": ["task", "metric", "organization", "country", "product", "programming language", "university", "person", "algorithm", "researcher", "location", "conference", "field"], "instance": {"id": "396", "words": ["The", "Audio", "Engineering", "Society", "recommends", "48", "kHz", "sampling", "rate", "for", "most", "applications", "but", "gives", "recognition", "to", "44.1", "kHz", "for", "Compact", "Disc", "(", "CD", ")", "and", "other", "consumer", "uses", ",", "32", "kHz", "for", "transmission-related", "applications", ",", "and", "96", "kHz", "for", "higher", "bandwidth", "or", "relaxed", "anti-aliasing", "filter", "ing", "."], "labels": ["O", "B-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, metric, organization, country, product, programming language, university, person, algorithm, researcher, location, conference, field and O.\nSentence: The Audio Engineering Society recommends 48 kHz sampling rate for most applications but gives recognition to 44.1 kHz for Compact Disc ( CD ) and other consumer uses , 32 kHz for transmission-related applications , and 96 kHz for higher bandwidth or relaxed anti-aliasing filter ing .", "prompt_labels": "The(O) Audio(B-conference) Engineering(I-conference) Society(I-conference) recommends(O) 48(O) kHz(O) sampling(O) rate(O) for(O) most(O) applications(O) but(O) gives(O) recognition(O) to(O) 44.1(O) kHz(O) for(O) Compact(O) Disc(O) ((O) CD(O) )(O) and(O) other(O) consumer(O) uses(O) ,(O) 32(O) kHz(O) for(O) transmission-related(O) applications(O) ,(O) and(O) 96(O) kHz(O) for(O) higher(O) bandwidth(O) or(O) relaxed(O) anti-aliasing(O) filter(O) ing(O) .(O)"}}
{"id": "4", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "task", "programming language", "person", "country", "researcher", "field", "location", "metric", "conference", "algorithm", "product", "organization"], "instance": {"id": "4", "words": ["Troponymy", "is", "one", "of", "the", "possible", "relations", "between", "verb", "s", "in", "the", "semantic", "network", "of", "the", "WordNet", "database", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, task, programming language, person, country, researcher, field, location, metric, conference, algorithm, product, organization and O.\nSentence: Troponymy is one of the possible relations between verb s in the semantic network of the WordNet database .", "prompt_labels": "Troponymy(O) is(O) one(O) of(O) the(O) possible(O) relations(O) between(O) verb(O) s(O) in(O) the(O) semantic(O) network(O) of(O) the(O) WordNet(B-product) database(I-product) .(O)"}}
{"id": "412", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "conference", "researcher", "university", "location", "programming language", "field", "organization", "person", "metric", "task", "product", "algorithm"], "instance": {"id": "412", "words": ["In", "particular", ",", "his", "research", "focused", "on", "areas", "such", "as", "text", "mining", "(", "extraction", ",", "categorization", ",", "novelty", "detection", ")", "and", "in", "new", "theoretical", "frameworks", "such", "as", "a", "unified", "utility-based", "theory", "bridging", "information", "retrieval", ",", "Automatic", "summarization", ",", "free-text", "Question", "Answering", "and", "related", "tasks", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-task", "O", "B-task", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "I-task", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, conference, researcher, university, location, programming language, field, organization, person, metric, task, product, algorithm and O.\nSentence: In particular , his research focused on areas such as text mining ( extraction , categorization , novelty detection ) and in new theoretical frameworks such as a unified utility-based theory bridging information retrieval , Automatic summarization , free-text Question Answering and related tasks .", "prompt_labels": "In(O) particular(O) ,(O) his(O) research(O) focused(O) on(O) areas(O) such(O) as(O) text(B-field) mining(I-field) ((O) extraction(B-task) ,(O) categorization(B-task) ,(O) novelty(B-task) detection(I-task) )(O) and(O) in(O) new(O) theoretical(O) frameworks(O) such(O) as(O) a(O) unified(O) utility-based(O) theory(O) bridging(O) information(B-task) retrieval(I-task) ,(O) Automatic(B-task) summarization(I-task) ,(O) free-text(B-task) Question(I-task) Answering(I-task) and(O) related(O) tasks(O) .(O)"}}
{"id": "405", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "metric", "task", "conference", "algorithm", "university", "person", "country", "field", "researcher", "product", "organization", "location"], "instance": {"id": "405", "words": ["Examples", "include", "Salford", "Systems", "CART", "(", "which", "licensed", "the", "proprietary", "code", "of", "the", "original", "CART", "authors", ")", ",", "IBM", "SPSS", "Modeler", ",", "RapidMiner", ",", "SAS", "Enterprise", "Miner", ",", "Matlab", ",", "R", "(", "an", "open-source", "software", "environment", "for", "statistical", "computing", ",", "which", "includes", "several", "CART", "implementations", "such", "as", "rpart", ",", "party", "and", "randomForest", "packages", ")", ",", "Weka", "(", "a", "free", "and", "open-source", "data-mining", "suite", ",", "contains", "many", "decision", "tree", "algorithms", ")", ",", "Orange", ",", "KNIME", ",", "Microsoft", "SQL", "Server", "programming", "language", ")", "."], "labels": ["O", "O", "B-organization", "I-organization", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "O", "O", "B-organization", "B-product", "I-product", "O", "B-product", "O", "B-product", "I-product", "I-product", "O", "B-product", "O", "B-programming language", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "B-product", "O", "O", "O", "B-algorithm", "O", "B-algorithm", "O", "B-algorithm", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "B-task", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "B-product", "O", "B-product", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, metric, task, conference, algorithm, university, person, country, field, researcher, product, organization, location and O.\nSentence: Examples include Salford Systems CART ( which licensed the proprietary code of the original CART authors ) , IBM SPSS Modeler , RapidMiner , SAS Enterprise Miner , Matlab , R ( an open-source software environment for statistical computing , which includes several CART implementations such as rpart , party and randomForest packages ) , Weka ( a free and open-source data-mining suite , contains many decision tree algorithms ) , Orange , KNIME , Microsoft SQL Server programming language ) .", "prompt_labels": "Examples(O) include(O) Salford(B-organization) Systems(I-organization) CART(B-product) ((O) which(O) licensed(O) the(O) proprietary(O) code(O) of(O) the(O) original(O) CART(B-product) authors(O) )(O) ,(O) IBM(B-organization) SPSS(B-product) Modeler(I-product) ,(O) RapidMiner(B-product) ,(O) SAS(B-product) Enterprise(I-product) Miner(I-product) ,(O) Matlab(B-product) ,(O) R(B-programming language) ((O) an(O) open-source(O) software(O) environment(O) for(O) statistical(B-field) computing(I-field) ,(O) which(O) includes(O) several(O) CART(B-product) implementations(O) such(O) as(O) rpart(B-algorithm) ,(O) party(B-algorithm) and(O) randomForest(B-algorithm) packages(O) )(O) ,(O) Weka(B-product) ((O) a(O) free(O) and(O) open-source(O) data-mining(B-task) suite(O) ,(O) contains(O) many(O) decision(B-algorithm) tree(I-algorithm) algorithms(O) )(O) ,(O) Orange(B-product) ,(O) KNIME(B-product) ,(O) Microsoft(B-product) SQL(I-product) Server(I-product) programming(O) language(O) )(O) .(O)"}}
{"id": "47", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "task", "organization", "person", "algorithm", "programming language", "university", "researcher", "country", "field", "metric", "conference", "product"], "instance": {"id": "47", "words": ["NSA", "Bethesda", "is", "responsible", "for", "base", "operational", "support", "for", "its", "major", "tenant", ",", "the", "Walter", "Reed", "National", "Military", "Medical", "Center", "."], "labels": ["B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, task, organization, person, algorithm, programming language, university, researcher, country, field, metric, conference, product and O.\nSentence: NSA Bethesda is responsible for base operational support for its major tenant , the Walter Reed National Military Medical Center .", "prompt_labels": "NSA(B-organization) Bethesda(I-organization) is(O) responsible(O) for(O) base(O) operational(O) support(O) for(O) its(O) major(O) tenant(O) ,(O) the(O) Walter(B-organization) Reed(I-organization) National(I-organization) Military(I-organization) Medical(I-organization) Center(I-organization) .(O)"}}
{"id": "249", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "researcher", "university", "algorithm", "organization", "field", "country", "task", "person", "product", "conference", "location", "programming language"], "instance": {"id": "249", "words": ["The", "NER", "model", "is", "one", "of", "a", "number", "of", "methods", "for", "determining", "the", "accuracy", "of", "live", "subtitles", "in", "television", "broadcasts", "and", "events", "that", "are", "produced", "using", "speech", "recognition", "."], "labels": ["O", "B-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, researcher, university, algorithm, organization, field, country, task, person, product, conference, location, programming language and O.\nSentence: The NER model is one of a number of methods for determining the accuracy of live subtitles in television broadcasts and events that are produced using speech recognition .", "prompt_labels": "The(O) NER(B-task) model(O) is(O) one(O) of(O) a(O) number(O) of(O) methods(O) for(O) determining(O) the(O) accuracy(B-metric) of(O) live(O) subtitles(O) in(O) television(O) broadcasts(O) and(O) events(O) that(O) are(O) produced(O) using(O) speech(B-task) recognition(I-task) .(O)"}}
{"id": "111", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "country", "task", "conference", "metric", "product", "person", "algorithm", "researcher", "organization", "location", "field", "programming language"], "instance": {"id": "111", "words": ["Pandora", "(", "also", "known", "as", "Pandora", "Media", "or", "Pandora", "Radio", ")", "is", "an", "American", "music", "streaming", "and", "automated", "Recommender", "system", "internet", "radio", "service", "powered", "by", "the", "Music", "Genome", "Project", "and", "headquartered", "in", "Oakland", ",", "California", "."], "labels": ["B-product", "O", "O", "O", "O", "B-product", "I-product", "O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, country, task, conference, metric, product, person, algorithm, researcher, organization, location, field, programming language and O.\nSentence: Pandora ( also known as Pandora Media or Pandora Radio ) is an American music streaming and automated Recommender system internet radio service powered by the Music Genome Project and headquartered in Oakland , California .", "prompt_labels": "Pandora(B-product) ((O) also(O) known(O) as(O) Pandora(B-product) Media(I-product) or(O) Pandora(B-product) Radio(I-product) )(O) is(O) an(O) American(O) music(O) streaming(O) and(O) automated(B-product) Recommender(I-product) system(I-product) internet(O) radio(O) service(O) powered(O) by(O) the(O) Music(O) Genome(O) Project(O) and(O) headquartered(O) in(O) Oakland(B-location) ,(O) California(B-location) .(O)"}}
{"id": "224", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "organization", "location", "university", "field", "algorithm", "country", "metric", "programming language", "task", "product", "researcher", "conference"], "instance": {"id": "224", "words": ["In", "2016", ",", "he", "was", "elected", "Fellow", "of", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, location, university, field, algorithm, country, metric, programming language, task, product, researcher, conference and O.\nSentence: In 2016 , he was elected Fellow of Association for the Advancement of Artificial Intelligence .", "prompt_labels": "In(O) 2016(O) ,(O) he(O) was(O) elected(O) Fellow(O) of(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) .(O)"}}
{"id": "430", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "university", "organization", "programming language", "researcher", "field", "task", "person", "algorithm", "metric", "location", "country", "conference"], "instance": {"id": "430", "words": ["Further", ",", "in", "the", "case", "of", "estimation", "based", "on", "a", "single", "sample", ",", "it", "demonstrates", "philosophical", "issues", "and", "possible", "misunderstandings", "in", "the", "use", "of", "maximum", "likelihood", "estimators", "and", "likelihood", "functions", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "I-metric", "I-metric", "I-metric", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, university, organization, programming language, researcher, field, task, person, algorithm, metric, location, country, conference and O.\nSentence: Further , in the case of estimation based on a single sample , it demonstrates philosophical issues and possible misunderstandings in the use of maximum likelihood estimators and likelihood functions .", "prompt_labels": "Further(O) ,(O) in(O) the(O) case(O) of(O) estimation(O) based(O) on(O) a(O) single(O) sample(O) ,(O) it(O) demonstrates(O) philosophical(O) issues(O) and(O) possible(O) misunderstandings(O) in(O) the(O) use(O) of(O) maximum(B-metric) likelihood(I-metric) estimators(I-metric) and(I-metric) likelihood(I-metric) functions(I-metric) .(O)"}}
{"id": "61", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "metric", "organization", "task", "algorithm", "product", "field", "programming language", "researcher", "location", "country", "university", "conference"], "instance": {"id": "61", "words": ["For", "instance", ",", "the", "term", "neural", "machine", "translation", "(", "NMT", ")", "emphasizes", "the", "fact", "that", "deep", "learning-based", "approaches", "to", "machine", "translation", "directly", "learn", "sequence-to-sequence", "transformations", ",", "obviating", "the", "need", "for", "intermediate", "steps", "such", "as", "word", "alignment", "and", "language", "modeling", "that", "was", "used", "in", "statistical", "machine", "translation", "(", "SMT", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-task", "I-task", "I-task", "O", "B-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "O", "O", "O", "B-task", "I-task", "I-task", "O", "B-task", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, metric, organization, task, algorithm, product, field, programming language, researcher, location, country, university, conference and O.\nSentence: For instance , the term neural machine translation ( NMT ) emphasizes the fact that deep learning-based approaches to machine translation directly learn sequence-to-sequence transformations , obviating the need for intermediate steps such as word alignment and language modeling that was used in statistical machine translation ( SMT ) .", "prompt_labels": "For(O) instance(O) ,(O) the(O) term(O) neural(B-task) machine(I-task) translation(I-task) ((O) NMT(B-task) )(O) emphasizes(O) the(O) fact(O) that(O) deep(O) learning-based(O) approaches(O) to(O) machine(B-task) translation(I-task) directly(O) learn(O) sequence-to-sequence(O) transformations(O) ,(O) obviating(O) the(O) need(O) for(O) intermediate(O) steps(O) such(O) as(O) word(B-task) alignment(I-task) and(O) language(B-task) modeling(I-task) that(O) was(O) used(O) in(O) statistical(B-task) machine(I-task) translation(I-task) ((O) SMT(B-task) )(O) .(O)"}}
{"id": "373", "dataset": "crossner_ai", "split": "test", "label_list": ["task", "location", "country", "conference", "researcher", "algorithm", "organization", "field", "metric", "person", "product", "university", "programming language"], "instance": {"id": "373", "words": ["There", "are", "also", "many", "programming", "libraries", "that", "contain", "neural", "network", "functionality", "and", "that", "can", "be", "used", "in", "custom", "implementations", "(", "such", "as", "TensorFlow", ",", "Theano", ",", "etc", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "B-product", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, location, country, conference, researcher, algorithm, organization, field, metric, person, product, university, programming language and O.\nSentence: There are also many programming libraries that contain neural network functionality and that can be used in custom implementations ( such as TensorFlow , Theano , etc .", "prompt_labels": "There(O) are(O) also(O) many(O) programming(O) libraries(O) that(O) contain(O) neural(B-algorithm) network(I-algorithm) functionality(O) and(O) that(O) can(O) be(O) used(O) in(O) custom(O) implementations(O) ((O) such(O) as(O) TensorFlow(B-product) ,(O) Theano(B-product) ,(O) etc(O) .(O)"}}
{"id": "296", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "location", "researcher", "university", "programming language", "person", "country", "task", "product", "algorithm", "organization", "field", "conference"], "instance": {"id": "296", "words": ["Subsequently", ",", "a", "similar", "GPU-based", "CNN", "by", "Alex", "Krizhevsky", "et", "al.", "won", "the", "ImageNet", "Large", "Scale", "Visual", "Recognition", "Challenge", "2012", "."], "labels": ["O", "O", "O", "O", "O", "B-algorithm", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, location, researcher, university, programming language, person, country, task, product, algorithm, organization, field, conference and O.\nSentence: Subsequently , a similar GPU-based CNN by Alex Krizhevsky et al. won the ImageNet Large Scale Visual Recognition Challenge 2012 .", "prompt_labels": "Subsequently(O) ,(O) a(O) similar(O) GPU-based(O) CNN(B-algorithm) by(O) Alex(B-researcher) Krizhevsky(I-researcher) et(O) al.(O) won(O) the(O) ImageNet(B-conference) Large(I-conference) Scale(I-conference) Visual(I-conference) Recognition(I-conference) Challenge(I-conference) 2012(I-conference) .(O)"}}
{"id": "221", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "researcher", "metric", "country", "conference", "location", "programming language", "algorithm", "university", "task", "product", "person", "organization"], "instance": {"id": "221", "words": ["Reinforcement", "learning", ",", "due", "to", "its", "generality", ",", "is", "studied", "in", "many", "other", "disciplines", ",", "such", "as", "game", ",", "control", "theory", ",", "operations", "research", ",", "information", "theory", ",", "simulation-based", "optimization", ",", "multi-agent", "systems", ",", "swarm", "intelligence", ",", "statistics", "and", "genetic", "algorithm", "s", "."], "labels": ["B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "O", "B-algorithm", "I-algorithm", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, researcher, metric, country, conference, location, programming language, algorithm, university, task, product, person, organization and O.\nSentence: Reinforcement learning , due to its generality , is studied in many other disciplines , such as game , control theory , operations research , information theory , simulation-based optimization , multi-agent systems , swarm intelligence , statistics and genetic algorithm s .", "prompt_labels": "Reinforcement(B-field) learning(I-field) ,(O) due(O) to(O) its(O) generality(O) ,(O) is(O) studied(O) in(O) many(O) other(O) disciplines(O) ,(O) such(O) as(O) game(B-field) ,(O) control(B-field) theory(I-field) ,(O) operations(B-field) research(I-field) ,(O) information(B-field) theory(I-field) ,(O) simulation-based(B-field) optimization(I-field) ,(O) multi-agent(B-field) systems(I-field) ,(O) swarm(B-field) intelligence(I-field) ,(O) statistics(B-field) and(O) genetic(B-algorithm) algorithm(I-algorithm) s(O) .(O)"}}
{"id": "84", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "field", "person", "algorithm", "country", "conference", "researcher", "programming language", "task", "location", "organization", "product", "university"], "instance": {"id": "84", "words": ["The", "WaveNet", "model", "proposed", "in", "2016", "achieves", "great", "performance", "on", "speech", "quality", "."], "labels": ["O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, field, person, algorithm, country, conference, researcher, programming language, task, location, organization, product, university and O.\nSentence: The WaveNet model proposed in 2016 achieves great performance on speech quality .", "prompt_labels": "The(O) WaveNet(B-product) model(O) proposed(O) in(O) 2016(O) achieves(O) great(O) performance(O) on(O) speech(O) quality(O) .(O)"}}
{"id": "335", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "metric", "organization", "country", "location", "conference", "person", "task", "product", "researcher", "field", "algorithm", "programming language"], "instance": {"id": "335", "words": ["In", "the", "first", "published", "paper", "on", "CGs", ",", "John", "F.", "Sowa", "applied", "them", "to", "a", "wide", "range", "of", "topics", "in", "artificial", "intelligence", ",", "computer", "science", ",", "and", "cognitive", "science", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-field", "O", "B-researcher", "I-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, metric, organization, country, location, conference, person, task, product, researcher, field, algorithm, programming language and O.\nSentence: In the first published paper on CGs , John F. Sowa applied them to a wide range of topics in artificial intelligence , computer science , and cognitive science .", "prompt_labels": "In(O) the(O) first(O) published(O) paper(O) on(O) CGs(B-field) ,(O) John(B-researcher) F.(I-researcher) Sowa(I-researcher) applied(O) them(O) to(O) a(O) wide(O) range(O) of(O) topics(O) in(O) artificial(B-field) intelligence(I-field) ,(O) computer(B-field) science(I-field) ,(O) and(O) cognitive(B-field) science(I-field) .(O)"}}
{"id": "233", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "country", "location", "algorithm", "person", "task", "metric", "organization", "conference", "product", "researcher", "field", "university"], "instance": {"id": "233", "words": ["The", "gradient", "descent", "can", "take", "many", "iterations", "to", "compute", "a", "local", "minimum", "with", "a", "required", "accuracy", ",", "if", "the", "curvature", "in", "different", "directions", "is", "very", "different", "for", "the", "given", "function", "."], "labels": ["O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, country, location, algorithm, person, task, metric, organization, conference, product, researcher, field, university and O.\nSentence: The gradient descent can take many iterations to compute a local minimum with a required accuracy , if the curvature in different directions is very different for the given function .", "prompt_labels": "The(O) gradient(B-algorithm) descent(I-algorithm) can(O) take(O) many(O) iterations(O) to(O) compute(O) a(O) local(O) minimum(O) with(O) a(O) required(O) accuracy(B-metric) ,(O) if(O) the(O) curvature(O) in(O) different(O) directions(O) is(O) very(O) different(O) for(O) the(O) given(O) function(O) .(O)"}}
{"id": "52", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "location", "university", "field", "conference", "organization", "product", "metric", "algorithm", "researcher", "programming language", "country", "task"], "instance": {"id": "52", "words": ["In", "July", "2011", "the", "15th", "edition", "of", "Campus", "Party", "Spain", "will", "be", "held", "at", "the", "City", "of", "Arts", "and", "Sciences", "in", "Valencia", "."], "labels": ["O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, university, field, conference, organization, product, metric, algorithm, researcher, programming language, country, task and O.\nSentence: In July 2011 the 15th edition of Campus Party Spain will be held at the City of Arts and Sciences in Valencia .", "prompt_labels": "In(O) July(O) 2011(O) the(O) 15th(B-conference) edition(I-conference) of(I-conference) Campus(I-conference) Party(I-conference) Spain(I-conference) will(O) be(O) held(O) at(O) the(O) City(B-location) of(I-location) Arts(I-location) and(I-location) Sciences(I-location) in(O) Valencia(B-location) .(O)"}}
{"id": "336", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "task", "conference", "university", "product", "country", "researcher", "programming language", "person", "algorithm", "location", "field", "organization"], "instance": {"id": "336", "words": ["NIST", "also", "differs", "from", "BLEU", "in", "its", "calculation", "of", "the", "brevity", "penalty", ",", "insofar", "as", "small", "variations", "in", "translation", "length", "do", "not", "impact", "the", "overall", "score", "as", "much", "."], "labels": ["B-metric", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, task, conference, university, product, country, researcher, programming language, person, algorithm, location, field, organization and O.\nSentence: NIST also differs from BLEU in its calculation of the brevity penalty , insofar as small variations in translation length do not impact the overall score as much .", "prompt_labels": "NIST(B-metric) also(O) differs(O) from(O) BLEU(B-metric) in(O) its(O) calculation(O) of(O) the(O) brevity(O) penalty(O) ,(O) insofar(O) as(O) small(O) variations(O) in(O) translation(O) length(O) do(O) not(O) impact(O) the(O) overall(O) score(O) as(O) much(O) .(O)"}}
{"id": "80", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "conference", "product", "university", "person", "metric", "algorithm", "programming language", "location", "researcher", "field", "task", "country"], "instance": {"id": "80", "words": ["The", "Code", "of", "Ethics", "on", "Human", "Augmentation", ",", "which", "was", "originally", "introduced", "by", "Steve", "Mann", "in", "2004", "and", "refined", "with", "Ray", "Kurzweil", "and", "Marvin", "Minsky", "in", "2013", ",", "was", "ultimately", "ratified", "at", "the", "Virtual", "Reality", "Toronto", "conference", "on", "June", "25", ",", "2017", "."], "labels": ["O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, conference, product, university, person, metric, algorithm, programming language, location, researcher, field, task, country and O.\nSentence: The Code of Ethics on Human Augmentation , which was originally introduced by Steve Mann in 2004 and refined with Ray Kurzweil and Marvin Minsky in 2013 , was ultimately ratified at the Virtual Reality Toronto conference on June 25 , 2017 .", "prompt_labels": "The(O) Code(O) of(O) Ethics(O) on(O) Human(B-field) Augmentation(I-field) ,(O) which(O) was(O) originally(O) introduced(O) by(O) Steve(B-researcher) Mann(I-researcher) in(O) 2004(O) and(O) refined(O) with(O) Ray(B-researcher) Kurzweil(I-researcher) and(O) Marvin(B-researcher) Minsky(I-researcher) in(O) 2013(O) ,(O) was(O) ultimately(O) ratified(O) at(O) the(O) Virtual(B-conference) Reality(I-conference) Toronto(I-conference) conference(I-conference) on(O) June(O) 25(O) ,(O) 2017(O) .(O)"}}
{"id": "11", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "metric", "country", "algorithm", "product", "location", "person", "field", "conference", "university", "programming language", "organization", "task"], "instance": {"id": "11", "words": ["Typical", "text", "mining", "tasks", "include", "text", "categorization", ",", "text", "clustering", ",", "concept", "/", "entity", "extraction", ",", "production", "of", "granular", "taxonomies", ",", "sentiment", "analysis", ",", "document", "summarization", ",", "and", "entity", "relation", "modeling", "(", "i.e.", ",", "learning", "relations", "between", "named", "entity", "recognition", ")", "."], "labels": ["O", "B-field", "I-field", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "I-task", "I-task", "O", "B-task", "I-task", "I-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "O", "B-task", "I-task", "I-task", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "I-task", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, metric, country, algorithm, product, location, person, field, conference, university, programming language, organization, task and O.\nSentence: Typical text mining tasks include text categorization , text clustering , concept / entity extraction , production of granular taxonomies , sentiment analysis , document summarization , and entity relation modeling ( i.e. , learning relations between named entity recognition ) .", "prompt_labels": "Typical(O) text(B-field) mining(I-field) tasks(O) include(O) text(B-task) categorization(I-task) ,(O) text(B-task) clustering(I-task) ,(O) concept(B-task) /(I-task) entity(I-task) extraction(I-task) ,(O) production(B-task) of(I-task) granular(I-task) taxonomies(I-task) ,(O) sentiment(B-task) analysis(I-task) ,(O) document(B-task) summarization(I-task) ,(O) and(O) entity(B-task) relation(I-task) modeling(I-task) ((O) i.e.(O) ,(O) learning(O) relations(O) between(O) named(B-task) entity(I-task) recognition(I-task) )(O) .(O)"}}
{"id": "139", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "task", "location", "metric", "country", "researcher", "organization", "algorithm", "product", "person", "field", "programming language", "conference"], "instance": {"id": "139", "words": ["The", "term", "machine", "learning", "was", "coined", "in", "1959", "by", "Arthur", "Samuel", ",", "an", "American", "IBMer", "and", "pioneer", "in", "the", "field", "of", "computer", "gaming", "and", "artificial", "intelligence", "."], "labels": ["O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, task, location, metric, country, researcher, organization, algorithm, product, person, field, programming language, conference and O.\nSentence: The term machine learning was coined in 1959 by Arthur Samuel , an American IBMer and pioneer in the field of computer gaming and artificial intelligence .", "prompt_labels": "The(O) term(O) machine(B-field) learning(I-field) was(O) coined(O) in(O) 1959(O) by(O) Arthur(B-researcher) Samuel(I-researcher) ,(O) an(O) American(O) IBMer(O) and(O) pioneer(O) in(O) the(O) field(O) of(O) computer(B-field) gaming(I-field) and(O) artificial(B-field) intelligence(I-field) .(O)"}}
{"id": "218", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "algorithm", "country", "product", "metric", "programming language", "organization", "conference", "researcher", "task", "person", "field", "location"], "instance": {"id": "218", "words": ["For", "example", ",", "the", "ambiguity", "of", "'", "mouse", "'", "(", "animal", "or", "device", ")", "is", "not", "relevant", "in", "machine", "translation", ",", "but", "is", "relevant", "in", "information", "retrieval", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, algorithm, country, product, metric, programming language, organization, conference, researcher, task, person, field, location and O.\nSentence: For example , the ambiguity of ' mouse ' ( animal or device ) is not relevant in machine translation , but is relevant in information retrieval .", "prompt_labels": "For(O) example(O) ,(O) the(O) ambiguity(O) of(O) '(O) mouse(O) '(O) ((O) animal(O) or(O) device(O) )(O) is(O) not(O) relevant(O) in(O) machine(B-task) translation(I-task) ,(O) but(O) is(O) relevant(O) in(O) information(B-task) retrieval(I-task) .(O)"}}
{"id": "43", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "field", "person", "metric", "algorithm", "task", "conference", "programming language", "organization", "country", "location", "university", "product"], "instance": {"id": "43", "words": ["Skeletons", "are", "widely", "used", "in", "computer", "vision", ",", "image", "analysis", ",", "pattern", "recognition", "and", "digital", "image", "processing", "for", "purposes", "such", "as", "optical", "character", "recognition", ",", "fingerprint", "recognition", ",", "visual", "inspection", "or", "compression", "."], "labels": ["O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "I-field", "O", "O", "O", "O", "B-task", "I-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "I-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, field, person, metric, algorithm, task, conference, programming language, organization, country, location, university, product and O.\nSentence: Skeletons are widely used in computer vision , image analysis , pattern recognition and digital image processing for purposes such as optical character recognition , fingerprint recognition , visual inspection or compression .", "prompt_labels": "Skeletons(O) are(O) widely(O) used(O) in(O) computer(B-field) vision(I-field) ,(O) image(B-field) analysis(I-field) ,(O) pattern(B-field) recognition(I-field) and(O) digital(B-field) image(I-field) processing(I-field) for(O) purposes(O) such(O) as(O) optical(B-task) character(I-task) recognition(I-task) ,(O) fingerprint(B-task) recognition(I-task) ,(O) visual(B-task) inspection(I-task) or(I-task) compression(I-task) .(O)"}}
{"id": "199", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "location", "conference", "programming language", "metric", "researcher", "product", "algorithm", "person", "field", "task", "university", "country"], "instance": {"id": "199", "words": ["Modular", "Audio", "Recognition", "Framework", "(", "MARF", ")", "is", "an", "open-source", "research", "platform", "and", "a", "collection", "of", "voice", ",", "sound", ",", "speech", ",", "text", "and", "natural", "language", "processing", "(", "NLP", ")", "algorithm", "s", "written", "in", "Java", "and", "arranged", "into", "a", "modular", "and", "extensible", "framework", "that", "attempts", "to", "facilitate", "addition", "of", "new", "algorithm", "s", "."], "labels": ["B-product", "I-product", "I-product", "I-product", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "I-field", "O", "B-field", "O", "O", "O", "O", "O", "B-programming language", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, conference, programming language, metric, researcher, product, algorithm, person, field, task, university, country and O.\nSentence: Modular Audio Recognition Framework ( MARF ) is an open-source research platform and a collection of voice , sound , speech , text and natural language processing ( NLP ) algorithm s written in Java and arranged into a modular and extensible framework that attempts to facilitate addition of new algorithm s .", "prompt_labels": "Modular(B-product) Audio(I-product) Recognition(I-product) Framework(I-product) ((O) MARF(B-product) )(O) is(O) an(O) open-source(O) research(O) platform(O) and(O) a(O) collection(O) of(O) voice(O) ,(O) sound(O) ,(O) speech(O) ,(O) text(O) and(O) natural(B-field) language(I-field) processing(I-field) ((O) NLP(B-field) )(O) algorithm(O) s(O) written(O) in(O) Java(B-programming language) and(O) arranged(O) into(O) a(O) modular(O) and(O) extensible(O) framework(O) that(O) attempts(O) to(O) facilitate(O) addition(O) of(O) new(O) algorithm(O) s(O) .(O)"}}
{"id": "10", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "organization", "product", "location", "conference", "algorithm", "metric", "researcher", "field", "person", "programming language", "university", "task"], "instance": {"id": "10", "words": ["Rethink", "Robotics", "-", "founded", "by", "Rodney", "Brooks", ",", "previously", "with", "iRobot", "-", "introduced", "Baxter", "in", "September", "2012", ";", "as", "an", "industrial", "robot", "designed", "to", "safely", "interact", "with", "neighboring", "human", "workers", ",", "and", "be", "programmable", "for", "performing", "simple", "tasks", "."], "labels": ["B-organization", "I-organization", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "B-organization", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, product, location, conference, algorithm, metric, researcher, field, person, programming language, university, task and O.\nSentence: Rethink Robotics - founded by Rodney Brooks , previously with iRobot - introduced Baxter in September 2012 ; as an industrial robot designed to safely interact with neighboring human workers , and be programmable for performing simple tasks .", "prompt_labels": "Rethink(B-organization) Robotics(I-organization) -(O) founded(O) by(O) Rodney(B-researcher) Brooks(I-researcher) ,(O) previously(O) with(O) iRobot(B-organization) -(O) introduced(O) Baxter(B-product) in(O) September(O) 2012(O) ;(O) as(O) an(O) industrial(B-product) robot(I-product) designed(O) to(O) safely(O) interact(O) with(O) neighboring(O) human(O) workers(O) ,(O) and(O) be(O) programmable(O) for(O) performing(O) simple(O) tasks(O) .(O)"}}
{"id": "97", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "conference", "task", "person", "field", "organization", "programming language", "product", "country", "researcher", "university", "location", "metric"], "instance": {"id": "97", "words": ["The", "start-up", "was", "founded", "by", "Demis", "Hassabis", ",", "Shane", "Legg", "and", "Mustafa", "Suleyman", "in", "2010", "."], "labels": ["O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-person", "I-person", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, conference, task, person, field, organization, programming language, product, country, researcher, university, location, metric and O.\nSentence: The start-up was founded by Demis Hassabis , Shane Legg and Mustafa Suleyman in 2010 .", "prompt_labels": "The(O) start-up(O) was(O) founded(O) by(O) Demis(B-researcher) Hassabis(I-researcher) ,(O) Shane(B-researcher) Legg(I-researcher) and(O) Mustafa(B-person) Suleyman(I-person) in(O) 2010(O) .(O)"}}
{"id": "167", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "researcher", "university", "conference", "country", "location", "field", "task", "metric", "algorithm", "person", "organization", "product"], "instance": {"id": "167", "words": ["However", ",", "in", "the", "version", "of", "the", "metric", "used", "by", "NIST", "evaluations", "prior", "to", "2009", ",", "the", "shortest", "reference", "sentence", "had", "been", "used", "instead", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, researcher, university, conference, country, location, field, task, metric, algorithm, person, organization, product and O.\nSentence: However , in the version of the metric used by NIST evaluations prior to 2009 , the shortest reference sentence had been used instead .", "prompt_labels": "However(O) ,(O) in(O) the(O) version(O) of(O) the(O) metric(O) used(O) by(O) NIST(B-metric) evaluations(O) prior(O) to(O) 2009(O) ,(O) the(O) shortest(O) reference(O) sentence(O) had(O) been(O) used(O) instead(O) .(O)"}}
{"id": "254", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "field", "organization", "task", "product", "metric", "person", "conference", "algorithm", "country", "university", "researcher", "programming language"], "instance": {"id": "254", "words": ["In", "1971", "Terry", "Winograd", "developed", "an", "early", "natural", "language", "processing", "engine", "capable", "of", "interpreting", "naturally", "written", "commands", "within", "a", "simple", "rule-governed", "environment", "."], "labels": ["O", "O", "B-researcher", "I-researcher", "O", "O", "O", "B-field", "I-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, field, organization, task, product, metric, person, conference, algorithm, country, university, researcher, programming language and O.\nSentence: In 1971 Terry Winograd developed an early natural language processing engine capable of interpreting naturally written commands within a simple rule-governed environment .", "prompt_labels": "In(O) 1971(O) Terry(B-researcher) Winograd(I-researcher) developed(O) an(O) early(O) natural(B-field) language(I-field) processing(I-field) engine(O) capable(O) of(O) interpreting(O) naturally(O) written(O) commands(O) within(O) a(O) simple(O) rule-governed(O) environment(O) .(O)"}}
{"id": "6", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "university", "researcher", "organization", "product", "metric", "conference", "task", "location", "person", "programming language", "algorithm", "field"], "instance": {"id": "6", "words": ["NIST", "also", "differs", "from", "Bilingual", "evaluation", "understudy", "in", "its", "calculation", "of", "the", "brevity", "penalty", "insofar", "as", "small", "variations", "in", "translation", "length", "do", "not", "impact", "the", "overall", "score", "as", "much", "."], "labels": ["B-metric", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, researcher, organization, product, metric, conference, task, location, person, programming language, algorithm, field and O.\nSentence: NIST also differs from Bilingual evaluation understudy in its calculation of the brevity penalty insofar as small variations in translation length do not impact the overall score as much .", "prompt_labels": "NIST(B-metric) also(O) differs(O) from(O) Bilingual(B-metric) evaluation(I-metric) understudy(I-metric) in(O) its(O) calculation(O) of(O) the(O) brevity(O) penalty(O) insofar(O) as(O) small(O) variations(O) in(O) translation(O) length(O) do(O) not(O) impact(O) the(O) overall(O) score(O) as(O) much(O) .(O)"}}
{"id": "398", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "programming language", "organization", "location", "researcher", "conference", "university", "country", "field", "person", "metric", "algorithm", "task"], "instance": {"id": "398", "words": ["In", "red-green", "anaglyph", ",", "the", "audience", "was", "presented", "three", "reels", "of", "tests", ",", "which", "included", "rural", "scenes", ",", "test", "shots", "of", "Marie", "Doro", ",", "a", "segment", "of", "John", "B.", "Mason", "playing", "a", "number", "of", "passages", "from", "Jim", "the", "Penman", "(", "a", "film", "released", "by", "Famous", "Players-Lasky", "that", "year", ",", "but", "not", "in", "3D", ")", ",", "Oriental", "dancers", ",", "and", "a", "reel", "of", "footage", "of", "Niagara", "Falls", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, programming language, organization, location, researcher, conference, university, country, field, person, metric, algorithm, task and O.\nSentence: In red-green anaglyph , the audience was presented three reels of tests , which included rural scenes , test shots of Marie Doro , a segment of John B. Mason playing a number of passages from Jim the Penman ( a film released by Famous Players-Lasky that year , but not in 3D ) , Oriental dancers , and a reel of footage of Niagara Falls .", "prompt_labels": "In(O) red-green(O) anaglyph(O) ,(O) the(O) audience(O) was(O) presented(O) three(O) reels(O) of(O) tests(O) ,(O) which(O) included(O) rural(O) scenes(O) ,(O) test(O) shots(O) of(O) Marie(B-person) Doro(I-person) ,(O) a(O) segment(O) of(O) John(B-person) B.(I-person) Mason(I-person) playing(O) a(O) number(O) of(O) passages(O) from(O) Jim(B-person) the(I-person) Penman(I-person) ((O) a(O) film(O) released(O) by(O) Famous(B-organization) Players-Lasky(I-organization) that(O) year(O) ,(O) but(O) not(O) in(O) 3D(O) )(O) ,(O) Oriental(O) dancers(O) ,(O) and(O) a(O) reel(O) of(O) footage(O) of(O) Niagara(B-location) Falls(I-location) .(O)"}}
{"id": "240", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "algorithm", "product", "university", "metric", "programming language", "researcher", "conference", "person", "organization", "field", "location", "task"], "instance": {"id": "240", "words": ["Some", "languages", "make", "it", "possible", "portably", "(", "e.g.", "Scheme", ",", "Common", "Lisp", ",", "Perl", "or", "D", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-programming language", "O", "B-programming language", "I-programming language", "O", "B-programming language", "O", "B-programming language", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, algorithm, product, university, metric, programming language, researcher, conference, person, organization, field, location, task and O.\nSentence: Some languages make it possible portably ( e.g. Scheme , Common Lisp , Perl or D ) .", "prompt_labels": "Some(O) languages(O) make(O) it(O) possible(O) portably(O) ((O) e.g.(O) Scheme(B-programming language) ,(O) Common(B-programming language) Lisp(I-programming language) ,(O) Perl(B-programming language) or(O) D(B-programming language) )(O) .(O)"}}
{"id": "137", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "researcher", "location", "task", "field", "country", "algorithm", "university", "person", "product", "conference", "programming language", "organization"], "instance": {"id": "137", "words": ["For", "example", ",", "speech", "synthesis", ",", "combined", "with", "speech", "recognition", ",", "allows", "for", "interaction", "with", "mobile", "devices", "via", "language", "processing", "interfaces", "."], "labels": ["O", "O", "O", "B-task", "I-task", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, researcher, location, task, field, country, algorithm, university, person, product, conference, programming language, organization and O.\nSentence: For example , speech synthesis , combined with speech recognition , allows for interaction with mobile devices via language processing interfaces .", "prompt_labels": "For(O) example(O) ,(O) speech(B-task) synthesis(I-task) ,(O) combined(O) with(O) speech(B-task) recognition(I-task) ,(O) allows(O) for(O) interaction(O) with(O) mobile(O) devices(O) via(O) language(B-field) processing(I-field) interfaces(O) .(O)"}}
{"id": "165", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "task", "programming language", "person", "location", "algorithm", "country", "researcher", "conference", "organization", "product", "metric", "field"], "instance": {"id": "165", "words": ["John", "D.", "Lafferty", ",", "Andrew", "McCallum", "and", "Pereiramath", "as", "follows", ":"], "labels": ["B-researcher", "I-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, task, programming language, person, location, algorithm, country, researcher, conference, organization, product, metric, field and O.\nSentence: John D. Lafferty , Andrew McCallum and Pereiramath as follows :", "prompt_labels": "John(B-researcher) D.(I-researcher) Lafferty(I-researcher) ,(O) Andrew(B-researcher) McCallum(I-researcher) and(O) Pereiramath(B-researcher) as(O) follows(O) :(O)"}}
{"id": "60", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "organization", "conference", "person", "product", "field", "country", "location", "programming language", "task", "researcher", "algorithm", "university"], "instance": {"id": "60", "words": ["A", "common", "theme", "of", "this", "work", "is", "the", "adoption", "of", "a", "sign-theoretic", "perspective", "on", "issues", "of", "artificial", "intelligence", "and", "knowledge", "representation", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, organization, conference, person, product, field, country, location, programming language, task, researcher, algorithm, university and O.\nSentence: A common theme of this work is the adoption of a sign-theoretic perspective on issues of artificial intelligence and knowledge representation .", "prompt_labels": "A(O) common(O) theme(O) of(O) this(O) work(O) is(O) the(O) adoption(O) of(O) a(O) sign-theoretic(O) perspective(O) on(O) issues(O) of(O) artificial(B-field) intelligence(I-field) and(O) knowledge(B-task) representation(I-task) .(O)"}}
{"id": "251", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "university", "algorithm", "product", "field", "country", "researcher", "person", "task", "location", "organization", "conference", "metric"], "instance": {"id": "251", "words": ["SHRDLU", "was", "an", "early", "natural", "language", "understanding", "computer", "program", ",", "developed", "by", "Terry", "Winograd", "at", "MIT", "in", "1968-1970"], "labels": ["B-product", "O", "O", "O", "B-task", "I-task", "I-task", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-university", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, university, algorithm, product, field, country, researcher, person, task, location, organization, conference, metric and O.\nSentence: SHRDLU was an early natural language understanding computer program , developed by Terry Winograd at MIT in 1968-1970", "prompt_labels": "SHRDLU(B-product) was(O) an(O) early(O) natural(B-task) language(I-task) understanding(I-task) computer(O) program(O) ,(O) developed(O) by(O) Terry(B-researcher) Winograd(I-researcher) at(O) MIT(B-university) in(O) 1968-1970(O)"}}
{"id": "164", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "university", "programming language", "location", "researcher", "person", "country", "conference", "task", "metric", "algorithm", "product", "field"], "instance": {"id": "164", "words": ["In", "the", "fall", "of", "2005", ",", "Thrun", "published", "a", "textbook", "entitled", "Probabilistic", "Robotics", "together", "with", "his", "long-term", "co-workers", "Dieter", "Fox", "and", "Wolfram", "Burgard", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, university, programming language, location, researcher, person, country, conference, task, metric, algorithm, product, field and O.\nSentence: In the fall of 2005 , Thrun published a textbook entitled Probabilistic Robotics together with his long-term co-workers Dieter Fox and Wolfram Burgard .", "prompt_labels": "In(O) the(O) fall(O) of(O) 2005(O) ,(O) Thrun(B-researcher) published(O) a(O) textbook(O) entitled(O) Probabilistic(O) Robotics(O) together(O) with(O) his(O) long-term(O) co-workers(O) Dieter(B-researcher) Fox(I-researcher) and(O) Wolfram(B-researcher) Burgard(I-researcher) .(O)"}}
{"id": "27", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "university", "researcher", "metric", "task", "location", "conference", "person", "country", "field", "product", "programming language", "organization"], "instance": {"id": "27", "words": ["The", "AIBO", "has", "seen", "much", "use", "as", "an", "inexpensive", "platform", "for", "artificial", "intelligence", "education", "and", "research", ",", "because", "integrates", "a", "computer", ",", "Computer", "vision", ",", "and", "articulators", "in", "a", "package", "vastly", "cheaper", "than", "conventional", "research", "robots", "."], "labels": ["O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, university, researcher, metric, task, location, conference, person, country, field, product, programming language, organization and O.\nSentence: The AIBO has seen much use as an inexpensive platform for artificial intelligence education and research , because integrates a computer , Computer vision , and articulators in a package vastly cheaper than conventional research robots .", "prompt_labels": "The(O) AIBO(B-product) has(O) seen(O) much(O) use(O) as(O) an(O) inexpensive(O) platform(O) for(O) artificial(B-field) intelligence(I-field) education(O) and(O) research(O) ,(O) because(O) integrates(O) a(O) computer(O) ,(O) Computer(B-field) vision(I-field) ,(O) and(O) articulators(O) in(O) a(O) package(O) vastly(O) cheaper(O) than(O) conventional(O) research(O) robots(O) .(O)"}}
{"id": "186", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "person", "programming language", "field", "conference", "researcher", "task", "metric", "university", "algorithm", "country", "organization", "location"], "instance": {"id": "186", "words": ["The", "following", "technique", "was", "described", "in", "Breiman", "'s", "original", "paper", "and", "is", "implemented", "in", "the", "R", "package", "randomForest", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, person, programming language, field, conference, researcher, task, metric, university, algorithm, country, organization, location and O.\nSentence: The following technique was described in Breiman 's original paper and is implemented in the R package randomForest .", "prompt_labels": "The(O) following(O) technique(O) was(O) described(O) in(O) Breiman(B-researcher) 's(O) original(O) paper(O) and(O) is(O) implemented(O) in(O) the(O) R(B-product) package(I-product) randomForest(I-product) .(O)"}}
{"id": "305", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "programming language", "algorithm", "task", "field", "product", "country", "metric", "location", "conference", "organization", "researcher", "person"], "instance": {"id": "305", "words": ["The", "returning", "judges", "are", "Fon", "Davis", ",", "Jessica", "Chobot", ",", "and", "Leland", "Melvin", ",", "as", "well", "as", "celebrity", "guest", "judges", "actor", "Clark", "Gregg", ",", "MythBusters", "host", "and", "former", "Battlebots", "builder", "Adam", "Savage", ",", "NFL", "tightend", "Vernon", "Davis", ",", "and", "YouTube", "star", "Michael", "Stevens", "a.k.a.", "Vsauce", "."], "labels": ["O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-organization", "O", "B-person", "I-person", "O", "O", "B-organization", "O", "B-person", "I-person", "O", "B-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, programming language, algorithm, task, field, product, country, metric, location, conference, organization, researcher, person and O.\nSentence: The returning judges are Fon Davis , Jessica Chobot , and Leland Melvin , as well as celebrity guest judges actor Clark Gregg , MythBusters host and former Battlebots builder Adam Savage , NFL tightend Vernon Davis , and YouTube star Michael Stevens a.k.a. Vsauce .", "prompt_labels": "The(O) returning(O) judges(O) are(O) Fon(B-person) Davis(I-person) ,(O) Jessica(B-person) Chobot(I-person) ,(O) and(O) Leland(B-person) Melvin(I-person) ,(O) as(O) well(O) as(O) celebrity(O) guest(O) judges(O) actor(O) Clark(B-person) Gregg(I-person) ,(O) MythBusters(O) host(O) and(O) former(O) Battlebots(O) builder(O) Adam(B-person) Savage(I-person) ,(O) NFL(B-organization) tightend(O) Vernon(B-person) Davis(I-person) ,(O) and(O) YouTube(B-organization) star(O) Michael(B-person) Stevens(I-person) a.k.a.(O) Vsauce(B-person) .(O)"}}
{"id": "364", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "product", "person", "researcher", "country", "metric", "field", "university", "organization", "algorithm", "location", "task", "programming language"], "instance": {"id": "364", "words": ["In", "2014", ",", "Schmidhuber", "formed", "a", "company", ",", "Nnaisense", ",", "to", "work", "on", "commercial", "applications", "of", "artificial", "intelligence", "in", "fields", "such", "as", "finance", ",", "heavy", "industry", "and", "self-driving", "car", "s", "."], "labels": ["O", "O", "O", "B-researcher", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, product, person, researcher, country, metric, field, university, organization, algorithm, location, task, programming language and O.\nSentence: In 2014 , Schmidhuber formed a company , Nnaisense , to work on commercial applications of artificial intelligence in fields such as finance , heavy industry and self-driving car s .", "prompt_labels": "In(O) 2014(O) ,(O) Schmidhuber(B-researcher) formed(O) a(O) company(O) ,(O) Nnaisense(B-organization) ,(O) to(O) work(O) on(O) commercial(O) applications(O) of(O) artificial(B-field) intelligence(I-field) in(O) fields(O) such(O) as(O) finance(O) ,(O) heavy(O) industry(O) and(O) self-driving(B-product) car(I-product) s(O) .(O)"}}
{"id": "295", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "task", "person", "product", "location", "researcher", "field", "university", "algorithm", "conference", "metric", "organization", "country"], "instance": {"id": "295", "words": ["Walter", "'s", "work", "inspired", "subsequent", "generations", "of", "robotics", "researchers", "such", "as", "Rodney", "Brooks", ",", "Hans", "Moravec", "and", "Mark", "Tilden", "."], "labels": ["B-researcher", "O", "O", "O", "O", "O", "O", "B-field", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, task, person, product, location, researcher, field, university, algorithm, conference, metric, organization, country and O.\nSentence: Walter 's work inspired subsequent generations of robotics researchers such as Rodney Brooks , Hans Moravec and Mark Tilden .", "prompt_labels": "Walter(B-researcher) 's(O) work(O) inspired(O) subsequent(O) generations(O) of(O) robotics(B-field) researchers(O) such(O) as(O) Rodney(B-researcher) Brooks(I-researcher) ,(O) Hans(B-researcher) Moravec(I-researcher) and(O) Mark(B-researcher) Tilden(I-researcher) .(O)"}}
{"id": "75", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "researcher", "metric", "field", "conference", "person", "country", "task", "location", "programming language", "product", "algorithm", "organization"], "instance": {"id": "75", "words": ["Like", "DBNs", ",", "DBMs", "can", "learn", "complex", "and", "abstract", "internal", "representations", "of", "the", "input", "in", "tasks", "such", "as", "Object", "recognition", "or", "speech", "recognition", ",", "using", "limited", ",", "labeled", "data", "to", "fine-tune", "the", "representations", "built", "using", "a", "large", "set", "of", "unlabeled", "sensory", "input", "data", "."], "labels": ["O", "B-algorithm", "O", "B-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, researcher, metric, field, conference, person, country, task, location, programming language, product, algorithm, organization and O.\nSentence: Like DBNs , DBMs can learn complex and abstract internal representations of the input in tasks such as Object recognition or speech recognition , using limited , labeled data to fine-tune the representations built using a large set of unlabeled sensory input data .", "prompt_labels": "Like(O) DBNs(B-algorithm) ,(O) DBMs(B-algorithm) can(O) learn(O) complex(O) and(O) abstract(O) internal(O) representations(O) of(O) the(O) input(O) in(O) tasks(O) such(O) as(O) Object(B-task) recognition(I-task) or(O) speech(B-task) recognition(I-task) ,(O) using(O) limited(O) ,(O) labeled(O) data(O) to(O) fine-tune(O) the(O) representations(O) built(O) using(O) a(O) large(O) set(O) of(O) unlabeled(O) sensory(O) input(O) data(O) .(O)"}}
{"id": "134", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "person", "product", "field", "algorithm", "conference", "metric", "university", "country", "location", "programming language", "task", "organization"], "instance": {"id": "134", "words": ["2004", "-", "The", "first", "Cobot", "KUKA", "LBR", "3", "is", "released", "."], "labels": ["O", "O", "O", "O", "B-product", "I-product", "I-product", "I-product", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, person, product, field, algorithm, conference, metric, university, country, location, programming language, task, organization and O.\nSentence: 2004 - The first Cobot KUKA LBR 3 is released .", "prompt_labels": "2004(O) -(O) The(O) first(O) Cobot(B-product) KUKA(I-product) LBR(I-product) 3(I-product) is(O) released(O) .(O)"}}
{"id": "50", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "person", "field", "researcher", "product", "conference", "metric", "task", "programming language", "location", "university", "algorithm", "organization"], "instance": {"id": "50", "words": ["In", "1991", "he", "was", "elected", "as", "a", "fellow", "of", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "(", "1990", ",", "founding", "fellow", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, field, researcher, product, conference, metric, task, programming language, location, university, algorithm, organization and O.\nSentence: In 1991 he was elected as a fellow of the Association for the Advancement of Artificial Intelligence ( 1990 , founding fellow ) .", "prompt_labels": "In(O) 1991(O) he(O) was(O) elected(O) as(O) a(O) fellow(O) of(O) the(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) ((O) 1990(O) ,(O) founding(O) fellow(O) )(O) .(O)"}}
{"id": "424", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "metric", "task", "researcher", "country", "product", "algorithm", "programming language", "location", "university", "organization", "field", "conference"], "instance": {"id": "424", "words": ["Recently", ",", "Google", "announced", "that", "Google", "Translate", "translates", "roughly", "enough", "text", "to", "fill", "1", "million", "books", "in", "one", "day", "(", "2012", ")", "."], "labels": ["O", "O", "B-organization", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, metric, task, researcher, country, product, algorithm, programming language, location, university, organization, field, conference and O.\nSentence: Recently , Google announced that Google Translate translates roughly enough text to fill 1 million books in one day ( 2012 ) .", "prompt_labels": "Recently(O) ,(O) Google(B-organization) announced(O) that(O) Google(B-product) Translate(I-product) translates(O) roughly(O) enough(O) text(O) to(O) fill(O) 1(O) million(O) books(O) in(O) one(O) day(O) ((O) 2012(O) )(O) .(O)"}}
{"id": "206", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "algorithm", "organization", "researcher", "task", "product", "person", "location", "country", "university", "metric", "programming language", "field"], "instance": {"id": "206", "words": ["Template", "matching", "has", "various", "applications", "and", "is", "used", "in", "such", "fields", "as", "face", "recognition", "(", "see", "facial", "recognition", "system", ")", "and", "medical", "image", "processing", "."], "labels": ["B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "B-product", "I-product", "I-product", "O", "O", "B-task", "I-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, algorithm, organization, researcher, task, product, person, location, country, university, metric, programming language, field and O.\nSentence: Template matching has various applications and is used in such fields as face recognition ( see facial recognition system ) and medical image processing .", "prompt_labels": "Template(B-algorithm) matching(I-algorithm) has(O) various(O) applications(O) and(O) is(O) used(O) in(O) such(O) fields(O) as(O) face(B-task) recognition(I-task) ((O) see(O) facial(B-product) recognition(I-product) system(I-product) )(O) and(O) medical(B-task) image(I-task) processing(I-task) .(O)"}}
{"id": "162", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "researcher", "programming language", "metric", "person", "product", "field", "task", "organization", "location", "university", "country", "conference"], "instance": {"id": "162", "words": ["While", "there", "is", "no", "perfect", "way", "of", "describing", "the", "confusion", "matrix", "of", "TRUE", "and", "FALSE", "positives", "and", "negatives", "by", "a", "single", "number", ",", "the", "Matthews", "correlation", "coefficient", "is", "generally", "regarded", "as", "being", "one", "of", "the", "best", "such", "measures", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, researcher, programming language, metric, person, product, field, task, organization, location, university, country, conference and O.\nSentence: While there is no perfect way of describing the confusion matrix of TRUE and FALSE positives and negatives by a single number , the Matthews correlation coefficient is generally regarded as being one of the best such measures .", "prompt_labels": "While(O) there(O) is(O) no(O) perfect(O) way(O) of(O) describing(O) the(O) confusion(B-metric) matrix(I-metric) of(O) TRUE(O) and(O) FALSE(O) positives(O) and(O) negatives(O) by(O) a(O) single(O) number(O) ,(O) the(O) Matthews(B-metric) correlation(I-metric) coefficient(I-metric) is(O) generally(O) regarded(O) as(O) being(O) one(O) of(O) the(O) best(O) such(O) measures(O) .(O)"}}
{"id": "54", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "task", "programming language", "conference", "algorithm", "field", "metric", "product", "person", "country", "location", "university", "researcher"], "instance": {"id": "54", "words": ["The", "difference", "between", "the", "multinomial", "logit", "model", "and", "numerous", "other", "methods", ",", "models", ",", "algorithms", ",", "etc.", "with", "the", "same", "basic", "setup", "(", "the", "perceptron", "algorithm", ",", "support", "vector", "machine", "s", ",", "linear", "discriminant", "analysis", ",", "etc", "."], "labels": ["O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, task, programming language, conference, algorithm, field, metric, product, person, country, location, university, researcher and O.\nSentence: The difference between the multinomial logit model and numerous other methods , models , algorithms , etc. with the same basic setup ( the perceptron algorithm , support vector machine s , linear discriminant analysis , etc .", "prompt_labels": "The(O) difference(O) between(O) the(O) multinomial(B-algorithm) logit(I-algorithm) model(I-algorithm) and(O) numerous(O) other(O) methods(O) ,(O) models(O) ,(O) algorithms(O) ,(O) etc.(O) with(O) the(O) same(O) basic(O) setup(O) ((O) the(O) perceptron(B-algorithm) algorithm(I-algorithm) ,(O) support(B-algorithm) vector(I-algorithm) machine(I-algorithm) s(O) ,(O) linear(B-algorithm) discriminant(I-algorithm) analysis(I-algorithm) ,(O) etc(O) .(O)"}}
{"id": "88", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "university", "person", "task", "field", "researcher", "conference", "location", "algorithm", "country", "product", "programming language", "metric"], "instance": {"id": "88", "words": ["WordNet", ",", "a", "freely", "available", "database", "originally", "designed", "as", "a", "semantic", "network", "based", "on", "psycholinguistic", "principles", ",", "was", "expanded", "by", "addition", "of", "definitions", "and", "is", "now", "also", "viewed", "as", "a", "dictionary", "."], "labels": ["B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, university, person, task, field, researcher, conference, location, algorithm, country, product, programming language, metric and O.\nSentence: WordNet , a freely available database originally designed as a semantic network based on psycholinguistic principles , was expanded by addition of definitions and is now also viewed as a dictionary .", "prompt_labels": "WordNet(B-product) ,(O) a(O) freely(O) available(O) database(O) originally(O) designed(O) as(O) a(O) semantic(O) network(O) based(O) on(O) psycholinguistic(O) principles(O) ,(O) was(O) expanded(O) by(O) addition(O) of(O) definitions(O) and(O) is(O) now(O) also(O) viewed(O) as(O) a(O) dictionary(O) .(O)"}}
{"id": "325", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "university", "metric", "organization", "programming language", "conference", "researcher", "algorithm", "task", "location", "field", "country", "person"], "instance": {"id": "325", "words": ["Semantic", "networks", "are", "used", "in", "natural", "language", "processing", "applications", "such", "as", "semantic", "parsing", "."], "labels": ["B-algorithm", "I-algorithm", "O", "O", "O", "B-field", "I-field", "I-field", "O", "O", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, university, metric, organization, programming language, conference, researcher, algorithm, task, location, field, country, person and O.\nSentence: Semantic networks are used in natural language processing applications such as semantic parsing .", "prompt_labels": "Semantic(B-algorithm) networks(I-algorithm) are(O) used(O) in(O) natural(B-field) language(I-field) processing(I-field) applications(O) such(O) as(O) semantic(B-task) parsing(I-task) .(O)"}}
{"id": "394", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "conference", "researcher", "algorithm", "location", "university", "organization", "programming language", "metric", "person", "task", "country", "field"], "instance": {"id": "394", "words": ["Where", "Bilingual", "evaluation", "understudy", "simply", "calculates", "n-gram", "precision", "adding", "equal", "weight", "to", "each", "one", ",", "NIST", "also", "calculates", "how", "informative", "a", "particular", "n-gram", "is", "."], "labels": ["O", "B-metric", "I-metric", "I-metric", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, conference, researcher, algorithm, location, university, organization, programming language, metric, person, task, country, field and O.\nSentence: Where Bilingual evaluation understudy simply calculates n-gram precision adding equal weight to each one , NIST also calculates how informative a particular n-gram is .", "prompt_labels": "Where(O) Bilingual(B-metric) evaluation(I-metric) understudy(I-metric) simply(O) calculates(O) n-gram(B-metric) precision(I-metric) adding(O) equal(O) weight(O) to(O) each(O) one(O) ,(O) NIST(B-metric) also(O) calculates(O) how(O) informative(O) a(O) particular(O) n-gram(O) is(O) .(O)"}}
{"id": "105", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "task", "country", "conference", "metric", "algorithm", "programming language", "person", "university", "researcher", "product", "field", "location"], "instance": {"id": "105", "words": ["dgp", "also", "occasionally", "hosts", "artists", "in", "residence", "(", "e.g.", ",", "Oscar", "-winner", "Chris", "Landreth", "."], "labels": ["B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, task, country, conference, metric, algorithm, programming language, person, university, researcher, product, field, location and O.\nSentence: dgp also occasionally hosts artists in residence ( e.g. , Oscar -winner Chris Landreth .", "prompt_labels": "dgp(B-organization) also(O) occasionally(O) hosts(O) artists(O) in(O) residence(O) ((O) e.g.(O) ,(O) Oscar(O) -winner(O) Chris(B-person) Landreth(I-person) .(O)"}}
{"id": "85", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "product", "country", "task", "organization", "metric", "programming language", "person", "university", "algorithm", "researcher", "field", "location"], "instance": {"id": "85", "words": ["Organizations", "known", "to", "use", "ALE", "for", "Emergency", "management", ",", "disaster", "relief", ",", "ordinary", "communication", "or", "extraordinary", "situation", "response", ":", "American", "Red", "Cross", ",", "FEMA", ",", "Disaster", "Medical", "Assistance", "Team", "s", ",", "NATO", ",", "Federal", "Bureau", "of", "Investigation", ",", "United", "Nations", ",", "AT", "&", "T", ",", "Civil", "Air", "Patrol", ",", "(", "ARES", ")", "."], "labels": ["O", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, product, country, task, organization, metric, programming language, person, university, algorithm, researcher, field, location and O.\nSentence: Organizations known to use ALE for Emergency management , disaster relief , ordinary communication or extraordinary situation response : American Red Cross , FEMA , Disaster Medical Assistance Team s , NATO , Federal Bureau of Investigation , United Nations , AT & T , Civil Air Patrol , ( ARES ) .", "prompt_labels": "Organizations(O) known(O) to(O) use(O) ALE(B-product) for(O) Emergency(O) management(O) ,(O) disaster(O) relief(O) ,(O) ordinary(O) communication(O) or(O) extraordinary(O) situation(O) response(O) :(O) American(B-organization) Red(I-organization) Cross(I-organization) ,(O) FEMA(B-organization) ,(O) Disaster(B-organization) Medical(I-organization) Assistance(I-organization) Team(I-organization) s(O) ,(O) NATO(B-organization) ,(O) Federal(B-organization) Bureau(I-organization) of(I-organization) Investigation(I-organization) ,(O) United(B-organization) Nations(I-organization) ,(O) AT(B-organization) &(I-organization) T(I-organization) ,(O) Civil(B-organization) Air(I-organization) Patrol(I-organization) ,(O) ((O) ARES(B-organization) )(O) .(O)"}}
{"id": "29", "dataset": "crossner_ai", "split": "test", "label_list": ["task", "metric", "university", "conference", "person", "organization", "product", "researcher", "programming language", "location", "algorithm", "country", "field"], "instance": {"id": "29", "words": ["Scheinman", ",", "after", "receiving", "a", "fellowship", "from", "Unimation", "to", "develop", "his", "designs", ",", "sold", "those", "designs", "to", "Unimation", "who", "further", "developed", "them", "with", "support", "from", "General", "Motors", "and", "later", "marketed", "it", "as", "the", "Programmable", "Universal", "Machine", "for", "Assembly", "(", "PUMA", ")", "."], "labels": ["B-researcher", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "I-product", "I-product", "O", "B-product", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, metric, university, conference, person, organization, product, researcher, programming language, location, algorithm, country, field and O.\nSentence: Scheinman , after receiving a fellowship from Unimation to develop his designs , sold those designs to Unimation who further developed them with support from General Motors and later marketed it as the Programmable Universal Machine for Assembly ( PUMA ) .", "prompt_labels": "Scheinman(B-researcher) ,(O) after(O) receiving(O) a(O) fellowship(O) from(O) Unimation(B-organization) to(O) develop(O) his(O) designs(O) ,(O) sold(O) those(O) designs(O) to(O) Unimation(B-organization) who(O) further(O) developed(O) them(O) with(O) support(O) from(O) General(B-organization) Motors(I-organization) and(O) later(O) marketed(O) it(O) as(O) the(O) Programmable(B-product) Universal(I-product) Machine(I-product) for(I-product) Assembly(I-product) ((O) PUMA(B-product) )(O) .(O)"}}
{"id": "317", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "metric", "product", "programming language", "location", "field", "organization", "country", "researcher", "task", "conference", "person", "university"], "instance": {"id": "317", "words": ["Other", "linear", "classification", "algorithms", "include", "Winnow", ",", "support", "vector", "machine", "and", "logistic", "regression", "."], "labels": ["O", "O", "O", "O", "O", "B-algorithm", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, metric, product, programming language, location, field, organization, country, researcher, task, conference, person, university and O.\nSentence: Other linear classification algorithms include Winnow , support vector machine and logistic regression .", "prompt_labels": "Other(O) linear(O) classification(O) algorithms(O) include(O) Winnow(B-algorithm) ,(O) support(B-algorithm) vector(I-algorithm) machine(I-algorithm) and(O) logistic(B-algorithm) regression(I-algorithm) .(O)"}}
{"id": "359", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "programming language", "location", "algorithm", "organization", "researcher", "university", "metric", "task", "person", "conference", "field", "product"], "instance": {"id": "359", "words": ["Techniques", "such", "as", "dynamic", "Markov", "Networks", ",", "Convolutional", "neural", "network", "and", "Long", "short-term", "memory", "are", "often", "employed", "to", "exploit", "the", "semantic", "correlations", "between", "consecutive", "video", "frames", "."], "labels": ["O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, programming language, location, algorithm, organization, researcher, university, metric, task, person, conference, field, product and O.\nSentence: Techniques such as dynamic Markov Networks , Convolutional neural network and Long short-term memory are often employed to exploit the semantic correlations between consecutive video frames .", "prompt_labels": "Techniques(O) such(O) as(O) dynamic(B-algorithm) Markov(I-algorithm) Networks(I-algorithm) ,(O) Convolutional(B-algorithm) neural(I-algorithm) network(I-algorithm) and(O) Long(B-algorithm) short-term(I-algorithm) memory(I-algorithm) are(O) often(O) employed(O) to(O) exploit(O) the(O) semantic(O) correlations(O) between(O) consecutive(O) video(O) frames(O) .(O)"}}
{"id": "309", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "university", "field", "location", "country", "person", "algorithm", "programming language", "organization", "conference", "metric", "task", "researcher"], "instance": {"id": "309", "words": ["In", "2006", ",", "for", "the", "25th", "anniversary", "of", "the", "algorithm", ",", "a", "workshop", "was", "organized", "at", "the", "International", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "(", "CVPR", ")", "to", "summarize", "the", "most", "recent", "contributions", "and", "variations", "to", "the", "original", "algorithm", ",", "mostly", "meant", "to", "improve", "the", "speed", "of", "the", "algorithm", ",", "the", "robustness", "and", "accuracy", "of", "the", "estimated", "solution", "and", "to", "decrease", "the", "dependency", "from", "user", "defined", "constants", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, university, field, location, country, person, algorithm, programming language, organization, conference, metric, task, researcher and O.\nSentence: In 2006 , for the 25th anniversary of the algorithm , a workshop was organized at the International Conference on Computer Vision and Pattern Recognition ( CVPR ) to summarize the most recent contributions and variations to the original algorithm , mostly meant to improve the speed of the algorithm , the robustness and accuracy of the estimated solution and to decrease the dependency from user defined constants .", "prompt_labels": "In(O) 2006(O) ,(O) for(O) the(O) 25th(O) anniversary(O) of(O) the(O) algorithm(O) ,(O) a(O) workshop(O) was(O) organized(O) at(O) the(O) International(B-conference) Conference(I-conference) on(I-conference) Computer(I-conference) Vision(I-conference) and(I-conference) Pattern(I-conference) Recognition(I-conference) ((O) CVPR(B-conference) )(O) to(O) summarize(O) the(O) most(O) recent(O) contributions(O) and(O) variations(O) to(O) the(O) original(O) algorithm(O) ,(O) mostly(O) meant(O) to(O) improve(O) the(O) speed(O) of(O) the(O) algorithm(O) ,(O) the(O) robustness(O) and(O) accuracy(O) of(O) the(O) estimated(O) solution(O) and(O) to(O) decrease(O) the(O) dependency(O) from(O) user(O) defined(O) constants(O) .(O)"}}
{"id": "273", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "country", "person", "algorithm", "field", "researcher", "metric", "product", "conference", "location", "organization", "programming language", "task"], "instance": {"id": "273", "words": ["Hidden", "Markov", "model", "s", "are", "the", "basis", "for", "most", "modern", "automatic", "speech", "recognition", "systems", "."], "labels": ["B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, country, person, algorithm, field, researcher, metric, product, conference, location, organization, programming language, task and O.\nSentence: Hidden Markov model s are the basis for most modern automatic speech recognition systems .", "prompt_labels": "Hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) s(O) are(O) the(O) basis(O) for(O) most(O) modern(O) automatic(B-product) speech(I-product) recognition(I-product) systems(I-product) .(O)"}}
{"id": "158", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "task", "university", "conference", "programming language", "organization", "field", "researcher", "country", "algorithm", "person", "location", "metric"], "instance": {"id": "158", "words": ["The", "F-score", "has", "been", "widely", "used", "in", "the", "natural", "language", "processing", "literature", ",", "such", "as", "the", "evaluation", "of", "named", "entity", "recognition", "(", "NER", ")", "and", "word", "segmentation", "."], "labels": ["O", "B-metric", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "I-task", "O", "B-task", "O", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, task, university, conference, programming language, organization, field, researcher, country, algorithm, person, location, metric and O.\nSentence: The F-score has been widely used in the natural language processing literature , such as the evaluation of named entity recognition ( NER ) and word segmentation .", "prompt_labels": "The(O) F-score(B-metric) has(O) been(O) widely(O) used(O) in(O) the(O) natural(B-field) language(I-field) processing(I-field) literature(O) ,(O) such(O) as(O) the(O) evaluation(O) of(O) named(B-task) entity(I-task) recognition(I-task) ((O) NER(B-task) )(O) and(O) word(B-task) segmentation(I-task) .(O)"}}
{"id": "95", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "metric", "location", "university", "algorithm", "researcher", "programming language", "organization", "conference", "field", "task", "product", "person"], "instance": {"id": "95", "words": ["After", "boosting", ",", "a", "classifier", "constructed", "from", "200", "features", "could", "yield", "a", "95", "%", "detection", "rate", "under", "a", "^", "{", "-5", "}", "/", "math", "FALSE", "positive", "rate", ".P.", "Viola", ",", "M.", "Jones", ",", "Robust", "Real-time", "Object", "Detection", ",", "2001", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-task", "I-task", "I-task", "I-task", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, metric, location, university, algorithm, researcher, programming language, organization, conference, field, task, product, person and O.\nSentence: After boosting , a classifier constructed from 200 features could yield a 95 % detection rate under a ^ { -5 } / math FALSE positive rate .P. Viola , M. Jones , Robust Real-time Object Detection , 2001 .", "prompt_labels": "After(O) boosting(O) ,(O) a(O) classifier(O) constructed(O) from(O) 200(O) features(O) could(O) yield(O) a(O) 95(O) %(O) detection(O) rate(O) under(O) a(O) ^(O) {(O) -5(O) }(O) /(O) math(O) FALSE(B-metric) positive(I-metric) rate(I-metric) .P.(B-researcher) Viola(I-researcher) ,(O) M.(B-researcher) Jones(I-researcher) ,(O) Robust(B-task) Real-time(I-task) Object(I-task) Detection(I-task) ,(O) 2001(O) .(O)"}}
{"id": "257", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "task", "algorithm", "location", "metric", "person", "conference", "university", "product", "country", "field", "programming language", "researcher"], "instance": {"id": "257", "words": ["Perhaps", "the", "simplest", "statistic", "is", "accuracy", "or", "Fraction", "Correct", "(", "FC", ")", ",", "which", "measures", "the", "fraction", "of", "all", "instances", "that", "are", "correctly", "categorized", ";", "it", "is", "the", "ratio", "of", "the", "number", "of", "correct", "classifications", "to", "the", "total", "number", "of", "correct", "or", "incorrect", "classifications", ":", "(", "TP", "+", "TN", ")", "/", "Total", "Population", "=", "(", "TP", "+", "TN", ")", "/", "(", "TP", "+", "TN", "+", "FP", "+", "FN", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-metric", "O", "B-metric", "I-metric", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "B-metric", "I-metric", "I-metric", "I-metric", "I-metric", "I-metric", "I-metric", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, task, algorithm, location, metric, person, conference, university, product, country, field, programming language, researcher and O.\nSentence: Perhaps the simplest statistic is accuracy or Fraction Correct ( FC ) , which measures the fraction of all instances that are correctly categorized ; it is the ratio of the number of correct classifications to the total number of correct or incorrect classifications : ( TP + TN ) / Total Population = ( TP + TN ) / ( TP + TN + FP + FN ) .", "prompt_labels": "Perhaps(O) the(O) simplest(O) statistic(O) is(O) accuracy(B-metric) or(O) Fraction(B-metric) Correct(I-metric) ((O) FC(B-metric) )(O) ,(O) which(O) measures(O) the(O) fraction(O) of(O) all(O) instances(O) that(O) are(O) correctly(O) categorized(O) ;(O) it(O) is(O) the(O) ratio(O) of(O) the(O) number(O) of(O) correct(O) classifications(O) to(O) the(O) total(O) number(O) of(O) correct(O) or(O) incorrect(O) classifications(O) :(O) ((O) TP(B-metric) +(I-metric) TN(I-metric) )(O) /(O) Total(O) Population(O) =(O) ((O) TP(B-metric) +(I-metric) TN(I-metric) )(O) /(O) ((O) TP(B-metric) +(I-metric) TN(I-metric) +(I-metric) FP(I-metric) +(I-metric) FN(I-metric) )(O) .(O)"}}
{"id": "293", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "person", "metric", "task", "university", "algorithm", "field", "product", "country", "organization", "programming language", "conference", "location"], "instance": {"id": "293", "words": ["Blade", "Runner", "used", "a", "number", "of", "then-lesser-known", "actors", ":", "Sean", "Young", "portrays", "Rachael", ",", "an", "experimental", "replicant", "implanted", "with", "the", "memories", "of", "Tyrell", "'s", "niece", ",", "causing", "her", "to", "believe", "she", "is", "human", ";", "Sammon", ",", "pp.", "92-93", "Nina", "Axelrod", "auditioned", "for", "the", "role", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, person, metric, task, university, algorithm, field, product, country, organization, programming language, conference, location and O.\nSentence: Blade Runner used a number of then-lesser-known actors : Sean Young portrays Rachael , an experimental replicant implanted with the memories of Tyrell 's niece , causing her to believe she is human ; Sammon , pp. 92-93 Nina Axelrod auditioned for the role .", "prompt_labels": "Blade(O) Runner(O) used(O) a(O) number(O) of(O) then-lesser-known(O) actors(O) :(O) Sean(B-person) Young(I-person) portrays(O) Rachael(B-person) ,(O) an(O) experimental(O) replicant(O) implanted(O) with(O) the(O) memories(O) of(O) Tyrell(B-person) 's(O) niece(O) ,(O) causing(O) her(O) to(O) believe(O) she(O) is(O) human(O) ;(O) Sammon(B-person) ,(O) pp.(O) 92-93(O) Nina(B-person) Axelrod(I-person) auditioned(O) for(O) the(O) role(O) .(O)"}}
{"id": "192", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "researcher", "person", "university", "organization", "conference", "country", "metric", "product", "task", "location", "algorithm", "programming language"], "instance": {"id": "192", "words": ["Prolog", "is", "a", "logic", "programming", "language", "associated", "with", "artificial", "intelligence", "and", "computational", "linguistics", "."], "labels": ["B-programming language", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, researcher, person, university, organization, conference, country, metric, product, task, location, algorithm, programming language and O.\nSentence: Prolog is a logic programming language associated with artificial intelligence and computational linguistics .", "prompt_labels": "Prolog(B-programming language) is(O) a(O) logic(O) programming(O) language(O) associated(O) with(O) artificial(B-field) intelligence(I-field) and(O) computational(B-field) linguistics(I-field) .(O)"}}
{"id": "407", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "location", "researcher", "task", "organization", "country", "algorithm", "programming language", "university", "conference", "person", "field", "metric"], "instance": {"id": "407", "words": ["An", "F-score", "is", "a", "combination", "of", "the", "precision", "and", "the", "recall", ",", "providing", "a", "single", "score", "."], "labels": ["O", "B-metric", "O", "O", "O", "O", "O", "B-metric", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, location, researcher, task, organization, country, algorithm, programming language, university, conference, person, field, metric and O.\nSentence: An F-score is a combination of the precision and the recall , providing a single score .", "prompt_labels": "An(O) F-score(B-metric) is(O) a(O) combination(O) of(O) the(O) precision(B-metric) and(O) the(O) recall(B-metric) ,(O) providing(O) a(O) single(O) score(O) .(O)"}}
{"id": "416", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "programming language", "metric", "location", "algorithm", "product", "person", "task", "field", "country", "researcher", "conference", "university"], "instance": {"id": "416", "words": ["For", "a", "recommender", "system", ",", "sentiment", "analysis", "has", "been", "proven", "to", "be", "a", "valuable", "technique", "."], "labels": ["O", "O", "B-product", "I-product", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, programming language, metric, location, algorithm, product, person, task, field, country, researcher, conference, university and O.\nSentence: For a recommender system , sentiment analysis has been proven to be a valuable technique .", "prompt_labels": "For(O) a(O) recommender(B-product) system(I-product) ,(O) sentiment(B-task) analysis(I-task) has(O) been(O) proven(O) to(O) be(O) a(O) valuable(O) technique(O) .(O)"}}
{"id": "114", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "country", "conference", "organization", "university", "programming language", "task", "algorithm", "product", "person", "field", "metric", "researcher"], "instance": {"id": "114", "words": ["Another", "class", "of", "direct", "search", "algorithms", "are", "the", "various", "evolutionary", "algorithm", "s", ",", "e.g.", "genetic", "algorithm", "s", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, conference, organization, university, programming language, task, algorithm, product, person, field, metric, researcher and O.\nSentence: Another class of direct search algorithms are the various evolutionary algorithm s , e.g. genetic algorithm s .", "prompt_labels": "Another(O) class(O) of(O) direct(O) search(O) algorithms(O) are(O) the(O) various(O) evolutionary(B-algorithm) algorithm(I-algorithm) s(O) ,(O) e.g.(O) genetic(B-algorithm) algorithm(I-algorithm) s(O) .(O)"}}
{"id": "217", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "country", "field", "university", "researcher", "location", "product", "conference", "programming language", "algorithm", "task", "person", "metric"], "instance": {"id": "217", "words": ["An", "early", "version", "of", "VMAF", "has", "been", "shown", "to", "outperform", "other", "image", "and", "video", "quality", "metrics", "such", "as", "SSIM", ",", "PSNR", "-HVS", "and", "VQM-VFD", "on", "three", "of", "four", "datasets", "in", "terms", "of", "prediction", "accuracy", ",", "when", "compared", "to", "subjective", "ratings", "."], "labels": ["O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "B-metric", "I-metric", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, field, university, researcher, location, product, conference, programming language, algorithm, task, person, metric and O.\nSentence: An early version of VMAF has been shown to outperform other image and video quality metrics such as SSIM , PSNR -HVS and VQM-VFD on three of four datasets in terms of prediction accuracy , when compared to subjective ratings .", "prompt_labels": "An(O) early(O) version(O) of(O) VMAF(B-metric) has(O) been(O) shown(O) to(O) outperform(O) other(O) image(O) and(O) video(O) quality(O) metrics(O) such(O) as(O) SSIM(B-metric) ,(O) PSNR(B-metric) -HVS(I-metric) and(O) VQM-VFD(B-metric) on(O) three(O) of(O) four(O) datasets(O) in(O) terms(O) of(O) prediction(O) accuracy(B-metric) ,(O) when(O) compared(O) to(O) subjective(O) ratings(O) .(O)"}}
{"id": "172", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "conference", "algorithm", "product", "field", "programming language", "task", "metric", "country", "organization", "location", "university", "person"], "instance": {"id": "172", "words": ["In", "2003", ",", "Honda", "released", "its", "Cog", "advertisement", "in", "the", "UK", "and", "on", "the", "Internet", "."], "labels": ["O", "O", "O", "B-organization", "O", "O", "B-product", "O", "O", "O", "B-country", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, conference, algorithm, product, field, programming language, task, metric, country, organization, location, university, person and O.\nSentence: In 2003 , Honda released its Cog advertisement in the UK and on the Internet .", "prompt_labels": "In(O) 2003(O) ,(O) Honda(B-organization) released(O) its(O) Cog(B-product) advertisement(O) in(O) the(O) UK(B-country) and(O) on(O) the(O) Internet(O) .(O)"}}
{"id": "366", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "researcher", "person", "programming language", "algorithm", "university", "country", "field", "location", "task", "conference", "metric", "organization"], "instance": {"id": "366", "words": ["Bigrams", "are", "used", "in", "most", "successful", "language", "model", "s", "for", "speech", "recognition", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, researcher, person, programming language, algorithm, university, country, field, location, task, conference, metric, organization and O.\nSentence: Bigrams are used in most successful language model s for speech recognition .", "prompt_labels": "Bigrams(O) are(O) used(O) in(O) most(O) successful(O) language(B-algorithm) model(I-algorithm) s(O) for(O) speech(B-task) recognition(I-task) .(O)"}}
{"id": "334", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "programming language", "researcher", "location", "conference", "person", "product", "task", "algorithm", "country", "field", "university", "metric"], "instance": {"id": "334", "words": ["Industrial", "robots", "have", "been", "implemented", "to", "collaborate", "with", "humans", "to", "perform", "industrial", "manufacturing", "tasks", "."], "labels": ["B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, programming language, researcher, location, conference, person, product, task, algorithm, country, field, university, metric and O.\nSentence: Industrial robots have been implemented to collaborate with humans to perform industrial manufacturing tasks .", "prompt_labels": "Industrial(B-product) robots(I-product) have(O) been(O) implemented(O) to(O) collaborate(O) with(O) humans(O) to(O) perform(O) industrial(O) manufacturing(O) tasks(O) .(O)"}}
{"id": "310", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "organization", "location", "conference", "product", "country", "person", "programming language", "task", "researcher", "university", "field", "metric"], "instance": {"id": "310", "words": ["The", "members", "went", "to", "the", "University", "of", "Debrecen", ",", "the", "Hungarian", "Academy", "of", "Sciences", ",", "Eötvös", "Loránd", "University", ",", "etc", "."], "labels": ["O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-university", "I-university", "I-university", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, organization, location, conference, product, country, person, programming language, task, researcher, university, field, metric and O.\nSentence: The members went to the University of Debrecen , the Hungarian Academy of Sciences , Eötvös Loránd University , etc .", "prompt_labels": "The(O) members(O) went(O) to(O) the(O) University(B-university) of(I-university) Debrecen(I-university) ,(O) the(O) Hungarian(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) Eötvös(B-university) Loránd(I-university) University(I-university) ,(O) etc(O) .(O)"}}
{"id": "209", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "algorithm", "conference", "person", "task", "field", "product", "university", "country", "organization", "programming language", "metric", "researcher"], "instance": {"id": "209", "words": ["When", "data", "are", "unlabelled", ",", "supervised", "learning", "is", "not", "possible", ",", "and", "an", "unsupervised", "learning", "approach", "is", "required", "which", "attempts", "to", "find", "natural", "Cluster", "analysis", "to", "groups", ",", "and", "then", "map", "new", "data", "to", "these", "formed", "groups", "."], "labels": ["O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, algorithm, conference, person, task, field, product, university, country, organization, programming language, metric, researcher and O.\nSentence: When data are unlabelled , supervised learning is not possible , and an unsupervised learning approach is required which attempts to find natural Cluster analysis to groups , and then map new data to these formed groups .", "prompt_labels": "When(O) data(O) are(O) unlabelled(O) ,(O) supervised(B-field) learning(I-field) is(O) not(O) possible(O) ,(O) and(O) an(O) unsupervised(B-field) learning(I-field) approach(O) is(O) required(O) which(O) attempts(O) to(O) find(O) natural(O) Cluster(B-task) analysis(I-task) to(O) groups(O) ,(O) and(O) then(O) map(O) new(O) data(O) to(O) these(O) formed(O) groups(O) .(O)"}}
{"id": "169", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "location", "algorithm", "organization", "programming language", "researcher", "metric", "field", "country", "product", "university", "person", "task"], "instance": {"id": "169", "words": ["The", "sample", "maximum", "is", "the", "maximum", "likelihood", "estimator", "for", "the", "population", "maximum", ",", "but", ",", "as", "discussed", "above", ",", "it", "is", "biased", "."], "labels": ["O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, location, algorithm, organization, programming language, researcher, metric, field, country, product, university, person, task and O.\nSentence: The sample maximum is the maximum likelihood estimator for the population maximum , but , as discussed above , it is biased .", "prompt_labels": "The(O) sample(O) maximum(O) is(O) the(O) maximum(B-metric) likelihood(I-metric) estimator(I-metric) for(O) the(O) population(O) maximum(O) ,(O) but(O) ,(O) as(O) discussed(O) above(O) ,(O) it(O) is(O) biased(O) .(O)"}}
{"id": "20", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "metric", "researcher", "product", "field", "person", "location", "conference", "programming language", "algorithm", "organization", "task", "university"], "instance": {"id": "20", "words": ["Decision", "tree", "learning", "is", "one", "of", "the", "predictive", "modeling", "approaches", "used", "in", "statistics", ",", "data", "mining", "and", "machine", "learning", "."], "labels": ["B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, metric, researcher, product, field, person, location, conference, programming language, algorithm, organization, task, university and O.\nSentence: Decision tree learning is one of the predictive modeling approaches used in statistics , data mining and machine learning .", "prompt_labels": "Decision(B-algorithm) tree(I-algorithm) learning(O) is(O) one(O) of(O) the(O) predictive(O) modeling(O) approaches(O) used(O) in(O) statistics(B-field) ,(O) data(B-field) mining(I-field) and(O) machine(B-field) learning(I-field) .(O)"}}
{"id": "79", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "field", "algorithm", "person", "researcher", "programming language", "country", "conference", "location", "organization", "product", "university", "task"], "instance": {"id": "79", "words": ["The", "concept", "is", "similar", "to", "the", "signal", "to", "noise", "ratio", "used", "in", "the", "sciences", "and", "confusion", "matrix", "used", "in", "artificial", "intelligence", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "I-metric", "O", "O", "O", "B-field", "O", "B-metric", "I-metric", "O", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, field, algorithm, person, researcher, programming language, country, conference, location, organization, product, university, task and O.\nSentence: The concept is similar to the signal to noise ratio used in the sciences and confusion matrix used in artificial intelligence .", "prompt_labels": "The(O) concept(O) is(O) similar(O) to(O) the(O) signal(B-metric) to(I-metric) noise(I-metric) ratio(I-metric) used(O) in(O) the(O) sciences(B-field) and(O) confusion(B-metric) matrix(I-metric) used(O) in(O) artificial(B-field) intelligence(I-field) .(O)"}}
{"id": "130", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "algorithm", "task", "country", "programming language", "metric", "researcher", "person", "location", "university", "product", "field", "conference"], "instance": {"id": "130", "words": ["The", "construction", "of", "a", "rich", "lexicon", "with", "a", "suitable", "ontology", "requires", "significant", "effort", ",", "e.g.", ",", "Wordnet", "lexicon", "required", "many", "person-years", "of", "effort.", "G.", "A.", "Miller", ",", "R.", "Beckwith", ",", "C.", "D.", "Fellbaum", ",", "D.", "Gross", ",", "K.", "Miller", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, algorithm, task, country, programming language, metric, researcher, person, location, university, product, field, conference and O.\nSentence: The construction of a rich lexicon with a suitable ontology requires significant effort , e.g. , Wordnet lexicon required many person-years of effort. G. A. Miller , R. Beckwith , C. D. Fellbaum , D. Gross , K. Miller .", "prompt_labels": "The(O) construction(O) of(O) a(O) rich(O) lexicon(O) with(O) a(O) suitable(O) ontology(O) requires(O) significant(O) effort(O) ,(O) e.g.(O) ,(O) Wordnet(B-product) lexicon(O) required(O) many(O) person-years(O) of(O) effort.(O) G.(B-researcher) A.(I-researcher) Miller(I-researcher) ,(O) R.(B-researcher) Beckwith(I-researcher) ,(O) C.(B-researcher) D.(I-researcher) Fellbaum(I-researcher) ,(O) D.(B-researcher) Gross(I-researcher) ,(O) K.(B-researcher) Miller(I-researcher) .(O)"}}
{"id": "323", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "product", "conference", "organization", "algorithm", "programming language", "location", "field", "university", "task", "person", "researcher", "country"], "instance": {"id": "323", "words": ["Pausch", "received", "two", "awards", "from", "Association", "for", "Computing", "Machinery", "in", "2007", "for", "his", "achievements", "in", "computing", "education", ":", "the", "Karl", "V.", "Karlstrom", "Outstanding", "Educator", "Award", "and", "the", "ACM", "SIGCSE", "Award", "for", "Outstanding", "Contributions", "to", "Computer", "Science", "Education", "."], "labels": ["B-researcher", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, product, conference, organization, algorithm, programming language, location, field, university, task, person, researcher, country and O.\nSentence: Pausch received two awards from Association for Computing Machinery in 2007 for his achievements in computing education : the Karl V. Karlstrom Outstanding Educator Award and the ACM SIGCSE Award for Outstanding Contributions to Computer Science Education .", "prompt_labels": "Pausch(B-researcher) received(O) two(O) awards(O) from(O) Association(B-conference) for(I-conference) Computing(I-conference) Machinery(I-conference) in(O) 2007(O) for(O) his(O) achievements(O) in(O) computing(B-field) education(I-field) :(O) the(O) Karl(O) V.(O) Karlstrom(O) Outstanding(O) Educator(O) Award(O) and(O) the(O) ACM(O) SIGCSE(O) Award(O) for(O) Outstanding(O) Contributions(O) to(O) Computer(O) Science(O) Education(O) .(O)"}}
{"id": "154", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "university", "researcher", "conference", "country", "organization", "programming language", "product", "task", "person", "algorithm", "field", "metric"], "instance": {"id": "154", "words": ["He", "is", "interested", "in", "knowledge", "representation", ",", "commonsense", "reasoning", ",", "and", "natural", "language", "understanding", ",", "believing", "that", "deep", "language", "understanding", "can", "only", "currently", "be", "achieved", "by", "significant", "hand-engineering", "of", "semantically-rich", "formalisms", "coupled", "with", "statistical", "preferences", "."], "labels": ["O", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "O", "B-task", "I-task", "I-task", "O", "O", "O", "B-task", "I-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, university, researcher, conference, country, organization, programming language, product, task, person, algorithm, field, metric and O.\nSentence: He is interested in knowledge representation , commonsense reasoning , and natural language understanding , believing that deep language understanding can only currently be achieved by significant hand-engineering of semantically-rich formalisms coupled with statistical preferences .", "prompt_labels": "He(O) is(O) interested(O) in(O) knowledge(B-task) representation(I-task) ,(O) commonsense(B-task) reasoning(I-task) ,(O) and(O) natural(B-task) language(I-task) understanding(I-task) ,(O) believing(O) that(O) deep(B-task) language(I-task) understanding(I-task) can(O) only(O) currently(O) be(O) achieved(O) by(O) significant(O) hand-engineering(O) of(O) semantically-rich(O) formalisms(O) coupled(O) with(O) statistical(O) preferences(O) .(O)"}}
{"id": "292", "dataset": "crossner_ai", "split": "test", "label_list": ["task", "field", "researcher", "metric", "organization", "programming language", "country", "person", "conference", "location", "university", "product", "algorithm"], "instance": {"id": "292", "words": ["The", "2001", "Scientific", "American", "article", "by", "Berners-Lee", ",", "James", "Hendler", ",", "and", "Ora", "Lassila", "described", "an", "expected", "evolution", "of", "the", "existing", "Web", "to", "a", "Semantic", "Web", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-researcher", "O", "B-researcher", "I-researcher", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "O", "B-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, field, researcher, metric, organization, programming language, country, person, conference, location, university, product, algorithm and O.\nSentence: The 2001 Scientific American article by Berners-Lee , James Hendler , and Ora Lassila described an expected evolution of the existing Web to a Semantic Web .", "prompt_labels": "The(O) 2001(O) Scientific(O) American(O) article(O) by(O) Berners-Lee(B-researcher) ,(O) James(B-researcher) Hendler(I-researcher) ,(O) and(O) Ora(B-researcher) Lassila(I-researcher) described(O) an(O) expected(O) evolution(O) of(O) the(O) existing(O) Web(B-product) to(O) a(O) Semantic(B-product) Web(I-product) .(O)"}}
{"id": "201", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "organization", "person", "location", "country", "task", "university", "metric", "field", "product", "algorithm", "programming language", "conference"], "instance": {"id": "201", "words": ["ANIMAL", "has", "been", "ported", "to", "R", ",", "a", "freely", "available", "language", "and", "environment", "for", "statistical", "computing", "and", "graphics", "."], "labels": ["B-product", "O", "O", "O", "O", "B-programming language", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, organization, person, location, country, task, university, metric, field, product, algorithm, programming language, conference and O.\nSentence: ANIMAL has been ported to R , a freely available language and environment for statistical computing and graphics .", "prompt_labels": "ANIMAL(B-product) has(O) been(O) ported(O) to(O) R(B-programming language) ,(O) a(O) freely(O) available(O) language(O) and(O) environment(O) for(O) statistical(B-field) computing(I-field) and(O) graphics(B-field) .(O)"}}
{"id": "179", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "algorithm", "conference", "person", "location", "task", "country", "organization", "metric", "university", "programming language", "field", "product"], "instance": {"id": "179", "words": ["Devol", "collaborated", "with", "Engelberger", ",", "who", "served", "as", "president", "of", "the", "company", ",", "to", "engineer", "and", "produce", "an", "industrial", "robot", "under", "the", "brand", "name", "Unimate", "."], "labels": ["B-researcher", "O", "O", "B-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "B-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, algorithm, conference, person, location, task, country, organization, metric, university, programming language, field, product and O.\nSentence: Devol collaborated with Engelberger , who served as president of the company , to engineer and produce an industrial robot under the brand name Unimate .", "prompt_labels": "Devol(B-researcher) collaborated(O) with(O) Engelberger(B-researcher) ,(O) who(O) served(O) as(O) president(O) of(O) the(O) company(O) ,(O) to(O) engineer(O) and(O) produce(O) an(O) industrial(B-product) robot(I-product) under(O) the(O) brand(O) name(O) Unimate(B-product) .(O)"}}
{"id": "413", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "conference", "person", "field", "location", "university", "country", "researcher", "task", "programming language", "organization", "metric", "product"], "instance": {"id": "413", "words": ["Delta", "robot", "s", "have", "base-mounted", "rotary", "actuator", "s", "that", "move", "a", "light", ",", "stiff", ",", "parallelogram", "arm", "."], "labels": ["B-product", "I-product", "O", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, conference, person, field, location, university, country, researcher, task, programming language, organization, metric, product and O.\nSentence: Delta robot s have base-mounted rotary actuator s that move a light , stiff , parallelogram arm .", "prompt_labels": "Delta(B-product) robot(I-product) s(O) have(O) base-mounted(O) rotary(B-product) actuator(I-product) s(O) that(O) move(O) a(O) light(O) ,(O) stiff(O) ,(O) parallelogram(O) arm(O) .(O)"}}
{"id": "34", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "product", "organization", "task", "conference", "location", "country", "algorithm", "researcher", "programming language", "university", "field", "metric"], "instance": {"id": "34", "words": ["At", "the", "IEEE", "International", "Conference", "on", "Image", "Processing", "in", "2010", ",", "Rui", "Hu", ",", "Mark", "Banard", ",", "and", "John", "Collomosse", "extended", "the", "HOG", "descriptor", "for", "use", "in", "sketch", "based", "image", "retrieval", "(", "SBIR", ")", "."], "labels": ["O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "B-researcher", "I-researcher", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "B-task", "I-task", "I-task", "I-task", "O", "B-task", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, product, organization, task, conference, location, country, algorithm, researcher, programming language, university, field, metric and O.\nSentence: At the IEEE International Conference on Image Processing in 2010 , Rui Hu , Mark Banard , and John Collomosse extended the HOG descriptor for use in sketch based image retrieval ( SBIR ) .", "prompt_labels": "At(O) the(O) IEEE(B-conference) International(I-conference) Conference(I-conference) on(I-conference) Image(I-conference) Processing(I-conference) in(O) 2010(O) ,(O) Rui(B-researcher) Hu(I-researcher) ,(O) Mark(B-researcher) Banard(I-researcher) ,(O) and(O) John(B-researcher) Collomosse(I-researcher) extended(O) the(O) HOG(B-algorithm) descriptor(I-algorithm) for(O) use(O) in(O) sketch(B-task) based(I-task) image(I-task) retrieval(I-task) ((O) SBIR(B-task) )(O) .(O)"}}
{"id": "106", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "product", "researcher", "field", "conference", "university", "task", "person", "location", "programming language", "algorithm", "country", "organization"], "instance": {"id": "106", "words": ["It", "currently", "includes", "four", "sub-competitions", "-", "the", "RoboMaster", "Robotics", "Competition", ",", "the", "RoboMaster", "Technical", "Challenge", ",", "the", "ICRA", "RoboMaster", "AI", "Challenge", ",", "and", "the", "new", "RoboMaster", "Youth", "Tournament", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, product, researcher, field, conference, university, task, person, location, programming language, algorithm, country, organization and O.\nSentence: It currently includes four sub-competitions - the RoboMaster Robotics Competition , the RoboMaster Technical Challenge , the ICRA RoboMaster AI Challenge , and the new RoboMaster Youth Tournament .", "prompt_labels": "It(O) currently(O) includes(O) four(O) sub-competitions(O) -(O) the(O) RoboMaster(O) Robotics(O) Competition(O) ,(O) the(O) RoboMaster(O) Technical(O) Challenge(O) ,(O) the(O) ICRA(O) RoboMaster(O) AI(O) Challenge(O) ,(O) and(O) the(O) new(O) RoboMaster(O) Youth(O) Tournament(O) .(O)"}}
{"id": "301", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "university", "algorithm", "country", "location", "field", "researcher", "conference", "task", "organization", "metric", "programming language", "product"], "instance": {"id": "301", "words": ["He", "co-developed", "optimal", "algorithms", "for", "Structure", "From", "Motion", "(", "SFM", ",", "or", "Visual", "SLAM", ",", "simultaneous", "localization", "and", "mapping", ",", "in", "Robotics", ";", "Best", "Paper", "Award", "at", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "1998", ")", ",", "characterized", "its", "ambiguities", "(", "David", "Marr", "Prize", "at", "ICCV", "1999", ")", ",", "also", "characterized", "the", "identifiability", "and", "observability", "of", "visual-inertial", "sensor", "fusion", "(", "Best", "Paper", "Award", "at", "Robotics", "2015", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-task", "I-task", "I-task", "O", "B-task", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "I-task", "I-task", "O", "O", "B-field", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, university, algorithm, country, location, field, researcher, conference, task, organization, metric, programming language, product and O.\nSentence: He co-developed optimal algorithms for Structure From Motion ( SFM , or Visual SLAM , simultaneous localization and mapping , in Robotics ; Best Paper Award at Conference on Computer Vision and Pattern Recognition 1998 ) , characterized its ambiguities ( David Marr Prize at ICCV 1999 ) , also characterized the identifiability and observability of visual-inertial sensor fusion ( Best Paper Award at Robotics 2015 ) .", "prompt_labels": "He(O) co-developed(O) optimal(O) algorithms(O) for(O) Structure(B-task) From(I-task) Motion(I-task) ((O) SFM(B-task) ,(O) or(O) Visual(B-task) SLAM(I-task) ,(O) simultaneous(B-task) localization(I-task) and(I-task) mapping(I-task) ,(O) in(O) Robotics(B-field) ;(O) Best(O) Paper(O) Award(O) at(O) Conference(B-conference) on(I-conference) Computer(I-conference) Vision(I-conference) and(I-conference) Pattern(I-conference) Recognition(I-conference) 1998(O) )(O) ,(O) characterized(O) its(O) ambiguities(O) ((O) David(O) Marr(O) Prize(O) at(O) ICCV(B-conference) 1999(I-conference) )(O) ,(O) also(O) characterized(O) the(O) identifiability(O) and(O) observability(O) of(O) visual-inertial(O) sensor(O) fusion(O) ((O) Best(O) Paper(O) Award(O) at(O) Robotics(B-field) 2015(O) )(O) .(O)"}}
{"id": "384", "dataset": "crossner_ai", "split": "test", "label_list": ["organization", "researcher", "person", "conference", "metric", "task", "field", "country", "programming language", "product", "location", "algorithm", "university"], "instance": {"id": "384", "words": ["He", "received", "his", "master", "'s", "degree", "in", "mathematics", "from", "the", "Samarkand", "State", "University", ",", "Samarkand", ",", "Uzbek", "Soviet", "Socialist", "Republic", "in", "1958", "and", "Ph.D", "in", "statistics", "at", "the", "Institute", "of", "Control", "Sciences", ",", "Moscow", "in", "1964", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-field", "O", "O", "B-university", "I-university", "I-university", "O", "B-location", "O", "B-country", "I-country", "I-country", "I-country", "O", "O", "O", "O", "O", "B-field", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-location", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, researcher, person, conference, metric, task, field, country, programming language, product, location, algorithm, university and O.\nSentence: He received his master 's degree in mathematics from the Samarkand State University , Samarkand , Uzbek Soviet Socialist Republic in 1958 and Ph.D in statistics at the Institute of Control Sciences , Moscow in 1964 .", "prompt_labels": "He(O) received(O) his(O) master(O) 's(O) degree(O) in(O) mathematics(B-field) from(O) the(O) Samarkand(B-university) State(I-university) University(I-university) ,(O) Samarkand(B-location) ,(O) Uzbek(B-country) Soviet(I-country) Socialist(I-country) Republic(I-country) in(O) 1958(O) and(O) Ph.D(O) in(O) statistics(B-field) at(O) the(O) Institute(B-organization) of(I-organization) Control(I-organization) Sciences(I-organization) ,(O) Moscow(B-location) in(O) 1964(O) .(O)"}}
{"id": "117", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "country", "programming language", "algorithm", "product", "location", "field", "organization", "researcher", "conference", "person", "metric", "task"], "instance": {"id": "117", "words": ["The", "trial", "of", "MICR", "E13B", "font", "was", "shown", "to", "the", "American", "Bankers", "Association", "(", "ABA", ")", "in", "July", "1956", ",", "which", "adopted", "it", "in", "1958", "as", "the", "MICR", "standard", "for", "negotiable", "document", "s", "in", "the", "United", "States", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, country, programming language, algorithm, product, location, field, organization, researcher, conference, person, metric, task and O.\nSentence: The trial of MICR E13B font was shown to the American Bankers Association ( ABA ) in July 1956 , which adopted it in 1958 as the MICR standard for negotiable document s in the United States .", "prompt_labels": "The(O) trial(O) of(O) MICR(O) E13B(O) font(O) was(O) shown(O) to(O) the(O) American(B-organization) Bankers(I-organization) Association(I-organization) ((O) ABA(B-organization) )(O) in(O) July(O) 1956(O) ,(O) which(O) adopted(O) it(O) in(O) 1958(O) as(O) the(O) MICR(O) standard(O) for(O) negotiable(O) document(O) s(O) in(O) the(O) United(B-country) States(I-country) .(O)"}}
{"id": "143", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "location", "person", "task", "university", "country", "metric", "programming language", "field", "conference", "algorithm", "product", "organization"], "instance": {"id": "143", "words": ["Neuromorphic", "engineering", "is", "an", "interdisciplinary", "subject", "that", "takes", "inspiration", "from", "biology", ",", "physics", ",", "mathematics", ",", "computer", "science", ",", "and", "electronic", "engineering", "to", "design", "artificial", "neural", "systems", ",", "such", "as", "vision", "systems", ",", "head-eye", "systems", ",", "auditory", "processors", ",", "and", "autonomous", "robots", ",", "whose", "physical", "architecture", "and", "design", "principles", "are", "based", "on", "those", "of", "biological", "nervous", "systems", "."], "labels": ["B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "O", "B-field", "O", "B-field", "O", "B-field", "I-field", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "B-product", "I-product", "O", "B-product", "I-product", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, location, person, task, university, country, metric, programming language, field, conference, algorithm, product, organization and O.\nSentence: Neuromorphic engineering is an interdisciplinary subject that takes inspiration from biology , physics , mathematics , computer science , and electronic engineering to design artificial neural systems , such as vision systems , head-eye systems , auditory processors , and autonomous robots , whose physical architecture and design principles are based on those of biological nervous systems .", "prompt_labels": "Neuromorphic(B-field) engineering(I-field) is(O) an(O) interdisciplinary(O) subject(O) that(O) takes(O) inspiration(O) from(O) biology(B-field) ,(O) physics(B-field) ,(O) mathematics(B-field) ,(O) computer(B-field) science(I-field) ,(O) and(O) electronic(B-field) engineering(I-field) to(O) design(O) artificial(O) neural(O) systems(O) ,(O) such(O) as(O) vision(B-product) systems(I-product) ,(O) head-eye(B-product) systems(I-product) ,(O) auditory(B-product) processors(I-product) ,(O) and(O) autonomous(B-product) robots(I-product) ,(O) whose(O) physical(O) architecture(O) and(O) design(O) principles(O) are(O) based(O) on(O) those(O) of(O) biological(B-product) nervous(I-product) systems(I-product) .(O)"}}
{"id": "269", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "task", "researcher", "location", "person", "field", "university", "organization", "product", "algorithm", "programming language", "metric", "country"], "instance": {"id": "269", "words": ["While", "studying", "at", "Stanford", ",", "Scheinman", "was", "awarded", "a", "fellowship", "sponsored", "by", "George", "Devol", ",", "the", "inventor", "of", "the", "Unimate", ",", "the", "first", "industrial", "robot", "."], "labels": ["O", "O", "O", "B-university", "O", "B-researcher", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "B-product", "O", "O", "O", "B-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, task, researcher, location, person, field, university, organization, product, algorithm, programming language, metric, country and O.\nSentence: While studying at Stanford , Scheinman was awarded a fellowship sponsored by George Devol , the inventor of the Unimate , the first industrial robot .", "prompt_labels": "While(O) studying(O) at(O) Stanford(B-university) ,(O) Scheinman(B-researcher) was(O) awarded(O) a(O) fellowship(O) sponsored(O) by(O) George(B-researcher) Devol(I-researcher) ,(O) the(O) inventor(O) of(O) the(O) Unimate(B-product) ,(O) the(O) first(O) industrial(B-product) robot(I-product) .(O)"}}
{"id": "375", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "university", "location", "product", "task", "researcher", "organization", "country", "conference", "algorithm", "field", "person", "metric"], "instance": {"id": "375", "words": ["A", "trial", "by", "RET", "in", "2011", "with", "Facial", "recognition", "system", "cameras", "mounted", "on", "trams", "made", "sure", "that", "people", "were", "banned", "from", "the", "city", "trams", "did", "not", "sneak", "on", "anyway", "."], "labels": ["O", "O", "O", "B-organization", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, university, location, product, task, researcher, organization, country, conference, algorithm, field, person, metric and O.\nSentence: A trial by RET in 2011 with Facial recognition system cameras mounted on trams made sure that people were banned from the city trams did not sneak on anyway .", "prompt_labels": "A(O) trial(O) by(O) RET(B-organization) in(O) 2011(O) with(O) Facial(B-product) recognition(I-product) system(I-product) cameras(O) mounted(O) on(O) trams(O) made(O) sure(O) that(O) people(O) were(O) banned(O) from(O) the(O) city(O) trams(O) did(O) not(O) sneak(O) on(O) anyway(O) .(O)"}}
{"id": "9", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "metric", "person", "location", "product", "researcher", "country", "task", "university", "organization", "programming language", "conference", "field"], "instance": {"id": "9", "words": ["This", "would", "include", "programs", "such", "as", "data", "analysis", "and", "extraction", "tools", ",", "spreadsheets", "(", "e.g.", "Excel", ")", ",", "databases", "(", "e.g.", "Access", ")", ",", "statistical", "analysis", "(", "e.g.", "SAS", ")", ",", "generalized", "audit", "software", "(", "e.g.", "ACL", ",", "Arbutus", ",", "EAS", ")", ",", "business", "intelligence", "(", "e.g.", "Crystal", "Reports", "and", "Business", "Objects", ")", ",", "etc", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "B-product", "O", "O", "B-field", "I-field", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "B-product", "O", "B-product", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "B-product", "I-product", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, metric, person, location, product, researcher, country, task, university, organization, programming language, conference, field and O.\nSentence: This would include programs such as data analysis and extraction tools , spreadsheets ( e.g. Excel ) , databases ( e.g. Access ) , statistical analysis ( e.g. SAS ) , generalized audit software ( e.g. ACL , Arbutus , EAS ) , business intelligence ( e.g. Crystal Reports and Business Objects ) , etc .", "prompt_labels": "This(O) would(O) include(O) programs(O) such(O) as(O) data(B-field) analysis(I-field) and(O) extraction(O) tools(O) ,(O) spreadsheets(O) ((O) e.g.(O) Excel(B-product) )(O) ,(O) databases(O) ((O) e.g.(O) Access(B-product) )(O) ,(O) statistical(B-field) analysis(I-field) ((O) e.g.(O) SAS(B-product) )(O) ,(O) generalized(O) audit(O) software(O) ((O) e.g.(O) ACL(B-product) ,(O) Arbutus(B-product) ,(O) EAS(B-product) )(O) ,(O) business(O) intelligence(O) ((O) e.g.(O) Crystal(B-product) Reports(I-product) and(O) Business(B-product) Objects(I-product) )(O) ,(O) etc(O) .(O)"}}
{"id": "133", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "conference", "university", "programming language", "researcher", "person", "location", "product", "algorithm", "task", "country", "organization", "metric"], "instance": {"id": "133", "words": ["With", "his", "students", "Sepp", "Hochreiter", ",", "Felix", "Gers", ",", "Fred", "Cummins", ",", "Alex", "Graves", ",", "and", "others", ",", "Schmidhuber", "published", "increasingly", "sophisticated", "versions", "of", "a", "type", "of", "recurrent", "neural", "network", "called", "the", "long", "short-term", "memory", "(", "LSTM", ")", "."], "labels": ["O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "B-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, conference, university, programming language, researcher, person, location, product, algorithm, task, country, organization, metric and O.\nSentence: With his students Sepp Hochreiter , Felix Gers , Fred Cummins , Alex Graves , and others , Schmidhuber published increasingly sophisticated versions of a type of recurrent neural network called the long short-term memory ( LSTM ) .", "prompt_labels": "With(O) his(O) students(O) Sepp(B-researcher) Hochreiter(I-researcher) ,(O) Felix(B-researcher) Gers(I-researcher) ,(O) Fred(B-researcher) Cummins(I-researcher) ,(O) Alex(B-researcher) Graves(I-researcher) ,(O) and(O) others(O) ,(O) Schmidhuber(B-researcher) published(O) increasingly(O) sophisticated(O) versions(O) of(O) a(O) type(O) of(O) recurrent(B-algorithm) neural(I-algorithm) network(I-algorithm) called(O) the(O) long(B-algorithm) short-term(I-algorithm) memory(I-algorithm) ((O) LSTM(B-algorithm) )(O) .(O)"}}
{"id": "376", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "university", "researcher", "person", "organization", "task", "programming language", "product", "field", "algorithm", "location", "conference", "metric"], "instance": {"id": "376", "words": ["The", "film", ",", "adapted", "from", "the", "popular", "Cole", "Porter", "Broadway", "musical", ",", "starred", "the", "MGM", "songbird", "team", "of", "Howard", "Keel", "and", "Kathryn", "Grayson", "as", "the", "leads", ",", "supported", "by", "Ann", "Miller", ",", "Keenan", "Wynn", ",", "Bobby", "Van", ",", "James", "Whitmore", ",", "Kurt", "Kasznar", "and", "Tommy", "Rall", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, researcher, person, organization, task, programming language, product, field, algorithm, location, conference, metric and O.\nSentence: The film , adapted from the popular Cole Porter Broadway musical , starred the MGM songbird team of Howard Keel and Kathryn Grayson as the leads , supported by Ann Miller , Keenan Wynn , Bobby Van , James Whitmore , Kurt Kasznar and Tommy Rall .", "prompt_labels": "The(O) film(O) ,(O) adapted(O) from(O) the(O) popular(O) Cole(B-person) Porter(I-person) Broadway(B-organization) musical(O) ,(O) starred(O) the(O) MGM(O) songbird(O) team(O) of(O) Howard(B-person) Keel(I-person) and(O) Kathryn(B-person) Grayson(I-person) as(O) the(O) leads(O) ,(O) supported(O) by(O) Ann(B-person) Miller(I-person) ,(O) Keenan(B-person) Wynn(I-person) ,(O) Bobby(B-person) Van(I-person) ,(O) James(B-person) Whitmore(I-person) ,(O) Kurt(B-person) Kasznar(I-person) and(O) Tommy(B-person) Rall(I-person) .(O)"}}
{"id": "351", "dataset": "crossner_ai", "split": "test", "label_list": ["programming language", "location", "product", "country", "researcher", "university", "person", "algorithm", "conference", "field", "organization", "metric", "task"], "instance": {"id": "351", "words": ["|", "Apple", "Apple", "Inc", "originally", "licensed", "software", "from", "Nuance", "to", "provide", "speech", "recognition", "capability", "to", "its", "digital", "assistant", "Siri", "."], "labels": ["O", "B-organization", "B-organization", "I-organization", "O", "O", "O", "O", "B-organization", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "B-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, location, product, country, researcher, university, person, algorithm, conference, field, organization, metric, task and O.\nSentence: | Apple Apple Inc originally licensed software from Nuance to provide speech recognition capability to its digital assistant Siri .", "prompt_labels": "|(O) Apple(B-organization) Apple(B-organization) Inc(I-organization) originally(O) licensed(O) software(O) from(O) Nuance(B-organization) to(O) provide(O) speech(B-task) recognition(I-task) capability(O) to(O) its(O) digital(O) assistant(O) Siri(B-product) .(O)"}}
{"id": "239", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "university", "task", "product", "programming language", "organization", "field", "person", "researcher", "algorithm", "conference", "location", "metric"], "instance": {"id": "239", "words": ["Euler", "Math", "Toolbox", "uses", "a", "matrix", "language", "similar", "to", "MATLAB", ",", "a", "system", "that", "had", "been", "under", "development", "since", "the", "1970s", "."], "labels": ["B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, task, product, programming language, organization, field, person, researcher, algorithm, conference, location, metric and O.\nSentence: Euler Math Toolbox uses a matrix language similar to MATLAB , a system that had been under development since the 1970s .", "prompt_labels": "Euler(B-product) Math(I-product) Toolbox(I-product) uses(O) a(O) matrix(O) language(O) similar(O) to(O) MATLAB(B-product) ,(O) a(O) system(O) that(O) had(O) been(O) under(O) development(O) since(O) the(O) 1970s(O) .(O)"}}
{"id": "286", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "organization", "university", "country", "task", "person", "metric", "product", "algorithm", "location", "conference", "programming language", "researcher"], "instance": {"id": "286", "words": ["It", "needs", "to", "Object", "recognition", ",", "recognize", "and", "locate", "humans", "and", "further", "emotion", "recognition", "."], "labels": ["O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, organization, university, country, task, person, metric, product, algorithm, location, conference, programming language, researcher and O.\nSentence: It needs to Object recognition , recognize and locate humans and further emotion recognition .", "prompt_labels": "It(O) needs(O) to(O) Object(B-task) recognition(I-task) ,(O) recognize(O) and(O) locate(O) humans(O) and(O) further(O) emotion(B-task) recognition(I-task) .(O)"}}
{"id": "65", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "country", "university", "location", "algorithm", "programming language", "organization", "metric", "person", "task", "conference", "field", "product"], "instance": {"id": "65", "words": ["In", "1997", "Thrun", "and", "his", "colleagues", "Wolfram", "Burgard", "and", "Dieter", "Fox", "developed", "the", "world", "'s", "first", "robotic", "tour", "guide", "in", "the", "Deutsches", "Museum", "Bonn", "(", "1997", ")", "."], "labels": ["O", "O", "B-researcher", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, country, university, location, algorithm, programming language, organization, metric, person, task, conference, field, product and O.\nSentence: In 1997 Thrun and his colleagues Wolfram Burgard and Dieter Fox developed the world 's first robotic tour guide in the Deutsches Museum Bonn ( 1997 ) .", "prompt_labels": "In(O) 1997(O) Thrun(B-researcher) and(O) his(O) colleagues(O) Wolfram(B-researcher) Burgard(I-researcher) and(O) Dieter(B-researcher) Fox(I-researcher) developed(O) the(O) world(O) 's(O) first(O) robotic(B-product) tour(I-product) guide(I-product) in(O) the(O) Deutsches(B-location) Museum(I-location) Bonn(I-location) ((O) 1997(O) )(O) .(O)"}}
{"id": "41", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "algorithm", "organization", "programming language", "location", "product", "researcher", "country", "conference", "field", "task", "person", "university"], "instance": {"id": "41", "words": ["Speech", "waveforms", "are", "generated", "from", "HMMs", "themselves", "based", "on", "the", "maximum", "likelihood", "criterion", "."], "labels": ["O", "O", "O", "O", "O", "B-algorithm", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, algorithm, organization, programming language, location, product, researcher, country, conference, field, task, person, university and O.\nSentence: Speech waveforms are generated from HMMs themselves based on the maximum likelihood criterion .", "prompt_labels": "Speech(O) waveforms(O) are(O) generated(O) from(O) HMMs(B-algorithm) themselves(O) based(O) on(O) the(O) maximum(B-algorithm) likelihood(I-algorithm) criterion(O) .(O)"}}
{"id": "346", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "location", "country", "algorithm", "organization", "university", "product", "researcher", "task", "programming language", "person", "field", "conference"], "instance": {"id": "346", "words": ["A", "voice-user", "interface", "(", "VUI", ")", "makes", "spoken", "human", "interaction", "with", "computers", "possible", ",", "using", "speech", "recognition", "to", "understand", "spoken", "commands", "and", "Question", "answering", ",", "and", "typically", "text", "to", "speech", "to", "play", "a", "reply", "."], "labels": ["O", "B-product", "I-product", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "B-task", "I-task", "I-task", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, location, country, algorithm, organization, university, product, researcher, task, programming language, person, field, conference and O.\nSentence: A voice-user interface ( VUI ) makes spoken human interaction with computers possible , using speech recognition to understand spoken commands and Question answering , and typically text to speech to play a reply .", "prompt_labels": "A(O) voice-user(B-product) interface(I-product) ((O) VUI(B-product) )(O) makes(O) spoken(O) human(O) interaction(O) with(O) computers(O) possible(O) ,(O) using(O) speech(B-task) recognition(I-task) to(O) understand(O) spoken(O) commands(O) and(O) Question(B-task) answering(I-task) ,(O) and(O) typically(O) text(B-task) to(I-task) speech(I-task) to(O) play(O) a(O) reply(O) .(O)"}}
{"id": "36", "dataset": "crossner_ai", "split": "test", "label_list": ["task", "algorithm", "product", "country", "field", "metric", "location", "organization", "person", "researcher", "university", "programming language", "conference"], "instance": {"id": "36", "words": ["For", "the", "case", "of", "a", "general", "base", "space", "math", "(", "Y", ",", "\\", "mathcal", "{", "B", "}", ",", "\\", "nu", ")", "/", "math", "(", "i.e.", "a", "base", "space", "which", "is", "not", "countable", ")", ",", "one", "typically", "considers", "the", "relative", "entropy", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, algorithm, product, country, field, metric, location, organization, person, researcher, university, programming language, conference and O.\nSentence: For the case of a general base space math ( Y , \\ mathcal { B } , \\ nu ) / math ( i.e. a base space which is not countable ) , one typically considers the relative entropy .", "prompt_labels": "For(O) the(O) case(O) of(O) a(O) general(O) base(O) space(O) math(O) ((O) Y(O) ,(O) \\(O) mathcal(O) {(O) B(O) }(O) ,(O) \\(O) nu(O) )(O) /(O) math(O) ((O) i.e.(O) a(O) base(O) space(O) which(O) is(O) not(O) countable(O) )(O) ,(O) one(O) typically(O) considers(O) the(O) relative(B-metric) entropy(I-metric) .(O)"}}
{"id": "391", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "university", "location", "researcher", "field", "person", "metric", "algorithm", "product", "programming language", "task", "organization", "country"], "instance": {"id": "391", "words": ["In", "recent", "research", ",", "kernel-based", "methods", "such", "as", "support", "vector", "machine", "s", "have", "shown", "superior", "performance", "in", "supervised", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "B-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, university, location, researcher, field, person, metric, algorithm, product, programming language, task, organization, country and O.\nSentence: In recent research , kernel-based methods such as support vector machine s have shown superior performance in supervised .", "prompt_labels": "In(O) recent(O) research(O) ,(O) kernel-based(O) methods(O) such(O) as(O) support(B-algorithm) vector(I-algorithm) machine(I-algorithm) s(O) have(O) shown(O) superior(O) performance(O) in(O) supervised(B-field) .(O)"}}
{"id": "290", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "researcher", "conference", "country", "programming language", "task", "location", "field", "person", "university", "algorithm", "product", "organization"], "instance": {"id": "290", "words": ["The", "activation", "function", "of", "the", "LSTM", "gates", "is", "often", "the", "logistic", "sigmoid", "function", "."], "labels": ["O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, researcher, conference, country, programming language, task, location, field, person, university, algorithm, product, organization and O.\nSentence: The activation function of the LSTM gates is often the logistic sigmoid function .", "prompt_labels": "The(O) activation(O) function(O) of(O) the(O) LSTM(B-algorithm) gates(I-algorithm) is(O) often(O) the(O) logistic(B-algorithm) sigmoid(I-algorithm) function(I-algorithm) .(O)"}}
{"id": "122", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "person", "task", "conference", "organization", "researcher", "product", "algorithm", "field", "country", "programming language", "university", "metric"], "instance": {"id": "122", "words": ["There", "are", "a", "number", "of", "other", "metrics", ",", "most", "simply", "the", "accuracy", "or", "Fraction", "Correct", "(", "FC", ")", ",", "which", "measures", "the", "fraction", "of", "all", "instances", "that", "are", "correctly", "categorized", ";", "the", "complement", "is", "the", "Fraction", "Incorrect", "(", "FiC", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "B-metric", "I-metric", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "B-metric", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, task, conference, organization, researcher, product, algorithm, field, country, programming language, university, metric and O.\nSentence: There are a number of other metrics , most simply the accuracy or Fraction Correct ( FC ) , which measures the fraction of all instances that are correctly categorized ; the complement is the Fraction Incorrect ( FiC ) .", "prompt_labels": "There(O) are(O) a(O) number(O) of(O) other(O) metrics(O) ,(O) most(O) simply(O) the(O) accuracy(B-metric) or(O) Fraction(B-metric) Correct(I-metric) ((O) FC(B-metric) )(O) ,(O) which(O) measures(O) the(O) fraction(O) of(O) all(O) instances(O) that(O) are(O) correctly(O) categorized(O) ;(O) the(O) complement(O) is(O) the(O) Fraction(B-metric) Incorrect(I-metric) ((O) FiC(B-metric) )(O) .(O)"}}
{"id": "266", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "person", "metric", "task", "algorithm", "organization", "conference", "country", "programming language", "field", "location", "researcher", "product"], "instance": {"id": "266", "words": ["In", "the", "film", "Westworld", ",", "female", "robots", "actually", "engaged", "in", "intercourse", "with", "human", "men", "as", "part", "of", "the", "make-believe", "vacation", "world", "human", "customers", "paid", "to", "attend", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, person, metric, task, algorithm, organization, conference, country, programming language, field, location, researcher, product and O.\nSentence: In the film Westworld , female robots actually engaged in intercourse with human men as part of the make-believe vacation world human customers paid to attend .", "prompt_labels": "In(O) the(O) film(O) Westworld(O) ,(O) female(O) robots(O) actually(O) engaged(O) in(O) intercourse(O) with(O) human(O) men(O) as(O) part(O) of(O) the(O) make-believe(O) vacation(O) world(O) human(O) customers(O) paid(O) to(O) attend(O) .(O)"}}
{"id": "109", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "algorithm", "researcher", "university", "organization", "metric", "field", "task", "location", "programming language", "country", "person", "product"], "instance": {"id": "109", "words": ["The", "MATLAB", "function", ","], "labels": ["O", "B-product", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, algorithm, researcher, university, organization, metric, field, task, location, programming language, country, person, product and O.\nSentence: The MATLAB function ,", "prompt_labels": "The(O) MATLAB(B-product) function(O) ,(O)"}}
{"id": "277", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "metric", "algorithm", "field", "organization", "location", "task", "programming language", "product", "conference", "university", "person", "country"], "instance": {"id": "277", "words": ["Hinton", "-", "together", "with", "Yoshua", "Bengio", "and", "Yann", "LeCun", "-", "are", "referred", "to", "by", "some", "as", "the", "Godfathers", "of", "AI", "and", "Godfathers", "of", "Deep", "Learning", "."], "labels": ["B-researcher", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, metric, algorithm, field, organization, location, task, programming language, product, conference, university, person, country and O.\nSentence: Hinton - together with Yoshua Bengio and Yann LeCun - are referred to by some as the Godfathers of AI and Godfathers of Deep Learning .", "prompt_labels": "Hinton(B-researcher) -(O) together(O) with(O) Yoshua(B-researcher) Bengio(I-researcher) and(O) Yann(B-researcher) LeCun(I-researcher) -(O) are(O) referred(O) to(O) by(O) some(O) as(O) the(O) Godfathers(O) of(O) AI(O) and(O) Godfathers(O) of(O) Deep(O) Learning(O) .(O)"}}
{"id": "238", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "field", "country", "programming language", "person", "conference", "researcher", "university", "algorithm", "location", "organization", "task", "metric"], "instance": {"id": "238", "words": ["Together", "with", "Yann", "LeCun", ",", "and", "Yoshua", "Bengio", ",", "Hinton", "won", "the", "2018", "Turing", "Award", "for", "conceptual", "and", "engineering", "breakthroughs", "that", "have", "made", "deep", "neural", "networks", "a", "critical", "component", "of", "computing", "."], "labels": ["O", "O", "B-researcher", "I-researcher", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, field, country, programming language, person, conference, researcher, university, algorithm, location, organization, task, metric and O.\nSentence: Together with Yann LeCun , and Yoshua Bengio , Hinton won the 2018 Turing Award for conceptual and engineering breakthroughs that have made deep neural networks a critical component of computing .", "prompt_labels": "Together(O) with(O) Yann(B-researcher) LeCun(I-researcher) ,(O) and(O) Yoshua(B-researcher) Bengio(I-researcher) ,(O) Hinton(B-researcher) won(O) the(O) 2018(O) Turing(O) Award(O) for(O) conceptual(O) and(O) engineering(O) breakthroughs(O) that(O) have(O) made(O) deep(B-algorithm) neural(I-algorithm) networks(I-algorithm) a(O) critical(O) component(O) of(O) computing(O) .(O)"}}
{"id": "24", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "task", "person", "country", "location", "university", "organization", "conference", "researcher", "product", "field", "programming language", "algorithm"], "instance": {"id": "24", "words": ["For", "instance", ",", "one", "can", "combine", "some", "measure", "based", "on", "the", "confusion", "matrix", "with", "the", "mean", "squared", "error", "evaluated", "between", "the", "raw", "model", "outputs", "and", "the", "actual", "values", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, task, person, country, location, university, organization, conference, researcher, product, field, programming language, algorithm and O.\nSentence: For instance , one can combine some measure based on the confusion matrix with the mean squared error evaluated between the raw model outputs and the actual values .", "prompt_labels": "For(O) instance(O) ,(O) one(O) can(O) combine(O) some(O) measure(O) based(O) on(O) the(O) confusion(B-metric) matrix(I-metric) with(O) the(O) mean(B-metric) squared(I-metric) error(I-metric) evaluated(O) between(O) the(O) raw(O) model(O) outputs(O) and(O) the(O) actual(O) values(O) .(O)"}}
{"id": "422", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "university", "algorithm", "metric", "conference", "person", "programming language", "country", "task", "organization", "researcher", "location", "product"], "instance": {"id": "422", "words": ["Other", "areas", "of", "usage", "for", "ontologies", "within", "NLP", "include", "information", "retrieval", ",", "information", "extraction", "and", "automatic", "summarization", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-field", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, university, algorithm, metric, conference, person, programming language, country, task, organization, researcher, location, product and O.\nSentence: Other areas of usage for ontologies within NLP include information retrieval , information extraction and automatic summarization .", "prompt_labels": "Other(O) areas(O) of(O) usage(O) for(O) ontologies(O) within(O) NLP(B-field) include(O) information(B-task) retrieval(I-task) ,(O) information(B-task) extraction(I-task) and(O) automatic(B-task) summarization(I-task) .(O)"}}
{"id": "420", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "conference", "organization", "person", "country", "task", "researcher", "field", "location", "algorithm", "programming language", "metric", "university"], "instance": {"id": "420", "words": ["The", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "has", "studied", "this", "topic", "in", "depth"], "labels": ["O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, conference, organization, person, country, task, researcher, field, location, algorithm, programming language, metric, university and O.\nSentence: The Association for the Advancement of Artificial Intelligence has studied this topic in depth", "prompt_labels": "The(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) has(O) studied(O) this(O) topic(O) in(O) depth(O)"}}
{"id": "313", "dataset": "crossner_ai", "split": "test", "label_list": ["field", "location", "country", "conference", "researcher", "person", "organization", "programming language", "product", "task", "metric", "algorithm", "university"], "instance": {"id": "313", "words": ["Eyring", "Research", "Institute", "was", "instrumental", "to", "the", "U.S.", "Air", "Force", "Missile", "Directorate", "at", "Hill", "Air", "Force", "Base", "near", "Ogden", ",", "Utah", "to", "produce", "in", "top", "military", "secrecy", ",", "the", "Intelligent", "Systems", "Technology", "Software", "that", "was", "foundational", "to", "the", "later", "named", "Reagan", "Star", "Wars", "program", "."], "labels": ["B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-location", "I-location", "I-location", "I-location", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, location, country, conference, researcher, person, organization, programming language, product, task, metric, algorithm, university and O.\nSentence: Eyring Research Institute was instrumental to the U.S. Air Force Missile Directorate at Hill Air Force Base near Ogden , Utah to produce in top military secrecy , the Intelligent Systems Technology Software that was foundational to the later named Reagan Star Wars program .", "prompt_labels": "Eyring(B-organization) Research(I-organization) Institute(I-organization) was(O) instrumental(O) to(O) the(O) U.S.(B-organization) Air(I-organization) Force(I-organization) Missile(I-organization) Directorate(I-organization) at(O) Hill(B-location) Air(I-location) Force(I-location) Base(I-location) near(O) Ogden(B-location) ,(O) Utah(B-location) to(O) produce(O) in(O) top(O) military(O) secrecy(O) ,(O) the(O) Intelligent(B-product) Systems(I-product) Technology(I-product) Software(I-product) that(O) was(O) foundational(O) to(O) the(O) later(O) named(O) Reagan(B-product) Star(I-product) Wars(I-product) program(I-product) .(O)"}}
{"id": "327", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "product", "programming language", "task", "person", "university", "field", "location", "algorithm", "organization", "metric", "country", "conference"], "instance": {"id": "327", "words": ["In", "addition", "to", "maintaining", "the", "Discovery", "One", "spacecraft", "systems", "during", "the", "interplanetary", "mission", "to", "Jupiter", "(", "or", "Saturn", "in", "the", "novel", ")", ",", "HAL", "is", "capable", "of", "speech", "synthesis", ",", "speech", "recognition", ",", "facial", "recognition", ",", "natural", "language", "processing", ",", "lip", "reading", ",", "art", "appreciation", ",", "Affective", "computing", ",", "automated", "reasoning", ",", "spacecraft", "piloting", "and", "playing", "chess", "."], "labels": ["O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-field", "I-field", "I-field", "O", "B-task", "I-task", "O", "B-field", "I-field", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, product, programming language, task, person, university, field, location, algorithm, organization, metric, country, conference and O.\nSentence: In addition to maintaining the Discovery One spacecraft systems during the interplanetary mission to Jupiter ( or Saturn in the novel ) , HAL is capable of speech synthesis , speech recognition , facial recognition , natural language processing , lip reading , art appreciation , Affective computing , automated reasoning , spacecraft piloting and playing chess .", "prompt_labels": "In(O) addition(O) to(O) maintaining(O) the(O) Discovery(B-product) One(I-product) spacecraft(I-product) systems(I-product) during(O) the(O) interplanetary(O) mission(O) to(O) Jupiter(O) ((O) or(O) Saturn(O) in(O) the(O) novel(O) )(O) ,(O) HAL(B-product) is(O) capable(O) of(O) speech(B-task) synthesis(I-task) ,(O) speech(B-task) recognition(I-task) ,(O) facial(B-task) recognition(I-task) ,(O) natural(B-field) language(I-field) processing(I-field) ,(O) lip(B-task) reading(I-task) ,(O) art(B-field) appreciation(I-field) ,(O) Affective(B-task) computing(I-task) ,(O) automated(B-task) reasoning(I-task) ,(O) spacecraft(B-task) piloting(I-task) and(O) playing(B-task) chess(I-task) .(O)"}}
{"id": "352", "dataset": "crossner_ai", "split": "test", "label_list": ["person", "location", "organization", "conference", "country", "product", "researcher", "algorithm", "programming language", "field", "metric", "task", "university"], "instance": {"id": "352", "words": ["Columbia", "released", "several", "3D", "westerns", "produced", "by", "Sam", "Katzman", "and", "directed", "by", "William", "Castle", "."], "labels": ["B-organization", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, organization, conference, country, product, researcher, algorithm, programming language, field, metric, task, university and O.\nSentence: Columbia released several 3D westerns produced by Sam Katzman and directed by William Castle .", "prompt_labels": "Columbia(B-organization) released(O) several(O) 3D(O) westerns(O) produced(O) by(O) Sam(B-person) Katzman(I-person) and(O) directed(O) by(O) William(B-person) Castle(I-person) .(O)"}}
{"id": "255", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "researcher", "programming language", "metric", "product", "algorithm", "organization", "person", "task", "country", "location", "field", "university"], "instance": {"id": "255", "words": ["In", "artificial", "intelligence", ",", "Marvin", "Minsky", ",", "Herbert", "A.", "Simon", ",", "and", "Allen", "Newell", "are", "prominent", "."], "labels": ["O", "B-field", "I-field", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "I-researcher", "O", "O", "B-researcher", "I-researcher", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, researcher, programming language, metric, product, algorithm, organization, person, task, country, location, field, university and O.\nSentence: In artificial intelligence , Marvin Minsky , Herbert A. Simon , and Allen Newell are prominent .", "prompt_labels": "In(O) artificial(B-field) intelligence(I-field) ,(O) Marvin(B-researcher) Minsky(I-researcher) ,(O) Herbert(B-researcher) A.(I-researcher) Simon(I-researcher) ,(O) and(O) Allen(B-researcher) Newell(I-researcher) are(O) prominent(O) .(O)"}}
{"id": "101", "dataset": "crossner_ai", "split": "test", "label_list": ["researcher", "conference", "person", "location", "product", "metric", "task", "field", "algorithm", "programming language", "university", "organization", "country"], "instance": {"id": "101", "words": ["This", "tends", "to", "yield", "very", "large", "performance", "gains", "when", "working", "with", "large", "corpora", "such", "as", "WordNet", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, conference, person, location, product, metric, task, field, algorithm, programming language, university, organization, country and O.\nSentence: This tends to yield very large performance gains when working with large corpora such as WordNet .", "prompt_labels": "This(O) tends(O) to(O) yield(O) very(O) large(O) performance(O) gains(O) when(O) working(O) with(O) large(O) corpora(O) such(O) as(O) WordNet(B-product) .(O)"}}
{"id": "244", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "university", "metric", "programming language", "researcher", "task", "person", "location", "product", "conference", "algorithm", "organization", "field"], "instance": {"id": "244", "words": ["An", "n", "-gram", "model", "is", "a", "type", "of", "probabilistic", "language", "model", "for", "predicting", "the", "next", "item", "in", "such", "a", "sequence", "in", "the", "form", "of", "a", "(", "n", "−", "1", ")", "-order", "Markov", "model", ".efficiently", "."], "labels": ["O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, metric, programming language, researcher, task, person, location, product, conference, algorithm, organization, field and O.\nSentence: An n -gram model is a type of probabilistic language model for predicting the next item in such a sequence in the form of a ( n − 1 ) -order Markov model .efficiently .", "prompt_labels": "An(O) n(B-algorithm) -gram(I-algorithm) model(I-algorithm) is(O) a(O) type(O) of(O) probabilistic(B-algorithm) language(I-algorithm) model(I-algorithm) for(O) predicting(O) the(O) next(O) item(O) in(O) such(O) a(O) sequence(O) in(O) the(O) form(O) of(O) a(O) ((O) n(O) −(O) 1(O) )(O) -order(O) Markov(B-algorithm) model(I-algorithm) .efficiently(O) .(O)"}}
{"id": "14", "dataset": "crossner_ai", "split": "test", "label_list": ["country", "researcher", "product", "location", "organization", "conference", "programming language", "field", "algorithm", "task", "university", "metric", "person"], "instance": {"id": "14", "words": ["Prova", "is", "an", "open", "source", "programming", "language", "that", "combines", "Prolog", "with", "Java", "."], "labels": ["B-programming language", "O", "O", "O", "O", "O", "O", "O", "O", "B-programming language", "O", "B-programming language", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, researcher, product, location, organization, conference, programming language, field, algorithm, task, university, metric, person and O.\nSentence: Prova is an open source programming language that combines Prolog with Java .", "prompt_labels": "Prova(B-programming language) is(O) an(O) open(O) source(O) programming(O) language(O) that(O) combines(O) Prolog(B-programming language) with(O) Java(B-programming language) .(O)"}}
{"id": "289", "dataset": "crossner_ai", "split": "test", "label_list": ["product", "field", "conference", "location", "person", "researcher", "task", "programming language", "university", "country", "organization", "algorithm", "metric"], "instance": {"id": "289", "words": ["Machine", "vision", "as", "a", "systems", "engineering", "discipline", "can", "be", "considered", "distinct", "from", "computer", "vision", ",", "a", "form", "of", "computer", "science", "."], "labels": ["B-field", "I-field", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, field, conference, location, person, researcher, task, programming language, university, country, organization, algorithm, metric and O.\nSentence: Machine vision as a systems engineering discipline can be considered distinct from computer vision , a form of computer science .", "prompt_labels": "Machine(B-field) vision(I-field) as(O) a(O) systems(B-field) engineering(I-field) discipline(O) can(O) be(O) considered(O) distinct(O) from(O) computer(B-field) vision(I-field) ,(O) a(O) form(O) of(O) computer(B-field) science(I-field) .(O)"}}
{"id": "425", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "programming language", "field", "researcher", "university", "person", "algorithm", "location", "conference", "product", "organization", "country", "task"], "instance": {"id": "425", "words": ["Events", "are", "held", "worldwide", ",", "and", "are", "most", "popular", "in", "the", "United", "Kingdom", ",", "United", "States", ",", "Japan", ",", "Singapore", ",", "India", ",", "South", "Korea", "and", "becoming", "popular", "in", "subcontinent", "countries", "such", "as", "Sri", "Lanka", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "B-country", "I-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, programming language, field, researcher, university, person, algorithm, location, conference, product, organization, country, task and O.\nSentence: Events are held worldwide , and are most popular in the United Kingdom , United States , Japan , Singapore , India , South Korea and becoming popular in subcontinent countries such as Sri Lanka .", "prompt_labels": "Events(O) are(O) held(O) worldwide(O) ,(O) and(O) are(O) most(O) popular(O) in(O) the(O) United(B-country) Kingdom(I-country) ,(O) United(B-country) States(I-country) ,(O) Japan(B-country) ,(O) Singapore(B-country) ,(O) India(B-country) ,(O) South(B-country) Korea(I-country) and(O) becoming(O) popular(O) in(O) subcontinent(O) countries(O) such(O) as(O) Sri(B-country) Lanka(I-country) .(O)"}}
{"id": "49", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "programming language", "country", "field", "conference", "task", "metric", "researcher", "person", "location", "organization", "university", "product"], "instance": {"id": "49", "words": ["Examples", "include", "control", ",", "planning", "and", "scheduling", ",", "the", "ability", "to", "answer", "diagnostic", "and", "consumer", "questions", ",", "handwriting", "recognition", ",", "natural", "language", "understanding", ",", "speech", "recognition", "and", "facial", "recognition", "."], "labels": ["O", "O", "B-task", "O", "B-task", "I-task", "I-task", "O", "O", "O", "O", "B-task", "I-task", "I-task", "I-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, programming language, country, field, conference, task, metric, researcher, person, location, organization, university, product and O.\nSentence: Examples include control , planning and scheduling , the ability to answer diagnostic and consumer questions , handwriting recognition , natural language understanding , speech recognition and facial recognition .", "prompt_labels": "Examples(O) include(O) control(B-task) ,(O) planning(B-task) and(I-task) scheduling(I-task) ,(O) the(O) ability(O) to(O) answer(B-task) diagnostic(I-task) and(I-task) consumer(I-task) questions(I-task) ,(O) handwriting(B-task) recognition(I-task) ,(O) natural(B-task) language(I-task) understanding(I-task) ,(O) speech(B-task) recognition(I-task) and(O) facial(B-task) recognition(I-task) .(O)"}}
{"id": "28", "dataset": "crossner_ai", "split": "test", "label_list": ["conference", "location", "organization", "person", "metric", "university", "programming language", "task", "algorithm", "researcher", "product", "field", "country"], "instance": {"id": "28", "words": ["She", "served", "as", "Program", "Chair", "of", "International", "Conference", "on", "Computer", "Vision", "2021", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, location, organization, person, metric, university, programming language, task, algorithm, researcher, product, field, country and O.\nSentence: She served as Program Chair of International Conference on Computer Vision 2021 .", "prompt_labels": "She(O) served(O) as(O) Program(O) Chair(O) of(O) International(B-conference) Conference(I-conference) on(I-conference) Computer(I-conference) Vision(I-conference) 2021(I-conference) .(O)"}}
{"id": "145", "dataset": "crossner_ai", "split": "test", "label_list": ["location", "organization", "task", "algorithm", "product", "person", "researcher", "university", "programming language", "country", "field", "metric", "conference"], "instance": {"id": "145", "words": ["2", "The", "program", "was", "rewritten", "in", "Java", "beginning", "in", "1998", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-programming language", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, task, algorithm, product, person, researcher, university, programming language, country, field, metric, conference and O.\nSentence: 2 The program was rewritten in Java beginning in 1998 .", "prompt_labels": "2(O) The(O) program(O) was(O) rewritten(O) in(O) Java(B-programming language) beginning(O) in(O) 1998(O) .(O)"}}
{"id": "344", "dataset": "crossner_ai", "split": "test", "label_list": ["algorithm", "organization", "task", "country", "metric", "conference", "location", "field", "university", "product", "person", "programming language", "researcher"], "instance": {"id": "344", "words": ["Cognitive", "maps", "serve", "the", "construction", "and", "accumulation", "of", "spatial", "knowledge", ",", "allowing", "the", "mind", "'s", "eye", "to", "visualize", "images", "in", "order", "to", "reduce", "cognitive", "load", ",", "enhance", "recall", "and", "learning", "of", "information", "."], "labels": ["B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, organization, task, country, metric, conference, location, field, university, product, person, programming language, researcher and O.\nSentence: Cognitive maps serve the construction and accumulation of spatial knowledge , allowing the mind 's eye to visualize images in order to reduce cognitive load , enhance recall and learning of information .", "prompt_labels": "Cognitive(B-algorithm) maps(I-algorithm) serve(O) the(O) construction(O) and(O) accumulation(O) of(O) spatial(O) knowledge(O) ,(O) allowing(O) the(O) mind(O) 's(O) eye(O) to(O) visualize(O) images(O) in(O) order(O) to(O) reduce(O) cognitive(O) load(O) ,(O) enhance(O) recall(B-metric) and(O) learning(O) of(O) information(O) .(O)"}}
{"id": "225", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "person", "programming language", "conference", "metric", "researcher", "field", "country", "task", "algorithm", "organization", "location", "product"], "instance": {"id": "225", "words": ["She", "serves", "as", "a", "member", "of", "the", "National", "Academy", "of", "Sciences", "(", "since", "2005", ")", ",", "American", "Academy", "of", "Arts", "and", "Sciences", "(", "since", "2009", ")", ","], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, person, programming language, conference, metric, researcher, field, country, task, algorithm, organization, location, product and O.\nSentence: She serves as a member of the National Academy of Sciences ( since 2005 ) , American Academy of Arts and Sciences ( since 2009 ) ,", "prompt_labels": "She(O) serves(O) as(O) a(O) member(O) of(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ((O) since(O) 2005(O) )(O) ,(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) ((O) since(O) 2009(O) )(O) ,(O)"}}
{"id": "312", "dataset": "crossner_ai", "split": "test", "label_list": ["metric", "person", "algorithm", "university", "conference", "organization", "researcher", "field", "country", "programming language", "location", "task", "product"], "instance": {"id": "312", "words": ["Logo", "is", "an", "educational", "programming", "language", ",", "designed", "in", "1967", "by", "Wally", "Feurzeig", ",", "Seymour", "Papert", ",", "and", "Cynthia", "Solomon", "."], "labels": ["B-programming language", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, person, algorithm, university, conference, organization, researcher, field, country, programming language, location, task, product and O.\nSentence: Logo is an educational programming language , designed in 1967 by Wally Feurzeig , Seymour Papert , and Cynthia Solomon .", "prompt_labels": "Logo(B-programming language) is(O) an(O) educational(O) programming(O) language(O) ,(O) designed(O) in(O) 1967(O) by(O) Wally(B-researcher) Feurzeig(I-researcher) ,(O) Seymour(B-researcher) Papert(I-researcher) ,(O) and(O) Cynthia(B-researcher) Solomon(I-researcher) .(O)"}}
{"id": "128", "dataset": "crossner_ai", "split": "test", "label_list": ["university", "field", "algorithm", "product", "programming language", "location", "conference", "task", "organization", "researcher", "country", "person", "metric"], "instance": {"id": "128", "words": ["The", "NeuralExpert", "centers", "the", "design", "specifications", "around", "the", "type", "of", "problem", "the", "user", "would", "like", "the", "neural", "network", "to", "solve", "(", "Classification", ",", "Prediction", ",", "Function", "approximation", "or", "Cluster", "analysis", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "B-task", "O", "B-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, field, algorithm, product, programming language, location, conference, task, organization, researcher, country, person, metric and O.\nSentence: The NeuralExpert centers the design specifications around the type of problem the user would like the neural network to solve ( Classification , Prediction , Function approximation or Cluster analysis ) .", "prompt_labels": "The(O) NeuralExpert(O) centers(O) the(O) design(O) specifications(O) around(O) the(O) type(O) of(O) problem(O) the(O) user(O) would(O) like(O) the(O) neural(B-algorithm) network(I-algorithm) to(O) solve(O) ((O) Classification(B-task) ,(O) Prediction(B-task) ,(O) Function(B-task) approximation(I-task) or(O) Cluster(B-task) analysis(I-task) )(O) .(O)"}}
{"id": "128", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "book", "event", "location", "poem", "literary genre", "organization", "country", "writer", "award", "person"], "instance": {"id": "128", "words": ["Have", "Space", "Suit", "-", "Will", "Travel", "is", "a", "science", "fiction", "novel", "for", "young", "readers", "by", "American", "writer", "Robert", "A.", "Heinlein", ",", "originally", "serialised", "in", "The", "Magazine", "of", "Fantasy", "&", "Science", "Fiction", "(", "August", ",", "September", ",", "October", "1958", ")", "and", "published", "by", "Scribner", "'s", "in", "hardcover", "in", "1958", "."], "labels": ["B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, book, event, location, poem, literary genre, organization, country, writer, award, person and O.\nSentence: Have Space Suit - Will Travel is a science fiction novel for young readers by American writer Robert A. Heinlein , originally serialised in The Magazine of Fantasy & Science Fiction ( August , September , October 1958 ) and published by Scribner 's in hardcover in 1958 .", "prompt_labels": "Have(B-book) Space(I-book) Suit(I-book) -(I-book) Will(I-book) Travel(I-book) is(O) a(O) science(B-literary genre) fiction(I-literary genre) novel(I-literary genre) for(O) young(O) readers(O) by(O) American(O) writer(O) Robert(B-writer) A.(I-writer) Heinlein(I-writer) ,(O) originally(O) serialised(O) in(O) The(B-magazine) Magazine(I-magazine) of(I-magazine) Fantasy(I-magazine) &(I-magazine) Science(I-magazine) Fiction(I-magazine) ((O) August(O) ,(O) September(O) ,(O) October(O) 1958(O) )(O) and(O) published(O) by(O) Scribner(B-organization) 's(O) in(O) hardcover(O) in(O) 1958(O) .(O)"}}
{"id": "346", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "literary genre", "magazine", "event", "award", "person", "organization", "location", "country", "poem", "writer"], "instance": {"id": "346", "words": ["He", "was", "also", "a", "lifetime", "member", "of", "the", "Veterans", "of", "Foreign", "Wars", ",", "the", "American", "Legion", ",", "and", "Sigma", "Chi", "fraternity", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, literary genre, magazine, event, award, person, organization, location, country, poem, writer and O.\nSentence: He was also a lifetime member of the Veterans of Foreign Wars , the American Legion , and Sigma Chi fraternity .", "prompt_labels": "He(O) was(O) also(O) a(O) lifetime(O) member(O) of(O) the(O) Veterans(B-organization) of(I-organization) Foreign(I-organization) Wars(I-organization) ,(O) the(O) American(B-organization) Legion(I-organization) ,(O) and(O) Sigma(B-organization) Chi(I-organization) fraternity(I-organization) .(O)"}}
{"id": "232", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "person", "writer", "book", "literary genre", "poem", "event", "location", "award", "organization", "magazine"], "instance": {"id": "232", "words": ["During", "his", "life", ",", "Cocteau", "was", "commander", "of", "the", "Legion", "of", "Honor", ",", "Member", "of", "the", "Mallarmé", "Academy", ",", "German", "Academy", "(", "Berlin", ")", ",", "American", "Academy", ",", "Mark", "Twain", "(", "U.S.A", ")", "Academy", ",", "Honorary", "President", "of", "the", "Cannes", "Film", "Festival", ",", "Honorary", "President", "of", "the", "France-Hungary", "Association", "and", "President", "of", "the", "Jazz", "Academy", "and", "of", "the", "Academy", "of", "the", "Disc", "."], "labels": ["O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-location", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, writer, book, literary genre, poem, event, location, award, organization, magazine and O.\nSentence: During his life , Cocteau was commander of the Legion of Honor , Member of the Mallarmé Academy , German Academy ( Berlin ) , American Academy , Mark Twain ( U.S.A ) Academy , Honorary President of the Cannes Film Festival , Honorary President of the France-Hungary Association and President of the Jazz Academy and of the Academy of the Disc .", "prompt_labels": "During(O) his(O) life(O) ,(O) Cocteau(B-writer) was(O) commander(O) of(O) the(O) Legion(B-organization) of(I-organization) Honor(I-organization) ,(O) Member(O) of(O) the(O) Mallarmé(B-organization) Academy(I-organization) ,(O) German(B-organization) Academy(I-organization) ((O) Berlin(B-location) )(O) ,(O) American(B-organization) Academy(I-organization) ,(O) Mark(B-organization) Twain(I-organization) ((I-organization) U.S.A(I-organization) )(I-organization) Academy(I-organization) ,(O) Honorary(O) President(O) of(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) ,(O) Honorary(O) President(O) of(O) the(O) France-Hungary(B-organization) Association(I-organization) and(O) President(O) of(O) the(O) Jazz(B-organization) Academy(I-organization) and(O) of(O) the(O) Academy(B-organization) of(I-organization) the(I-organization) Disc(I-organization) .(O)"}}
{"id": "125", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "writer", "country", "organization", "magazine", "book", "literary genre", "location", "event", "person", "poem"], "instance": {"id": "125", "words": ["But", ",", "in", "the", "1970s", ",", "new", "arguments", "concerning", "Israel", "'s", "past", "and", "the", "biblical", "texts", "challenged", "these", "views", ";", "these", "arguments", "can", "be", "found", "in", "Thomas", "L.", "Thompson", "'", "s", "The", "Historicity", "of", "the", "Patriarchal", "Narratives", "(", "1974", ")", ",", "and", "John", "Van", "Seters", "'", "Abraham", "in", "History", "and", "Tradition", "(", "1975", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, writer, country, organization, magazine, book, literary genre, location, event, person, poem and O.\nSentence: But , in the 1970s , new arguments concerning Israel 's past and the biblical texts challenged these views ; these arguments can be found in Thomas L. Thompson ' s The Historicity of the Patriarchal Narratives ( 1974 ) , and John Van Seters ' Abraham in History and Tradition ( 1975 ) .", "prompt_labels": "But(O) ,(O) in(O) the(O) 1970s(O) ,(O) new(O) arguments(O) concerning(O) Israel(B-country) 's(O) past(O) and(O) the(O) biblical(O) texts(O) challenged(O) these(O) views(O) ;(O) these(O) arguments(O) can(O) be(O) found(O) in(O) Thomas(B-writer) L.(I-writer) Thompson(I-writer) '(O) s(O) The(B-book) Historicity(I-book) of(I-book) the(I-book) Patriarchal(I-book) Narratives(I-book) ((O) 1974(O) )(O) ,(O) and(O) John(B-writer) Van(I-writer) Seters(I-writer) '(O) Abraham(B-book) in(I-book) History(I-book) and(I-book) Tradition(I-book) ((O) 1975(O) )(O) .(O)"}}
{"id": "58", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "person", "location", "magazine", "literary genre", "book", "country", "award", "poem", "writer", "event"], "instance": {"id": "58", "words": ["He", "was", "the", "Playboy", "interview", "subject", "in", "December", "1965", ",", "in", "a", "conversation", "conducted", "by", "Alvin", "Toffler", "."], "labels": ["O", "O", "O", "B-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, location, magazine, literary genre, book, country, award, poem, writer, event and O.\nSentence: He was the Playboy interview subject in December 1965 , in a conversation conducted by Alvin Toffler .", "prompt_labels": "He(O) was(O) the(O) Playboy(B-magazine) interview(O) subject(O) in(O) December(O) 1965(O) ,(O) in(O) a(O) conversation(O) conducted(O) by(O) Alvin(B-writer) Toffler(I-writer) .(O)"}}
{"id": "361", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "award", "book", "location", "magazine", "event", "organization", "person", "literary genre", "poem", "writer"], "instance": {"id": "361", "words": ["Although", "Williams", "attracted", "the", "attention", "and", "admiration", "of", "some", "of", "the", "most", "notable", "writers", "of", "his", "day", ",", "including", "T.", "S.", "Eliot", "and", "W.", "H.", "Auden", ",", "his", "greatest", "admirer", "was", "probably", "C.", "S.", "Lewis", ",", "whose", "novel", "That", "Hideous", "Strength", "(", "1945", ")", "has", "been", "regarded", "as", "partially", "inspired", "by", "his", "acquaintance", "with", "both", "the", "man", "and", "his", "novels", "and", "poems", "."], "labels": ["O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-literary genre", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "B-literary genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, book, location, magazine, event, organization, person, literary genre, poem, writer and O.\nSentence: Although Williams attracted the attention and admiration of some of the most notable writers of his day , including T. S. Eliot and W. H. Auden , his greatest admirer was probably C. S. Lewis , whose novel That Hideous Strength ( 1945 ) has been regarded as partially inspired by his acquaintance with both the man and his novels and poems .", "prompt_labels": "Although(O) Williams(B-writer) attracted(O) the(O) attention(O) and(O) admiration(O) of(O) some(O) of(O) the(O) most(O) notable(O) writers(O) of(O) his(O) day(O) ,(O) including(O) T.(B-writer) S.(I-writer) Eliot(I-writer) and(O) W.(B-writer) H.(I-writer) Auden(I-writer) ,(O) his(O) greatest(O) admirer(O) was(O) probably(O) C.(B-writer) S.(I-writer) Lewis(I-writer) ,(O) whose(O) novel(B-literary genre) That(B-book) Hideous(I-book) Strength(I-book) ((O) 1945(O) )(O) has(O) been(O) regarded(O) as(O) partially(O) inspired(O) by(O) his(O) acquaintance(O) with(O) both(O) the(O) man(O) and(O) his(O) novels(B-literary genre) and(O) poems(B-literary genre) .(O)"}}
{"id": "191", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "event", "organization", "poem", "country", "person", "award", "literary genre", "magazine", "location", "writer"], "instance": {"id": "191", "words": ["This", "group", "(", "jokingly", "designated", "The", "Collective", ")", "included", "future", "Federal", "Reserve", "Chairman", "Alan", "Greenspan", ",", "a", "young", "psychology", "student", "named", "Nathan", "Blumenthal", "(", "later", "Nathaniel", "Branden", ")", "and", "his", "wife", "Barbara", "Branden", "and", "Barbara", "'s", "cousin", "Leonard", "Peikoff", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, event, organization, poem, country, person, award, literary genre, magazine, location, writer and O.\nSentence: This group ( jokingly designated The Collective ) included future Federal Reserve Chairman Alan Greenspan , a young psychology student named Nathan Blumenthal ( later Nathaniel Branden ) and his wife Barbara Branden and Barbara 's cousin Leonard Peikoff .", "prompt_labels": "This(O) group(O) ((O) jokingly(O) designated(O) The(B-organization) Collective(I-organization) )(O) included(O) future(O) Federal(B-organization) Reserve(I-organization) Chairman(O) Alan(B-person) Greenspan(I-person) ,(O) a(O) young(O) psychology(O) student(O) named(O) Nathan(B-person) Blumenthal(I-person) ((O) later(O) Nathaniel(B-person) Branden(I-person) )(O) and(O) his(O) wife(O) Barbara(B-writer) Branden(I-writer) and(O) Barbara(B-writer) 's(O) cousin(O) Leonard(B-person) Peikoff(I-person) .(O)"}}
{"id": "342", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "poem", "book", "country", "writer", "person", "magazine", "literary genre", "location", "award", "organization"], "instance": {"id": "342", "words": ["Angel", "is", "skeptical", "about", "the", "efficacy", "of", "astrology", ",", "and", "believes", "that", "the", "unfolding", "existential", "situation", "of", "Tim", "and", "Kirsten", "is", "akin", "to", "Friedrich", "Schiller", "'", "s", "Germany", "Romanticism", "era", "masterpiece", ",", "the", "Wallenstein", "trilogy", "(", "insofar", "as", "their", "credulity", "reflects", "the", "loss", "of", "rational", "belief", "in", "contemporary", "consensual", "reality", ")", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "B-person", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, poem, book, country, writer, person, magazine, literary genre, location, award, organization and O.\nSentence: Angel is skeptical about the efficacy of astrology , and believes that the unfolding existential situation of Tim and Kirsten is akin to Friedrich Schiller ' s Germany Romanticism era masterpiece , the Wallenstein trilogy ( insofar as their credulity reflects the loss of rational belief in contemporary consensual reality ) .", "prompt_labels": "Angel(B-writer) is(O) skeptical(O) about(O) the(O) efficacy(O) of(O) astrology(B-literary genre) ,(O) and(O) believes(O) that(O) the(O) unfolding(O) existential(O) situation(O) of(O) Tim(B-person) and(O) Kirsten(B-person) is(O) akin(O) to(O) Friedrich(B-writer) Schiller(I-writer) '(O) s(O) Germany(B-literary genre) Romanticism(I-literary genre) era(O) masterpiece(O) ,(O) the(O) Wallenstein(B-book) trilogy(I-book) ((O) insofar(O) as(O) their(O) credulity(O) reflects(O) the(O) loss(O) of(O) rational(O) belief(O) in(O) contemporary(O) consensual(O) reality(O) )(O) .(O)"}}
{"id": "136", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "award", "writer", "location", "organization", "country", "poem", "book", "literary genre", "magazine", "person"], "instance": {"id": "136", "words": ["Paradoxically", ",", "one", "of", "his", "most", "famous", "works", ",", "a", "book", "called", "Safahat", ",", "was", "not", "widely", "read", "or", "published", "until", "recently", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, writer, location, organization, country, poem, book, literary genre, magazine, person and O.\nSentence: Paradoxically , one of his most famous works , a book called Safahat , was not widely read or published until recently .", "prompt_labels": "Paradoxically(O) ,(O) one(O) of(O) his(O) most(O) famous(O) works(O) ,(O) a(O) book(O) called(O) Safahat(B-poem) ,(O) was(O) not(O) widely(O) read(O) or(O) published(O) until(O) recently(O) .(O)"}}
{"id": "266", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "location", "poem", "country", "book", "writer", "magazine", "literary genre", "person", "organization", "event"], "instance": {"id": "266", "words": ["The", "sequel", "The", "Voyages", "of", "Doctor", "Dolittle", "(", "1922", ")", "won", "Lofting", "the", "prestigious", "Newbery", "Medal", "."], "labels": ["O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-award", "O", "O", "B-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, poem, country, book, writer, magazine, literary genre, person, organization, event and O.\nSentence: The sequel The Voyages of Doctor Dolittle ( 1922 ) won Lofting the prestigious Newbery Medal .", "prompt_labels": "The(O) sequel(O) The(B-book) Voyages(I-book) of(I-book) Doctor(I-book) Dolittle(I-book) ((O) 1922(O) )(O) won(O) Lofting(B-award) the(O) prestigious(O) Newbery(B-award) Medal(I-award) .(O)"}}
{"id": "229", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "book", "organization", "writer", "location", "person", "event", "award", "country", "poem", "literary genre"], "instance": {"id": "229", "words": ["(", "See", "The", "Snow", "Man", "and", "Gubbinal", "for", "some", "references", "."], "labels": ["O", "O", "B-poem", "I-poem", "I-poem", "O", "B-poem", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, book, organization, writer, location, person, event, award, country, poem, literary genre and O.\nSentence: ( See The Snow Man and Gubbinal for some references .", "prompt_labels": "((O) See(O) The(B-poem) Snow(I-poem) Man(I-poem) and(O) Gubbinal(B-poem) for(O) some(O) references(O) .(O)"}}
{"id": "110", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "country", "magazine", "writer", "location", "organization", "person", "event", "literary genre", "book", "award"], "instance": {"id": "110", "words": ["He", "eventually", "returned", "to", "Christianity", ",", "having", "been", "influenced", "by", "arguments", "with", "his", "Oxford", "colleague", "and", "Christian", "friend", "J.", "R.", "R.", "Tolkien", ",", "whom", "he", "seems", "to", "have", "met", "for", "the", "first", "time", "on", "11", "May", "1926", ",", "and", "the", "book", "The", "Everlasting", "Man", "by", "G.", "K.", "Chesterton", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, country, magazine, writer, location, organization, person, event, literary genre, book, award and O.\nSentence: He eventually returned to Christianity , having been influenced by arguments with his Oxford colleague and Christian friend J. R. R. Tolkien , whom he seems to have met for the first time on 11 May 1926 , and the book The Everlasting Man by G. K. Chesterton .", "prompt_labels": "He(O) eventually(O) returned(O) to(O) Christianity(O) ,(O) having(O) been(O) influenced(O) by(O) arguments(O) with(O) his(O) Oxford(B-organization) colleague(O) and(O) Christian(O) friend(O) J.(B-writer) R.(I-writer) R.(I-writer) Tolkien(I-writer) ,(O) whom(O) he(O) seems(O) to(O) have(O) met(O) for(O) the(O) first(O) time(O) on(O) 11(O) May(O) 1926(O) ,(O) and(O) the(O) book(O) The(B-book) Everlasting(I-book) Man(I-book) by(O) G.(B-writer) K.(I-writer) Chesterton(I-writer) .(O)"}}
{"id": "189", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "event", "location", "book", "poem", "person", "magazine", "literary genre", "writer", "award", "country"], "instance": {"id": "189", "words": ["Chaplin", "received", "three", "Academy", "Awards", ":", "an", "Academy", "Honorary", "Award", "for", "versatility", "and", "genius", "in", "acting", ",", "writing", ",", "directing", ",", "and", "producing", "The", "Circus", "in", "1929", ","], "labels": ["B-writer", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, location, book, poem, person, magazine, literary genre, writer, award, country and O.\nSentence: Chaplin received three Academy Awards : an Academy Honorary Award for versatility and genius in acting , writing , directing , and producing The Circus in 1929 ,", "prompt_labels": "Chaplin(B-writer) received(O) three(O) Academy(B-award) Awards(I-award) :(O) an(O) Academy(B-award) Honorary(I-award) Award(I-award) for(O) versatility(O) and(O) genius(O) in(O) acting(O) ,(O) writing(O) ,(O) directing(O) ,(O) and(O) producing(O) The(O) Circus(O) in(O) 1929(O) ,(O)"}}
{"id": "296", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "organization", "country", "person", "award", "location", "book", "event", "writer", "poem", "magazine"], "instance": {"id": "296", "words": ["Frances", "Yates", "in", "her", "1966", "study", "The", "Art", "of", "Memory", "argues", "that", "a", "brief", "passage", "of", "the", "Confessions", ",", "10.8.12", ",", "in", "which", "Augustine", "writes", "of", "walking", "up", "a", "flight", "of", "stairs", "and", "entering", "the", "vast", "fields", "of", "memory", "technique", "for", "organizing", "large", "amounts", "of", "information", "."], "labels": ["B-writer", "I-writer", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, organization, country, person, award, location, book, event, writer, poem, magazine and O.\nSentence: Frances Yates in her 1966 study The Art of Memory argues that a brief passage of the Confessions , 10.8.12 , in which Augustine writes of walking up a flight of stairs and entering the vast fields of memory technique for organizing large amounts of information .", "prompt_labels": "Frances(B-writer) Yates(I-writer) in(O) her(O) 1966(O) study(O) The(B-book) Art(I-book) of(I-book) Memory(I-book) argues(O) that(O) a(O) brief(O) passage(O) of(O) the(O) Confessions(B-book) ,(O) 10.8.12(O) ,(O) in(O) which(O) Augustine(B-writer) writes(O) of(O) walking(O) up(O) a(O) flight(O) of(O) stairs(O) and(O) entering(O) the(O) vast(O) fields(O) of(O) memory(O) technique(O) for(O) organizing(O) large(O) amounts(O) of(O) information(O) .(O)"}}
{"id": "98", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "award", "poem", "person", "country", "magazine", "writer", "organization", "event", "literary genre", "location"], "instance": {"id": "98", "words": ["Humorist", "Henry", "Morgan", "had", "a", "recurring", "role", "as", "a", "humor", "writer", "for", "The", "New", "Yorker", ",", "which", "was", "said", "to", "be", "based", "on", "real-life", "humorist", "/", "actor", "Robert", "Benchley", "."], "labels": ["O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, award, poem, person, country, magazine, writer, organization, event, literary genre, location and O.\nSentence: Humorist Henry Morgan had a recurring role as a humor writer for The New Yorker , which was said to be based on real-life humorist / actor Robert Benchley .", "prompt_labels": "Humorist(O) Henry(B-person) Morgan(I-person) had(O) a(O) recurring(O) role(O) as(O) a(O) humor(O) writer(O) for(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) ,(O) which(O) was(O) said(O) to(O) be(O) based(O) on(O) real-life(O) humorist(O) /(O) actor(O) Robert(B-writer) Benchley(I-writer) .(O)"}}
{"id": "359", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "award", "country", "person", "writer", "location", "book", "magazine", "poem", "literary genre", "event"], "instance": {"id": "359", "words": ["Pauline", "Kael", ",", "who", "wrote", "a", "lengthy", "freelance", "essay", "in", "The", "New", "Yorker", "in", "praise", "of", "the", "film", ",", "was", "hired", "as", "the", "magazine", "'s", "new", "staff-critic", "."], "labels": ["B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, country, person, writer, location, book, magazine, poem, literary genre, event and O.\nSentence: Pauline Kael , who wrote a lengthy freelance essay in The New Yorker in praise of the film , was hired as the magazine 's new staff-critic .", "prompt_labels": "Pauline(B-writer) Kael(I-writer) ,(O) who(O) wrote(O) a(O) lengthy(O) freelance(O) essay(O) in(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) in(O) praise(O) of(O) the(O) film(O) ,(O) was(O) hired(O) as(O) the(O) magazine(O) 's(O) new(O) staff-critic(O) .(O)"}}
{"id": "256", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "organization", "award", "person", "location", "writer", "country", "poem", "magazine", "book", "event"], "instance": {"id": "256", "words": ["At", "the", "2012", "Pride", "of", "Britain", "Awards", "shown", "on", "ITV", "on", "30", "October", ",", "Fry", ",", "along", "with", "Michael", "Caine", ",", "Elton", "John", ",", "Richard", "Branson", "and", "Simon", "Cowell", ",", "recited", "Rudyard", "Kipling", "'", "s", "poem", "If", "-", "in", "tribute", "to", "the", "2012", "British", "Olympic", "and", "Paralympic", "athletes", "."], "labels": ["O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-writer", "I-writer", "O", "O", "B-literary genre", "B-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, organization, award, person, location, writer, country, poem, magazine, book, event and O.\nSentence: At the 2012 Pride of Britain Awards shown on ITV on 30 October , Fry , along with Michael Caine , Elton John , Richard Branson and Simon Cowell , recited Rudyard Kipling ' s poem If - in tribute to the 2012 British Olympic and Paralympic athletes .", "prompt_labels": "At(O) the(O) 2012(O) Pride(B-award) of(I-award) Britain(I-award) Awards(I-award) shown(O) on(O) ITV(O) on(O) 30(O) October(O) ,(O) Fry(B-person) ,(O) along(O) with(O) Michael(B-person) Caine(I-person) ,(O) Elton(B-person) John(I-person) ,(O) Richard(B-person) Branson(I-person) and(O) Simon(B-person) Cowell(I-person) ,(O) recited(O) Rudyard(B-writer) Kipling(I-writer) '(O) s(O) poem(B-literary genre) If(B-poem) -(I-poem) in(O) tribute(O) to(O) the(O) 2012(O) British(O) Olympic(O) and(O) Paralympic(O) athletes(O) .(O)"}}
{"id": "288", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "writer", "book", "country", "person", "award", "organization", "poem", "location", "event", "magazine"], "instance": {"id": "288", "words": ["These", "styles", "included", "the", "historical", "fiction", "of", "Memories", "of", "the", "Ford", "Administration", "(", "1992", ")", ",", "the", "magical", "realism", "of", "Brazil", "(", "1994", ")", ",", "the", "science", "fiction", "of", "Toward", "the", "End", "of", "Time", "(", "1997", ")", ",", "the", "postmodernism", "of", "Gertrude", "and", "Claudius", "(", "2000", ")", ",", "and", "the", "experimental", "fiction", "of", "Seek", "My", "Face", "(", "2002", ")", "."], "labels": ["O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-book", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-literary genre", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, writer, book, country, person, award, organization, poem, location, event, magazine and O.\nSentence: These styles included the historical fiction of Memories of the Ford Administration ( 1992 ) , the magical realism of Brazil ( 1994 ) , the science fiction of Toward the End of Time ( 1997 ) , the postmodernism of Gertrude and Claudius ( 2000 ) , and the experimental fiction of Seek My Face ( 2002 ) .", "prompt_labels": "These(O) styles(O) included(O) the(O) historical(B-literary genre) fiction(I-literary genre) of(O) Memories(B-book) of(I-book) the(I-book) Ford(I-book) Administration(I-book) ((O) 1992(O) )(O) ,(O) the(O) magical(B-literary genre) realism(I-literary genre) of(O) Brazil(B-book) ((O) 1994(O) )(O) ,(O) the(O) science(B-literary genre) fiction(I-literary genre) of(O) Toward(B-book) the(I-book) End(I-book) of(I-book) Time(I-book) ((O) 1997(O) )(O) ,(O) the(O) postmodernism(B-literary genre) of(O) Gertrude(B-book) and(I-book) Claudius(I-book) ((O) 2000(O) )(O) ,(O) and(O) the(O) experimental(B-literary genre) fiction(I-literary genre) of(O) Seek(B-book) My(I-book) Face(I-book) ((O) 2002(O) )(O) .(O)"}}
{"id": "233", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "writer", "literary genre", "event", "magazine", "organization", "location", "book", "person", "country", "award"], "instance": {"id": "233", "words": ["After", "making", "a", "series", "of", "westerns", "and", "comedies", ",", "Dwan", "directed", "fellow", "Canadian-American", "Mary", "Pickford", "in", "several", "very", "successful", "movies", "as", "well", "as", "her", "husband", ",", "Douglas", "Fairbanks", ",", "notably", "in", "the", "acclaimed", "1922", "Robin", "Hood", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, writer, literary genre, event, magazine, organization, location, book, person, country, award and O.\nSentence: After making a series of westerns and comedies , Dwan directed fellow Canadian-American Mary Pickford in several very successful movies as well as her husband , Douglas Fairbanks , notably in the acclaimed 1922 Robin Hood .", "prompt_labels": "After(O) making(O) a(O) series(O) of(O) westerns(O) and(O) comedies(O) ,(O) Dwan(B-person) directed(O) fellow(O) Canadian-American(O) Mary(B-person) Pickford(I-person) in(O) several(O) very(O) successful(O) movies(O) as(O) well(O) as(O) her(O) husband(O) ,(O) Douglas(B-person) Fairbanks(I-person) ,(O) notably(O) in(O) the(O) acclaimed(O) 1922(O) Robin(O) Hood(O) .(O)"}}
{"id": "246", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "organization", "location", "poem", "magazine", "literary genre", "event", "award", "book", "writer", "person"], "instance": {"id": "246", "words": ["In", "1987", ",", "the", "British", "Academy", "of", "Film", "and", "Television", "Arts", "awarded", "the", "BAFTA", "Award", "for", "Best", "Foreign", "Language", "Film", "to", "The", "Sacrifice", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, location, poem, magazine, literary genre, event, award, book, writer, person and O.\nSentence: In 1987 , the British Academy of Film and Television Arts awarded the BAFTA Award for Best Foreign Language Film to The Sacrifice .", "prompt_labels": "In(O) 1987(O) ,(O) the(O) British(B-organization) Academy(I-organization) of(I-organization) Film(I-organization) and(I-organization) Television(I-organization) Arts(I-organization) awarded(O) the(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Foreign(I-award) Language(I-award) Film(I-award) to(O) The(O) Sacrifice(O) .(O)"}}
{"id": "235", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "person", "award", "writer", "country", "location", "book", "magazine", "event", "literary genre", "organization"], "instance": {"id": "235", "words": ["On", "the", "DVD", "commentary", "for", "the", "1969", "movie", ",", "Guthrie", "stated", "that", "the", "events", "presented", "in", "the", "song", "all", "actually", "happened", "(", "others", ",", "such", "as", "the", "arresting", "officer", ",", "William", "Obanhein", ",", "disputed", "some", "of", "the", "song", "'s", "details", ",", "Saul", "Braun", ",", "Alice", "&", "Ray", "&", "Yesterday", "'s", "Flowers", ",", "in", "Playboy", "'", "s", "Music", "Scene", ",", "Chicago", ",", "IL", ",", "1972", ",", "pp.", "122-125", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, person, award, writer, country, location, book, magazine, event, literary genre, organization and O.\nSentence: On the DVD commentary for the 1969 movie , Guthrie stated that the events presented in the song all actually happened ( others , such as the arresting officer , William Obanhein , disputed some of the song 's details , Saul Braun , Alice & Ray & Yesterday 's Flowers , in Playboy ' s Music Scene , Chicago , IL , 1972 , pp. 122-125 .", "prompt_labels": "On(O) the(O) DVD(O) commentary(O) for(O) the(O) 1969(O) movie(O) ,(O) Guthrie(B-writer) stated(O) that(O) the(O) events(O) presented(O) in(O) the(O) song(O) all(O) actually(O) happened(O) ((O) others(O) ,(O) such(O) as(O) the(O) arresting(O) officer(O) ,(O) William(B-person) Obanhein(I-person) ,(O) disputed(O) some(O) of(O) the(O) song(O) 's(O) details(O) ,(O) Saul(B-person) Braun(I-person) ,(O) Alice(O) &(O) Ray(O) &(O) Yesterday(O) 's(O) Flowers(O) ,(O) in(O) Playboy(B-magazine) '(O) s(O) Music(O) Scene(O) ,(O) Chicago(B-location) ,(O) IL(B-location) ,(O) 1972(O) ,(O) pp.(O) 122-125(O) .(O)"}}
{"id": "412", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "country", "literary genre", "poem", "book", "writer", "location", "magazine", "organization", "person", "award"], "instance": {"id": "412", "words": ["Oliver", "Twist", "and", "Great", "Expectations", "are", "also", "frequently", "adapted", "and", ",", "like", "many", "of", "his", "novels", ",", "evoke", "images", "of", "early", "Victorian", "London", "."], "labels": ["B-book", "I-book", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, literary genre, poem, book, writer, location, magazine, organization, person, award and O.\nSentence: Oliver Twist and Great Expectations are also frequently adapted and , like many of his novels , evoke images of early Victorian London .", "prompt_labels": "Oliver(B-book) Twist(I-book) and(O) Great(B-book) Expectations(I-book) are(O) also(O) frequently(O) adapted(O) and(O) ,(O) like(O) many(O) of(O) his(O) novels(B-literary genre) ,(O) evoke(O) images(O) of(O) early(O) Victorian(B-location) London(I-location) .(O)"}}
{"id": "197", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "writer", "country", "literary genre", "award", "location", "event", "organization", "person", "book", "magazine"], "instance": {"id": "197", "words": ["Its", "sequel", ",", "Nova", "Swing", "(", "2006", ")", ",", "won", "the", "Arthur", "C.", "Clarke", "Award", "in", "2007", "Ansible", "newsletter"], "labels": ["O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "B-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, writer, country, literary genre, award, location, event, organization, person, book, magazine and O.\nSentence: Its sequel , Nova Swing ( 2006 ) , won the Arthur C. Clarke Award in 2007 Ansible newsletter", "prompt_labels": "Its(O) sequel(O) ,(O) Nova(B-book) Swing(I-book) ((O) 2006(O) )(O) ,(O) won(O) the(O) Arthur(B-award) C.(I-award) Clarke(I-award) Award(I-award) in(O) 2007(O) Ansible(B-organization) newsletter(O)"}}
{"id": "250", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "poem", "writer", "magazine", "award", "country", "organization", "location", "literary genre", "book", "event"], "instance": {"id": "250", "words": ["Writing", "for", "The", "Spectator", ",", "Graham", "Greene", "expressed", "similar", "views", ",", "acerbically", "noting", "of", "the", "film", "that", "it", "goes", "on", "too", "long", ",", "otherwise", "it", "might", "have", "been", "the", "funniest", "film", "since", "The", "Crusades", "."], "labels": ["O", "O", "B-magazine", "I-magazine", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, poem, writer, magazine, award, country, organization, location, literary genre, book, event and O.\nSentence: Writing for The Spectator , Graham Greene expressed similar views , acerbically noting of the film that it goes on too long , otherwise it might have been the funniest film since The Crusades .", "prompt_labels": "Writing(O) for(O) The(B-magazine) Spectator(I-magazine) ,(O) Graham(B-writer) Greene(I-writer) expressed(O) similar(O) views(O) ,(O) acerbically(O) noting(O) of(O) the(O) film(O) that(O) it(O) goes(O) on(O) too(O) long(O) ,(O) otherwise(O) it(O) might(O) have(O) been(O) the(O) funniest(O) film(O) since(O) The(O) Crusades(O) .(O)"}}
{"id": "295", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "award", "person", "location", "writer", "magazine", "book", "event", "literary genre", "country", "poem"], "instance": {"id": "295", "words": ["During", "China", "'", "s", "Warring", "States", "period", ",", "the", "Songs", "of", "Chu", "collected", "by", "Qu", "Yuan", "and", "Song", "Yu", "defined", "a", "new", "form", "of", "poetry", "that", "came", "from", "the", "exotic", "Yangtze", "Valley", ",", "far", "from", "the", "Wei", "and", "Yellow", "River", "homeland", "of", "the", "traditional", "four-character", "verses", "collected", "in", "the", "Classic", "of", "Poetry", "."], "labels": ["O", "B-country", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-book", "I-book", "I-book", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "B-location", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "B-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, person, location, writer, magazine, book, event, literary genre, country, poem and O.\nSentence: During China ' s Warring States period , the Songs of Chu collected by Qu Yuan and Song Yu defined a new form of poetry that came from the exotic Yangtze Valley , far from the Wei and Yellow River homeland of the traditional four-character verses collected in the Classic of Poetry .", "prompt_labels": "During(O) China(B-country) '(O) s(O) Warring(B-event) States(I-event) period(I-event) ,(O) the(O) Songs(B-book) of(I-book) Chu(I-book) collected(O) by(O) Qu(B-writer) Yuan(I-writer) and(O) Song(B-writer) Yu(I-writer) defined(O) a(O) new(O) form(O) of(O) poetry(B-literary genre) that(O) came(O) from(O) the(O) exotic(O) Yangtze(B-location) Valley(I-location) ,(O) far(O) from(O) the(O) Wei(B-location) and(O) Yellow(B-location) River(I-location) homeland(O) of(O) the(O) traditional(O) four-character(O) verses(B-literary genre) collected(O) in(O) the(O) Classic(B-book) of(I-book) Poetry(I-book) .(O)"}}
{"id": "69", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "poem", "country", "magazine", "location", "event", "book", "literary genre", "person", "award", "writer"], "instance": {"id": "69", "words": ["This", ",", "and", "other", "early", "work", "by", "Derleth", ",", "made", "him", "a", "well-known", "figure", "among", "the", "regional", "literary", "figures", "of", "his", "time", ":", "early", "Pulitzer", "Prize", "winners", "Hamlin", "Garland", "and", "Zona", "Gale", ",", "as", "well", "as", "Sinclair", "Lewis", ",", "the", "last", "both", "an", "admirer", "and", "critic", "of", "Derleth", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, poem, country, magazine, location, event, book, literary genre, person, award, writer and O.\nSentence: This , and other early work by Derleth , made him a well-known figure among the regional literary figures of his time : early Pulitzer Prize winners Hamlin Garland and Zona Gale , as well as Sinclair Lewis , the last both an admirer and critic of Derleth .", "prompt_labels": "This(O) ,(O) and(O) other(O) early(O) work(O) by(O) Derleth(B-writer) ,(O) made(O) him(O) a(O) well-known(O) figure(O) among(O) the(O) regional(O) literary(O) figures(O) of(O) his(O) time(O) :(O) early(O) Pulitzer(B-award) Prize(I-award) winners(O) Hamlin(B-writer) Garland(I-writer) and(O) Zona(B-writer) Gale(I-writer) ,(O) as(O) well(O) as(O) Sinclair(B-writer) Lewis(I-writer) ,(O) the(O) last(O) both(O) an(O) admirer(O) and(O) critic(O) of(O) Derleth(B-writer) .(O)"}}
{"id": "174", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "magazine", "person", "location", "poem", "event", "award", "book", "writer", "organization", "country"], "instance": {"id": "174", "words": ["In", "recognition", "of", "his", "work", "with", "electricity", ",", "Franklin", "received", "the", "Royal", "Society", "'", "s", "Copley", "Medal", "in", "1753", ",", "and", "in", "1756", ",", "he", "became", "one", "of", "the", "few", "18th-century", "Americans", "elected", "as", "a", "Fellow", "of", "the", "Society", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "B-organization", "I-organization", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, magazine, person, location, poem, event, award, book, writer, organization, country and O.\nSentence: In recognition of his work with electricity , Franklin received the Royal Society ' s Copley Medal in 1753 , and in 1756 , he became one of the few 18th-century Americans elected as a Fellow of the Society .", "prompt_labels": "In(O) recognition(O) of(O) his(O) work(O) with(O) electricity(O) ,(O) Franklin(B-person) received(O) the(O) Royal(B-organization) Society(I-organization) '(O) s(O) Copley(B-award) Medal(I-award) in(O) 1753(O) ,(O) and(O) in(O) 1756(O) ,(O) he(O) became(O) one(O) of(O) the(O) few(O) 18th-century(O) Americans(O) elected(O) as(O) a(O) Fellow(B-award) of(I-award) the(I-award) Society(I-award) .(O)"}}
{"id": "149", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "event", "organization", "writer", "literary genre", "book", "award", "country", "location", "magazine", "person"], "instance": {"id": "149", "words": ["In", "April", "he", "chaired", "the", "South", "Side", "Writers", "Group", ",", "whose", "members", "included", "Arna", "Bontemps", "and", "Margaret", "Walker", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, event, organization, writer, literary genre, book, award, country, location, magazine, person and O.\nSentence: In April he chaired the South Side Writers Group , whose members included Arna Bontemps and Margaret Walker .", "prompt_labels": "In(O) April(O) he(O) chaired(O) the(O) South(B-organization) Side(I-organization) Writers(I-organization) Group(I-organization) ,(O) whose(O) members(O) included(O) Arna(B-writer) Bontemps(I-writer) and(O) Margaret(B-writer) Walker(I-writer) .(O)"}}
{"id": "331", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "event", "location", "writer", "award", "country", "person", "poem", "magazine", "literary genre", "book"], "instance": {"id": "331", "words": ["Its", "glory", "rests", "chiefly", "on", "three", "works", "...", "'", "A", "song", "of", "sorrow", "inside", "the", "royal", "harem", "'", "...", "by", "Nguyễn", "Gia", "Thiều", ",", "'", "Calling", "all", "souls", "'", "...", "by", "Nguyễn", "Du", ",", "and", "'", "Chinh", "phụ", "ngâm", "'", "...", "by", "Phan", "Huy", "Ích", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-poem", "I-poem", "I-poem", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "B-poem", "I-poem", "I-poem", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, location, writer, award, country, person, poem, magazine, literary genre, book and O.\nSentence: Its glory rests chiefly on three works ... ' A song of sorrow inside the royal harem ' ... by Nguyễn Gia Thiều , ' Calling all souls ' ... by Nguyễn Du , and ' Chinh phụ ngâm ' ... by Phan Huy Ích .", "prompt_labels": "Its(O) glory(O) rests(O) chiefly(O) on(O) three(O) works(O) ...(O) '(O) A(B-poem) song(I-poem) of(I-poem) sorrow(I-poem) inside(I-poem) the(I-poem) royal(I-poem) harem(I-poem) '(O) ...(O) by(O) Nguyễn(B-writer) Gia(I-writer) Thiều(I-writer) ,(O) '(O) Calling(B-poem) all(I-poem) souls(I-poem) '(O) ...(O) by(O) Nguyễn(B-writer) Du(I-writer) ,(O) and(O) '(O) Chinh(B-poem) phụ(I-poem) ngâm(I-poem) '(O) ...(O) by(O) Phan(B-writer) Huy(I-writer) Ích(I-writer) .(O)"}}
{"id": "53", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "poem", "award", "writer", "person", "organization", "location", "country", "literary genre", "event", "book"], "instance": {"id": "53", "words": ["He", "founded", "Toffler", "Associates", ",", "a", "management", "consulting", "company", ",", "and", "was", "a", "visiting", "scholar", "at", "the", "Russell", "Sage", "Foundation", ",", "visiting", "professor", "at", "Cornell", "University", ",", "faculty", "member", "of", "the", "New", "School", "for", "Social", "Research", ",", "a", "White", "House", "correspondent", ",", "and", "a", "business", "consultant", ".."], "labels": ["O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, poem, award, writer, person, organization, location, country, literary genre, event, book and O.\nSentence: He founded Toffler Associates , a management consulting company , and was a visiting scholar at the Russell Sage Foundation , visiting professor at Cornell University , faculty member of the New School for Social Research , a White House correspondent , and a business consultant ..", "prompt_labels": "He(O) founded(O) Toffler(B-organization) Associates(I-organization) ,(O) a(O) management(O) consulting(O) company(O) ,(O) and(O) was(O) a(O) visiting(O) scholar(O) at(O) the(O) Russell(B-organization) Sage(I-organization) Foundation(I-organization) ,(O) visiting(O) professor(O) at(O) Cornell(B-organization) University(I-organization) ,(O) faculty(O) member(O) of(O) the(O) New(B-organization) School(I-organization) for(I-organization) Social(I-organization) Research(I-organization) ,(O) a(O) White(B-organization) House(I-organization) correspondent(O) ,(O) and(O) a(O) business(O) consultant(O) ..(O)"}}
{"id": "383", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "literary genre", "location", "book", "event", "award", "poem", "organization", "country", "person", "magazine"], "instance": {"id": "383", "words": ["Vernor", "Steffen", "Vinge", "(", "He", "has", "won", "the", "Hugo", "Award", "for", "his", "novels", "and", "novellas", "A", "Fire", "Upon", "the", "Deep", "(", "1992", ")", ",", "A", "Deepness", "in", "the", "Sky", "(", "1999", ")", ",", "Rainbows", "End", "(", "2006", ")", ",", "Fast", "Times", "at", "Fairmont", "High", "(", "2002", ")", ",", "and", "The", "Cookie", "Monster", "(", "2004", ")", "."], "labels": ["B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-literary genre", "O", "B-literary genre", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, literary genre, location, book, event, award, poem, organization, country, person, magazine and O.\nSentence: Vernor Steffen Vinge ( He has won the Hugo Award for his novels and novellas A Fire Upon the Deep ( 1992 ) , A Deepness in the Sky ( 1999 ) , Rainbows End ( 2006 ) , Fast Times at Fairmont High ( 2002 ) , and The Cookie Monster ( 2004 ) .", "prompt_labels": "Vernor(B-writer) Steffen(I-writer) Vinge(I-writer) ((O) He(O) has(O) won(O) the(O) Hugo(B-award) Award(I-award) for(O) his(O) novels(B-literary genre) and(O) novellas(B-literary genre) A(B-book) Fire(I-book) Upon(I-book) the(I-book) Deep(I-book) ((O) 1992(O) )(O) ,(O) A(B-book) Deepness(I-book) in(I-book) the(I-book) Sky(I-book) ((O) 1999(O) )(O) ,(O) Rainbows(B-book) End(I-book) ((O) 2006(O) )(O) ,(O) Fast(B-book) Times(I-book) at(I-book) Fairmont(I-book) High(I-book) ((O) 2002(O) )(O) ,(O) and(O) The(B-book) Cookie(I-book) Monster(I-book) ((O) 2004(O) )(O) .(O)"}}
{"id": "305", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "country", "poem", "literary genre", "book", "writer", "person", "magazine", "event", "location", "organization"], "instance": {"id": "305", "words": ["According", "to", "Plutarch", "'", "s", "Life", "of", "Theseus", ",", "the", "ship", "Theseus", "used", "on", "his", "return", "from", "Crete", "to", "Classical", "Athens", "was", "kept", "in", "the", "Athenian", "harbour", "as", "a", "memorial", "for", "several", "centuries", "."], "labels": ["O", "O", "B-writer", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "B-location", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, poem, literary genre, book, writer, person, magazine, event, location, organization and O.\nSentence: According to Plutarch ' s Life of Theseus , the ship Theseus used on his return from Crete to Classical Athens was kept in the Athenian harbour as a memorial for several centuries .", "prompt_labels": "According(O) to(O) Plutarch(B-writer) '(O) s(O) Life(B-book) of(I-book) Theseus(I-book) ,(O) the(O) ship(O) Theseus(B-person) used(O) on(O) his(O) return(O) from(O) Crete(B-location) to(O) Classical(B-country) Athens(I-country) was(O) kept(O) in(O) the(O) Athenian(O) harbour(O) as(O) a(O) memorial(O) for(O) several(O) centuries(O) .(O)"}}
{"id": "292", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "writer", "literary genre", "person", "organization", "award", "location", "event", "poem", "book", "country"], "instance": {"id": "292", "words": ["Among", "lyric", "poets", ",", "the", "most", "important", "figures", "are", "Anne", "Bradstreet", ",", "who", "wrote", "personal", "poems", "about", "her", "family", "and", "homelife", ";", "pastor", "Edward", "Taylor", ",", "whose", "best", "poems", ",", "the", "Preparatory", "Meditations", ",", "were", "written", "to", "help", "him", "prepare", "for", "leading", "worship", ";", "and", "Michael", "Wigglesworth", ",", "whose", "best-selling", "poem", ",", "The", "Day", "of", "Doom", "(", "1660", ")", ",", "describes", "the", "time", "of", "judgment", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "B-literary genre", "O", "B-poem", "I-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, writer, literary genre, person, organization, award, location, event, poem, book, country and O.\nSentence: Among lyric poets , the most important figures are Anne Bradstreet , who wrote personal poems about her family and homelife ; pastor Edward Taylor , whose best poems , the Preparatory Meditations , were written to help him prepare for leading worship ; and Michael Wigglesworth , whose best-selling poem , The Day of Doom ( 1660 ) , describes the time of judgment .", "prompt_labels": "Among(O) lyric(O) poets(O) ,(O) the(O) most(O) important(O) figures(O) are(O) Anne(B-writer) Bradstreet(I-writer) ,(O) who(O) wrote(O) personal(O) poems(O) about(O) her(O) family(O) and(O) homelife(O) ;(O) pastor(O) Edward(B-writer) Taylor(I-writer) ,(O) whose(O) best(O) poems(O) ,(O) the(O) Preparatory(B-poem) Meditations(I-poem) ,(O) were(O) written(O) to(O) help(O) him(O) prepare(O) for(O) leading(O) worship(O) ;(O) and(O) Michael(B-writer) Wigglesworth(I-writer) ,(O) whose(O) best-selling(O) poem(B-literary genre) ,(O) The(B-poem) Day(I-poem) of(I-poem) Doom(I-poem) ((O) 1660(O) )(O) ,(O) describes(O) the(O) time(O) of(O) judgment(O) .(O)"}}
{"id": "117", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "poem", "award", "writer", "country", "literary genre", "magazine", "book", "location", "event", "organization"], "instance": {"id": "117", "words": ["His", "most", "famous", "works", "were", "his", "longer", "and", "more", "moralistic", "Troy", "Book", "(", "1412-20", ")", ",", "a", "30,000", "line", "translation", "of", "the", "Latin", "prose", "narrative", "by", "Guido", "delle", "Colonne", ",", "Historia", "destructionis", "Troiae", ",", "the", "Siege", "of", "Thebes", "which", "was", "translated", "from", "a", "French", "prose", "redaction", "of", "the", "Roman", "de", "Thebes", "and", "the", "Fall", "of", "Princes", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-poem", "I-poem", "I-poem", "O", "O", "B-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "B-poem", "I-poem", "I-poem", "O", "O", "B-poem", "I-poem", "I-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, poem, award, writer, country, literary genre, magazine, book, location, event, organization and O.\nSentence: His most famous works were his longer and more moralistic Troy Book ( 1412-20 ) , a 30,000 line translation of the Latin prose narrative by Guido delle Colonne , Historia destructionis Troiae , the Siege of Thebes which was translated from a French prose redaction of the Roman de Thebes and the Fall of Princes .", "prompt_labels": "His(O) most(O) famous(O) works(O) were(O) his(O) longer(O) and(O) more(O) moralistic(O) Troy(B-poem) Book(I-poem) ((O) 1412-20(O) )(O) ,(O) a(O) 30,000(O) line(O) translation(O) of(O) the(O) Latin(B-literary genre) prose(I-literary genre) narrative(O) by(O) Guido(B-writer) delle(I-writer) Colonne(I-writer) ,(O) Historia(B-poem) destructionis(I-poem) Troiae(I-poem) ,(O) the(O) Siege(B-poem) of(I-poem) Thebes(I-poem) which(O) was(O) translated(O) from(O) a(O) French(B-literary genre) prose(I-literary genre) redaction(O) of(O) the(O) Roman(B-poem) de(I-poem) Thebes(I-poem) and(O) the(O) Fall(B-poem) of(I-poem) Princes(I-poem) .(O)"}}
{"id": "106", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "literary genre", "writer", "organization", "poem", "event", "book", "award", "location", "person", "country"], "instance": {"id": "106", "words": ["Initially", ",", "after", "the", "Roman", "appeared", ",", "other", "authors", "who", "refer", "to", "the", "story", ",", "for", "example", ",", "Azalais", "d", "'Altier", "in", "her", "poem", "Tanz", "salutz", "e", "tantas", "amors", "and", "Guido", "delle", "Colonne", "in", "his", "Historia", "destructionis", "Troiae", ",", "continue", "to", "use", "names", "derived", "from", "that", "of", "Briseis", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "B-poem", "I-poem", "I-poem", "I-poem", "I-poem", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, literary genre, writer, organization, poem, event, book, award, location, person, country and O.\nSentence: Initially , after the Roman appeared , other authors who refer to the story , for example , Azalais d 'Altier in her poem Tanz salutz e tantas amors and Guido delle Colonne in his Historia destructionis Troiae , continue to use names derived from that of Briseis .", "prompt_labels": "Initially(O) ,(O) after(O) the(O) Roman(O) appeared(O) ,(O) other(O) authors(O) who(O) refer(O) to(O) the(O) story(O) ,(O) for(O) example(O) ,(O) Azalais(O) d(O) 'Altier(O) in(O) her(O) poem(B-literary genre) Tanz(B-poem) salutz(I-poem) e(I-poem) tantas(I-poem) amors(I-poem) and(O) Guido(B-writer) delle(I-writer) Colonne(I-writer) in(O) his(O) Historia(B-poem) destructionis(I-poem) Troiae(I-poem) ,(O) continue(O) to(O) use(O) names(O) derived(O) from(O) that(O) of(O) Briseis(B-person) .(O)"}}
{"id": "287", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "writer", "country", "literary genre", "book", "award", "poem", "person", "location", "event", "organization"], "instance": {"id": "287", "words": ["It", "is", "based", "on", "the", "novel", "Schindler", "'s", "Ark", "by", "Australian", "novelist", "Thomas", "Keneally", "."], "labels": ["O", "O", "O", "O", "O", "B-literary genre", "B-book", "I-book", "I-book", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, writer, country, literary genre, book, award, poem, person, location, event, organization and O.\nSentence: It is based on the novel Schindler 's Ark by Australian novelist Thomas Keneally .", "prompt_labels": "It(O) is(O) based(O) on(O) the(O) novel(B-literary genre) Schindler(B-book) 's(I-book) Ark(I-book) by(O) Australian(O) novelist(O) Thomas(B-writer) Keneally(I-writer) .(O)"}}
{"id": "231", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "organization", "magazine", "book", "location", "award", "literary genre", "person", "poem", "writer", "event"], "instance": {"id": "231", "words": ["Andy", "Warhol", "was", "commissioned", "in", "1984", "by", "collector", "and", "gallerist", "Alexander", "Iolas", "to", "produce", "work", "based", "on", "Leonardo", "da", "Vinci", "'", "s", "The", "Last", "Supper", "for", "an", "exhibition", "at", "the", "old", "refectory", "of", "the", "Palazzo", "delle", "Stelline", "in", "Milan", ",", "opposite", "from", "the", "Santa", "Maria", "delle", "Grazie", "where", "Leonardo", "da", "Vinci", "'s", "mural", "can", "be", "seen", "."], "labels": ["B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, magazine, book, location, award, literary genre, person, poem, writer, event and O.\nSentence: Andy Warhol was commissioned in 1984 by collector and gallerist Alexander Iolas to produce work based on Leonardo da Vinci ' s The Last Supper for an exhibition at the old refectory of the Palazzo delle Stelline in Milan , opposite from the Santa Maria delle Grazie where Leonardo da Vinci 's mural can be seen .", "prompt_labels": "Andy(B-writer) Warhol(I-writer) was(O) commissioned(O) in(O) 1984(O) by(O) collector(O) and(O) gallerist(O) Alexander(B-person) Iolas(I-person) to(O) produce(O) work(O) based(O) on(O) Leonardo(B-person) da(I-person) Vinci(I-person) '(O) s(O) The(O) Last(O) Supper(O) for(O) an(O) exhibition(O) at(O) the(O) old(O) refectory(O) of(O) the(O) Palazzo(B-location) delle(I-location) Stelline(I-location) in(O) Milan(B-location) ,(O) opposite(O) from(O) the(O) Santa(B-location) Maria(I-location) delle(I-location) Grazie(I-location) where(O) Leonardo(B-person) da(I-person) Vinci(I-person) 's(O) mural(O) can(O) be(O) seen(O) .(O)"}}
{"id": "200", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "organization", "country", "magazine", "person", "poem", "literary genre", "location", "book", "writer", "award"], "instance": {"id": "200", "words": ["In", "1930", ",", "an", "American", "film", "of", "the", "novel", "was", "made", ",", "directed", "by", "Lewis", "Milestone", ";", "with", "a", "screenplay", "by", "Maxwell", "Anderson", ",", "George", "Abbott", ",", "Del", "Andrews", ",", "C.", "Gardner", "Sullivan", ";", "and", "with", "uncredited", "work", "by", "Walter", "Anthony", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, country, magazine, person, poem, literary genre, location, book, writer, award and O.\nSentence: In 1930 , an American film of the novel was made , directed by Lewis Milestone ; with a screenplay by Maxwell Anderson , George Abbott , Del Andrews , C. Gardner Sullivan ; and with uncredited work by Walter Anthony .", "prompt_labels": "In(O) 1930(O) ,(O) an(O) American(O) film(O) of(O) the(O) novel(B-literary genre) was(O) made(O) ,(O) directed(O) by(O) Lewis(B-person) Milestone(I-person) ;(O) with(O) a(O) screenplay(O) by(O) Maxwell(B-writer) Anderson(I-writer) ,(O) George(B-writer) Abbott(I-writer) ,(O) Del(B-writer) Andrews(I-writer) ,(O) C.(B-writer) Gardner(I-writer) Sullivan(I-writer) ;(O) and(O) with(O) uncredited(O) work(O) by(O) Walter(B-writer) Anthony(I-writer) .(O)"}}
{"id": "397", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "country", "book", "location", "event", "award", "person", "magazine", "writer", "literary genre", "poem"], "instance": {"id": "397", "words": ["She", "also", "read", "the", "work", "of", "Edith", "Nesbit", ",", "including", "The", "Story", "of", "the", "Treasure", "Seekers", "(", "1899", ")", ",", "The", "Phoenix", "and", "the", "Carpet", "(", "1903", ")", ",", "and", "The", "Railway", "Children", "(", "1906", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, book, location, event, award, person, magazine, writer, literary genre, poem and O.\nSentence: She also read the work of Edith Nesbit , including The Story of the Treasure Seekers ( 1899 ) , The Phoenix and the Carpet ( 1903 ) , and The Railway Children ( 1906 ) .", "prompt_labels": "She(O) also(O) read(O) the(O) work(O) of(O) Edith(B-writer) Nesbit(I-writer) ,(O) including(O) The(B-book) Story(I-book) of(I-book) the(I-book) Treasure(I-book) Seekers(I-book) ((O) 1899(O) )(O) ,(O) The(B-book) Phoenix(I-book) and(I-book) the(I-book) Carpet(I-book) ((O) 1903(O) )(O) ,(O) and(O) The(B-book) Railway(I-book) Children(I-book) ((O) 1906(O) )(O) .(O)"}}
{"id": "390", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "poem", "literary genre", "location", "person", "event", "country", "organization", "book", "magazine", "writer"], "instance": {"id": "390", "words": ["Aung", "San", "Suu", "Kyi", "(", ";", "born", "19", "June", "1945", ")", "is", "a", "Myanmar", "politician", ",", "diplomat", ",", "author", ",", "and", "a", "1991", "Nobel", "Peace", "Prize", "laureate", "."], "labels": ["B-writer", "I-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, poem, literary genre, location, person, event, country, organization, book, magazine, writer and O.\nSentence: Aung San Suu Kyi ( ; born 19 June 1945 ) is a Myanmar politician , diplomat , author , and a 1991 Nobel Peace Prize laureate .", "prompt_labels": "Aung(B-writer) San(I-writer) Suu(I-writer) Kyi(I-writer) ((O) ;(O) born(O) 19(O) June(O) 1945(O) )(O) is(O) a(O) Myanmar(B-country) politician(O) ,(O) diplomat(O) ,(O) author(O) ,(O) and(O) a(O) 1991(O) Nobel(B-award) Peace(I-award) Prize(I-award) laureate(O) .(O)"}}
{"id": "137", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "award", "person", "location", "country", "magazine", "literary genre", "event", "book", "poem", "organization"], "instance": {"id": "137", "words": ["It", "is", "partially", "a", "science-fictional", "pastiche", "of", "Moby-Dick", "by", "Herman", "Melville", "."], "labels": ["O", "O", "O", "O", "B-literary genre", "O", "O", "B-book", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, award, person, location, country, magazine, literary genre, event, book, poem, organization and O.\nSentence: It is partially a science-fictional pastiche of Moby-Dick by Herman Melville .", "prompt_labels": "It(O) is(O) partially(O) a(O) science-fictional(B-literary genre) pastiche(O) of(O) Moby-Dick(B-book) by(O) Herman(B-writer) Melville(I-writer) .(O)"}}
{"id": "211", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "event", "country", "literary genre", "book", "person", "award", "magazine", "location", "poem", "writer"], "instance": {"id": "211", "words": ["Recommended", "Reading", ",", "The", "Magazine", "of", "Fantasy", "&", "Science", "Fiction", ",", "February", "1952", ",", "p.105", "P.", "Schuyler", "Miller", ",", "noting", "that", "the", "novel", "'s", "climactic", "situations", "seem", "to", "be", "telegraphed", ",", "suggested", "that", "Heinlein", "presented", "his", "background", "situations", "so", "effectively", "that", "readers", "solve", "the", "story", "'s", "mysteries", "more", "quickly", "than", "Heinlein", "allowed", "his", "characters", "to", "."], "labels": ["O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, country, literary genre, book, person, award, magazine, location, poem, writer and O.\nSentence: Recommended Reading , The Magazine of Fantasy & Science Fiction , February 1952 , p.105 P. Schuyler Miller , noting that the novel 's climactic situations seem to be telegraphed , suggested that Heinlein presented his background situations so effectively that readers solve the story 's mysteries more quickly than Heinlein allowed his characters to .", "prompt_labels": "Recommended(O) Reading(O) ,(O) The(B-magazine) Magazine(I-magazine) of(I-magazine) Fantasy(I-magazine) &(I-magazine) Science(I-magazine) Fiction(I-magazine) ,(O) February(O) 1952(O) ,(O) p.105(O) P.(B-writer) Schuyler(I-writer) Miller(I-writer) ,(O) noting(O) that(O) the(O) novel(B-literary genre) 's(O) climactic(O) situations(O) seem(O) to(O) be(O) telegraphed(O) ,(O) suggested(O) that(O) Heinlein(B-writer) presented(O) his(O) background(O) situations(O) so(O) effectively(O) that(O) readers(O) solve(O) the(O) story(O) 's(O) mysteries(O) more(O) quickly(O) than(O) Heinlein(B-writer) allowed(O) his(O) characters(O) to(O) .(O)"}}
{"id": "29", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "location", "event", "person", "poem", "organization", "magazine", "writer", "country", "book", "award"], "instance": {"id": "29", "words": ["Later", ",", "he", "was", "best", "known", "for", "his", "setting", "the", "Draumkvedet", "(", "1905", ")", "and", "the", "Poetic", "Edda", "(", "1908", ")", "into", "modern", "Norwegian", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, location, event, person, poem, organization, magazine, writer, country, book, award and O.\nSentence: Later , he was best known for his setting the Draumkvedet ( 1905 ) and the Poetic Edda ( 1908 ) into modern Norwegian .", "prompt_labels": "Later(O) ,(O) he(O) was(O) best(O) known(O) for(O) his(O) setting(O) the(O) Draumkvedet(B-poem) ((O) 1905(O) )(O) and(O) the(O) Poetic(B-book) Edda(I-book) ((O) 1908(O) )(O) into(O) modern(O) Norwegian(O) .(O)"}}
{"id": "265", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "organization", "literary genre", "book", "award", "poem", "person", "writer", "magazine", "event", "location"], "instance": {"id": "265", "words": ["Barry", "Lyndon", "won", "four", "Oscars", "at", "the", "48th", "Academy", "Awards", ":", "Academy", "Award", "for", "Best", "Original", "Score", ",", "Academy", "Award", "for", "Best", "Costume", "Design", ",", "Academy", "Award", "for", "Best", "Production", "Design", "and", "Academy", "Award", "for", "Best", "Cinematography", "."], "labels": ["O", "O", "O", "O", "B-award", "O", "O", "B-event", "I-event", "I-event", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, literary genre, book, award, poem, person, writer, magazine, event, location and O.\nSentence: Barry Lyndon won four Oscars at the 48th Academy Awards : Academy Award for Best Original Score , Academy Award for Best Costume Design , Academy Award for Best Production Design and Academy Award for Best Cinematography .", "prompt_labels": "Barry(O) Lyndon(O) won(O) four(O) Oscars(B-award) at(O) the(O) 48th(B-event) Academy(I-event) Awards(I-event) :(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Costume(I-award) Design(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Production(I-award) Design(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Cinematography(I-award) .(O)"}}
{"id": "212", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "writer", "book", "award", "location", "event", "country", "poem", "magazine", "person", "organization"], "instance": {"id": "212", "words": ["Live-action", "film", "adaptations", "have", "been", "made", "of", "three", "of", "The", "Chronicles", "of", "Narnia", ":", "The", "Lion", ",", "the", "Witch", ",", "and", "the", "Wardrobe", "(", "2005", ")", ",", "Prince", "Caspian", "(", "2008", ")", "and", "The", "Voyage", "of", "the", "Dawn", "Treader", "(", "2010", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "O", "B-book", "I-book", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, writer, book, award, location, event, country, poem, magazine, person, organization and O.\nSentence: Live-action film adaptations have been made of three of The Chronicles of Narnia : The Lion , the Witch , and the Wardrobe ( 2005 ) , Prince Caspian ( 2008 ) and The Voyage of the Dawn Treader ( 2010 ) .", "prompt_labels": "Live-action(O) film(O) adaptations(O) have(O) been(O) made(O) of(O) three(O) of(O) The(B-book) Chronicles(I-book) of(I-book) Narnia(I-book) :(O) The(B-book) Lion(I-book) ,(O) the(B-book) Witch(I-book) ,(O) and(O) the(B-book) Wardrobe(I-book) ((O) 2005(O) )(O) ,(O) Prince(B-book) Caspian(I-book) ((O) 2008(O) )(O) and(O) The(B-book) Voyage(I-book) of(I-book) the(I-book) Dawn(I-book) Treader(I-book) ((O) 2010(O) )(O) .(O)"}}
{"id": "17", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "event", "location", "organization", "poem", "award", "literary genre", "magazine", "book", "person", "writer"], "instance": {"id": "17", "words": ["Gide", "went", "successively", "to", "Middle", "Congo", "(", "now", "the", "Republic", "of", "the", "Congo", ")", ",", "Ubangi-Shari", "(", "now", "the", "Central", "African", "Republic", ")", ",", "briefly", "to", "Chad", "and", "then", "to", "Cameroon", "before", "returning", "to", "France", "."], "labels": ["B-person", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "B-country", "I-country", "I-country", "I-country", "O", "O", "B-country", "O", "O", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-country", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, location, organization, poem, award, literary genre, magazine, book, person, writer and O.\nSentence: Gide went successively to Middle Congo ( now the Republic of the Congo ) , Ubangi-Shari ( now the Central African Republic ) , briefly to Chad and then to Cameroon before returning to France .", "prompt_labels": "Gide(B-person) went(O) successively(O) to(O) Middle(B-country) Congo(I-country) ((O) now(O) the(O) Republic(B-country) of(I-country) the(I-country) Congo(I-country) )(O) ,(O) Ubangi-Shari(B-country) ((O) now(O) the(O) Central(B-country) African(I-country) Republic(I-country) )(O) ,(O) briefly(O) to(O) Chad(B-country) and(O) then(O) to(O) Cameroon(B-country) before(O) returning(O) to(O) France(B-country) .(O)"}}
{"id": "178", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "writer", "location", "award", "poem", "magazine", "organization", "country", "person", "event", "book"], "instance": {"id": "178", "words": ["Three", "of", "his", "films", "-", "Andrei", "Rublev", ",", "Mirror", ",", "and", "Stalker", "-", "featured", "in", "Sight", "&", "Sound", "s", "2012", "poll", "of", "the", "50", "greatest", "films", "of", "all", "time", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, writer, location, award, poem, magazine, organization, country, person, event, book and O.\nSentence: Three of his films - Andrei Rublev , Mirror , and Stalker - featured in Sight & Sound s 2012 poll of the 50 greatest films of all time .", "prompt_labels": "Three(O) of(O) his(O) films(O) -(O) Andrei(O) Rublev(O) ,(O) Mirror(O) ,(O) and(O) Stalker(O) -(O) featured(O) in(O) Sight(B-magazine) &(I-magazine) Sound(I-magazine) s(O) 2012(B-award) poll(I-award) of(I-award) the(I-award) 50(I-award) greatest(I-award) films(I-award) of(I-award) all(I-award) time(I-award) .(O)"}}
{"id": "159", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "person", "location", "event", "organization", "literary genre", "award", "book", "country", "writer", "magazine"], "instance": {"id": "159", "words": ["The", "film", "was", "overwhelmingly", "lauded", "by", "critics", "when", "it", "finally", "appeared", "in", "1979", "and", "was", "selected", "for", "the", "1979", "Cannes", "Film", "Festival", ",", "winning", "the", "Palme", "d", "'Or", "along", "with", "The", "Tin", "Drum", ",", "directed", "by", "Volker", "Schlöndorff", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, person, location, event, organization, literary genre, award, book, country, writer, magazine and O.\nSentence: The film was overwhelmingly lauded by critics when it finally appeared in 1979 and was selected for the 1979 Cannes Film Festival , winning the Palme d 'Or along with The Tin Drum , directed by Volker Schlöndorff .", "prompt_labels": "The(O) film(O) was(O) overwhelmingly(O) lauded(O) by(O) critics(O) when(O) it(O) finally(O) appeared(O) in(O) 1979(O) and(O) was(O) selected(O) for(O) the(O) 1979(O) Cannes(B-event) Film(I-event) Festival(I-event) ,(O) winning(O) the(O) Palme(B-award) d(I-award) 'Or(I-award) along(O) with(O) The(O) Tin(O) Drum(O) ,(O) directed(O) by(O) Volker(B-person) Schlöndorff(I-person) .(O)"}}
{"id": "181", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "book", "person", "award", "country", "writer", "literary genre", "event", "magazine", "location", "poem"], "instance": {"id": "181", "words": ["The", "character", "is", "brought", "back", "as", "a", "ghola", "in", "the", "Herbert", "/", "Anderson", "sequels", "which", "conclude", "the", "original", "series", ",", "Hunters", "of", "Dune", "(", "2006", ")", "and", "Sandworms", "of", "Dune", "(", "2007", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, book, person, award, country, writer, literary genre, event, magazine, location, poem and O.\nSentence: The character is brought back as a ghola in the Herbert / Anderson sequels which conclude the original series , Hunters of Dune ( 2006 ) and Sandworms of Dune ( 2007 ) .", "prompt_labels": "The(O) character(O) is(O) brought(O) back(O) as(O) a(O) ghola(O) in(O) the(O) Herbert(B-person) /(I-person) Anderson(I-person) sequels(O) which(O) conclude(O) the(O) original(O) series(O) ,(O) Hunters(B-book) of(I-book) Dune(I-book) ((O) 2006(O) )(O) and(O) Sandworms(B-book) of(I-book) Dune(I-book) ((O) 2007(O) )(O) .(O)"}}
{"id": "247", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "organization", "book", "person", "writer", "location", "magazine", "event", "award", "country", "poem"], "instance": {"id": "247", "words": ["Another", "well-known", "version", "of", "the", "play", "is", "Jedermann", "by", "the", "Austrian", "playwright", "Hugo", "von", "Hofmannsthal", ",", "which", "has", "been", "performed", "annually", "at", "the", "Salzburg", "Festival", "since", "1920", ",", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, organization, book, person, writer, location, magazine, event, award, country, poem and O.\nSentence: Another well-known version of the play is Jedermann by the Austrian playwright Hugo von Hofmannsthal , which has been performed annually at the Salzburg Festival since 1920 , .", "prompt_labels": "Another(O) well-known(O) version(O) of(O) the(O) play(O) is(O) Jedermann(B-book) by(O) the(O) Austrian(O) playwright(O) Hugo(B-writer) von(I-writer) Hofmannsthal(I-writer) ,(O) which(O) has(O) been(O) performed(O) annually(O) at(O) the(O) Salzburg(B-event) Festival(I-event) since(O) 1920(O) ,(O) .(O)"}}
{"id": "177", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "magazine", "award", "literary genre", "poem", "location", "organization", "person", "event", "book", "country"], "instance": {"id": "177", "words": ["They", "published", "numerous", "criticisms", "in", "the", "1950s", "and", "1960s", "by", "Whittaker", "Chambers", ",", "Garry", "Wills", ",", "and", "M.", "Stanton", "Evans", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, magazine, award, literary genre, poem, location, organization, person, event, book, country and O.\nSentence: They published numerous criticisms in the 1950s and 1960s by Whittaker Chambers , Garry Wills , and M. Stanton Evans .", "prompt_labels": "They(O) published(O) numerous(O) criticisms(O) in(O) the(O) 1950s(O) and(O) 1960s(O) by(O) Whittaker(B-writer) Chambers(I-writer) ,(O) Garry(B-writer) Wills(I-writer) ,(O) and(O) M.(B-writer) Stanton(I-writer) Evans(I-writer) .(O)"}}
{"id": "108", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "location", "event", "literary genre", "magazine", "award", "organization", "poem", "book", "writer", "country"], "instance": {"id": "108", "words": ["Alternate", "history", "shades", "off", "into", "other", "fantasy", "subgenres", "when", "the", "use", "of", "actual", ",", "though", "altered", ",", "history", "and", "geography", "decreases", ",", "although", "a", "culture", "may", "still", "be", "clearly", "the", "original", "source", ";", "Barry", "Hughart", "'", "s", "Bridge", "of", "Birds", "and", "its", "sequels", "take", "place", "in", "a", "fantasy", "world", ",", "albeit", "one", "clearly", "based", "on", "China", ",", "and", "with", "allusions", "to", "actual", "Chinese", "history", ",", "such", "as", "the", "Empress", "Wu", "."], "labels": ["B-literary genre", "I-literary genre", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, event, literary genre, magazine, award, organization, poem, book, writer, country and O.\nSentence: Alternate history shades off into other fantasy subgenres when the use of actual , though altered , history and geography decreases , although a culture may still be clearly the original source ; Barry Hughart ' s Bridge of Birds and its sequels take place in a fantasy world , albeit one clearly based on China , and with allusions to actual Chinese history , such as the Empress Wu .", "prompt_labels": "Alternate(B-literary genre) history(I-literary genre) shades(O) off(O) into(O) other(O) fantasy(B-literary genre) subgenres(O) when(O) the(O) use(O) of(O) actual(O) ,(O) though(O) altered(O) ,(O) history(O) and(O) geography(O) decreases(O) ,(O) although(O) a(O) culture(O) may(O) still(O) be(O) clearly(O) the(O) original(O) source(O) ;(O) Barry(B-writer) Hughart(I-writer) '(O) s(O) Bridge(B-book) of(I-book) Birds(I-book) and(O) its(O) sequels(O) take(O) place(O) in(O) a(O) fantasy(O) world(O) ,(O) albeit(O) one(O) clearly(O) based(O) on(O) China(B-country) ,(O) and(O) with(O) allusions(O) to(O) actual(O) Chinese(O) history(O) ,(O) such(O) as(O) the(O) Empress(B-person) Wu(I-person) .(O)"}}
{"id": "26", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "literary genre", "event", "location", "country", "book", "award", "person", "organization", "writer", "poem"], "instance": {"id": "26", "words": ["Kim", "Stanley", "Robinson", "'", "s", "novel", ",", "The", "Years", "of", "Rice", "and", "Salt", "(", "2002", ")", ",", "starts", "at", "the", "point", "of", "divergence", "with", "Timur", "turning", "his", "army", "away", "from", "Europe", ",", "and", "the", "Black", "Death", "has", "killed", "99", "%", "of", "Europe", "'s", "population", ",", "instead", "of", "only", "a", "third", "."], "labels": ["B-writer", "I-writer", "I-writer", "O", "O", "B-literary genre", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, literary genre, event, location, country, book, award, person, organization, writer, poem and O.\nSentence: Kim Stanley Robinson ' s novel , The Years of Rice and Salt ( 2002 ) , starts at the point of divergence with Timur turning his army away from Europe , and the Black Death has killed 99 % of Europe 's population , instead of only a third .", "prompt_labels": "Kim(B-writer) Stanley(I-writer) Robinson(I-writer) '(O) s(O) novel(B-literary genre) ,(O) The(B-book) Years(I-book) of(I-book) Rice(I-book) and(I-book) Salt(I-book) ((O) 2002(O) )(O) ,(O) starts(O) at(O) the(O) point(O) of(O) divergence(O) with(O) Timur(B-person) turning(O) his(O) army(O) away(O) from(O) Europe(B-location) ,(O) and(O) the(O) Black(O) Death(O) has(O) killed(O) 99(O) %(O) of(O) Europe(B-location) 's(O) population(O) ,(O) instead(O) of(O) only(O) a(O) third(O) .(O)"}}
{"id": "187", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "poem", "country", "person", "organization", "writer", "literary genre", "location", "book", "magazine", "event"], "instance": {"id": "187", "words": ["Following", "the", "American", "publication", ",", "James", "T.", "Farrell", ",", "writing", "in", "The", "New", "Republic", ",", "called", "it", "genuine", ",", "unexaggerated", "and", "intelligent", ",", "while", "Herbert", "Gorman", "wrote", "for", "the", "New", "York", "Times", "Book", "Review", ",", "He", "possesses", "a", "keen", "eye", "for", "character", "and", "a", "rough-and-ready", "'", "styleless", "style", "'", "that", "plunges", "along", "and", "makes", "the", "reader", "see", "what", "the", "author", "wants", "him", "to", "see", "."], "labels": ["O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, poem, country, person, organization, writer, literary genre, location, book, magazine, event and O.\nSentence: Following the American publication , James T. Farrell , writing in The New Republic , called it genuine , unexaggerated and intelligent , while Herbert Gorman wrote for the New York Times Book Review , He possesses a keen eye for character and a rough-and-ready ' styleless style ' that plunges along and makes the reader see what the author wants him to see .", "prompt_labels": "Following(O) the(O) American(O) publication(O) ,(O) James(B-writer) T.(I-writer) Farrell(I-writer) ,(O) writing(O) in(O) The(B-magazine) New(I-magazine) Republic(I-magazine) ,(O) called(O) it(O) genuine(O) ,(O) unexaggerated(O) and(O) intelligent(O) ,(O) while(O) Herbert(B-writer) Gorman(I-writer) wrote(O) for(O) the(O) New(B-magazine) York(I-magazine) Times(I-magazine) Book(I-magazine) Review(I-magazine) ,(O) He(O) possesses(O) a(O) keen(O) eye(O) for(O) character(O) and(O) a(O) rough-and-ready(O) '(O) styleless(O) style(O) '(O) that(O) plunges(O) along(O) and(O) makes(O) the(O) reader(O) see(O) what(O) the(O) author(O) wants(O) him(O) to(O) see(O) .(O)"}}
{"id": "223", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "literary genre", "poem", "country", "writer", "award", "location", "person", "magazine", "book", "event"], "instance": {"id": "223", "words": ["In", "the", "Swahili", "and", "Indonesian", "culture", ",", "many", "of", "his", "stories", "are", "being", "told", "under", "the", "name", "of", "Abunuwasi", "or", "Abunawas", ",", "though", "this", "confuses", "Nasreddin", "with", "an", "entirely", "different", "man", "-", "the", "poet", "Abu", "Nuwas", ",", "known", "for", "homoerotic", "verse", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "B-person", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "B-literary genre", "I-literary genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, literary genre, poem, country, writer, award, location, person, magazine, book, event and O.\nSentence: In the Swahili and Indonesian culture , many of his stories are being told under the name of Abunuwasi or Abunawas , though this confuses Nasreddin with an entirely different man - the poet Abu Nuwas , known for homoerotic verse .", "prompt_labels": "In(O) the(O) Swahili(O) and(O) Indonesian(O) culture(O) ,(O) many(O) of(O) his(O) stories(O) are(O) being(O) told(O) under(O) the(O) name(O) of(O) Abunuwasi(B-person) or(O) Abunawas(B-person) ,(O) though(O) this(O) confuses(O) Nasreddin(B-person) with(O) an(O) entirely(O) different(O) man(O) -(O) the(O) poet(O) Abu(B-writer) Nuwas(I-writer) ,(O) known(O) for(O) homoerotic(B-literary genre) verse(I-literary genre) .(O)"}}
{"id": "31", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "literary genre", "person", "writer", "magazine", "award", "event", "poem", "book", "country", "organization"], "instance": {"id": "31", "words": ["The", "Faerie", "Queene", "by", "Edmund", "Spenser", ",", "with", "its", "stanzas", "of", "eight", "iambic", "pentameter", "lines", "followed", "by", "one", "alexandrine", ",", "exemplifies", "what", "came", "to", "be", "its", "chief", "role", ":", "as", "a", "somewhat", "infrequent", "variant", "line", "in", "an", "otherwise", "iambic", "pentameter", "context", "."], "labels": ["B-book", "I-book", "I-book", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, literary genre, person, writer, magazine, award, event, poem, book, country, organization and O.\nSentence: The Faerie Queene by Edmund Spenser , with its stanzas of eight iambic pentameter lines followed by one alexandrine , exemplifies what came to be its chief role : as a somewhat infrequent variant line in an otherwise iambic pentameter context .", "prompt_labels": "The(B-book) Faerie(I-book) Queene(I-book) by(O) Edmund(B-writer) Spenser(I-writer) ,(O) with(O) its(O) stanzas(O) of(O) eight(O) iambic(O) pentameter(O) lines(O) followed(O) by(O) one(O) alexandrine(O) ,(O) exemplifies(O) what(O) came(O) to(O) be(O) its(O) chief(O) role(O) :(O) as(O) a(O) somewhat(O) infrequent(O) variant(O) line(O) in(O) an(O) otherwise(O) iambic(O) pentameter(O) context(O) .(O)"}}
{"id": "234", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "writer", "book", "poem", "organization", "event", "magazine", "award", "country", "person", "location"], "instance": {"id": "234", "words": ["Years", "later", ",", "Lead", "Me", "On", "would", "be", "chosen", "as", "the", "greatest", "Contemporary", "Christian", "album", "of", "all", "time", "by", "CCM", "Magazine", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, writer, book, poem, organization, event, magazine, award, country, person, location and O.\nSentence: Years later , Lead Me On would be chosen as the greatest Contemporary Christian album of all time by CCM Magazine .", "prompt_labels": "Years(O) later(O) ,(O) Lead(O) Me(O) On(O) would(O) be(O) chosen(O) as(O) the(O) greatest(O) Contemporary(O) Christian(O) album(O) of(O) all(O) time(O) by(O) CCM(B-magazine) Magazine(I-magazine) .(O)"}}
{"id": "5", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "poem", "writer", "country", "location", "literary genre", "organization", "event", "award", "book", "magazine"], "instance": {"id": "5", "words": ["The", "Forsyte", "Saga", ",", "first", "published", "under", "that", "title", "in", "1922", ",", "is", "a", "series", "of", "three", "novels", "and", "two", "interludes", "published", "between", "1906", "and", "1921", "by", "Nobel", "Prize", "in", "Literature", "-winning", "English", "author", "John", "Galsworthy", "."], "labels": ["B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, poem, writer, country, location, literary genre, organization, event, award, book, magazine and O.\nSentence: The Forsyte Saga , first published under that title in 1922 , is a series of three novels and two interludes published between 1906 and 1921 by Nobel Prize in Literature -winning English author John Galsworthy .", "prompt_labels": "The(B-book) Forsyte(I-book) Saga(I-book) ,(O) first(O) published(O) under(O) that(O) title(O) in(O) 1922(O) ,(O) is(O) a(O) series(O) of(O) three(O) novels(B-literary genre) and(O) two(O) interludes(O) published(O) between(O) 1906(O) and(O) 1921(O) by(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) -winning(O) English(O) author(O) John(B-writer) Galsworthy(I-writer) .(O)"}}
{"id": "18", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "country", "award", "person", "book", "poem", "magazine", "literary genre", "writer", "organization", "event"], "instance": {"id": "18", "words": ["They", "were", "Rachel", ",", "Rachel", "(", "1968", ")", ",", "based", "on", "Margaret", "Laurence", "'", "s", "A", "Jest", "of", "God", ",", "the", "screen", "version", "of", "the", "Pulitzer", "Prize", "-winning", "play", "The", "Effect", "of", "Gamma", "Rays", "on", "Man-in-the-Moon", "Marigolds", "(", "1972", ")", ",", "the", "television", "screen", "version", "of", "the", "Pulitzer", "Prize-winning", "play", "The", "Shadow", "Box", "(", "1980", ")", ",", "and", "a", "screen", "version", "of", "Tennessee", "Williams", "'", "The", "Glass", "Menagerie", "(", "1987", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, award, person, book, poem, magazine, literary genre, writer, organization, event and O.\nSentence: They were Rachel , Rachel ( 1968 ) , based on Margaret Laurence ' s A Jest of God , the screen version of the Pulitzer Prize -winning play The Effect of Gamma Rays on Man-in-the-Moon Marigolds ( 1972 ) , the television screen version of the Pulitzer Prize-winning play The Shadow Box ( 1980 ) , and a screen version of Tennessee Williams ' The Glass Menagerie ( 1987 ) .", "prompt_labels": "They(O) were(O) Rachel(O) ,(O) Rachel(O) ((O) 1968(O) )(O) ,(O) based(O) on(O) Margaret(B-writer) Laurence(I-writer) '(O) s(O) A(B-book) Jest(I-book) of(I-book) God(I-book) ,(O) the(O) screen(O) version(O) of(O) the(O) Pulitzer(B-award) Prize(I-award) -winning(O) play(O) The(B-book) Effect(I-book) of(I-book) Gamma(I-book) Rays(I-book) on(I-book) Man-in-the-Moon(I-book) Marigolds(I-book) ((O) 1972(O) )(O) ,(O) the(O) television(O) screen(O) version(O) of(O) the(O) Pulitzer(B-award) Prize-winning(I-award) play(O) The(B-book) Shadow(I-book) Box(I-book) ((O) 1980(O) )(O) ,(O) and(O) a(O) screen(O) version(O) of(O) Tennessee(B-writer) Williams(I-writer) '(O) The(B-book) Glass(I-book) Menagerie(I-book) ((O) 1987(O) )(O) .(O)"}}
{"id": "297", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "person", "organization", "event", "magazine", "location", "country", "book", "award", "poem", "literary genre"], "instance": {"id": "297", "words": ["Daniel", "11", ":", "A", "future", "king", "of", "Persia", "will", "make", "war", "on", "the", "king", "of", "Greece", ",", "a", "mighty", "king", "will", "arise", "and", "wield", "power", "until", "his", "empire", "is", "broken", "up", "and", "given", "to", "others", ",", "and", "finally", "the", "king", "of", "the", "south", "(", "identified", "in", "verse", "8", "as", "Egypt", ")", "will", "go", "to", "war", "with", "the", "king", "of", "the", "north", "."], "labels": ["B-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, person, organization, event, magazine, location, country, book, award, poem, literary genre and O.\nSentence: Daniel 11 : A future king of Persia will make war on the king of Greece , a mighty king will arise and wield power until his empire is broken up and given to others , and finally the king of the south ( identified in verse 8 as Egypt ) will go to war with the king of the north .", "prompt_labels": "Daniel(B-book) 11(I-book) :(O) A(O) future(O) king(O) of(O) Persia(O) will(O) make(O) war(O) on(O) the(O) king(O) of(O) Greece(B-country) ,(O) a(O) mighty(O) king(O) will(O) arise(O) and(O) wield(O) power(O) until(O) his(O) empire(O) is(O) broken(O) up(O) and(O) given(O) to(O) others(O) ,(O) and(O) finally(O) the(O) king(O) of(O) the(O) south(O) ((O) identified(O) in(O) verse(O) 8(O) as(O) Egypt(B-country) )(O) will(O) go(O) to(O) war(O) with(O) the(O) king(O) of(O) the(O) north(O) .(O)"}}
{"id": "209", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "location", "organization", "poem", "magazine", "award", "book", "writer", "literary genre", "country", "event"], "instance": {"id": "209", "words": ["Among", "his", "most", "celebrated", "tales", "are", "The", "Call", "of", "Cthulhu", ",", "The", "Rats", "in", "the", "Walls", ",", "At", "the", "Mountains", "of", "Madness", ",", "The", "Shadow", "over", "Innsmouth", ",", "and", "The", "Shadow", "Out", "of", "Time", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, organization, poem, magazine, award, book, writer, literary genre, country, event and O.\nSentence: Among his most celebrated tales are The Call of Cthulhu , The Rats in the Walls , At the Mountains of Madness , The Shadow over Innsmouth , and The Shadow Out of Time .", "prompt_labels": "Among(O) his(O) most(O) celebrated(O) tales(O) are(O) The(B-book) Call(I-book) of(I-book) Cthulhu(I-book) ,(O) The(B-book) Rats(I-book) in(I-book) the(I-book) Walls(I-book) ,(O) At(B-book) the(I-book) Mountains(I-book) of(I-book) Madness(I-book) ,(O) The(B-book) Shadow(I-book) over(I-book) Innsmouth(I-book) ,(O) and(O) The(B-book) Shadow(I-book) Out(I-book) of(I-book) Time(I-book) .(O)"}}
{"id": "158", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "magazine", "country", "award", "poem", "literary genre", "book", "person", "writer", "organization", "event"], "instance": {"id": "158", "words": ["He", "has", "received", "five", "Robert", "Awards", "(", "including", "Best", "Film", "and", "Best", "Director", ")", "and", "three", "Bodil", "Awards", "for", "Best", "Danish", "Film", "."], "labels": ["O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, magazine, country, award, poem, literary genre, book, person, writer, organization, event and O.\nSentence: He has received five Robert Awards ( including Best Film and Best Director ) and three Bodil Awards for Best Danish Film .", "prompt_labels": "He(O) has(O) received(O) five(O) Robert(B-award) Awards(I-award) ((O) including(O) Best(B-award) Film(I-award) and(O) Best(B-award) Director(I-award) )(O) and(O) three(O) Bodil(B-award) Awards(I-award) for(O) Best(B-award) Danish(I-award) Film(I-award) .(O)"}}
{"id": "124", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "literary genre", "award", "magazine", "writer", "location", "book", "person", "event", "organization", "poem"], "instance": {"id": "124", "words": ["Hunter", "S.", "Thompson", "wrote", "a", "scathing", "piece", "denouncing", "Nixon", "for", "Rolling", "Stone", ",", "entitled", "He", "Was", "a", "Crook", "(", "which", "also", "appeared", "a", "month", "later", "in", "The", "Atlantic", ")", ".ref", "name", "=", "atlantic", "In", "his", "article", ",", "Thompson", "described", "Nixon", "as", "a", "political", "monster", "straight", "out", "of", "Grendel", "and", "a", "very", "dangerous", "enemy", "."], "labels": ["B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "B-person", "O", "B-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, literary genre, award, magazine, writer, location, book, person, event, organization, poem and O.\nSentence: Hunter S. Thompson wrote a scathing piece denouncing Nixon for Rolling Stone , entitled He Was a Crook ( which also appeared a month later in The Atlantic ) .ref name = atlantic In his article , Thompson described Nixon as a political monster straight out of Grendel and a very dangerous enemy .", "prompt_labels": "Hunter(B-writer) S.(I-writer) Thompson(I-writer) wrote(O) a(O) scathing(O) piece(O) denouncing(O) Nixon(B-person) for(O) Rolling(B-magazine) Stone(I-magazine) ,(O) entitled(O) He(O) Was(O) a(O) Crook(O) ((O) which(O) also(O) appeared(O) a(O) month(O) later(O) in(O) The(B-magazine) Atlantic(I-magazine) )(O) .ref(O) name(O) =(O) atlantic(O) In(O) his(O) article(O) ,(O) Thompson(B-writer) described(O) Nixon(B-person) as(O) a(O) political(O) monster(O) straight(O) out(O) of(O) Grendel(O) and(O) a(O) very(O) dangerous(O) enemy(O) .(O)"}}
{"id": "54", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "person", "location", "country", "literary genre", "magazine", "book", "writer", "poem", "organization", "event"], "instance": {"id": "54", "words": ["Among", "his", "best-known", "works", "are", "Moby-Dick", "(", "1851", ")", ",", "Typee", "(", "1846", ")", ",", "a", "romanticized", "account", "of", "his", "experiences", "in", "Polynesia", ",", "and", "Billy", "Budd", ",", "a", "posthumously", "published", "novella", "."], "labels": ["O", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "B-literary genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, location, country, literary genre, magazine, book, writer, poem, organization, event and O.\nSentence: Among his best-known works are Moby-Dick ( 1851 ) , Typee ( 1846 ) , a romanticized account of his experiences in Polynesia , and Billy Budd , a posthumously published novella .", "prompt_labels": "Among(O) his(O) best-known(O) works(O) are(O) Moby-Dick(B-book) ((O) 1851(O) )(O) ,(O) Typee(B-book) ((O) 1846(O) )(O) ,(O) a(O) romanticized(O) account(O) of(O) his(O) experiences(O) in(O) Polynesia(B-country) ,(O) and(O) Billy(B-book) Budd(I-book) ,(O) a(O) posthumously(O) published(O) novella(B-literary genre) .(O)"}}
{"id": "76", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "event", "writer", "country", "location", "magazine", "person", "organization", "literary genre", "book", "poem"], "instance": {"id": "76", "words": ["Dick", "said", "he", "conceived", "The", "Man", "in", "the", "High", "Castle", "when", "reading", "Bring", "the", "Jubilee", "(", "1953", ")", ",", "by", "Ward", "Moore", ",", "which", "occurs", "mainly", "in", "an", "alternative", "20th-century", "US", "wherein", "the", "Confederate", "States", "of", "America", "won", "the", "American", "Civil", "War", "."], "labels": ["B-writer", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, writer, country, location, magazine, person, organization, literary genre, book, poem and O.\nSentence: Dick said he conceived The Man in the High Castle when reading Bring the Jubilee ( 1953 ) , by Ward Moore , which occurs mainly in an alternative 20th-century US wherein the Confederate States of America won the American Civil War .", "prompt_labels": "Dick(B-writer) said(O) he(O) conceived(O) The(B-book) Man(I-book) in(I-book) the(I-book) High(I-book) Castle(I-book) when(O) reading(O) Bring(B-book) the(I-book) Jubilee(I-book) ((O) 1953(O) )(O) ,(O) by(O) Ward(B-writer) Moore(I-writer) ,(O) which(O) occurs(O) mainly(O) in(O) an(O) alternative(O) 20th-century(O) US(B-country) wherein(O) the(O) Confederate(B-organization) States(I-organization) of(I-organization) America(I-organization) won(O) the(O) American(B-event) Civil(I-event) War(I-event) .(O)"}}
{"id": "91", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "event", "book", "organization", "writer", "person", "location", "poem", "country", "award", "magazine"], "instance": {"id": "91", "words": ["With", "William", "Butler", "Yeats", "and", "Edward", "Martyn", ",", "she", "co-founded", "the", "Irish", "Literary", "Theatre", "and", "the", "Abbey", "Theatre", ",", "and", "wrote", "numerous", "short", "works", "for", "both", "companies", "."], "labels": ["O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, event, book, organization, writer, person, location, poem, country, award, magazine and O.\nSentence: With William Butler Yeats and Edward Martyn , she co-founded the Irish Literary Theatre and the Abbey Theatre , and wrote numerous short works for both companies .", "prompt_labels": "With(O) William(B-writer) Butler(I-writer) Yeats(I-writer) and(O) Edward(B-writer) Martyn(I-writer) ,(O) she(O) co-founded(O) the(O) Irish(B-organization) Literary(I-organization) Theatre(I-organization) and(O) the(O) Abbey(B-organization) Theatre(I-organization) ,(O) and(O) wrote(O) numerous(O) short(O) works(O) for(O) both(O) companies(O) .(O)"}}
{"id": "65", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "award", "organization", "literary genre", "writer", "location", "book", "poem", "event", "person", "country"], "instance": {"id": "65", "words": ["Smith", "briefly", "moved", "among", "the", "circle", "that", "included", "Ambrose", "Bierce", "and", "Jack", "London", ",", "but", "his", "early", "fame", "soon", "faded", "away", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, award, organization, literary genre, writer, location, book, poem, event, person, country and O.\nSentence: Smith briefly moved among the circle that included Ambrose Bierce and Jack London , but his early fame soon faded away .", "prompt_labels": "Smith(B-writer) briefly(O) moved(O) among(O) the(O) circle(O) that(O) included(O) Ambrose(B-writer) Bierce(I-writer) and(O) Jack(B-writer) London(I-writer) ,(O) but(O) his(O) early(O) fame(O) soon(O) faded(O) away(O) .(O)"}}
{"id": "129", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "country", "book", "location", "writer", "literary genre", "award", "organization", "event", "poem", "person"], "instance": {"id": "129", "words": ["Prahlādacharita", "a", "Sanskrit", "work", "written", "by", "Rama", "Varma", "Parikshith", "Thampuran", ",", "former", "Maharaja", "of", "Cochin", "is", "in", "Champu", "style", "."], "labels": ["B-poem", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "I-writer", "O", "O", "O", "O", "B-country", "O", "O", "B-literary genre", "I-literary genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, country, book, location, writer, literary genre, award, organization, event, poem, person and O.\nSentence: Prahlādacharita a Sanskrit work written by Rama Varma Parikshith Thampuran , former Maharaja of Cochin is in Champu style .", "prompt_labels": "Prahlādacharita(B-poem) a(O) Sanskrit(O) work(O) written(O) by(O) Rama(B-writer) Varma(I-writer) Parikshith(I-writer) Thampuran(I-writer) ,(O) former(O) Maharaja(O) of(O) Cochin(B-country) is(O) in(O) Champu(B-literary genre) style(I-literary genre) .(O)"}}
{"id": "74", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "person", "award", "literary genre", "location", "writer", "country", "organization", "event", "magazine", "poem"], "instance": {"id": "74", "words": ["Alternate", "history", "has", "long", "been", "a", "staple", "of", "Japanese", "speculative", "fiction", "with", "such", "authors", "as", "Futaro", "Yamada", "and", "Ryō", "Hanmura", "writing", "novels", "set", "in", "recognizable", "historical", "settings", "with", "supernatural", "or", "science", "fiction", "elements", "present", "."], "labels": ["B-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "B-literary genre", "I-literary genre", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, person, award, literary genre, location, writer, country, organization, event, magazine, poem and O.\nSentence: Alternate history has long been a staple of Japanese speculative fiction with such authors as Futaro Yamada and Ryō Hanmura writing novels set in recognizable historical settings with supernatural or science fiction elements present .", "prompt_labels": "Alternate(B-literary genre) history(I-literary genre) has(O) long(O) been(O) a(O) staple(O) of(O) Japanese(O) speculative(O) fiction(B-literary genre) with(O) such(O) authors(O) as(O) Futaro(B-writer) Yamada(I-writer) and(O) Ryō(B-writer) Hanmura(I-writer) writing(O) novels(B-literary genre) set(O) in(O) recognizable(O) historical(O) settings(O) with(O) supernatural(B-literary genre) or(O) science(B-literary genre) fiction(I-literary genre) elements(O) present(O) .(O)"}}
{"id": "405", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "person", "award", "event", "book", "writer", "organization", "poem", "country", "literary genre", "location"], "instance": {"id": "405", "words": ["See", "also", "Two", "Figures", "In", "Dense", "Violet", "Night", "which", "can", "be", "read", "as", "a", "humorous", "anecdote", "about", "the", "gauche", "male", ",", "or", "a", "meditation", "on", "the", "lover", "'s", "otherness", ",", "or", "the", "poet", "'s", "challenge", "to", "the", "imagination", "of", "the", "reader", "."], "labels": ["O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, person, award, event, book, writer, organization, poem, country, literary genre, location and O.\nSentence: See also Two Figures In Dense Violet Night which can be read as a humorous anecdote about the gauche male , or a meditation on the lover 's otherness , or the poet 's challenge to the imagination of the reader .", "prompt_labels": "See(O) also(O) Two(B-poem) Figures(I-poem) In(I-poem) Dense(I-poem) Violet(I-poem) Night(I-poem) which(O) can(O) be(O) read(O) as(O) a(O) humorous(O) anecdote(O) about(O) the(O) gauche(O) male(O) ,(O) or(O) a(O) meditation(O) on(O) the(O) lover(O) 's(O) otherness(O) ,(O) or(O) the(O) poet(O) 's(O) challenge(O) to(O) the(O) imagination(O) of(O) the(O) reader(O) .(O)"}}
{"id": "337", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "person", "magazine", "location", "writer", "literary genre", "country", "event", "poem", "organization", "book"], "instance": {"id": "337", "words": ["Baudelaire", "'s", "highly", "original", "style", "of", "prose-poetry", "influenced", "a", "whole", "generation", "of", "poets", "including", "Paul", "Verlaine", ",", "Arthur", "Rimbaud", "and", "Stéphane", "Mallarmé", ",", "among", "many", "others", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, magazine, location, writer, literary genre, country, event, poem, organization, book and O.\nSentence: Baudelaire 's highly original style of prose-poetry influenced a whole generation of poets including Paul Verlaine , Arthur Rimbaud and Stéphane Mallarmé , among many others .", "prompt_labels": "Baudelaire(B-writer) 's(O) highly(O) original(O) style(O) of(O) prose-poetry(B-literary genre) influenced(O) a(O) whole(O) generation(O) of(O) poets(O) including(O) Paul(B-writer) Verlaine(I-writer) ,(O) Arthur(B-writer) Rimbaud(I-writer) and(O) Stéphane(B-writer) Mallarmé(I-writer) ,(O) among(O) many(O) others(O) .(O)"}}
{"id": "334", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "writer", "literary genre", "country", "book", "person", "organization", "poem", "magazine", "award", "location"], "instance": {"id": "334", "words": ["247", "Many", "lyricists", "worked", "on", "the", "show", ",", "including", "James", "Agee", ",", "Dorothy", "Parker", ",", "John", "Latouche", ",", "Richard", "Wilbur", ",", "Leonard", "and", "Felicia", "Bernstein", ",", "and", "Hellman", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "O", "B-writer", "I-writer", "O", "O", "B-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, writer, literary genre, country, book, person, organization, poem, magazine, award, location and O.\nSentence: 247 Many lyricists worked on the show , including James Agee , Dorothy Parker , John Latouche , Richard Wilbur , Leonard and Felicia Bernstein , and Hellman .", "prompt_labels": "247(O) Many(O) lyricists(O) worked(O) on(O) the(O) show(O) ,(O) including(O) James(B-writer) Agee(I-writer) ,(O) Dorothy(B-writer) Parker(I-writer) ,(O) John(B-writer) Latouche(I-writer) ,(O) Richard(B-writer) Wilbur(I-writer) ,(O) Leonard(B-writer) and(O) Felicia(B-writer) Bernstein(I-writer) ,(O) and(O) Hellman(B-writer) .(O)"}}
{"id": "381", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "location", "literary genre", "poem", "organization", "writer", "event", "person", "country", "book", "magazine"], "instance": {"id": "381", "words": ["His", "first", "published", "novels", "were", "social", "satires", ",", "Crome", "Yellow", "(", "1921", ")", ",", "Antic", "Hay", "(", "1923", ")", ",", "Those", "Barren", "Leaves", "(", "1925", ")", ",", "and", "Point", "Counter", "Point", "(", "1928", ")", "."], "labels": ["O", "O", "O", "B-literary genre", "O", "B-literary genre", "I-literary genre", "O", "B-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, literary genre, poem, organization, writer, event, person, country, book, magazine and O.\nSentence: His first published novels were social satires , Crome Yellow ( 1921 ) , Antic Hay ( 1923 ) , Those Barren Leaves ( 1925 ) , and Point Counter Point ( 1928 ) .", "prompt_labels": "His(O) first(O) published(O) novels(B-literary genre) were(O) social(B-literary genre) satires(I-literary genre) ,(O) Crome(B-book) Yellow(I-book) ((O) 1921(O) )(O) ,(O) Antic(B-book) Hay(I-book) ((O) 1923(O) )(O) ,(O) Those(B-book) Barren(I-book) Leaves(I-book) ((O) 1925(O) )(O) ,(O) and(O) Point(B-book) Counter(I-book) Point(I-book) ((O) 1928(O) )(O) .(O)"}}
{"id": "116", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "literary genre", "magazine", "organization", "poem", "writer", "book", "event", "location", "country", "person"], "instance": {"id": "116", "words": ["His", "film", "Night", "Train", "to", "Lisbon", "(", "2013", ")", "premiered", "out", "of", "competition", "at", "the", "63rd", "Berlin", "International", "Film", "Festival", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, literary genre, magazine, organization, poem, writer, book, event, location, country, person and O.\nSentence: His film Night Train to Lisbon ( 2013 ) premiered out of competition at the 63rd Berlin International Film Festival .", "prompt_labels": "His(O) film(O) Night(O) Train(O) to(O) Lisbon(O) ((O) 2013(O) )(O) premiered(O) out(O) of(O) competition(O) at(O) the(O) 63rd(B-event) Berlin(I-event) International(I-event) Film(I-event) Festival(I-event) .(O)"}}
{"id": "336", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "organization", "poem", "writer", "magazine", "location", "event", "literary genre", "award", "person", "book"], "instance": {"id": "336", "words": ["His", "works", "have", "been", "short", "listed", "for", "the", "Booker", "Prize", "five", "times", ",", "in", "1981", "for", "Midnight", "'s", "Children", ",", "1983", "for", "Shame", ",", "1988", "for", "The", "Satanic", "Verses", ",", "1995", "for", "The", "Moor", "'s", "Last", "Sigh", ",", "2019", "for", "Quichotte", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "B-book", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "B-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, poem, writer, magazine, location, event, literary genre, award, person, book and O.\nSentence: His works have been short listed for the Booker Prize five times , in 1981 for Midnight 's Children , 1983 for Shame , 1988 for The Satanic Verses , 1995 for The Moor 's Last Sigh , 2019 for Quichotte .", "prompt_labels": "His(O) works(O) have(O) been(O) short(O) listed(O) for(O) the(O) Booker(B-award) Prize(I-award) five(O) times(O) ,(O) in(O) 1981(O) for(O) Midnight(B-book) 's(I-book) Children(I-book) ,(O) 1983(O) for(O) Shame(B-book) ,(O) 1988(O) for(O) The(B-book) Satanic(I-book) Verses(I-book) ,(O) 1995(O) for(O) The(B-book) Moor(I-book) 's(I-book) Last(I-book) Sigh(I-book) ,(O) 2019(O) for(O) Quichotte(B-book) .(O)"}}
{"id": "155", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "writer", "poem", "location", "country", "book", "magazine", "award", "literary genre", "event", "organization"], "instance": {"id": "155", "words": ["In", "the", "mid-1970s", ",", "Pohl", "acquired", "and", "edited", "novels", "for", "Bantam", "Books", ",", "published", "as", "Frederik", "Pohl", "Selections", ";", "these", "included", "Samuel", "R.", "Delany", "'", "s", "Dhalgren", "and", "Joanna", "Russ", "'", "s", "The", "Female", "Man", "."], "labels": ["O", "O", "O", "O", "B-writer", "O", "O", "O", "B-literary genre", "O", "B-organization", "I-organization", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-book", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, poem, location, country, book, magazine, award, literary genre, event, organization and O.\nSentence: In the mid-1970s , Pohl acquired and edited novels for Bantam Books , published as Frederik Pohl Selections ; these included Samuel R. Delany ' s Dhalgren and Joanna Russ ' s The Female Man .", "prompt_labels": "In(O) the(O) mid-1970s(O) ,(O) Pohl(B-writer) acquired(O) and(O) edited(O) novels(B-literary genre) for(O) Bantam(B-organization) Books(I-organization) ,(O) published(O) as(O) Frederik(B-writer) Pohl(I-writer) Selections(O) ;(O) these(O) included(O) Samuel(B-writer) R.(I-writer) Delany(I-writer) '(O) s(O) Dhalgren(B-book) and(O) Joanna(B-writer) Russ(I-writer) '(O) s(O) The(B-book) Female(I-book) Man(I-book) .(O)"}}
{"id": "120", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "event", "location", "writer", "magazine", "poem", "literary genre", "country", "organization", "award", "person"], "instance": {"id": "120", "words": ["Certain", "works", ",", "though", "published", "legally", "by", "the", "State-controlled", "media", ",", "were", "practically", "impossible", "to", "find", "in", "bookshops", "and", "libraries", ",", "and", "found", "their", "way", "into", "samizdat", ":", "for", "example", "Aleksandr", "Solzhenitsyn", "'", "s", "novel", "One", "Day", "in", "the", "Life", "of", "Ivan", "Denisovich", "was", "widely", "distributed", "via", "samizdat", ".", "November", "1962", "issue", "of", "the", "Novy", "Mir", "literary", "magazine"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-literary genre", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, event, location, writer, magazine, poem, literary genre, country, organization, award, person and O.\nSentence: Certain works , though published legally by the State-controlled media , were practically impossible to find in bookshops and libraries , and found their way into samizdat : for example Aleksandr Solzhenitsyn ' s novel One Day in the Life of Ivan Denisovich was widely distributed via samizdat . November 1962 issue of the Novy Mir literary magazine", "prompt_labels": "Certain(O) works(O) ,(O) though(O) published(O) legally(O) by(O) the(O) State-controlled(O) media(O) ,(O) were(O) practically(O) impossible(O) to(O) find(O) in(O) bookshops(O) and(O) libraries(O) ,(O) and(O) found(O) their(O) way(O) into(O) samizdat(O) :(O) for(O) example(O) Aleksandr(B-writer) Solzhenitsyn(I-writer) '(O) s(O) novel(B-literary genre) One(B-book) Day(I-book) in(I-book) the(I-book) Life(I-book) of(I-book) Ivan(I-book) Denisovich(I-book) was(O) widely(O) distributed(O) via(O) samizdat(O) .(O) November(O) 1962(O) issue(O) of(O) the(O) Novy(B-magazine) Mir(I-magazine) literary(O) magazine(O)"}}
{"id": "70", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "writer", "magazine", "literary genre", "event", "organization", "person", "poem", "country", "location", "book"], "instance": {"id": "70", "words": ["Classicist", "Bernard", "Knox", "made", "direct", "reference", "to", "this", "topic", "when", "he", "delivered", "his", "1992", "Jefferson", "Lecture", "(", "the", "U.S.", "federal", "government", "'s", "highest", "honor", "for", "achievement", "in", "the", "humanities", ")", "."], "labels": ["O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, writer, magazine, literary genre, event, organization, person, poem, country, location, book and O.\nSentence: Classicist Bernard Knox made direct reference to this topic when he delivered his 1992 Jefferson Lecture ( the U.S. federal government 's highest honor for achievement in the humanities ) .", "prompt_labels": "Classicist(O) Bernard(B-writer) Knox(I-writer) made(O) direct(O) reference(O) to(O) this(O) topic(O) when(O) he(O) delivered(O) his(O) 1992(O) Jefferson(B-award) Lecture(I-award) ((O) the(O) U.S.(B-country) federal(O) government(O) 's(O) highest(O) honor(O) for(O) achievement(O) in(O) the(O) humanities(O) )(O) .(O)"}}
{"id": "350", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "book", "location", "person", "literary genre", "magazine", "organization", "country", "award", "event", "poem"], "instance": {"id": "350", "words": ["Two", "decades", "later", ",", "Herbert", "'s", "son", "Brian", "Herbert", ",", "along", "with", "Kevin", "J.", "Anderson", ",", "published", "two", "sequel", "s", "-", "Hunters", "of", "Dune", "(", "2006", ")", "and", "Sandworms", "of", "Dune", "(", "2007", ")", "-", "based", "in", "part", "on", "notes", "left", "behind", "by", "Frank", "Herbert", "for", "what", "he", "referred", "to", "as", "Dune", "7", ",", "his", "own", "planned", "seventh", "novel", "in", "the", "Dune", "series", "."], "labels": ["O", "O", "O", "O", "B-writer", "O", "O", "B-writer", "I-writer", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, location, person, literary genre, magazine, organization, country, award, event, poem and O.\nSentence: Two decades later , Herbert 's son Brian Herbert , along with Kevin J. Anderson , published two sequel s - Hunters of Dune ( 2006 ) and Sandworms of Dune ( 2007 ) - based in part on notes left behind by Frank Herbert for what he referred to as Dune 7 , his own planned seventh novel in the Dune series .", "prompt_labels": "Two(O) decades(O) later(O) ,(O) Herbert(B-writer) 's(O) son(O) Brian(B-writer) Herbert(I-writer) ,(O) along(O) with(O) Kevin(B-writer) J.(I-writer) Anderson(I-writer) ,(O) published(O) two(O) sequel(O) s(O) -(O) Hunters(B-book) of(I-book) Dune(I-book) ((O) 2006(O) )(O) and(O) Sandworms(B-book) of(I-book) Dune(I-book) ((O) 2007(O) )(O) -(O) based(O) in(O) part(O) on(O) notes(O) left(O) behind(O) by(O) Frank(B-writer) Herbert(I-writer) for(O) what(O) he(O) referred(O) to(O) as(O) Dune(B-book) 7(I-book) ,(O) his(O) own(O) planned(O) seventh(O) novel(B-literary genre) in(O) the(O) Dune(O) series(O) .(O)"}}
{"id": "41", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "person", "book", "country", "writer", "award", "location", "event", "poem", "organization", "magazine"], "instance": {"id": "41", "words": ["She", "appeared", "in", "Catherine", "Cookson", "'", "s", "The", "Fifteen", "Streets", ",", "alongside", "Sean", "Bean", "and", "Owen", "Teale", "in", "1989", ";", "Our", "Own", "Kind", "(", "Bush", ",", "1991", ")", ";", "Deadly", "Advice", "(", "Fletcher", ",", "1993", ")", ";", "Cabaret", "(", "Donmar", "Warehouse", "1994", ")", ";", "Macbeth", "(", "Greenwich", "Theatre", ",", "1995", ")", ";", "and", "Absurd", "Person", "Singular", "(", "Garrick", "Theatre", ",", "2007", ")", "."], "labels": ["O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "B-book", "I-book", "I-book", "O", "B-person", "O", "O", "O", "O", "B-book", "I-book", "O", "B-person", "O", "O", "O", "O", "B-book", "O", "B-location", "I-location", "O", "O", "O", "B-book", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "B-location", "I-location", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, person, book, country, writer, award, location, event, poem, organization, magazine and O.\nSentence: She appeared in Catherine Cookson ' s The Fifteen Streets , alongside Sean Bean and Owen Teale in 1989 ; Our Own Kind ( Bush , 1991 ) ; Deadly Advice ( Fletcher , 1993 ) ; Cabaret ( Donmar Warehouse 1994 ) ; Macbeth ( Greenwich Theatre , 1995 ) ; and Absurd Person Singular ( Garrick Theatre , 2007 ) .", "prompt_labels": "She(O) appeared(O) in(O) Catherine(B-writer) Cookson(I-writer) '(O) s(O) The(B-book) Fifteen(I-book) Streets(I-book) ,(O) alongside(O) Sean(B-person) Bean(I-person) and(O) Owen(B-person) Teale(I-person) in(O) 1989(O) ;(O) Our(B-book) Own(I-book) Kind(I-book) ((O) Bush(B-person) ,(O) 1991(O) )(O) ;(O) Deadly(B-book) Advice(I-book) ((O) Fletcher(B-person) ,(O) 1993(O) )(O) ;(O) Cabaret(B-book) ((O) Donmar(B-location) Warehouse(I-location) 1994(O) )(O) ;(O) Macbeth(B-book) ((O) Greenwich(B-location) Theatre(I-location) ,(O) 1995(O) )(O) ;(O) and(O) Absurd(B-book) Person(I-book) Singular(I-book) ((O) Garrick(B-location) Theatre(I-location) ,(O) 2007(O) )(O) .(O)"}}
{"id": "188", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "award", "location", "writer", "country", "book", "poem", "person", "magazine", "organization", "literary genre"], "instance": {"id": "188", "words": ["In", "1962", ",", "two", "major", "anthologies", "of", "Borges", "'s", "writings", "were", "published", "in", "English", "by", "New", "York", "presses", ":", "Ficciones", "and", "Labyrinths", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-book", "O", "B-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, location, writer, country, book, poem, person, magazine, organization, literary genre and O.\nSentence: In 1962 , two major anthologies of Borges 's writings were published in English by New York presses : Ficciones and Labyrinths .", "prompt_labels": "In(O) 1962(O) ,(O) two(O) major(O) anthologies(O) of(O) Borges(B-writer) 's(O) writings(O) were(O) published(O) in(O) English(O) by(O) New(B-organization) York(I-organization) presses(I-organization) :(O) Ficciones(B-book) and(O) Labyrinths(B-book) .(O)"}}
{"id": "132", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "person", "event", "book", "poem", "award", "magazine", "literary genre", "country", "organization", "location"], "instance": {"id": "132", "words": ["In", "1992", ",", "he", "narrated", "Dr.", "Seuss", "Video", "Classics", ":", "Horton", "Hatches", "the", "Egg", "."], "labels": ["O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, person, event, book, poem, award, magazine, literary genre, country, organization, location and O.\nSentence: In 1992 , he narrated Dr. Seuss Video Classics : Horton Hatches the Egg .", "prompt_labels": "In(O) 1992(O) ,(O) he(O) narrated(O) Dr.(B-writer) Seuss(I-writer) Video(O) Classics(O) :(O) Horton(B-book) Hatches(I-book) the(I-book) Egg(I-book) .(O)"}}
{"id": "277", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "writer", "award", "book", "event", "poem", "location", "person", "country", "magazine", "literary genre"], "instance": {"id": "277", "words": ["Pauline", "Kael", "of", "The", "New", "Yorker", "wrote", "that", "Kubrick", "has", "taken", "a", "quick-witted", "story", "and", "controlled", "it", "so", "meticulously", "that", "he", "'s", "drained", "the", "blood", "out", "of", "it", ",", "adding", ",", "It", "'s", "a", "coffee-table", "movie", ";", "we", "might", "as", "well", "be", "at", "a", "three-hour", "slide", "show", "for", "art-history", "majors", "."], "labels": ["B-writer", "I-writer", "O", "B-magazine", "I-magazine", "I-magazine", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, writer, award, book, event, poem, location, person, country, magazine, literary genre and O.\nSentence: Pauline Kael of The New Yorker wrote that Kubrick has taken a quick-witted story and controlled it so meticulously that he 's drained the blood out of it , adding , It 's a coffee-table movie ; we might as well be at a three-hour slide show for art-history majors .", "prompt_labels": "Pauline(B-writer) Kael(I-writer) of(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) wrote(O) that(O) Kubrick(B-writer) has(O) taken(O) a(O) quick-witted(O) story(O) and(O) controlled(O) it(O) so(O) meticulously(O) that(O) he(O) 's(O) drained(O) the(O) blood(O) out(O) of(O) it(O) ,(O) adding(O) ,(O) It(O) 's(O) a(O) coffee-table(O) movie(O) ;(O) we(O) might(O) as(O) well(O) be(O) at(O) a(O) three-hour(O) slide(O) show(O) for(O) art-history(O) majors(O) .(O)"}}
{"id": "323", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "organization", "country", "event", "literary genre", "award", "location", "magazine", "writer", "person", "book"], "instance": {"id": "323", "words": ["Controversial", "wars", "in", "Afghanistan", "and", "South", "Africa", "undermined", "his", "public", "support", "."], "labels": ["O", "O", "O", "B-country", "O", "B-country", "I-country", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, organization, country, event, literary genre, award, location, magazine, writer, person, book and O.\nSentence: Controversial wars in Afghanistan and South Africa undermined his public support .", "prompt_labels": "Controversial(O) wars(O) in(O) Afghanistan(B-country) and(O) South(B-country) Africa(I-country) undermined(O) his(O) public(O) support(O) .(O)"}}
{"id": "230", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "award", "organization", "writer", "event", "magazine", "poem", "location", "book", "literary genre", "country"], "instance": {"id": "230", "words": ["He", "was", "a", "high-ranking", "Song", "dynasty", "scholar-official", "and", "historian", "who", "authored", "the", "monumental", "history", "book", "Zizhi", "Tongjian", "."], "labels": ["O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, organization, writer, event, magazine, poem, location, book, literary genre, country and O.\nSentence: He was a high-ranking Song dynasty scholar-official and historian who authored the monumental history book Zizhi Tongjian .", "prompt_labels": "He(O) was(O) a(O) high-ranking(O) Song(B-country) dynasty(I-country) scholar-official(O) and(O) historian(O) who(O) authored(O) the(O) monumental(B-literary genre) history(I-literary genre) book(O) Zizhi(B-book) Tongjian(I-book) .(O)"}}
{"id": "100", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "poem", "book", "award", "organization", "magazine", "location", "person", "country", "literary genre", "writer"], "instance": {"id": "100", "words": ["Marilynne", "Robinson", ",", "On", "Edgar", "Allan", "Poe", ",", "The", "New", "York", "Review", "of", "Books", ",", "vol", "."], "labels": ["B-writer", "I-writer", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, poem, book, award, organization, magazine, location, person, country, literary genre, writer and O.\nSentence: Marilynne Robinson , On Edgar Allan Poe , The New York Review of Books , vol .", "prompt_labels": "Marilynne(B-writer) Robinson(I-writer) ,(O) On(O) Edgar(B-writer) Allan(I-writer) Poe(I-writer) ,(O) The(B-magazine) New(I-magazine) York(I-magazine) Review(I-magazine) of(I-magazine) Books(I-magazine) ,(O) vol(O) .(O)"}}
{"id": "122", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "poem", "literary genre", "organization", "magazine", "writer", "event", "book", "award", "person", "country"], "instance": {"id": "122", "words": ["In", "1929", ",", "Mann", "had", "a", "cottage", "built", "in", "the", "fishing", "village", "of", "Nidden", ",", "Memel", "Territory", "(", "now", "Nida", ",", "Lithuania", ")", "on", "the", "Curonian", "Spit", ",", "where", "there", "was", "a", "German", "art", "colony", "and", "where", "he", "spent", "the", "summers", "of", "1930-1932", "working", "on", "Joseph", "and", "His", "Brothers", "."], "labels": ["O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "I-location", "O", "O", "B-location", "O", "B-country", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, poem, literary genre, organization, magazine, writer, event, book, award, person, country and O.\nSentence: In 1929 , Mann had a cottage built in the fishing village of Nidden , Memel Territory ( now Nida , Lithuania ) on the Curonian Spit , where there was a German art colony and where he spent the summers of 1930-1932 working on Joseph and His Brothers .", "prompt_labels": "In(O) 1929(O) ,(O) Mann(B-writer) had(O) a(O) cottage(O) built(O) in(O) the(O) fishing(O) village(O) of(O) Nidden(B-location) ,(O) Memel(B-location) Territory(I-location) ((O) now(O) Nida(B-location) ,(O) Lithuania(B-country) )(O) on(O) the(O) Curonian(B-location) Spit(I-location) ,(O) where(O) there(O) was(O) a(O) German(O) art(O) colony(O) and(O) where(O) he(O) spent(O) the(O) summers(O) of(O) 1930-1932(O) working(O) on(O) Joseph(B-book) and(I-book) His(I-book) Brothers(I-book) .(O)"}}
{"id": "376", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "literary genre", "country", "poem", "person", "writer", "location", "book", "organization", "magazine", "event"], "instance": {"id": "376", "words": ["It", "was", "joint", "winner", "of", "the", "Nebula", "Award", "for", "Best", "Novel", "in", "1967", "(", "with", "Flowers", "for", "Algernon", ")", "{", "{", "cite", "web"], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, literary genre, country, poem, person, writer, location, book, organization, magazine, event and O.\nSentence: It was joint winner of the Nebula Award for Best Novel in 1967 ( with Flowers for Algernon ) { { cite web", "prompt_labels": "It(O) was(O) joint(O) winner(O) of(O) the(O) Nebula(B-award) Award(I-award) for(I-award) Best(I-award) Novel(I-award) in(O) 1967(O) ((O) with(O) Flowers(B-book) for(I-book) Algernon(I-book) )(O) {(O) {(O) cite(O) web(O)"}}
{"id": "330", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "poem", "location", "literary genre", "person", "event", "book", "award", "country", "writer", "organization"], "instance": {"id": "330", "words": ["Cuarón", "'s", "next", "feature", "was", "also", "a", "literary", "adaptation", ",", "a", "modernized", "version", "of", "Charles", "Dickens", "'", "s", "Great", "Expectations", "starring", "Ethan", "Hawke", ",", "Gwyneth", "Paltrow", "and", "Robert", "De", "Niro", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, poem, location, literary genre, person, event, book, award, country, writer, organization and O.\nSentence: Cuarón 's next feature was also a literary adaptation , a modernized version of Charles Dickens ' s Great Expectations starring Ethan Hawke , Gwyneth Paltrow and Robert De Niro .", "prompt_labels": "Cuarón(B-writer) 's(O) next(O) feature(O) was(O) also(O) a(O) literary(O) adaptation(O) ,(O) a(O) modernized(O) version(O) of(O) Charles(B-writer) Dickens(I-writer) '(O) s(O) Great(B-book) Expectations(I-book) starring(O) Ethan(B-person) Hawke(I-person) ,(O) Gwyneth(B-person) Paltrow(I-person) and(O) Robert(B-person) De(I-person) Niro(I-person) .(O)"}}
{"id": "39", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "writer", "literary genre", "person", "location", "country", "award", "organization", "event", "magazine", "book"], "instance": {"id": "39", "words": ["She", "portrayed", "Nora", "in", "Henrik", "Ibsen", "'", "s", "A", "Doll", "'s", "House", "at", "the", "Donmar", "Warehouse", "in", "London", "'s", "West", "End", "during", "a", "limited", "engagement", "which", "ran", "from", "May", "14", ",", "2009", ",", "until", "July", "18", ",", "2009", "."], "labels": ["O", "O", "B-person", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, writer, literary genre, person, location, country, award, organization, event, magazine, book and O.\nSentence: She portrayed Nora in Henrik Ibsen ' s A Doll 's House at the Donmar Warehouse in London 's West End during a limited engagement which ran from May 14 , 2009 , until July 18 , 2009 .", "prompt_labels": "She(O) portrayed(O) Nora(B-person) in(O) Henrik(B-writer) Ibsen(I-writer) '(O) s(O) A(B-book) Doll(I-book) 's(I-book) House(I-book) at(O) the(O) Donmar(B-location) Warehouse(I-location) in(O) London(B-location) 's(I-location) West(I-location) End(I-location) during(O) a(O) limited(O) engagement(O) which(O) ran(O) from(O) May(O) 14(O) ,(O) 2009(O) ,(O) until(O) July(O) 18(O) ,(O) 2009(O) .(O)"}}
{"id": "45", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "event", "location", "book", "award", "organization", "literary genre", "country", "person", "poem", "magazine"], "instance": {"id": "45", "words": ["In", "the", "second", "quarter", "of", "the", "13th", "century", ",", "a", "version", "in", "Latin", "verse", ",", "the", "Gesta", "Regum", "Britanniae", ",", "was", "produced", "by", "William", "of", "Rennes", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "B-poem", "I-poem", "I-poem", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, event, location, book, award, organization, literary genre, country, person, poem, magazine and O.\nSentence: In the second quarter of the 13th century , a version in Latin verse , the Gesta Regum Britanniae , was produced by William of Rennes .", "prompt_labels": "In(O) the(O) second(O) quarter(O) of(O) the(O) 13th(O) century(O) ,(O) a(O) version(O) in(O) Latin(B-literary genre) verse(I-literary genre) ,(O) the(O) Gesta(B-poem) Regum(I-poem) Britanniae(I-poem) ,(O) was(O) produced(O) by(O) William(B-writer) of(I-writer) Rennes(I-writer) .(O)"}}
{"id": "184", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "country", "poem", "writer", "organization", "location", "award", "literary genre", "person", "book", "magazine"], "instance": {"id": "184", "words": ["The", "details", "of", "their", "narrative", "of", "the", "war", "were", "copied", ",", "for", "example", ",", "in", "the", "Laud", "Troy", "Book", "and", "Lydgate", "Troy", "Books", "and", "also", "in", "Raoul", "Lefevre", "'", "s", "Recuyell", "of", "the", "Historyes", "of", "Troye", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "O", "B-writer", "B-poem", "I-poem", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, poem, writer, organization, location, award, literary genre, person, book, magazine and O.\nSentence: The details of their narrative of the war were copied , for example , in the Laud Troy Book and Lydgate Troy Books and also in Raoul Lefevre ' s Recuyell of the Historyes of Troye .", "prompt_labels": "The(O) details(O) of(O) their(O) narrative(O) of(O) the(O) war(O) were(O) copied(O) ,(O) for(O) example(O) ,(O) in(O) the(O) Laud(B-poem) Troy(I-poem) Book(I-poem) and(O) Lydgate(B-writer) Troy(B-poem) Books(I-poem) and(O) also(O) in(O) Raoul(B-writer) Lefevre(I-writer) '(O) s(O) Recuyell(B-book) of(I-book) the(I-book) Historyes(I-book) of(I-book) Troye(I-book) .(O)"}}
{"id": "304", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "magazine", "literary genre", "poem", "country", "organization", "person", "book", "award", "event", "location"], "instance": {"id": "304", "words": ["He", "was", "commissioned", "by", "Walt", "Disney", "in", "1945", "to", "write", "a", "script", "based", "on", "Alice", "'s", "Adventures", "in", "Wonderland", "and", "the", "biography", "of", "the", "story", "'s", "author", ",", "Lewis", "Carroll", "."], "labels": ["O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, magazine, literary genre, poem, country, organization, person, book, award, event, location and O.\nSentence: He was commissioned by Walt Disney in 1945 to write a script based on Alice 's Adventures in Wonderland and the biography of the story 's author , Lewis Carroll .", "prompt_labels": "He(O) was(O) commissioned(O) by(O) Walt(B-person) Disney(I-person) in(O) 1945(O) to(O) write(O) a(O) script(O) based(O) on(O) Alice(B-book) 's(I-book) Adventures(I-book) in(I-book) Wonderland(I-book) and(O) the(O) biography(B-literary genre) of(O) the(O) story(O) 's(O) author(O) ,(O) Lewis(B-writer) Carroll(I-writer) .(O)"}}
{"id": "203", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "country", "book", "event", "organization", "location", "literary genre", "poem", "award", "writer", "magazine"], "instance": {"id": "203", "words": ["In", "his", "1981", "book", "about", "the", "horror", "genre", ",", "Danse", "Macabre", ",", "Stephen", "King", "reviewed", "Ellison", "'s", "collection", "Strange", "Wine", "and", "considered", "it", "one", "of", "the", "best", "horror", "books", "published", "between", "1950", "and", "1980", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-poem", "I-poem", "O", "B-writer", "I-writer", "O", "B-writer", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, book, event, organization, location, literary genre, poem, award, writer, magazine and O.\nSentence: In his 1981 book about the horror genre , Danse Macabre , Stephen King reviewed Ellison 's collection Strange Wine and considered it one of the best horror books published between 1950 and 1980 .", "prompt_labels": "In(O) his(O) 1981(O) book(O) about(O) the(O) horror(B-literary genre) genre(I-literary genre) ,(O) Danse(B-poem) Macabre(I-poem) ,(O) Stephen(B-writer) King(I-writer) reviewed(O) Ellison(B-writer) 's(O) collection(O) Strange(B-book) Wine(I-book) and(O) considered(O) it(O) one(O) of(O) the(O) best(O) horror(B-literary genre) books(I-literary genre) published(O) between(O) 1950(O) and(O) 1980(O) .(O)"}}
{"id": "12", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "country", "event", "person", "award", "book", "writer", "location", "magazine", "organization", "literary genre"], "instance": {"id": "12", "words": ["Rudyard", "Kipling", "was", "born", "on", "30", "December", "1865", "in", "Bombay", ",", "in", "the", "Bombay", "Presidency", "of", "British", "Raj", ",", "to", "Alice", "Kipling", "(", "née", "MacDonald", ")", "and", "John", "Lockwood", "Kipling", "."], "labels": ["B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "B-location", "I-location", "O", "B-country", "I-country", "O", "O", "B-writer", "I-writer", "I-writer", "I-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, country, event, person, award, book, writer, location, magazine, organization, literary genre and O.\nSentence: Rudyard Kipling was born on 30 December 1865 in Bombay , in the Bombay Presidency of British Raj , to Alice Kipling ( née MacDonald ) and John Lockwood Kipling .", "prompt_labels": "Rudyard(B-writer) Kipling(I-writer) was(O) born(O) on(O) 30(O) December(O) 1865(O) in(O) Bombay(B-location) ,(O) in(O) the(O) Bombay(B-location) Presidency(I-location) of(O) British(B-country) Raj(I-country) ,(O) to(O) Alice(B-writer) Kipling(I-writer) ((I-writer) née(I-writer) MacDonald(I-writer) )(I-writer) and(O) John(B-writer) Lockwood(I-writer) Kipling(I-writer) .(O)"}}
{"id": "222", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "award", "organization", "location", "book", "magazine", "event", "poem", "person", "writer", "literary genre"], "instance": {"id": "222", "words": ["Barry", "Lyndon", "is", "a", "1975", "period", "drama", "film", "written", ",", "directed", "and", "produced", "by", "Stanley", "Kubrick", ",", "based", "on", "the", "1844", "novel", "The", "Luck", "of", "Barry", "Lyndon", "by", "William", "Makepeace", "Thackeray", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-literary genre", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, organization, location, book, magazine, event, poem, person, writer, literary genre and O.\nSentence: Barry Lyndon is a 1975 period drama film written , directed and produced by Stanley Kubrick , based on the 1844 novel The Luck of Barry Lyndon by William Makepeace Thackeray .", "prompt_labels": "Barry(O) Lyndon(O) is(O) a(O) 1975(O) period(O) drama(O) film(O) written(O) ,(O) directed(O) and(O) produced(O) by(O) Stanley(B-writer) Kubrick(I-writer) ,(O) based(O) on(O) the(O) 1844(O) novel(B-literary genre) The(B-book) Luck(I-book) of(I-book) Barry(I-book) Lyndon(I-book) by(O) William(B-writer) Makepeace(I-writer) Thackeray(I-writer) .(O)"}}
{"id": "332", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "book", "award", "country", "literary genre", "organization", "writer", "person", "event", "poem", "magazine"], "instance": {"id": "332", "words": ["The", "contest", "over", "the", "authenticity", "of", "Macpherson", "'s", "pseudo-Gaelic", "productions", ",", "Curley", "asserts", ",", "became", "a", "seismograph", "of", "the", "fragile", "unity", "within", "restive", "diversity", "of", "British", "Empire", "Kingdom", "of", "Great", "Britain", "in", "the", "age", "of", "Samuel", "Johnson", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "B-country", "I-country", "I-country", "I-country", "O", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, book, award, country, literary genre, organization, writer, person, event, poem, magazine and O.\nSentence: The contest over the authenticity of Macpherson 's pseudo-Gaelic productions , Curley asserts , became a seismograph of the fragile unity within restive diversity of British Empire Kingdom of Great Britain in the age of Samuel Johnson .", "prompt_labels": "The(O) contest(O) over(O) the(O) authenticity(O) of(O) Macpherson(B-writer) 's(O) pseudo-Gaelic(O) productions(O) ,(O) Curley(B-person) asserts(O) ,(O) became(O) a(O) seismograph(O) of(O) the(O) fragile(O) unity(O) within(O) restive(O) diversity(O) of(O) British(B-country) Empire(I-country) Kingdom(B-country) of(I-country) Great(I-country) Britain(I-country) in(O) the(O) age(O) of(O) Samuel(B-writer) Johnson(I-writer) .(O)"}}
{"id": "343", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "country", "literary genre", "person", "writer", "poem", "book", "award", "organization", "event", "magazine"], "instance": {"id": "343", "words": ["Love", "subsequently", "attracted", "media", "attention", "in", "May", "1998", "after", "punching", "journalist", "Belissa", "Cohen", "in", "the", "face", "at", "a", "party", ";", "the", "suit", "was", "settled", "out", "of", "court", "for", "an", "undisclosed", "sum.", "and", "were", "subsequently", "nominated", "for", "three", "Grammy", "Award", "s", "at", "the", "41st", "Annual", "Grammy", "Awards", "ceremony", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, literary genre, person, writer, poem, book, award, organization, event, magazine and O.\nSentence: Love subsequently attracted media attention in May 1998 after punching journalist Belissa Cohen in the face at a party ; the suit was settled out of court for an undisclosed sum. and were subsequently nominated for three Grammy Award s at the 41st Annual Grammy Awards ceremony .", "prompt_labels": "Love(B-writer) subsequently(O) attracted(O) media(O) attention(O) in(O) May(O) 1998(O) after(O) punching(O) journalist(O) Belissa(B-writer) Cohen(I-writer) in(O) the(O) face(O) at(O) a(O) party(O) ;(O) the(O) suit(O) was(O) settled(O) out(O) of(O) court(O) for(O) an(O) undisclosed(O) sum.(O) and(O) were(O) subsequently(O) nominated(O) for(O) three(O) Grammy(B-award) Award(I-award) s(O) at(O) the(O) 41st(B-event) Annual(I-event) Grammy(I-event) Awards(I-event) ceremony(I-event) .(O)"}}
{"id": "312", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "writer", "award", "magazine", "book", "location", "poem", "person", "country", "literary genre", "event"], "instance": {"id": "312", "words": ["Pierre", "Teilhard", "de", "Chardin", "was", "born", "in", "the", "Château", "of", "Sarcenat", ",", "Orcines", "commune", ",", "some", "4", "km", "north-west", "of", "Clermont-Ferrand", ",", "Auvergne", ",", "French", "Third", "Republic", ",", "on", "1", "May", "1881", ",", "as", "the", "fourth", "of", "eleven", "children", "of", "librarian", "Emmanuel", "Teilhard", "de", "Chardin", "(", "1844-1932", ")", "and", "of", "Berthe-Adèle", ",", "née", "de", "Dompierre", "d", "'Hornoys", "of", "Picardy", ",", "a", "great-grandniece", "of", "Voltaire", "."], "labels": ["B-person", "I-person", "I-person", "I-person", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "I-person", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "I-person", "I-person", "I-person", "I-person", "I-person", "I-person", "O", "O", "O", "O", "B-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, writer, award, magazine, book, location, poem, person, country, literary genre, event and O.\nSentence: Pierre Teilhard de Chardin was born in the Château of Sarcenat , Orcines commune , some 4 km north-west of Clermont-Ferrand , Auvergne , French Third Republic , on 1 May 1881 , as the fourth of eleven children of librarian Emmanuel Teilhard de Chardin ( 1844-1932 ) and of Berthe-Adèle , née de Dompierre d 'Hornoys of Picardy , a great-grandniece of Voltaire .", "prompt_labels": "Pierre(B-person) Teilhard(I-person) de(I-person) Chardin(I-person) was(O) born(O) in(O) the(O) Château(B-location) of(I-location) Sarcenat(I-location) ,(O) Orcines(B-location) commune(I-location) ,(O) some(O) 4(O) km(O) north-west(O) of(O) Clermont-Ferrand(B-location) ,(O) Auvergne(B-location) ,(O) French(B-country) Third(I-country) Republic(I-country) ,(O) on(O) 1(O) May(O) 1881(O) ,(O) as(O) the(O) fourth(O) of(O) eleven(O) children(O) of(O) librarian(O) Emmanuel(B-person) Teilhard(I-person) de(I-person) Chardin(I-person) ((O) 1844-1932(O) )(O) and(O) of(O) Berthe-Adèle(B-person) ,(I-person) née(I-person) de(I-person) Dompierre(I-person) d(I-person) 'Hornoys(I-person) of(I-person) Picardy(I-person) ,(O) a(O) great-grandniece(O) of(O) Voltaire(B-writer) .(O)"}}
{"id": "213", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "organization", "poem", "book", "award", "event", "writer", "magazine", "literary genre", "country", "location"], "instance": {"id": "213", "words": ["Although", "German", "language", "writers", "such", "as", "Hilde", "Domin", ",", "Luise", "Rinser", "and", "Nelly", "Sachs", "had", "published", "notable", "works", "on", "women", "'s", "issues", "in", "the", "post-war", "period", "it", "was", "only", "in", "the", "1970s", "that", "a", "feminist", "movement", "emerged", "in", "West", "Germany", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, poem, book, award, event, writer, magazine, literary genre, country, location and O.\nSentence: Although German language writers such as Hilde Domin , Luise Rinser and Nelly Sachs had published notable works on women 's issues in the post-war period it was only in the 1970s that a feminist movement emerged in West Germany .", "prompt_labels": "Although(O) German(O) language(O) writers(O) such(O) as(O) Hilde(B-writer) Domin(I-writer) ,(O) Luise(B-writer) Rinser(I-writer) and(O) Nelly(B-writer) Sachs(I-writer) had(O) published(O) notable(O) works(O) on(O) women(O) 's(O) issues(O) in(O) the(O) post-war(O) period(O) it(O) was(O) only(O) in(O) the(O) 1970s(O) that(O) a(O) feminist(O) movement(O) emerged(O) in(O) West(B-country) Germany(I-country) .(O)"}}
{"id": "94", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "event", "magazine", "person", "location", "literary genre", "writer", "organization", "country", "award", "book"], "instance": {"id": "94", "words": ["Polanski", "'s", "next", "film", ",", "An", "Officer", "and", "a", "Spy", ",", "centers", "on", "the", "notorious", "19th", "century", "Dreyfus", "affair", ",", "The", "film", "stars", "Jean", "Dujardin", "as", "French", "officer", "Georges", "Picquart", "and", "follows", "his", "struggle", "from", "1896-1906", "to", "expose", "the", "truth", "about", "the", "doctored", "evidence", "that", "led", "to", "Alfred", "Dreyfus", ",", "one", "of", "the", "few", "Jewish", "members", "of", "the", "French", "Army", "'s", "general", "staff", ",", "being", "wrongly", "convicted", "of", "passing", "military", "secrets", "to", "the", "German", "Empire", "and", "sent", "to", "Devil", "'s", "Island", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, event, magazine, person, location, literary genre, writer, organization, country, award, book and O.\nSentence: Polanski 's next film , An Officer and a Spy , centers on the notorious 19th century Dreyfus affair , The film stars Jean Dujardin as French officer Georges Picquart and follows his struggle from 1896-1906 to expose the truth about the doctored evidence that led to Alfred Dreyfus , one of the few Jewish members of the French Army 's general staff , being wrongly convicted of passing military secrets to the German Empire and sent to Devil 's Island .", "prompt_labels": "Polanski(B-person) 's(O) next(O) film(O) ,(O) An(O) Officer(O) and(O) a(O) Spy(O) ,(O) centers(O) on(O) the(O) notorious(O) 19th(O) century(O) Dreyfus(B-event) affair(I-event) ,(O) The(O) film(O) stars(O) Jean(B-person) Dujardin(I-person) as(O) French(O) officer(O) Georges(B-person) Picquart(I-person) and(O) follows(O) his(O) struggle(O) from(O) 1896-1906(O) to(O) expose(O) the(O) truth(O) about(O) the(O) doctored(O) evidence(O) that(O) led(O) to(O) Alfred(B-person) Dreyfus(I-person) ,(O) one(O) of(O) the(O) few(O) Jewish(O) members(O) of(O) the(O) French(O) Army(O) 's(O) general(O) staff(O) ,(O) being(O) wrongly(O) convicted(O) of(O) passing(O) military(O) secrets(O) to(O) the(O) German(B-country) Empire(I-country) and(O) sent(O) to(O) Devil(B-location) 's(I-location) Island(I-location) .(O)"}}
{"id": "4", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "magazine", "poem", "event", "award", "writer", "literary genre", "person", "organization", "location", "book"], "instance": {"id": "4", "words": ["The", "film", "was", "completed", "in", "1979", "and", "won", "the", "Prize", "of", "the", "Ecumenical", "Jury", "at", "the", "Cannes", "Film", "Festival", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, magazine, poem, event, award, writer, literary genre, person, organization, location, book and O.\nSentence: The film was completed in 1979 and won the Prize of the Ecumenical Jury at the Cannes Film Festival .", "prompt_labels": "The(O) film(O) was(O) completed(O) in(O) 1979(O) and(O) won(O) the(O) Prize(B-award) of(I-award) the(I-award) Ecumenical(I-award) Jury(I-award) at(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) .(O)"}}
{"id": "103", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "organization", "poem", "book", "country", "person", "event", "literary genre", "location", "writer", "award"], "instance": {"id": "103", "words": ["Xuanxue", "was", "a", "philosophical", "school", "that", "combined", "elements", "of", "Confucianism", "and", "Taoism", "to", "reinterpret", "the", "I", "Ching", ",", "Tao", "Te", "Ching", ",", "and", "Zhuangzi", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "B-book", "I-book", "I-book", "O", "O", "B-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, organization, poem, book, country, person, event, literary genre, location, writer, award and O.\nSentence: Xuanxue was a philosophical school that combined elements of Confucianism and Taoism to reinterpret the I Ching , Tao Te Ching , and Zhuangzi .", "prompt_labels": "Xuanxue(O) was(O) a(O) philosophical(O) school(O) that(O) combined(O) elements(O) of(O) Confucianism(O) and(O) Taoism(O) to(O) reinterpret(O) the(O) I(B-book) Ching(I-book) ,(O) Tao(B-book) Te(I-book) Ching(I-book) ,(O) and(O) Zhuangzi(B-book) .(O)"}}
{"id": "111", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "poem", "book", "person", "literary genre", "writer", "country", "award", "event", "organization", "magazine"], "instance": {"id": "111", "words": ["Landing", "in", "Boston", ",", "he", "devoted", "the", "rest", "of", "the", "month", "to", "a", "round", "of", "dinners", "with", "such", "notables", "as", "Ralph", "Waldo", "Emerson", ",", "Henry", "Wadsworth", "Longfellow", ",", "and", "his", "American", "publisher", ",", "James", "Thomas", "Fields", "."], "labels": ["O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, poem, book, person, literary genre, writer, country, award, event, organization, magazine and O.\nSentence: Landing in Boston , he devoted the rest of the month to a round of dinners with such notables as Ralph Waldo Emerson , Henry Wadsworth Longfellow , and his American publisher , James Thomas Fields .", "prompt_labels": "Landing(O) in(O) Boston(B-location) ,(O) he(O) devoted(O) the(O) rest(O) of(O) the(O) month(O) to(O) a(O) round(O) of(O) dinners(O) with(O) such(O) notables(O) as(O) Ralph(B-writer) Waldo(I-writer) Emerson(I-writer) ,(O) Henry(B-writer) Wadsworth(I-writer) Longfellow(I-writer) ,(O) and(O) his(O) American(O) publisher(O) ,(O) James(B-writer) Thomas(I-writer) Fields(I-writer) .(O)"}}
{"id": "109", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "organization", "book", "person", "literary genre", "country", "event", "writer", "magazine", "location", "award"], "instance": {"id": "109", "words": ["Media", "coverage", "of", "Lewis", "'s", "death", "was", "almost", "completely", "overshadowed", "by", "news", "of", "the", "assassination", "of", "US", "President", "John", "F.", "Kennedy", ",", "which", "occurred", "on", "the", "same", "day", "(", "approximately", "55", "minutes", "following", "Lewis", "'s", "collapse", ")", ",", "as", "did", "the", "death", "of", "English", "writer", "Aldous", "Huxley", ",", "author", "of", "Brave", "New", "World", "."], "labels": ["O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "B-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, organization, book, person, literary genre, country, event, writer, magazine, location, award and O.\nSentence: Media coverage of Lewis 's death was almost completely overshadowed by news of the assassination of US President John F. Kennedy , which occurred on the same day ( approximately 55 minutes following Lewis 's collapse ) , as did the death of English writer Aldous Huxley , author of Brave New World .", "prompt_labels": "Media(O) coverage(O) of(O) Lewis(B-writer) 's(O) death(O) was(O) almost(O) completely(O) overshadowed(O) by(O) news(O) of(O) the(O) assassination(O) of(O) US(B-country) President(O) John(B-person) F.(I-person) Kennedy(I-person) ,(O) which(O) occurred(O) on(O) the(O) same(O) day(O) ((O) approximately(O) 55(O) minutes(O) following(O) Lewis(B-writer) 's(O) collapse(O) )(O) ,(O) as(O) did(O) the(O) death(O) of(O) English(O) writer(O) Aldous(B-writer) Huxley(I-writer) ,(O) author(O) of(O) Brave(B-book) New(I-book) World(I-book) .(O)"}}
{"id": "327", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "poem", "country", "event", "writer", "award", "person", "book", "magazine", "organization", "location"], "instance": {"id": "327", "words": ["The", "title", "is", "taken", "from", "his", "poem", "The", "Negro", "Speaks", "of", "Rivers", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-literary genre", "B-poem", "I-poem", "I-poem", "I-poem", "I-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, poem, country, event, writer, award, person, book, magazine, organization, location and O.\nSentence: The title is taken from his poem The Negro Speaks of Rivers .", "prompt_labels": "The(O) title(O) is(O) taken(O) from(O) his(O) poem(B-literary genre) The(B-poem) Negro(I-poem) Speaks(I-poem) of(I-poem) Rivers(I-poem) .(O)"}}
{"id": "302", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "writer", "poem", "person", "location", "country", "literary genre", "organization", "magazine", "event", "award"], "instance": {"id": "302", "words": ["Naguib", "Mahfouz", "(", "1911", "&", "ndash", ";", "2006", ")", "was", "an", "Egyptian", "writer", "who", "won", "the", "1988", "Nobel", "Prize", "for", "Literature", "."], "labels": ["B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, writer, poem, person, location, country, literary genre, organization, magazine, event, award and O.\nSentence: Naguib Mahfouz ( 1911 & ndash ; 2006 ) was an Egyptian writer who won the 1988 Nobel Prize for Literature .", "prompt_labels": "Naguib(B-writer) Mahfouz(I-writer) ((O) 1911(O) &(O) ndash(O) ;(O) 2006(O) )(O) was(O) an(O) Egyptian(O) writer(O) who(O) won(O) the(O) 1988(O) Nobel(B-award) Prize(I-award) for(I-award) Literature(I-award) .(O)"}}
{"id": "47", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "writer", "literary genre", "location", "organization", "book", "event", "magazine", "country", "poem", "person"], "instance": {"id": "47", "words": ["The", "character", "of", "Conan", "has", "proven", "durably", "popular", ",", "resulting", "in", "Conan", "stories", "by", "later", "writers", "such", "as", "Poul", "Anderson", ",", "Leonard", "Carpenter", ",", "Lin", "Carter", ",", "L.", "Sprague", "de", "Camp", ",", "Roland", "J.", "Green", ",", "John", "C.", "Hocking", ",", "Robert", "Jordan", ",", "Sean", "A.", "Moore", ",", "Björn", "Nyberg", ",", "Andrew", "J.", "Offutt", ",", "Steve", "Perry", ",", "John", "Maddox", "Roberts", ",", "Harry", "Turtledove", ",", "and", "Karl", "Edward", "Wagner", "."], "labels": ["O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, writer, literary genre, location, organization, book, event, magazine, country, poem, person and O.\nSentence: The character of Conan has proven durably popular , resulting in Conan stories by later writers such as Poul Anderson , Leonard Carpenter , Lin Carter , L. Sprague de Camp , Roland J. Green , John C. Hocking , Robert Jordan , Sean A. Moore , Björn Nyberg , Andrew J. Offutt , Steve Perry , John Maddox Roberts , Harry Turtledove , and Karl Edward Wagner .", "prompt_labels": "The(O) character(O) of(O) Conan(B-person) has(O) proven(O) durably(O) popular(O) ,(O) resulting(O) in(O) Conan(B-person) stories(O) by(O) later(O) writers(O) such(O) as(O) Poul(B-writer) Anderson(I-writer) ,(O) Leonard(B-writer) Carpenter(I-writer) ,(O) Lin(B-writer) Carter(I-writer) ,(O) L.(B-writer) Sprague(I-writer) de(I-writer) Camp(I-writer) ,(O) Roland(B-writer) J.(I-writer) Green(I-writer) ,(O) John(B-writer) C.(I-writer) Hocking(I-writer) ,(O) Robert(B-writer) Jordan(I-writer) ,(O) Sean(B-writer) A.(I-writer) Moore(I-writer) ,(O) Björn(B-writer) Nyberg(I-writer) ,(O) Andrew(B-writer) J.(I-writer) Offutt(I-writer) ,(O) Steve(B-writer) Perry(I-writer) ,(O) John(B-writer) Maddox(I-writer) Roberts(I-writer) ,(O) Harry(B-writer) Turtledove(I-writer) ,(O) and(O) Karl(B-writer) Edward(I-writer) Wagner(I-writer) .(O)"}}
{"id": "276", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "event", "writer", "award", "country", "organization", "literary genre", "poem", "book", "magazine", "location"], "instance": {"id": "276", "words": ["A", "documentary", "was", "also", "made", "for", "BBC", "Radio", "4", "entitled", "The", "Story", "of", "O", ":", "The", "Vice", "Francaise", ",", "presented", "by", "Rowan", "Pelling", ",", "former", "editor", "of", "the", "Erotic", "Review", ",", "which", "looked", "at", "the", "history", "of", "the", "book", "and", "its", "author", "Anne", "Desclos", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, writer, award, country, organization, literary genre, poem, book, magazine, location and O.\nSentence: A documentary was also made for BBC Radio 4 entitled The Story of O : The Vice Francaise , presented by Rowan Pelling , former editor of the Erotic Review , which looked at the history of the book and its author Anne Desclos .", "prompt_labels": "A(O) documentary(O) was(O) also(O) made(O) for(O) BBC(B-organization) Radio(I-organization) 4(I-organization) entitled(O) The(O) Story(O) of(O) O(O) :(O) The(O) Vice(O) Francaise(O) ,(O) presented(O) by(O) Rowan(B-writer) Pelling(I-writer) ,(O) former(O) editor(O) of(O) the(O) Erotic(B-magazine) Review(I-magazine) ,(O) which(O) looked(O) at(O) the(O) history(O) of(O) the(O) book(O) and(O) its(O) author(O) Anne(B-writer) Desclos(I-writer) .(O)"}}
{"id": "373", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "country", "writer", "award", "organization", "poem", "literary genre", "location", "book", "person", "event"], "instance": {"id": "373", "words": ["His", "fictional", "work", ",", "Redburn", "(", "1849", ")", ",", "and", "his", "non-fiction", "White-Jacket", "(", "1850", ")", "were", "given", "better", "reviews", "but", "did", "not", "provide", "financial", "security", "."], "labels": ["O", "B-literary genre", "I-literary genre", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, country, writer, award, organization, poem, literary genre, location, book, person, event and O.\nSentence: His fictional work , Redburn ( 1849 ) , and his non-fiction White-Jacket ( 1850 ) were given better reviews but did not provide financial security .", "prompt_labels": "His(O) fictional(B-literary genre) work(I-literary genre) ,(O) Redburn(B-book) ((O) 1849(O) )(O) ,(O) and(O) his(O) non-fiction(O) White-Jacket(B-book) ((O) 1850(O) )(O) were(O) given(O) better(O) reviews(O) but(O) did(O) not(O) provide(O) financial(O) security(O) .(O)"}}
{"id": "374", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "event", "poem", "person", "literary genre", "award", "book", "country", "writer", "magazine", "location"], "instance": {"id": "374", "words": ["Adaptations", "of", "the", "novel", "include", "François", "Truffaut", "'", "s", "1966", "film", "adaptation", "and", "a", "1982", "BBC", "Radio", "dramatization.Nov", "13", "1982", "Fahrenheit", "451", ",", "BBC", "Radio", "4", "Bradbury", "published", "a", "stage", "play", "version", "in", "1979", "and", "helped", "develop", "a", "1984", "interactive", "fiction", "computer", "game", "titled", "Fahrenheit", "451", ",", "as", "well", "as", "a", "collection", "of", "his", "short", "stories", "titled", "A", "Pleasure", "to", "Burn", "."], "labels": ["O", "O", "O", "B-literary genre", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "B-book", "I-book", "O", "B-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-book", "I-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, poem, person, literary genre, award, book, country, writer, magazine, location and O.\nSentence: Adaptations of the novel include François Truffaut ' s 1966 film adaptation and a 1982 BBC Radio dramatization.Nov 13 1982 Fahrenheit 451 , BBC Radio 4 Bradbury published a stage play version in 1979 and helped develop a 1984 interactive fiction computer game titled Fahrenheit 451 , as well as a collection of his short stories titled A Pleasure to Burn .", "prompt_labels": "Adaptations(O) of(O) the(O) novel(B-literary genre) include(O) François(B-writer) Truffaut(I-writer) '(O) s(O) 1966(O) film(O) adaptation(O) and(O) a(O) 1982(O) BBC(B-organization) Radio(I-organization) dramatization.Nov(O) 13(O) 1982(O) Fahrenheit(B-book) 451(I-book) ,(O) BBC(B-organization) Radio(I-organization) 4(O) Bradbury(B-organization) published(O) a(O) stage(O) play(O) version(O) in(O) 1979(O) and(O) helped(O) develop(O) a(O) 1984(O) interactive(B-literary genre) fiction(I-literary genre) computer(O) game(O) titled(O) Fahrenheit(O) 451(O) ,(O) as(O) well(O) as(O) a(O) collection(O) of(O) his(O) short(B-literary genre) stories(I-literary genre) titled(O) A(B-book) Pleasure(I-book) to(I-book) Burn(I-book) .(O)"}}
{"id": "183", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "book", "organization", "country", "writer", "event", "award", "poem", "literary genre", "person", "location"], "instance": {"id": "183", "words": ["It", "was", "followed", "the", "next", "year", "by", "The", "Tale", "of", "Squirrel", "Nutkin", "and", "The", "Tailor", "of", "Gloucester", ",", "which", "had", "also", "first", "been", "written", "as", "picture", "letters", "to", "the", "Moore", "children", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, book, organization, country, writer, event, award, poem, literary genre, person, location and O.\nSentence: It was followed the next year by The Tale of Squirrel Nutkin and The Tailor of Gloucester , which had also first been written as picture letters to the Moore children .", "prompt_labels": "It(O) was(O) followed(O) the(O) next(O) year(O) by(O) The(B-book) Tale(I-book) of(I-book) Squirrel(I-book) Nutkin(I-book) and(O) The(B-book) Tailor(I-book) of(I-book) Gloucester(I-book) ,(O) which(O) had(O) also(O) first(O) been(O) written(O) as(O) picture(O) letters(O) to(O) the(O) Moore(O) children(O) .(O)"}}
{"id": "384", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "organization", "country", "literary genre", "poem", "location", "person", "event", "writer", "award", "book"], "instance": {"id": "384", "words": ["This", "period", "also", "saw", "alternate", "history", "works", "by", "S.", "M.", "Stirling", ",", "Kim", "Stanley", "Robinson", ",", "Harry", "Harrison", ",", "Howard", "Waldrop", ",", "and", "others", "."], "labels": ["O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, organization, country, literary genre, poem, location, person, event, writer, award, book and O.\nSentence: This period also saw alternate history works by S. M. Stirling , Kim Stanley Robinson , Harry Harrison , Howard Waldrop , and others .", "prompt_labels": "This(O) period(O) also(O) saw(O) alternate(B-literary genre) history(I-literary genre) works(O) by(O) S.(B-writer) M.(I-writer) Stirling(I-writer) ,(O) Kim(B-writer) Stanley(I-writer) Robinson(I-writer) ,(O) Harry(B-writer) Harrison(I-writer) ,(O) Howard(B-writer) Waldrop(I-writer) ,(O) and(O) others(O) .(O)"}}
{"id": "145", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "magazine", "organization", "event", "person", "writer", "award", "poem", "book", "country", "location"], "instance": {"id": "145", "words": ["During", "this", "period", ",", "Stoker", "was", "part", "of", "the", "literary", "staff", "of", "The", "Daily", "Telegraph", "in", "London", ",", "and", "he", "wrote", "other", "fiction", ",", "including", "the", "horror", "novels", "The", "Lady", "of", "the", "Shroud", "(", "1909", ")", "and", "The", "Lair", "of", "the", "White", "Worm", "(", "1911", ")", "."], "labels": ["O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, magazine, organization, event, person, writer, award, poem, book, country, location and O.\nSentence: During this period , Stoker was part of the literary staff of The Daily Telegraph in London , and he wrote other fiction , including the horror novels The Lady of the Shroud ( 1909 ) and The Lair of the White Worm ( 1911 ) .", "prompt_labels": "During(O) this(O) period(O) ,(O) Stoker(B-writer) was(O) part(O) of(O) the(O) literary(O) staff(O) of(O) The(B-organization) Daily(I-organization) Telegraph(I-organization) in(O) London(B-location) ,(O) and(O) he(O) wrote(O) other(O) fiction(O) ,(O) including(O) the(O) horror(B-literary genre) novels(I-literary genre) The(B-book) Lady(I-book) of(I-book) the(I-book) Shroud(I-book) ((O) 1909(O) )(O) and(O) The(B-book) Lair(I-book) of(I-book) the(I-book) White(I-book) Worm(I-book) ((O) 1911(O) )(O) .(O)"}}
{"id": "104", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "magazine", "writer", "event", "person", "poem", "organization", "book", "country", "literary genre", "award"], "instance": {"id": "104", "words": ["Unable", "to", "pay", "independently", "to", "return", "for", "the", "appeal", "and", "second", "trial", ",", "Hurston", "contacted", "journalist", "William", "Bradford", "Huie", ",", "with", "whom", "she", "had", "worked", "at", "The", "American", "Mercury", ",", "to", "try", "to", "interest", "him", "in", "the", "case", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, magazine, writer, event, person, poem, organization, book, country, literary genre, award and O.\nSentence: Unable to pay independently to return for the appeal and second trial , Hurston contacted journalist William Bradford Huie , with whom she had worked at The American Mercury , to try to interest him in the case .", "prompt_labels": "Unable(O) to(O) pay(O) independently(O) to(O) return(O) for(O) the(O) appeal(O) and(O) second(O) trial(O) ,(O) Hurston(B-writer) contacted(O) journalist(O) William(B-writer) Bradford(I-writer) Huie(I-writer) ,(O) with(O) whom(O) she(O) had(O) worked(O) at(O) The(B-magazine) American(I-magazine) Mercury(I-magazine) ,(O) to(O) try(O) to(O) interest(O) him(O) in(O) the(O) case(O) .(O)"}}
{"id": "105", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "event", "person", "book", "writer", "award", "magazine", "country", "location", "poem", "literary genre"], "instance": {"id": "105", "words": ["The", "other", "main", "course", "in", "Smith", "'s", "self-education", "was", "to", "read", "the", "complete", "Encyclopædia", "Britannica", "Eleventh", "Edition", "of", "the", "Encyclopædia", "Britannica", "at", "least", "twice", "."], "labels": ["O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "B-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, person, book, writer, award, magazine, country, location, poem, literary genre and O.\nSentence: The other main course in Smith 's self-education was to read the complete Encyclopædia Britannica Eleventh Edition of the Encyclopædia Britannica at least twice .", "prompt_labels": "The(O) other(O) main(O) course(O) in(O) Smith(B-writer) 's(O) self-education(O) was(O) to(O) read(O) the(O) complete(O) Encyclopædia(B-book) Britannica(I-book) Eleventh(I-book) Edition(I-book) of(O) the(O) Encyclopædia(B-book) Britannica(I-book) at(O) least(O) twice(O) .(O)"}}
{"id": "290", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "award", "poem", "country", "book", "location", "writer", "person", "event", "magazine", "literary genre"], "instance": {"id": "290", "words": ["Although", "Friedman", "never", "visited", "Estonia", ",", "his", "book", "Free", "to", "Choose", "exercised", "a", "great", "influence", "on", "that", "nation", "'s", "then", "32-year-old", "prime", "minister", ",", "Mart", "Laar", ",", "who", "has", "claimed", "that", "it", "was", "the", "only", "book", "on", "economics", "he", "had", "read", "before", "taking", "office", "."], "labels": ["O", "B-writer", "O", "O", "B-country", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, poem, country, book, location, writer, person, event, magazine, literary genre and O.\nSentence: Although Friedman never visited Estonia , his book Free to Choose exercised a great influence on that nation 's then 32-year-old prime minister , Mart Laar , who has claimed that it was the only book on economics he had read before taking office .", "prompt_labels": "Although(O) Friedman(B-writer) never(O) visited(O) Estonia(B-country) ,(O) his(O) book(O) Free(B-book) to(I-book) Choose(I-book) exercised(O) a(O) great(O) influence(O) on(O) that(O) nation(O) 's(O) then(O) 32-year-old(O) prime(O) minister(O) ,(O) Mart(B-person) Laar(I-person) ,(O) who(O) has(O) claimed(O) that(O) it(O) was(O) the(O) only(O) book(O) on(O) economics(O) he(O) had(O) read(O) before(O) taking(O) office(O) .(O)"}}
{"id": "190", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "country", "literary genre", "person", "organization", "location", "poem", "award", "writer", "magazine", "book"], "instance": {"id": "190", "words": ["In", "1550", ",", "one", "year", "after", "Marguerite", "'s", "death", ",", "a", "tributary", "poem", ",", "Hecatodistichon", ",", "was", "published", "in", "England", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-poem", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, literary genre, person, organization, location, poem, award, writer, magazine, book and O.\nSentence: In 1550 , one year after Marguerite 's death , a tributary poem , Hecatodistichon , was published in England .", "prompt_labels": "In(O) 1550(O) ,(O) one(O) year(O) after(O) Marguerite(B-writer) 's(O) death(O) ,(O) a(O) tributary(B-literary genre) poem(I-literary genre) ,(O) Hecatodistichon(B-poem) ,(O) was(O) published(O) in(O) England(B-country) .(O)"}}
{"id": "86", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "magazine", "country", "writer", "location", "poem", "award", "person", "literary genre", "event", "organization"], "instance": {"id": "86", "words": ["After", "some", "unsuccessful", "films", ",", "she", "had", "her", "critical", "breakthrough", "playing", "a", "vulgar", "waitress", "in", "Of", "Human", "Bondage", "(", "1934", ")", ",", "although", ",", "contentiously", ",", "she", "was", "not", "among", "the", "three", "nominees", "for", "the", "Academy", "Award", "for", "Best", "Actress", "that", "year", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, magazine, country, writer, location, poem, award, person, literary genre, event, organization and O.\nSentence: After some unsuccessful films , she had her critical breakthrough playing a vulgar waitress in Of Human Bondage ( 1934 ) , although , contentiously , she was not among the three nominees for the Academy Award for Best Actress that year .", "prompt_labels": "After(O) some(O) unsuccessful(O) films(O) ,(O) she(O) had(O) her(O) critical(O) breakthrough(O) playing(O) a(O) vulgar(O) waitress(O) in(O) Of(B-book) Human(I-book) Bondage(I-book) ((O) 1934(O) )(O) ,(O) although(O) ,(O) contentiously(O) ,(O) she(O) was(O) not(O) among(O) the(O) three(O) nominees(O) for(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) that(O) year(O) .(O)"}}
{"id": "3", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "country", "magazine", "poem", "organization", "literary genre", "person", "writer", "event", "award", "location"], "instance": {"id": "3", "words": ["Stoker", "'s", "inspirations", "for", "the", "story", ",", "in", "addition", "to", "Whitby", ",", "may", "have", "included", "a", "visit", "to", "Slains", "Castle", "in", "Aberdeenshire", ",", "a", "visit", "to", "the", "crypts", "of", "St.", "Michan", "'s", "Church", "in", "Dublin", ",", "and", "the", "novella", "Carmilla", "by", "Sheridan", "Le", "Fanu", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "B-location", "O", "O", "O", "B-literary genre", "B-book", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, country, magazine, poem, organization, literary genre, person, writer, event, award, location and O.\nSentence: Stoker 's inspirations for the story , in addition to Whitby , may have included a visit to Slains Castle in Aberdeenshire , a visit to the crypts of St. Michan 's Church in Dublin , and the novella Carmilla by Sheridan Le Fanu .", "prompt_labels": "Stoker(B-writer) 's(O) inspirations(O) for(O) the(O) story(O) ,(O) in(O) addition(O) to(O) Whitby(B-location) ,(O) may(O) have(O) included(O) a(O) visit(O) to(O) Slains(B-location) Castle(I-location) in(O) Aberdeenshire(B-location) ,(O) a(O) visit(O) to(O) the(O) crypts(O) of(O) St.(B-location) Michan(I-location) 's(I-location) Church(I-location) in(O) Dublin(B-location) ,(O) and(O) the(O) novella(B-literary genre) Carmilla(B-book) by(O) Sheridan(B-writer) Le(I-writer) Fanu(I-writer) .(O)"}}
{"id": "89", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "organization", "location", "country", "literary genre", "book", "magazine", "poem", "event", "person", "writer"], "instance": {"id": "89", "words": ["His", "name", "had", "sometimes", "been", "mentioned", "as", "a", "contender", "for", "the", "Nobel", "Prize", "in", "Literature", ",", "but", "in", "1971", ",", "after", "losing", "to", "Aleksandr", "Solzhenitsyn", ",", "he", "wrote", "to", "a", "friend", ":", "That", "Nobel", "Prize", "!", "I", "hope", "I", "never", "hear", "it", "mentioned", "again", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, location, country, literary genre, book, magazine, poem, event, person, writer and O.\nSentence: His name had sometimes been mentioned as a contender for the Nobel Prize in Literature , but in 1971 , after losing to Aleksandr Solzhenitsyn , he wrote to a friend : That Nobel Prize ! I hope I never hear it mentioned again .", "prompt_labels": "His(O) name(O) had(O) sometimes(O) been(O) mentioned(O) as(O) a(O) contender(O) for(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) ,(O) but(O) in(O) 1971(O) ,(O) after(O) losing(O) to(O) Aleksandr(B-writer) Solzhenitsyn(I-writer) ,(O) he(O) wrote(O) to(O) a(O) friend(O) :(O) That(O) Nobel(B-award) Prize(I-award) !(O) I(O) hope(O) I(O) never(O) hear(O) it(O) mentioned(O) again(O) .(O)"}}
{"id": "112", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "event", "book", "organization", "magazine", "literary genre", "poem", "writer", "award", "country", "location"], "instance": {"id": "112", "words": ["She", "visited", "Hill", "Top", "at", "every", "opportunity", ",", "and", "her", "books", "written", "during", "this", "period", "(", "such", "as", "The", "Tale", "of", "Ginger", "and", "Pickles", ",", "about", "the", "local", "shop", "in", "Near", "Sawrey", "and", "The", "Tale", "of", "Mrs.", "Tittlemouse", ",", "a", "wood", "mouse", ")", "reflect", "her", "increasing", "participation", "in", "village", "life", "and", "her", "delight", "in", "country", "living.Taylor", ",", "ed", "."], "labels": ["O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, book, organization, magazine, literary genre, poem, writer, award, country, location and O.\nSentence: She visited Hill Top at every opportunity , and her books written during this period ( such as The Tale of Ginger and Pickles , about the local shop in Near Sawrey and The Tale of Mrs. Tittlemouse , a wood mouse ) reflect her increasing participation in village life and her delight in country living.Taylor , ed .", "prompt_labels": "She(O) visited(O) Hill(B-location) Top(I-location) at(O) every(O) opportunity(O) ,(O) and(O) her(O) books(O) written(O) during(O) this(O) period(O) ((O) such(O) as(O) The(B-book) Tale(I-book) of(I-book) Ginger(I-book) and(I-book) Pickles(I-book) ,(O) about(O) the(O) local(O) shop(O) in(O) Near(B-location) Sawrey(I-location) and(O) The(B-book) Tale(I-book) of(I-book) Mrs.(I-book) Tittlemouse(I-book) ,(O) a(O) wood(O) mouse(O) )(O) reflect(O) her(O) increasing(O) participation(O) in(O) village(O) life(O) and(O) her(O) delight(O) in(O) country(O) living.Taylor(O) ,(O) ed(O) .(O)"}}
{"id": "221", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "organization", "writer", "country", "poem", "literary genre", "magazine", "location", "award", "book", "event"], "instance": {"id": "221", "words": ["His", "best", "known", "works", "include", "the", "Our", "Ancestors", "trilogy", "(", "1952-1959", ")", ",", "the", "Cosmicomics", "collection", "of", "short", "stories", "(", "1965", ")", ",", "and", "the", "novels", "Invisible", "Cities", "(", "1972", ")", "and", "If", "on", "a", "winter", "'s", "night", "a", "traveler", "(", "1979", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "B-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, writer, country, poem, literary genre, magazine, location, award, book, event and O.\nSentence: His best known works include the Our Ancestors trilogy ( 1952-1959 ) , the Cosmicomics collection of short stories ( 1965 ) , and the novels Invisible Cities ( 1972 ) and If on a winter 's night a traveler ( 1979 ) .", "prompt_labels": "His(O) best(O) known(O) works(O) include(O) the(O) Our(B-book) Ancestors(I-book) trilogy(O) ((O) 1952-1959(O) )(O) ,(O) the(O) Cosmicomics(B-book) collection(O) of(O) short(O) stories(O) ((O) 1965(O) )(O) ,(O) and(O) the(O) novels(B-literary genre) Invisible(B-book) Cities(I-book) ((O) 1972(O) )(O) and(O) If(B-book) on(I-book) a(I-book) winter(I-book) 's(I-book) night(I-book) a(I-book) traveler(I-book) ((O) 1979(O) )(O) .(O)"}}
{"id": "193", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "poem", "organization", "book", "award", "magazine", "country", "location", "event", "writer", "literary genre"], "instance": {"id": "193", "words": ["Two", "years", "later", ",", "Wilder", "earned", "the", "Academy", "Award", "for", "Best", "Director", "and", "Academy", "Award", "for", "Best", "Adapted", "Screenplay", "Academy", "Awards", "for", "the", "adaptation", "of", "a", "Charles", "R.", "Jackson", "story", ",", "The", "Lost", "Weekend", "(", "1945", ")", ",", "the", "first", "major", "American", "film", "to", "make", "a", "serious", "examination", "of", "alcoholism", ",", "another", "difficult", "theme", "under", "the", "Production", "Code", "."], "labels": ["O", "O", "O", "O", "B-writer", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "B-award", "I-award", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, poem, organization, book, award, magazine, country, location, event, writer, literary genre and O.\nSentence: Two years later , Wilder earned the Academy Award for Best Director and Academy Award for Best Adapted Screenplay Academy Awards for the adaptation of a Charles R. Jackson story , The Lost Weekend ( 1945 ) , the first major American film to make a serious examination of alcoholism , another difficult theme under the Production Code .", "prompt_labels": "Two(O) years(O) later(O) ,(O) Wilder(B-writer) earned(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) Academy(B-award) Awards(I-award) for(O) the(O) adaptation(O) of(O) a(O) Charles(B-writer) R.(I-writer) Jackson(I-writer) story(O) ,(O) The(B-book) Lost(I-book) Weekend(I-book) ((O) 1945(O) )(O) ,(O) the(O) first(O) major(O) American(O) film(O) to(O) make(O) a(O) serious(O) examination(O) of(O) alcoholism(O) ,(O) another(O) difficult(O) theme(O) under(O) the(O) Production(O) Code(O) .(O)"}}
{"id": "348", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "country", "literary genre", "magazine", "organization", "award", "writer", "event", "poem", "person", "book"], "instance": {"id": "348", "words": ["See", "also", "The", "Snow", "Man", "and", "Gubbinal", "for", "related", "experiments", "in", "perspective", "."], "labels": ["O", "O", "B-poem", "I-poem", "I-poem", "O", "B-poem", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, literary genre, magazine, organization, award, writer, event, poem, person, book and O.\nSentence: See also The Snow Man and Gubbinal for related experiments in perspective .", "prompt_labels": "See(O) also(O) The(B-poem) Snow(I-poem) Man(I-poem) and(O) Gubbinal(B-poem) for(O) related(O) experiments(O) in(O) perspective(O) .(O)"}}
{"id": "1", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "literary genre", "award", "poem", "location", "event", "book", "writer", "magazine", "organization", "person"], "instance": {"id": "1", "words": ["February", "2015", "saw", "Ellis", "release", "his", "first", "album", "as", "Zarelli", ",", "Soft", "Rains", "on", "the", "Seriés", "Aphōnos", "label", "-", "a", "mostly", "instrumental", "synthesizer", "record", ",", "featuring", "the", "voice", "of", "Leonard", "Nimoy", "reading", "the", "Ray", "Bradbury", "short", "story", "There", "Will", "Come", "Soft", "Rains", "."], "labels": ["O", "O", "O", "B-person", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "B-writer", "I-writer", "B-literary genre", "I-literary genre", "B-poem", "I-poem", "I-poem", "I-poem", "I-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, literary genre, award, poem, location, event, book, writer, magazine, organization, person and O.\nSentence: February 2015 saw Ellis release his first album as Zarelli , Soft Rains on the Seriés Aphōnos label - a mostly instrumental synthesizer record , featuring the voice of Leonard Nimoy reading the Ray Bradbury short story There Will Come Soft Rains .", "prompt_labels": "February(O) 2015(O) saw(O) Ellis(B-person) release(O) his(O) first(O) album(O) as(O) Zarelli(B-person) ,(O) Soft(O) Rains(O) on(O) the(O) Seriés(B-organization) Aphōnos(I-organization) label(I-organization) -(O) a(O) mostly(O) instrumental(O) synthesizer(O) record(O) ,(O) featuring(O) the(O) voice(O) of(O) Leonard(B-person) Nimoy(I-person) reading(O) the(O) Ray(B-writer) Bradbury(I-writer) short(B-literary genre) story(I-literary genre) There(B-poem) Will(I-poem) Come(I-poem) Soft(I-poem) Rains(I-poem) .(O)"}}
{"id": "286", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "organization", "writer", "event", "award", "location", "magazine", "literary genre", "book", "person", "country"], "instance": {"id": "286", "words": ["It", "was", "released", "in", "hardback", "by", "Angus", "and", "Robertson", "in", "1895", ",", "and", "features", "the", "poet", "'s", "widely", "anthologised", "poems", "The", "Man", "from", "Snowy", "River", ",", "Clancy", "of", "the", "Overflow", ",", "Saltbush", "Bill", "and", "The", "Man", "from", "Ironbark", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "B-poem", "I-poem", "I-poem", "I-poem", "I-poem", "O", "B-poem", "I-poem", "I-poem", "I-poem", "O", "B-poem", "I-poem", "O", "B-poem", "I-poem", "I-poem", "I-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, organization, writer, event, award, location, magazine, literary genre, book, person, country and O.\nSentence: It was released in hardback by Angus and Robertson in 1895 , and features the poet 's widely anthologised poems The Man from Snowy River , Clancy of the Overflow , Saltbush Bill and The Man from Ironbark .", "prompt_labels": "It(O) was(O) released(O) in(O) hardback(O) by(O) Angus(B-organization) and(I-organization) Robertson(I-organization) in(O) 1895(O) ,(O) and(O) features(O) the(O) poet(O) 's(O) widely(O) anthologised(O) poems(B-literary genre) The(B-poem) Man(I-poem) from(I-poem) Snowy(I-poem) River(I-poem) ,(O) Clancy(B-poem) of(I-poem) the(I-poem) Overflow(I-poem) ,(O) Saltbush(B-poem) Bill(I-poem) and(O) The(B-poem) Man(I-poem) from(I-poem) Ironbark(I-poem) .(O)"}}
{"id": "410", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "person", "location", "award", "organization", "book", "writer", "country", "literary genre", "magazine", "poem"], "instance": {"id": "410", "words": ["Among", "his", "earliest", "influences", ",", "Simon", "cited", "Norman", "Angell", "for", "his", "book", "The", "Great", "Illusion", "and", "Henry", "George", "for", "his", "book", "Progress", "and", "Poverty", "."], "labels": ["O", "O", "O", "O", "O", "B-writer", "O", "B-writer", "I-writer", "O", "O", "O", "B-book", "I-book", "I-book", "O", "B-writer", "I-writer", "O", "O", "O", "B-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, location, award, organization, book, writer, country, literary genre, magazine, poem and O.\nSentence: Among his earliest influences , Simon cited Norman Angell for his book The Great Illusion and Henry George for his book Progress and Poverty .", "prompt_labels": "Among(O) his(O) earliest(O) influences(O) ,(O) Simon(B-writer) cited(O) Norman(B-writer) Angell(I-writer) for(O) his(O) book(O) The(B-book) Great(I-book) Illusion(I-book) and(O) Henry(B-writer) George(I-writer) for(O) his(O) book(O) Progress(B-book) and(I-book) Poverty(I-book) .(O)"}}
{"id": "313", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "location", "magazine", "poem", "writer", "country", "organization", "book", "person", "award", "event"], "instance": {"id": "313", "words": ["This", "included", "The", "500", "Hats", "of", "Bartholomew", "Cubbins", "in", "1938", ",", "as", "well", "as", "The", "King", "'s", "Stilts", "and", "The", "Seven", "Lady", "Godivas", "in", "1939", ",", "all", "of", "which", "were", "in", "prose", ",", "atypically", "for", "him", "."], "labels": ["O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, location, magazine, poem, writer, country, organization, book, person, award, event and O.\nSentence: This included The 500 Hats of Bartholomew Cubbins in 1938 , as well as The King 's Stilts and The Seven Lady Godivas in 1939 , all of which were in prose , atypically for him .", "prompt_labels": "This(O) included(O) The(B-book) 500(I-book) Hats(I-book) of(I-book) Bartholomew(I-book) Cubbins(I-book) in(O) 1938(O) ,(O) as(O) well(O) as(O) The(B-book) King(I-book) 's(I-book) Stilts(I-book) and(O) The(B-book) Seven(I-book) Lady(I-book) Godivas(I-book) in(O) 1939(O) ,(O) all(O) of(O) which(O) were(O) in(O) prose(O) ,(O) atypically(O) for(O) him(O) .(O)"}}
{"id": "61", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "magazine", "country", "location", "event", "book", "award", "organization", "writer", "literary genre", "poem"], "instance": {"id": "61", "words": ["Percy", "is", "known", "for", "his", "philosophical", "novels", "set", "in", "and", "around", "New", "Orleans", ";", "the", "first", ",", "The", "Moviegoer", ",", "won", "the", "U.S.", "National", "Book", "Award", "for", "Fiction", "."], "labels": ["B-writer", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "B-country", "B-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, magazine, country, location, event, book, award, organization, writer, literary genre, poem and O.\nSentence: Percy is known for his philosophical novels set in and around New Orleans ; the first , The Moviegoer , won the U.S. National Book Award for Fiction .", "prompt_labels": "Percy(B-writer) is(O) known(O) for(O) his(O) philosophical(B-literary genre) novels(I-literary genre) set(O) in(O) and(O) around(O) New(B-location) Orleans(I-location) ;(O) the(O) first(O) ,(O) The(B-book) Moviegoer(I-book) ,(O) won(O) the(O) U.S.(B-country) National(B-award) Book(I-award) Award(I-award) for(I-award) Fiction(I-award) .(O)"}}
{"id": "151", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "award", "writer", "magazine", "book", "literary genre", "event", "person", "country", "poem", "organization"], "instance": {"id": "151", "words": ["Novels", "such", "as", "Kipps", "and", "The", "History", "of", "Mr", "Polly", ",", "which", "describe", "lower-middle-class", "life", ",", "led", "to", "the", "suggestion", "that", "he", "was", "a", "worthy", "successor", "to", "Charles", "Dickens", ",", "Vincent", "Brome", ",", "H.", "G.", "Wells", ":", "A", "Biography", "(", "London", ",", "New", "York", ",", "and", "Toronto", ":", "Longmans", ",", "Green", ",", "1951", ")", ",", "p", "."], "labels": ["B-literary genre", "O", "O", "B-book", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "B-location", "O", "B-location", "I-location", "O", "O", "B-location", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, writer, magazine, book, literary genre, event, person, country, poem, organization and O.\nSentence: Novels such as Kipps and The History of Mr Polly , which describe lower-middle-class life , led to the suggestion that he was a worthy successor to Charles Dickens , Vincent Brome , H. G. Wells : A Biography ( London , New York , and Toronto : Longmans , Green , 1951 ) , p .", "prompt_labels": "Novels(B-literary genre) such(O) as(O) Kipps(B-book) and(O) The(B-book) History(I-book) of(I-book) Mr(I-book) Polly(I-book) ,(O) which(O) describe(O) lower-middle-class(O) life(O) ,(O) led(O) to(O) the(O) suggestion(O) that(O) he(O) was(O) a(O) worthy(O) successor(O) to(O) Charles(B-writer) Dickens(I-writer) ,(O) Vincent(B-writer) Brome(I-writer) ,(O) H.(B-book) G.(I-book) Wells(I-book) :(I-book) A(I-book) Biography(I-book) ((O) London(B-location) ,(O) New(B-location) York(I-location) ,(O) and(O) Toronto(B-location) :(O) Longmans(B-organization) ,(I-organization) Green(I-organization) ,(O) 1951(O) )(O) ,(O) p(O) .(O)"}}
{"id": "260", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "person", "writer", "event", "literary genre", "award", "location", "magazine", "poem", "country", "book"], "instance": {"id": "260", "words": ["Writing", "for", "The", "Spectator", "in", "1936", ",", "Graham", "Greene", "gave", "the", "film", "a", "tepid", "review", ",", "describing", "it", "on", "the", "one", "hand", "as", "his", "favorite", "of", "the", "films", "he", "reviewed", "that", "week", ",", "but", "describing", "it", "as", "a", "fine", "spirited", "mix-up", "and", "making", "pointed", "note", "of", "the", "magnificently", "wrong", "characterization", "of", "bad", "King", "James", "."], "labels": ["O", "O", "B-magazine", "I-magazine", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, writer, event, literary genre, award, location, magazine, poem, country, book and O.\nSentence: Writing for The Spectator in 1936 , Graham Greene gave the film a tepid review , describing it on the one hand as his favorite of the films he reviewed that week , but describing it as a fine spirited mix-up and making pointed note of the magnificently wrong characterization of bad King James .", "prompt_labels": "Writing(O) for(O) The(B-magazine) Spectator(I-magazine) in(O) 1936(O) ,(O) Graham(B-writer) Greene(I-writer) gave(O) the(O) film(O) a(O) tepid(O) review(O) ,(O) describing(O) it(O) on(O) the(O) one(O) hand(O) as(O) his(O) favorite(O) of(O) the(O) films(O) he(O) reviewed(O) that(O) week(O) ,(O) but(O) describing(O) it(O) as(O) a(O) fine(O) spirited(O) mix-up(O) and(O) making(O) pointed(O) note(O) of(O) the(O) magnificently(O) wrong(O) characterization(O) of(O) bad(O) King(B-person) James(I-person) .(O)"}}
{"id": "127", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "location", "book", "poem", "literary genre", "person", "writer", "magazine", "organization", "country", "award"], "instance": {"id": "127", "words": ["Susan", "Cooper", "related", "that", "The", "power", "and", "range", "of", "Alan", "Garner", "'s", "astounding", "talent", "has", "grown", "with", "every", "book", "he", "'s", "written", ",", "whilst", "David", "Almond", "called", "him", "one", "of", "Britain", "'s", "greatest", "writers", "whose", "works", "really", "matter", "."], "labels": ["B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, book, poem, literary genre, person, writer, magazine, organization, country, award and O.\nSentence: Susan Cooper related that The power and range of Alan Garner 's astounding talent has grown with every book he 's written , whilst David Almond called him one of Britain 's greatest writers whose works really matter .", "prompt_labels": "Susan(B-writer) Cooper(I-writer) related(O) that(O) The(O) power(O) and(O) range(O) of(O) Alan(B-writer) Garner(I-writer) 's(O) astounding(O) talent(O) has(O) grown(O) with(O) every(O) book(O) he(O) 's(O) written(O) ,(O) whilst(O) David(B-writer) Almond(I-writer) called(O) him(O) one(O) of(O) Britain(B-country) 's(O) greatest(O) writers(O) whose(O) works(O) really(O) matter(O) .(O)"}}
{"id": "245", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "organization", "country", "literary genre", "person", "event", "poem", "magazine", "location", "book", "writer"], "instance": {"id": "245", "words": ["In", "2005", ",", "McEntire", "starred", "as", "Nellie", "Forbush", "in", "the", "Carnegie", "Hall", "concert", "production", "of", "the", "Broadway", "musical", "South", "Pacific", "with", "Alec", "Baldwin", "as", "Luther", "Billis", "and", "Brian", "Stokes", "Mitchell", "as", "Emile", "de", "Becque", ",", "directed", "by", "Walter", "Bobbie", "and", "with", "an", "adapted", "script", "by", "David", "Ives", "."], "labels": ["O", "O", "O", "B-person", "O", "O", "B-person", "I-person", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "B-organization", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, country, literary genre, person, event, poem, magazine, location, book, writer and O.\nSentence: In 2005 , McEntire starred as Nellie Forbush in the Carnegie Hall concert production of the Broadway musical South Pacific with Alec Baldwin as Luther Billis and Brian Stokes Mitchell as Emile de Becque , directed by Walter Bobbie and with an adapted script by David Ives .", "prompt_labels": "In(O) 2005(O) ,(O) McEntire(B-person) starred(O) as(O) Nellie(B-person) Forbush(I-person) in(O) the(O) Carnegie(B-location) Hall(I-location) concert(I-location) production(O) of(O) the(O) Broadway(B-organization) musical(O) South(O) Pacific(O) with(O) Alec(B-person) Baldwin(I-person) as(O) Luther(B-person) Billis(I-person) and(O) Brian(B-person) Stokes(I-person) Mitchell(I-person) as(O) Emile(B-person) de(I-person) Becque(I-person) ,(O) directed(O) by(O) Walter(B-person) Bobbie(I-person) and(O) with(O) an(O) adapted(O) script(O) by(O) David(B-writer) Ives(I-writer) .(O)"}}
{"id": "273", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "poem", "event", "organization", "location", "country", "writer", "book", "literary genre", "person", "magazine"], "instance": {"id": "273", "words": ["His", "first", "story", "to", "attract", "major", "attention", "was", "A", "Rose", "for", "Ecclesiastes", ",", "published", "in", "The", "Magazine", "of", "Fantasy", "&", "Science", "Fiction", ",", "with", "cover", "art", "by", "Hannes", "Bok", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, poem, event, organization, location, country, writer, book, literary genre, person, magazine and O.\nSentence: His first story to attract major attention was A Rose for Ecclesiastes , published in The Magazine of Fantasy & Science Fiction , with cover art by Hannes Bok .", "prompt_labels": "His(O) first(O) story(O) to(O) attract(O) major(O) attention(O) was(O) A(B-book) Rose(I-book) for(I-book) Ecclesiastes(I-book) ,(O) published(O) in(O) The(B-magazine) Magazine(I-magazine) of(I-magazine) Fantasy(I-magazine) &(I-magazine) Science(I-magazine) Fiction(I-magazine) ,(O) with(O) cover(O) art(O) by(O) Hannes(B-writer) Bok(I-writer) .(O)"}}
{"id": "140", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "event", "country", "person", "award", "poem", "writer", "book", "magazine", "location", "organization"], "instance": {"id": "140", "words": ["Akhundzade", "'s", "first", "published", "work", "was", "Eastern", "poem", "on", "the", "death", "of", "Pushkin", "(", "1837", ")", ",", "written", "to", "lament", "the", "death", "of", "the", "great", "Russian", "poet", "Alexander", "Pushkin", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, event, country, person, award, poem, writer, book, magazine, location, organization and O.\nSentence: Akhundzade 's first published work was Eastern poem on the death of Pushkin ( 1837 ) , written to lament the death of the great Russian poet Alexander Pushkin .", "prompt_labels": "Akhundzade(B-writer) 's(O) first(O) published(O) work(O) was(O) Eastern(B-poem) poem(I-poem) on(I-poem) the(I-poem) death(I-poem) of(I-poem) Pushkin(I-poem) ((O) 1837(O) )(O) ,(O) written(O) to(O) lament(O) the(O) death(O) of(O) the(O) great(O) Russian(O) poet(O) Alexander(B-writer) Pushkin(I-writer) .(O)"}}
{"id": "318", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "person", "writer", "poem", "organization", "book", "magazine", "country", "literary genre", "event", "award"], "instance": {"id": "318", "words": ["He", "began", "acting", "on-screen", "in", "the", "early", "1980s", ",", "with", "his", "mainstream", "breakthrough", "coming", "with", "Lee", "'s", "Do", "the", "Right", "Thing", "(", "1989", ")", "and", "the", "Coens", "'", "Miller", "'s", "Crossing", "(", "1990", ")", "and", "Barton", "Fink", "(", "1991", ")", ",", "for", "which", "he", "won", "the", "Cannes", "Film", "Festival", "Award", "for", "Best", "Actor", "Award", "at", "the", "Cannes", "Film", "Festival", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, writer, poem, organization, book, magazine, country, literary genre, event, award and O.\nSentence: He began acting on-screen in the early 1980s , with his mainstream breakthrough coming with Lee 's Do the Right Thing ( 1989 ) and the Coens ' Miller 's Crossing ( 1990 ) and Barton Fink ( 1991 ) , for which he won the Cannes Film Festival Award for Best Actor Award at the Cannes Film Festival .", "prompt_labels": "He(O) began(O) acting(O) on-screen(O) in(O) the(O) early(O) 1980s(O) ,(O) with(O) his(O) mainstream(O) breakthrough(O) coming(O) with(O) Lee(B-person) 's(O) Do(O) the(O) Right(O) Thing(O) ((O) 1989(O) )(O) and(O) the(O) Coens(B-person) '(O) Miller(O) 's(O) Crossing(O) ((O) 1990(O) )(O) and(O) Barton(O) Fink(O) ((O) 1991(O) )(O) ,(O) for(O) which(O) he(O) won(O) the(O) Cannes(B-award) Film(I-award) Festival(I-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) Award(O) at(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) .(O)"}}
{"id": "321", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "event", "award", "organization", "location", "writer", "book", "magazine", "country", "poem", "person"], "instance": {"id": "321", "words": ["Rashomon", ",", "which", "premiered", "in", "Tokyo", ",", "became", "the", "surprise", "winner", "of", "the", "Golden", "Lion", "at", "the", "1951", "Venice", "Film", "Festival", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, event, award, organization, location, writer, book, magazine, country, poem, person and O.\nSentence: Rashomon , which premiered in Tokyo , became the surprise winner of the Golden Lion at the 1951 Venice Film Festival .", "prompt_labels": "Rashomon(O) ,(O) which(O) premiered(O) in(O) Tokyo(O) ,(O) became(O) the(O) surprise(O) winner(O) of(O) the(O) Golden(B-award) Lion(I-award) at(O) the(O) 1951(O) Venice(B-event) Film(I-event) Festival(I-event) .(O)"}}
{"id": "387", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "person", "poem", "writer", "country", "location", "magazine", "book", "award", "organization", "event"], "instance": {"id": "387", "words": ["91", "In", "the", "same", "year", "The", "Saturday", "Evening", "Post", "paid", "$", "3,500", "to", "serialise", "Something", "Fresh", ",", "the", "first", "of", "what", "became", "a", "series", "of", "novels", "set", "at", "Blandings", "Castle", ".", "Wodehouse", "and", "Ratcliffe", ",", "p", "."], "labels": ["O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "B-book", "I-book", "O", "B-writer", "O", "B-writer", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, person, poem, writer, country, location, magazine, book, award, organization, event and O.\nSentence: 91 In the same year The Saturday Evening Post paid $ 3,500 to serialise Something Fresh , the first of what became a series of novels set at Blandings Castle . Wodehouse and Ratcliffe , p .", "prompt_labels": "91(O) In(O) the(O) same(O) year(O) The(B-magazine) Saturday(I-magazine) Evening(I-magazine) Post(I-magazine) paid(O) $(O) 3,500(O) to(O) serialise(O) Something(B-book) Fresh(I-book) ,(O) the(O) first(O) of(O) what(O) became(O) a(O) series(O) of(O) novels(B-literary genre) set(O) at(O) Blandings(B-book) Castle(I-book) .(O) Wodehouse(B-writer) and(O) Ratcliffe(B-writer) ,(O) p(O) .(O)"}}
{"id": "366", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "country", "literary genre", "person", "location", "event", "organization", "writer", "poem", "book", "award"], "instance": {"id": "366", "words": ["Edwards", "delivered", "the", "sermon", "Sinners", "in", "the", "Hands", "of", "an", "Angry", "God", ",", "a", "classic", "of", "early", "American", "literature", ",", "during", "another", "revival", "in", "1741", ",", "following", "George", "Whitefield", "'", "s", "tour", "of", "the", "Thirteen", "Colonies", "."], "labels": ["B-person", "O", "O", "B-literary genre", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, country, literary genre, person, location, event, organization, writer, poem, book, award and O.\nSentence: Edwards delivered the sermon Sinners in the Hands of an Angry God , a classic of early American literature , during another revival in 1741 , following George Whitefield ' s tour of the Thirteen Colonies .", "prompt_labels": "Edwards(B-person) delivered(O) the(O) sermon(B-literary genre) Sinners(B-book) in(I-book) the(I-book) Hands(I-book) of(I-book) an(I-book) Angry(I-book) God(I-book) ,(O) a(O) classic(O) of(O) early(O) American(O) literature(O) ,(O) during(O) another(O) revival(O) in(O) 1741(O) ,(O) following(O) George(B-writer) Whitefield(I-writer) '(O) s(O) tour(O) of(O) the(O) Thirteen(B-country) Colonies(I-country) .(O)"}}
{"id": "27", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "writer", "organization", "poem", "literary genre", "award", "magazine", "event", "location", "country", "person"], "instance": {"id": "27", "words": ["H.", "G.", "Wells", "praised", "Growth", "of", "the", "Soil", "(", "1917", ")", "for", "which", "Hamsun", "was", "awarded", "the", "Nobel", "Prize", "in", "Literature", "."], "labels": ["B-writer", "I-writer", "I-writer", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, writer, organization, poem, literary genre, award, magazine, event, location, country, person and O.\nSentence: H. G. Wells praised Growth of the Soil ( 1917 ) for which Hamsun was awarded the Nobel Prize in Literature .", "prompt_labels": "H.(B-writer) G.(I-writer) Wells(I-writer) praised(O) Growth(B-book) of(I-book) the(I-book) Soil(I-book) ((O) 1917(O) )(O) for(O) which(O) Hamsun(B-writer) was(O) awarded(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) .(O)"}}
{"id": "219", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "poem", "literary genre", "book", "event", "country", "award", "magazine", "person", "writer", "organization"], "instance": {"id": "219", "words": ["He", "continued", "his", "Dune", "saga", ",", "following", "it", "with", "Dune", "Messiah", ",", "Children", "of", "Dune", ",", "and", "God", "Emperor", "of", "Dune", "."], "labels": ["O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "O", "B-book", "I-book", "I-book", "O", "O", "B-book", "I-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, poem, literary genre, book, event, country, award, magazine, person, writer, organization and O.\nSentence: He continued his Dune saga , following it with Dune Messiah , Children of Dune , and God Emperor of Dune .", "prompt_labels": "He(O) continued(O) his(O) Dune(B-book) saga(I-book) ,(O) following(O) it(O) with(O) Dune(B-book) Messiah(I-book) ,(O) Children(B-book) of(I-book) Dune(I-book) ,(O) and(O) God(B-book) Emperor(I-book) of(I-book) Dune(I-book) .(O)"}}
{"id": "63", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "award", "magazine", "poem", "organization", "event", "location", "literary genre", "book", "person", "country"], "instance": {"id": "63", "words": ["The", "existing", "repertoire", "of", "Scottish-themed", "plays", "included", "John", "Home", "'", "s", "Douglas", "(", "1756", ")", "and", "Allan", "Ramsay", "'", "s", "The", "Gentle", "Shepherd", "(", "1725", ")", ",", "with", "the", "last", "two", "being", "the", "most", "popular", "plays", "among", "amateur", "groups", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-poem", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, award, magazine, poem, organization, event, location, literary genre, book, person, country and O.\nSentence: The existing repertoire of Scottish-themed plays included John Home ' s Douglas ( 1756 ) and Allan Ramsay ' s The Gentle Shepherd ( 1725 ) , with the last two being the most popular plays among amateur groups .", "prompt_labels": "The(O) existing(O) repertoire(O) of(O) Scottish-themed(O) plays(O) included(O) John(B-writer) Home(I-writer) '(O) s(O) Douglas(B-poem) ((O) 1756(O) )(O) and(O) Allan(B-writer) Ramsay(I-writer) '(O) s(O) The(B-poem) Gentle(I-poem) Shepherd(I-poem) ((O) 1725(O) )(O) ,(O) with(O) the(O) last(O) two(O) being(O) the(O) most(O) popular(O) plays(O) among(O) amateur(O) groups(O) .(O)"}}
{"id": "252", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "book", "person", "location", "organization", "literary genre", "award", "poem", "magazine", "country", "event"], "instance": {"id": "252", "words": ["In", "addition", "to", "the", "Vietnam", "War", ",", "he", "has", "also", "been", "through", "the", "Iran", "hostage", "crisis", "and", "the", "Gulf", "War", ",", "plus", "a", "number", "of", "missions", "in", "the", "Soviet", "Union", ",", "and", "claims", "to", "have", "had", "Abu", "Nidal", "'", "s", "head", "in", "my", "gunsights", ",", "but", "never", "got", "the", "green", "light", "allowing", "him", "to", "kill", "the", "man", "(", "Clear", "and", "Present", "Danger", ")", "."], "labels": ["O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, person, location, organization, literary genre, award, poem, magazine, country, event and O.\nSentence: In addition to the Vietnam War , he has also been through the Iran hostage crisis and the Gulf War , plus a number of missions in the Soviet Union , and claims to have had Abu Nidal ' s head in my gunsights , but never got the green light allowing him to kill the man ( Clear and Present Danger ) .", "prompt_labels": "In(O) addition(O) to(O) the(O) Vietnam(B-event) War(I-event) ,(O) he(O) has(O) also(O) been(O) through(O) the(O) Iran(B-event) hostage(I-event) crisis(I-event) and(O) the(O) Gulf(B-event) War(I-event) ,(O) plus(O) a(O) number(O) of(O) missions(O) in(O) the(O) Soviet(B-country) Union(I-country) ,(O) and(O) claims(O) to(O) have(O) had(O) Abu(B-person) Nidal(I-person) '(O) s(O) head(O) in(O) my(O) gunsights(O) ,(O) but(O) never(O) got(O) the(O) green(O) light(O) allowing(O) him(O) to(O) kill(O) the(O) man(O) ((O) Clear(B-book) and(I-book) Present(I-book) Danger(I-book) )(O) .(O)"}}
{"id": "79", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "event", "literary genre", "country", "award", "writer", "poem", "location", "magazine", "person", "organization"], "instance": {"id": "79", "words": ["The", "second", "volume", "(", "The", "Black", "Cauldron", ")", "was", "a", "runner-up", "for", "the", "1966", "Newbery", "Medal", ";", "the", "fourth", "(", "Taran", "Wanderer", ")", "was", "a", "School", "Library", "Journal", "Best", "Book", "of", "the", "Year", ";", "the", "fifth", "and", "concluding", "volume", "(", "The", "High", "King", ")", "won", "the", "1969", "Newbery", "."], "labels": ["O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "B-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, event, literary genre, country, award, writer, poem, location, magazine, person, organization and O.\nSentence: The second volume ( The Black Cauldron ) was a runner-up for the 1966 Newbery Medal ; the fourth ( Taran Wanderer ) was a School Library Journal Best Book of the Year ; the fifth and concluding volume ( The High King ) won the 1969 Newbery .", "prompt_labels": "The(O) second(O) volume(O) ((O) The(B-book) Black(I-book) Cauldron(I-book) )(O) was(O) a(O) runner-up(O) for(O) the(O) 1966(O) Newbery(B-award) Medal(I-award) ;(O) the(O) fourth(O) ((O) Taran(B-book) Wanderer(I-book) )(O) was(O) a(O) School(B-magazine) Library(I-magazine) Journal(I-magazine) Best(B-award) Book(I-award) of(I-award) the(I-award) Year(I-award) ;(O) the(O) fifth(O) and(O) concluding(O) volume(O) ((O) The(B-book) High(I-book) King(I-book) )(O) won(O) the(O) 1969(O) Newbery(B-award) .(O)"}}
{"id": "400", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "organization", "award", "magazine", "country", "event", "person", "location", "writer", "literary genre", "book"], "instance": {"id": "400", "words": ["He", "won", "the", "Pulitzer", "Prize", "for", "Poetry", "for", "his", "1947", "long", "poem", "The", "Age", "of", "Anxiety", ",", "the", "title", "of", "which", "became", "a", "popular", "phrase", "describing", "the", "modern", "era", "."], "labels": ["O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, organization, award, magazine, country, event, person, location, writer, literary genre, book and O.\nSentence: He won the Pulitzer Prize for Poetry for his 1947 long poem The Age of Anxiety , the title of which became a popular phrase describing the modern era .", "prompt_labels": "He(O) won(O) the(O) Pulitzer(B-award) Prize(I-award) for(I-award) Poetry(I-award) for(O) his(O) 1947(O) long(O) poem(O) The(B-poem) Age(I-poem) of(I-poem) Anxiety(I-poem) ,(O) the(O) title(O) of(O) which(O) became(O) a(O) popular(O) phrase(O) describing(O) the(O) modern(O) era(O) .(O)"}}
{"id": "284", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "writer", "location", "literary genre", "magazine", "organization", "event", "poem", "person", "country", "award"], "instance": {"id": "284", "words": ["In", "December", "of", "the", "same", "year", ",", "allegations", "by", "Arkansas", "state", "troopers", "Larry", "Patterson", "and", "Roger", "Perry", "were", "first", "reported", "by", "David", "Brock", "in", "The", "American", "Spectator", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-magazine", "I-magazine", "I-magazine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, writer, location, literary genre, magazine, organization, event, poem, person, country, award and O.\nSentence: In December of the same year , allegations by Arkansas state troopers Larry Patterson and Roger Perry were first reported by David Brock in The American Spectator .", "prompt_labels": "In(O) December(O) of(O) the(O) same(O) year(O) ,(O) allegations(O) by(O) Arkansas(B-location) state(O) troopers(O) Larry(B-person) Patterson(I-person) and(O) Roger(B-person) Perry(I-person) were(O) first(O) reported(O) by(O) David(B-writer) Brock(I-writer) in(O) The(B-magazine) American(I-magazine) Spectator(I-magazine) .(O)"}}
{"id": "68", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "organization", "writer", "book", "magazine", "location", "literary genre", "event", "poem", "award", "person"], "instance": {"id": "68", "words": ["Other", "highlights", "were", "The", "Dosadi", "Experiment", ",", "The", "Godmakers", ",", "The", "White", "Plague", "and", "the", "books", "he", "wrote", "in", "partnership", "with", "Bill", "Ransom", ":", "The", "Jesus", "Incident", ",", "The", "Lazarus", "Effect", ",", "and", "The", "Ascension", "Factor", "which", "were", "sequels", "to", "Destination", ":", "Void", "."], "labels": ["O", "O", "O", "B-book", "I-book", "I-book", "O", "B-book", "I-book", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, writer, book, magazine, location, literary genre, event, poem, award, person and O.\nSentence: Other highlights were The Dosadi Experiment , The Godmakers , The White Plague and the books he wrote in partnership with Bill Ransom : The Jesus Incident , The Lazarus Effect , and The Ascension Factor which were sequels to Destination : Void .", "prompt_labels": "Other(O) highlights(O) were(O) The(B-book) Dosadi(I-book) Experiment(I-book) ,(O) The(B-book) Godmakers(I-book) ,(O) The(B-book) White(I-book) Plague(I-book) and(O) the(O) books(O) he(O) wrote(O) in(O) partnership(O) with(O) Bill(B-writer) Ransom(I-writer) :(O) The(B-book) Jesus(I-book) Incident(I-book) ,(O) The(B-book) Lazarus(I-book) Effect(I-book) ,(O) and(O) The(B-book) Ascension(I-book) Factor(I-book) which(O) were(O) sequels(O) to(O) Destination(B-book) :(I-book) Void(I-book) .(O)"}}
{"id": "242", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "literary genre", "poem", "person", "writer", "location", "organization", "award", "country", "magazine", "event"], "instance": {"id": "242", "words": ["This", "heartbreak", "was", "reflected", "in", "her", "early", "poetry", "and", "earned", "Mistral", "her", "first", "recognized", "literary", "work", "in", "1914", "with", "Sonetos", "de", "la", "Muerte", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, literary genre, poem, person, writer, location, organization, award, country, magazine, event and O.\nSentence: This heartbreak was reflected in her early poetry and earned Mistral her first recognized literary work in 1914 with Sonetos de la Muerte .", "prompt_labels": "This(O) heartbreak(O) was(O) reflected(O) in(O) her(O) early(O) poetry(B-literary genre) and(O) earned(O) Mistral(B-writer) her(O) first(O) recognized(O) literary(O) work(O) in(O) 1914(O) with(O) Sonetos(B-poem) de(I-poem) la(I-poem) Muerte(I-poem) .(O)"}}
{"id": "60", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "organization", "literary genre", "writer", "person", "book", "poem", "location", "award", "magazine", "country"], "instance": {"id": "60", "words": ["As", "a", "writer", ",", "Orwell", "produced", "literary", "criticism", "and", "poetry", ",", "fiction", "and", "polemic", "al", "journalism", ";", "and", "is", "best", "known", "for", "the", "allegorical", "novella", "Animal", "Farm", "(", "1945", ")", "and", "the", "dystopian", "novel", "Nineteen", "Eighty-Four", "(", "1949", ")", "."], "labels": ["O", "O", "O", "O", "B-writer", "O", "B-literary genre", "I-literary genre", "O", "B-literary genre", "O", "B-literary genre", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "B-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, literary genre, writer, person, book, poem, location, award, magazine, country and O.\nSentence: As a writer , Orwell produced literary criticism and poetry , fiction and polemic al journalism ; and is best known for the allegorical novella Animal Farm ( 1945 ) and the dystopian novel Nineteen Eighty-Four ( 1949 ) .", "prompt_labels": "As(O) a(O) writer(O) ,(O) Orwell(B-writer) produced(O) literary(B-literary genre) criticism(I-literary genre) and(O) poetry(B-literary genre) ,(O) fiction(B-literary genre) and(O) polemic(B-literary genre) al(O) journalism(O) ;(O) and(O) is(O) best(O) known(O) for(O) the(O) allegorical(O) novella(O) Animal(B-book) Farm(I-book) ((O) 1945(O) )(O) and(O) the(O) dystopian(B-literary genre) novel(I-literary genre) Nineteen(B-book) Eighty-Four(I-book) ((O) 1949(O) )(O) .(O)"}}
{"id": "307", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "organization", "literary genre", "event", "country", "award", "poem", "person", "magazine", "writer", "book"], "instance": {"id": "307", "words": ["Wilhelm", "Meister", "'s", "Apprenticeship", "provided", "the", "text", "for", "many", "lied", "er", ",", "among", "others", "by", "Beethoven", ",", "for", "example", "Sehnsucht", ":", "Gedicht", "von", "Goethe", "viermal", "in", "Musik", "gesetzt", "von", "L.", "van", "Beethoven", ",", "four", "settings", "of", "Nur", "wer", "die", "Sehnsucht", "kennt", ",", "WoO", "."], "labels": ["B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "I-poem", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, literary genre, event, country, award, poem, person, magazine, writer, book and O.\nSentence: Wilhelm Meister 's Apprenticeship provided the text for many lied er , among others by Beethoven , for example Sehnsucht : Gedicht von Goethe viermal in Musik gesetzt von L. van Beethoven , four settings of Nur wer die Sehnsucht kennt , WoO .", "prompt_labels": "Wilhelm(B-book) Meister(I-book) 's(I-book) Apprenticeship(I-book) provided(O) the(O) text(O) for(O) many(O) lied(O) er(O) ,(O) among(O) others(O) by(O) Beethoven(B-person) ,(O) for(O) example(O) Sehnsucht(B-poem) :(I-poem) Gedicht(I-poem) von(I-poem) Goethe(I-poem) viermal(O) in(O) Musik(O) gesetzt(O) von(O) L.(B-person) van(I-person) Beethoven(I-person) ,(O) four(O) settings(O) of(O) Nur(B-poem) wer(I-poem) die(I-poem) Sehnsucht(I-poem) kennt(I-poem) ,(O) WoO(O) .(O)"}}
{"id": "115", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "location", "literary genre", "writer", "magazine", "person", "award", "event", "poem", "country", "book"], "instance": {"id": "115", "words": ["Adapted", "by", "Harry", "Behn", "from", "the", "autobiographical", "novel", "Plumes", "by", "Laurence", "Stallings", ",", "with", "titles", "by", "Joseph", "W.", "Farnham", ",", "the", "film", "is", "about", "an", "idle", "rich", "boy", "who", "joins", "the", "US", "Army", "'", "s", "Rainbow", "Division", "and", "is", "sent", "to", "France", "to", "fight", "in", "World", "War", "I", ",", "becomes", "a", "friend", "of", "two", "working", "class", "men", ",", "experiences", "the", "horrors", "of", "trench", "warfare", ",", "and", "finds", "love", "with", "a", "French", "girl", "."], "labels": ["O", "O", "B-writer", "I-writer", "O", "O", "B-literary genre", "I-literary genre", "B-book", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, literary genre, writer, magazine, person, award, event, poem, country, book and O.\nSentence: Adapted by Harry Behn from the autobiographical novel Plumes by Laurence Stallings , with titles by Joseph W. Farnham , the film is about an idle rich boy who joins the US Army ' s Rainbow Division and is sent to France to fight in World War I , becomes a friend of two working class men , experiences the horrors of trench warfare , and finds love with a French girl .", "prompt_labels": "Adapted(O) by(O) Harry(B-writer) Behn(I-writer) from(O) the(O) autobiographical(B-literary genre) novel(I-literary genre) Plumes(B-book) by(O) Laurence(B-writer) Stallings(I-writer) ,(O) with(O) titles(O) by(O) Joseph(B-writer) W.(I-writer) Farnham(I-writer) ,(O) the(O) film(O) is(O) about(O) an(O) idle(O) rich(O) boy(O) who(O) joins(O) the(O) US(O) Army(O) '(O) s(O) Rainbow(O) Division(O) and(O) is(O) sent(O) to(O) France(B-country) to(O) fight(O) in(O) World(B-event) War(I-event) I(I-event) ,(O) becomes(O) a(O) friend(O) of(O) two(O) working(O) class(O) men(O) ,(O) experiences(O) the(O) horrors(O) of(O) trench(O) warfare(O) ,(O) and(O) finds(O) love(O) with(O) a(O) French(O) girl(O) .(O)"}}
{"id": "135", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "person", "book", "location", "writer", "event", "magazine", "poem", "literary genre", "award", "organization"], "instance": {"id": "135", "words": ["His", "books", "Difference", "and", "Repetition", "(", "1968", ")", "and", "The", "Logic", "of", "Sense", "(", "1969", ")", "led", "Michel", "Foucault", "to", "declare", "that", "one", "day", ",", "perhaps", ",", "this", "century", "will", "be", "called", "Deleuzian", "."], "labels": ["O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, book, location, writer, event, magazine, poem, literary genre, award, organization and O.\nSentence: His books Difference and Repetition ( 1968 ) and The Logic of Sense ( 1969 ) led Michel Foucault to declare that one day , perhaps , this century will be called Deleuzian .", "prompt_labels": "His(O) books(O) Difference(B-book) and(I-book) Repetition(I-book) ((O) 1968(O) )(O) and(O) The(B-book) Logic(I-book) of(I-book) Sense(I-book) ((O) 1969(O) )(O) led(O) Michel(B-writer) Foucault(I-writer) to(O) declare(O) that(O) one(O) day(O) ,(O) perhaps(O) ,(O) this(O) century(O) will(O) be(O) called(O) Deleuzian(B-writer) .(O)"}}
{"id": "75", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "poem", "location", "event", "writer", "award", "magazine", "person", "book", "organization", "country"], "instance": {"id": "75", "words": ["In", "Poland", "he", "is", "best", "known", "for", "his", "Trilogy", "of", "historical", "novels", "-", "With", "Fire", "and", "Sword", ",", "The", "Deluge", ",", "and", "Fire", "in", "the", "Steppe", "-", "set", "in", "the", "17th-century", "Polish-Lithuanian", "Commonwealth", ";", "internationally", "he", "is", "best", "known", "for", "Quo", "Vadis", ",", "set", "in", "Nero", "'", "s", "Ancient", "Rome", "."], "labels": ["O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "B-person", "O", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, poem, location, event, writer, award, magazine, person, book, organization, country and O.\nSentence: In Poland he is best known for his Trilogy of historical novels - With Fire and Sword , The Deluge , and Fire in the Steppe - set in the 17th-century Polish-Lithuanian Commonwealth ; internationally he is best known for Quo Vadis , set in Nero ' s Ancient Rome .", "prompt_labels": "In(O) Poland(B-country) he(O) is(O) best(O) known(O) for(O) his(O) Trilogy(O) of(O) historical(B-literary genre) novels(I-literary genre) -(O) With(B-book) Fire(I-book) and(I-book) Sword(I-book) ,(O) The(B-book) Deluge(I-book) ,(O) and(O) Fire(B-book) in(I-book) the(I-book) Steppe(I-book) -(O) set(O) in(O) the(O) 17th-century(O) Polish-Lithuanian(B-location) Commonwealth(I-location) ;(O) internationally(O) he(O) is(O) best(O) known(O) for(O) Quo(B-book) Vadis(I-book) ,(O) set(O) in(O) Nero(B-person) '(O) s(O) Ancient(B-country) Rome(I-country) .(O)"}}
{"id": "194", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "magazine", "literary genre", "book", "writer", "event", "poem", "person", "location", "country", "award"], "instance": {"id": "194", "words": ["He", "associated", "with", "a", "variety", "of", "figures", "in", "Britain", "'s", "intelligence", "community", "at", "the", "time", ",", "including", "Dennis", "Wheatley", ",", "Roald", "Dahl", ",", "Ian", "Fleming", ",", "and", "Maxwell", "Knight", ","], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, magazine, literary genre, book, writer, event, poem, person, location, country, award and O.\nSentence: He associated with a variety of figures in Britain 's intelligence community at the time , including Dennis Wheatley , Roald Dahl , Ian Fleming , and Maxwell Knight ,", "prompt_labels": "He(O) associated(O) with(O) a(O) variety(O) of(O) figures(O) in(O) Britain(B-country) 's(O) intelligence(O) community(O) at(O) the(O) time(O) ,(O) including(O) Dennis(B-writer) Wheatley(I-writer) ,(O) Roald(B-writer) Dahl(I-writer) ,(O) Ian(B-writer) Fleming(I-writer) ,(O) and(O) Maxwell(B-person) Knight(I-person) ,(O)"}}
{"id": "257", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "organization", "book", "magazine", "event", "award", "literary genre", "location", "writer", "poem", "person"], "instance": {"id": "257", "words": ["Solzhenitsyn", "criticized", "the", "2003", "invasion", "of", "Iraq", "and", "accused", "the", "United", "States", "of", "the", "occupation", "of", "Kosovo", ",", "Afghanistan", "and", "Iraq", "."], "labels": ["B-writer", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, book, magazine, event, award, literary genre, location, writer, poem, person and O.\nSentence: Solzhenitsyn criticized the 2003 invasion of Iraq and accused the United States of the occupation of Kosovo , Afghanistan and Iraq .", "prompt_labels": "Solzhenitsyn(B-writer) criticized(O) the(O) 2003(B-event) invasion(I-event) of(I-event) Iraq(I-event) and(O) accused(O) the(O) United(B-country) States(I-country) of(O) the(O) occupation(O) of(O) Kosovo(B-country) ,(O) Afghanistan(B-country) and(O) Iraq(B-country) .(O)"}}
{"id": "364", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "award", "location", "literary genre", "magazine", "poem", "book", "person", "country", "writer", "organization"], "instance": {"id": "364", "words": ["A", "Fire", "Upon", "the", "Deep", "won", "the", "Hugo", "Award", "for", "Best", "Novel", "in", "1993", ",", "sharing", "it", "with", "Doomsday", "Book", "by", "Connie", "Willis", "."], "labels": ["B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, location, literary genre, magazine, poem, book, person, country, writer, organization and O.\nSentence: A Fire Upon the Deep won the Hugo Award for Best Novel in 1993 , sharing it with Doomsday Book by Connie Willis .", "prompt_labels": "A(B-book) Fire(I-book) Upon(I-book) the(I-book) Deep(I-book) won(O) the(O) Hugo(B-award) Award(I-award) for(I-award) Best(I-award) Novel(I-award) in(O) 1993(O) ,(O) sharing(O) it(O) with(O) Doomsday(B-book) Book(I-book) by(O) Connie(B-writer) Willis(I-writer) .(O)"}}
{"id": "157", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "person", "event", "magazine", "book", "organization", "literary genre", "poem", "award", "writer", "location"], "instance": {"id": "157", "words": ["Faulkner", "was", "awarded", "two", "Pulitzer", "Prize", "s", "for", "what", "are", "considered", "minor", "novels", ":", "his", "1954", "novel", "A", "Fable", ",", "which", "took", "the", "Pulitzer", "in", "1955", ",", "and", "the", "1962", "novel", ",", "The", "Reivers", ",", "which", "was", "posthumously", "awarded", "the", "Pulitzer", "in", "1963", "."], "labels": ["B-writer", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "B-literary genre", "B-book", "I-book", "O", "O", "O", "O", "B-award", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "B-award", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, event, magazine, book, organization, literary genre, poem, award, writer, location and O.\nSentence: Faulkner was awarded two Pulitzer Prize s for what are considered minor novels : his 1954 novel A Fable , which took the Pulitzer in 1955 , and the 1962 novel , The Reivers , which was posthumously awarded the Pulitzer in 1963 .", "prompt_labels": "Faulkner(B-writer) was(O) awarded(O) two(O) Pulitzer(B-award) Prize(I-award) s(O) for(O) what(O) are(O) considered(O) minor(O) novels(B-literary genre) :(O) his(O) 1954(O) novel(B-literary genre) A(B-book) Fable(I-book) ,(O) which(O) took(O) the(O) Pulitzer(B-award) in(O) 1955(O) ,(O) and(O) the(O) 1962(O) novel(B-literary genre) ,(O) The(B-book) Reivers(I-book) ,(O) which(O) was(O) posthumously(O) awarded(O) the(O) Pulitzer(B-award) in(O) 1963(O) .(O)"}}
{"id": "270", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "poem", "magazine", "award", "event", "literary genre", "book", "location", "country", "person", "writer"], "instance": {"id": "270", "words": ["Using", "donations", "and", "sales", "of", "their", "magazine", ",", "Skeptical", "Inquirer", ",", "they", "and", "secular", "humanist", "philosopher", "Paul", "Kurtz", "took", "seats", "on", "the", "executive", "board", ",", "with", "Isaac", "Asimov", "and", "Carl", "Sagan", "joining", "as", "founding", "members", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-person", "I-person", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, poem, magazine, award, event, literary genre, book, location, country, person, writer and O.\nSentence: Using donations and sales of their magazine , Skeptical Inquirer , they and secular humanist philosopher Paul Kurtz took seats on the executive board , with Isaac Asimov and Carl Sagan joining as founding members .", "prompt_labels": "Using(O) donations(O) and(O) sales(O) of(O) their(O) magazine(O) ,(O) Skeptical(B-magazine) Inquirer(I-magazine) ,(O) they(O) and(O) secular(O) humanist(O) philosopher(O) Paul(B-person) Kurtz(I-person) took(O) seats(O) on(O) the(O) executive(O) board(O) ,(O) with(O) Isaac(B-writer) Asimov(I-writer) and(O) Carl(B-person) Sagan(I-person) joining(O) as(O) founding(O) members(O) .(O)"}}
{"id": "9", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "writer", "location", "book", "organization", "poem", "award", "person", "magazine", "country", "event"], "instance": {"id": "9", "words": ["Truffaut", "is", "admired", "among", "other", "filmmakers", "and", "several", "tributes", "to", "his", "work", "have", "appeared", "in", "other", "films", "such", "as", "Almost", "Famous", ",", "Face", "and", "The", "Diving", "Bell", "and", "the", "Butterfly", ",", "as", "well", "as", "novelist", "Haruki", "Murakami", "'", "s", "Kafka", "on", "the", "Shore", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, writer, location, book, organization, poem, award, person, magazine, country, event and O.\nSentence: Truffaut is admired among other filmmakers and several tributes to his work have appeared in other films such as Almost Famous , Face and The Diving Bell and the Butterfly , as well as novelist Haruki Murakami ' s Kafka on the Shore .", "prompt_labels": "Truffaut(B-writer) is(O) admired(O) among(O) other(O) filmmakers(O) and(O) several(O) tributes(O) to(O) his(O) work(O) have(O) appeared(O) in(O) other(O) films(O) such(O) as(O) Almost(O) Famous(O) ,(O) Face(O) and(O) The(O) Diving(O) Bell(O) and(O) the(O) Butterfly(O) ,(O) as(O) well(O) as(O) novelist(O) Haruki(B-writer) Murakami(I-writer) '(O) s(O) Kafka(B-book) on(I-book) the(I-book) Shore(I-book) .(O)"}}
{"id": "0", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "event", "writer", "location", "poem", "organization", "book", "magazine", "person", "award", "country"], "instance": {"id": "0", "words": ["Two", "decades", "after", "Frank", "Herbert", "'s", "death", ",", "his", "son", "Brian", "Herbert", ",", "along", "with", "Kevin", "J.", "Anderson", ",", "published", "two", "sequel", "s", "-", "Hunters", "of", "Dune", "(", "2006", ")", "and", "Sandworms", "of", "Dune", "(", "2007", ")", "-", "based", "on", "notes", "left", "behind", "by", "Frank", "Herbert", "for", "what", "he", "referred", "to", "as", "Dune", "7", ",", "his", "own", "planned", "seventh", "novel", "in", "the", "Dune", "series", "."], "labels": ["O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, event, writer, location, poem, organization, book, magazine, person, award, country and O.\nSentence: Two decades after Frank Herbert 's death , his son Brian Herbert , along with Kevin J. Anderson , published two sequel s - Hunters of Dune ( 2006 ) and Sandworms of Dune ( 2007 ) - based on notes left behind by Frank Herbert for what he referred to as Dune 7 , his own planned seventh novel in the Dune series .", "prompt_labels": "Two(O) decades(O) after(O) Frank(B-writer) Herbert(I-writer) 's(O) death(O) ,(O) his(O) son(O) Brian(B-writer) Herbert(I-writer) ,(O) along(O) with(O) Kevin(B-writer) J.(I-writer) Anderson(I-writer) ,(O) published(O) two(O) sequel(O) s(O) -(O) Hunters(B-book) of(I-book) Dune(I-book) ((O) 2006(O) )(O) and(O) Sandworms(B-book) of(I-book) Dune(I-book) ((O) 2007(O) )(O) -(O) based(O) on(O) notes(O) left(O) behind(O) by(O) Frank(B-writer) Herbert(I-writer) for(O) what(O) he(O) referred(O) to(O) as(O) Dune(B-book) 7(I-book) ,(O) his(O) own(O) planned(O) seventh(O) novel(B-literary genre) in(O) the(O) Dune(O) series(O) .(O)"}}
{"id": "97", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "poem", "person", "location", "literary genre", "country", "magazine", "organization", "writer", "award", "book"], "instance": {"id": "97", "words": ["During", "his", "visits", "to", "England", "in", "these", "years", ",", "Swift", "published", "A", "Tale", "of", "a", "Tub", "and", "The", "Battle", "of", "the", "Books", "(", "1704", ")", "and", "began", "to", "gain", "a", "reputation", "as", "a", "writer", "."], "labels": ["O", "O", "O", "O", "B-country", "O", "O", "O", "O", "B-writer", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, poem, person, location, literary genre, country, magazine, organization, writer, award, book and O.\nSentence: During his visits to England in these years , Swift published A Tale of a Tub and The Battle of the Books ( 1704 ) and began to gain a reputation as a writer .", "prompt_labels": "During(O) his(O) visits(O) to(O) England(B-country) in(O) these(O) years(O) ,(O) Swift(B-writer) published(O) A(B-book) Tale(I-book) of(I-book) a(I-book) Tub(I-book) and(O) The(B-book) Battle(I-book) of(I-book) the(I-book) Books(I-book) ((O) 1704(O) )(O) and(O) began(O) to(O) gain(O) a(O) reputation(O) as(O) a(O) writer(O) .(O)"}}
{"id": "46", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "location", "award", "country", "literary genre", "writer", "person", "magazine", "organization", "poem", "event"], "instance": {"id": "46", "words": ["His", "best", "known", "works", "include", "The", "Day", "of", "the", "Triffids", "(", "1951", ")", "and", "The", "Midwich", "Cuckoos", "(", "1957", ")", ",", "the", "latter", "filmed", "twice", "as", "Village", "of", "the", "Damned", "."], "labels": ["O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, location, award, country, literary genre, writer, person, magazine, organization, poem, event and O.\nSentence: His best known works include The Day of the Triffids ( 1951 ) and The Midwich Cuckoos ( 1957 ) , the latter filmed twice as Village of the Damned .", "prompt_labels": "His(O) best(O) known(O) works(O) include(O) The(B-book) Day(I-book) of(I-book) the(I-book) Triffids(I-book) ((O) 1951(O) )(O) and(O) The(B-book) Midwich(I-book) Cuckoos(I-book) ((O) 1957(O) )(O) ,(O) the(O) latter(O) filmed(O) twice(O) as(O) Village(O) of(O) the(O) Damned(O) .(O)"}}
{"id": "316", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "award", "writer", "event", "magazine", "book", "country", "organization", "location", "poem", "literary genre"], "instance": {"id": "316", "words": ["Conversely", ",", "Joe", "Haldeman", "'", "s", "1974", "anti-war", ",", "Hugo", "Award", "-", "and", "Nebula", "Award", "-winning", "science", "fiction", "novel", "The", "Forever", "War", "is", "popularly", "thought", "to", "be", "a", "direct", "reply", "to", "Starship", "Troopers", ",", "and", "though", "Haldeman", "has", "stated", "that", "it", "is", "actually", "a", "result", "of", "his", "personal", "experiences", "in", "the", "Vietnam", "War", ",", "he", "has", "admitted", "to", "being", "influenced", "by", "Starship", "Troopers", "."], "labels": ["O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "O", "B-literary genre", "I-literary genre", "I-literary genre", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, writer, event, magazine, book, country, organization, location, poem, literary genre and O.\nSentence: Conversely , Joe Haldeman ' s 1974 anti-war , Hugo Award - and Nebula Award -winning science fiction novel The Forever War is popularly thought to be a direct reply to Starship Troopers , and though Haldeman has stated that it is actually a result of his personal experiences in the Vietnam War , he has admitted to being influenced by Starship Troopers .", "prompt_labels": "Conversely(O) ,(O) Joe(B-writer) Haldeman(I-writer) '(O) s(O) 1974(O) anti-war(O) ,(O) Hugo(B-award) Award(I-award) -(O) and(O) Nebula(B-award) Award(I-award) -winning(O) science(B-literary genre) fiction(I-literary genre) novel(I-literary genre) The(B-book) Forever(I-book) War(I-book) is(O) popularly(O) thought(O) to(O) be(O) a(O) direct(O) reply(O) to(O) Starship(B-book) Troopers(I-book) ,(O) and(O) though(O) Haldeman(B-writer) has(O) stated(O) that(O) it(O) is(O) actually(O) a(O) result(O) of(O) his(O) personal(O) experiences(O) in(O) the(O) Vietnam(B-event) War(I-event) ,(O) he(O) has(O) admitted(O) to(O) being(O) influenced(O) by(O) Starship(B-book) Troopers(I-book) .(O)"}}
{"id": "44", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "writer", "organization", "country", "award", "literary genre", "poem", "person", "event", "magazine", "location"], "instance": {"id": "44", "words": ["Four", "more", "children", "followed", ":", "Charlotte", "Brontë", ",", "(", "1816-1855", ")", ",", "Branwell", "Brontë", "(", "1817-1848", ")", ",", "Emily", "Brontë", ",", "(", "1818-1848", ")", "and", "Anne", "(", "1820-1849", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, writer, organization, country, award, literary genre, poem, person, event, magazine, location and O.\nSentence: Four more children followed : Charlotte Brontë , ( 1816-1855 ) , Branwell Brontë ( 1817-1848 ) , Emily Brontë , ( 1818-1848 ) and Anne ( 1820-1849 ) .", "prompt_labels": "Four(O) more(O) children(O) followed(O) :(O) Charlotte(B-writer) Brontë(I-writer) ,(O) ((O) 1816-1855(O) )(O) ,(O) Branwell(B-writer) Brontë(I-writer) ((O) 1817-1848(O) )(O) ,(O) Emily(B-writer) Brontë(I-writer) ,(O) ((O) 1818-1848(O) )(O) and(O) Anne(O) ((O) 1820-1849(O) )(O) .(O)"}}
{"id": "49", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "organization", "literary genre", "magazine", "event", "poem", "person", "book", "country", "writer", "location"], "instance": {"id": "49", "words": ["In", "addition", "to", "his", "Best", "Picture", "Awards", ",", "he", "received", "an", "Academy", "Honorary", "Award", "for", "his", "film", "contributions", ",", "the", "Palme", "d", "'Or", "(", "posthumously", ")", "for", "Union", "Pacific", "(", "1939", ")", ",", "a", "Directors", "Guild", "of", "America", "Award", "for", "Lifetime", "Achievement", ",", "and", "the", "Irving", "G.", "Thalberg", "Memorial", "Award", "."], "labels": ["O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, literary genre, magazine, event, poem, person, book, country, writer, location and O.\nSentence: In addition to his Best Picture Awards , he received an Academy Honorary Award for his film contributions , the Palme d 'Or ( posthumously ) for Union Pacific ( 1939 ) , a Directors Guild of America Award for Lifetime Achievement , and the Irving G. Thalberg Memorial Award .", "prompt_labels": "In(O) addition(O) to(O) his(O) Best(B-award) Picture(I-award) Awards(I-award) ,(O) he(O) received(O) an(O) Academy(B-award) Honorary(I-award) Award(I-award) for(O) his(O) film(O) contributions(O) ,(O) the(O) Palme(B-award) d(I-award) 'Or(I-award) ((O) posthumously(O) )(O) for(O) Union(B-organization) Pacific(I-organization) ((O) 1939(O) )(O) ,(O) a(O) Directors(B-award) Guild(I-award) of(I-award) America(I-award) Award(I-award) for(I-award) Lifetime(I-award) Achievement(I-award) ,(O) and(O) the(O) Irving(B-award) G.(I-award) Thalberg(I-award) Memorial(I-award) Award(I-award) .(O)"}}
{"id": "325", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "location", "poem", "country", "writer", "organization", "event", "literary genre", "person", "book", "award"], "instance": {"id": "325", "words": ["Tallulah", "premiered", "at", "the", "2016", "Sundance", "Film", "Festival", "on", "January", "23", ",", "2016", "and", "was", "released", "on", "Netflix", "on", "July", "29", ",", "2016", "."], "labels": ["O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, poem, country, writer, organization, event, literary genre, person, book, award and O.\nSentence: Tallulah premiered at the 2016 Sundance Film Festival on January 23 , 2016 and was released on Netflix on July 29 , 2016 .", "prompt_labels": "Tallulah(O) premiered(O) at(O) the(O) 2016(B-event) Sundance(I-event) Film(I-event) Festival(I-event) on(O) January(O) 23(O) ,(O) 2016(O) and(O) was(O) released(O) on(O) Netflix(B-organization) on(O) July(O) 29(O) ,(O) 2016(O) .(O)"}}
{"id": "164", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "poem", "book", "country", "magazine", "award", "writer", "literary genre", "event", "location", "organization"], "instance": {"id": "164", "words": ["From", "Adam", "Bede", "to", "The", "Mill", "on", "the", "Floss", "and", "Silas", "Marner", ",", "Eliot", "presented", "the", "cases", "of", "social", "outsiders", "and", "small-town", "persecution", "."], "labels": ["O", "B-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, poem, book, country, magazine, award, writer, literary genre, event, location, organization and O.\nSentence: From Adam Bede to The Mill on the Floss and Silas Marner , Eliot presented the cases of social outsiders and small-town persecution .", "prompt_labels": "From(O) Adam(B-book) Bede(I-book) to(O) The(B-book) Mill(I-book) on(I-book) the(I-book) Floss(I-book) and(O) Silas(B-book) Marner(I-book) ,(O) Eliot(B-writer) presented(O) the(O) cases(O) of(O) social(O) outsiders(O) and(O) small-town(O) persecution(O) .(O)"}}
{"id": "333", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "organization", "literary genre", "poem", "location", "award", "person", "country", "magazine", "event", "writer"], "instance": {"id": "333", "words": ["He", "also", "continued", "acting", ";", "his", "roles", "included", "Major", "Jack", "Celliers", "in", "Merry", "Christmas", ",", "Mr.", "Lawrence", "(", "1983", ")", ",", "Jareth", "the", "Goblin", "King", "in", "Labyrinth", "(", "1986", ")", ",", "Pontius", "Pilate", "in", "The", "Last", "Temptation", "of", "Christ", "(", "1988", ")", ",", "and", "Nikola", "Tesla", "in", "The", "Prestige", "(", "2006", ")", ",", "among", "other", "film", "and", "television", "appearances", "and", "cameos", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, organization, literary genre, poem, location, award, person, country, magazine, event, writer and O.\nSentence: He also continued acting ; his roles included Major Jack Celliers in Merry Christmas , Mr. Lawrence ( 1983 ) , Jareth the Goblin King in Labyrinth ( 1986 ) , Pontius Pilate in The Last Temptation of Christ ( 1988 ) , and Nikola Tesla in The Prestige ( 2006 ) , among other film and television appearances and cameos .", "prompt_labels": "He(O) also(O) continued(O) acting(O) ;(O) his(O) roles(O) included(O) Major(B-person) Jack(I-person) Celliers(I-person) in(O) Merry(O) Christmas(O) ,(O) Mr.(O) Lawrence(O) ((O) 1983(O) )(O) ,(O) Jareth(B-person) the(O) Goblin(O) King(O) in(O) Labyrinth(O) ((O) 1986(O) )(O) ,(O) Pontius(B-person) Pilate(I-person) in(O) The(O) Last(O) Temptation(O) of(O) Christ(O) ((O) 1988(O) )(O) ,(O) and(O) Nikola(B-person) Tesla(I-person) in(O) The(O) Prestige(O) ((O) 2006(O) )(O) ,(O) among(O) other(O) film(O) and(O) television(O) appearances(O) and(O) cameos(O) .(O)"}}
{"id": "202", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "award", "location", "writer", "event", "magazine", "organization", "country", "poem", "person", "literary genre"], "instance": {"id": "202", "words": ["In", "her", "later", "years", ",", "she", "won", "her", "third", "Academy", "Award", ",", "this", "one", "for", "Academy", "Award", "for", "Best", "Supporting", "Actress", ",", "for", "her", "small", "performance", "in", "Murder", "on", "the", "Orient", "Express", "(", "1974", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, award, location, writer, event, magazine, organization, country, poem, person, literary genre and O.\nSentence: In her later years , she won her third Academy Award , this one for Academy Award for Best Supporting Actress , for her small performance in Murder on the Orient Express ( 1974 ) .", "prompt_labels": "In(O) her(O) later(O) years(O) ,(O) she(O) won(O) her(O) third(O) Academy(B-award) Award(I-award) ,(O) this(O) one(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actress(I-award) ,(O) for(O) her(O) small(O) performance(O) in(O) Murder(B-book) on(I-book) the(I-book) Orient(I-book) Express(I-book) ((O) 1974(O) )(O) .(O)"}}
{"id": "173", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "literary genre", "location", "magazine", "organization", "writer", "award", "poem", "country", "person", "event"], "instance": {"id": "173", "words": ["The", "friendship", "between", "Graves", "and", "Sassoon", "is", "documented", "in", "Graves", "'s", "letters", "and", "biographies", ";", "the", "story", "is", "fictionalised", "in", "Pat", "Barker", "'", "s", "novel", "Regeneration", "."], "labels": ["O", "O", "O", "B-writer", "O", "B-writer", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-literary genre", "B-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, literary genre, location, magazine, organization, writer, award, poem, country, person, event and O.\nSentence: The friendship between Graves and Sassoon is documented in Graves 's letters and biographies ; the story is fictionalised in Pat Barker ' s novel Regeneration .", "prompt_labels": "The(O) friendship(O) between(O) Graves(B-writer) and(O) Sassoon(B-writer) is(O) documented(O) in(O) Graves(B-writer) 's(O) letters(O) and(O) biographies(O) ;(O) the(O) story(O) is(O) fictionalised(O) in(O) Pat(B-writer) Barker(I-writer) '(O) s(O) novel(B-literary genre) Regeneration(B-book) .(O)"}}
{"id": "206", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "location", "event", "award", "writer", "poem", "person", "organization", "magazine", "literary genre", "book"], "instance": {"id": "206", "words": ["As", "of", "June", "2018", ",", "Amnesty", "International", "and", "the", "Inter-American", "Commission", "on", "Human", "Rights", "of", "the", "Organization", "of", "American", "States", "have", "reported", "that", "Ortega", "has", "engaged", "in", "a", "violent", "oppression", "campaign", "against", "protesters", "in", "response", "to", "anti-Ortega", "protests", "since", "April", "2018", ",", "while", "government", "officials", "and", "government-owned", "media", "have", "denied", "such", "actions", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, event, award, writer, poem, person, organization, magazine, literary genre, book and O.\nSentence: As of June 2018 , Amnesty International and the Inter-American Commission on Human Rights of the Organization of American States have reported that Ortega has engaged in a violent oppression campaign against protesters in response to anti-Ortega protests since April 2018 , while government officials and government-owned media have denied such actions .", "prompt_labels": "As(O) of(O) June(O) 2018(O) ,(O) Amnesty(B-organization) International(I-organization) and(O) the(O) Inter-American(B-organization) Commission(I-organization) on(I-organization) Human(I-organization) Rights(I-organization) of(O) the(O) Organization(B-organization) of(I-organization) American(I-organization) States(I-organization) have(O) reported(O) that(O) Ortega(B-person) has(O) engaged(O) in(O) a(O) violent(O) oppression(O) campaign(O) against(O) protesters(O) in(O) response(O) to(O) anti-Ortega(B-event) protests(I-event) since(O) April(O) 2018(O) ,(O) while(O) government(O) officials(O) and(O) government-owned(O) media(O) have(O) denied(O) such(O) actions(O) .(O)"}}
{"id": "267", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "magazine", "country", "event", "person", "location", "award", "organization", "poem", "book", "literary genre"], "instance": {"id": "267", "words": ["The", "episode", "depicts", "the", "attempted", "assassination", "of", "Warhol", "by", "Valerie", "Solanas", "(", "Lena", "Dunham", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "B-writer", "I-writer", "O", "B-person", "I-person", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, magazine, country, event, person, location, award, organization, poem, book, literary genre and O.\nSentence: The episode depicts the attempted assassination of Warhol by Valerie Solanas ( Lena Dunham ) .", "prompt_labels": "The(O) episode(O) depicts(O) the(O) attempted(O) assassination(O) of(O) Warhol(B-writer) by(O) Valerie(B-writer) Solanas(I-writer) ((O) Lena(B-person) Dunham(I-person) )(O) .(O)"}}
{"id": "386", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "country", "magazine", "writer", "literary genre", "person", "event", "book", "award", "poem", "organization"], "instance": {"id": "386", "words": ["Like", "The", "Black", "Diamonds", ",", "it", "uses", "a", "medieval", ",", "Arabian", "Nights", "-like", "setting", ",", "and", "the", "Arabian", "Nights", ",", "like", "the", "fairy", "tales", "of", "the", "Brothers", "Grimm", "and", "the", "works", "of", "Edgar", "Allan", "Poe", ",", "are", "known", "to", "have", "strongly", "influenced", "Smith", "'s", "early", "writing", ",", "as", "did", "William", "Beckford", "'", "s", "Vathek", "."], "labels": ["O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, magazine, writer, literary genre, person, event, book, award, poem, organization and O.\nSentence: Like The Black Diamonds , it uses a medieval , Arabian Nights -like setting , and the Arabian Nights , like the fairy tales of the Brothers Grimm and the works of Edgar Allan Poe , are known to have strongly influenced Smith 's early writing , as did William Beckford ' s Vathek .", "prompt_labels": "Like(O) The(B-book) Black(I-book) Diamonds(I-book) ,(O) it(O) uses(O) a(O) medieval(O) ,(O) Arabian(B-book) Nights(I-book) -like(O) setting(O) ,(O) and(O) the(O) Arabian(B-book) Nights(I-book) ,(O) like(O) the(O) fairy(B-literary genre) tales(I-literary genre) of(O) the(B-writer) Brothers(I-writer) Grimm(I-writer) and(O) the(O) works(O) of(O) Edgar(B-writer) Allan(I-writer) Poe(I-writer) ,(O) are(O) known(O) to(O) have(O) strongly(O) influenced(O) Smith(B-writer) 's(O) early(O) writing(O) ,(O) as(O) did(O) William(B-writer) Beckford(I-writer) '(O) s(O) Vathek(B-book) .(O)"}}
{"id": "66", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "literary genre", "event", "poem", "writer", "country", "location", "person", "award", "book", "organization"], "instance": {"id": "66", "words": ["Parodies", "of", "courtly", "poetry", "also", "exist", ",", "among", "them", "Der", "Weinschwelg", "."], "labels": ["O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "B-poem", "I-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, literary genre, event, poem, writer, country, location, person, award, book, organization and O.\nSentence: Parodies of courtly poetry also exist , among them Der Weinschwelg .", "prompt_labels": "Parodies(O) of(O) courtly(B-literary genre) poetry(I-literary genre) also(O) exist(O) ,(O) among(O) them(O) Der(B-poem) Weinschwelg(I-poem) .(O)"}}
{"id": "251", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "location", "organization", "event", "country", "writer", "award", "magazine", "poem", "person", "book"], "instance": {"id": "251", "words": ["For", "his", "work", ",", "he", "received", "numerous", "accolades", ",", "including", "six", "Primetime", "Emmy", "Award", "s", ",", "a", "Tony", "Award", ",", "a", "Mark", "Twain", "Prize", "and", "a", "star", "on", "the", "Hollywood", "Walk", "of", "Fame", "in", "1991", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, location, organization, event, country, writer, award, magazine, poem, person, book and O.\nSentence: For his work , he received numerous accolades , including six Primetime Emmy Award s , a Tony Award , a Mark Twain Prize and a star on the Hollywood Walk of Fame in 1991 .", "prompt_labels": "For(O) his(O) work(O) ,(O) he(O) received(O) numerous(O) accolades(O) ,(O) including(O) six(O) Primetime(B-award) Emmy(I-award) Award(I-award) s(O) ,(O) a(O) Tony(B-award) Award(I-award) ,(O) a(O) Mark(B-award) Twain(I-award) Prize(I-award) and(O) a(O) star(O) on(O) the(O) Hollywood(B-location) Walk(I-location) of(I-location) Fame(I-location) in(O) 1991(O) .(O)"}}
{"id": "255", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "country", "organization", "writer", "person", "literary genre", "award", "magazine", "poem", "book", "location"], "instance": {"id": "255", "words": ["Sometimes", "a", "Great", "Notion", "inspired", "a", "1970", "film", "starring", "and", "directed", "by", "Paul", "Newman", ";", "it", "was", "nominated", "for", "two", "Academy", "Awards", ",", "and", "in", "1972", "was", "the", "first", "film", "shown", "by", "the", "new", "television", "network", "HBO", ","], "labels": ["B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, organization, writer, person, literary genre, award, magazine, poem, book, location and O.\nSentence: Sometimes a Great Notion inspired a 1970 film starring and directed by Paul Newman ; it was nominated for two Academy Awards , and in 1972 was the first film shown by the new television network HBO ,", "prompt_labels": "Sometimes(B-book) a(I-book) Great(I-book) Notion(I-book) inspired(O) a(O) 1970(O) film(O) starring(O) and(O) directed(O) by(O) Paul(B-person) Newman(I-person) ;(O) it(O) was(O) nominated(O) for(O) two(O) Academy(B-award) Awards(I-award) ,(O) and(O) in(O) 1972(O) was(O) the(O) first(O) film(O) shown(O) by(O) the(O) new(O) television(O) network(O) HBO(B-organization) ,(O)"}}
{"id": "52", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "book", "magazine", "poem", "award", "literary genre", "location", "event", "writer", "person", "country"], "instance": {"id": "52", "words": ["Powers", "'", "first", "major", "novel", "was", "The", "Drawing", "of", "the", "Dark", "(", "1979", ")", ",", "but", "the", "novel", "that", "earned", "him", "wide", "praise", "was", "The", "Anubis", "Gates", ",", "which", "won", "the", "Philip", "K.", "Dick", "Award", ",", "and", "has", "since", "been", "published", "in", "many", "other", "languages", "."], "labels": ["B-writer", "O", "O", "O", "B-literary genre", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, book, magazine, poem, award, literary genre, location, event, writer, person, country and O.\nSentence: Powers ' first major novel was The Drawing of the Dark ( 1979 ) , but the novel that earned him wide praise was The Anubis Gates , which won the Philip K. Dick Award , and has since been published in many other languages .", "prompt_labels": "Powers(B-writer) '(O) first(O) major(O) novel(B-literary genre) was(O) The(B-book) Drawing(I-book) of(I-book) the(I-book) Dark(I-book) ((O) 1979(O) )(O) ,(O) but(O) the(O) novel(B-literary genre) that(O) earned(O) him(O) wide(O) praise(O) was(O) The(B-book) Anubis(I-book) Gates(I-book) ,(O) which(O) won(O) the(O) Philip(B-award) K.(I-award) Dick(I-award) Award(I-award) ,(O) and(O) has(O) since(O) been(O) published(O) in(O) many(O) other(O) languages(O) .(O)"}}
{"id": "172", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "event", "person", "magazine", "writer", "literary genre", "country", "location", "award", "organization", "book"], "instance": {"id": "172", "words": ["However", ",", "in", "the", "parallel", "poem", "The", "Greene", "Knight", ",", "the", "lace", "is", "white", ",", "not", "green", ",", "and", "is", "considered", "the", "origin", "of", "the", "collar", "worn", "by", "the", "knights", "of", "the", "Bath", ",", "not", "the", "Order", "of", "the", "Garter", "."], "labels": ["O", "O", "O", "O", "B-literary genre", "I-literary genre", "B-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, event, person, magazine, writer, literary genre, country, location, award, organization, book and O.\nSentence: However , in the parallel poem The Greene Knight , the lace is white , not green , and is considered the origin of the collar worn by the knights of the Bath , not the Order of the Garter .", "prompt_labels": "However(O) ,(O) in(O) the(O) parallel(B-literary genre) poem(I-literary genre) The(B-poem) Greene(I-poem) Knight(I-poem) ,(O) the(O) lace(O) is(O) white(O) ,(O) not(O) green(O) ,(O) and(O) is(O) considered(O) the(O) origin(O) of(O) the(O) collar(O) worn(O) by(O) the(O) knights(O) of(O) the(O) Bath(O) ,(O) not(O) the(O) Order(O) of(O) the(O) Garter(O) .(O)"}}
{"id": "36", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "country", "event", "organization", "magazine", "location", "poem", "book", "writer", "award", "literary genre"], "instance": {"id": "36", "words": ["Jonathan", "Lethem", ",", "in", "a", "1998", "essay", "in", "the", "Village", "Voice", "entitled", "Close", "Encounters", ":", "The", "Squandered", "Promise", "of", "Science", "Fiction", ",", "suggested", "that", "the", "point", "in", "1973", "when", "Thomas", "Pynchon", "Gravity", "'s", "Rainbow", "was", "nominated", "for", "the", "Nebula", "Award", "and", "was", "passed", "over", "in", "favor", "of", "Clarke", "'s", "Rendezvous", "with", "Rama", ",", "stands", "as", "a", "hidden", "tombstone", "marking", "the", "death", "of", "the", "hope", "that", "SF", "was", "about", "to", "merge", "with", "the", "mainstream", "."], "labels": ["B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "B-book", "I-book", "I-book", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, event, organization, magazine, location, poem, book, writer, award, literary genre and O.\nSentence: Jonathan Lethem , in a 1998 essay in the Village Voice entitled Close Encounters : The Squandered Promise of Science Fiction , suggested that the point in 1973 when Thomas Pynchon Gravity 's Rainbow was nominated for the Nebula Award and was passed over in favor of Clarke 's Rendezvous with Rama , stands as a hidden tombstone marking the death of the hope that SF was about to merge with the mainstream .", "prompt_labels": "Jonathan(B-writer) Lethem(I-writer) ,(O) in(O) a(O) 1998(O) essay(O) in(O) the(O) Village(B-magazine) Voice(I-magazine) entitled(O) Close(B-book) Encounters(I-book) :(I-book) The(I-book) Squandered(I-book) Promise(I-book) of(I-book) Science(I-book) Fiction(I-book) ,(O) suggested(O) that(O) the(O) point(O) in(O) 1973(O) when(O) Thomas(B-writer) Pynchon(I-writer) Gravity(B-book) 's(I-book) Rainbow(I-book) was(O) nominated(O) for(O) the(O) Nebula(B-award) Award(I-award) and(O) was(O) passed(O) over(O) in(O) favor(O) of(O) Clarke(B-writer) 's(O) Rendezvous(B-book) with(I-book) Rama(I-book) ,(O) stands(O) as(O) a(O) hidden(O) tombstone(O) marking(O) the(O) death(O) of(O) the(O) hope(O) that(O) SF(B-literary genre) was(O) about(O) to(O) merge(O) with(O) the(O) mainstream(O) .(O)"}}
{"id": "262", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "magazine", "literary genre", "writer", "person", "event", "location", "award", "country", "organization", "book"], "instance": {"id": "262", "words": ["Romanticism", "began", "in", "Portugal", "with", "the", "publication", "of", "the", "poem", "Camões", "(", "1825", ")", ",", "by", "Almeida", "Garrett", ",", "who", "was", "raised", "by", "his", "uncle", "D.", "Alexandre", ",", "bishop", "of", "Angra", ",", "in", "the", "precepts", "of", "Neoclassicism", ",", "which", "can", "be", "observed", "in", "his", "early", "work", "."], "labels": ["B-literary genre", "O", "O", "B-country", "O", "O", "O", "O", "O", "B-literary genre", "B-book", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, magazine, literary genre, writer, person, event, location, award, country, organization, book and O.\nSentence: Romanticism began in Portugal with the publication of the poem Camões ( 1825 ) , by Almeida Garrett , who was raised by his uncle D. Alexandre , bishop of Angra , in the precepts of Neoclassicism , which can be observed in his early work .", "prompt_labels": "Romanticism(B-literary genre) began(O) in(O) Portugal(B-country) with(O) the(O) publication(O) of(O) the(O) poem(B-literary genre) Camões(B-book) ((O) 1825(O) )(O) ,(O) by(O) Almeida(B-writer) Garrett(I-writer) ,(O) who(O) was(O) raised(O) by(O) his(O) uncle(O) D.(B-person) Alexandre(I-person) ,(O) bishop(O) of(O) Angra(O) ,(O) in(O) the(O) precepts(O) of(O) Neoclassicism(B-literary genre) ,(O) which(O) can(O) be(O) observed(O) in(O) his(O) early(O) work(O) .(O)"}}
{"id": "182", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "award", "poem", "event", "country", "writer", "magazine", "organization", "person", "book", "literary genre"], "instance": {"id": "182", "words": ["In", "Patricia", "Wrede", "'", "s", "Regency", "fantasies", ",", "Great", "Britain", "has", "a", "Royal", "Society", "of", "Wizards", ",", "and", "in", "Poul", "Anderson", "'s", "A", "Midsummer", "Tempest", "William", "Shakespeare", "is", "remembered", "as", "the", "Great", "Historian", ",", "with", "the", "novel", "itself", "taking", "place", "in", "the", "era", "of", "Oliver", "Cromwell", "and", "Charles", "I", ",", "with", "an", "alternate", "outcome", "for", "the", "English", "Civil", "War", "and", "an", "earlier", "Industrial", "Revolution", "."], "labels": ["O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-writer", "I-writer", "O", "B-book", "I-book", "I-book", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "B-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, poem, event, country, writer, magazine, organization, person, book, literary genre and O.\nSentence: In Patricia Wrede ' s Regency fantasies , Great Britain has a Royal Society of Wizards , and in Poul Anderson 's A Midsummer Tempest William Shakespeare is remembered as the Great Historian , with the novel itself taking place in the era of Oliver Cromwell and Charles I , with an alternate outcome for the English Civil War and an earlier Industrial Revolution .", "prompt_labels": "In(O) Patricia(B-writer) Wrede(I-writer) '(O) s(O) Regency(B-book) fantasies(I-book) ,(O) Great(O) Britain(O) has(O) a(O) Royal(B-organization) Society(I-organization) of(I-organization) Wizards(I-organization) ,(O) and(O) in(O) Poul(B-writer) Anderson(I-writer) 's(O) A(B-book) Midsummer(I-book) Tempest(I-book) William(B-writer) Shakespeare(I-writer) is(O) remembered(O) as(O) the(O) Great(O) Historian(O) ,(O) with(O) the(O) novel(B-literary genre) itself(O) taking(O) place(O) in(O) the(O) era(O) of(O) Oliver(B-person) Cromwell(I-person) and(O) Charles(B-person) I(I-person) ,(O) with(O) an(O) alternate(O) outcome(O) for(O) the(O) English(B-event) Civil(I-event) War(I-event) and(O) an(O) earlier(O) Industrial(B-event) Revolution(I-event) .(O)"}}
{"id": "362", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "literary genre", "magazine", "award", "poem", "event", "person", "writer", "location", "organization", "country"], "instance": {"id": "362", "words": ["The", "film", "premiered", "at", "the", "Venice", "Film", "Festival", "where", "it", "won", "the", "festival", "'s", "Mimmo", "Rotella", "Foundation", "Award", "."], "labels": ["O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, literary genre, magazine, award, poem, event, person, writer, location, organization, country and O.\nSentence: The film premiered at the Venice Film Festival where it won the festival 's Mimmo Rotella Foundation Award .", "prompt_labels": "The(O) film(O) premiered(O) at(O) the(O) Venice(B-event) Film(I-event) Festival(I-event) where(O) it(O) won(O) the(O) festival(O) 's(O) Mimmo(B-award) Rotella(I-award) Foundation(I-award) Award(I-award) .(O)"}}
{"id": "216", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "organization", "poem", "literary genre", "book", "award", "writer", "magazine", "event", "country", "location"], "instance": {"id": "216", "words": ["The", "first", "professional", "critic", "to", "comment", "on", "Howard", "'s", "work", "was", "Hoffman", "Reynolds", "Hays", ",", "reviewing", "the", "Arkham", "House", "collection", "Skull-Face", "and", "Others", "in", "The", "New", "York", "Times", "Book", "Review", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "B-organization", "I-organization", "O", "B-book", "I-book", "I-book", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, poem, literary genre, book, award, writer, magazine, event, country, location and O.\nSentence: The first professional critic to comment on Howard 's work was Hoffman Reynolds Hays , reviewing the Arkham House collection Skull-Face and Others in The New York Times Book Review .", "prompt_labels": "The(O) first(O) professional(O) critic(O) to(O) comment(O) on(O) Howard(B-writer) 's(O) work(O) was(O) Hoffman(B-person) Reynolds(I-person) Hays(I-person) ,(O) reviewing(O) the(O) Arkham(B-organization) House(I-organization) collection(O) Skull-Face(B-book) and(I-book) Others(I-book) in(O) The(B-magazine) New(I-magazine) York(I-magazine) Times(I-magazine) Book(I-magazine) Review(I-magazine) .(O)"}}
{"id": "269", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "person", "organization", "poem", "writer", "book", "country", "award", "event", "magazine", "location"], "instance": {"id": "269", "words": ["For", "his", "100th", ",", "200th", ",", "and", "300th", "books", "(", "based", "on", "his", "personal", "count", ")", ",", "Asimov", "published", "Opus", "100", "(", "1969", ")", ",", "Opus", "200", "(", "1979", ")", ",", "and", "Opus", "300", "(", "1984", ")", ",", "celebrating", "his", "writing", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "B-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, person, organization, poem, writer, book, country, award, event, magazine, location and O.\nSentence: For his 100th , 200th , and 300th books ( based on his personal count ) , Asimov published Opus 100 ( 1969 ) , Opus 200 ( 1979 ) , and Opus 300 ( 1984 ) , celebrating his writing .", "prompt_labels": "For(O) his(O) 100th(O) ,(O) 200th(O) ,(O) and(O) 300th(O) books(O) ((O) based(O) on(O) his(O) personal(O) count(O) )(O) ,(O) Asimov(B-writer) published(O) Opus(B-book) 100(I-book) ((O) 1969(O) )(O) ,(O) Opus(B-book) 200(I-book) ((O) 1979(O) )(O) ,(O) and(O) Opus(B-book) 300(I-book) ((O) 1984(O) )(O) ,(O) celebrating(O) his(O) writing(O) .(O)"}}
{"id": "84", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "country", "magazine", "book", "event", "writer", "poem", "location", "literary genre", "award", "person"], "instance": {"id": "84", "words": ["Some", "of", "his", "best", "known", "poems", "are", "about", "love", ",", "such", "as", "Funeral", "Blues", ";", "on", "political", "and", "social", "themes", ",", "such", "as", "September", "1", ",", "1939", "and", "The", "Shield", "of", "Achilles", ";", "on", "cultural", "and", "psychological", "themes", ",", "such", "as", "The", "Age", "of", "Anxiety", ";", "and", "on", "religious", "themes", "such", "as", "For", "the", "Time", "Being", "and", "Horae", "Canonicae", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "B-poem", "I-poem", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "I-literary genre", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "O", "B-poem", "I-poem", "I-poem", "I-poem", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "I-literary genre", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "O", "B-poem", "I-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, magazine, book, event, writer, poem, location, literary genre, award, person and O.\nSentence: Some of his best known poems are about love , such as Funeral Blues ; on political and social themes , such as September 1 , 1939 and The Shield of Achilles ; on cultural and psychological themes , such as The Age of Anxiety ; and on religious themes such as For the Time Being and Horae Canonicae .", "prompt_labels": "Some(O) of(O) his(O) best(O) known(O) poems(O) are(O) about(O) love(B-literary genre) ,(O) such(O) as(O) Funeral(B-poem) Blues(I-poem) ;(O) on(O) political(B-literary genre) and(I-literary genre) social(I-literary genre) themes(I-literary genre) ,(O) such(O) as(O) September(B-poem) 1(I-poem) ,(I-poem) 1939(I-poem) and(O) The(B-poem) Shield(I-poem) of(I-poem) Achilles(I-poem) ;(O) on(O) cultural(B-literary genre) and(I-literary genre) psychological(I-literary genre) themes(I-literary genre) ,(O) such(O) as(O) The(B-poem) Age(I-poem) of(I-poem) Anxiety(I-poem) ;(O) and(O) on(O) religious(B-literary genre) themes(I-literary genre) such(O) as(O) For(B-poem) the(I-poem) Time(I-poem) Being(I-poem) and(O) Horae(B-poem) Canonicae(I-poem) .(O)"}}
{"id": "123", "dataset": "crossner_literature", "split": "test", "label_list": ["book", "magazine", "literary genre", "writer", "award", "event", "country", "poem", "organization", "person", "location"], "instance": {"id": "123", "words": ["However", ",", "in", "late", "1970", ",", "Pauline", "Kael", ",", "in", "her", "negative", "The", "New", "Yorker", "review", "of", "the", "Maysles", "'", "subsequent", "documentary", "Gimme", "Shelter", ",", "alleged", "that", "Salesman", "was", "set", "up", "and", "acted", "by", "its", "principals", ",", "rather", "than", "actually", "being", "direct", "cinema", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, magazine, literary genre, writer, award, event, country, poem, organization, person, location and O.\nSentence: However , in late 1970 , Pauline Kael , in her negative The New Yorker review of the Maysles ' subsequent documentary Gimme Shelter , alleged that Salesman was set up and acted by its principals , rather than actually being direct cinema .", "prompt_labels": "However(O) ,(O) in(O) late(O) 1970(O) ,(O) Pauline(B-writer) Kael(I-writer) ,(O) in(O) her(O) negative(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) review(O) of(O) the(O) Maysles(B-person) '(O) subsequent(O) documentary(O) Gimme(O) Shelter(O) ,(O) alleged(O) that(O) Salesman(O) was(O) set(O) up(O) and(O) acted(O) by(O) its(O) principals(O) ,(O) rather(O) than(O) actually(O) being(O) direct(O) cinema(O) .(O)"}}
{"id": "399", "dataset": "crossner_literature", "split": "test", "label_list": ["writer", "book", "award", "organization", "country", "poem", "event", "person", "magazine", "literary genre", "location"], "instance": {"id": "399", "words": ["Their", "first", "child", ",", "a", "daughter", ",", "Maria", "Teresa", ",", "was", "born", "on", "9", "March", "1938", "and", "a", "son", ",", "Auberon", "Waugh", ",", "on", "17", "November", "1939", "Hastings", ",", "pp.", "336", "and", "392", "Between", "these", "events", ",", "Scoop", "was", "published", "in", "May", "1938", "to", "wide", "critical", "acclaim.", "Stannard", ",", "Vol.", "i", "pp.", "470-71", "In", "August", "1938", "Waugh", ",", "with", "Laura", ",", "made", "a", "three-month", "trip", "to", "Mexico", "after", "which", "he", "wrote", "Robbery", "Under", "Law", ",", "based", "on", "his", "experiences", "there", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, award, organization, country, poem, event, person, magazine, literary genre, location and O.\nSentence: Their first child , a daughter , Maria Teresa , was born on 9 March 1938 and a son , Auberon Waugh , on 17 November 1939 Hastings , pp. 336 and 392 Between these events , Scoop was published in May 1938 to wide critical acclaim. Stannard , Vol. i pp. 470-71 In August 1938 Waugh , with Laura , made a three-month trip to Mexico after which he wrote Robbery Under Law , based on his experiences there .", "prompt_labels": "Their(O) first(O) child(O) ,(O) a(O) daughter(O) ,(O) Maria(B-person) Teresa(I-person) ,(O) was(O) born(O) on(O) 9(O) March(O) 1938(O) and(O) a(O) son(O) ,(O) Auberon(B-writer) Waugh(I-writer) ,(O) on(O) 17(O) November(O) 1939(O) Hastings(B-writer) ,(O) pp.(O) 336(O) and(O) 392(O) Between(O) these(O) events(O) ,(O) Scoop(B-book) was(O) published(O) in(O) May(O) 1938(O) to(O) wide(O) critical(O) acclaim.(O) Stannard(B-person) ,(O) Vol.(O) i(O) pp.(O) 470-71(O) In(O) August(O) 1938(O) Waugh(B-writer) ,(O) with(O) Laura(B-person) ,(O) made(O) a(O) three-month(O) trip(O) to(O) Mexico(B-country) after(O) which(O) he(O) wrote(O) Robbery(B-book) Under(I-book) Law(I-book) ,(O) based(O) on(O) his(O) experiences(O) there(O) .(O)"}}
{"id": "208", "dataset": "crossner_literature", "split": "test", "label_list": ["organization", "book", "location", "person", "writer", "event", "literary genre", "country", "award", "poem", "magazine"], "instance": {"id": "208", "words": ["During", "the", "Commonwealth", "of", "England", "period", "he", "was", "a", "colleague", "and", "friend", "of", "John", "Milton", "."], "labels": ["O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, book, location, person, writer, event, literary genre, country, award, poem, magazine and O.\nSentence: During the Commonwealth of England period he was a colleague and friend of John Milton .", "prompt_labels": "During(O) the(O) Commonwealth(B-organization) of(I-organization) England(I-organization) period(O) he(O) was(O) a(O) colleague(O) and(O) friend(O) of(O) John(B-writer) Milton(I-writer) .(O)"}}
{"id": "285", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "country", "location", "poem", "event", "book", "magazine", "award", "writer", "organization", "literary genre"], "instance": {"id": "285", "words": ["In", "1947", ",", "Miller", "'s", "play", "All", "My", "Sons", ",", "the", "writing", "of", "which", "had", "commenced", "in", "1941", ",", "was", "a", "success", "on", "Broadway", "(", "earning", "him", "his", "first", "Tony", "Award", ",", "Tony", "Award", "for", "Best", "Author", ")", "and", "his", "reputation", "as", "a", "playwright", "was", "established", "."], "labels": ["O", "O", "O", "B-person", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, location, poem, event, book, magazine, award, writer, organization, literary genre and O.\nSentence: In 1947 , Miller 's play All My Sons , the writing of which had commenced in 1941 , was a success on Broadway ( earning him his first Tony Award , Tony Award for Best Author ) and his reputation as a playwright was established .", "prompt_labels": "In(O) 1947(O) ,(O) Miller(B-person) 's(O) play(O) All(B-book) My(I-book) Sons(I-book) ,(O) the(O) writing(O) of(O) which(O) had(O) commenced(O) in(O) 1941(O) ,(O) was(O) a(O) success(O) on(O) Broadway(B-organization) ((O) earning(O) him(O) his(O) first(O) Tony(B-award) Award(I-award) ,(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Author(I-award) )(O) and(O) his(O) reputation(O) as(O) a(O) playwright(O) was(O) established(O) .(O)"}}
{"id": "152", "dataset": "crossner_literature", "split": "test", "label_list": ["location", "poem", "book", "country", "magazine", "writer", "person", "literary genre", "organization", "award", "event"], "instance": {"id": "152", "words": ["During", "this", "time", ",", "he", "made", "the", "short", "film", "I", "Am", "Joaquin", "based", "on", "the", "legendary", "poem", "by", "Rodolfo", "Corky", "Gonzáles", "(", "it", "was", "later", "inducted", "into", "the", "National", "Film", "Registry", "in", "2010", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "O", "O", "O", "O", "B-literary genre", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, poem, book, country, magazine, writer, person, literary genre, organization, award, event and O.\nSentence: During this time , he made the short film I Am Joaquin based on the legendary poem by Rodolfo Corky Gonzáles ( it was later inducted into the National Film Registry in 2010 ) .", "prompt_labels": "During(O) this(O) time(O) ,(O) he(O) made(O) the(O) short(O) film(O) I(B-poem) Am(I-poem) Joaquin(I-poem) based(O) on(O) the(O) legendary(O) poem(B-literary genre) by(O) Rodolfo(B-writer) Corky(I-writer) Gonzáles(I-writer) ((O) it(O) was(O) later(O) inducted(O) into(O) the(O) National(B-organization) Film(I-organization) Registry(I-organization) in(O) 2010(O) )(O) .(O)"}}
{"id": "309", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "magazine", "poem", "book", "writer", "country", "award", "location", "organization", "literary genre", "person"], "instance": {"id": "309", "words": ["Thorpe", "began", "competition", "in", "2002", "at", "the", "Australian", "Championships", "in", "Brisbane", "in", "March", ",", "which", "were", "used", "to", "select", "the", "team", "for", "the", "2002", "Commonwealth", "Games", "in", "Manchester", "and", "the", "2002", "Pan", "Pacific", "Swimming", "Championships", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "B-location", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, magazine, poem, book, writer, country, award, location, organization, literary genre, person and O.\nSentence: Thorpe began competition in 2002 at the Australian Championships in Brisbane in March , which were used to select the team for the 2002 Commonwealth Games in Manchester and the 2002 Pan Pacific Swimming Championships .", "prompt_labels": "Thorpe(B-person) began(O) competition(O) in(O) 2002(O) at(O) the(O) Australian(O) Championships(O) in(O) Brisbane(B-location) in(O) March(O) ,(O) which(O) were(O) used(O) to(O) select(O) the(O) team(O) for(O) the(O) 2002(B-event) Commonwealth(I-event) Games(I-event) in(O) Manchester(B-location) and(O) the(O) 2002(B-event) Pan(I-event) Pacific(I-event) Swimming(I-event) Championships(I-event) .(O)"}}
{"id": "308", "dataset": "crossner_literature", "split": "test", "label_list": ["award", "person", "organization", "location", "country", "event", "magazine", "book", "writer", "literary genre", "poem"], "instance": {"id": "308", "words": ["Rushdie", "wrote", "a", "non-fiction", "book", "about", "Nicaragua", "in", "1987", "called", "The", "Jaguar", "Smile", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, organization, location, country, event, magazine, book, writer, literary genre, poem and O.\nSentence: Rushdie wrote a non-fiction book about Nicaragua in 1987 called The Jaguar Smile .", "prompt_labels": "Rushdie(B-writer) wrote(O) a(O) non-fiction(O) book(O) about(O) Nicaragua(B-country) in(O) 1987(O) called(O) The(B-book) Jaguar(I-book) Smile(I-book) .(O)"}}
{"id": "380", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "book", "person", "award", "event", "writer", "location", "organization", "poem", "country", "magazine"], "instance": {"id": "380", "words": ["The", "most", "celebrated", "work", "of", "Ramapurathu", "Warrier", "is", "Kuchelavritham", "Vanchippattu", ",", "which", "depicts", "the", "story", "of", "Kuchela", ",", "a", "devotee", "and", "an", "old", "classmate", "of", "Krishna", ",", "going", "to", "Dwaraka", "to", "meet", "with", "him", "."], "labels": ["O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-poem", "I-poem", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "B-location", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, book, person, award, event, writer, location, organization, poem, country, magazine and O.\nSentence: The most celebrated work of Ramapurathu Warrier is Kuchelavritham Vanchippattu , which depicts the story of Kuchela , a devotee and an old classmate of Krishna , going to Dwaraka to meet with him .", "prompt_labels": "The(O) most(O) celebrated(O) work(O) of(O) Ramapurathu(B-writer) Warrier(I-writer) is(O) Kuchelavritham(B-poem) Vanchippattu(I-poem) ,(O) which(O) depicts(O) the(O) story(O) of(O) Kuchela(B-person) ,(O) a(O) devotee(O) and(O) an(O) old(O) classmate(O) of(O) Krishna(B-person) ,(O) going(O) to(O) Dwaraka(B-location) to(O) meet(O) with(O) him(O) .(O)"}}
{"id": "241", "dataset": "crossner_literature", "split": "test", "label_list": ["poem", "organization", "literary genre", "book", "country", "magazine", "event", "location", "person", "writer", "award"], "instance": {"id": "241", "words": ["Remment", "Koolhaas", ",", "usually", "abbreviated", "to", "Rem", "Koolhaas", ",", "was", "born", "on", "17", "November", "1944", "in", "Rotterdam", ",", "Netherlands", ",", "to", "Anton", "Koolhaas", "(", "1912-1992", ")", "and", "Selinde", "Pietertje", "Roosenburg", "(", "born", "1920", ")", "."], "labels": ["B-person", "I-person", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-country", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, organization, literary genre, book, country, magazine, event, location, person, writer, award and O.\nSentence: Remment Koolhaas , usually abbreviated to Rem Koolhaas , was born on 17 November 1944 in Rotterdam , Netherlands , to Anton Koolhaas ( 1912-1992 ) and Selinde Pietertje Roosenburg ( born 1920 ) .", "prompt_labels": "Remment(B-person) Koolhaas(I-person) ,(O) usually(O) abbreviated(O) to(O) Rem(B-person) Koolhaas(I-person) ,(O) was(O) born(O) on(O) 17(O) November(O) 1944(O) in(O) Rotterdam(B-location) ,(O) Netherlands(B-country) ,(O) to(O) Anton(B-writer) Koolhaas(I-writer) ((O) 1912-1992(O) )(O) and(O) Selinde(B-person) Pietertje(I-person) Roosenburg(I-person) ((O) born(O) 1920(O) )(O) .(O)"}}
{"id": "372", "dataset": "crossner_literature", "split": "test", "label_list": ["person", "organization", "event", "award", "writer", "poem", "literary genre", "location", "country", "book", "magazine"], "instance": {"id": "372", "words": ["He", "is", "best", "known", "for", "his", "works", "of", "fiction", ",", "especially", "The", "Screwtape", "Letters", ",", "The", "Chronicles", "of", "Narnia", ",", "and", "The", "Space", "Trilogy", ",", "and", "for", "his", "non-fiction", "Christian", "apologetics", ",", "such", "as", "Mere", "Christianity", ",", "Miracles", ",", "and", "The", "Problem", "of", "Pain", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "B-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "O", "O", "O", "B-book", "I-book", "O", "B-book", "O", "O", "B-book", "I-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, event, award, writer, poem, literary genre, location, country, book, magazine and O.\nSentence: He is best known for his works of fiction , especially The Screwtape Letters , The Chronicles of Narnia , and The Space Trilogy , and for his non-fiction Christian apologetics , such as Mere Christianity , Miracles , and The Problem of Pain .", "prompt_labels": "He(O) is(O) best(O) known(O) for(O) his(O) works(O) of(O) fiction(B-literary genre) ,(O) especially(O) The(B-book) Screwtape(I-book) Letters(I-book) ,(O) The(B-book) Chronicles(I-book) of(I-book) Narnia(I-book) ,(O) and(O) The(B-book) Space(I-book) Trilogy(I-book) ,(O) and(O) for(O) his(O) non-fiction(B-literary genre) Christian(I-literary genre) apologetics(I-literary genre) ,(O) such(O) as(O) Mere(B-book) Christianity(I-book) ,(O) Miracles(B-book) ,(O) and(O) The(B-book) Problem(I-book) of(I-book) Pain(I-book) .(O)"}}
{"id": "218", "dataset": "crossner_literature", "split": "test", "label_list": ["event", "book", "magazine", "literary genre", "award", "poem", "country", "location", "organization", "person", "writer"], "instance": {"id": "218", "words": ["Slaughterhouse-Five", "received", "generally", "positive", "reviews", ",", "with", "Michael", "Crichton", "writing", "in", "The", "New", "Republic", ",", "he", "writes", "about", "the", "most", "excruciatingly", "painful", "things", "."], "labels": ["B-book", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, book, magazine, literary genre, award, poem, country, location, organization, person, writer and O.\nSentence: Slaughterhouse-Five received generally positive reviews , with Michael Crichton writing in The New Republic , he writes about the most excruciatingly painful things .", "prompt_labels": "Slaughterhouse-Five(B-book) received(O) generally(O) positive(O) reviews(O) ,(O) with(O) Michael(B-writer) Crichton(I-writer) writing(O) in(O) The(B-magazine) New(I-magazine) Republic(I-magazine) ,(O) he(O) writes(O) about(O) the(O) most(O) excruciatingly(O) painful(O) things(O) .(O)"}}
{"id": "263", "dataset": "crossner_literature", "split": "test", "label_list": ["country", "event", "literary genre", "location", "magazine", "writer", "book", "person", "award", "poem", "organization"], "instance": {"id": "263", "words": ["The", "film", "is", "about", "feuding", "gangsters", "in", "the", "Prohibition", "era", ",", "inspired", "by", "Dashiell", "Hammett", "'", "s", "novels", "Red", "Harvest", "(", "1929", ")", "and", "The", "Glass", "Key", "(", "serialized", "in", "1930", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-literary genre", "B-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, literary genre, location, magazine, writer, book, person, award, poem, organization and O.\nSentence: The film is about feuding gangsters in the Prohibition era , inspired by Dashiell Hammett ' s novels Red Harvest ( 1929 ) and The Glass Key ( serialized in 1930 ) .", "prompt_labels": "The(O) film(O) is(O) about(O) feuding(O) gangsters(O) in(O) the(O) Prohibition(O) era(O) ,(O) inspired(O) by(O) Dashiell(B-writer) Hammett(I-writer) '(O) s(O) novels(B-literary genre) Red(B-book) Harvest(I-book) ((O) 1929(O) )(O) and(O) The(B-book) Glass(I-book) Key(I-book) ((O) serialized(O) in(O) 1930(O) )(O) .(O)"}}
{"id": "64", "dataset": "crossner_literature", "split": "test", "label_list": ["magazine", "person", "country", "event", "writer", "poem", "organization", "literary genre", "location", "book", "award"], "instance": {"id": "64", "words": ["The", "first", "two", "Sherlock", "Holmes", "stories", ",", "the", "novels", "A", "Study", "in", "Scarlet", "(", "1887", ")", "and", "The", "Sign", "of", "the", "Four", "(", "1890", ")", ",", "were", "moderately", "well", "received", ",", "but", "Holmes", "first", "became", "widely", "popular", "early", "in", "1891", ",", "when", "the", "first", "six", "short", "stories", "featuring", "the", "character", "were", "published", "in", "The", "Strand", "Magazine", "."], "labels": ["O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "B-literary genre", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, person, country, event, writer, poem, organization, literary genre, location, book, award and O.\nSentence: The first two Sherlock Holmes stories , the novels A Study in Scarlet ( 1887 ) and The Sign of the Four ( 1890 ) , were moderately well received , but Holmes first became widely popular early in 1891 , when the first six short stories featuring the character were published in The Strand Magazine .", "prompt_labels": "The(O) first(O) two(O) Sherlock(B-book) Holmes(I-book) stories(I-book) ,(O) the(O) novels(B-literary genre) A(B-book) Study(I-book) in(I-book) Scarlet(I-book) ((O) 1887(O) )(O) and(O) The(B-book) Sign(I-book) of(I-book) the(I-book) Four(I-book) ((O) 1890(O) )(O) ,(O) were(O) moderately(O) well(O) received(O) ,(O) but(O) Holmes(B-book) first(O) became(O) widely(O) popular(O) early(O) in(O) 1891(O) ,(O) when(O) the(O) first(O) six(O) short(O) stories(O) featuring(O) the(O) character(O) were(O) published(O) in(O) The(B-magazine) Strand(I-magazine) Magazine(I-magazine) .(O)"}}
{"id": "179", "dataset": "crossner_literature", "split": "test", "label_list": ["literary genre", "poem", "person", "location", "award", "organization", "book", "event", "magazine", "writer", "country"], "instance": {"id": "179", "words": ["Early", "in", "his", "career", ",", "he", "published", "short", "stories", "and", "poetry", "and", "edited", "the", "literary", "magazine", "Oxford", "Poetry", ",", "before", "going", "on", "to", "publish", "travel", "writing", ",", "satire", ",", "and", "screenplays", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-literary genre", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, poem, person, location, award, organization, book, event, magazine, writer, country and O.\nSentence: Early in his career , he published short stories and poetry and edited the literary magazine Oxford Poetry , before going on to publish travel writing , satire , and screenplays .", "prompt_labels": "Early(O) in(O) his(O) career(O) ,(O) he(O) published(O) short(B-literary genre) stories(I-literary genre) and(O) poetry(B-literary genre) and(O) edited(O) the(O) literary(O) magazine(O) Oxford(B-magazine) Poetry(I-magazine) ,(O) before(O) going(O) on(O) to(O) publish(O) travel(O) writing(O) ,(O) satire(B-literary genre) ,(O) and(O) screenplays(O) .(O)"}}
{"id": "262", "dataset": "crossner_music", "split": "test", "label_list": ["song", "band", "award", "event", "album", "location", "person", "country", "organization", "musical instrument", "musical artist", "music genre"], "instance": {"id": "262", "words": ["The", "band", "headlined", "the", "Soul", "Assassins", "tour", "with", "House", "of", "Pain", "and", "Funkdoobiest", "as", "support", ",", "then", "performed", "on", "a", "college", "tour", "with", "Rage", "Against", "the", "Machine", "and", "Seven", "Year", "Bitch", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, band, award, event, album, location, person, country, organization, musical instrument, musical artist, music genre and O.\nSentence: The band headlined the Soul Assassins tour with House of Pain and Funkdoobiest as support , then performed on a college tour with Rage Against the Machine and Seven Year Bitch .", "prompt_labels": "The(O) band(O) headlined(O) the(O) Soul(O) Assassins(O) tour(O) with(O) House(B-band) of(I-band) Pain(I-band) and(O) Funkdoobiest(B-band) as(O) support(O) ,(O) then(O) performed(O) on(O) a(O) college(O) tour(O) with(O) Rage(B-band) Against(I-band) the(I-band) Machine(I-band) and(O) Seven(B-band) Year(I-band) Bitch(I-band) .(O)"}}
{"id": "171", "dataset": "crossner_music", "split": "test", "label_list": ["event", "band", "organization", "music genre", "musical instrument", "location", "album", "song", "musical artist", "award", "country", "person"], "instance": {"id": "171", "words": ["Many", "early", "rock", "and", "roll", "songs", "are", "based", "on", "blues", ":", "That", "'s", "All", "Right", "Mama", ",", "Johnny", "B.", "Goode", ",", "Blue", "Suede", "Shoes", ",", "Whole", "Lotta", "Shakin", "'", "Goin", "On", ",", "Shake", ",", "Rattle", ",", "and", "Roll", ",", "and", "Long", "Tall", "Sally", "."], "labels": ["O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "B-music genre", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "B-song", "I-song", "I-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, band, organization, music genre, musical instrument, location, album, song, musical artist, award, country, person and O.\nSentence: Many early rock and roll songs are based on blues : That 's All Right Mama , Johnny B. Goode , Blue Suede Shoes , Whole Lotta Shakin ' Goin On , Shake , Rattle , and Roll , and Long Tall Sally .", "prompt_labels": "Many(O) early(O) rock(B-music genre) and(I-music genre) roll(I-music genre) songs(O) are(O) based(O) on(O) blues(B-music genre) :(O) That(B-song) 's(I-song) All(I-song) Right(I-song) Mama(I-song) ,(O) Johnny(B-song) B.(I-song) Goode(I-song) ,(O) Blue(B-song) Suede(I-song) Shoes(I-song) ,(O) Whole(B-song) Lotta(I-song) Shakin(I-song) '(I-song) Goin(I-song) On(I-song) ,(O) Shake(B-song) ,(I-song) Rattle(I-song) ,(I-song) and(I-song) Roll(I-song) ,(O) and(O) Long(B-song) Tall(I-song) Sally(I-song) .(O)"}}
{"id": "297", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "location", "music genre", "country", "band", "album", "song", "event", "award", "person", "organization", "musical artist"], "instance": {"id": "297", "words": ["Indie", "band", "Rilo", "Kiley", ",", "in", "keeping", "with", "their", "tendency", "to", "explore", "a", "variety", "of", "rockish", "styles", ",", "incorporated", "funk", "into", "their", "song", "The", "Moneymaker", "on", "the", "album", "Under", "the", "Blacklight", "."], "labels": ["O", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "B-song", "I-song", "O", "O", "O", "B-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, location, music genre, country, band, album, song, event, award, person, organization, musical artist and O.\nSentence: Indie band Rilo Kiley , in keeping with their tendency to explore a variety of rockish styles , incorporated funk into their song The Moneymaker on the album Under the Blacklight .", "prompt_labels": "Indie(O) band(O) Rilo(B-band) Kiley(I-band) ,(O) in(O) keeping(O) with(O) their(O) tendency(O) to(O) explore(O) a(O) variety(O) of(O) rockish(O) styles(O) ,(O) incorporated(O) funk(B-music genre) into(O) their(O) song(O) The(B-song) Moneymaker(I-song) on(O) the(O) album(O) Under(B-album) the(I-album) Blacklight(I-album) .(O)"}}
{"id": "455", "dataset": "crossner_music", "split": "test", "label_list": ["person", "country", "song", "band", "location", "musical instrument", "musical artist", "organization", "music genre", "album", "event", "award"], "instance": {"id": "455", "words": ["Autechre", "remain", "going", "strong", "to", "this", "day", ",", "where", "they", "continue", "to", "call", "Warp", "Records", "their", "home", ",", "having", "released", "numerous", "albums", "to", "critical", "acclaim", "in", "the", "years", "to", "follow", ",", "such", "as", "Confield", ",", "Draft", "7.30", ",", "Untilted", ",", "Quaristice", ",", "Oversteps", ",", "Exai", "and", "elseq", "1-5", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "O", "B-album", "I-album", "O", "B-album", "O", "B-album", "O", "B-album", "O", "B-album", "O", "B-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, song, band, location, musical instrument, musical artist, organization, music genre, album, event, award and O.\nSentence: Autechre remain going strong to this day , where they continue to call Warp Records their home , having released numerous albums to critical acclaim in the years to follow , such as Confield , Draft 7.30 , Untilted , Quaristice , Oversteps , Exai and elseq 1-5 .", "prompt_labels": "Autechre(B-musical artist) remain(O) going(O) strong(O) to(O) this(O) day(O) ,(O) where(O) they(O) continue(O) to(O) call(O) Warp(B-organization) Records(I-organization) their(O) home(O) ,(O) having(O) released(O) numerous(O) albums(O) to(O) critical(O) acclaim(O) in(O) the(O) years(O) to(O) follow(O) ,(O) such(O) as(O) Confield(B-album) ,(O) Draft(B-album) 7.30(I-album) ,(O) Untilted(B-album) ,(O) Quaristice(B-album) ,(O) Oversteps(B-album) ,(O) Exai(B-album) and(O) elseq(B-album) 1-5(I-album) .(O)"}}
{"id": "239", "dataset": "crossner_music", "split": "test", "label_list": ["person", "musical instrument", "event", "location", "musical artist", "music genre", "organization", "country", "album", "song", "award", "band"], "instance": {"id": "239", "words": ["Rob", "Sheffield", "in", "an", "AllMusic", "review", "feels", "that", ",", "like", "Mellow", "Gold", ",", "Odelay", "incorporates", "elements", "from", "various", "genres", ",", "including", "Folk", "music", "and", "Country", "music", ",", "grungy", "garage", "rock", ",", "stiff-boned", "electro", ",", "louche", "exotica", ",", "Old-school", "hip", "hop", "and", "noise", "rock", "."], "labels": ["B-musical artist", "I-musical artist", "O", "O", "B-organization", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "B-album", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "B-music genre", "O", "O", "B-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, musical instrument, event, location, musical artist, music genre, organization, country, album, song, award, band and O.\nSentence: Rob Sheffield in an AllMusic review feels that , like Mellow Gold , Odelay incorporates elements from various genres , including Folk music and Country music , grungy garage rock , stiff-boned electro , louche exotica , Old-school hip hop and noise rock .", "prompt_labels": "Rob(B-musical artist) Sheffield(I-musical artist) in(O) an(O) AllMusic(B-organization) review(O) feels(O) that(O) ,(O) like(O) Mellow(B-album) Gold(I-album) ,(O) Odelay(B-album) incorporates(O) elements(O) from(O) various(O) genres(O) ,(O) including(O) Folk(B-music genre) music(I-music genre) and(O) Country(B-music genre) music(I-music genre) ,(O) grungy(B-music genre) garage(I-music genre) rock(I-music genre) ,(O) stiff-boned(O) electro(B-music genre) ,(O) louche(O) exotica(B-music genre) ,(O) Old-school(B-music genre) hip(I-music genre) hop(I-music genre) and(O) noise(B-music genre) rock(I-music genre) .(O)"}}
{"id": "150", "dataset": "crossner_music", "split": "test", "label_list": ["country", "award", "person", "band", "event", "musical artist", "music genre", "organization", "song", "location", "album", "musical instrument"], "instance": {"id": "150", "words": ["The", "new", "millennium", "brought", "about", "a", "drastic", "change", "in", "Autechre", "'s", "style", ",", "demonstrated", "by", "Confield", "(", "2001", ")", "and", "Draft", "7.30", "(", "2003", ")", ",", "as", "well", "as", "the", "Gantz", "Graf", "EP", "(", "2002", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "O", "O", "O", "O", "B-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, person, band, event, musical artist, music genre, organization, song, location, album, musical instrument and O.\nSentence: The new millennium brought about a drastic change in Autechre 's style , demonstrated by Confield ( 2001 ) and Draft 7.30 ( 2003 ) , as well as the Gantz Graf EP ( 2002 ) .", "prompt_labels": "The(O) new(O) millennium(O) brought(O) about(O) a(O) drastic(O) change(O) in(O) Autechre(B-band) 's(O) style(O) ,(O) demonstrated(O) by(O) Confield(B-album) ((O) 2001(O) )(O) and(O) Draft(B-album) 7.30(I-album) ((O) 2003(O) )(O) ,(O) as(O) well(O) as(O) the(O) Gantz(B-album) Graf(I-album) EP(O) ((O) 2002(O) )(O) .(O)"}}
{"id": "61", "dataset": "crossner_music", "split": "test", "label_list": ["country", "musical artist", "musical instrument", "album", "song", "music genre", "event", "band", "person", "organization", "location", "award"], "instance": {"id": "61", "words": ["They", "also", "released", "five", "singles", "to", "promote", "the", "album", ":", "Enter", "Sandman", ",", "The", "Unforgiven", ",", "Nothing", "Else", "Matters", ",", "Wherever", "I", "May", "Roam", ",", "and", "Sad", "but", "TRUE", ",", "all", "of", "which", "have", "been", "considered", "to", "be", "among", "the", "band", "'s", "best-known", "songs", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "O", "B-song", "I-song", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "B-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, musical artist, musical instrument, album, song, music genre, event, band, person, organization, location, award and O.\nSentence: They also released five singles to promote the album : Enter Sandman , The Unforgiven , Nothing Else Matters , Wherever I May Roam , and Sad but TRUE , all of which have been considered to be among the band 's best-known songs .", "prompt_labels": "They(O) also(O) released(O) five(O) singles(O) to(O) promote(O) the(O) album(O) :(O) Enter(B-song) Sandman(I-song) ,(O) The(B-song) Unforgiven(I-song) ,(O) Nothing(B-song) Else(I-song) Matters(I-song) ,(O) Wherever(B-song) I(I-song) May(I-song) Roam(I-song) ,(O) and(O) Sad(B-song) but(I-song) TRUE(I-song) ,(O) all(O) of(O) which(O) have(O) been(O) considered(O) to(O) be(O) among(O) the(O) band(O) 's(O) best-known(O) songs(O) .(O)"}}
{"id": "339", "dataset": "crossner_music", "split": "test", "label_list": ["album", "musical artist", "music genre", "song", "country", "musical instrument", "location", "person", "band", "organization", "award", "event"], "instance": {"id": "339", "words": ["Distinct", "from", "Afrobeat", "is", "Afrobeats", "-", "a", "sound", "originating", "in", "West", "Africa", "in", "the", "21st", "century", ",", "one", "which", "takes", "in", "diverse", "influences", "and", "is", "an", "eclectic", "combination", "of", "genres", "such", "as", "British", "house", "music", ",", "hiplife", ",", "hip", "hop", ",", "dancehall", ",", "Soca", "music", ",", "Jùjú", "music", ",", "highlife", ",", "Rhythm", "and", "blues", ",", "Ndombolo", ",", "Naija", "beats", ",", "Azonto", ",", "and", "Palm-wine", "music", "."], "labels": ["O", "O", "B-music genre", "O", "B-music genre", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical artist, music genre, song, country, musical instrument, location, person, band, organization, award, event and O.\nSentence: Distinct from Afrobeat is Afrobeats - a sound originating in West Africa in the 21st century , one which takes in diverse influences and is an eclectic combination of genres such as British house music , hiplife , hip hop , dancehall , Soca music , Jùjú music , highlife , Rhythm and blues , Ndombolo , Naija beats , Azonto , and Palm-wine music .", "prompt_labels": "Distinct(O) from(O) Afrobeat(B-music genre) is(O) Afrobeats(B-music genre) -(O) a(O) sound(O) originating(O) in(O) West(B-location) Africa(I-location) in(O) the(O) 21st(O) century(O) ,(O) one(O) which(O) takes(O) in(O) diverse(O) influences(O) and(O) is(O) an(O) eclectic(O) combination(O) of(O) genres(O) such(O) as(O) British(O) house(B-music genre) music(I-music genre) ,(O) hiplife(B-music genre) ,(O) hip(B-music genre) hop(I-music genre) ,(O) dancehall(B-music genre) ,(O) Soca(B-music genre) music(I-music genre) ,(O) Jùjú(B-music genre) music(I-music genre) ,(O) highlife(B-music genre) ,(O) Rhythm(B-music genre) and(I-music genre) blues(I-music genre) ,(O) Ndombolo(B-music genre) ,(O) Naija(B-music genre) beats(I-music genre) ,(O) Azonto(B-music genre) ,(O) and(O) Palm-wine(B-music genre) music(I-music genre) .(O)"}}
{"id": "205", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "award", "band", "organization", "location", "song", "album", "music genre", "event", "person", "musical instrument", "country"], "instance": {"id": "205", "words": ["Bush", "'s", "albums", "include", "the", "aforementioned", "Sixteen", "Stone", ",", "Razorblade", "Suitcase", ",", "Deconstructed", ",", "The", "Science", "of", "Things", ",", "and", "Golden", "State", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "B-album", "I-album", "O", "B-album", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "B-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, award, band, organization, location, song, album, music genre, event, person, musical instrument, country and O.\nSentence: Bush 's albums include the aforementioned Sixteen Stone , Razorblade Suitcase , Deconstructed , The Science of Things , and Golden State .", "prompt_labels": "Bush(B-musical artist) 's(O) albums(O) include(O) the(O) aforementioned(O) Sixteen(B-album) Stone(I-album) ,(O) Razorblade(B-album) Suitcase(I-album) ,(O) Deconstructed(B-album) ,(O) The(B-album) Science(I-album) of(I-album) Things(I-album) ,(O) and(O) Golden(B-album) State(I-album) .(O)"}}
{"id": "237", "dataset": "crossner_music", "split": "test", "label_list": ["location", "song", "person", "musical instrument", "organization", "country", "musical artist", "award", "event", "band", "music genre", "album"], "instance": {"id": "237", "words": ["After", "several", "line-up", "changes", ",", "the", "band", "went", "on", "to", "release", "a", "series", "of", "UK", "and", "US", "platinum", "and", "gold", "albums", ",", "including", "1982", "'s", "The", "Number", "of", "the", "Beast", ",", "1983", "'s", "Piece", "of", "Mind", ",", "1984", "'s", "Powerslave", ",", "1985", "'s", "live", "release", "Live", "After", "Death", ",", "1986", "'s", "Somewhere", "in", "Time", ",", "and", "1988", "'s", "Seventh", "Son", "of", "a", "Seventh", "Son", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "B-album", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, song, person, musical instrument, organization, country, musical artist, award, event, band, music genre, album and O.\nSentence: After several line-up changes , the band went on to release a series of UK and US platinum and gold albums , including 1982 's The Number of the Beast , 1983 's Piece of Mind , 1984 's Powerslave , 1985 's live release Live After Death , 1986 's Somewhere in Time , and 1988 's Seventh Son of a Seventh Son .", "prompt_labels": "After(O) several(O) line-up(O) changes(O) ,(O) the(O) band(O) went(O) on(O) to(O) release(O) a(O) series(O) of(O) UK(B-country) and(O) US(B-country) platinum(O) and(O) gold(O) albums(O) ,(O) including(O) 1982(O) 's(O) The(B-album) Number(I-album) of(I-album) the(I-album) Beast(I-album) ,(O) 1983(O) 's(O) Piece(B-album) of(I-album) Mind(I-album) ,(O) 1984(O) 's(O) Powerslave(B-album) ,(O) 1985(O) 's(O) live(O) release(O) Live(B-album) After(I-album) Death(I-album) ,(O) 1986(O) 's(O) Somewhere(B-album) in(I-album) Time(I-album) ,(O) and(O) 1988(O) 's(O) Seventh(B-album) Son(I-album) of(I-album) a(I-album) Seventh(I-album) Son(I-album) .(O)"}}
{"id": "166", "dataset": "crossner_music", "split": "test", "label_list": ["award", "event", "song", "music genre", "country", "organization", "person", "band", "musical instrument", "location", "album", "musical artist"], "instance": {"id": "166", "words": ["For", "portraying", "both", "gunfighter", "Kid", "Shelleen", "and", "criminal", "Tim", "Strawn", ",", "he", "won", "the", "Academy", "Award", "for", "Best", "Actor", ",", "along", "with", "a", "BAFTA", "Award", "for", "Best", "Actor", "in", "a", "Leading", "Role", ",", "a", "Golden", "Globe", "Award", ",", "an", "National", "Board", "of", "Review", "Award", "for", "Best", "Actor", ",", "and", "the", "Silver", "Bear", "for", "Best", "Actor", "."], "labels": ["O", "O", "O", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, song, music genre, country, organization, person, band, musical instrument, location, album, musical artist and O.\nSentence: For portraying both gunfighter Kid Shelleen and criminal Tim Strawn , he won the Academy Award for Best Actor , along with a BAFTA Award for Best Actor in a Leading Role , a Golden Globe Award , an National Board of Review Award for Best Actor , and the Silver Bear for Best Actor .", "prompt_labels": "For(O) portraying(O) both(O) gunfighter(O) Kid(B-person) Shelleen(I-person) and(O) criminal(O) Tim(B-person) Strawn(I-person) ,(O) he(O) won(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) ,(O) along(O) with(O) a(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) in(I-award) a(I-award) Leading(I-award) Role(I-award) ,(O) a(O) Golden(B-award) Globe(I-award) Award(I-award) ,(O) an(O) National(B-award) Board(I-award) of(I-award) Review(I-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) ,(O) and(O) the(O) Silver(B-award) Bear(I-award) for(I-award) Best(I-award) Actor(I-award) .(O)"}}
{"id": "332", "dataset": "crossner_music", "split": "test", "label_list": ["band", "organization", "musical artist", "country", "award", "person", "album", "location", "musical instrument", "song", "event", "music genre"], "instance": {"id": "332", "words": ["Armstrong", "had", "nineteen", "Top", "Ten", "records", "including", "Stardust", ",", "What", "a", "Wonderful", "World", ",", "When", "The", "Saints", "Go", "Marching", "In", ",", "Dream", "a", "Little", "Dream", "of", "Me", ",", "Ain", "'t", "Misbehavin", "'", ",", "You", "Rascal", "You", ",", "and", "Stompin", "'", "at", "the", "Savoy", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "B-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, organization, musical artist, country, award, person, album, location, musical instrument, song, event, music genre and O.\nSentence: Armstrong had nineteen Top Ten records including Stardust , What a Wonderful World , When The Saints Go Marching In , Dream a Little Dream of Me , Ain 't Misbehavin ' , You Rascal You , and Stompin ' at the Savoy .", "prompt_labels": "Armstrong(B-musical artist) had(O) nineteen(O) Top(O) Ten(O) records(O) including(O) Stardust(B-song) ,(O) What(B-song) a(I-song) Wonderful(I-song) World(I-song) ,(O) When(B-song) The(I-song) Saints(I-song) Go(I-song) Marching(I-song) In(I-song) ,(O) Dream(B-song) a(I-song) Little(I-song) Dream(I-song) of(I-song) Me(I-song) ,(O) Ain(B-song) 't(I-song) Misbehavin(I-song) '(I-song) ,(O) You(B-song) Rascal(I-song) You(I-song) ,(O) and(O) Stompin(B-song) '(I-song) at(I-song) the(I-song) Savoy(I-song) .(O)"}}
{"id": "115", "dataset": "crossner_music", "split": "test", "label_list": ["location", "band", "person", "organization", "musical artist", "music genre", "country", "event", "album", "song", "award", "musical instrument"], "instance": {"id": "115", "words": ["Looking", "to", "depart", "from", "the", "distorted", "production", "of", "their", "previous", "record", ",", "The", "Downward", "Spiral", "(", "1994", ")", ",", "the", "album", "features", "elements", "of", "Ambient", "music", "and", "Electronic", "music", "music", ",", "alongside", "the", "band", "'s", "traditional", "industrial", "rock", "sound", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, band, person, organization, musical artist, music genre, country, event, album, song, award, musical instrument and O.\nSentence: Looking to depart from the distorted production of their previous record , The Downward Spiral ( 1994 ) , the album features elements of Ambient music and Electronic music music , alongside the band 's traditional industrial rock sound .", "prompt_labels": "Looking(O) to(O) depart(O) from(O) the(O) distorted(O) production(O) of(O) their(O) previous(O) record(O) ,(O) The(B-album) Downward(I-album) Spiral(I-album) ((O) 1994(O) )(O) ,(O) the(O) album(O) features(O) elements(O) of(O) Ambient(B-music genre) music(I-music genre) and(O) Electronic(B-music genre) music(I-music genre) music(O) ,(O) alongside(O) the(O) band(O) 's(O) traditional(O) industrial(B-music genre) rock(I-music genre) sound(O) .(O)"}}
{"id": "207", "dataset": "crossner_music", "split": "test", "label_list": ["location", "song", "music genre", "country", "award", "person", "organization", "musical instrument", "band", "event", "album", "musical artist"], "instance": {"id": "207", "words": ["For", "Revolver", "he", "selected", "five", "non-", "Heavy", "metal", "music", "records", "that", "influenced", "him", ":", "The", "Cure", "'s", "Pornography", ",", "Helium", "'s", "No", "Guitars", ",", "Mogwai", "EP", "+", "2", ",", "My", "Bloody", "Valentine", "'s", "Loveless", "and", "The", "Smashing", "Pumpkins", "'", "Siamese", "Dream", "."], "labels": ["O", "B-band", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "B-band", "O", "B-album", "O", "B-band", "O", "B-album", "I-album", "O", "B-band", "B-album", "I-album", "I-album", "O", "B-band", "I-band", "I-band", "O", "B-album", "O", "B-band", "I-band", "I-band", "O", "B-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, song, music genre, country, award, person, organization, musical instrument, band, event, album, musical artist and O.\nSentence: For Revolver he selected five non- Heavy metal music records that influenced him : The Cure 's Pornography , Helium 's No Guitars , Mogwai EP + 2 , My Bloody Valentine 's Loveless and The Smashing Pumpkins ' Siamese Dream .", "prompt_labels": "For(O) Revolver(B-band) he(O) selected(O) five(O) non-(O) Heavy(B-music genre) metal(I-music genre) music(I-music genre) records(O) that(O) influenced(O) him(O) :(O) The(O) Cure(B-band) 's(O) Pornography(B-album) ,(O) Helium(B-band) 's(O) No(B-album) Guitars(I-album) ,(O) Mogwai(B-band) EP(B-album) +(I-album) 2(I-album) ,(O) My(B-band) Bloody(I-band) Valentine(I-band) 's(O) Loveless(B-album) and(O) The(B-band) Smashing(I-band) Pumpkins(I-band) '(O) Siamese(B-album) Dream(I-album) .(O)"}}
{"id": "341", "dataset": "crossner_music", "split": "test", "label_list": ["award", "musical artist", "location", "person", "country", "album", "band", "organization", "music genre", "song", "event", "musical instrument"], "instance": {"id": "341", "words": ["Along", "with", "one", "other", "Original", "Six", "indoor", "ice", "hockey", "arena", ",", "the", "Boston", "Garden", ",", "the", "Montreal", "Forum", "used", "a", "high-pitched", "siren", "to", "signal", "the", "end", "of", "an", "NHL", "game", "'s", "period", "-", "the", "siren", "would", "later", "be", "re-installed", "in", "the", "Forum", "'s", "successor", "facility", ",", "the", "Bell", "Centre", "(", "and", "still", "in", "use", "there", ")", ",", "much", "as", "the", "TD", "Garden", "in", "Boston", "inherited", "the", "lower-pitched", "Garden", "'s", "siren", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "O", "O", "O", "B-location", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical artist, location, person, country, album, band, organization, music genre, song, event, musical instrument and O.\nSentence: Along with one other Original Six indoor ice hockey arena , the Boston Garden , the Montreal Forum used a high-pitched siren to signal the end of an NHL game 's period - the siren would later be re-installed in the Forum 's successor facility , the Bell Centre ( and still in use there ) , much as the TD Garden in Boston inherited the lower-pitched Garden 's siren .", "prompt_labels": "Along(O) with(O) one(O) other(O) Original(O) Six(O) indoor(O) ice(O) hockey(O) arena(O) ,(O) the(O) Boston(B-location) Garden(I-location) ,(O) the(O) Montreal(B-location) Forum(I-location) used(O) a(O) high-pitched(O) siren(O) to(O) signal(O) the(O) end(O) of(O) an(O) NHL(O) game(O) 's(O) period(O) -(O) the(O) siren(O) would(O) later(O) be(O) re-installed(O) in(O) the(O) Forum(B-location) 's(O) successor(O) facility(O) ,(O) the(O) Bell(B-location) Centre(I-location) ((O) and(O) still(O) in(O) use(O) there(O) )(O) ,(O) much(O) as(O) the(O) TD(B-location) Garden(I-location) in(O) Boston(B-location) inherited(O) the(O) lower-pitched(O) Garden(B-location) 's(O) siren(O) .(O)"}}
{"id": "246", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "musical instrument", "country", "award", "event", "location", "person", "band", "album", "musical artist", "song", "music genre"], "instance": {"id": "246", "words": ["Love", "has", "been", "candid", "about", "her", "diverse", "musical", "influences", ",", "the", "earliest", "being", "Patti", "Smith", ",", "The", "Runaways", ",", "and", "The", "Pretenders", ",", "artists", "she", "discovered", "while", "in", "juvenile", "hall", "at", "age", "fifteen", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, musical instrument, country, award, event, location, person, band, album, musical artist, song, music genre and O.\nSentence: Love has been candid about her diverse musical influences , the earliest being Patti Smith , The Runaways , and The Pretenders , artists she discovered while in juvenile hall at age fifteen .", "prompt_labels": "Love(B-musical artist) has(O) been(O) candid(O) about(O) her(O) diverse(O) musical(O) influences(O) ,(O) the(O) earliest(O) being(O) Patti(B-musical artist) Smith(I-musical artist) ,(O) The(B-band) Runaways(I-band) ,(O) and(O) The(B-band) Pretenders(I-band) ,(O) artists(O) she(O) discovered(O) while(O) in(O) juvenile(O) hall(O) at(O) age(O) fifteen(O) .(O)"}}
{"id": "80", "dataset": "crossner_music", "split": "test", "label_list": ["country", "musical artist", "location", "song", "award", "organization", "person", "album", "event", "band", "musical instrument", "music genre"], "instance": {"id": "80", "words": ["He", "received", "a", "Kennedy", "Center", "Honors", "in", "2009", ",", "a", "Hollywood", "Walk", "of", "Fame", "star", "in", "2010", ",", "the", "41st", "AFI", "Life", "Achievement", "Award", "in", "June", "2013", ",", "a", "British", "Film", "Institute", "Fellowship", "in", "March", "2015", ",", "a", "National", "Medal", "of", "Arts", "in", "September", "2016", ",", "and", "a", "BAFTA", "Fellowship", "in", "February", "2017", "."], "labels": ["O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, musical artist, location, song, award, organization, person, album, event, band, musical instrument, music genre and O.\nSentence: He received a Kennedy Center Honors in 2009 , a Hollywood Walk of Fame star in 2010 , the 41st AFI Life Achievement Award in June 2013 , a British Film Institute Fellowship in March 2015 , a National Medal of Arts in September 2016 , and a BAFTA Fellowship in February 2017 .", "prompt_labels": "He(O) received(O) a(O) Kennedy(B-award) Center(I-award) Honors(I-award) in(O) 2009(O) ,(O) a(O) Hollywood(B-location) Walk(I-location) of(I-location) Fame(I-location) star(O) in(O) 2010(O) ,(O) the(O) 41st(B-award) AFI(I-award) Life(I-award) Achievement(I-award) Award(I-award) in(O) June(O) 2013(O) ,(O) a(O) British(B-award) Film(I-award) Institute(I-award) Fellowship(I-award) in(O) March(O) 2015(O) ,(O) a(O) National(B-award) Medal(I-award) of(I-award) Arts(I-award) in(O) September(O) 2016(O) ,(O) and(O) a(O) BAFTA(B-award) Fellowship(I-award) in(O) February(O) 2017(O) .(O)"}}
{"id": "190", "dataset": "crossner_music", "split": "test", "label_list": ["country", "location", "song", "person", "musical artist", "album", "event", "organization", "music genre", "award", "musical instrument", "band"], "instance": {"id": "190", "words": ["She", "reprised", "her", "role", "in", "it", "the", "next", "year", ",", "playing", "the", "Edinburgh", "Playhouse", "from", "November", "19", "to", "December", "8", ",", "2007", "and", "the", "Wales", "Millennium", "Centre", "in", "the", "Donald", "Gordon", "Theatre", "from", "December", "13", ",", "2007", "through", "January", "12", ",", "2008", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, song, person, musical artist, album, event, organization, music genre, award, musical instrument, band and O.\nSentence: She reprised her role in it the next year , playing the Edinburgh Playhouse from November 19 to December 8 , 2007 and the Wales Millennium Centre in the Donald Gordon Theatre from December 13 , 2007 through January 12 , 2008 .", "prompt_labels": "She(O) reprised(O) her(O) role(O) in(O) it(O) the(O) next(O) year(O) ,(O) playing(O) the(O) Edinburgh(B-location) Playhouse(I-location) from(O) November(O) 19(O) to(O) December(O) 8(O) ,(O) 2007(O) and(O) the(O) Wales(B-location) Millennium(I-location) Centre(I-location) in(O) the(O) Donald(B-location) Gordon(I-location) Theatre(I-location) from(O) December(O) 13(O) ,(O) 2007(O) through(O) January(O) 12(O) ,(O) 2008(O) .(O)"}}
{"id": "195", "dataset": "crossner_music", "split": "test", "label_list": ["album", "musical instrument", "organization", "event", "country", "music genre", "award", "band", "song", "musical artist", "location", "person"], "instance": {"id": "195", "words": ["Notable", "post-punk", "groups", "that", "presaged", "that", "genre", "and", "helped", "develop", "and", "shape", "the", "subculture", ",", "include", "Siouxsie", "and", "the", "Banshees", ",", "Joy", "Division", ",", "Bauhaus", "and", "The", "Cure", "."], "labels": ["O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical instrument, organization, event, country, music genre, award, band, song, musical artist, location, person and O.\nSentence: Notable post-punk groups that presaged that genre and helped develop and shape the subculture , include Siouxsie and the Banshees , Joy Division , Bauhaus and The Cure .", "prompt_labels": "Notable(O) post-punk(B-music genre) groups(O) that(O) presaged(O) that(O) genre(O) and(O) helped(O) develop(O) and(O) shape(O) the(O) subculture(O) ,(O) include(O) Siouxsie(B-band) and(I-band) the(I-band) Banshees(I-band) ,(O) Joy(B-band) Division(I-band) ,(O) Bauhaus(B-band) and(O) The(B-band) Cure(I-band) .(O)"}}
{"id": "147", "dataset": "crossner_music", "split": "test", "label_list": ["album", "person", "award", "music genre", "event", "song", "organization", "location", "musical artist", "musical instrument", "country", "band"], "instance": {"id": "147", "words": ["At", "the", "time", "of", "his", "sixteenth", "birthday", "on", "June", "20", ",", "1958", ",", "Brian", "Wilson", "shared", "a", "bedroom", "with", "his", "brothers", ",", "Dennis", "Wilson", "and", "Carl", "Wilson", "-", "aged", "thirteen", "and", "eleven", ",", "respectively", "-", "in", "their", "family", "home", "in", "Hawthorne", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, person, award, music genre, event, song, organization, location, musical artist, musical instrument, country, band and O.\nSentence: At the time of his sixteenth birthday on June 20 , 1958 , Brian Wilson shared a bedroom with his brothers , Dennis Wilson and Carl Wilson - aged thirteen and eleven , respectively - in their family home in Hawthorne .", "prompt_labels": "At(O) the(O) time(O) of(O) his(O) sixteenth(O) birthday(O) on(O) June(O) 20(O) ,(O) 1958(O) ,(O) Brian(B-musical artist) Wilson(I-musical artist) shared(O) a(O) bedroom(O) with(O) his(O) brothers(O) ,(O) Dennis(B-musical artist) Wilson(I-musical artist) and(O) Carl(B-musical artist) Wilson(I-musical artist) -(O) aged(O) thirteen(O) and(O) eleven(O) ,(O) respectively(O) -(O) in(O) their(O) family(O) home(O) in(O) Hawthorne(B-location) .(O)"}}
{"id": "380", "dataset": "crossner_music", "split": "test", "label_list": ["award", "event", "musical artist", "location", "band", "song", "musical instrument", "organization", "country", "music genre", "person", "album"], "instance": {"id": "380", "words": ["Brad", "Shoup", "of", "Stereogum", "surmised", "that", ",", "thanks", "to", "the", "Ramones", "'", "praise", "for", "the", "group", ",", "many", "punk", ",", "pop", "punk", ",", "or", "punk-adjacent", "artists", "showed", "influence", "from", "the", "Beach", "Boys", ",", "noting", "cover", "versions", "of", "the", "band", "'s", "songs", "recorded", "by", "Slickee", "Boys", ",", "Agent", "Orange", ",", "Bad", "Religion", ",", "Shonen", "Knife", ",", "the", "Queers", ",", "Hi-Standard", ",", "the", "Descendents", ",", "the", "Donnas", ",", "M.O.D.", ",", "and", "the", "Vandals", "."], "labels": ["B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, musical artist, location, band, song, musical instrument, organization, country, music genre, person, album and O.\nSentence: Brad Shoup of Stereogum surmised that , thanks to the Ramones ' praise for the group , many punk , pop punk , or punk-adjacent artists showed influence from the Beach Boys , noting cover versions of the band 's songs recorded by Slickee Boys , Agent Orange , Bad Religion , Shonen Knife , the Queers , Hi-Standard , the Descendents , the Donnas , M.O.D. , and the Vandals .", "prompt_labels": "Brad(B-person) Shoup(I-person) of(O) Stereogum(O) surmised(O) that(O) ,(O) thanks(O) to(O) the(O) Ramones(B-band) '(O) praise(O) for(O) the(O) group(O) ,(O) many(O) punk(B-music genre) ,(O) pop(B-music genre) punk(I-music genre) ,(O) or(O) punk-adjacent(B-music genre) artists(O) showed(O) influence(O) from(O) the(B-band) Beach(I-band) Boys(I-band) ,(O) noting(O) cover(O) versions(O) of(O) the(O) band(O) 's(O) songs(O) recorded(O) by(O) Slickee(B-band) Boys(I-band) ,(O) Agent(B-band) Orange(I-band) ,(O) Bad(B-band) Religion(I-band) ,(O) Shonen(B-band) Knife(I-band) ,(O) the(B-band) Queers(I-band) ,(O) Hi-Standard(B-band) ,(O) the(B-band) Descendents(I-band) ,(O) the(B-band) Donnas(I-band) ,(O) M.O.D.(B-band) ,(O) and(O) the(B-band) Vandals(I-band) .(O)"}}
{"id": "107", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "award", "musical instrument", "band", "music genre", "event", "person", "album", "location", "country", "musical artist", "song"], "instance": {"id": "107", "words": ["He", "also", "performed", "in", "The", "Rocky", "Horror", "Show", ",", "as", "the", "narrator", ",", "at", "the", "Churchill", "Theatre", "in", "Bromley", "and", "the", "New", "Wimbledon", "Theatre", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, musical instrument, band, music genre, event, person, album, location, country, musical artist, song and O.\nSentence: He also performed in The Rocky Horror Show , as the narrator , at the Churchill Theatre in Bromley and the New Wimbledon Theatre .", "prompt_labels": "He(O) also(O) performed(O) in(O) The(O) Rocky(O) Horror(O) Show(O) ,(O) as(O) the(O) narrator(O) ,(O) at(O) the(O) Churchill(B-location) Theatre(I-location) in(O) Bromley(B-location) and(O) the(O) New(B-location) Wimbledon(I-location) Theatre(I-location) .(O)"}}
{"id": "233", "dataset": "crossner_music", "split": "test", "label_list": ["award", "band", "song", "location", "person", "musical artist", "country", "music genre", "album", "musical instrument", "event", "organization"], "instance": {"id": "233", "words": ["That", "January", ",", "the", "San", "Francisco", "band", "Blue", "Cheer", "released", "a", "cover", "of", "Eddie", "Cochran", "'", "s", "classic", "Summertime", "Blues", ",", "from", "their", "debut", "album", "Vincebus", "Eruptum", ",", "that", "many", "consider", "the", "first", "TRUE", "heavy", "metal", "recording.McCleary", "(", "2004", ")", ",", "pp.", "240", ",", "506", "."], "labels": ["O", "O", "O", "O", "B-location", "I-location", "O", "B-band", "I-band", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "B-song", "I-song", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, band, song, location, person, musical artist, country, music genre, album, musical instrument, event, organization and O.\nSentence: That January , the San Francisco band Blue Cheer released a cover of Eddie Cochran ' s classic Summertime Blues , from their debut album Vincebus Eruptum , that many consider the first TRUE heavy metal recording.McCleary ( 2004 ) , pp. 240 , 506 .", "prompt_labels": "That(O) January(O) ,(O) the(O) San(B-location) Francisco(I-location) band(O) Blue(B-band) Cheer(I-band) released(O) a(O) cover(O) of(O) Eddie(B-musical artist) Cochran(I-musical artist) '(O) s(O) classic(O) Summertime(B-song) Blues(I-song) ,(O) from(O) their(O) debut(O) album(O) Vincebus(B-album) Eruptum(I-album) ,(O) that(O) many(O) consider(O) the(O) first(O) TRUE(O) heavy(B-music genre) metal(I-music genre) recording.McCleary(O) ((O) 2004(O) )(O) ,(O) pp.(O) 240(O) ,(O) 506(O) .(O)"}}
{"id": "260", "dataset": "crossner_music", "split": "test", "label_list": ["band", "album", "event", "musical instrument", "person", "award", "country", "music genre", "organization", "musical artist", "location", "song"], "instance": {"id": "260", "words": ["The", "stadium", "hosted", "the", "2003", "World", "Championships", "in", "Athletics", "and", "from", "1999", "to", "2016", "it", "hosted", "the", "annual", "Meeting", "Areva", "athletics", "meet", "."], "labels": ["O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, album, event, musical instrument, person, award, country, music genre, organization, musical artist, location, song and O.\nSentence: The stadium hosted the 2003 World Championships in Athletics and from 1999 to 2016 it hosted the annual Meeting Areva athletics meet .", "prompt_labels": "The(O) stadium(O) hosted(O) the(O) 2003(B-event) World(I-event) Championships(I-event) in(I-event) Athletics(I-event) and(O) from(O) 1999(O) to(O) 2016(O) it(O) hosted(O) the(O) annual(O) Meeting(B-event) Areva(I-event) athletics(O) meet(O) .(O)"}}
{"id": "461", "dataset": "crossner_music", "split": "test", "label_list": ["band", "musical instrument", "location", "event", "music genre", "organization", "country", "album", "person", "song", "musical artist", "award"], "instance": {"id": "461", "words": ["Due", "to", "its", "highly", "syncretic", "nature", ",", "Latin", "American", "music", "encompasses", "a", "wide", "variety", "of", "styles", ",", "including", "influential", "genres", "such", "as", "cumbia", ",", "bachata", ",", "bossa", "nova", ",", "Merengue", "music", ",", "Cuban", "rumba", ",", "Salsa", "music", ",", "samba", ",", "son", ",", "and", "Tango", "music", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical instrument, location, event, music genre, organization, country, album, person, song, musical artist, award and O.\nSentence: Due to its highly syncretic nature , Latin American music encompasses a wide variety of styles , including influential genres such as cumbia , bachata , bossa nova , Merengue music , Cuban rumba , Salsa music , samba , son , and Tango music .", "prompt_labels": "Due(O) to(O) its(O) highly(O) syncretic(O) nature(O) ,(O) Latin(O) American(O) music(O) encompasses(O) a(O) wide(O) variety(O) of(O) styles(O) ,(O) including(O) influential(O) genres(O) such(O) as(O) cumbia(B-music genre) ,(O) bachata(B-music genre) ,(O) bossa(B-music genre) nova(I-music genre) ,(O) Merengue(B-music genre) music(I-music genre) ,(O) Cuban(B-music genre) rumba(I-music genre) ,(O) Salsa(B-music genre) music(I-music genre) ,(O) samba(B-music genre) ,(O) son(B-music genre) ,(O) and(O) Tango(B-music genre) music(I-music genre) .(O)"}}
{"id": "58", "dataset": "crossner_music", "split": "test", "label_list": ["song", "music genre", "musical instrument", "event", "location", "award", "musical artist", "organization", "band", "person", "album", "country"], "instance": {"id": "58", "words": ["It", "includes", "collaborations", "with", "Pete", "Seeger", ",", "Ivan", "Neville", ",", "Cyril", "Neville", ",", "Skerik", ",", "Adam", "Levy", ",", "Righteous", "Babe", "recording", "artist", "Anaïs", "Mitchell", ",", "CC", "Adcock", ",", "and", "a", "host", "of", "New", "Orleans-based", "horn", "players", "known", "for", "their", "work", "in", "such", "outfits", "as", "Galactic", ",", "Bonerama", ",", "and", "Rebirth", "Brass", "Band", "."], "labels": ["O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-organization", "I-organization", "I-organization", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "B-band", "O", "O", "B-band", "I-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, music genre, musical instrument, event, location, award, musical artist, organization, band, person, album, country and O.\nSentence: It includes collaborations with Pete Seeger , Ivan Neville , Cyril Neville , Skerik , Adam Levy , Righteous Babe recording artist Anaïs Mitchell , CC Adcock , and a host of New Orleans-based horn players known for their work in such outfits as Galactic , Bonerama , and Rebirth Brass Band .", "prompt_labels": "It(O) includes(O) collaborations(O) with(O) Pete(B-musical artist) Seeger(I-musical artist) ,(O) Ivan(B-musical artist) Neville(I-musical artist) ,(O) Cyril(B-musical artist) Neville(I-musical artist) ,(O) Skerik(B-musical artist) ,(O) Adam(B-musical artist) Levy(I-musical artist) ,(O) Righteous(B-organization) Babe(I-organization) recording(I-organization) artist(O) Anaïs(B-musical artist) Mitchell(I-musical artist) ,(O) CC(B-musical artist) Adcock(I-musical artist) ,(O) and(O) a(O) host(O) of(O) New(O) Orleans-based(O) horn(O) players(O) known(O) for(O) their(O) work(O) in(O) such(O) outfits(O) as(O) Galactic(B-band) ,(O) Bonerama(B-band) ,(O) and(O) Rebirth(B-band) Brass(I-band) Band(I-band) .(O)"}}
{"id": "333", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "country", "album", "musical artist", "location", "music genre", "event", "band", "song", "organization", "person", "award"], "instance": {"id": "333", "words": ["It", "won", "five", "Grammys", ":", "Grammy", "Award", "for", "Best", "Rock", "Album", ",", "Grammy", "Award", "for", "Best", "Rock", "Song", "(", "Dani", "California", ")", ",", "Grammy", "Award", "for", "Best", "Rock", "Performance", "by", "a", "Duo", "or", "Group", "with", "Vocal", "(", "Dani", "California", ")", ",", "Best", "Boxed", "Or", "Special", "Limited", "Edition", "Package", ",", "and", "Best", "Producer", "(", "Rick", "Rubin", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-song", "I-song", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-song", "I-song", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "O", "B-song", "I-song", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, country, album, musical artist, location, music genre, event, band, song, organization, person, award and O.\nSentence: It won five Grammys : Grammy Award for Best Rock Album , Grammy Award for Best Rock Song ( Dani California ) , Grammy Award for Best Rock Performance by a Duo or Group with Vocal ( Dani California ) , Best Boxed Or Special Limited Edition Package , and Best Producer ( Rick Rubin ) .", "prompt_labels": "It(O) won(O) five(O) Grammys(O) :(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Rock(I-award) Album(I-award) ,(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Rock(I-award) Song(I-award) ((O) Dani(B-song) California(I-song) )(O) ,(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Rock(I-award) Performance(I-award) by(I-award) a(I-award) Duo(I-award) or(I-award) Group(I-award) with(I-award) Vocal(I-award) ((O) Dani(B-song) California(I-song) )(O) ,(O) Best(B-award) Boxed(I-award) Or(I-award) Special(I-award) Limited(I-award) Edition(I-award) Package(I-award) ,(O) and(O) Best(B-award) Producer(I-award) ((O) Rick(B-song) Rubin(I-song) )(O) .(O)"}}
{"id": "153", "dataset": "crossner_music", "split": "test", "label_list": ["album", "event", "music genre", "person", "organization", "musical instrument", "band", "award", "country", "location", "musical artist", "song"], "instance": {"id": "153", "words": ["Some", "of", "Mattacks", "'", "most", "notable", "participation", "in", "studio", "recordings", "in", "the", "late", "1970s", "are", "the", "work", "on", "art", "rock", "studio", "albums", "by", "Brian", "Eno", "(", "Before", "and", "After", "Science", ")", "and", "801", "'", "s", "Listen", "Now", ",", "as", "well", "as", "several", "Ashley", "Hutchings", "-related", "folk", "rock", "projects", "(", "The", "Compleat", "Dancing", "Master", ",", "Son", "of", "Morris", "On", "etc", "."], "labels": ["O", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "B-band", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-music genre", "I-music genre", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, event, music genre, person, organization, musical instrument, band, award, country, location, musical artist, song and O.\nSentence: Some of Mattacks ' most notable participation in studio recordings in the late 1970s are the work on art rock studio albums by Brian Eno ( Before and After Science ) and 801 ' s Listen Now , as well as several Ashley Hutchings -related folk rock projects ( The Compleat Dancing Master , Son of Morris On etc .", "prompt_labels": "Some(O) of(O) Mattacks(B-musical artist) '(O) most(O) notable(O) participation(O) in(O) studio(O) recordings(O) in(O) the(O) late(O) 1970s(O) are(O) the(O) work(O) on(O) art(O) rock(O) studio(O) albums(O) by(O) Brian(B-musical artist) Eno(I-musical artist) ((O) Before(B-album) and(I-album) After(I-album) Science(I-album) )(O) and(O) 801(B-band) '(O) s(O) Listen(B-album) Now(I-album) ,(O) as(O) well(O) as(O) several(O) Ashley(B-musical artist) Hutchings(I-musical artist) -related(O) folk(B-music genre) rock(I-music genre) projects(O) ((O) The(B-album) Compleat(I-album) Dancing(I-album) Master(I-album) ,(O) Son(B-album) of(I-album) Morris(I-album) On(I-album) etc(O) .(O)"}}
{"id": "7", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "award", "event", "location", "song", "musical instrument", "album", "music genre", "person", "band", "musical artist", "country"], "instance": {"id": "7", "words": ["Often", "termed", "soul", "blues", "or", "Southern", "soul", ",", "the", "music", "at", "the", "heart", "of", "this", "movement", "was", "given", "new", "life", "by", "the", "unexpected", "success", "of", "two", "particular", "recordings", "on", "the", "Jackson-based", "Malaco", "label", ":", "Z.", "Z.", "Hill", "'", "s", "Down", "Home", "Blues", "(", "1982", ")", "and", "Little", "Milton", "'", "s", "The", "Blues", "is", "Alright", "(", "1984", ")", "."], "labels": ["O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "B-song", "I-song", "I-song", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, event, location, song, musical instrument, album, music genre, person, band, musical artist, country and O.\nSentence: Often termed soul blues or Southern soul , the music at the heart of this movement was given new life by the unexpected success of two particular recordings on the Jackson-based Malaco label : Z. Z. Hill ' s Down Home Blues ( 1982 ) and Little Milton ' s The Blues is Alright ( 1984 ) .", "prompt_labels": "Often(O) termed(O) soul(B-music genre) blues(I-music genre) or(O) Southern(B-music genre) soul(I-music genre) ,(O) the(O) music(O) at(O) the(O) heart(O) of(O) this(O) movement(O) was(O) given(O) new(O) life(O) by(O) the(O) unexpected(O) success(O) of(O) two(O) particular(O) recordings(O) on(O) the(O) Jackson-based(O) Malaco(B-organization) label(I-organization) :(O) Z.(B-musical artist) Z.(I-musical artist) Hill(I-musical artist) '(O) s(O) Down(B-song) Home(I-song) Blues(I-song) ((O) 1982(O) )(O) and(O) Little(B-musical artist) Milton(I-musical artist) '(O) s(O) The(B-song) Blues(I-song) is(I-song) Alright(I-song) ((O) 1984(O) )(O) .(O)"}}
{"id": "418", "dataset": "crossner_music", "split": "test", "label_list": ["location", "album", "band", "song", "award", "musical instrument", "country", "organization", "music genre", "event", "person", "musical artist"], "instance": {"id": "418", "words": ["After", "attaining", "mainstream", "success", ",", "Cobain", "became", "a", "devoted", "champion", "of", "lesser", "known", "indie", "bands", ",", "covering", "songs", "by", "The", "Vaselines", ",", "Meat", "Puppets", ",", "Wipers", "and", "Fang", "onstage", "and", "/", "or", "in", "the", "studio", ",", "wearing", "Daniel", "Johnston", "T-shirts", "during", "photo", "shoots", ",", "having", "the", "K", "Records", "logo", "tattooed", "on", "his", "forearm", ",", "and", "enlisting", "bands", "like", "Butthole", "Surfers", ",", "Shonen", "Knife", ",", "Chokebore", "and", "Half", "Japanese", "along", "for", "the", "In", "Utero", "tour", "in", "late", "1993", "and", "early", "1994", "."], "labels": ["O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, album, band, song, award, musical instrument, country, organization, music genre, event, person, musical artist and O.\nSentence: After attaining mainstream success , Cobain became a devoted champion of lesser known indie bands , covering songs by The Vaselines , Meat Puppets , Wipers and Fang onstage and / or in the studio , wearing Daniel Johnston T-shirts during photo shoots , having the K Records logo tattooed on his forearm , and enlisting bands like Butthole Surfers , Shonen Knife , Chokebore and Half Japanese along for the In Utero tour in late 1993 and early 1994 .", "prompt_labels": "After(O) attaining(O) mainstream(O) success(O) ,(O) Cobain(B-musical artist) became(O) a(O) devoted(O) champion(O) of(O) lesser(O) known(O) indie(O) bands(O) ,(O) covering(O) songs(O) by(O) The(B-band) Vaselines(I-band) ,(O) Meat(B-band) Puppets(I-band) ,(O) Wipers(B-band) and(O) Fang(B-band) onstage(O) and(O) /(O) or(O) in(O) the(O) studio(O) ,(O) wearing(O) Daniel(B-musical artist) Johnston(I-musical artist) T-shirts(O) during(O) photo(O) shoots(O) ,(O) having(O) the(O) K(O) Records(O) logo(O) tattooed(O) on(O) his(O) forearm(O) ,(O) and(O) enlisting(O) bands(O) like(O) Butthole(B-band) Surfers(I-band) ,(O) Shonen(B-band) Knife(I-band) ,(O) Chokebore(B-band) and(O) Half(B-band) Japanese(I-band) along(O) for(O) the(O) In(B-album) Utero(I-album) tour(O) in(O) late(O) 1993(O) and(O) early(O) 1994(O) .(O)"}}
{"id": "201", "dataset": "crossner_music", "split": "test", "label_list": ["album", "organization", "song", "event", "music genre", "musical instrument", "location", "band", "award", "person", "country", "musical artist"], "instance": {"id": "201", "words": ["XXXVII", "No.", "32", ",", "pp.", "69", ",", "82", ")", "Spaces", "in", "Manhattan", "that", "supported", "Downtown", "music", "from", "the", "1960s", "on", "included", "the", "Judson", "Memorial", "Church", ",", "The", "Kitchen", ",", "Experimental", "Intermedia", ",", "Roulette", "Intermedium", ",", "the", "Knitting", "Factory", ",", "Dance", "Theater", "Workshop", ",", "Tonic", ",", "the", "Gas", "Station", ",", "the", "Paula", "Cooper", "Gallery", ",", "and", "others", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, organization, song, event, music genre, musical instrument, location, band, award, person, country, musical artist and O.\nSentence: XXXVII No. 32 , pp. 69 , 82 ) Spaces in Manhattan that supported Downtown music from the 1960s on included the Judson Memorial Church , The Kitchen , Experimental Intermedia , Roulette Intermedium , the Knitting Factory , Dance Theater Workshop , Tonic , the Gas Station , the Paula Cooper Gallery , and others .", "prompt_labels": "XXXVII(O) No.(O) 32(O) ,(O) pp.(O) 69(O) ,(O) 82(O) )(O) Spaces(O) in(O) Manhattan(B-location) that(O) supported(O) Downtown(B-music genre) music(I-music genre) from(O) the(O) 1960s(O) on(O) included(O) the(O) Judson(B-location) Memorial(I-location) Church(I-location) ,(O) The(B-location) Kitchen(I-location) ,(O) Experimental(B-location) Intermedia(I-location) ,(O) Roulette(B-location) Intermedium(I-location) ,(O) the(O) Knitting(B-location) Factory(I-location) ,(O) Dance(B-location) Theater(I-location) Workshop(I-location) ,(O) Tonic(B-location) ,(O) the(O) Gas(B-location) Station(I-location) ,(O) the(O) Paula(B-location) Cooper(I-location) Gallery(I-location) ,(O) and(O) others(O) .(O)"}}
{"id": "214", "dataset": "crossner_music", "split": "test", "label_list": ["location", "person", "event", "musical instrument", "musical artist", "band", "country", "album", "music genre", "award", "song", "organization"], "instance": {"id": "214", "words": ["It", "received", "a", "major", "expansion", "ahead", "of", "the", "1983", "Summer", "Universiade", ",", "when", "it", "reached", "a", "capacity", "of", "60,081", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, event, musical instrument, musical artist, band, country, album, music genre, award, song, organization and O.\nSentence: It received a major expansion ahead of the 1983 Summer Universiade , when it reached a capacity of 60,081 .", "prompt_labels": "It(O) received(O) a(O) major(O) expansion(O) ahead(O) of(O) the(O) 1983(B-event) Summer(I-event) Universiade(I-event) ,(O) when(O) it(O) reached(O) a(O) capacity(O) of(O) 60,081(O) .(O)"}}
{"id": "276", "dataset": "crossner_music", "split": "test", "label_list": ["person", "organization", "song", "band", "musical instrument", "award", "event", "album", "location", "music genre", "musical artist", "country"], "instance": {"id": "276", "words": ["Other", "acts", "who", "became", "prominent", "in", "the", "alt-country", "genre", "during", "the", "1990s", "and", "2000s", "included", "The", "Bottle", "Rockets", ",", "The", "Handsome", "Family", ",", "Blue", "Mountain", ",", "Robbie", "Fulks", ",", "Blood", "Oranges", ",", "Bright", "Eyes", ",", "Drive-By", "Truckers", ",", "Old", "97", "'s", ",", "Old", "Crow", "Medicine", "Show", ",", "Nickel", "Creek", ",", "Neko", "Case", ",", "and", "Whiskeytown", ",", "whose", "lead", "singer", "Ryan", "Adams", "later", "had", "a", "successful", "solo-career", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "O", "B-band", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, song, band, musical instrument, award, event, album, location, music genre, musical artist, country and O.\nSentence: Other acts who became prominent in the alt-country genre during the 1990s and 2000s included The Bottle Rockets , The Handsome Family , Blue Mountain , Robbie Fulks , Blood Oranges , Bright Eyes , Drive-By Truckers , Old 97 's , Old Crow Medicine Show , Nickel Creek , Neko Case , and Whiskeytown , whose lead singer Ryan Adams later had a successful solo-career .", "prompt_labels": "Other(O) acts(O) who(O) became(O) prominent(O) in(O) the(O) alt-country(B-music genre) genre(O) during(O) the(O) 1990s(O) and(O) 2000s(O) included(O) The(B-band) Bottle(I-band) Rockets(I-band) ,(O) The(B-band) Handsome(I-band) Family(I-band) ,(O) Blue(B-band) Mountain(I-band) ,(O) Robbie(B-band) Fulks(I-band) ,(O) Blood(B-band) Oranges(I-band) ,(O) Bright(B-band) Eyes(I-band) ,(O) Drive-By(B-band) Truckers(I-band) ,(O) Old(B-band) 97(I-band) 's(O) ,(O) Old(B-band) Crow(I-band) Medicine(I-band) Show(I-band) ,(O) Nickel(B-band) Creek(I-band) ,(O) Neko(B-musical artist) Case(I-musical artist) ,(O) and(O) Whiskeytown(B-band) ,(O) whose(O) lead(O) singer(O) Ryan(B-musical artist) Adams(I-musical artist) later(O) had(O) a(O) successful(O) solo-career(O) .(O)"}}
{"id": "62", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "music genre", "band", "country", "award", "location", "person", "organization", "event", "song", "musical artist", "album"], "instance": {"id": "62", "words": ["He", "helped", "found", "the", "American", "Conservatory", "Theater", "in", "San", "Francisco", ",", "the", "Mark", "Taper", "Forum", "in", "Los", "Angeles", ",", "and", "the", "Brooklyn", "Academy", "of", "Music", "Repertory", "Company", "in", "New", "York", "City", "."], "labels": ["O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "B-location", "I-location", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, music genre, band, country, award, location, person, organization, event, song, musical artist, album and O.\nSentence: He helped found the American Conservatory Theater in San Francisco , the Mark Taper Forum in Los Angeles , and the Brooklyn Academy of Music Repertory Company in New York City .", "prompt_labels": "He(O) helped(O) found(O) the(O) American(B-location) Conservatory(I-location) Theater(I-location) in(O) San(B-location) Francisco(I-location) ,(O) the(O) Mark(B-location) Taper(I-location) Forum(I-location) in(O) Los(B-location) Angeles(I-location) ,(O) and(O) the(O) Brooklyn(B-location) Academy(I-location) of(I-location) Music(I-location) Repertory(B-location) Company(I-location) in(O) New(B-location) York(I-location) City(I-location) .(O)"}}
{"id": "317", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "country", "song", "musical instrument", "organization", "award", "album", "person", "band", "event", "location", "music genre"], "instance": {"id": "317", "words": ["The", "Chess", "-", "concept", "album", "-", "with", "vocals", "by", "Elaine", "Paige", ",", "Barbara", "Dickson", ",", "Murray", "Head", "and", "Swedes", "Tommy", "Körberg", "and", "Björn", "Skifs", "-", "was", "released", "in", "October", "1984", ",", "selling", "two", "million", "copies", "worldwide", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, country, song, musical instrument, organization, award, album, person, band, event, location, music genre and O.\nSentence: The Chess - concept album - with vocals by Elaine Paige , Barbara Dickson , Murray Head and Swedes Tommy Körberg and Björn Skifs - was released in October 1984 , selling two million copies worldwide .", "prompt_labels": "The(O) Chess(O) -(O) concept(O) album(O) -(O) with(O) vocals(O) by(O) Elaine(B-musical artist) Paige(I-musical artist) ,(O) Barbara(B-musical artist) Dickson(I-musical artist) ,(O) Murray(B-musical artist) Head(I-musical artist) and(O) Swedes(O) Tommy(B-musical artist) Körberg(I-musical artist) and(O) Björn(B-musical artist) Skifs(I-musical artist) -(O) was(O) released(O) in(O) October(O) 1984(O) ,(O) selling(O) two(O) million(O) copies(O) worldwide(O) .(O)"}}
{"id": "29", "dataset": "crossner_music", "split": "test", "label_list": ["award", "event", "music genre", "musical artist", "musical instrument", "country", "song", "location", "person", "album", "band", "organization"], "instance": {"id": "29", "words": ["Her", "charting", "singles", "include", "Crucify", ",", "Silent", "All", "These", "Years", ",", "God", ",", "Cornflake", "Girl", ",", "Caught", "a", "Lite", "Sneeze", ",", "Professional", "Widow", ",", "Spark", ",", "1000", "Oceans", ",", "Flavor", "and", "A", "Sorta", "Fairytale", ",", "her", "most", "commercially", "successful", "single", "in", "the", "U.S.", "to", "date", "."], "labels": ["O", "O", "O", "O", "B-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "O", "B-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "O", "B-song", "O", "B-song", "I-song", "O", "B-song", "O", "B-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, music genre, musical artist, musical instrument, country, song, location, person, album, band, organization and O.\nSentence: Her charting singles include Crucify , Silent All These Years , God , Cornflake Girl , Caught a Lite Sneeze , Professional Widow , Spark , 1000 Oceans , Flavor and A Sorta Fairytale , her most commercially successful single in the U.S. to date .", "prompt_labels": "Her(O) charting(O) singles(O) include(O) Crucify(B-song) ,(O) Silent(B-song) All(I-song) These(I-song) Years(I-song) ,(O) God(B-song) ,(O) Cornflake(B-song) Girl(I-song) ,(O) Caught(B-song) a(I-song) Lite(I-song) Sneeze(I-song) ,(O) Professional(B-song) Widow(I-song) ,(O) Spark(B-song) ,(O) 1000(B-song) Oceans(I-song) ,(O) Flavor(B-song) and(O) A(B-song) Sorta(I-song) Fairytale(I-song) ,(O) her(O) most(O) commercially(O) successful(O) single(O) in(O) the(O) U.S.(B-country) to(O) date(O) .(O)"}}
{"id": "283", "dataset": "crossner_music", "split": "test", "label_list": ["country", "person", "album", "band", "event", "organization", "music genre", "song", "musical instrument", "location", "musical artist", "award"], "instance": {"id": "283", "words": ["In", "Finland", ",", "there", "emerged", "a", "scene", "that", "mixed", "the", "first", "wave", "black", "metal", "style", "with", "elements", "of", "death", "metal", "and", "grindcore", ";", "this", "included", "Beherit", ",", "Archgoat", "and", "Impaled", "Nazarene", ",", "whose", "debut", "album", "Tol", "Cormpt", "Norz", "Norz", "Norz", "Rock", "Hard", "journalist", "Wolf-Rüdiger", "Mühlmann", "considers", "a", "part", "of", "war", "metal", "'s", "roots", "."], "labels": ["O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "O", "O", "B-band", "O", "B-band", "O", "B-band", "I-band", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "B-organization", "I-organization", "O", "B-person", "I-person", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, album, band, event, organization, music genre, song, musical instrument, location, musical artist, award and O.\nSentence: In Finland , there emerged a scene that mixed the first wave black metal style with elements of death metal and grindcore ; this included Beherit , Archgoat and Impaled Nazarene , whose debut album Tol Cormpt Norz Norz Norz Rock Hard journalist Wolf-Rüdiger Mühlmann considers a part of war metal 's roots .", "prompt_labels": "In(O) Finland(B-country) ,(O) there(O) emerged(O) a(O) scene(O) that(O) mixed(O) the(O) first(O) wave(O) black(B-music genre) metal(I-music genre) style(O) with(O) elements(O) of(O) death(B-music genre) metal(I-music genre) and(O) grindcore(B-music genre) ;(O) this(O) included(O) Beherit(B-band) ,(O) Archgoat(B-band) and(O) Impaled(B-band) Nazarene(I-band) ,(O) whose(O) debut(O) album(O) Tol(B-album) Cormpt(I-album) Norz(I-album) Norz(I-album) Norz(I-album) Rock(B-organization) Hard(I-organization) journalist(O) Wolf-Rüdiger(B-person) Mühlmann(I-person) considers(O) a(O) part(O) of(O) war(B-music genre) metal(I-music genre) 's(O) roots(O) .(O)"}}
{"id": "151", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "person", "musical instrument", "country", "location", "band", "event", "music genre", "award", "organization", "song", "album"], "instance": {"id": "151", "words": ["The", "agreement", "sees", "an", "annual", "fixture", "at", "the", "MCG", ",", "beginning", "with", "a", "clash", "between", "Australia", "and", "European", "champions", "Greece", "on", "25", "May", "2006", "in", "front", "of", "a", "sell-out", "crowd", "of", "95,103", ",", "before", "Australia", "left", "to", "contest", "in", "the", "World", "Cup", "finals", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, person, musical instrument, country, location, band, event, music genre, award, organization, song, album and O.\nSentence: The agreement sees an annual fixture at the MCG , beginning with a clash between Australia and European champions Greece on 25 May 2006 in front of a sell-out crowd of 95,103 , before Australia left to contest in the World Cup finals .", "prompt_labels": "The(O) agreement(O) sees(O) an(O) annual(O) fixture(O) at(O) the(O) MCG(B-location) ,(O) beginning(O) with(O) a(O) clash(O) between(O) Australia(B-country) and(O) European(O) champions(O) Greece(B-country) on(O) 25(O) May(O) 2006(O) in(O) front(O) of(O) a(O) sell-out(O) crowd(O) of(O) 95,103(O) ,(O) before(O) Australia(B-country) left(O) to(O) contest(O) in(O) the(O) World(B-event) Cup(I-event) finals(O) .(O)"}}
{"id": "413", "dataset": "crossner_music", "split": "test", "label_list": ["band", "album", "location", "person", "award", "country", "musical instrument", "organization", "musical artist", "song", "event", "music genre"], "instance": {"id": "413", "words": ["He", "lent", "active", "support", "to", "several", "philanthropic", "endeavours", "such", "as", "the", "library", "at", "St.", "Mary", "'s", "School", ",", "Mumbai", ",", "Bombay", "Hospital", ",", "Child", "Rights", "and", "You", ",", "Save", "the", "Children", "and", "ALMA", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "B-location", "O", "B-location", "I-location", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, album, location, person, award, country, musical instrument, organization, musical artist, song, event, music genre and O.\nSentence: He lent active support to several philanthropic endeavours such as the library at St. Mary 's School , Mumbai , Bombay Hospital , Child Rights and You , Save the Children and ALMA .", "prompt_labels": "He(O) lent(O) active(O) support(O) to(O) several(O) philanthropic(O) endeavours(O) such(O) as(O) the(O) library(O) at(O) St.(B-location) Mary(I-location) 's(I-location) School(I-location) ,(O) Mumbai(B-location) ,(O) Bombay(B-location) Hospital(I-location) ,(O) Child(B-organization) Rights(I-organization) and(I-organization) You(I-organization) ,(O) Save(B-organization) the(I-organization) Children(I-organization) and(O) ALMA(B-organization) .(O)"}}
{"id": "270", "dataset": "crossner_music", "split": "test", "label_list": ["award", "person", "band", "musical artist", "event", "song", "location", "music genre", "album", "musical instrument", "country", "organization"], "instance": {"id": "270", "words": ["The", "mid", "2000s", ",", "especially", "the", "United", "Kingdom", "and", "the", "rest", "of", "Europe", ",", "saw", "the", "continued", "longevity", "of", "nineties", "boy", "bands", "such", "as", "Backstreet", "Boys", "and", "Westlife", "(", "before", "they", "disbanded", "in", "2012", ")", ",", "and", "the", "successful", "comeback", "of", "Take", "That", "in", "2005", ",", "Boyzone", "in", "2007", ",", "and", "New", "Kids", "on", "the", "Block", "in", "2008", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "O", "B-band", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, band, musical artist, event, song, location, music genre, album, musical instrument, country, organization and O.\nSentence: The mid 2000s , especially the United Kingdom and the rest of Europe , saw the continued longevity of nineties boy bands such as Backstreet Boys and Westlife ( before they disbanded in 2012 ) , and the successful comeback of Take That in 2005 , Boyzone in 2007 , and New Kids on the Block in 2008 .", "prompt_labels": "The(O) mid(O) 2000s(O) ,(O) especially(O) the(O) United(B-country) Kingdom(I-country) and(O) the(O) rest(O) of(O) Europe(B-location) ,(O) saw(O) the(O) continued(O) longevity(O) of(O) nineties(O) boy(O) bands(O) such(O) as(O) Backstreet(B-band) Boys(I-band) and(O) Westlife(B-band) ((O) before(O) they(O) disbanded(O) in(O) 2012(O) )(O) ,(O) and(O) the(O) successful(O) comeback(O) of(O) Take(B-band) That(I-band) in(O) 2005(O) ,(O) Boyzone(B-band) in(O) 2007(O) ,(O) and(O) New(B-band) Kids(I-band) on(I-band) the(I-band) Block(I-band) in(O) 2008(O) .(O)"}}
{"id": "365", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "musical artist", "location", "music genre", "organization", "award", "band", "event", "album", "person", "song", "country"], "instance": {"id": "365", "words": ["The", "station", "broadcast", "Christian", "feature", "stories", ",", "preaching", "and", "teaching", "to", "various", "fundamentalist", "and", "Pentecostal", "denominations", "and", "playing", "Traditional", "black", "gospel", ",", "Southern", "gospel", ",", "and", "inspirational", "music", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, musical artist, location, music genre, organization, award, band, event, album, person, song, country and O.\nSentence: The station broadcast Christian feature stories , preaching and teaching to various fundamentalist and Pentecostal denominations and playing Traditional black gospel , Southern gospel , and inspirational music .", "prompt_labels": "The(O) station(O) broadcast(O) Christian(O) feature(O) stories(O) ,(O) preaching(O) and(O) teaching(O) to(O) various(O) fundamentalist(O) and(O) Pentecostal(O) denominations(O) and(O) playing(O) Traditional(B-music genre) black(I-music genre) gospel(I-music genre) ,(O) Southern(B-music genre) gospel(I-music genre) ,(O) and(O) inspirational(O) music(O) .(O)"}}
{"id": "442", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "band", "musical instrument", "event", "album", "musical artist", "award", "country", "person", "song", "location", "organization"], "instance": {"id": "442", "words": ["Boogie-woogie", "was", "pioneered", "by", "the", "Chicago-based", "Jimmy", "Yancey", "and", "the", "Boogie-Woogie", "Trio", "(", "Albert", "Ammons", ",", "Pete", "Johnson", "and", "Meade", "Lux", "Lewis", ")", "."], "labels": ["B-music genre", "O", "O", "O", "O", "B-location", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, band, musical instrument, event, album, musical artist, award, country, person, song, location, organization and O.\nSentence: Boogie-woogie was pioneered by the Chicago-based Jimmy Yancey and the Boogie-Woogie Trio ( Albert Ammons , Pete Johnson and Meade Lux Lewis ) .", "prompt_labels": "Boogie-woogie(B-music genre) was(O) pioneered(O) by(O) the(O) Chicago-based(B-location) Jimmy(B-musical artist) Yancey(I-musical artist) and(O) the(O) Boogie-Woogie(O) Trio(O) ((O) Albert(B-musical artist) Ammons(I-musical artist) ,(O) Pete(B-musical artist) Johnson(I-musical artist) and(O) Meade(B-musical artist) Lux(I-musical artist) Lewis(I-musical artist) )(O) .(O)"}}
{"id": "248", "dataset": "crossner_music", "split": "test", "label_list": ["event", "country", "musical instrument", "album", "organization", "award", "musical artist", "person", "music genre", "location", "band", "song"], "instance": {"id": "248", "words": ["Every", "style", "of", "music", "was", "offered", ",", "from", "Rock", "music", "and", "Pop", "music", "to", "Spanish-language", "programming", "(", "for", "Mexican", "restaurants", ")", ",", "jazz", ",", "blues", ",", "classical", "and", "even", "easy", "listening", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "O", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, musical instrument, album, organization, award, musical artist, person, music genre, location, band, song and O.\nSentence: Every style of music was offered , from Rock music and Pop music to Spanish-language programming ( for Mexican restaurants ) , jazz , blues , classical and even easy listening .", "prompt_labels": "Every(O) style(O) of(O) music(O) was(O) offered(O) ,(O) from(O) Rock(B-music genre) music(I-music genre) and(O) Pop(B-music genre) music(I-music genre) to(O) Spanish-language(O) programming(O) ((O) for(O) Mexican(O) restaurants(O) )(O) ,(O) jazz(B-music genre) ,(O) blues(B-music genre) ,(O) classical(B-music genre) and(O) even(O) easy(B-music genre) listening(I-music genre) .(O)"}}
{"id": "451", "dataset": "crossner_music", "split": "test", "label_list": ["album", "event", "award", "country", "music genre", "musical artist", "musical instrument", "song", "location", "person", "organization", "band"], "instance": {"id": "451", "words": ["Country", "influences", "combined", "with", "Punk", "rock", "and", "alternative", "rock", "to", "forge", "the", "cowpunk", "scene", "in", "Southern", "California", "during", "the", "1980s", ",", "which", "included", "bands", "such", "as", "The", "Long", "Ryders", ",", "Lone", "Justice", "and", "The", "Beat", "Farmers", ",", "as", "well", "as", "the", "established", "punk", "group", "X", ",", "whose", "music", "had", "begun", "to", "include", "country", "and", "rockabilly", "influences", "."], "labels": ["O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "B-music genre", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-band", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, event, award, country, music genre, musical artist, musical instrument, song, location, person, organization, band and O.\nSentence: Country influences combined with Punk rock and alternative rock to forge the cowpunk scene in Southern California during the 1980s , which included bands such as The Long Ryders , Lone Justice and The Beat Farmers , as well as the established punk group X , whose music had begun to include country and rockabilly influences .", "prompt_labels": "Country(O) influences(O) combined(O) with(O) Punk(B-music genre) rock(I-music genre) and(O) alternative(B-music genre) rock(I-music genre) to(O) forge(O) the(O) cowpunk(B-music genre) scene(O) in(O) Southern(B-location) California(I-location) during(O) the(O) 1980s(O) ,(O) which(O) included(O) bands(O) such(O) as(O) The(B-band) Long(I-band) Ryders(I-band) ,(O) Lone(B-band) Justice(I-band) and(O) The(B-band) Beat(I-band) Farmers(I-band) ,(O) as(O) well(O) as(O) the(O) established(O) punk(B-music genre) group(O) X(B-band) ,(O) whose(O) music(O) had(O) begun(O) to(O) include(O) country(B-music genre) and(O) rockabilly(B-music genre) influences(O) .(O)"}}
{"id": "94", "dataset": "crossner_music", "split": "test", "label_list": ["award", "band", "song", "organization", "musical artist", "musical instrument", "music genre", "event", "person", "country", "location", "album"], "instance": {"id": "94", "words": ["Parsons", "acted", "as", "Assistant", "Engineer", "on", "the", "Beatles", "'", "albums", "Abbey", "Road", "(", "1969", ")", "and", "Let", "It", "Be", "(", "1970", ")", ",", "engineered", "Pink", "Floyd", "'", "s", "The", "Dark", "Side", "of", "the", "Moon", "(", "1973", ")", ",", "and", "produced", "several", "acts", "for", "EMI", "Records", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, band, song, organization, musical artist, musical instrument, music genre, event, person, country, location, album and O.\nSentence: Parsons acted as Assistant Engineer on the Beatles ' albums Abbey Road ( 1969 ) and Let It Be ( 1970 ) , engineered Pink Floyd ' s The Dark Side of the Moon ( 1973 ) , and produced several acts for EMI Records .", "prompt_labels": "Parsons(B-person) acted(O) as(O) Assistant(O) Engineer(O) on(O) the(B-band) Beatles(I-band) '(O) albums(O) Abbey(B-album) Road(I-album) ((O) 1969(O) )(O) and(O) Let(B-album) It(I-album) Be(I-album) ((O) 1970(O) )(O) ,(O) engineered(O) Pink(B-band) Floyd(I-band) '(O) s(O) The(B-album) Dark(I-album) Side(I-album) of(I-album) the(I-album) Moon(I-album) ((O) 1973(O) )(O) ,(O) and(O) produced(O) several(O) acts(O) for(O) EMI(B-organization) Records(I-organization) .(O)"}}
{"id": "173", "dataset": "crossner_music", "split": "test", "label_list": ["location", "organization", "musical artist", "band", "person", "album", "event", "music genre", "award", "musical instrument", "song", "country"], "instance": {"id": "173", "words": ["In", "June", "2010", ",", "ASCAP", "sent", "letters", "to", "its", "members", "soliciting", "donations", "to", "fight", "entities", "that", "support", "weaker", "copyright", "restrictions", ",", "such", "as", "Public", "Knowledge", ",", "the", "Electronic", "Frontier", "Foundation", ",", "and", "Creative", "Commons", ","], "labels": ["O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, musical artist, band, person, album, event, music genre, award, musical instrument, song, country and O.\nSentence: In June 2010 , ASCAP sent letters to its members soliciting donations to fight entities that support weaker copyright restrictions , such as Public Knowledge , the Electronic Frontier Foundation , and Creative Commons ,", "prompt_labels": "In(O) June(O) 2010(O) ,(O) ASCAP(B-organization) sent(O) letters(O) to(O) its(O) members(O) soliciting(O) donations(O) to(O) fight(O) entities(O) that(O) support(O) weaker(O) copyright(O) restrictions(O) ,(O) such(O) as(O) Public(B-organization) Knowledge(I-organization) ,(O) the(O) Electronic(B-organization) Frontier(I-organization) Foundation(I-organization) ,(O) and(O) Creative(B-organization) Commons(I-organization) ,(O)"}}
{"id": "230", "dataset": "crossner_music", "split": "test", "label_list": ["award", "music genre", "musical instrument", "event", "location", "organization", "musical artist", "song", "country", "band", "album", "person"], "instance": {"id": "230", "words": ["He", "toured", "with", "his", "band", "Les", "Mistigris", "(", "not", "related", "to", "Mistigris", ")", "in", "Germany", ",", "Belgium", ",", "France", "and", "Turkey", "until", "1967", "."], "labels": ["O", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "B-organization", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, music genre, musical instrument, event, location, organization, musical artist, song, country, band, album, person and O.\nSentence: He toured with his band Les Mistigris ( not related to Mistigris ) in Germany , Belgium , France and Turkey until 1967 .", "prompt_labels": "He(O) toured(O) with(O) his(O) band(O) Les(B-band) Mistigris(I-band) ((O) not(O) related(O) to(O) Mistigris(B-organization) )(O) in(O) Germany(B-country) ,(O) Belgium(B-country) ,(O) France(B-country) and(O) Turkey(B-country) until(O) 1967(O) .(O)"}}
{"id": "434", "dataset": "crossner_music", "split": "test", "label_list": ["event", "person", "band", "award", "country", "location", "album", "musical instrument", "musical artist", "song", "organization", "music genre"], "instance": {"id": "434", "words": ["Furthermore", ",", "he", "wrote", "the", "music", "for", "the", "Sestriere", "FIS", "Alpine", "World", "Ski", "Championships", "1997", "opening", "ceremony", "of", "1997", "including", "the", "official", "anthem", ",", "incidental", "music", "for", "stage", "productions", ",", "and", "a", "film", "score", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-location", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, band, award, country, location, album, musical instrument, musical artist, song, organization, music genre and O.\nSentence: Furthermore , he wrote the music for the Sestriere FIS Alpine World Ski Championships 1997 opening ceremony of 1997 including the official anthem , incidental music for stage productions , and a film score .", "prompt_labels": "Furthermore(O) ,(O) he(O) wrote(O) the(O) music(O) for(O) the(O) Sestriere(B-location) FIS(B-event) Alpine(I-event) World(I-event) Ski(I-event) Championships(I-event) 1997(I-event) opening(O) ceremony(O) of(O) 1997(O) including(O) the(O) official(O) anthem(O) ,(O) incidental(O) music(O) for(O) stage(O) productions(O) ,(O) and(O) a(O) film(O) score(O) .(O)"}}
{"id": "321", "dataset": "crossner_music", "split": "test", "label_list": ["band", "award", "organization", "country", "song", "music genre", "musical instrument", "event", "location", "person", "musical artist", "album"], "instance": {"id": "321", "words": ["In", "conjunction", "with", "the", "book", "and", "the", "play", "that", "also", "paid", "tribute", "to", "his", "uncle", ",", "Milt", "Gabler", ",", "Crystal", "produced", "two", "CD", "compilations", ":", "Billy", "Crystal", "Presents", ":", "The", "Milt", "Gabler", "Story", ",", "which", "featured", "his", "uncle", "'s", "most", "influential", "recordings", "from", "Billie", "Holiday", "'", "s", "Strange", "Fruit", "to", "Rock", "Around", "the", "Clock", "by", "Bill", "Haley", "&", "His", "Comets", ";", "and", "Billy", "Remembers", "Billie", "featuring", "Crystal", "'s", "favorite", "Holiday", "recordings", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-musical artist", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "O", "O", "B-album", "I-album", "I-album", "O", "B-musical artist", "O", "O", "B-musical artist", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, award, organization, country, song, music genre, musical instrument, event, location, person, musical artist, album and O.\nSentence: In conjunction with the book and the play that also paid tribute to his uncle , Milt Gabler , Crystal produced two CD compilations : Billy Crystal Presents : The Milt Gabler Story , which featured his uncle 's most influential recordings from Billie Holiday ' s Strange Fruit to Rock Around the Clock by Bill Haley & His Comets ; and Billy Remembers Billie featuring Crystal 's favorite Holiday recordings .", "prompt_labels": "In(O) conjunction(O) with(O) the(O) book(O) and(O) the(O) play(O) that(O) also(O) paid(O) tribute(O) to(O) his(O) uncle(O) ,(O) Milt(B-person) Gabler(I-person) ,(O) Crystal(B-musical artist) produced(O) two(O) CD(O) compilations(O) :(O) Billy(B-musical artist) Crystal(I-musical artist) Presents(O) :(O) The(B-album) Milt(I-album) Gabler(I-album) Story(I-album) ,(O) which(O) featured(O) his(O) uncle(O) 's(O) most(O) influential(O) recordings(O) from(O) Billie(B-musical artist) Holiday(I-musical artist) '(O) s(O) Strange(B-song) Fruit(I-song) to(O) Rock(B-song) Around(I-song) the(I-song) Clock(I-song) by(O) Bill(B-band) Haley(I-band) &(I-band) His(I-band) Comets(I-band) ;(O) and(O) Billy(B-album) Remembers(I-album) Billie(I-album) featuring(O) Crystal(B-musical artist) 's(O) favorite(O) Holiday(B-musical artist) recordings(O) .(O)"}}
{"id": "343", "dataset": "crossner_music", "split": "test", "label_list": ["award", "musical instrument", "event", "music genre", "location", "country", "musical artist", "song", "band", "organization", "person", "album"], "instance": {"id": "343", "words": ["In", "Southeast", "Asia", ",", "local", "boy", "bands", "also", "emerged", "as", "a", "result", "of", "the", "continued", "success", "of", "Korean", "and", "Japanese", "boy", "bands", "such", "as", "SMAP", ",", "Shinhwa", ",", "TVXQ", "!", ",", "Arashi", ",", "Exile", ",", "Super", "Junior", ",", "Big", "Bang", ",", "SHINee", ",", "EXO", ",", "and", "BTS", "."], "labels": ["O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "O", "O", "B-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical instrument, event, music genre, location, country, musical artist, song, band, organization, person, album and O.\nSentence: In Southeast Asia , local boy bands also emerged as a result of the continued success of Korean and Japanese boy bands such as SMAP , Shinhwa , TVXQ ! , Arashi , Exile , Super Junior , Big Bang , SHINee , EXO , and BTS .", "prompt_labels": "In(O) Southeast(B-location) Asia(I-location) ,(O) local(O) boy(O) bands(O) also(O) emerged(O) as(O) a(O) result(O) of(O) the(O) continued(O) success(O) of(O) Korean(O) and(O) Japanese(O) boy(O) bands(O) such(O) as(O) SMAP(B-band) ,(O) Shinhwa(B-band) ,(O) TVXQ(B-band) !(I-band) ,(O) Arashi(B-band) ,(O) Exile(B-band) ,(O) Super(B-band) Junior(I-band) ,(O) Big(B-band) Bang(I-band) ,(O) SHINee(B-band) ,(O) EXO(B-band) ,(O) and(O) BTS(B-band) .(O)"}}
{"id": "227", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "person", "award", "album", "musical instrument", "musical artist", "band", "event", "music genre", "location", "song", "country"], "instance": {"id": "227", "words": ["The", "latter", "earned", "Kidman", "the", "Primetime", "Emmy", "Award", "for", "Primetime", "Emmy", "Award", "for", "Outstanding", "Lead", "Actress", "in", "a", "Limited", "Series", "or", "Movie", "and", "Primetime", "Emmy", "Award", "for", "Outstanding", "Limited", "Series", "."], "labels": ["O", "O", "O", "B-person", "O", "B-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, award, album, musical instrument, musical artist, band, event, music genre, location, song, country and O.\nSentence: The latter earned Kidman the Primetime Emmy Award for Primetime Emmy Award for Outstanding Lead Actress in a Limited Series or Movie and Primetime Emmy Award for Outstanding Limited Series .", "prompt_labels": "The(O) latter(O) earned(O) Kidman(B-person) the(O) Primetime(B-award) Emmy(I-award) Award(I-award) for(O) Primetime(B-award) Emmy(I-award) Award(I-award) for(I-award) Outstanding(I-award) Lead(I-award) Actress(I-award) in(I-award) a(I-award) Limited(I-award) Series(I-award) or(I-award) Movie(I-award) and(O) Primetime(B-award) Emmy(I-award) Award(I-award) for(I-award) Outstanding(I-award) Limited(I-award) Series(I-award) .(O)"}}
{"id": "90", "dataset": "crossner_music", "split": "test", "label_list": ["event", "musical artist", "country", "award", "album", "song", "music genre", "organization", "band", "location", "person", "musical instrument"], "instance": {"id": "90", "words": ["He", "also", "co-wrote", "Posible", ",", "which", "has", "been", "used", "as", "a", "theme", "song", "for", "the", "2005", "Southeast", "Asian", "Games", "."], "labels": ["O", "O", "O", "B-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical artist, country, award, album, song, music genre, organization, band, location, person, musical instrument and O.\nSentence: He also co-wrote Posible , which has been used as a theme song for the 2005 Southeast Asian Games .", "prompt_labels": "He(O) also(O) co-wrote(O) Posible(B-song) ,(O) which(O) has(O) been(O) used(O) as(O) a(O) theme(O) song(O) for(O) the(O) 2005(B-event) Southeast(I-event) Asian(I-event) Games(I-event) .(O)"}}
{"id": "364", "dataset": "crossner_music", "split": "test", "label_list": ["album", "country", "song", "musical instrument", "location", "event", "musical artist", "person", "organization", "music genre", "award", "band"], "instance": {"id": "364", "words": ["The", "group", "'s", "original", "lineup", "consisted", "of", "brothers", "Brian", "Wilson", ",", "Dennis", "Wilson", ",", "and", "Carl", "Wilson", ",", "their", "cousin", "Mike", "Love", ",", "and", "their", "friend", "Al", "Jardine", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, country, song, musical instrument, location, event, musical artist, person, organization, music genre, award, band and O.\nSentence: The group 's original lineup consisted of brothers Brian Wilson , Dennis Wilson , and Carl Wilson , their cousin Mike Love , and their friend Al Jardine .", "prompt_labels": "The(O) group(O) 's(O) original(O) lineup(O) consisted(O) of(O) brothers(O) Brian(B-musical artist) Wilson(I-musical artist) ,(O) Dennis(B-musical artist) Wilson(I-musical artist) ,(O) and(O) Carl(B-musical artist) Wilson(I-musical artist) ,(O) their(O) cousin(O) Mike(B-musical artist) Love(I-musical artist) ,(O) and(O) their(O) friend(O) Al(B-musical artist) Jardine(I-musical artist) .(O)"}}
{"id": "142", "dataset": "crossner_music", "split": "test", "label_list": ["person", "song", "organization", "award", "location", "band", "country", "musical artist", "music genre", "album", "musical instrument", "event"], "instance": {"id": "142", "words": ["Three", "of", "them", ",", "Emergency", "on", "Planet", "Earth", "(", "1993", ")", ",", "Synkronized", "(", "1999", ")", ",", "A", "Funk", "Odyssey", "(", "2001", ")", ",", "along", "with", "their", "greatest", "hits", "compilation", ",", "charted", "at", "number", "1", "."], "labels": ["O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, song, organization, award, location, band, country, musical artist, music genre, album, musical instrument, event and O.\nSentence: Three of them , Emergency on Planet Earth ( 1993 ) , Synkronized ( 1999 ) , A Funk Odyssey ( 2001 ) , along with their greatest hits compilation , charted at number 1 .", "prompt_labels": "Three(O) of(O) them(O) ,(O) Emergency(B-album) on(I-album) Planet(I-album) Earth(I-album) ((O) 1993(O) )(O) ,(O) Synkronized(B-album) ((O) 1999(O) )(O) ,(O) A(B-album) Funk(I-album) Odyssey(I-album) ((O) 2001(O) )(O) ,(O) along(O) with(O) their(B-album) greatest(I-album) hits(I-album) compilation(O) ,(O) charted(O) at(O) number(O) 1(O) .(O)"}}
{"id": "424", "dataset": "crossner_music", "split": "test", "label_list": ["event", "music genre", "musical artist", "song", "country", "location", "album", "band", "musical instrument", "organization", "award", "person"], "instance": {"id": "424", "words": ["Rain", "Man", "won", "four", "Academy", "Awards", ",", "including", "Academy", "Award", "for", "Best", "Picture", ",", "Academy", "Award", "for", "Best", "Actor", "for", "Hoffman", ",", "and", "Academy", "Award", "for", "Best", "Director", "for", "Barry", "Levinson", "."], "labels": ["O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, music genre, musical artist, song, country, location, album, band, musical instrument, organization, award, person and O.\nSentence: Rain Man won four Academy Awards , including Academy Award for Best Picture , Academy Award for Best Actor for Hoffman , and Academy Award for Best Director for Barry Levinson .", "prompt_labels": "Rain(O) Man(O) won(O) four(O) Academy(B-award) Awards(I-award) ,(O) including(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) for(O) Hoffman(B-person) ,(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) for(O) Barry(B-person) Levinson(I-person) .(O)"}}
{"id": "229", "dataset": "crossner_music", "split": "test", "label_list": ["album", "music genre", "musical instrument", "event", "country", "award", "song", "musical artist", "person", "location", "organization", "band"], "instance": {"id": "229", "words": ["Martin", "Fry", "and", "band", "were", "once", "more", "accompanied", "by", "the", "Southbank", "Sinfonia", "Orchestra", "for", "dates", "at", "Liverpool", "'s", "Philharmonic", "Hall", ",", "Glasgow", "Royal", "Concert", "Hall", ",", "Sheffield", "City", "Hall", ",", "the", "Theatre", "Royal", ",", "Drury", "Lane", "in", "London", "and", "Symphony", "Hall", ",", "Birmingham", ",", "between", "November", "4th", "and", "9th", ",", "2015", "."], "labels": ["B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "O", "O", "B-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "I-location", "I-location", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "O", "B-location", "I-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, music genre, musical instrument, event, country, award, song, musical artist, person, location, organization, band and O.\nSentence: Martin Fry and band were once more accompanied by the Southbank Sinfonia Orchestra for dates at Liverpool 's Philharmonic Hall , Glasgow Royal Concert Hall , Sheffield City Hall , the Theatre Royal , Drury Lane in London and Symphony Hall , Birmingham , between November 4th and 9th , 2015 .", "prompt_labels": "Martin(B-musical artist) Fry(I-musical artist) and(O) band(O) were(O) once(O) more(O) accompanied(O) by(O) the(O) Southbank(B-band) Sinfonia(I-band) Orchestra(I-band) for(O) dates(O) at(O) Liverpool(B-location) 's(O) Philharmonic(B-location) Hall(I-location) ,(O) Glasgow(B-location) Royal(I-location) Concert(I-location) Hall(I-location) ,(O) Sheffield(B-location) City(I-location) Hall(I-location) ,(O) the(O) Theatre(B-location) Royal(I-location) ,(O) Drury(B-location) Lane(I-location) in(O) London(B-location) and(O) Symphony(B-location) Hall(I-location) ,(O) Birmingham(B-location) ,(O) between(O) November(O) 4th(O) and(O) 9th(O) ,(O) 2015(O) .(O)"}}
{"id": "356", "dataset": "crossner_music", "split": "test", "label_list": ["band", "country", "album", "organization", "song", "person", "musical instrument", "musical artist", "location", "music genre", "event", "award"], "instance": {"id": "356", "words": ["Traditional", "power", "metal", "bands", "like", "Sweden", "'s", "HammerFall", ",", "England", "'s", "DragonForce", ",", "and", "America", "'s", "Iced", "Earth", "have", "a", "sound", "clearly", "indebted", "to", "the", "classic", "NWOBHM", "style.See", ",", "e.g.", ",", "Reesman", "Bryan", "."], "labels": ["O", "B-music genre", "I-music genre", "O", "O", "B-country", "O", "B-band", "O", "B-country", "O", "B-band", "O", "O", "B-country", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, country, album, organization, song, person, musical instrument, musical artist, location, music genre, event, award and O.\nSentence: Traditional power metal bands like Sweden 's HammerFall , England 's DragonForce , and America 's Iced Earth have a sound clearly indebted to the classic NWOBHM style.See , e.g. , Reesman Bryan .", "prompt_labels": "Traditional(O) power(B-music genre) metal(I-music genre) bands(O) like(O) Sweden(B-country) 's(O) HammerFall(B-band) ,(O) England(B-country) 's(O) DragonForce(B-band) ,(O) and(O) America(B-country) 's(O) Iced(B-band) Earth(I-band) have(O) a(O) sound(O) clearly(O) indebted(O) to(O) the(O) classic(O) NWOBHM(B-music genre) style.See(O) ,(O) e.g.(O) ,(O) Reesman(B-person) Bryan(I-person) .(O)"}}
{"id": "310", "dataset": "crossner_music", "split": "test", "label_list": ["person", "country", "musical instrument", "band", "organization", "musical artist", "song", "album", "event", "award", "location", "music genre"], "instance": {"id": "310", "words": ["Pink", "is", "also", "involved", "with", "several", "charities", ",", "including", "Human", "Rights", "Campaign", ",", "ONE", "Campaign", ",", "Prince", "'s", "Trust", ",", "New", "York", "Restoration", "Project", ",", "Run", "for", "the", "Cure", "Foundation", ",", "Save", "the", "Children", ",", "Take", "Back", "the", "Night", ",", "UNICEF", "and", "World", "Animal", "Protection", ".", "Look", "to", "the", "Stars", ":", "that", "swept", "through", "the", "Australian", "state", "of", "Victoria", "earlier", "that", "month", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, musical instrument, band, organization, musical artist, song, album, event, award, location, music genre and O.\nSentence: Pink is also involved with several charities , including Human Rights Campaign , ONE Campaign , Prince 's Trust , New York Restoration Project , Run for the Cure Foundation , Save the Children , Take Back the Night , UNICEF and World Animal Protection . Look to the Stars : that swept through the Australian state of Victoria earlier that month .", "prompt_labels": "Pink(B-musical artist) is(O) also(O) involved(O) with(O) several(O) charities(O) ,(O) including(O) Human(B-organization) Rights(I-organization) Campaign(I-organization) ,(O) ONE(B-organization) Campaign(I-organization) ,(O) Prince(B-organization) 's(I-organization) Trust(I-organization) ,(O) New(B-organization) York(I-organization) Restoration(I-organization) Project(I-organization) ,(O) Run(B-organization) for(I-organization) the(I-organization) Cure(I-organization) Foundation(I-organization) ,(O) Save(B-organization) the(I-organization) Children(I-organization) ,(O) Take(B-organization) Back(I-organization) the(I-organization) Night(I-organization) ,(O) UNICEF(B-organization) and(O) World(B-organization) Animal(I-organization) Protection(I-organization) .(O) Look(O) to(O) the(O) Stars(O) :(O) that(O) swept(O) through(O) the(O) Australian(O) state(O) of(O) Victoria(B-location) earlier(O) that(O) month(O) .(O)"}}
{"id": "412", "dataset": "crossner_music", "split": "test", "label_list": ["location", "band", "award", "organization", "music genre", "musical instrument", "event", "song", "musical artist", "country", "person", "album"], "instance": {"id": "412", "words": ["Sometimes", "it", "is", "taken", "to", "include", "Greenland", "and", "historically", "the", "Baltic", "countries", "of", "Estonia", ",", "Latvia", "and", "Lithuania", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, band, award, organization, music genre, musical instrument, event, song, musical artist, country, person, album and O.\nSentence: Sometimes it is taken to include Greenland and historically the Baltic countries of Estonia , Latvia and Lithuania .", "prompt_labels": "Sometimes(O) it(O) is(O) taken(O) to(O) include(O) Greenland(B-country) and(O) historically(O) the(O) Baltic(O) countries(O) of(O) Estonia(B-country) ,(O) Latvia(B-country) and(O) Lithuania(B-country) .(O)"}}
{"id": "305", "dataset": "crossner_music", "split": "test", "label_list": ["location", "album", "music genre", "organization", "country", "event", "musical instrument", "band", "award", "song", "person", "musical artist"], "instance": {"id": "305", "words": ["The", "viola", "is", "also", "an", "important", "accompaniment", "instrument", "in", "Slovakia", "n", ",", "Hungary", "and", "Romania", "n", "folk", "string", "band", "music", ",", "especially", "in", "Transylvania", "."], "labels": ["O", "B-musical instrument", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-country", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, album, music genre, organization, country, event, musical instrument, band, award, song, person, musical artist and O.\nSentence: The viola is also an important accompaniment instrument in Slovakia n , Hungary and Romania n folk string band music , especially in Transylvania .", "prompt_labels": "The(O) viola(B-musical instrument) is(O) also(O) an(O) important(O) accompaniment(O) instrument(O) in(O) Slovakia(B-country) n(O) ,(O) Hungary(B-country) and(O) Romania(B-country) n(O) folk(O) string(O) band(O) music(O) ,(O) especially(O) in(O) Transylvania(B-location) .(O)"}}
{"id": "34", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "location", "event", "album", "country", "person", "band", "song", "musical instrument", "award", "organization", "music genre"], "instance": {"id": "34", "words": ["Other", "artists", "featured", "on", "the", "show", "include", "Michael", "Jackson", ",", "Barry", "White", ",", "Al", "Green", ",", "Tina", "Turner", ",", "Macy", "Gray", ",", "Gloria", "Gaynor", ",", "Chayanne", ",", "Barry", "Manilow", ",", "Anastacia", ",", "Elton", "John", ",", "Sting", "and", "Mariah", "Carey", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, event, album, country, person, band, song, musical instrument, award, organization, music genre and O.\nSentence: Other artists featured on the show include Michael Jackson , Barry White , Al Green , Tina Turner , Macy Gray , Gloria Gaynor , Chayanne , Barry Manilow , Anastacia , Elton John , Sting and Mariah Carey .", "prompt_labels": "Other(O) artists(O) featured(O) on(O) the(O) show(O) include(O) Michael(B-musical artist) Jackson(I-musical artist) ,(O) Barry(B-musical artist) White(I-musical artist) ,(O) Al(B-musical artist) Green(I-musical artist) ,(O) Tina(B-musical artist) Turner(I-musical artist) ,(O) Macy(B-musical artist) Gray(I-musical artist) ,(O) Gloria(B-musical artist) Gaynor(I-musical artist) ,(O) Chayanne(B-musical artist) ,(O) Barry(B-musical artist) Manilow(I-musical artist) ,(O) Anastacia(B-musical artist) ,(O) Elton(B-musical artist) John(I-musical artist) ,(O) Sting(B-musical artist) and(O) Mariah(B-musical artist) Carey(I-musical artist) .(O)"}}
{"id": "51", "dataset": "crossner_music", "split": "test", "label_list": ["song", "location", "album", "band", "musical artist", "music genre", "organization", "country", "event", "award", "person", "musical instrument"], "instance": {"id": "51", "words": ["Harper", "also", "contributed", "a", "recital", "of", "Jabberwocky", "for", "The", "Wildlife", "Album", ",", "an", "18-track", "compilation", "CD", "to", "benefit", "the", "World", "Wide", "Fund", "for", "Nature", "and", "the", "Ulster", "Wildlife", "Trust", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "B-song", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, location, album, band, musical artist, music genre, organization, country, event, award, person, musical instrument and O.\nSentence: Harper also contributed a recital of Jabberwocky for The Wildlife Album , an 18-track compilation CD to benefit the World Wide Fund for Nature and the Ulster Wildlife Trust .", "prompt_labels": "Harper(B-musical artist) also(O) contributed(O) a(O) recital(O) of(O) Jabberwocky(B-song) for(O) The(O) Wildlife(B-album) Album(I-album) ,(O) an(O) 18-track(O) compilation(O) CD(O) to(O) benefit(O) the(O) World(B-organization) Wide(I-organization) Fund(I-organization) for(I-organization) Nature(I-organization) and(O) the(O) Ulster(B-organization) Wildlife(I-organization) Trust(I-organization) .(O)"}}
{"id": "164", "dataset": "crossner_music", "split": "test", "label_list": ["award", "organization", "album", "person", "musical instrument", "musical artist", "music genre", "event", "band", "country", "location", "song"], "instance": {"id": "164", "words": ["Australian", "Film", "Institute", ",", "Blockbuster", "Entertainment", "Awards", ",", "Empire", "Awards", ",", "Hollywood", "Film", "Festival", ",", "London", "Film", "Critics", "'", "Circle", ",", "Russian", "Guild", "of", "Film", "Critics", ",", "Satellite", "Awards", ",", "and", "Southeastern", "Film", "Critics", "Association", "."], "labels": ["B-organization", "I-organization", "I-organization", "O", "B-award", "I-award", "I-award", "O", "B-award", "I-award", "O", "B-event", "I-event", "I-event", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-award", "I-award", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, album, person, musical instrument, musical artist, music genre, event, band, country, location, song and O.\nSentence: Australian Film Institute , Blockbuster Entertainment Awards , Empire Awards , Hollywood Film Festival , London Film Critics ' Circle , Russian Guild of Film Critics , Satellite Awards , and Southeastern Film Critics Association .", "prompt_labels": "Australian(B-organization) Film(I-organization) Institute(I-organization) ,(O) Blockbuster(B-award) Entertainment(I-award) Awards(I-award) ,(O) Empire(B-award) Awards(I-award) ,(O) Hollywood(B-event) Film(I-event) Festival(I-event) ,(O) London(B-organization) Film(I-organization) Critics(I-organization) '(I-organization) Circle(I-organization) ,(O) Russian(B-organization) Guild(I-organization) of(I-organization) Film(I-organization) Critics(I-organization) ,(O) Satellite(B-award) Awards(I-award) ,(O) and(O) Southeastern(B-organization) Film(I-organization) Critics(I-organization) Association(I-organization) .(O)"}}
{"id": "247", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "location", "person", "album", "award", "organization", "song", "band", "music genre", "musical artist", "event", "country"], "instance": {"id": "247", "words": ["In", "1999", ",", "it", "was", "certified", "3", "×", "platinum", "by", "the", "British", "Phonographic", "Industry", "(", "BPI", ")", ",", "3", "×", "Platinum", "by", "the", "Australian", "Record", "Industry", "Association", "and", "platinum", "by", "the", "Recording", "Industry", "Association", "of", "America", "(", "RIAA", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, location, person, album, award, organization, song, band, music genre, musical artist, event, country and O.\nSentence: In 1999 , it was certified 3 × platinum by the British Phonographic Industry ( BPI ) , 3 × Platinum by the Australian Record Industry Association and platinum by the Recording Industry Association of America ( RIAA ) .", "prompt_labels": "In(O) 1999(O) ,(O) it(O) was(O) certified(O) 3(O) ×(O) platinum(O) by(O) the(O) British(B-organization) Phonographic(I-organization) Industry(I-organization) ((O) BPI(B-organization) )(O) ,(O) 3(O) ×(O) Platinum(O) by(O) the(O) Australian(B-organization) Record(I-organization) Industry(I-organization) Association(I-organization) and(O) platinum(O) by(O) the(O) Recording(B-organization) Industry(I-organization) Association(I-organization) of(I-organization) America(I-organization) ((O) RIAA(B-organization) )(O) .(O)"}}
{"id": "279", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "music genre", "location", "person", "event", "song", "album", "band", "musical instrument", "award", "country", "organization"], "instance": {"id": "279", "words": ["Black", "Messiah", ",", "Beyoncé", "'", "s", "self-titled", "album", "(", "2013", ")", ",", "Run", "the", "Jewels", "'", "Run", "the", "Jewels", "2", "(", "2014", ")", ",", "and", "Kendrick", "Lamar", "'", "s", "To", "Pimp", "a", "Butterfly", "(", "2015", ")", "were", "noted", "as", "laying", "the", "groundwork", "down", "for", "the", "politically", "charged", "releases", "that", "happened", "in", "2016", ",", "which", "included", "Rihanna", "'", "s", "Anti", ",", "Kanye", "West", "'s", "The", "Life", "of", "Pablo", ",", "and", "Beyonce", "'s", "Formation", "."], "labels": ["B-album", "I-album", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "B-album", "O", "B-musical artist", "I-musical artist", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "B-musical artist", "O", "B-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, music genre, location, person, event, song, album, band, musical instrument, award, country, organization and O.\nSentence: Black Messiah , Beyoncé ' s self-titled album ( 2013 ) , Run the Jewels ' Run the Jewels 2 ( 2014 ) , and Kendrick Lamar ' s To Pimp a Butterfly ( 2015 ) were noted as laying the groundwork down for the politically charged releases that happened in 2016 , which included Rihanna ' s Anti , Kanye West 's The Life of Pablo , and Beyonce 's Formation .", "prompt_labels": "Black(B-album) Messiah(I-album) ,(O) Beyoncé(B-musical artist) '(O) s(O) self-titled(O) album(O) ((O) 2013(O) )(O) ,(O) Run(B-band) the(I-band) Jewels(I-band) '(O) Run(B-album) the(I-album) Jewels(I-album) 2(I-album) ((O) 2014(O) )(O) ,(O) and(O) Kendrick(B-musical artist) Lamar(I-musical artist) '(O) s(O) To(B-album) Pimp(I-album) a(I-album) Butterfly(I-album) ((O) 2015(O) )(O) were(O) noted(O) as(O) laying(O) the(O) groundwork(O) down(O) for(O) the(O) politically(O) charged(O) releases(O) that(O) happened(O) in(O) 2016(O) ,(O) which(O) included(O) Rihanna(B-musical artist) '(O) s(O) Anti(B-album) ,(O) Kanye(B-musical artist) West(I-musical artist) 's(O) The(B-album) Life(I-album) of(I-album) Pablo(I-album) ,(O) and(O) Beyonce(B-musical artist) 's(O) Formation(B-album) .(O)"}}
{"id": "281", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "song", "award", "musical artist", "country", "event", "location", "person", "music genre", "band", "musical instrument", "album"], "instance": {"id": "281", "words": ["With", "Nevermind", "The", "success", "of", "Nevermind", "provided", "numerous", "Seattle", "bands", ",", "such", "as", "Alice", "in", "Chains", ",", "Pearl", "Jam", ",", "and", "Soundgarden", ",", "access", "to", "wider", "audiences", "."], "labels": ["O", "B-album", "O", "O", "O", "B-album", "O", "O", "B-location", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "O", "B-band", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, song, award, musical artist, country, event, location, person, music genre, band, musical instrument, album and O.\nSentence: With Nevermind The success of Nevermind provided numerous Seattle bands , such as Alice in Chains , Pearl Jam , and Soundgarden , access to wider audiences .", "prompt_labels": "With(O) Nevermind(B-album) The(O) success(O) of(O) Nevermind(B-album) provided(O) numerous(O) Seattle(B-location) bands(O) ,(O) such(O) as(O) Alice(B-band) in(I-band) Chains(I-band) ,(O) Pearl(B-band) Jam(I-band) ,(O) and(O) Soundgarden(B-band) ,(O) access(O) to(O) wider(O) audiences(O) .(O)"}}
{"id": "137", "dataset": "crossner_music", "split": "test", "label_list": ["award", "country", "album", "musical instrument", "musical artist", "song", "person", "music genre", "location", "event", "band", "organization"], "instance": {"id": "137", "words": ["House", "of", "Pain", "abruptly", "broke", "up", "in", "1996", "after", "the", "release", "of", "their", "third", "album", ",", "Truth", "Crushed", "to", "Earth", "Shall", "Rise", "Again", ",", "which", "featured", "guest", "appearances", "by", "rappers", "Sadat", "X", "of", "Brand", "Nubian", ",", "Guru", "of", "Gang", "Starr", ",", "Divine", "Styler", "and", "Cokni", "O", "'Dire", "of", "the", "Scheme", "Team", "."], "labels": ["B-band", "I-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "B-musical artist", "O", "B-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, album, musical instrument, musical artist, song, person, music genre, location, event, band, organization and O.\nSentence: House of Pain abruptly broke up in 1996 after the release of their third album , Truth Crushed to Earth Shall Rise Again , which featured guest appearances by rappers Sadat X of Brand Nubian , Guru of Gang Starr , Divine Styler and Cokni O 'Dire of the Scheme Team .", "prompt_labels": "House(B-band) of(I-band) Pain(I-band) abruptly(O) broke(O) up(O) in(O) 1996(O) after(O) the(O) release(O) of(O) their(O) third(O) album(O) ,(O) Truth(B-album) Crushed(I-album) to(I-album) Earth(I-album) Shall(I-album) Rise(I-album) Again(I-album) ,(O) which(O) featured(O) guest(O) appearances(O) by(O) rappers(O) Sadat(B-musical artist) X(I-musical artist) of(O) Brand(B-band) Nubian(I-band) ,(O) Guru(B-musical artist) of(O) Gang(B-band) Starr(I-band) ,(O) Divine(B-musical artist) Styler(I-musical artist) and(O) Cokni(B-musical artist) O(I-musical artist) 'Dire(I-musical artist) of(O) the(O) Scheme(B-band) Team(I-band) .(O)"}}
{"id": "394", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "award", "country", "album", "organization", "person", "event", "musical artist", "song", "location", "band", "musical instrument"], "instance": {"id": "394", "words": ["The", "exceptions", "to", "this", "scheme", "were", "the", "band", "'s", "fourth", "album", ",", "a", "live", "boxed", "set", "entitled", "Chicago", "at", "Carnegie", "Hall", ",", "their", "twelfth", "album", "Hot", "Streets", ",", "and", "the", "Arabic-numbered", "Chicago", "13", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, award, country, album, organization, person, event, musical artist, song, location, band, musical instrument and O.\nSentence: The exceptions to this scheme were the band 's fourth album , a live boxed set entitled Chicago at Carnegie Hall , their twelfth album Hot Streets , and the Arabic-numbered Chicago 13 .", "prompt_labels": "The(O) exceptions(O) to(O) this(O) scheme(O) were(O) the(O) band(O) 's(O) fourth(O) album(O) ,(O) a(O) live(O) boxed(O) set(O) entitled(O) Chicago(B-album) at(I-album) Carnegie(I-album) Hall(I-album) ,(O) their(O) twelfth(O) album(O) Hot(B-album) Streets(I-album) ,(O) and(O) the(O) Arabic-numbered(O) Chicago(B-album) 13(I-album) .(O)"}}
{"id": "445", "dataset": "crossner_music", "split": "test", "label_list": ["song", "musical artist", "music genre", "band", "location", "album", "award", "musical instrument", "person", "country", "organization", "event"], "instance": {"id": "445", "words": ["O", "'Connell", "co-hosted", "the", "Miss", "USA", "and", "Miss", "Universe", "pageants", "with", "Bob", "Barker", "from", "1972", "to", "1980", "and", "was", "nominated", "for", "an", "Emmy", "award", "in", "1976", "for", "her", "coverage", "of", "the", "Miss", "Universe", "pageant", "."], "labels": ["B-musical artist", "I-musical artist", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, musical artist, music genre, band, location, album, award, musical instrument, person, country, organization, event and O.\nSentence: O 'Connell co-hosted the Miss USA and Miss Universe pageants with Bob Barker from 1972 to 1980 and was nominated for an Emmy award in 1976 for her coverage of the Miss Universe pageant .", "prompt_labels": "O(B-musical artist) 'Connell(I-musical artist) co-hosted(O) the(O) Miss(B-organization) USA(I-organization) and(O) Miss(B-organization) Universe(I-organization) pageants(O) with(O) Bob(B-person) Barker(I-person) from(O) 1972(O) to(O) 1980(O) and(O) was(O) nominated(O) for(O) an(O) Emmy(B-award) award(I-award) in(O) 1976(O) for(O) her(O) coverage(O) of(O) the(O) Miss(B-organization) Universe(I-organization) pageant(O) .(O)"}}
{"id": "374", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "country", "music genre", "musical instrument", "person", "organization", "event", "band", "award", "location", "album", "song"], "instance": {"id": "374", "words": ["Finn", "has", "recorded", "four", "solo", "albums", ",", "Try", "Whistling", "This", "(", "1998", ")", ",", "One", "Nil", "(", "2001", ")", ",", "Dizzy", "Heights", "(", "2014", ")", ",", "and", "Out", "of", "Silence", "(", "2017", ")", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, country, music genre, musical instrument, person, organization, event, band, award, location, album, song and O.\nSentence: Finn has recorded four solo albums , Try Whistling This ( 1998 ) , One Nil ( 2001 ) , Dizzy Heights ( 2014 ) , and Out of Silence ( 2017 ) .", "prompt_labels": "Finn(B-musical artist) has(O) recorded(O) four(O) solo(O) albums(O) ,(O) Try(B-album) Whistling(I-album) This(I-album) ((O) 1998(O) )(O) ,(O) One(B-album) Nil(I-album) ((O) 2001(O) )(O) ,(O) Dizzy(B-album) Heights(I-album) ((O) 2014(O) )(O) ,(O) and(O) Out(B-album) of(I-album) Silence(I-album) ((O) 2017(O) )(O) .(O)"}}
{"id": "298", "dataset": "crossner_music", "split": "test", "label_list": ["band", "musical artist", "person", "song", "award", "event", "country", "music genre", "album", "musical instrument", "organization", "location"], "instance": {"id": "298", "words": ["The", "box", "set", "was", "accompanied", "by", "5", "Album", "Studio", "Set", ",", "which", "contains", "only", "the", "first", "five", "studio", "albums", "(", "excluding", "Cut", "the", "Crap", ")", ",", "and", "The", "Clash", "Hits", "Back", ",", "a", "33-track", ",", "two-CD", "best", "of", "collection", "sequenced", "to", "copy", "the", "set", "played", "by", "the", "band", "at", "the", "Brixton", "Fair", "Deal", "(", "now", "the", "Academy", ")", "on", "19", "July", "1982", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical artist, person, song, award, event, country, music genre, album, musical instrument, organization, location and O.\nSentence: The box set was accompanied by 5 Album Studio Set , which contains only the first five studio albums ( excluding Cut the Crap ) , and The Clash Hits Back , a 33-track , two-CD best of collection sequenced to copy the set played by the band at the Brixton Fair Deal ( now the Academy ) on 19 July 1982 .", "prompt_labels": "The(O) box(O) set(O) was(O) accompanied(O) by(O) 5(B-album) Album(I-album) Studio(I-album) Set(I-album) ,(O) which(O) contains(O) only(O) the(O) first(O) five(O) studio(O) albums(O) ((O) excluding(O) Cut(B-album) the(I-album) Crap(I-album) )(O) ,(O) and(O) The(B-album) Clash(I-album) Hits(I-album) Back(I-album) ,(O) a(O) 33-track(O) ,(O) two-CD(O) best(O) of(O) collection(O) sequenced(O) to(O) copy(O) the(O) set(O) played(O) by(O) the(O) band(O) at(O) the(O) Brixton(B-organization) Fair(I-organization) Deal(I-organization) ((O) now(O) the(O) Academy(O) )(O) on(O) 19(O) July(O) 1982(O) .(O)"}}
{"id": "245", "dataset": "crossner_music", "split": "test", "label_list": ["band", "musical instrument", "location", "organization", "award", "song", "person", "album", "country", "event", "musical artist", "music genre"], "instance": {"id": "245", "words": ["In", "2004", ",", "Grohl", "drummed", "on", "several", "tracks", "for", "Nine", "Inch", "Nails", "'", "2005", "album", "With", "Teeth", ",", "later", "returning", "to", "play", "drums", "on", "'", "The", "Idea", "of", "You", "'", "from", "their", "2016", "EP", "Not", "the", "Actual", "Events", "."], "labels": ["O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "B-musical instrument", "O", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical instrument, location, organization, award, song, person, album, country, event, musical artist, music genre and O.\nSentence: In 2004 , Grohl drummed on several tracks for Nine Inch Nails ' 2005 album With Teeth , later returning to play drums on ' The Idea of You ' from their 2016 EP Not the Actual Events .", "prompt_labels": "In(O) 2004(O) ,(O) Grohl(B-musical artist) drummed(O) on(O) several(O) tracks(O) for(O) Nine(B-band) Inch(I-band) Nails(I-band) '(O) 2005(O) album(O) With(B-album) Teeth(I-album) ,(O) later(O) returning(O) to(O) play(O) drums(B-musical instrument) on(O) '(O) The(B-song) Idea(I-song) of(I-song) You(I-song) '(O) from(O) their(O) 2016(O) EP(O) Not(B-album) the(I-album) Actual(I-album) Events(I-album) .(O)"}}
{"id": "79", "dataset": "crossner_music", "split": "test", "label_list": ["person", "album", "country", "organization", "music genre", "song", "musical instrument", "location", "event", "band", "award", "musical artist"], "instance": {"id": "79", "words": ["They", "play", "Avant-garde", "jazz", "versions", "of", "tradition", "American", "Folk", "music", "&", "amp", ";", "Blues", "songs", "with", "Ritchie", "'s", "shakuhachi", "playing", "as", "the", "focal", "point", "."], "labels": ["O", "O", "B-music genre", "I-music genre", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "B-music genre", "O", "O", "B-musical artist", "O", "B-musical instrument", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, album, country, organization, music genre, song, musical instrument, location, event, band, award, musical artist and O.\nSentence: They play Avant-garde jazz versions of tradition American Folk music & amp ; Blues songs with Ritchie 's shakuhachi playing as the focal point .", "prompt_labels": "They(O) play(O) Avant-garde(B-music genre) jazz(I-music genre) versions(O) of(O) tradition(O) American(B-music genre) Folk(I-music genre) music(I-music genre) &(O) amp(O) ;(O) Blues(B-music genre) songs(O) with(O) Ritchie(B-musical artist) 's(O) shakuhachi(B-musical instrument) playing(O) as(O) the(O) focal(O) point(O) .(O)"}}
{"id": "117", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "country", "person", "organization", "album", "music genre", "song", "band", "musical instrument", "award", "event", "location"], "instance": {"id": "117", "words": ["Buckingham", "'s", "Second", "Hand", "News", ",", "Nicks", "'s", "Gold", "Dust", "Woman", "and", "The", "Chain", "(", "the", "only", "song", "written", "by", "all", "five", "band", "members", ")", "also", "received", "significant", "radio", "airplay", "."], "labels": ["B-musical artist", "O", "B-song", "I-song", "I-song", "O", "B-musical artist", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, country, person, organization, album, music genre, song, band, musical instrument, award, event, location and O.\nSentence: Buckingham 's Second Hand News , Nicks 's Gold Dust Woman and The Chain ( the only song written by all five band members ) also received significant radio airplay .", "prompt_labels": "Buckingham(B-musical artist) 's(O) Second(B-song) Hand(I-song) News(I-song) ,(O) Nicks(B-musical artist) 's(O) Gold(B-song) Dust(I-song) Woman(I-song) and(O) The(B-song) Chain(I-song) ((O) the(O) only(O) song(O) written(O) by(O) all(O) five(O) band(O) members(O) )(O) also(O) received(O) significant(O) radio(O) airplay(O) .(O)"}}
{"id": "138", "dataset": "crossner_music", "split": "test", "label_list": ["event", "person", "musical instrument", "musical artist", "song", "organization", "location", "award", "album", "band", "country", "music genre"], "instance": {"id": "138", "words": ["Kid", "A", "received", "a", "Grammy", "Award", "for", "Grammy", "Award", "for", "Best", "Alternative", "Music", "Album", "and", "a", "nomination", "for", "Grammy", "Award", "for", "Album", "of", "the", "Year", "in", "early", "2001", "."], "labels": ["B-album", "I-album", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, musical instrument, musical artist, song, organization, location, award, album, band, country, music genre and O.\nSentence: Kid A received a Grammy Award for Grammy Award for Best Alternative Music Album and a nomination for Grammy Award for Album of the Year in early 2001 .", "prompt_labels": "Kid(B-album) A(I-album) received(O) a(O) Grammy(B-award) Award(I-award) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Alternative(I-award) Music(I-award) Album(I-award) and(O) a(O) nomination(O) for(O) Grammy(B-award) Award(I-award) for(I-award) Album(I-award) of(I-award) the(I-award) Year(I-award) in(O) early(O) 2001(O) .(O)"}}
{"id": "71", "dataset": "crossner_music", "split": "test", "label_list": ["award", "music genre", "location", "event", "person", "organization", "musical artist", "song", "band", "country", "musical instrument", "album"], "instance": {"id": "71", "words": ["There", "have", "also", "been", "several", "spin-offs", "featuring", "one", "or", "more", "core", "members", ",", "such", "as", "Dead", "&", "amp", ";", "Company", ",", "Furthur", ",", "the", "Rhythm", "Devils", ",", "Phil", "Lesh", "and", "Friends", ",", "RatDog", ",", "and", "Billy", "&", "amp", ";", "the", "Kids", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "O", "B-band", "O", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "O", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, music genre, location, event, person, organization, musical artist, song, band, country, musical instrument, album and O.\nSentence: There have also been several spin-offs featuring one or more core members , such as Dead & amp ; Company , Furthur , the Rhythm Devils , Phil Lesh and Friends , RatDog , and Billy & amp ; the Kids .", "prompt_labels": "There(O) have(O) also(O) been(O) several(O) spin-offs(O) featuring(O) one(O) or(O) more(O) core(O) members(O) ,(O) such(O) as(O) Dead(B-band) &(I-band) amp(I-band) ;(I-band) Company(I-band) ,(O) Furthur(B-band) ,(O) the(O) Rhythm(B-band) Devils(I-band) ,(O) Phil(B-band) Lesh(I-band) and(I-band) Friends(I-band) ,(O) RatDog(B-band) ,(O) and(O) Billy(B-band) &(I-band) amp(I-band) ;(I-band) the(I-band) Kids(I-band) .(O)"}}
{"id": "406", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "song", "album", "band", "event", "location", "country", "music genre", "award", "musical artist", "organization", "person"], "instance": {"id": "406", "words": ["Notable", "recital", "engagements", "have", "included", "her", "Carnegie", "Hall", "debut", "and", "performances", "at", "the", "Kennedy", "Center", ",", "Symphony", "Center", ",", "Symphony", "Hall", ",", "Barbican", "Centre", ",", "Philharmonie", ",", "and", "Concertgebouw", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, song, album, band, event, location, country, music genre, award, musical artist, organization, person and O.\nSentence: Notable recital engagements have included her Carnegie Hall debut and performances at the Kennedy Center , Symphony Center , Symphony Hall , Barbican Centre , Philharmonie , and Concertgebouw .", "prompt_labels": "Notable(O) recital(O) engagements(O) have(O) included(O) her(O) Carnegie(B-location) Hall(I-location) debut(O) and(O) performances(O) at(O) the(O) Kennedy(B-location) Center(I-location) ,(O) Symphony(B-location) Center(I-location) ,(O) Symphony(B-location) Hall(I-location) ,(O) Barbican(B-location) Centre(I-location) ,(O) Philharmonie(B-location) ,(O) and(O) Concertgebouw(B-location) .(O)"}}
{"id": "209", "dataset": "crossner_music", "split": "test", "label_list": ["album", "country", "band", "location", "music genre", "musical instrument", "event", "song", "organization", "award", "musical artist", "person"], "instance": {"id": "209", "words": ["He", "also", "tried", "his", "hand", "at", "music", "production", ",", "producing", "Idoli", "'", "s", "1980", "seven-inch", "single", "Maljčiki", "/", "Retko", "te", "viđam", "sa", "devojkama", "and", "co-producing", ",", "alongside", "Kornelije", "Kovač", ",", "Zdravko", "Čolić", "'", "s", "fourth", "studio", "album", "Malo", "pojačaj", "radio", "in", "1981", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "O", "O", "O", "O", "B-song", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, country, band, location, music genre, musical instrument, event, song, organization, award, musical artist, person and O.\nSentence: He also tried his hand at music production , producing Idoli ' s 1980 seven-inch single Maljčiki / Retko te viđam sa devojkama and co-producing , alongside Kornelije Kovač , Zdravko Čolić ' s fourth studio album Malo pojačaj radio in 1981 .", "prompt_labels": "He(O) also(O) tried(O) his(O) hand(O) at(O) music(O) production(O) ,(O) producing(O) Idoli(B-band) '(O) s(O) 1980(O) seven-inch(O) single(O) Maljčiki(B-song) /(O) Retko(B-song) te(I-song) viđam(I-song) sa(I-song) devojkama(I-song) and(O) co-producing(O) ,(O) alongside(O) Kornelije(B-musical artist) Kovač(I-musical artist) ,(O) Zdravko(B-musical artist) Čolić(I-musical artist) '(O) s(O) fourth(O) studio(O) album(O) Malo(B-album) pojačaj(I-album) radio(I-album) in(O) 1981(O) .(O)"}}
{"id": "154", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "person", "song", "music genre", "event", "country", "album", "location", "musical artist", "award", "band", "organization"], "instance": {"id": "154", "words": ["Contemporary", "African-American", "performers", "who", "work", "in", "this", "style", "of", "the", "blues", "include", "Bobby", "Rush", ",", "Denise", "LaSalle", ",", "Sir", "Charles", "Jones", ",", "Bettye", "LaVette", ",", "Marvin", "Sease", ",", "Peggy", "Scott-Adams", ",", "Mel", "Waiters", ",", "Clarence", "Carter", ",", "Dr.", "Feelgood", "Potts", ",", "O.B.", "Buchana", ",", "Ms.", "Jody", ",", "Shirley", "Brown", ",", "and", "dozens", "of", "others", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, person, song, music genre, event, country, album, location, musical artist, award, band, organization and O.\nSentence: Contemporary African-American performers who work in this style of the blues include Bobby Rush , Denise LaSalle , Sir Charles Jones , Bettye LaVette , Marvin Sease , Peggy Scott-Adams , Mel Waiters , Clarence Carter , Dr. Feelgood Potts , O.B. Buchana , Ms. Jody , Shirley Brown , and dozens of others .", "prompt_labels": "Contemporary(O) African-American(O) performers(O) who(O) work(O) in(O) this(O) style(O) of(O) the(O) blues(B-music genre) include(O) Bobby(B-musical artist) Rush(I-musical artist) ,(O) Denise(B-musical artist) LaSalle(I-musical artist) ,(O) Sir(B-musical artist) Charles(I-musical artist) Jones(I-musical artist) ,(O) Bettye(B-musical artist) LaVette(I-musical artist) ,(O) Marvin(B-musical artist) Sease(I-musical artist) ,(O) Peggy(B-musical artist) Scott-Adams(I-musical artist) ,(O) Mel(B-musical artist) Waiters(I-musical artist) ,(O) Clarence(B-musical artist) Carter(I-musical artist) ,(O) Dr.(B-musical artist) Feelgood(I-musical artist) Potts(I-musical artist) ,(O) O.B.(B-musical artist) Buchana(I-musical artist) ,(O) Ms.(B-musical artist) Jody(I-musical artist) ,(O) Shirley(B-musical artist) Brown(I-musical artist) ,(O) and(O) dozens(O) of(O) others(O) .(O)"}}
{"id": "307", "dataset": "crossner_music", "split": "test", "label_list": ["person", "song", "country", "location", "event", "band", "musical instrument", "music genre", "organization", "award", "album", "musical artist"], "instance": {"id": "307", "words": ["However", ",", "some", "alt-country", "songs", "have", "been", "crossover", "hits", "to", "mainstream", "country", "radio", "in", "cover", "versions", "by", "established", "artists", "on", "the", "format", ";", "Lucinda", "Williams", "'", "Passionate", "Kisses", "was", "a", "hit", "for", "Mary", "Chapin", "Carpenter", "in", "1993", ",", "Ryan", "Adams", "'s", "When", "The", "Stars", "Go", "Blue", "was", "a", "hit", "for", "Tim", "McGraw", "in", "2007", ",", "and", "Old", "Crow", "Medicine", "Show", "'s", "Wagon", "Wheel", "was", "a", "hit", "for", "Darius", "Rucker", "in", "2013", "."], "labels": ["O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-song", "I-song", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "B-band", "I-band", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, song, country, location, event, band, musical instrument, music genre, organization, award, album, musical artist and O.\nSentence: However , some alt-country songs have been crossover hits to mainstream country radio in cover versions by established artists on the format ; Lucinda Williams ' Passionate Kisses was a hit for Mary Chapin Carpenter in 1993 , Ryan Adams 's When The Stars Go Blue was a hit for Tim McGraw in 2007 , and Old Crow Medicine Show 's Wagon Wheel was a hit for Darius Rucker in 2013 .", "prompt_labels": "However(O) ,(O) some(O) alt-country(B-music genre) songs(O) have(O) been(O) crossover(O) hits(O) to(O) mainstream(O) country(O) radio(O) in(O) cover(O) versions(O) by(O) established(O) artists(O) on(O) the(O) format(O) ;(O) Lucinda(B-musical artist) Williams(I-musical artist) '(O) Passionate(B-song) Kisses(I-song) was(O) a(O) hit(O) for(O) Mary(B-musical artist) Chapin(I-musical artist) Carpenter(I-musical artist) in(O) 1993(O) ,(O) Ryan(B-musical artist) Adams(I-musical artist) 's(O) When(B-song) The(I-song) Stars(I-song) Go(I-song) Blue(I-song) was(O) a(O) hit(O) for(O) Tim(B-musical artist) McGraw(I-musical artist) in(O) 2007(O) ,(O) and(O) Old(B-band) Crow(I-band) Medicine(I-band) Show(I-band) 's(I-band) Wagon(B-band) Wheel(I-band) was(O) a(O) hit(O) for(O) Darius(B-musical artist) Rucker(I-musical artist) in(O) 2013(O) .(O)"}}
{"id": "405", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "event", "award", "musical artist", "location", "musical instrument", "country", "person", "organization", "song", "band", "album"], "instance": {"id": "405", "words": ["He", "worked", "with", "Berlin", "State", "Opera", ";", "La", "Scala", ",", "Milan", ";", "Royal", "Opera", "Stockholm", ";", "the", "Royal", "Opera", "House", "at", "Covent", "Garden", ",", "Chorégies", "d", "'Orange", "and", "Houston", "Grand", "Opera", "."], "labels": ["O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "I-location", "O", "B-location", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, event, award, musical artist, location, musical instrument, country, person, organization, song, band, album and O.\nSentence: He worked with Berlin State Opera ; La Scala , Milan ; Royal Opera Stockholm ; the Royal Opera House at Covent Garden , Chorégies d 'Orange and Houston Grand Opera .", "prompt_labels": "He(O) worked(O) with(O) Berlin(B-organization) State(I-organization) Opera(I-organization) ;(O) La(B-location) Scala(I-location) ,(O) Milan(B-location) ;(O) Royal(B-location) Opera(I-location) Stockholm(I-location) ;(O) the(O) Royal(B-location) Opera(I-location) House(I-location) at(O) Covent(B-location) Garden(I-location) ,(O) Chorégies(B-location) d(I-location) 'Orange(I-location) and(O) Houston(B-location) Grand(I-location) Opera(I-location) .(O)"}}
{"id": "25", "dataset": "crossner_music", "split": "test", "label_list": ["song", "person", "location", "musical instrument", "band", "organization", "country", "event", "album", "award", "music genre", "musical artist"], "instance": {"id": "25", "words": ["The", "Convention", "Centre", "hosted", "preliminary", "rounds", "of", "the", "men", "'s", "basketball", "competition", "at", "the", "Commonwealth", "Games", ",", "which", "was", "held", "on", "Queensland", "'s", "Gold", "Coast", "in", "April", "2018", "Commonwealth", "Games", "."], "labels": ["O", "B-location", "I-location", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, person, location, musical instrument, band, organization, country, event, album, award, music genre, musical artist and O.\nSentence: The Convention Centre hosted preliminary rounds of the men 's basketball competition at the Commonwealth Games , which was held on Queensland 's Gold Coast in April 2018 Commonwealth Games .", "prompt_labels": "The(O) Convention(B-location) Centre(I-location) hosted(O) preliminary(O) rounds(O) of(O) the(O) men(B-event) 's(I-event) basketball(I-event) competition(I-event) at(O) the(O) Commonwealth(B-event) Games(I-event) ,(O) which(O) was(O) held(O) on(O) Queensland(B-location) 's(I-location) Gold(I-location) Coast(I-location) in(O) April(O) 2018(B-event) Commonwealth(I-event) Games(I-event) .(O)"}}
{"id": "16", "dataset": "crossner_music", "split": "test", "label_list": ["country", "band", "music genre", "musical artist", "person", "award", "event", "musical instrument", "album", "song", "location", "organization"], "instance": {"id": "16", "words": ["The", "film", "won", "Academy", "Awards", "for", "Academy", "Award", "for", "Best", "Costume", "Design", ",", "Academy", "Award", "for", "Best", "Makeup", "and", "Hairstyling", "and", "Academy", "Award", "for", "Best", "Sound", "Editing", "."], "labels": ["O", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, band, music genre, musical artist, person, award, event, musical instrument, album, song, location, organization and O.\nSentence: The film won Academy Awards for Academy Award for Best Costume Design , Academy Award for Best Makeup and Hairstyling and Academy Award for Best Sound Editing .", "prompt_labels": "The(O) film(O) won(O) Academy(B-award) Awards(I-award) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Costume(I-award) Design(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Makeup(I-award) and(I-award) Hairstyling(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Sound(I-award) Editing(I-award) .(O)"}}
{"id": "430", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "musical instrument", "musical artist", "band", "person", "album", "award", "event", "location", "organization", "country", "song"], "instance": {"id": "430", "words": ["Nitin", "Mishra", "came", "up", "with", "contemporary", "storylines", "like", "'", "Hum", "Honge", "Kamyaab", "'", "in", "2010", ",", "which", "was", "his", "take", "on", "the", "controversies", "attached", "with", "the", "2010", "Commonwealth", "Games", "conducted", "in", "India", "in", "2010", "."], "labels": ["B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-country", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical instrument, musical artist, band, person, album, award, event, location, organization, country, song and O.\nSentence: Nitin Mishra came up with contemporary storylines like ' Hum Honge Kamyaab ' in 2010 , which was his take on the controversies attached with the 2010 Commonwealth Games conducted in India in 2010 .", "prompt_labels": "Nitin(B-musical artist) Mishra(I-musical artist) came(O) up(O) with(O) contemporary(O) storylines(O) like(O) '(O) Hum(B-song) Honge(I-song) Kamyaab(I-song) '(O) in(O) 2010(O) ,(O) which(O) was(O) his(O) take(O) on(O) the(O) controversies(O) attached(O) with(O) the(O) 2010(B-event) Commonwealth(I-event) Games(I-event) conducted(O) in(O) India(B-country) in(O) 2010(O) .(O)"}}
{"id": "273", "dataset": "crossner_music", "split": "test", "label_list": ["award", "event", "band", "person", "musical artist", "location", "organization", "musical instrument", "song", "country", "album", "music genre"], "instance": {"id": "273", "words": ["A", "few", "of", "those", "include", "bagad", "(", "Breton", "pipe", "bands", ")", ",", "Fairport", "Convention", ",", "Pentangle", ",", "Steeleye", "Span", "and", "Horslips", "."], "labels": ["O", "O", "O", "O", "O", "B-band", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, band, person, musical artist, location, organization, musical instrument, song, country, album, music genre and O.\nSentence: A few of those include bagad ( Breton pipe bands ) , Fairport Convention , Pentangle , Steeleye Span and Horslips .", "prompt_labels": "A(O) few(O) of(O) those(O) include(O) bagad(B-band) ((O) Breton(O) pipe(O) bands(O) )(O) ,(O) Fairport(B-band) Convention(I-band) ,(O) Pentangle(B-band) ,(O) Steeleye(B-band) Span(I-band) and(O) Horslips(B-band) .(O)"}}
{"id": "383", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "event", "organization", "country", "song", "band", "music genre", "award", "album", "musical artist", "person", "location"], "instance": {"id": "383", "words": ["A", "third", "UK", "tour", "for", "2017", "/", "2018", "opened", "at", "the", "Curve", "in", "Leicester", ",", "and", "also", "toured", "to", "the", "Birmingham", "Hippodrome", ",", "the", "Bord", "Gáis", "Energy", "Theatre", "in", "Dublin", ",", "the", "Wales", "Millennium", "Centre", "in", "Cardiff", ",", "the", "Edinburgh", "Festival", "Theatre", ",", "the", "Mayflower", "Theatre", "in", "Southampton", "and", "the", "Palace", "Theatre", "in", "Manchester", "."], "labels": ["O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, event, organization, country, song, band, music genre, award, album, musical artist, person, location and O.\nSentence: A third UK tour for 2017 / 2018 opened at the Curve in Leicester , and also toured to the Birmingham Hippodrome , the Bord Gáis Energy Theatre in Dublin , the Wales Millennium Centre in Cardiff , the Edinburgh Festival Theatre , the Mayflower Theatre in Southampton and the Palace Theatre in Manchester .", "prompt_labels": "A(O) third(O) UK(B-country) tour(O) for(O) 2017(O) /(O) 2018(O) opened(O) at(O) the(O) Curve(B-organization) in(O) Leicester(B-location) ,(O) and(O) also(O) toured(O) to(O) the(O) Birmingham(B-location) Hippodrome(I-location) ,(O) the(O) Bord(B-location) Gáis(I-location) Energy(I-location) Theatre(I-location) in(O) Dublin(B-location) ,(O) the(O) Wales(B-location) Millennium(I-location) Centre(I-location) in(O) Cardiff(B-location) ,(O) the(O) Edinburgh(B-location) Festival(I-location) Theatre(I-location) ,(O) the(O) Mayflower(B-location) Theatre(I-location) in(O) Southampton(B-location) and(O) the(O) Palace(B-location) Theatre(I-location) in(O) Manchester(B-location) .(O)"}}
{"id": "250", "dataset": "crossner_music", "split": "test", "label_list": ["event", "person", "album", "band", "award", "music genre", "musical instrument", "musical artist", "song", "location", "country", "organization"], "instance": {"id": "250", "words": ["The", "band", "also", "saw", "success", "with", "the", "rest", "of", "the", "album", "'s", "singles", ",", "Bleed", "It", "Out", ",", "Shadow", "of", "the", "Day", ",", "Given", "Up", ",", "and", "Leave", "Out", "All", "the", "Rest", ",", "which", "were", "released", "throughout", "2007", "and", "early", "2008", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, album, band, award, music genre, musical instrument, musical artist, song, location, country, organization and O.\nSentence: The band also saw success with the rest of the album 's singles , Bleed It Out , Shadow of the Day , Given Up , and Leave Out All the Rest , which were released throughout 2007 and early 2008 .", "prompt_labels": "The(O) band(O) also(O) saw(O) success(O) with(O) the(O) rest(O) of(O) the(O) album(O) 's(O) singles(O) ,(O) Bleed(B-song) It(I-song) Out(I-song) ,(O) Shadow(B-song) of(I-song) the(I-song) Day(I-song) ,(O) Given(B-song) Up(I-song) ,(O) and(O) Leave(B-song) Out(I-song) All(I-song) the(I-song) Rest(I-song) ,(O) which(O) were(O) released(O) throughout(O) 2007(O) and(O) early(O) 2008(O) .(O)"}}
{"id": "127", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "musical instrument", "award", "location", "song", "band", "album", "event", "musical artist", "organization", "country", "person"], "instance": {"id": "127", "words": ["Several", "countries", "geographically", "outside", "the", "boundaries", "of", "Europe", "have", "competed", ":", "Israel", ",", "Cyprus", "and", "Armenia", "in", "Western", "Asia", "(", "Cyprus", "is", "a", "member", "of", "the", "Council", "of", "Europe", "and", "a", "member", "state", "of", "the", "European", "Union", ")", ",", "since", "1973", ",", "1981", "and", "Eurovision", "Song", "Contest", "2006", "respectively", ";", "Australia", "since", "Eurovision", "Song", "Contest", "2015"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-location", "I-location", "O", "B-country", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "B-country", "O", "B-event", "I-event", "I-event", "I-event"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical instrument, award, location, song, band, album, event, musical artist, organization, country, person and O.\nSentence: Several countries geographically outside the boundaries of Europe have competed : Israel , Cyprus and Armenia in Western Asia ( Cyprus is a member of the Council of Europe and a member state of the European Union ) , since 1973 , 1981 and Eurovision Song Contest 2006 respectively ; Australia since Eurovision Song Contest 2015", "prompt_labels": "Several(O) countries(O) geographically(O) outside(O) the(O) boundaries(O) of(O) Europe(B-location) have(O) competed(O) :(O) Israel(B-country) ,(O) Cyprus(B-country) and(O) Armenia(B-country) in(O) Western(B-location) Asia(I-location) ((O) Cyprus(B-country) is(O) a(O) member(O) of(O) the(O) Council(B-organization) of(I-organization) Europe(I-organization) and(O) a(O) member(O) state(O) of(O) the(O) European(B-organization) Union(I-organization) )(O) ,(O) since(O) 1973(O) ,(O) 1981(O) and(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2006(I-event) respectively(O) ;(O) Australia(B-country) since(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2015(I-event)"}}
{"id": "392", "dataset": "crossner_music", "split": "test", "label_list": ["person", "organization", "musical artist", "location", "song", "award", "event", "band", "album", "country", "music genre", "musical instrument"], "instance": {"id": "392", "words": ["Returning", "to", "his", "solo", "work", ",", "Iommi", "enlisted", "bassist", "Dave", "Spitz", "(", "ex-", "Great", "White", ")", ",", "drummer", "Eric", "Singer", "and", "initially", "intended", "to", "use", "multiple", "singers", ",", "including", "Rob", "Halford", "of", "Judas", "Priest", ",", "former", "Deep", "Purple", "and", "Trapeze", "vocalist", "Glenn", "Hughes", ",", "and", "former", "Sabbath", "vocalist", "Ronnie", "James", "Dio", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-band", "I-band", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "O", "B-band", "I-band", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "B-band", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, musical artist, location, song, award, event, band, album, country, music genre, musical instrument and O.\nSentence: Returning to his solo work , Iommi enlisted bassist Dave Spitz ( ex- Great White ) , drummer Eric Singer and initially intended to use multiple singers , including Rob Halford of Judas Priest , former Deep Purple and Trapeze vocalist Glenn Hughes , and former Sabbath vocalist Ronnie James Dio .", "prompt_labels": "Returning(O) to(O) his(O) solo(O) work(O) ,(O) Iommi(B-musical artist) enlisted(O) bassist(O) Dave(B-musical artist) Spitz(I-musical artist) ((O) ex-(O) Great(B-band) White(I-band) )(O) ,(O) drummer(O) Eric(B-musical artist) Singer(I-musical artist) and(O) initially(O) intended(O) to(O) use(O) multiple(O) singers(O) ,(O) including(O) Rob(B-musical artist) Halford(I-musical artist) of(O) Judas(B-band) Priest(I-band) ,(O) former(O) Deep(B-band) Purple(I-band) and(O) Trapeze(B-band) vocalist(O) Glenn(B-musical artist) Hughes(I-musical artist) ,(O) and(O) former(O) Sabbath(B-band) vocalist(O) Ronnie(B-musical artist) James(I-musical artist) Dio(I-musical artist) .(O)"}}
{"id": "271", "dataset": "crossner_music", "split": "test", "label_list": ["event", "person", "album", "song", "organization", "location", "musical instrument", "music genre", "band", "award", "country", "musical artist"], "instance": {"id": "271", "words": ["The", "film", "received", "Academy", "Awards", "nominations", "for", "Academy", "Award", "for", "Best", "Picture", "(", "producer", "Lawrence", "Turman", ")", ",", "Academy", "Award", "for", "Best", "Actor", "(", "Dustin", "Hoffman", ")", ",", "Academy", "Award", "for", "Best", "Actress", "(", "Anne", "Bancroft", ")", ",", "Academy", "Award", "for", "Best", "Supporting", "Actress", "(", "Katharine", "Ross", ")", ",", "Academy", "Award", "for", "Best", "Adapted", "Screenplay", "(", "Buck", "Henry", "and", "Calder", "Willingham", ")", ",", "and", "Academy", "Award", "for", "Best", "Cinematography", "(", "Robert", "L.", "Surtees", ")", "."], "labels": ["O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "I-person", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, album, song, organization, location, musical instrument, music genre, band, award, country, musical artist and O.\nSentence: The film received Academy Awards nominations for Academy Award for Best Picture ( producer Lawrence Turman ) , Academy Award for Best Actor ( Dustin Hoffman ) , Academy Award for Best Actress ( Anne Bancroft ) , Academy Award for Best Supporting Actress ( Katharine Ross ) , Academy Award for Best Adapted Screenplay ( Buck Henry and Calder Willingham ) , and Academy Award for Best Cinematography ( Robert L. Surtees ) .", "prompt_labels": "The(O) film(O) received(O) Academy(B-award) Awards(I-award) nominations(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ((O) producer(O) Lawrence(B-person) Turman(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) ((O) Dustin(B-person) Hoffman(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) ((O) Anne(B-person) Bancroft(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actress(I-award) ((O) Katharine(B-person) Ross(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) ((O) Buck(B-person) Henry(I-person) and(O) Calder(B-person) Willingham(I-person) )(O) ,(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Cinematography(I-award) ((O) Robert(B-person) L.(I-person) Surtees(I-person) )(O) .(O)"}}
{"id": "116", "dataset": "crossner_music", "split": "test", "label_list": ["song", "person", "award", "musical instrument", "location", "country", "album", "organization", "musical artist", "event", "music genre", "band"], "instance": {"id": "116", "words": ["G-Mex", "was", "also", "the", "2002", "Commonwealth", "Games", "venue", "for", "gymnastics", ",", "weightlifting", ",", "judo", "and", "wrestling", "."], "labels": ["B-location", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, person, award, musical instrument, location, country, album, organization, musical artist, event, music genre, band and O.\nSentence: G-Mex was also the 2002 Commonwealth Games venue for gymnastics , weightlifting , judo and wrestling .", "prompt_labels": "G-Mex(B-location) was(O) also(O) the(O) 2002(B-event) Commonwealth(I-event) Games(I-event) venue(O) for(O) gymnastics(O) ,(O) weightlifting(O) ,(O) judo(O) and(O) wrestling(O) .(O)"}}
{"id": "258", "dataset": "crossner_music", "split": "test", "label_list": ["event", "album", "musical instrument", "song", "award", "location", "music genre", "organization", "person", "musical artist", "band", "country"], "instance": {"id": "258", "words": ["Hank", "Williams", ",", "Jr", "(", "and", ",", "to", "an", "even", "greater", "extent", ",", "Hank", "Williams", "III", ")", ",", "Gary", "Allan", ",", "Shania", "Twain", ",", "Brooks", "&", "amp", ";", "Dunn", ",", "Faith", "Hill", ",", "Garth", "Brooks", ",", "Alan", "Jackson", ",", "Dwight", "Yoakam", ",", "Steve", "Earle", ",", "Dolly", "Parton", ",", "Rosanne", "Cash", "and", "Linda", "Ronstadt", "moved", "country", "further", "towards", "rock", "influence", "."], "labels": ["B-musical artist", "I-musical artist", "I-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "B-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, album, musical instrument, song, award, location, music genre, organization, person, musical artist, band, country and O.\nSentence: Hank Williams , Jr ( and , to an even greater extent , Hank Williams III ) , Gary Allan , Shania Twain , Brooks & amp ; Dunn , Faith Hill , Garth Brooks , Alan Jackson , Dwight Yoakam , Steve Earle , Dolly Parton , Rosanne Cash and Linda Ronstadt moved country further towards rock influence .", "prompt_labels": "Hank(B-musical artist) Williams(I-musical artist) ,(I-musical artist) Jr(I-musical artist) ((O) and(O) ,(O) to(O) an(O) even(O) greater(O) extent(O) ,(O) Hank(B-musical artist) Williams(I-musical artist) III(I-musical artist) )(O) ,(O) Gary(B-musical artist) Allan(I-musical artist) ,(O) Shania(B-musical artist) Twain(I-musical artist) ,(O) Brooks(B-band) &(I-band) amp(I-band) ;(I-band) Dunn(I-band) ,(O) Faith(B-musical artist) Hill(I-musical artist) ,(O) Garth(B-musical artist) Brooks(I-musical artist) ,(O) Alan(B-musical artist) Jackson(I-musical artist) ,(O) Dwight(B-musical artist) Yoakam(I-musical artist) ,(O) Steve(B-musical artist) Earle(I-musical artist) ,(O) Dolly(B-musical artist) Parton(I-musical artist) ,(O) Rosanne(B-musical artist) Cash(I-musical artist) and(O) Linda(B-musical artist) Ronstadt(I-musical artist) moved(O) country(O) further(O) towards(O) rock(B-music genre) influence(O) .(O)"}}
{"id": "361", "dataset": "crossner_music", "split": "test", "label_list": ["person", "location", "country", "song", "organization", "album", "music genre", "award", "musical artist", "event", "musical instrument", "band"], "instance": {"id": "361", "words": ["Hailed", "as", "a", "great", "innovator", "of", "rock", "and", "roll", "by", "contemporaries", "such", "as", "Little", "Richard", "and", "Johnny", "Otis", ",", "He", "is", "also", "inducted", "into", "the", "St.", "Louis", "Walk", "of", "Fame", ",", "the", "Clarksdale", "Walk", "of", "Fame", ",", "the", "Mississippi", "Musicians", "Hall", "of", "Fame", ",", "the", "Blues", "Hall", "of", "Fame", ",", "and", "the", "Rhythm", "and", "Blues", "Music", "Hall", "of", "Fame", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, country, song, organization, album, music genre, award, musical artist, event, musical instrument, band and O.\nSentence: Hailed as a great innovator of rock and roll by contemporaries such as Little Richard and Johnny Otis , He is also inducted into the St. Louis Walk of Fame , the Clarksdale Walk of Fame , the Mississippi Musicians Hall of Fame , the Blues Hall of Fame , and the Rhythm and Blues Music Hall of Fame .", "prompt_labels": "Hailed(B-musical artist) as(O) a(O) great(O) innovator(O) of(O) rock(B-music genre) and(I-music genre) roll(I-music genre) by(O) contemporaries(O) such(O) as(O) Little(B-musical artist) Richard(I-musical artist) and(O) Johnny(B-musical artist) Otis(I-musical artist) ,(O) He(O) is(O) also(O) inducted(O) into(O) the(O) St.(B-award) Louis(I-award) Walk(I-award) of(I-award) Fame(I-award) ,(O) the(O) Clarksdale(B-award) Walk(I-award) of(I-award) Fame(I-award) ,(O) the(O) Mississippi(B-award) Musicians(I-award) Hall(I-award) of(I-award) Fame(I-award) ,(O) the(O) Blues(B-award) Hall(I-award) of(I-award) Fame(I-award) ,(O) and(O) the(O) Rhythm(B-award) and(I-award) Blues(I-award) Music(I-award) Hall(I-award) of(I-award) Fame(I-award) .(O)"}}
{"id": "385", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "organization", "song", "location", "country", "music genre", "album", "band", "musical instrument", "event", "person", "award"], "instance": {"id": "385", "words": ["Van", "Bentum", "competed", "in", "three", "consecutive", "Summer", "Olympics", "for", "her", "native", "country", ",", "starting", "in", "Swimming", "at", "the", "1980", "Summer", "Olympics", "."], "labels": ["B-person", "I-person", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, organization, song, location, country, music genre, album, band, musical instrument, event, person, award and O.\nSentence: Van Bentum competed in three consecutive Summer Olympics for her native country , starting in Swimming at the 1980 Summer Olympics .", "prompt_labels": "Van(B-person) Bentum(I-person) competed(O) in(O) three(O) consecutive(O) Summer(B-event) Olympics(I-event) for(O) her(O) native(O) country(O) ,(O) starting(O) in(O) Swimming(O) at(O) the(O) 1980(B-event) Summer(I-event) Olympics(I-event) .(O)"}}
{"id": "159", "dataset": "crossner_music", "split": "test", "label_list": ["event", "person", "organization", "musical artist", "musical instrument", "country", "band", "music genre", "album", "award", "location", "song"], "instance": {"id": "159", "words": ["His", "mix", "of", "traditional", "Senegalese", "mbalax", "with", "eclectic", "influences", "ranging", "from", "Cuban", "rumba", "to", "Hip", "hop", "music", ",", "jazz", ",", "and", "Soul", "music", "won", "him", "an", "international", "fan", "base", "of", "millions", "."], "labels": ["O", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, organization, musical artist, musical instrument, country, band, music genre, album, award, location, song and O.\nSentence: His mix of traditional Senegalese mbalax with eclectic influences ranging from Cuban rumba to Hip hop music , jazz , and Soul music won him an international fan base of millions .", "prompt_labels": "His(O) mix(O) of(O) traditional(O) Senegalese(O) mbalax(B-music genre) with(O) eclectic(B-music genre) influences(O) ranging(O) from(O) Cuban(B-music genre) rumba(I-music genre) to(O) Hip(B-music genre) hop(I-music genre) music(I-music genre) ,(O) jazz(B-music genre) ,(O) and(O) Soul(B-music genre) music(I-music genre) won(O) him(O) an(O) international(O) fan(O) base(O) of(O) millions(O) .(O)"}}
{"id": "135", "dataset": "crossner_music", "split": "test", "label_list": ["song", "country", "band", "location", "musical artist", "music genre", "person", "album", "organization", "musical instrument", "award", "event"], "instance": {"id": "135", "words": ["Shoegazing", ",", "Stoner", "rock", "and", "noise", "pop", "genres", "emerged", "into", "the", "mainstream", "with", "the", "explosion", "of", "bands", "such", "as", "Kyuss", ",", "Monster", "Magnet", ",", "the", "Desert", "Sessions", ",", "Slowdive", ",", "the", "Verve", ",", "My", "Bloody", "Valentine", ",", "Flying", "Saucer", "Attack", ",", "Loop", ",", "Ride", ",", "Shiner", ",", "the", "Flaming", "Lips", ",", "Failure", ",", "Year", "of", "the", "Rabbit", ",", "Cave", "In", ",", "Sun", "Dial", ",", "Hum", ",", "Orange", "Goblin", ",", "Porcupine", "Tree", ",", "Spacemen", "3", ",", "Spiritualized", ",", "and", "Mercury", "Rev", "."], "labels": ["B-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "O", "B-band", "O", "B-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, country, band, location, musical artist, music genre, person, album, organization, musical instrument, award, event and O.\nSentence: Shoegazing , Stoner rock and noise pop genres emerged into the mainstream with the explosion of bands such as Kyuss , Monster Magnet , the Desert Sessions , Slowdive , the Verve , My Bloody Valentine , Flying Saucer Attack , Loop , Ride , Shiner , the Flaming Lips , Failure , Year of the Rabbit , Cave In , Sun Dial , Hum , Orange Goblin , Porcupine Tree , Spacemen 3 , Spiritualized , and Mercury Rev .", "prompt_labels": "Shoegazing(B-music genre) ,(O) Stoner(B-music genre) rock(I-music genre) and(O) noise(B-music genre) pop(I-music genre) genres(O) emerged(O) into(O) the(O) mainstream(O) with(O) the(O) explosion(O) of(O) bands(O) such(O) as(O) Kyuss(B-band) ,(O) Monster(B-band) Magnet(I-band) ,(O) the(B-band) Desert(I-band) Sessions(I-band) ,(O) Slowdive(B-band) ,(O) the(B-band) Verve(I-band) ,(O) My(B-band) Bloody(I-band) Valentine(I-band) ,(O) Flying(B-band) Saucer(I-band) Attack(I-band) ,(O) Loop(B-band) ,(O) Ride(B-band) ,(O) Shiner(B-band) ,(O) the(B-band) Flaming(I-band) Lips(I-band) ,(O) Failure(B-band) ,(O) Year(B-band) of(I-band) the(I-band) Rabbit(I-band) ,(O) Cave(B-band) In(I-band) ,(O) Sun(B-band) Dial(I-band) ,(O) Hum(B-band) ,(O) Orange(B-band) Goblin(I-band) ,(O) Porcupine(B-band) Tree(I-band) ,(O) Spacemen(B-band) 3(I-band) ,(O) Spiritualized(B-band) ,(O) and(O) Mercury(B-band) Rev(I-band) .(O)"}}
{"id": "252", "dataset": "crossner_music", "split": "test", "label_list": ["person", "band", "musical artist", "song", "award", "musical instrument", "country", "music genre", "organization", "album", "event", "location"], "instance": {"id": "252", "words": ["Reed", "'s", "1984", "album", "New", "Sensations", "marked", "the", "first", "time", "that", "Reed", "had", "charted", "within", "the", "US", "Top", "100", "since", "1978", "'s", "Street", "Hassle", ",", "and", "the", "first", "time", "that", "Reed", "had", "charted", "in", "the", "UK", "altogether", "since", "1976", "'s", "Coney", "Island", "Baby", "."], "labels": ["B-musical artist", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, band, musical artist, song, award, musical instrument, country, music genre, organization, album, event, location and O.\nSentence: Reed 's 1984 album New Sensations marked the first time that Reed had charted within the US Top 100 since 1978 's Street Hassle , and the first time that Reed had charted in the UK altogether since 1976 's Coney Island Baby .", "prompt_labels": "Reed(B-musical artist) 's(O) 1984(O) album(O) New(B-album) Sensations(I-album) marked(O) the(O) first(O) time(O) that(O) Reed(B-musical artist) had(O) charted(O) within(O) the(O) US(B-country) Top(O) 100(O) since(O) 1978(O) 's(O) Street(B-album) Hassle(I-album) ,(O) and(O) the(O) first(O) time(O) that(O) Reed(B-musical artist) had(O) charted(O) in(O) the(O) UK(B-country) altogether(O) since(O) 1976(O) 's(O) Coney(B-album) Island(I-album) Baby(I-album) .(O)"}}
{"id": "440", "dataset": "crossner_music", "split": "test", "label_list": ["award", "musical instrument", "album", "location", "song", "musical artist", "organization", "country", "event", "person", "music genre", "band"], "instance": {"id": "440", "words": ["Parton", "recorded", "a", "series", "of", "bluegrass", "-inspired", "albums", ",", "beginning", "with", "The", "Grass", "Is", "Blue", "(", "1999", ")", ",", "winning", "a", "Grammy", "Award", "for", "Best", "Bluegrass", "Album", ";", "and", "Little", "Sparrow", "(", "2001", ")", ",", "with", "its", "cover", "of", "Collective", "Soul", "'", "s", "Shine", "winning", "a", "Grammy", "Award", "for", "Best", "Female", "Country", "Vocal", "Performance", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "B-song", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical instrument, album, location, song, musical artist, organization, country, event, person, music genre, band and O.\nSentence: Parton recorded a series of bluegrass -inspired albums , beginning with The Grass Is Blue ( 1999 ) , winning a Grammy Award for Best Bluegrass Album ; and Little Sparrow ( 2001 ) , with its cover of Collective Soul ' s Shine winning a Grammy Award for Best Female Country Vocal Performance .", "prompt_labels": "Parton(B-musical artist) recorded(O) a(O) series(O) of(O) bluegrass(B-music genre) -inspired(O) albums(O) ,(O) beginning(O) with(O) The(B-album) Grass(I-album) Is(I-album) Blue(I-album) ((O) 1999(O) )(O) ,(O) winning(O) a(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Bluegrass(I-award) Album(I-award) ;(O) and(O) Little(B-album) Sparrow(I-album) ((O) 2001(O) )(O) ,(O) with(O) its(O) cover(O) of(O) Collective(B-band) Soul(I-band) '(O) s(O) Shine(B-song) winning(O) a(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Female(I-award) Country(I-award) Vocal(I-award) Performance(I-award) .(O)"}}
{"id": "453", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "musical instrument", "country", "location", "event", "song", "award", "album", "organization", "band", "person", "music genre"], "instance": {"id": "453", "words": ["Independent", "from", "the", "United", "Kingdom", "scene", ",", "in", "the", "late", "1970s", "and", "early", "1980s", "in", "California", ",", "deathrock", "developed", "as", "a", "distinct", "branch", "of", "American", "punk", "rock", ",", "with", "acts", "such", "as", "Christian", "Death", "and", "45", "Grave", "."], "labels": ["O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, musical instrument, country, location, event, song, award, album, organization, band, person, music genre and O.\nSentence: Independent from the United Kingdom scene , in the late 1970s and early 1980s in California , deathrock developed as a distinct branch of American punk rock , with acts such as Christian Death and 45 Grave .", "prompt_labels": "Independent(O) from(O) the(O) United(B-country) Kingdom(I-country) scene(O) ,(O) in(O) the(O) late(O) 1970s(O) and(O) early(O) 1980s(O) in(O) California(B-location) ,(O) deathrock(B-music genre) developed(O) as(O) a(O) distinct(O) branch(O) of(O) American(O) punk(B-music genre) rock(I-music genre) ,(O) with(O) acts(O) such(O) as(O) Christian(B-band) Death(I-band) and(O) 45(B-band) Grave(I-band) .(O)"}}
{"id": "372", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "music genre", "band", "musical artist", "award", "country", "location", "organization", "album", "event", "person", "song"], "instance": {"id": "372", "words": ["Steve", "Huey", "of", "AllMusic", "said", "that", "the", "band", "'s", "earlier", "Punk", "rock", "influences", "are", "rarely", "detectable", ",", "replaced", "by", "surprisingly", "effective", "appropriations", "of", "Pop", "music", "and", "psychedelia", "."], "labels": ["B-musical artist", "I-musical artist", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, music genre, band, musical artist, award, country, location, organization, album, event, person, song and O.\nSentence: Steve Huey of AllMusic said that the band 's earlier Punk rock influences are rarely detectable , replaced by surprisingly effective appropriations of Pop music and psychedelia .", "prompt_labels": "Steve(B-musical artist) Huey(I-musical artist) of(O) AllMusic(B-organization) said(O) that(O) the(O) band(O) 's(O) earlier(O) Punk(B-music genre) rock(I-music genre) influences(O) are(O) rarely(O) detectable(O) ,(O) replaced(O) by(O) surprisingly(O) effective(O) appropriations(O) of(O) Pop(B-music genre) music(I-music genre) and(O) psychedelia(B-music genre) .(O)"}}
{"id": "163", "dataset": "crossner_music", "split": "test", "label_list": ["event", "band", "location", "person", "album", "musical instrument", "music genre", "country", "song", "musical artist", "organization", "award"], "instance": {"id": "163", "words": ["Baron", "Cohen", "was", "named", "Best", "Newcomer", "at", "the", "1999", "British", "Comedy", "Awards", "for", "The", "11", "O", "'Clock", "Show", ",", "and", "since", "then", ",", "he", "has", "received", "two", "British", "Academy", "of", "Film", "and", "Television", "Arts", "Awards", "for", "Da", "Ali", "G", "Show", ",", "several", "Emmy", "nominations", ",", "a", "nomination", "for", "an", "Academy", "Award", "for", "Best", "Adapted", "Screenplay", ",", "and", "a", "Golden", "Globe", "for", "Best", "Actor", "for", "his", "work", "in", "the", "feature", "film", "Borat", "."], "labels": ["B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "B-award", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, band, location, person, album, musical instrument, music genre, country, song, musical artist, organization, award and O.\nSentence: Baron Cohen was named Best Newcomer at the 1999 British Comedy Awards for The 11 O 'Clock Show , and since then , he has received two British Academy of Film and Television Arts Awards for Da Ali G Show , several Emmy nominations , a nomination for an Academy Award for Best Adapted Screenplay , and a Golden Globe for Best Actor for his work in the feature film Borat .", "prompt_labels": "Baron(B-person) Cohen(I-person) was(O) named(O) Best(B-award) Newcomer(I-award) at(I-award) the(I-award) 1999(I-award) British(I-award) Comedy(I-award) Awards(I-award) for(O) The(B-event) 11(I-event) O(I-event) 'Clock(I-event) Show(I-event) ,(O) and(O) since(O) then(O) ,(O) he(O) has(O) received(O) two(O) British(B-award) Academy(I-award) of(I-award) Film(I-award) and(I-award) Television(I-award) Arts(I-award) Awards(I-award) for(O) Da(B-event) Ali(I-event) G(I-event) Show(I-event) ,(O) several(O) Emmy(B-award) nominations(O) ,(O) a(O) nomination(O) for(O) an(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) ,(O) and(O) a(O) Golden(B-award) Globe(I-award) for(I-award) Best(I-award) Actor(I-award) for(O) his(O) work(O) in(O) the(O) feature(O) film(O) Borat(O) .(O)"}}
{"id": "464", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "song", "musical artist", "organization", "album", "location", "event", "award", "person", "musical instrument", "country", "band"], "instance": {"id": "464", "words": ["But", "in", "2009", ",", "Cape", "Town", "Opera", "'", "s", "production", ",", "set", "in", "1970s", "South", "Africa", "and", "inspired", "by", "life", "in", "Soweto", ",", "toured", "Britain", ",", "opening", "at", "the", "Wales", "Millennium", "Centre", "in", "Cardiff", "and", "going", "on", "to", "the", "Royal", "Festival", "Hall", "in", "London", "and", "Edinburgh", "Festival", "Theatre", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "B-location", "O", "O", "B-location", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, song, musical artist, organization, album, location, event, award, person, musical instrument, country, band and O.\nSentence: But in 2009 , Cape Town Opera ' s production , set in 1970s South Africa and inspired by life in Soweto , toured Britain , opening at the Wales Millennium Centre in Cardiff and going on to the Royal Festival Hall in London and Edinburgh Festival Theatre .", "prompt_labels": "But(O) in(O) 2009(O) ,(O) Cape(B-organization) Town(I-organization) Opera(I-organization) '(O) s(O) production(O) ,(O) set(O) in(O) 1970s(O) South(B-country) Africa(I-country) and(O) inspired(O) by(O) life(O) in(O) Soweto(B-location) ,(O) toured(O) Britain(B-location) ,(O) opening(O) at(O) the(O) Wales(B-location) Millennium(I-location) Centre(I-location) in(O) Cardiff(B-location) and(O) going(O) on(O) to(O) the(O) Royal(B-location) Festival(I-location) Hall(I-location) in(O) London(B-location) and(O) Edinburgh(B-location) Festival(I-location) Theatre(I-location) .(O)"}}
{"id": "130", "dataset": "crossner_music", "split": "test", "label_list": ["location", "musical artist", "award", "music genre", "song", "band", "country", "organization", "musical instrument", "event", "person", "album"], "instance": {"id": "130", "words": ["The", "tour", "opened", "at", "the", "Palace", "Theatre", ",", "Manchester", "and", "also", "played", "in", "the", "Birmingham", "Hippodrome", ",", "the", "Mayflower", "Theatre", "in", "Southampton", ",", "the", "Edinburgh", "Playhouse", ",", "the", "Bristol", "Hippodrome", "and", "The", "Point", "Theatre", "in", "Dublin", "."], "labels": ["O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "I-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical artist, award, music genre, song, band, country, organization, musical instrument, event, person, album and O.\nSentence: The tour opened at the Palace Theatre , Manchester and also played in the Birmingham Hippodrome , the Mayflower Theatre in Southampton , the Edinburgh Playhouse , the Bristol Hippodrome and The Point Theatre in Dublin .", "prompt_labels": "The(O) tour(O) opened(O) at(O) the(O) Palace(B-location) Theatre(I-location) ,(I-location) Manchester(I-location) and(O) also(O) played(O) in(O) the(O) Birmingham(B-location) Hippodrome(I-location) ,(O) the(O) Mayflower(B-location) Theatre(I-location) in(O) Southampton(B-location) ,(O) the(O) Edinburgh(B-location) Playhouse(I-location) ,(O) the(O) Bristol(B-location) Hippodrome(I-location) and(O) The(B-location) Point(I-location) Theatre(I-location) in(O) Dublin(B-location) .(O)"}}
{"id": "59", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "organization", "event", "music genre", "album", "award", "band", "musical instrument", "person", "song", "location", "country"], "instance": {"id": "59", "words": ["Indo-Caribbean", "music", "of", "Indo-Caribbean", "people", "in", "Caribbean", "is", "most", "common", "in", "Trinidad", "and", "Tobago", ",", "Guyana", ",", "Jamaica", ",", "and", "Suriname", ",", "which", "reflects", "their", "Bhojpuri", "heritage", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "B-country", "I-country", "I-country", "O", "B-country", "O", "B-country", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, organization, event, music genre, album, award, band, musical instrument, person, song, location, country and O.\nSentence: Indo-Caribbean music of Indo-Caribbean people in Caribbean is most common in Trinidad and Tobago , Guyana , Jamaica , and Suriname , which reflects their Bhojpuri heritage .", "prompt_labels": "Indo-Caribbean(O) music(O) of(O) Indo-Caribbean(O) people(O) in(O) Caribbean(B-location) is(O) most(O) common(O) in(O) Trinidad(B-country) and(I-country) Tobago(I-country) ,(O) Guyana(B-country) ,(O) Jamaica(B-country) ,(O) and(O) Suriname(B-country) ,(O) which(O) reflects(O) their(O) Bhojpuri(O) heritage(O) .(O)"}}
{"id": "325", "dataset": "crossner_music", "split": "test", "label_list": ["person", "song", "country", "album", "musical artist", "event", "musical instrument", "organization", "music genre", "location", "award", "band"], "instance": {"id": "325", "words": ["Chicago", "blues", "is", "influenced", "to", "a", "large", "extent", "by", "the", "Delta", "blues", "style", ",", "because", "many", "performers", "had", "migrated", "from", "the", "Mississippi", "region", "."], "labels": ["B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, song, country, album, musical artist, event, musical instrument, organization, music genre, location, award, band and O.\nSentence: Chicago blues is influenced to a large extent by the Delta blues style , because many performers had migrated from the Mississippi region .", "prompt_labels": "Chicago(B-music genre) blues(I-music genre) is(O) influenced(O) to(O) a(O) large(O) extent(O) by(O) the(O) Delta(B-music genre) blues(I-music genre) style(O) ,(O) because(O) many(O) performers(O) had(O) migrated(O) from(O) the(O) Mississippi(B-location) region(O) .(O)"}}
{"id": "282", "dataset": "crossner_music", "split": "test", "label_list": ["song", "event", "person", "country", "album", "musical artist", "organization", "band", "location", "award", "musical instrument", "music genre"], "instance": {"id": "282", "words": ["Since", "2001", ",", "the", "dominance", "of", "traditional", "boy", "bands", "on", "pop", "charts", "began", "to", "fade", "in", "the", "western", "hemisphere", ",", "although", "Gil", "Kaufman", "of", "MTV", "has", "described", "new", "boy", "bands", "that", "are", "more", "likely", "to", "resemble", "My", "Chemical", "Romance", ",", "Sum", "41", ",", "and", "Simple", "Plan", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, person, country, album, musical artist, organization, band, location, award, musical instrument, music genre and O.\nSentence: Since 2001 , the dominance of traditional boy bands on pop charts began to fade in the western hemisphere , although Gil Kaufman of MTV has described new boy bands that are more likely to resemble My Chemical Romance , Sum 41 , and Simple Plan .", "prompt_labels": "Since(O) 2001(O) ,(O) the(O) dominance(O) of(O) traditional(O) boy(O) bands(O) on(O) pop(B-music genre) charts(O) began(O) to(O) fade(O) in(O) the(O) western(O) hemisphere(O) ,(O) although(O) Gil(B-person) Kaufman(I-person) of(O) MTV(B-organization) has(O) described(O) new(O) boy(O) bands(O) that(O) are(O) more(O) likely(O) to(O) resemble(O) My(B-band) Chemical(I-band) Romance(I-band) ,(O) Sum(B-band) 41(I-band) ,(O) and(O) Simple(B-band) Plan(I-band) .(O)"}}
{"id": "366", "dataset": "crossner_music", "split": "test", "label_list": ["country", "event", "song", "music genre", "location", "organization", "band", "album", "musical instrument", "musical artist", "person", "award"], "instance": {"id": "366", "words": ["Clooney", "lists", "9", "hotels", "including", "The", "Dorchester", ",", "45", "Park", "Lane", ",", "Coworth", "Park", ",", "The", "Beverly", "Hills", "Hotel", ",", "Hotel", "Bel-Air", ",", "Le", "Meurice", ",", "Hotel", "Plaza", "Athenee", ",", "Hotel", "Eden", "and", "Hotel", "Principe", "di", "Savoia", "and", "asks", "readers", "to", "consider", "how", "we", "are", "putting", "money", "directly", "into", "the", "pockets", "of", "men", "who", "choose", "to", "stone", "and", "whip", "to", "death", "their", "own", "citizens", "for", "being", "gay", "or", "accused", "of", "adultery", "."], "labels": ["B-person", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "I-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, song, music genre, location, organization, band, album, musical instrument, musical artist, person, award and O.\nSentence: Clooney lists 9 hotels including The Dorchester , 45 Park Lane , Coworth Park , The Beverly Hills Hotel , Hotel Bel-Air , Le Meurice , Hotel Plaza Athenee , Hotel Eden and Hotel Principe di Savoia and asks readers to consider how we are putting money directly into the pockets of men who choose to stone and whip to death their own citizens for being gay or accused of adultery .", "prompt_labels": "Clooney(B-person) lists(O) 9(O) hotels(O) including(O) The(B-location) Dorchester(I-location) ,(O) 45(B-location) Park(I-location) Lane(I-location) ,(O) Coworth(B-location) Park(I-location) ,(O) The(B-location) Beverly(I-location) Hills(I-location) Hotel(I-location) ,(O) Hotel(B-location) Bel-Air(I-location) ,(O) Le(B-location) Meurice(I-location) ,(O) Hotel(B-location) Plaza(I-location) Athenee(I-location) ,(O) Hotel(B-location) Eden(I-location) and(O) Hotel(B-location) Principe(I-location) di(I-location) Savoia(I-location) and(O) asks(O) readers(O) to(O) consider(O) how(O) we(O) are(O) putting(O) money(O) directly(O) into(O) the(O) pockets(O) of(O) men(O) who(O) choose(O) to(O) stone(O) and(O) whip(O) to(O) death(O) their(O) own(O) citizens(O) for(O) being(O) gay(O) or(O) accused(O) of(O) adultery(O) .(O)"}}
{"id": "198", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "location", "band", "album", "country", "award", "musical instrument", "song", "person", "organization", "event", "music genre"], "instance": {"id": "198", "words": ["After", "a", "long", "campaign", ",", "the", "World", "DanceSport", "Federation", "(", "WDSF", ")", ",", "formerly", "IDSF", ",", "was", "recognized", "by", "the", "International", "Olympic", "Committee", "as", "the", "sole", "representative", "body", "for", "dancesport", ",", "on", "September", "5", ",", "1997.Long", ",", "Daniel", "1999", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, band, album, country, award, musical instrument, song, person, organization, event, music genre and O.\nSentence: After a long campaign , the World DanceSport Federation ( WDSF ) , formerly IDSF , was recognized by the International Olympic Committee as the sole representative body for dancesport , on September 5 , 1997.Long , Daniel 1999 .", "prompt_labels": "After(O) a(O) long(O) campaign(O) ,(O) the(O) World(B-organization) DanceSport(I-organization) Federation(I-organization) ((O) WDSF(B-organization) )(O) ,(O) formerly(O) IDSF(B-organization) ,(O) was(O) recognized(O) by(O) the(O) International(B-organization) Olympic(I-organization) Committee(I-organization) as(O) the(O) sole(O) representative(O) body(O) for(O) dancesport(O) ,(O) on(O) September(O) 5(O) ,(O) 1997.Long(O) ,(O) Daniel(O) 1999(O) .(O)"}}
{"id": "437", "dataset": "crossner_music", "split": "test", "label_list": ["person", "award", "music genre", "song", "location", "album", "organization", "country", "musical instrument", "band", "event", "musical artist"], "instance": {"id": "437", "words": ["In", "addition", ",", "McBride", "has", "the", "Country", "Music", "Association", "'", "s", "Female", "Vocalist", "of", "the", "Year", "award", "four", "times", "(", "tied", "with", "Reba", "McEntire", "for", "the", "second-most", "wins", ")", "and", "the", "Academy", "of", "Country", "Music", "'", "s", "Top", "Female", "Vocalist", "award", "three", "times", "."], "labels": ["O", "O", "O", "B-musical artist", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, music genre, song, location, album, organization, country, musical instrument, band, event, musical artist and O.\nSentence: In addition , McBride has the Country Music Association ' s Female Vocalist of the Year award four times ( tied with Reba McEntire for the second-most wins ) and the Academy of Country Music ' s Top Female Vocalist award three times .", "prompt_labels": "In(O) addition(O) ,(O) McBride(B-musical artist) has(O) the(O) Country(B-organization) Music(I-organization) Association(I-organization) '(O) s(O) Female(B-award) Vocalist(I-award) of(I-award) the(I-award) Year(I-award) award(I-award) four(O) times(O) ((O) tied(O) with(O) Reba(B-musical artist) McEntire(I-musical artist) for(O) the(O) second-most(O) wins(O) )(O) and(O) the(O) Academy(B-organization) of(I-organization) Country(I-organization) Music(I-organization) '(O) s(O) Top(B-award) Female(I-award) Vocalist(I-award) award(I-award) three(O) times(O) .(O)"}}
{"id": "362", "dataset": "crossner_music", "split": "test", "label_list": ["country", "location", "award", "organization", "music genre", "band", "musical instrument", "person", "musical artist", "song", "album", "event"], "instance": {"id": "362", "words": ["By", "the", "1990s", ",", "country", "music", "had", "attained", "crossover", "success", "in", "the", "pop", "charts", ",", "with", "artists", "like", "James", "Blundell", "and", "James", "Reyne", "singing", "The", "Dingoes", ",", "and", "country", "star", "Kasey", "Chambers", "winning", "the", "ARIA", "Award", "for", "Best", "Female", "Artist", "in", "2000", ",", "2002", "and", "2004", ",", "tying", "with", "pop", "stars", "Wendy", "Matthews", "and", "Sia", "for", "the", "most", "wins", "in", "that", "category", "."], "labels": ["O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "O", "B-music genre", "O", "B-musical artist", "I-musical artist", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, award, organization, music genre, band, musical instrument, person, musical artist, song, album, event and O.\nSentence: By the 1990s , country music had attained crossover success in the pop charts , with artists like James Blundell and James Reyne singing The Dingoes , and country star Kasey Chambers winning the ARIA Award for Best Female Artist in 2000 , 2002 and 2004 , tying with pop stars Wendy Matthews and Sia for the most wins in that category .", "prompt_labels": "By(O) the(O) 1990s(O) ,(O) country(B-music genre) music(I-music genre) had(O) attained(O) crossover(O) success(O) in(O) the(O) pop(B-music genre) charts(O) ,(O) with(O) artists(O) like(O) James(B-musical artist) Blundell(I-musical artist) and(O) James(B-musical artist) Reyne(I-musical artist) singing(O) The(B-band) Dingoes(I-band) ,(O) and(O) country(B-music genre) star(O) Kasey(B-musical artist) Chambers(I-musical artist) winning(O) the(O) ARIA(B-award) Award(I-award) for(I-award) Best(I-award) Female(I-award) Artist(I-award) in(O) 2000(O) ,(O) 2002(O) and(O) 2004(O) ,(O) tying(O) with(O) pop(B-music genre) stars(O) Wendy(B-musical artist) Matthews(I-musical artist) and(O) Sia(B-musical artist) for(O) the(O) most(O) wins(O) in(O) that(O) category(O) .(O)"}}
{"id": "358", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "award", "musical instrument", "person", "event", "country", "location", "music genre", "band", "song", "album", "organization"], "instance": {"id": "358", "words": ["The", "band", "'s", "biggest", "hit", "singles", "are", "Sentimental", "ballad", "such", "as", "Easy", ",", "Three", "Times", "a", "Lady", ",", "and", "Nightshift", ";", "and", "funky", "dance", "hits", "which", "include", "Brick", "House", ",", "Fancy", "Dancer", ",", "Lady", "(", "You", "Bring", "Me", "Up", ")", ",", "and", "Too", "Hot", "ta", "Trot", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "B-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "B-song", "O", "O", "B-music genre", "O", "O", "O", "O", "B-song", "I-song", "O", "B-song", "I-song", "O", "B-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, award, musical instrument, person, event, country, location, music genre, band, song, album, organization and O.\nSentence: The band 's biggest hit singles are Sentimental ballad such as Easy , Three Times a Lady , and Nightshift ; and funky dance hits which include Brick House , Fancy Dancer , Lady ( You Bring Me Up ) , and Too Hot ta Trot .", "prompt_labels": "The(O) band(O) 's(O) biggest(O) hit(O) singles(O) are(O) Sentimental(B-music genre) ballad(I-music genre) such(O) as(O) Easy(B-song) ,(O) Three(B-song) Times(I-song) a(I-song) Lady(I-song) ,(O) and(O) Nightshift(B-song) ;(O) and(O) funky(B-music genre) dance(O) hits(O) which(O) include(O) Brick(B-song) House(I-song) ,(O) Fancy(B-song) Dancer(I-song) ,(O) Lady(B-song) ((O) You(B-song) Bring(I-song) Me(I-song) Up(I-song) )(O) ,(O) and(O) Too(B-song) Hot(I-song) ta(I-song) Trot(I-song) .(O)"}}
{"id": "42", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "band", "organization", "album", "award", "song", "event", "location", "person", "musical artist", "country", "musical instrument"], "instance": {"id": "42", "words": ["By", "the", "end", "of", "the", "decade", ",", "Underwood", "had", "amassed", "eight", "No.", "1", "songs", "on", "the", "Billboard", "Hot", "Country", "Songs", "chart", ",", "along", "with", "numerous", "awards", "from", "the", "Country", "Music", "Association", ",", "Academy", "of", "Country", "Music", "and", "others", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, band, organization, album, award, song, event, location, person, musical artist, country, musical instrument and O.\nSentence: By the end of the decade , Underwood had amassed eight No. 1 songs on the Billboard Hot Country Songs chart , along with numerous awards from the Country Music Association , Academy of Country Music and others .", "prompt_labels": "By(O) the(O) end(O) of(O) the(O) decade(O) ,(O) Underwood(B-musical artist) had(O) amassed(O) eight(O) No.(O) 1(O) songs(O) on(O) the(O) Billboard(B-organization) Hot(O) Country(O) Songs(O) chart(O) ,(O) along(O) with(O) numerous(O) awards(O) from(O) the(O) Country(B-organization) Music(I-organization) Association(I-organization) ,(O) Academy(B-organization) of(I-organization) Country(I-organization) Music(I-organization) and(O) others(O) .(O)"}}
{"id": "170", "dataset": "crossner_music", "split": "test", "label_list": ["event", "song", "musical artist", "person", "organization", "band", "location", "musical instrument", "music genre", "country", "album", "award"], "instance": {"id": "170", "words": ["It", "has", "influenced", ",", "in", "some", "capacity", ",", "Garage", "house", ",", "Jungle", "music", ",", "Eurodance", ",", "Electropop", ",", "Dubstep", ",", "and", "even", "certain", "elements", "of", "Alternative", "rock", "and", "Hip", "hop", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, song, musical artist, person, organization, band, location, musical instrument, music genre, country, album, award and O.\nSentence: It has influenced , in some capacity , Garage house , Jungle music , Eurodance , Electropop , Dubstep , and even certain elements of Alternative rock and Hip hop .", "prompt_labels": "It(O) has(O) influenced(O) ,(O) in(O) some(O) capacity(O) ,(O) Garage(B-music genre) house(I-music genre) ,(O) Jungle(B-music genre) music(I-music genre) ,(O) Eurodance(B-music genre) ,(O) Electropop(B-music genre) ,(O) Dubstep(B-music genre) ,(O) and(O) even(O) certain(O) elements(O) of(O) Alternative(B-music genre) rock(I-music genre) and(O) Hip(B-music genre) hop(I-music genre) .(O)"}}
{"id": "324", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "musical instrument", "organization", "event", "location", "person", "music genre", "band", "album", "song", "country", "award"], "instance": {"id": "324", "words": ["He", "provided", "musical", "direction", "for", "the", "ceremonies", "of", "the", "1978", "Commonwealth", "Games", ",", "EXPO", "86", ",", "The", "World", "University", "Games", ",", "the", "XV", "Olympic", "Winter", "Games", ",", "and", "for", "countless", "television", "shows", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "B-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, musical instrument, organization, event, location, person, music genre, band, album, song, country, award and O.\nSentence: He provided musical direction for the ceremonies of the 1978 Commonwealth Games , EXPO 86 , The World University Games , the XV Olympic Winter Games , and for countless television shows .", "prompt_labels": "He(O) provided(O) musical(O) direction(O) for(O) the(O) ceremonies(O) of(O) the(O) 1978(B-event) Commonwealth(I-event) Games(I-event) ,(O) EXPO(B-event) 86(I-event) ,(O) The(B-event) World(I-event) University(I-event) Games(I-event) ,(O) the(O) XV(B-event) Olympic(I-event) Winter(I-event) Games(I-event) ,(O) and(O) for(O) countless(O) television(O) shows(O) .(O)"}}
{"id": "52", "dataset": "crossner_music", "split": "test", "label_list": ["person", "location", "song", "award", "musical artist", "band", "event", "album", "musical instrument", "music genre", "country", "organization"], "instance": {"id": "52", "words": ["After", "their", "first", "two", "albums", ",", "Fungus", "Amongus", "(", "1995", ")", "and", "S.C.I.E.N.C.E.", "(", "1997", ")", ",", "the", "band", "earned", "mainstream", "recognition", "with", "the", "release", "of", "their", "1999", "album", "Make", "Yourself", "which", "spawned", "several", "hits", ",", "including", "the", "band", "'s", "highest", "charting", "song", "Drive", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, song, award, musical artist, band, event, album, musical instrument, music genre, country, organization and O.\nSentence: After their first two albums , Fungus Amongus ( 1995 ) and S.C.I.E.N.C.E. ( 1997 ) , the band earned mainstream recognition with the release of their 1999 album Make Yourself which spawned several hits , including the band 's highest charting song Drive .", "prompt_labels": "After(O) their(O) first(O) two(O) albums(O) ,(O) Fungus(B-album) Amongus(I-album) ((O) 1995(O) )(O) and(O) S.C.I.E.N.C.E.(B-album) ((O) 1997(O) )(O) ,(O) the(O) band(O) earned(O) mainstream(O) recognition(O) with(O) the(O) release(O) of(O) their(O) 1999(O) album(O) Make(B-album) Yourself(I-album) which(O) spawned(O) several(O) hits(O) ,(O) including(O) the(O) band(O) 's(O) highest(O) charting(O) song(O) Drive(B-song) .(O)"}}
{"id": "92", "dataset": "crossner_music", "split": "test", "label_list": ["band", "music genre", "musical instrument", "person", "album", "musical artist", "award", "location", "organization", "event", "song", "country"], "instance": {"id": "92", "words": ["Their", "farewell", "concerts", "at", "Wembley", "Arena", "were", "multiple", "sell-outs", ";", "their", "final", "concert", "took", "place", "at", "the", "Brighton", "Centre", "on", "11", "December", "1982", "."], "labels": ["O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, music genre, musical instrument, person, album, musical artist, award, location, organization, event, song, country and O.\nSentence: Their farewell concerts at Wembley Arena were multiple sell-outs ; their final concert took place at the Brighton Centre on 11 December 1982 .", "prompt_labels": "Their(O) farewell(O) concerts(O) at(O) Wembley(B-location) Arena(I-location) were(O) multiple(O) sell-outs(O) ;(O) their(O) final(O) concert(O) took(O) place(O) at(O) the(O) Brighton(B-location) Centre(I-location) on(O) 11(O) December(O) 1982(O) .(O)"}}
{"id": "35", "dataset": "crossner_music", "split": "test", "label_list": ["album", "band", "event", "musical artist", "person", "award", "song", "location", "country", "music genre", "organization", "musical instrument"], "instance": {"id": "35", "words": ["The", "festival", "presents", "concerts", "in", "Dixieland", ",", "swing", ",", "Jazz", "fusion", ",", "blues", ",", "gospel", ",", "funk", ",", "soul", "and", "drum", "and", "bass", "."], "labels": ["O", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, band, event, musical artist, person, award, song, location, country, music genre, organization, musical instrument and O.\nSentence: The festival presents concerts in Dixieland , swing , Jazz fusion , blues , gospel , funk , soul and drum and bass .", "prompt_labels": "The(O) festival(O) presents(O) concerts(O) in(O) Dixieland(B-music genre) ,(O) swing(B-music genre) ,(O) Jazz(B-music genre) fusion(I-music genre) ,(O) blues(B-music genre) ,(O) gospel(B-music genre) ,(O) funk(B-music genre) ,(O) soul(B-music genre) and(O) drum(B-music genre) and(I-music genre) bass(I-music genre) .(O)"}}
{"id": "300", "dataset": "crossner_music", "split": "test", "label_list": ["song", "band", "person", "musical artist", "country", "organization", "music genre", "musical instrument", "album", "award", "event", "location"], "instance": {"id": "300", "words": ["In", "the", "Southwestern", "United", "States", ",", "it", "was", "the", "Rocky", "Mountains", ",", "American", "frontier", ",", "and", "Rio", "Grande", "that", "acted", "as", "a", "similar", "backdrop", "for", "Indigenous", "music", "of", "North", "America", ",", "Mexican", ",", "and", "cowboy", "ballads", ",", "which", "resulted", "in", "New", "Mexico", "music", "and", "the", "development", "of", "Western", "music", ",", "and", "its", "directly", "related", "Red", "Dirt", ",", "Texas", "country", "music", ",", "and", "Tejano", "music", "styles", "."], "labels": ["O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, band, person, musical artist, country, organization, music genre, musical instrument, album, award, event, location and O.\nSentence: In the Southwestern United States , it was the Rocky Mountains , American frontier , and Rio Grande that acted as a similar backdrop for Indigenous music of North America , Mexican , and cowboy ballads , which resulted in New Mexico music and the development of Western music , and its directly related Red Dirt , Texas country music , and Tejano music styles .", "prompt_labels": "In(O) the(O) Southwestern(B-location) United(I-location) States(I-location) ,(O) it(O) was(O) the(O) Rocky(B-location) Mountains(I-location) ,(O) American(B-location) frontier(I-location) ,(O) and(O) Rio(B-location) Grande(I-location) that(O) acted(O) as(O) a(O) similar(O) backdrop(O) for(O) Indigenous(B-music genre) music(I-music genre) of(I-music genre) North(I-music genre) America(I-music genre) ,(O) Mexican(O) ,(O) and(O) cowboy(O) ballads(O) ,(O) which(O) resulted(O) in(O) New(B-music genre) Mexico(I-music genre) music(I-music genre) and(O) the(O) development(O) of(O) Western(O) music(O) ,(O) and(O) its(O) directly(O) related(O) Red(B-music genre) Dirt(I-music genre) ,(O) Texas(B-music genre) country(I-music genre) music(I-music genre) ,(O) and(O) Tejano(B-music genre) music(I-music genre) styles(O) .(O)"}}
{"id": "133", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "award", "location", "band", "organization", "musical artist", "song", "person", "country", "album", "event", "musical instrument"], "instance": {"id": "133", "words": ["The", "group", "have", "released", "six", "studio", "albums", ":", "both", "The", "Official", "Fiction", "(", "2003", ")", "and", "Desert", "Lights", "(", "2006", ")", "topped", "the", "ARIA", "Albums", "Chart", ";", "while", "Beautiful", "Sharks", "(", "1999", ")", ",", "Echolalia", "(", "2001", ")", "and", "Leave", "Your", "Soul", "to", "Science", "(", "2012", ")", "reached", "the", "top", "10", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, award, location, band, organization, musical artist, song, person, country, album, event, musical instrument and O.\nSentence: The group have released six studio albums : both The Official Fiction ( 2003 ) and Desert Lights ( 2006 ) topped the ARIA Albums Chart ; while Beautiful Sharks ( 1999 ) , Echolalia ( 2001 ) and Leave Your Soul to Science ( 2012 ) reached the top 10 .", "prompt_labels": "The(O) group(O) have(O) released(O) six(O) studio(O) albums(O) :(O) both(O) The(B-album) Official(I-album) Fiction(I-album) ((O) 2003(O) )(O) and(O) Desert(B-album) Lights(I-album) ((O) 2006(O) )(O) topped(O) the(O) ARIA(O) Albums(O) Chart(O) ;(O) while(O) Beautiful(B-album) Sharks(I-album) ((O) 1999(O) )(O) ,(O) Echolalia(B-album) ((O) 2001(O) )(O) and(O) Leave(B-album) Your(I-album) Soul(I-album) to(I-album) Science(I-album) ((O) 2012(O) )(O) reached(O) the(O) top(O) 10(O) .(O)"}}
{"id": "255", "dataset": "crossner_music", "split": "test", "label_list": ["person", "album", "band", "organization", "event", "musical instrument", "musical artist", "award", "country", "music genre", "song", "location"], "instance": {"id": "255", "words": ["Benson", "also", "did", "a", "version", "of", "The", "Beatles", "'", "s", "1969", "album", "Abbey", "Road", "called", "The", "Other", "Side", "of", "Abbey", "Road", ",", "also", "released", "in", "1969", ",", "and", "a", "version", "of", "White", "Rabbit", ",", "originally", "written", "and", "recorded", "by", "San", "Francisco", "rock", "group", "Great", "Society", ",", "and", "made", "famous", "by", "Jefferson", "Airplane", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "B-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "B-music genre", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, album, band, organization, event, musical instrument, musical artist, award, country, music genre, song, location and O.\nSentence: Benson also did a version of The Beatles ' s 1969 album Abbey Road called The Other Side of Abbey Road , also released in 1969 , and a version of White Rabbit , originally written and recorded by San Francisco rock group Great Society , and made famous by Jefferson Airplane .", "prompt_labels": "Benson(B-person) also(O) did(O) a(O) version(O) of(O) The(B-band) Beatles(I-band) '(O) s(O) 1969(O) album(O) Abbey(B-album) Road(I-album) called(O) The(B-album) Other(I-album) Side(I-album) of(I-album) Abbey(I-album) Road(I-album) ,(O) also(O) released(O) in(O) 1969(O) ,(O) and(O) a(O) version(O) of(O) White(B-song) Rabbit(I-song) ,(O) originally(O) written(O) and(O) recorded(O) by(O) San(B-location) Francisco(I-location) rock(B-music genre) group(O) Great(B-band) Society(I-band) ,(O) and(O) made(O) famous(O) by(O) Jefferson(B-band) Airplane(I-band) .(O)"}}
{"id": "141", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "event", "country", "location", "musical instrument", "album", "band", "award", "song", "music genre", "person", "organization"], "instance": {"id": "141", "words": ["In", "1998", "Warp", "signed", "Boards", "Of", "Canada", ",", "a", "duo", "that", "would", "go", "on", "to", "release", "some", "of", "the", "most", "highly", "revered", "electronic", "music", "albums", "of", "their", "time", ":", "Music", "Has", "the", "Right", "to", "Children", "(", "1998", ")", ",", "Geogaddi", "(", "2002", ")", ",", "The", "Campfire", "Headphase", "(", "2005", ")", "and", "Tomorrow", "'s", "Harvest", "(", "2013", ")", "."], "labels": ["O", "O", "B-musical artist", "O", "B-band", "I-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, event, country, location, musical instrument, album, band, award, song, music genre, person, organization and O.\nSentence: In 1998 Warp signed Boards Of Canada , a duo that would go on to release some of the most highly revered electronic music albums of their time : Music Has the Right to Children ( 1998 ) , Geogaddi ( 2002 ) , The Campfire Headphase ( 2005 ) and Tomorrow 's Harvest ( 2013 ) .", "prompt_labels": "In(O) 1998(O) Warp(B-musical artist) signed(O) Boards(B-band) Of(I-band) Canada(I-band) ,(O) a(O) duo(O) that(O) would(O) go(O) on(O) to(O) release(O) some(O) of(O) the(O) most(O) highly(O) revered(O) electronic(B-music genre) music(I-music genre) albums(O) of(O) their(O) time(O) :(O) Music(B-album) Has(I-album) the(I-album) Right(I-album) to(I-album) Children(I-album) ((O) 1998(O) )(O) ,(O) Geogaddi(B-album) ((O) 2002(O) )(O) ,(O) The(B-album) Campfire(I-album) Headphase(I-album) ((O) 2005(O) )(O) and(O) Tomorrow(B-album) 's(I-album) Harvest(I-album) ((O) 2013(O) )(O) .(O)"}}
{"id": "346", "dataset": "crossner_music", "split": "test", "label_list": ["event", "musical instrument", "album", "organization", "band", "person", "song", "country", "musical artist", "location", "award", "music genre"], "instance": {"id": "346", "words": ["About", "the", "musical", "style", "of", "the", "album", "Pitchfork", "writer", "Joe", "Tangari", "wrote", "that", "Generation", "Terrorists", "walked", "a", "weird", "line", "between", "agit-", "Punk", "rock", ",", "cock", "rock", ",", "romantic", "melodicism", "and", "Glam", "rock", ",", "and", "was", "so", "obviously", "patterned", "after", "the", "Clash", "'s", "London", "Calling", "that", "it", "was", "actually", "kind", "of", "cute", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "B-person", "I-person", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical instrument, album, organization, band, person, song, country, musical artist, location, award, music genre and O.\nSentence: About the musical style of the album Pitchfork writer Joe Tangari wrote that Generation Terrorists walked a weird line between agit- Punk rock , cock rock , romantic melodicism and Glam rock , and was so obviously patterned after the Clash 's London Calling that it was actually kind of cute .", "prompt_labels": "About(O) the(O) musical(O) style(O) of(O) the(O) album(O) Pitchfork(B-organization) writer(O) Joe(B-person) Tangari(I-person) wrote(O) that(O) Generation(B-album) Terrorists(I-album) walked(O) a(O) weird(O) line(O) between(O) agit-(O) Punk(B-music genre) rock(I-music genre) ,(O) cock(B-music genre) rock(I-music genre) ,(O) romantic(B-music genre) melodicism(I-music genre) and(O) Glam(B-music genre) rock(I-music genre) ,(O) and(O) was(O) so(O) obviously(O) patterned(O) after(O) the(O) Clash(B-band) 's(O) London(B-album) Calling(I-album) that(O) it(O) was(O) actually(O) kind(O) of(O) cute(O) .(O)"}}
{"id": "39", "dataset": "crossner_music", "split": "test", "label_list": ["award", "person", "country", "song", "band", "location", "music genre", "album", "event", "musical instrument", "musical artist", "organization"], "instance": {"id": "39", "words": ["It", "contained", "what", "would", "become", "Brooks", "'", "signature", "song", ",", "the", "blue", "collar", "anthem", "Friends", "in", "Low", "Places", ",", "as", "well", "as", "other", "popular", "singles", ",", "The", "Thunder", "Rolls", "and", "Unanswered", "Prayers", "."], "labels": ["O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, country, song, band, location, music genre, album, event, musical instrument, musical artist, organization and O.\nSentence: It contained what would become Brooks ' signature song , the blue collar anthem Friends in Low Places , as well as other popular singles , The Thunder Rolls and Unanswered Prayers .", "prompt_labels": "It(O) contained(O) what(O) would(O) become(O) Brooks(B-musical artist) '(O) signature(O) song(O) ,(O) the(O) blue(B-song) collar(I-song) anthem(I-song) Friends(B-song) in(I-song) Low(I-song) Places(I-song) ,(O) as(O) well(O) as(O) other(O) popular(O) singles(O) ,(O) The(B-song) Thunder(I-song) Rolls(I-song) and(O) Unanswered(B-song) Prayers(I-song) .(O)"}}
{"id": "191", "dataset": "crossner_music", "split": "test", "label_list": ["award", "organization", "song", "person", "musical artist", "location", "musical instrument", "band", "country", "event", "album", "music genre"], "instance": {"id": "191", "words": ["He", "is", "also", "a", "recipient", "of", "the", "AFI", "Life", "Achievement", "Award", "for", "his", "contributions", "to", "the", "cinema", ",", "and", "has", "won", "an", "Academy", "Awards", ",", "a", "Palme", "d", "'Or", ",", "Cannes", "Film", "Festival", "Best", "Director", "Award", ",", "Silver", "Lion", ",", "Grammy", "Award", ",", "Emmy", "Award", ",", "Golden", "Globes", ",", "British", "Academy", "Film", "Awards", ",", "and", "Directors", "Guild", "of", "America", "Award", "s", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "O", "B-award", "I-award", "O", "B-award", "I-award", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, song, person, musical artist, location, musical instrument, band, country, event, album, music genre and O.\nSentence: He is also a recipient of the AFI Life Achievement Award for his contributions to the cinema , and has won an Academy Awards , a Palme d 'Or , Cannes Film Festival Best Director Award , Silver Lion , Grammy Award , Emmy Award , Golden Globes , British Academy Film Awards , and Directors Guild of America Award s .", "prompt_labels": "He(O) is(O) also(O) a(O) recipient(O) of(O) the(O) AFI(B-award) Life(I-award) Achievement(I-award) Award(I-award) for(O) his(O) contributions(O) to(O) the(O) cinema(O) ,(O) and(O) has(O) won(O) an(O) Academy(B-award) Awards(I-award) ,(O) a(O) Palme(B-award) d(I-award) 'Or(I-award) ,(O) Cannes(B-award) Film(I-award) Festival(I-award) Best(I-award) Director(I-award) Award(I-award) ,(O) Silver(B-award) Lion(I-award) ,(O) Grammy(B-award) Award(I-award) ,(O) Emmy(B-award) Award(I-award) ,(O) Golden(B-award) Globes(I-award) ,(O) British(B-award) Academy(I-award) Film(I-award) Awards(I-award) ,(O) and(O) Directors(B-award) Guild(I-award) of(I-award) America(I-award) Award(I-award) s(O) .(O)"}}
{"id": "409", "dataset": "crossner_music", "split": "test", "label_list": ["band", "person", "music genre", "album", "award", "musical artist", "location", "song", "musical instrument", "country", "organization", "event"], "instance": {"id": "409", "words": ["Nine", "Inch", "Nails", "covered", "the", "song", "Metal", "on", "The", "Fragile", "remix", "album", "Things", "Falling", "Apart", "as", "did", "Afrika", "Bambaataa", "(", "with", "Numan", "himself", ")", "on", "the", "album", "Dark", "Matter", "Moving", "at", "the", "Speed", "of", "Light", "."], "labels": ["B-band", "I-band", "I-band", "O", "O", "O", "B-song", "O", "B-album", "I-album", "O", "O", "B-album", "I-album", "I-album", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, person, music genre, album, award, musical artist, location, song, musical instrument, country, organization, event and O.\nSentence: Nine Inch Nails covered the song Metal on The Fragile remix album Things Falling Apart as did Afrika Bambaataa ( with Numan himself ) on the album Dark Matter Moving at the Speed of Light .", "prompt_labels": "Nine(B-band) Inch(I-band) Nails(I-band) covered(O) the(O) song(O) Metal(B-song) on(O) The(B-album) Fragile(I-album) remix(O) album(O) Things(B-album) Falling(I-album) Apart(I-album) as(O) did(O) Afrika(B-musical artist) Bambaataa(I-musical artist) ((O) with(O) Numan(B-musical artist) himself(O) )(O) on(O) the(O) album(O) Dark(B-album) Matter(I-album) Moving(I-album) at(I-album) the(I-album) Speed(I-album) of(I-album) Light(I-album) .(O)"}}
{"id": "69", "dataset": "crossner_music", "split": "test", "label_list": ["award", "song", "location", "album", "organization", "music genre", "country", "musical instrument", "musical artist", "person", "band", "event"], "instance": {"id": "69", "words": ["Today", ",", "zydeco", "integrates", "genres", "such", "as", "Rhythm", "and", "blues", ",", "Soul", "music", ",", "brass", "band", ",", "reggae", ",", "Hip", "hop", "music", ",", "ska", ",", "Rock", "music", ",", "Afro-Caribbean", "music", "and", "other", "styles", ",", "in", "addition", "to", "the", "traditional", "forms", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, song, location, album, organization, music genre, country, musical instrument, musical artist, person, band, event and O.\nSentence: Today , zydeco integrates genres such as Rhythm and blues , Soul music , brass band , reggae , Hip hop music , ska , Rock music , Afro-Caribbean music and other styles , in addition to the traditional forms .", "prompt_labels": "Today(O) ,(O) zydeco(O) integrates(O) genres(O) such(O) as(O) Rhythm(B-music genre) and(I-music genre) blues(I-music genre) ,(O) Soul(B-music genre) music(I-music genre) ,(O) brass(B-music genre) band(I-music genre) ,(O) reggae(B-music genre) ,(O) Hip(B-music genre) hop(I-music genre) music(I-music genre) ,(O) ska(B-music genre) ,(O) Rock(B-music genre) music(I-music genre) ,(O) Afro-Caribbean(B-music genre) music(I-music genre) and(O) other(O) styles(O) ,(O) in(O) addition(O) to(O) the(O) traditional(O) forms(O) .(O)"}}
{"id": "235", "dataset": "crossner_music", "split": "test", "label_list": ["event", "song", "musical instrument", "musical artist", "country", "organization", "music genre", "album", "location", "band", "award", "person"], "instance": {"id": "235", "words": ["It", "includes", "folklore", "music", "of", "parts", "of", "Bolivia", ",", "Ecuador", ",", "Chile", ",", "Colombia", ",", "Peru", "and", "Venezuela", "."], "labels": ["O", "O", "B-music genre", "I-music genre", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, song, musical instrument, musical artist, country, organization, music genre, album, location, band, award, person and O.\nSentence: It includes folklore music of parts of Bolivia , Ecuador , Chile , Colombia , Peru and Venezuela .", "prompt_labels": "It(O) includes(O) folklore(B-music genre) music(I-music genre) of(O) parts(O) of(O) Bolivia(B-country) ,(O) Ecuador(B-country) ,(O) Chile(B-country) ,(O) Colombia(B-country) ,(O) Peru(B-country) and(O) Venezuela(B-country) .(O)"}}
{"id": "338", "dataset": "crossner_music", "split": "test", "label_list": ["band", "person", "country", "award", "album", "musical artist", "song", "event", "musical instrument", "location", "music genre", "organization"], "instance": {"id": "338", "words": ["Os", "Mutantes", "(", "meaning", "the", "mutants", ";", ")", "are", "an", "influential", "Brazilian", "Rock", "music", "band", "that", "were", "linked", "with", "the", "Tropicália", "movement", ",", "a", "dissident", "musical", "movement", "during", "the", "Brazilian", "dictatorship", "of", "the", "late", "1960s", "."], "labels": ["B-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, person, country, award, album, musical artist, song, event, musical instrument, location, music genre, organization and O.\nSentence: Os Mutantes ( meaning the mutants ; ) are an influential Brazilian Rock music band that were linked with the Tropicália movement , a dissident musical movement during the Brazilian dictatorship of the late 1960s .", "prompt_labels": "Os(B-band) Mutantes(I-band) ((O) meaning(O) the(O) mutants(O) ;(O) )(O) are(O) an(O) influential(O) Brazilian(O) Rock(B-music genre) music(I-music genre) band(O) that(O) were(O) linked(O) with(O) the(O) Tropicália(B-music genre) movement(O) ,(O) a(O) dissident(O) musical(O) movement(O) during(O) the(O) Brazilian(O) dictatorship(O) of(O) the(O) late(O) 1960s(O) .(O)"}}
{"id": "120", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "country", "event", "location", "song", "band", "music genre", "musical instrument", "album", "award", "organization", "person"], "instance": {"id": "120", "words": ["His", "songs", "for", "the", "group", "included", "Taxman", ",", "Within", "You", "Without", "You", ",", "While", "My", "Guitar", "Gently", "Weeps", ",", "Here", "Comes", "the", "Sun", "and", "Something", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, country, event, location, song, band, music genre, musical instrument, album, award, organization, person and O.\nSentence: His songs for the group included Taxman , Within You Without You , While My Guitar Gently Weeps , Here Comes the Sun and Something .", "prompt_labels": "His(O) songs(O) for(O) the(O) group(O) included(O) Taxman(B-song) ,(O) Within(B-song) You(I-song) Without(I-song) You(I-song) ,(O) While(B-song) My(I-song) Guitar(I-song) Gently(I-song) Weeps(I-song) ,(O) Here(B-song) Comes(I-song) the(I-song) Sun(I-song) and(O) Something(B-song) .(O)"}}
{"id": "306", "dataset": "crossner_music", "split": "test", "label_list": ["person", "event", "musical artist", "organization", "song", "album", "music genre", "country", "band", "location", "musical instrument", "award"], "instance": {"id": "306", "words": ["Folsom", "Field", "is", "also", "used", "as", "the", "finish", "line", "for", "the", "Bolder", "Boulder", ",", "a", "popular", "10K", "run", "."], "labels": ["B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, musical artist, organization, song, album, music genre, country, band, location, musical instrument, award and O.\nSentence: Folsom Field is also used as the finish line for the Bolder Boulder , a popular 10K run .", "prompt_labels": "Folsom(B-location) Field(I-location) is(O) also(O) used(O) as(O) the(O) finish(O) line(O) for(O) the(O) Bolder(B-event) Boulder(I-event) ,(O) a(O) popular(O) 10K(O) run(O) .(O)"}}
{"id": "84", "dataset": "crossner_music", "split": "test", "label_list": ["person", "musical instrument", "album", "country", "musical artist", "music genre", "location", "song", "award", "band", "event", "organization"], "instance": {"id": "84", "words": ["She", "had", "her", "first", "experience", "in", "show", "business", "when", "she", "was", "crowned", "Miss", "Teenager", "Universal", "in", "1971", ",", "and", "was", "Miss", "World", "/", "Venezuela", "in", "1975", "where", "she", "became", "sixth", "runner-up", "in", "the", "Miss", "World", "pageant", "won", "by", "Puerto", "Rico", "'s", "Wilnelia", "Merced", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-location", "I-location", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, musical instrument, album, country, musical artist, music genre, location, song, award, band, event, organization and O.\nSentence: She had her first experience in show business when she was crowned Miss Teenager Universal in 1971 , and was Miss World / Venezuela in 1975 where she became sixth runner-up in the Miss World pageant won by Puerto Rico 's Wilnelia Merced .", "prompt_labels": "She(O) had(O) her(O) first(O) experience(O) in(O) show(O) business(O) when(O) she(O) was(O) crowned(O) Miss(B-award) Teenager(I-award) Universal(I-award) in(O) 1971(O) ,(O) and(O) was(O) Miss(B-award) World(I-award) /(O) Venezuela(B-country) in(O) 1975(O) where(O) she(O) became(O) sixth(O) runner-up(O) in(O) the(O) Miss(B-event) World(I-event) pageant(I-event) won(O) by(O) Puerto(B-location) Rico(I-location) 's(O) Wilnelia(B-person) Merced(I-person) .(O)"}}
{"id": "386", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "organization", "location", "band", "music genre", "award", "person", "album", "country", "event", "song", "musical instrument"], "instance": {"id": "386", "words": ["Morissette", "assumed", "creative", "control", "and", "producing", "duties", "for", "her", "subsequent", "studio", "albums", ",", "including", "Under", "Rug", "Swept", "(", "2002", ")", ",", "So-Called", "Chaos", "(", "2004", ")", ",", "and", "Flavors", "of", "Entanglement", "(", "2008", ")", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, organization, location, band, music genre, award, person, album, country, event, song, musical instrument and O.\nSentence: Morissette assumed creative control and producing duties for her subsequent studio albums , including Under Rug Swept ( 2002 ) , So-Called Chaos ( 2004 ) , and Flavors of Entanglement ( 2008 ) .", "prompt_labels": "Morissette(B-musical artist) assumed(O) creative(O) control(O) and(O) producing(O) duties(O) for(O) her(O) subsequent(O) studio(O) albums(O) ,(O) including(O) Under(B-album) Rug(I-album) Swept(I-album) ((O) 2002(O) )(O) ,(O) So-Called(B-album) Chaos(I-album) ((O) 2004(O) )(O) ,(O) and(O) Flavors(B-album) of(I-album) Entanglement(I-album) ((O) 2008(O) )(O) .(O)"}}
{"id": "462", "dataset": "crossner_music", "split": "test", "label_list": ["award", "music genre", "song", "musical artist", "musical instrument", "location", "person", "event", "band", "album", "country", "organization"], "instance": {"id": "462", "words": ["Crosby", "gained", "valuable", "experience", "on", "tour", "for", "a", "year", "with", "Whiteman", "and", "performing", "and", "recording", "with", "Bix", "Beiderbecke", ",", "Jack", "Teagarden", ",", "Tommy", "Dorsey", ",", "Jimmy", "Dorsey", ",", "Eddie", "Lang", ",", "and", "Hoagy", "Carmichael", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, music genre, song, musical artist, musical instrument, location, person, event, band, album, country, organization and O.\nSentence: Crosby gained valuable experience on tour for a year with Whiteman and performing and recording with Bix Beiderbecke , Jack Teagarden , Tommy Dorsey , Jimmy Dorsey , Eddie Lang , and Hoagy Carmichael .", "prompt_labels": "Crosby(B-musical artist) gained(O) valuable(O) experience(O) on(O) tour(O) for(O) a(O) year(O) with(O) Whiteman(B-musical artist) and(O) performing(O) and(O) recording(O) with(O) Bix(B-musical artist) Beiderbecke(I-musical artist) ,(O) Jack(B-musical artist) Teagarden(I-musical artist) ,(O) Tommy(B-musical artist) Dorsey(I-musical artist) ,(O) Jimmy(B-musical artist) Dorsey(I-musical artist) ,(O) Eddie(B-musical artist) Lang(I-musical artist) ,(O) and(O) Hoagy(B-musical artist) Carmichael(I-musical artist) .(O)"}}
{"id": "441", "dataset": "crossner_music", "split": "test", "label_list": ["band", "musical artist", "award", "musical instrument", "song", "event", "location", "music genre", "country", "album", "organization", "person"], "instance": {"id": "441", "words": ["The", "Doors", "incorporated", "Asturias", "into", "their", "song", "Spanish", "Caravan", ";", "also", ",", "Iron", "Maiden", "'", "s", "Piece", "of", "Mind", "uses", "the", "introduction", "of", "the", "piece", "for", "the", "song", "bridge", "."], "labels": ["B-band", "I-band", "O", "B-location", "O", "O", "O", "B-song", "I-song", "O", "O", "O", "B-band", "I-band", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical artist, award, musical instrument, song, event, location, music genre, country, album, organization, person and O.\nSentence: The Doors incorporated Asturias into their song Spanish Caravan ; also , Iron Maiden ' s Piece of Mind uses the introduction of the piece for the song bridge .", "prompt_labels": "The(B-band) Doors(I-band) incorporated(O) Asturias(B-location) into(O) their(O) song(O) Spanish(B-song) Caravan(I-song) ;(O) also(O) ,(O) Iron(B-band) Maiden(I-band) '(O) s(O) Piece(B-album) of(I-album) Mind(I-album) uses(O) the(O) introduction(O) of(O) the(O) piece(O) for(O) the(O) song(O) bridge(O) .(O)"}}
{"id": "126", "dataset": "crossner_music", "split": "test", "label_list": ["song", "country", "music genre", "band", "person", "musical instrument", "musical artist", "organization", "award", "location", "album", "event"], "instance": {"id": "126", "words": ["Due", "to", "shared", "cultural", "heritage", "and", "language", ",", "Indian", "music", "and", "Bollywood", "films", "are", "also", "popular", "in", "Afghanistan", ",", "Pakistan", ",", "Bangladesh", ",", "and", "Nepal", ",", "where", "Hindustani", "is", "widely", "understood", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, country, music genre, band, person, musical instrument, musical artist, organization, award, location, album, event and O.\nSentence: Due to shared cultural heritage and language , Indian music and Bollywood films are also popular in Afghanistan , Pakistan , Bangladesh , and Nepal , where Hindustani is widely understood .", "prompt_labels": "Due(O) to(O) shared(O) cultural(O) heritage(O) and(O) language(O) ,(O) Indian(O) music(O) and(O) Bollywood(O) films(O) are(O) also(O) popular(O) in(O) Afghanistan(B-country) ,(O) Pakistan(B-country) ,(O) Bangladesh(B-country) ,(O) and(O) Nepal(B-country) ,(O) where(O) Hindustani(O) is(O) widely(O) understood(O) .(O)"}}
{"id": "83", "dataset": "crossner_music", "split": "test", "label_list": ["country", "band", "song", "person", "album", "musical artist", "location", "musical instrument", "music genre", "organization", "award", "event"], "instance": {"id": "83", "words": ["Mathis", "has", "undergone", "rehabilitation", "for", "both", "alcohol", "and", "prescription", "drug", "addictions", ",", "and", "he", "has", "supported", "many", "organizations", "through", "the", "years", ",", "including", "the", "American", "Cancer", "Society", ",", "the", "March", "of", "Dimes", ",", "the", "YWCA", "and", "YMCA", ",", "the", "Muscular", "Dystrophy", "Association", "and", "the", "NAACP", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "O", "B-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, band, song, person, album, musical artist, location, musical instrument, music genre, organization, award, event and O.\nSentence: Mathis has undergone rehabilitation for both alcohol and prescription drug addictions , and he has supported many organizations through the years , including the American Cancer Society , the March of Dimes , the YWCA and YMCA , the Muscular Dystrophy Association and the NAACP .", "prompt_labels": "Mathis(B-musical artist) has(O) undergone(O) rehabilitation(O) for(O) both(O) alcohol(O) and(O) prescription(O) drug(O) addictions(O) ,(O) and(O) he(O) has(O) supported(O) many(O) organizations(O) through(O) the(O) years(O) ,(O) including(O) the(O) American(B-organization) Cancer(I-organization) Society(I-organization) ,(O) the(O) March(B-organization) of(I-organization) Dimes(I-organization) ,(O) the(O) YWCA(B-organization) and(O) YMCA(B-organization) ,(O) the(O) Muscular(B-organization) Dystrophy(I-organization) Association(I-organization) and(O) the(O) NAACP(B-organization) .(O)"}}
{"id": "38", "dataset": "crossner_music", "split": "test", "label_list": ["song", "album", "band", "organization", "music genre", "location", "musical artist", "country", "award", "event", "musical instrument", "person"], "instance": {"id": "38", "words": ["Some", "managers", "in", "Europe", "soon", "created", "their", "own", "acts", "after", "being", "inspired", "by", "New", "Kids", "on", "the", "Block", ",", "beginning", "with", "Nigel", "Martin-Smith", "'", "s", "Take", "That", "in", "the", "UK", "(", "formed", "in", "1990", ")", "and", "followed", "by", "Tom", "Watkins", ",", "who", "had", "success", "with", "Bros", "in", "the", "late", "1980s", "and", "formed", "East", "17", "in", "1991", "."], "labels": ["O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-band", "I-band", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-band", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, album, band, organization, music genre, location, musical artist, country, award, event, musical instrument, person and O.\nSentence: Some managers in Europe soon created their own acts after being inspired by New Kids on the Block , beginning with Nigel Martin-Smith ' s Take That in the UK ( formed in 1990 ) and followed by Tom Watkins , who had success with Bros in the late 1980s and formed East 17 in 1991 .", "prompt_labels": "Some(O) managers(O) in(O) Europe(B-location) soon(O) created(O) their(O) own(O) acts(O) after(O) being(O) inspired(O) by(O) New(B-band) Kids(I-band) on(I-band) the(I-band) Block(I-band) ,(O) beginning(O) with(O) Nigel(B-musical artist) Martin-Smith(I-musical artist) '(O) s(O) Take(B-band) That(I-band) in(O) the(O) UK(B-country) ((O) formed(O) in(O) 1990(O) )(O) and(O) followed(O) by(O) Tom(B-musical artist) Watkins(I-musical artist) ,(O) who(O) had(O) success(O) with(O) Bros(B-band) in(O) the(O) late(O) 1980s(O) and(O) formed(O) East(B-band) 17(I-band) in(O) 1991(O) .(O)"}}
{"id": "347", "dataset": "crossner_music", "split": "test", "label_list": ["album", "location", "award", "music genre", "song", "country", "musical instrument", "event", "person", "band", "organization", "musical artist"], "instance": {"id": "347", "words": ["At", "the", "ARIA", "Music", "Awards", "of", "1995", "Arena", "was", "nominated", "in", "six", "categories", "and", "won", "four", "trophies", ":", "ARIA", "Award", "for", "Best", "Pop", "Release", "and", "Song", "of", "the", "Year", "for", "Chains", ";", "and", "ARIA", "Award", "for", "Album", "of", "the", "Year", "and", "ARIA", "Award", "for", "Best", "Female", "Artist", "for", "Don", "'t", "Ask", ".", "ARIA", "Music", "Awards", "for", "Tina", "Arena", ":"], "labels": ["O", "O", "B-award", "I-award", "I-award", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "O", "B-song", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-album", "I-album", "I-album", "O", "B-award", "I-award", "I-award", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, location, award, music genre, song, country, musical instrument, event, person, band, organization, musical artist and O.\nSentence: At the ARIA Music Awards of 1995 Arena was nominated in six categories and won four trophies : ARIA Award for Best Pop Release and Song of the Year for Chains ; and ARIA Award for Album of the Year and ARIA Award for Best Female Artist for Don 't Ask . ARIA Music Awards for Tina Arena :", "prompt_labels": "At(O) the(O) ARIA(B-award) Music(I-award) Awards(I-award) of(O) 1995(O) Arena(B-musical artist) was(O) nominated(O) in(O) six(O) categories(O) and(O) won(O) four(O) trophies(O) :(O) ARIA(B-award) Award(I-award) for(I-award) Best(I-award) Pop(I-award) Release(I-award) and(O) Song(B-award) of(I-award) the(I-award) Year(I-award) for(O) Chains(B-song) ;(O) and(O) ARIA(B-award) Award(I-award) for(I-award) Album(I-award) of(I-award) the(I-award) Year(I-award) and(O) ARIA(B-award) Award(I-award) for(I-award) Best(I-award) Female(I-award) Artist(I-award) for(O) Don(B-album) 't(I-album) Ask(I-album) .(O) ARIA(B-award) Music(I-award) Awards(I-award) for(O) Tina(B-musical artist) Arena(I-musical artist) :(O)"}}
{"id": "46", "dataset": "crossner_music", "split": "test", "label_list": ["song", "album", "musical artist", "award", "organization", "person", "music genre", "country", "band", "location", "musical instrument", "event"], "instance": {"id": "46", "words": ["In", "January", "1970", "Lucian", "K.", "Truscott", "IV", "reviewing", "Led", "Zeppelin", "II", "for", "the", "Village", "Voice", "described", "the", "sound", "as", "heavy", "and", "made", "comparisons", "with", "Blue", "Cheer", "and", "Vanilla", "Fudge", "..", "January", "22", ",", "1970", "."], "labels": ["O", "O", "O", "B-person", "I-person", "I-person", "I-person", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, album, musical artist, award, organization, person, music genre, country, band, location, musical instrument, event and O.\nSentence: In January 1970 Lucian K. Truscott IV reviewing Led Zeppelin II for the Village Voice described the sound as heavy and made comparisons with Blue Cheer and Vanilla Fudge .. January 22 , 1970 .", "prompt_labels": "In(O) January(O) 1970(O) Lucian(B-person) K.(I-person) Truscott(I-person) IV(I-person) reviewing(O) Led(B-album) Zeppelin(I-album) II(I-album) for(O) the(O) Village(O) Voice(O) described(O) the(O) sound(O) as(O) heavy(O) and(O) made(O) comparisons(O) with(O) Blue(B-band) Cheer(I-band) and(O) Vanilla(B-band) Fudge(I-band) ..(O) January(O) 22(O) ,(O) 1970(O) .(O)"}}
{"id": "182", "dataset": "crossner_music", "split": "test", "label_list": ["location", "musical artist", "song", "country", "album", "band", "musical instrument", "music genre", "person", "event", "organization", "award"], "instance": {"id": "182", "words": ["Steel-string", "guitars", "are", "also", "important", "in", "the", "world", "of", "flatpicking", ",", "as", "utilized", "by", "such", "artists", "as", "Clarence", "White", ",", "Tony", "Rice", ",", "Bryan", "Sutton", ",", "Doc", "Watson", "and", "David", "Grier", "."], "labels": ["B-musical instrument", "I-musical instrument", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical artist, song, country, album, band, musical instrument, music genre, person, event, organization, award and O.\nSentence: Steel-string guitars are also important in the world of flatpicking , as utilized by such artists as Clarence White , Tony Rice , Bryan Sutton , Doc Watson and David Grier .", "prompt_labels": "Steel-string(B-musical instrument) guitars(I-musical instrument) are(O) also(O) important(O) in(O) the(O) world(O) of(O) flatpicking(O) ,(O) as(O) utilized(O) by(O) such(O) artists(O) as(O) Clarence(B-musical artist) White(I-musical artist) ,(O) Tony(B-musical artist) Rice(I-musical artist) ,(O) Bryan(B-musical artist) Sutton(I-musical artist) ,(O) Doc(B-musical artist) Watson(I-musical artist) and(O) David(B-musical artist) Grier(I-musical artist) .(O)"}}
{"id": "329", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "award", "musical artist", "music genre", "country", "organization", "album", "person", "band", "event", "location", "song"], "instance": {"id": "329", "words": ["In", "the", "1990s", ",", "artists", "like", "Me", "'shell", "Ndegeocello", ",", "Brooklyn", "Funk", "Essentials", "and", "the", "(", "predominantly", "UK-based", ")", "acid", "jazz", "movement", "including", "artists", "and", "bands", "such", "as", "Jamiroquai", ",", "Incognito", ",", "Galliano", ",", "Omar", "Lye-Fook", ",", "Los", "Tetas", "and", "the", "Brand", "New", "Heavies", "carried", "on", "with", "strong", "elements", "of", "funk", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-band", "I-band", "I-band", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "B-band", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "O", "B-band", "I-band", "I-band", "O", "O", "O", "O", "O", "O", "B-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, award, musical artist, music genre, country, organization, album, person, band, event, location, song and O.\nSentence: In the 1990s , artists like Me 'shell Ndegeocello , Brooklyn Funk Essentials and the ( predominantly UK-based ) acid jazz movement including artists and bands such as Jamiroquai , Incognito , Galliano , Omar Lye-Fook , Los Tetas and the Brand New Heavies carried on with strong elements of funk .", "prompt_labels": "In(O) the(O) 1990s(O) ,(O) artists(O) like(O) Me(B-musical artist) 'shell(I-musical artist) Ndegeocello(I-musical artist) ,(O) Brooklyn(B-band) Funk(I-band) Essentials(I-band) and(O) the(O) ((O) predominantly(O) UK-based(O) )(O) acid(B-music genre) jazz(I-music genre) movement(O) including(O) artists(O) and(O) bands(O) such(O) as(O) Jamiroquai(B-band) ,(O) Incognito(B-band) ,(O) Galliano(B-band) ,(O) Omar(B-musical artist) Lye-Fook(I-musical artist) ,(O) Los(B-band) Tetas(I-band) and(O) the(O) Brand(B-band) New(I-band) Heavies(I-band) carried(O) on(O) with(O) strong(O) elements(O) of(O) funk(B-music genre) .(O)"}}
{"id": "371", "dataset": "crossner_music", "split": "test", "label_list": ["band", "musical instrument", "song", "album", "location", "music genre", "country", "organization", "event", "person", "award", "musical artist"], "instance": {"id": "371", "words": ["She", "represented", "America", "in", "the", "opening", "ceremony", "of", "2007", "Special", "Olympics", "World", "Summer", "Games", "singing", "the", "song", "I", "Know", "I", "Can", ",", "and", "in", "the", "opening", "ceremony", "of", "Expo", "2010", "Shanghai", "China", ",", "singing", "the", "song", "Better", "City", ",", "Better", "Life", "with", "Jon", "B.", ",", "both", "songs", "which", "she", "co-wrote", "with", "Quincy", "Jones", "."], "labels": ["O", "O", "B-country", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical instrument, song, album, location, music genre, country, organization, event, person, award, musical artist and O.\nSentence: She represented America in the opening ceremony of 2007 Special Olympics World Summer Games singing the song I Know I Can , and in the opening ceremony of Expo 2010 Shanghai China , singing the song Better City , Better Life with Jon B. , both songs which she co-wrote with Quincy Jones .", "prompt_labels": "She(O) represented(O) America(B-country) in(O) the(O) opening(O) ceremony(O) of(O) 2007(B-event) Special(I-event) Olympics(I-event) World(I-event) Summer(I-event) Games(I-event) singing(O) the(O) song(O) I(B-song) Know(I-song) I(I-song) Can(I-song) ,(O) and(O) in(O) the(O) opening(O) ceremony(O) of(O) Expo(B-event) 2010(I-event) Shanghai(I-event) China(I-event) ,(O) singing(O) the(O) song(O) Better(B-song) City(I-song) ,(I-song) Better(I-song) Life(I-song) with(O) Jon(B-musical artist) B.(I-musical artist) ,(O) both(O) songs(O) which(O) she(O) co-wrote(O) with(O) Quincy(B-musical artist) Jones(I-musical artist) .(O)"}}
{"id": "225", "dataset": "crossner_music", "split": "test", "label_list": ["song", "organization", "musical instrument", "country", "event", "award", "album", "musical artist", "band", "location", "music genre", "person"], "instance": {"id": "225", "words": ["His", "producing", "credits", "also", "include", "the", "UK", "or", "US", "Top", "10", "albums", "Cloud", "Nine", "(", "Harrison", ",", "1987", ")", ",", "Mystery", "Girl", "(", "Orbison", ",", "1989", ")", ",", "Full", "Moon", "Fever", "(", "Petty", ",", "1989", ")", ",", "Into", "the", "Great", "Wide", "Open", "(", "Petty", ",", "1991", ")", ",", "Flaming", "Pie", "(", "Paul", "McCartney", ",", "1997", ")", "and", "Get", "Up", "!", "(", "Bryan", "Adams", ",", "2015", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "O", "B-album", "I-album", "O", "B-musical artist", "O", "O", "O", "O", "B-album", "I-album", "O", "B-musical artist", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "B-musical artist", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "B-musical artist", "O", "O", "O", "O", "B-album", "I-album", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, organization, musical instrument, country, event, award, album, musical artist, band, location, music genre, person and O.\nSentence: His producing credits also include the UK or US Top 10 albums Cloud Nine ( Harrison , 1987 ) , Mystery Girl ( Orbison , 1989 ) , Full Moon Fever ( Petty , 1989 ) , Into the Great Wide Open ( Petty , 1991 ) , Flaming Pie ( Paul McCartney , 1997 ) and Get Up ! ( Bryan Adams , 2015 ) .", "prompt_labels": "His(O) producing(O) credits(O) also(O) include(O) the(O) UK(B-country) or(O) US(B-country) Top(O) 10(O) albums(O) Cloud(B-album) Nine(I-album) ((O) Harrison(B-musical artist) ,(O) 1987(O) )(O) ,(O) Mystery(B-album) Girl(I-album) ((O) Orbison(B-musical artist) ,(O) 1989(O) )(O) ,(O) Full(B-album) Moon(I-album) Fever(I-album) ((O) Petty(B-musical artist) ,(O) 1989(O) )(O) ,(O) Into(B-album) the(I-album) Great(I-album) Wide(I-album) Open(I-album) ((O) Petty(B-musical artist) ,(O) 1991(O) )(O) ,(O) Flaming(B-album) Pie(I-album) ((O) Paul(B-musical artist) McCartney(I-musical artist) ,(O) 1997(O) )(O) and(O) Get(B-album) Up(I-album) !(I-album) ((O) Bryan(B-musical artist) Adams(I-musical artist) ,(O) 2015(O) )(O) .(O)"}}
{"id": "259", "dataset": "crossner_music", "split": "test", "label_list": ["band", "music genre", "event", "organization", "country", "album", "location", "musical artist", "award", "song", "musical instrument", "person"], "instance": {"id": "259", "words": ["On", "numerous", "songs", "from", "band", "'s", "fourth", "album", ",", "A", "Thousand", "Suns", ",", "such", "as", "the", "album", "'s", "singles", "(", "The", "Catalyst", ",", "Burning", "in", "the", "Skies", ",", "Iridescent", ")", ",", "both", "Shinoda", "and", "Bennington", "sing", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "O", "O", "O", "B-musical artist", "O", "B-musical artist", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, music genre, event, organization, country, album, location, musical artist, award, song, musical instrument, person and O.\nSentence: On numerous songs from band 's fourth album , A Thousand Suns , such as the album 's singles ( The Catalyst , Burning in the Skies , Iridescent ) , both Shinoda and Bennington sing .", "prompt_labels": "On(O) numerous(O) songs(O) from(O) band(O) 's(O) fourth(O) album(O) ,(O) A(B-album) Thousand(I-album) Suns(I-album) ,(O) such(O) as(O) the(O) album(O) 's(O) singles(O) ((O) The(B-song) Catalyst(I-song) ,(O) Burning(B-song) in(I-song) the(I-song) Skies(I-song) ,(O) Iridescent(B-song) )(O) ,(O) both(O) Shinoda(B-musical artist) and(O) Bennington(B-musical artist) sing(O) .(O)"}}
{"id": "81", "dataset": "crossner_music", "split": "test", "label_list": ["band", "award", "event", "musical artist", "country", "location", "person", "music genre", "musical instrument", "organization", "song", "album"], "instance": {"id": "81", "words": ["Shulman", ",", "Art", "Dynamo", "-", "Country", "Style", "(", "1956", ")", ",", "TV", "Guide", ",", "p", ",", "28", "The", "late", "1950s", "saw", "the", "emergence", "of", "Buddy", "Holly", ",", "but", "by", "the", "end", "of", "the", "decade", ",", "backlash", "as", "well", "as", "traditional", "artists", "such", "as", "Ray", "Price", ",", "Marty", "Robbins", ",", "and", "Johnny", "Horton", "began", "to", "shift", "the", "industry", "away", "from", "the", "rock", "n", "'", "roll", "influences", "of", "the", "mid-1950s", "."], "labels": ["B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, award, event, musical artist, country, location, person, music genre, musical instrument, organization, song, album and O.\nSentence: Shulman , Art Dynamo - Country Style ( 1956 ) , TV Guide , p , 28 The late 1950s saw the emergence of Buddy Holly , but by the end of the decade , backlash as well as traditional artists such as Ray Price , Marty Robbins , and Johnny Horton began to shift the industry away from the rock n ' roll influences of the mid-1950s .", "prompt_labels": "Shulman(B-person) ,(I-person) Art(I-person) Dynamo(O) -(O) Country(O) Style(O) ((O) 1956(O) )(O) ,(O) TV(O) Guide(O) ,(O) p(O) ,(O) 28(O) The(O) late(O) 1950s(O) saw(O) the(O) emergence(O) of(O) Buddy(B-album) Holly(I-album) ,(O) but(O) by(O) the(O) end(O) of(O) the(O) decade(O) ,(O) backlash(O) as(O) well(O) as(O) traditional(O) artists(O) such(O) as(O) Ray(B-musical artist) Price(I-musical artist) ,(O) Marty(B-musical artist) Robbins(I-musical artist) ,(O) and(O) Johnny(B-musical artist) Horton(I-musical artist) began(O) to(O) shift(O) the(O) industry(O) away(O) from(O) the(O) rock(B-music genre) n(I-music genre) '(I-music genre) roll(I-music genre) influences(O) of(O) the(O) mid-1950s(O) .(O)"}}
{"id": "31", "dataset": "crossner_music", "split": "test", "label_list": ["country", "musical instrument", "location", "event", "band", "award", "musical artist", "album", "person", "organization", "song", "music genre"], "instance": {"id": "31", "words": ["The", "style", "of", "British", "blues", "developed", "in", "the", "UK", ",", "when", "bands", "such", "as", "the", "The", "Animals", ",", "Fleetwood", "Mac", ",", "John", "Mayall", "&", "the", "Bluesbreakers", ",", "the", "The", "Rolling", "Stones", ",", "the", "The", "Yardbirds", ",", "the", "supergroup", "Cream", "and", "the", "Irish", "musician", "Rory", "Gallagher", "performed", "classic", "blues", "songs", "from", "the", "Delta", "blues", "or", "Chicago", "blues", "traditions", "."], "labels": ["O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "O", "O", "B-band", "I-band", "I-band", "O", "O", "B-band", "I-band", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-music genre", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, musical instrument, location, event, band, award, musical artist, album, person, organization, song, music genre and O.\nSentence: The style of British blues developed in the UK , when bands such as the The Animals , Fleetwood Mac , John Mayall & the Bluesbreakers , the The Rolling Stones , the The Yardbirds , the supergroup Cream and the Irish musician Rory Gallagher performed classic blues songs from the Delta blues or Chicago blues traditions .", "prompt_labels": "The(O) style(O) of(O) British(B-music genre) blues(I-music genre) developed(O) in(O) the(O) UK(B-country) ,(O) when(O) bands(O) such(O) as(O) the(O) The(B-band) Animals(I-band) ,(O) Fleetwood(B-band) Mac(I-band) ,(O) John(B-band) Mayall(I-band) &(I-band) the(I-band) Bluesbreakers(I-band) ,(O) the(O) The(B-band) Rolling(I-band) Stones(I-band) ,(O) the(O) The(B-band) Yardbirds(I-band) ,(O) the(O) supergroup(B-band) Cream(I-band) and(O) the(O) Irish(O) musician(O) Rory(B-musical artist) Gallagher(I-musical artist) performed(O) classic(O) blues(B-music genre) songs(O) from(O) the(O) Delta(B-music genre) blues(I-music genre) or(O) Chicago(B-music genre) blues(I-music genre) traditions(O) .(O)"}}
{"id": "292", "dataset": "crossner_music", "split": "test", "label_list": ["country", "organization", "musical instrument", "music genre", "album", "event", "person", "award", "song", "band", "location", "musical artist"], "instance": {"id": "292", "words": ["Rock", "and", "roll", "songs", "critical", "of", "disco", "included", "Bob", "Seger", "'", "s", "Old", "Time", "Rock", "and", "Roll", "and", ",", "especially", ",", "The", "Who", "'", "s", "Sister", "Disco", "(", "both", "1978", ")", "-", "although", "The", "Who", "'s", "Eminence", "Front", "(", "four", "years", "later", ")", "had", "a", "disco", "feel", "."], "labels": ["B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "B-music genre", "O", "B-musical artist", "I-musical artist", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "B-song", "I-song", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, musical instrument, music genre, album, event, person, award, song, band, location, musical artist and O.\nSentence: Rock and roll songs critical of disco included Bob Seger ' s Old Time Rock and Roll and , especially , The Who ' s Sister Disco ( both 1978 ) - although The Who 's Eminence Front ( four years later ) had a disco feel .", "prompt_labels": "Rock(B-music genre) and(I-music genre) roll(I-music genre) songs(O) critical(O) of(O) disco(B-music genre) included(O) Bob(B-musical artist) Seger(I-musical artist) '(O) s(O) Old(B-song) Time(I-song) Rock(I-song) and(I-song) Roll(I-song) and(O) ,(O) especially(O) ,(O) The(B-band) Who(I-band) '(O) s(O) Sister(B-song) Disco(I-song) ((O) both(O) 1978(O) )(O) -(O) although(O) The(B-band) Who(I-band) 's(O) Eminence(B-song) Front(I-song) ((O) four(O) years(O) later(O) )(O) had(O) a(O) disco(B-music genre) feel(O) .(O)"}}
{"id": "219", "dataset": "crossner_music", "split": "test", "label_list": ["album", "band", "music genre", "country", "song", "person", "event", "musical instrument", "organization", "location", "award", "musical artist"], "instance": {"id": "219", "words": ["On", "June", "11-12", ",", "2008", ",", "they", "played", "in", "Philadelphia", "and", "New", "Jersey", "at", "the", "Trocadero", "Theatre", "and", "Starland", "Ballroom", ",", "respectively", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, band, music genre, country, song, person, event, musical instrument, organization, location, award, musical artist and O.\nSentence: On June 11-12 , 2008 , they played in Philadelphia and New Jersey at the Trocadero Theatre and Starland Ballroom , respectively .", "prompt_labels": "On(O) June(O) 11-12(O) ,(O) 2008(O) ,(O) they(O) played(O) in(O) Philadelphia(B-location) and(O) New(B-location) Jersey(I-location) at(O) the(O) Trocadero(B-location) Theatre(I-location) and(O) Starland(B-location) Ballroom(I-location) ,(O) respectively(O) .(O)"}}
{"id": "74", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "person", "award", "organization", "band", "event", "song", "location", "album", "country", "musical artist", "music genre"], "instance": {"id": "74", "words": ["The", "territory", "band", "s", "operating", "out", "of", "Kansas", "City", ",", "the", "Bennie", "Moten", "orchestra", ",", "Jay", "McShann", ",", "and", "the", "Count", "Basie", "Orchestra", "were", "also", "concentrating", "on", "the", "blues", ",", "with", "12-bar", "blues", "instrumentals", "such", "as", "Basie", "'s", "One", "O", "'Clock", "Jump", "and", "Jumpin", "'", "at", "the", "Woodside", "and", "boisterous", "blues", "shouting", "by", "Jimmy", "Rushing", "on", "songs", "such", "as", "Going", "to", "Chicago", "and", "Sent", "for", "You", "Yesterday", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-band", "I-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "B-band", "I-band", "I-band", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "B-music genre", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, person, award, organization, band, event, song, location, album, country, musical artist, music genre and O.\nSentence: The territory band s operating out of Kansas City , the Bennie Moten orchestra , Jay McShann , and the Count Basie Orchestra were also concentrating on the blues , with 12-bar blues instrumentals such as Basie 's One O 'Clock Jump and Jumpin ' at the Woodside and boisterous blues shouting by Jimmy Rushing on songs such as Going to Chicago and Sent for You Yesterday .", "prompt_labels": "The(O) territory(O) band(O) s(O) operating(O) out(O) of(O) Kansas(B-location) City(I-location) ,(O) the(O) Bennie(B-band) Moten(I-band) orchestra(I-band) ,(O) Jay(B-musical artist) McShann(I-musical artist) ,(O) and(O) the(O) Count(B-band) Basie(I-band) Orchestra(I-band) were(O) also(O) concentrating(O) on(O) the(O) blues(B-music genre) ,(O) with(O) 12-bar(O) blues(O) instrumentals(O) such(O) as(O) Basie(B-musical artist) 's(O) One(B-song) O(I-song) 'Clock(I-song) Jump(I-song) and(O) Jumpin(B-song) '(I-song) at(I-song) the(I-song) Woodside(I-song) and(O) boisterous(O) blues(B-music genre) shouting(O) by(O) Jimmy(B-musical artist) Rushing(I-musical artist) on(O) songs(O) such(O) as(O) Going(B-song) to(I-song) Chicago(I-song) and(O) Sent(B-song) for(I-song) You(I-song) Yesterday(I-song) .(O)"}}
{"id": "158", "dataset": "crossner_music", "split": "test", "label_list": ["band", "person", "musical instrument", "event", "location", "organization", "music genre", "song", "musical artist", "album", "country", "award"], "instance": {"id": "158", "words": ["The", "four-piece", "Indie", "rock", "band", "played", "gigs", "at", "pubs", "and", "festivals", "from", "2005", "to", "2007", "such", "as", "Knitting", "Factory", ",", "Bamboozle", "Left", ",", "The", "Roxy", ",", "Spaceland", ",", "and", "The", "Viper", "Room", "."], "labels": ["O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, person, musical instrument, event, location, organization, music genre, song, musical artist, album, country, award and O.\nSentence: The four-piece Indie rock band played gigs at pubs and festivals from 2005 to 2007 such as Knitting Factory , Bamboozle Left , The Roxy , Spaceland , and The Viper Room .", "prompt_labels": "The(O) four-piece(O) Indie(B-music genre) rock(I-music genre) band(O) played(O) gigs(O) at(O) pubs(O) and(O) festivals(O) from(O) 2005(O) to(O) 2007(O) such(O) as(O) Knitting(B-location) Factory(I-location) ,(O) Bamboozle(B-location) Left(I-location) ,(O) The(B-location) Roxy(I-location) ,(O) Spaceland(B-location) ,(O) and(O) The(B-location) Viper(I-location) Room(I-location) .(O)"}}
{"id": "376", "dataset": "crossner_music", "split": "test", "label_list": ["location", "person", "award", "song", "event", "band", "music genre", "country", "album", "musical instrument", "organization", "musical artist"], "instance": {"id": "376", "words": ["A", "breakthrough", "came", "when", "Van", "Damm", "began", "to", "incorporate", "glamorous", "nude", "females", "on", "stage", ",", "inspired", "by", "the", "Folies", "Bergère", "and", "Moulin", "Rouge", "in", "Paris", "."], "labels": ["O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, award, song, event, band, music genre, country, album, musical instrument, organization, musical artist and O.\nSentence: A breakthrough came when Van Damm began to incorporate glamorous nude females on stage , inspired by the Folies Bergère and Moulin Rouge in Paris .", "prompt_labels": "A(O) breakthrough(O) came(O) when(O) Van(B-person) Damm(I-person) began(O) to(O) incorporate(O) glamorous(O) nude(O) females(O) on(O) stage(O) ,(O) inspired(O) by(O) the(O) Folies(B-location) Bergère(I-location) and(O) Moulin(B-location) Rouge(I-location) in(O) Paris(B-location) .(O)"}}
{"id": "446", "dataset": "crossner_music", "split": "test", "label_list": ["person", "band", "music genre", "award", "album", "event", "musical instrument", "song", "organization", "country", "musical artist", "location"], "instance": {"id": "446", "words": ["Sosa", "performed", "in", "venues", "such", "as", "the", "Lincoln", "Center", "in", "New", "York", "City", ",", "the", "Théâtre", "Mogador", "in", "Paris", "and", "the", "Sistine", "Chapel", "in", "Vatican", "City", ",", "as", "well", "as", "sell-out", "shows", "in", "New", "York", "'s", "Carnegie", "Hall", "and", "the", "Roman", "Colosseum", "during", "her", "final", "decade", "of", "life", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, band, music genre, award, album, event, musical instrument, song, organization, country, musical artist, location and O.\nSentence: Sosa performed in venues such as the Lincoln Center in New York City , the Théâtre Mogador in Paris and the Sistine Chapel in Vatican City , as well as sell-out shows in New York 's Carnegie Hall and the Roman Colosseum during her final decade of life .", "prompt_labels": "Sosa(B-musical artist) performed(O) in(O) venues(O) such(O) as(O) the(O) Lincoln(B-location) Center(I-location) in(O) New(B-location) York(I-location) City(I-location) ,(O) the(O) Théâtre(B-location) Mogador(I-location) in(O) Paris(B-location) and(O) the(O) Sistine(B-location) Chapel(I-location) in(O) Vatican(B-location) City(I-location) ,(O) as(O) well(O) as(O) sell-out(O) shows(O) in(O) New(B-location) York(I-location) 's(O) Carnegie(B-location) Hall(I-location) and(O) the(O) Roman(B-location) Colosseum(I-location) during(O) her(O) final(O) decade(O) of(O) life(O) .(O)"}}
{"id": "416", "dataset": "crossner_music", "split": "test", "label_list": ["person", "band", "event", "location", "musical instrument", "award", "country", "song", "organization", "music genre", "musical artist", "album"], "instance": {"id": "416", "words": ["Electropop", "is", "a", "music", "genre", "combining", "elements", "of", "Electronic", "music", "and", "Pop", "music", "genres", "."], "labels": ["B-music genre", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, band, event, location, musical instrument, award, country, song, organization, music genre, musical artist, album and O.\nSentence: Electropop is a music genre combining elements of Electronic music and Pop music genres .", "prompt_labels": "Electropop(B-music genre) is(O) a(O) music(O) genre(O) combining(O) elements(O) of(O) Electronic(B-music genre) music(I-music genre) and(O) Pop(B-music genre) music(I-music genre) genres(O) .(O)"}}
{"id": "375", "dataset": "crossner_music", "split": "test", "label_list": ["event", "award", "album", "location", "song", "music genre", "musical artist", "band", "musical instrument", "country", "organization", "person"], "instance": {"id": "375", "words": ["They", "have", "also", "won", "two", "Grammy", "Award", "s", "for", "Grammy", "Award", "for", "Best", "Metal", "Performance", ",", "and", "in", "2019", "the", "band", "were", "presented", "a", "Grammy", "Lifetime", "Achievement", "Award", "."], "labels": ["O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, album, location, song, music genre, musical artist, band, musical instrument, country, organization, person and O.\nSentence: They have also won two Grammy Award s for Grammy Award for Best Metal Performance , and in 2019 the band were presented a Grammy Lifetime Achievement Award .", "prompt_labels": "They(O) have(O) also(O) won(O) two(O) Grammy(B-award) Award(I-award) s(O) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Metal(I-award) Performance(I-award) ,(O) and(O) in(O) 2019(O) the(O) band(O) were(O) presented(O) a(O) Grammy(B-award) Lifetime(I-award) Achievement(I-award) Award(I-award) .(O)"}}
{"id": "395", "dataset": "crossner_music", "split": "test", "label_list": ["country", "person", "musical instrument", "organization", "band", "musical artist", "album", "song", "location", "event", "award", "music genre"], "instance": {"id": "395", "words": ["Deacon", "Blue", "also", "performed", "at", "the", "Glasgow", "2014", "Commonwealth", "Games", "closing", "ceremony", "on", "3", "August", "2014", ",", "performing", "their", "hit", ",", "Dignity", "."], "labels": ["B-band", "I-band", "O", "O", "O", "O", "B-location", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, musical instrument, organization, band, musical artist, album, song, location, event, award, music genre and O.\nSentence: Deacon Blue also performed at the Glasgow 2014 Commonwealth Games closing ceremony on 3 August 2014 , performing their hit , Dignity .", "prompt_labels": "Deacon(B-band) Blue(I-band) also(O) performed(O) at(O) the(O) Glasgow(B-location) 2014(B-event) Commonwealth(I-event) Games(I-event) closing(O) ceremony(O) on(O) 3(O) August(O) 2014(O) ,(O) performing(O) their(O) hit(O) ,(O) Dignity(B-song) .(O)"}}
{"id": "400", "dataset": "crossner_music", "split": "test", "label_list": ["band", "country", "musical instrument", "person", "music genre", "album", "musical artist", "event", "song", "award", "organization", "location"], "instance": {"id": "400", "words": ["The", "Locust", ",", "from", "San", "Diego", ",", "In", "Los", "Angeles", ",", "Hole", "also", "initially", "drew", "influence", "from", "grindcore", "in", "their", "early", "releases", ",", "particularly", "on", "their", "singles", "Dicknail", "and", "Teenage", "Whore", ",", "as", "well", "as", "on", "their", "debut", "album", ",", "Pretty", "on", "the", "Inside", "(", "1991", ")", ",", "{", "{", "cite", "AV", "media", "notes"], "labels": ["B-band", "I-band", "O", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "O", "B-band", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "O", "B-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, country, musical instrument, person, music genre, album, musical artist, event, song, award, organization, location and O.\nSentence: The Locust , from San Diego , In Los Angeles , Hole also initially drew influence from grindcore in their early releases , particularly on their singles Dicknail and Teenage Whore , as well as on their debut album , Pretty on the Inside ( 1991 ) , { { cite AV media notes", "prompt_labels": "The(B-band) Locust(I-band) ,(O) from(O) San(B-location) Diego(I-location) ,(O) In(O) Los(B-location) Angeles(I-location) ,(O) Hole(B-band) also(O) initially(O) drew(O) influence(O) from(O) grindcore(B-music genre) in(O) their(O) early(O) releases(O) ,(O) particularly(O) on(O) their(O) singles(O) Dicknail(B-song) and(O) Teenage(B-song) Whore(I-song) ,(O) as(O) well(O) as(O) on(O) their(O) debut(O) album(O) ,(O) Pretty(B-album) on(I-album) the(I-album) Inside(I-album) ((O) 1991(O) )(O) ,(O) {(O) {(O) cite(O) AV(O) media(O) notes(O)"}}
{"id": "131", "dataset": "crossner_music", "split": "test", "label_list": ["award", "music genre", "band", "album", "musical instrument", "person", "country", "event", "song", "location", "organization", "musical artist"], "instance": {"id": "131", "words": ["Artists", "who", "typified", "this", "sound", "included", "Travis", "Tritt", ",", "Reba", "McEntire", ",", "George", "Strait", ",", "Keith", "Whitley", ",", "Alan", "Jackson", ",", "Ricky", "Skaggs", ",", "Patty", "Loveless", ",", "Kathy", "Mattea", ",", "Randy", "Travis", ",", "Dwight", "Yoakam", ",", "and", "The", "Judds", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, music genre, band, album, musical instrument, person, country, event, song, location, organization, musical artist and O.\nSentence: Artists who typified this sound included Travis Tritt , Reba McEntire , George Strait , Keith Whitley , Alan Jackson , Ricky Skaggs , Patty Loveless , Kathy Mattea , Randy Travis , Dwight Yoakam , and The Judds .", "prompt_labels": "Artists(O) who(O) typified(O) this(O) sound(O) included(O) Travis(B-musical artist) Tritt(I-musical artist) ,(O) Reba(B-musical artist) McEntire(I-musical artist) ,(O) George(B-musical artist) Strait(I-musical artist) ,(O) Keith(B-musical artist) Whitley(I-musical artist) ,(O) Alan(B-musical artist) Jackson(I-musical artist) ,(O) Ricky(B-musical artist) Skaggs(I-musical artist) ,(O) Patty(B-musical artist) Loveless(I-musical artist) ,(O) Kathy(B-musical artist) Mattea(I-musical artist) ,(O) Randy(B-musical artist) Travis(I-musical artist) ,(O) Dwight(B-musical artist) Yoakam(I-musical artist) ,(O) and(O) The(B-band) Judds(I-band) .(O)"}}
{"id": "10", "dataset": "crossner_music", "split": "test", "label_list": ["person", "band", "event", "location", "award", "song", "musical artist", "country", "music genre", "album", "organization", "musical instrument"], "instance": {"id": "10", "words": ["With", "Fredriksson", "'s", "continued", "treatment", "and", "recuperation", "he", "made", "three", "more", "albums", ",", "Son", "of", "a", "Plumber", "(", "2005", ")", ",", "En", "händig", "man", "(", "A", "handy", "man", ",", "2007", ")", "and", "Party", "Crasher", "(", "2008", ")", "."], "labels": ["O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, band, event, location, award, song, musical artist, country, music genre, album, organization, musical instrument and O.\nSentence: With Fredriksson 's continued treatment and recuperation he made three more albums , Son of a Plumber ( 2005 ) , En händig man ( A handy man , 2007 ) and Party Crasher ( 2008 ) .", "prompt_labels": "With(O) Fredriksson(B-musical artist) 's(O) continued(O) treatment(O) and(O) recuperation(O) he(O) made(O) three(O) more(O) albums(O) ,(O) Son(B-album) of(I-album) a(I-album) Plumber(I-album) ((O) 2005(O) )(O) ,(O) En(B-album) händig(I-album) man(I-album) ((O) A(B-album) handy(I-album) man(I-album) ,(O) 2007(O) )(O) and(O) Party(B-album) Crasher(I-album) ((O) 2008(O) )(O) .(O)"}}
{"id": "463", "dataset": "crossner_music", "split": "test", "label_list": ["band", "musical instrument", "musical artist", "location", "music genre", "organization", "person", "event", "song", "award", "country", "album"], "instance": {"id": "463", "words": ["Today", "there", "are", "Celtic-influenced", "subgenres", "of", "virtually", "every", "type", "of", "popular", "music", "including", "electronica", ",", "Celtic", "rock", ",", "Celtic", "metal", ",", "Celtic", "punk", ",", "Hip", "hop", "music", ",", "reggae", ",", "New-age", "music", ",", "Latin", ",", "Andean", "and", "Pop", "music", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical instrument, musical artist, location, music genre, organization, person, event, song, award, country, album and O.\nSentence: Today there are Celtic-influenced subgenres of virtually every type of popular music including electronica , Celtic rock , Celtic metal , Celtic punk , Hip hop music , reggae , New-age music , Latin , Andean and Pop music .", "prompt_labels": "Today(O) there(O) are(O) Celtic-influenced(O) subgenres(O) of(O) virtually(O) every(O) type(O) of(O) popular(B-music genre) music(I-music genre) including(O) electronica(B-music genre) ,(O) Celtic(B-music genre) rock(I-music genre) ,(O) Celtic(B-music genre) metal(I-music genre) ,(O) Celtic(B-music genre) punk(I-music genre) ,(O) Hip(B-music genre) hop(I-music genre) music(I-music genre) ,(O) reggae(B-music genre) ,(O) New-age(B-music genre) music(I-music genre) ,(O) Latin(B-music genre) ,(O) Andean(B-music genre) and(O) Pop(B-music genre) music(I-music genre) .(O)"}}
{"id": "309", "dataset": "crossner_music", "split": "test", "label_list": ["award", "location", "person", "album", "country", "music genre", "song", "musical instrument", "musical artist", "event", "band", "organization"], "instance": {"id": "309", "words": ["This", "was", "reflected", "in", "a", "series", "of", "albums", "released", "by", "Island", "Records", ",", "including", "Swordfishtrombones", "(", "1983", ")", ",", "Rain", "Dogs", "(", "1985", ")", ",", "and", "Franks", "Wild", "Years", "(", "1987", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, person, album, country, music genre, song, musical instrument, musical artist, event, band, organization and O.\nSentence: This was reflected in a series of albums released by Island Records , including Swordfishtrombones ( 1983 ) , Rain Dogs ( 1985 ) , and Franks Wild Years ( 1987 ) .", "prompt_labels": "This(O) was(O) reflected(O) in(O) a(O) series(O) of(O) albums(O) released(O) by(O) Island(B-organization) Records(I-organization) ,(O) including(O) Swordfishtrombones(B-album) ((O) 1983(O) )(O) ,(O) Rain(B-album) Dogs(I-album) ((O) 1985(O) )(O) ,(O) and(O) Franks(B-album) Wild(I-album) Years(I-album) ((O) 1987(O) )(O) .(O)"}}
{"id": "106", "dataset": "crossner_music", "split": "test", "label_list": ["location", "country", "album", "band", "musical artist", "award", "music genre", "person", "song", "musical instrument", "event", "organization"], "instance": {"id": "106", "words": ["McTell", "'s", "influence", "extended", "over", "a", "wide", "variety", "of", "artists", ",", "including", "the", "The", "Allman", "Brothers", "Band", ",", "who", "covered", "his", "Statesboro", "Blues", ",", "and", "Bob", "Dylan", ",", "who", "paid", "tribute", "to", "him", "in", "his", "1983", "song", "Blind", "Willie", "McTell", ",", "the", "refrain", "of", "which", "is", "And", "I", "know", "no", "one", "can", "sing", "the", "blues", "like", "Blind", "Willie", "McTell", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "O", "O", "O", "O", "B-song", "I-song", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, album, band, musical artist, award, music genre, person, song, musical instrument, event, organization and O.\nSentence: McTell 's influence extended over a wide variety of artists , including the The Allman Brothers Band , who covered his Statesboro Blues , and Bob Dylan , who paid tribute to him in his 1983 song Blind Willie McTell , the refrain of which is And I know no one can sing the blues like Blind Willie McTell .", "prompt_labels": "McTell(B-musical artist) 's(O) influence(O) extended(O) over(O) a(O) wide(O) variety(O) of(O) artists(O) ,(O) including(O) the(O) The(B-band) Allman(I-band) Brothers(I-band) Band(I-band) ,(O) who(O) covered(O) his(O) Statesboro(B-song) Blues(I-song) ,(O) and(O) Bob(B-person) Dylan(I-person) ,(O) who(O) paid(O) tribute(O) to(O) him(O) in(O) his(O) 1983(O) song(O) Blind(B-musical artist) Willie(I-musical artist) McTell(I-musical artist) ,(O) the(O) refrain(O) of(O) which(O) is(O) And(O) I(O) know(O) no(O) one(O) can(O) sing(O) the(O) blues(O) like(O) Blind(B-musical artist) Willie(I-musical artist) McTell(I-musical artist) .(O)"}}
{"id": "213", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "song", "country", "award", "event", "music genre", "location", "organization", "musical instrument", "album", "person", "band"], "instance": {"id": "213", "words": ["Howlin", "'", "Wolf", ",", "Muddy", "Waters", ",", "Willie", "Dixon", "and", "Jimmy", "Reed", "were", "all", "born", "in", "Mississippi", "and", "moved", "to", "Chicago", "during", "the", "Great", "Migration", "."], "labels": ["B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "B-location", "O", "O", "O", "B-location", "O", "O", "B-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, song, country, award, event, music genre, location, organization, musical instrument, album, person, band and O.\nSentence: Howlin ' Wolf , Muddy Waters , Willie Dixon and Jimmy Reed were all born in Mississippi and moved to Chicago during the Great Migration .", "prompt_labels": "Howlin(B-musical artist) '(I-musical artist) Wolf(I-musical artist) ,(O) Muddy(B-musical artist) Waters(I-musical artist) ,(O) Willie(B-musical artist) Dixon(I-musical artist) and(O) Jimmy(B-musical artist) Reed(I-musical artist) were(O) all(O) born(O) in(O) Mississippi(B-location) and(O) moved(O) to(O) Chicago(B-location) during(O) the(O) Great(B-event) Migration(I-event) .(O)"}}
{"id": "169", "dataset": "crossner_music", "split": "test", "label_list": ["location", "musical instrument", "musical artist", "person", "organization", "award", "band", "song", "album", "music genre", "country", "event"], "instance": {"id": "169", "words": ["Rohani", "was", "commissioned", "in", "1998", "by", "the", "government", "of", "Thailand", "and", "the", "committee", "of", "the", "1998", "Asian", "Games", "to", "compose", "and", "conduct", "the", "music", "for", "opening", "ceremonies", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical instrument, musical artist, person, organization, award, band, song, album, music genre, country, event and O.\nSentence: Rohani was commissioned in 1998 by the government of Thailand and the committee of the 1998 Asian Games to compose and conduct the music for opening ceremonies .", "prompt_labels": "Rohani(B-person) was(O) commissioned(O) in(O) 1998(O) by(O) the(O) government(O) of(O) Thailand(B-country) and(O) the(O) committee(O) of(O) the(O) 1998(B-event) Asian(I-event) Games(I-event) to(O) compose(O) and(O) conduct(O) the(O) music(O) for(O) opening(O) ceremonies(O) .(O)"}}
{"id": "265", "dataset": "crossner_music", "split": "test", "label_list": ["award", "event", "music genre", "song", "organization", "person", "album", "musical instrument", "musical artist", "location", "country", "band"], "instance": {"id": "265", "words": ["The", "set", "includes", "the", "first", "round", "of", "the", "remastered", "series", "plus", "the", "long-awaited", "remastered", "versions", "of", "On", "Your", "Feet", "or", "on", "Your", "Knees", "(", "1975", ")", ",", "Mirrors", ",", "Cultösaurus", "Erectus", ",", "Fire", "Of", "Unknown", "Origin", ",", "Extraterrestrial", "Live", ",", "The", "Revölution", "by", "Night", ",", "Club", "Ninja", "and", "Imaginos", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "O", "B-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "O", "B-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, music genre, song, organization, person, album, musical instrument, musical artist, location, country, band and O.\nSentence: The set includes the first round of the remastered series plus the long-awaited remastered versions of On Your Feet or on Your Knees ( 1975 ) , Mirrors , Cultösaurus Erectus , Fire Of Unknown Origin , Extraterrestrial Live , The Revölution by Night , Club Ninja and Imaginos .", "prompt_labels": "The(O) set(O) includes(O) the(O) first(O) round(O) of(O) the(O) remastered(O) series(O) plus(O) the(O) long-awaited(O) remastered(O) versions(O) of(O) On(B-album) Your(I-album) Feet(I-album) or(I-album) on(I-album) Your(I-album) Knees(I-album) ((O) 1975(O) )(O) ,(O) Mirrors(B-album) ,(O) Cultösaurus(B-album) Erectus(I-album) ,(O) Fire(B-album) Of(I-album) Unknown(I-album) Origin(I-album) ,(O) Extraterrestrial(B-album) Live(I-album) ,(O) The(B-album) Revölution(I-album) by(I-album) Night(I-album) ,(O) Club(B-album) Ninja(I-album) and(O) Imaginos(B-album) .(O)"}}
{"id": "162", "dataset": "crossner_music", "split": "test", "label_list": ["musical instrument", "country", "album", "person", "music genre", "band", "musical artist", "location", "song", "event", "organization", "award"], "instance": {"id": "162", "words": ["The", "Nordic", "Choir", "has", "also", "appeared", "throughout", "the", "United", "States", ",", "performing", "in", "well-known", "concert", "halls", "as", "Lincoln", "Center", "in", "New", "York", "and", "the", "John", "F.", "Kennedy", "Center", "for", "the", "Performing", "Arts", "in", "Washington", ",", "D.C.", "Additionally", ",", "Luther", "College", "has", "the", "largest", "collegiate", "choral", "program", "in", "the", "United", "States", "with", "almost", "600", "student", "singers", "."], "labels": ["O", "B-band", "I-band", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "I-location", "I-location", "I-location", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, country, album, person, music genre, band, musical artist, location, song, event, organization, award and O.\nSentence: The Nordic Choir has also appeared throughout the United States , performing in well-known concert halls as Lincoln Center in New York and the John F. Kennedy Center for the Performing Arts in Washington , D.C. Additionally , Luther College has the largest collegiate choral program in the United States with almost 600 student singers .", "prompt_labels": "The(O) Nordic(B-band) Choir(I-band) has(O) also(O) appeared(O) throughout(O) the(O) United(B-country) States(I-country) ,(O) performing(O) in(O) well-known(O) concert(O) halls(O) as(O) Lincoln(B-location) Center(I-location) in(O) New(B-location) York(I-location) and(O) the(O) John(B-location) F.(I-location) Kennedy(I-location) Center(I-location) for(I-location) the(I-location) Performing(I-location) Arts(I-location) in(O) Washington(B-location) ,(I-location) D.C.(I-location) Additionally(O) ,(O) Luther(B-location) College(I-location) has(O) the(O) largest(O) collegiate(O) choral(O) program(O) in(O) the(O) United(B-country) States(I-country) with(O) almost(O) 600(O) student(O) singers(O) .(O)"}}
{"id": "174", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "organization", "album", "band", "musical instrument", "song", "location", "person", "award", "event", "country", "musical artist"], "instance": {"id": "174", "words": ["Hee", "Haw", "featured", "a", "premiere", "showcase", "on", "commercial", "television", "throughout", "its", "run", "for", "Country", "music", ",", "Bluegrass", "music", ",", "Gospel", "music", ",", "and", "other", "styles", "of", "American", "traditional", "music", ",", "featuring", "hundreds", "of", "elite", "musical", "performances", "that", "were", "paramount", "to", "the", "success", ",", "popularity", "and", "legacy", "of", "the", "series", "for", "a", "broad", "audience", "of", "Southern", ",", "rural", "and", "purely", "music", "fans", "alike", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, organization, album, band, musical instrument, song, location, person, award, event, country, musical artist and O.\nSentence: Hee Haw featured a premiere showcase on commercial television throughout its run for Country music , Bluegrass music , Gospel music , and other styles of American traditional music , featuring hundreds of elite musical performances that were paramount to the success , popularity and legacy of the series for a broad audience of Southern , rural and purely music fans alike .", "prompt_labels": "Hee(O) Haw(O) featured(O) a(O) premiere(O) showcase(O) on(O) commercial(O) television(O) throughout(O) its(O) run(O) for(O) Country(B-music genre) music(I-music genre) ,(O) Bluegrass(B-music genre) music(I-music genre) ,(O) Gospel(B-music genre) music(I-music genre) ,(O) and(O) other(O) styles(O) of(O) American(B-music genre) traditional(I-music genre) music(I-music genre) ,(O) featuring(O) hundreds(O) of(O) elite(O) musical(O) performances(O) that(O) were(O) paramount(O) to(O) the(O) success(O) ,(O) popularity(O) and(O) legacy(O) of(O) the(O) series(O) for(O) a(O) broad(O) audience(O) of(O) Southern(B-music genre) ,(O) rural(B-music genre) and(O) purely(B-music genre) music(I-music genre) fans(O) alike(O) .(O)"}}
{"id": "275", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "country", "musical artist", "location", "band", "song", "person", "music genre", "event", "award", "musical instrument", "album"], "instance": {"id": "275", "words": ["The", "British", "and", "blues", "musicians", "of", "the", "early", "1960s", "inspired", "a", "number", "of", "American", "blues", "rock", "fusion", "performers", ",", "including", "the", "The", "Doors", ",", "Canned", "Heat", ",", "the", "early", "Jefferson", "Airplane", ",", "Janis", "Joplin", ",", "Johnny", "Winter", ",", "The", "J.", "Geils", "Band", ",", "Ry", "Cooder", ",", "and", "the", "Allman", "Brothers", "Band", "."], "labels": ["O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "O", "O", "B-band", "I-band", "O", "B-person", "I-person", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "B-band", "I-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, musical artist, location, band, song, person, music genre, event, award, musical instrument, album and O.\nSentence: The British and blues musicians of the early 1960s inspired a number of American blues rock fusion performers , including the The Doors , Canned Heat , the early Jefferson Airplane , Janis Joplin , Johnny Winter , The J. Geils Band , Ry Cooder , and the Allman Brothers Band .", "prompt_labels": "The(O) British(O) and(O) blues(B-music genre) musicians(O) of(O) the(O) early(O) 1960s(O) inspired(O) a(O) number(O) of(O) American(O) blues(B-music genre) rock(I-music genre) fusion(I-music genre) performers(O) ,(O) including(O) the(O) The(B-band) Doors(I-band) ,(O) Canned(B-band) Heat(I-band) ,(O) the(O) early(O) Jefferson(B-band) Airplane(I-band) ,(O) Janis(B-person) Joplin(I-person) ,(O) Johnny(B-musical artist) Winter(I-musical artist) ,(O) The(B-band) J.(I-band) Geils(I-band) Band(I-band) ,(O) Ry(B-musical artist) Cooder(I-musical artist) ,(O) and(O) the(O) Allman(B-band) Brothers(I-band) Band(I-band) .(O)"}}
{"id": "304", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "location", "album", "song", "organization", "person", "band", "award", "musical instrument", "music genre", "country", "event"], "instance": {"id": "304", "words": ["In", "2016", "Chuck", "D", "joined", "the", "band", "Prophets", "of", "Rage", "along", "with", "B-Real", "and", "former", "members", "of", "Rage", "Against", "the", "Machine", "."], "labels": ["O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "B-band", "I-band", "I-band", "O", "O", "B-musical artist", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, album, song, organization, person, band, award, musical instrument, music genre, country, event and O.\nSentence: In 2016 Chuck D joined the band Prophets of Rage along with B-Real and former members of Rage Against the Machine .", "prompt_labels": "In(O) 2016(O) Chuck(B-musical artist) D(I-musical artist) joined(O) the(O) band(O) Prophets(B-band) of(I-band) Rage(I-band) along(O) with(O) B-Real(B-musical artist) and(O) former(O) members(O) of(O) Rage(B-band) Against(I-band) the(I-band) Machine(I-band) .(O)"}}
{"id": "422", "dataset": "crossner_music", "split": "test", "label_list": ["country", "organization", "musical instrument", "event", "person", "song", "music genre", "location", "musical artist", "award", "album", "band"], "instance": {"id": "422", "words": ["The", "groups", "released", "albums", "such", "as", "Maggot", "Brain", "(", "1971", ")", ",", "Mothership", "Connection", "(", "1975", ")", ",", "and", "One", "Nation", "Under", "a", "Groove", "(", "1978", ")", "to", "critical", "praise", ",", "and", "scored", "charting", "hits", "with", "singles", "such", "as", "Give", "Up", "the", "Funk", "(", "1976", ")", ",", "One", "Nation", "Under", "a", "Groove", "(", "1978", ")", ",", "and", "Flash", "Light", "(", "1978", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "B-song", "I-song", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, musical instrument, event, person, song, music genre, location, musical artist, award, album, band and O.\nSentence: The groups released albums such as Maggot Brain ( 1971 ) , Mothership Connection ( 1975 ) , and One Nation Under a Groove ( 1978 ) to critical praise , and scored charting hits with singles such as Give Up the Funk ( 1976 ) , One Nation Under a Groove ( 1978 ) , and Flash Light ( 1978 ) .", "prompt_labels": "The(O) groups(O) released(O) albums(O) such(O) as(O) Maggot(B-album) Brain(I-album) ((O) 1971(O) )(O) ,(O) Mothership(B-album) Connection(I-album) ((O) 1975(O) )(O) ,(O) and(O) One(B-album) Nation(I-album) Under(I-album) a(I-album) Groove(I-album) ((O) 1978(O) )(O) to(O) critical(O) praise(O) ,(O) and(O) scored(O) charting(O) hits(O) with(O) singles(O) such(O) as(O) Give(B-song) Up(I-song) the(I-song) Funk(I-song) ((O) 1976(O) )(O) ,(O) One(B-song) Nation(I-song) Under(I-song) a(I-song) Groove(I-song) ((O) 1978(O) )(O) ,(O) and(O) Flash(B-song) Light(I-song) ((O) 1978(O) )(O) .(O)"}}
{"id": "327", "dataset": "crossner_music", "split": "test", "label_list": ["event", "musical instrument", "award", "location", "organization", "album", "song", "country", "musical artist", "band", "music genre", "person"], "instance": {"id": "327", "words": ["The", "World", "DanceSport", "Federation", "(", "WDSF", ")", ",", "formerly", "the", "International", "DanceSport", "Federation", "(", "IDSF", ")", ",", "is", "the", "international", "governing", "body", "of", "DanceSport", "and", "Wheelchair", "DanceSport", ",", "as", "recognised", "by", "the", "International", "Olympic", "Committee", "(", "IOC", ")", "and", "the", "International", "Paralympic", "Committee", "(", "IPC", ")", "."], "labels": ["O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical instrument, award, location, organization, album, song, country, musical artist, band, music genre, person and O.\nSentence: The World DanceSport Federation ( WDSF ) , formerly the International DanceSport Federation ( IDSF ) , is the international governing body of DanceSport and Wheelchair DanceSport , as recognised by the International Olympic Committee ( IOC ) and the International Paralympic Committee ( IPC ) .", "prompt_labels": "The(O) World(B-organization) DanceSport(I-organization) Federation(I-organization) ((O) WDSF(B-organization) )(O) ,(O) formerly(O) the(O) International(B-organization) DanceSport(I-organization) Federation(I-organization) ((O) IDSF(B-organization) )(O) ,(O) is(O) the(O) international(O) governing(O) body(O) of(O) DanceSport(O) and(O) Wheelchair(O) DanceSport(O) ,(O) as(O) recognised(O) by(O) the(O) International(B-organization) Olympic(I-organization) Committee(I-organization) ((O) IOC(B-organization) )(O) and(O) the(O) International(B-organization) Paralympic(I-organization) Committee(I-organization) ((O) IPC(B-organization) )(O) .(O)"}}
{"id": "9", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "music genre", "location", "award", "country", "song", "musical artist", "album", "person", "event", "musical instrument", "band"], "instance": {"id": "9", "words": ["Following", "this", "success", ",", "Francis", "recorded", "seven", "more", "albums", "of", "favorites", "between", "1960", "and", "1964", ",", "including", "Connie", "Francis", "Sings", "Jewish", "Favorites", ",", "Connie", "Francis", "Sings", "German", "Favorites", ",", "and", "Connie", "Francis", "Sings", "Irish", "Favorites", ",", "among", "others", "."], "labels": ["O", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, music genre, location, award, country, song, musical artist, album, person, event, musical instrument, band and O.\nSentence: Following this success , Francis recorded seven more albums of favorites between 1960 and 1964 , including Connie Francis Sings Jewish Favorites , Connie Francis Sings German Favorites , and Connie Francis Sings Irish Favorites , among others .", "prompt_labels": "Following(O) this(O) success(O) ,(O) Francis(B-musical artist) recorded(O) seven(O) more(O) albums(O) of(O) favorites(O) between(O) 1960(O) and(O) 1964(O) ,(O) including(O) Connie(B-album) Francis(I-album) Sings(I-album) Jewish(I-album) Favorites(I-album) ,(O) Connie(B-album) Francis(I-album) Sings(I-album) German(I-album) Favorites(I-album) ,(O) and(O) Connie(B-album) Francis(I-album) Sings(I-album) Irish(I-album) Favorites(I-album) ,(O) among(O) others(O) .(O)"}}
{"id": "289", "dataset": "crossner_music", "split": "test", "label_list": ["album", "musical instrument", "event", "musical artist", "band", "award", "organization", "location", "person", "music genre", "country", "song"], "instance": {"id": "289", "words": ["It", "won", "two", "Academy", "Awards", ",", "Academy", "Award", "for", "Best", "Original", "Score", "and", "Academy", "Award", "for", "Best", "Original", "Song", "for", "A", "Whole", "New", "World", "and", "receiving", "nominations", "for", "Best", "Song", "(", "Friend", "Like", "Me", ")", ",", "Academy", "Award", "for", "Best", "Sound", "Editing", "(", "Mark", "A.", "Mangini", ")", ",", "and", "Academy", "Award", "for", "Best", "Sound", "Mixing", "(", "Terry", "Porter", ",", "Mel", "Metcalfe", ",", "David", "J.", "Hudson", "and", "Doc", "Kane", ")", "."], "labels": ["O", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "B-award", "I-award", "O", "B-song", "I-song", "I-song", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical instrument, event, musical artist, band, award, organization, location, person, music genre, country, song and O.\nSentence: It won two Academy Awards , Academy Award for Best Original Score and Academy Award for Best Original Song for A Whole New World and receiving nominations for Best Song ( Friend Like Me ) , Academy Award for Best Sound Editing ( Mark A. Mangini ) , and Academy Award for Best Sound Mixing ( Terry Porter , Mel Metcalfe , David J. Hudson and Doc Kane ) .", "prompt_labels": "It(O) won(O) two(O) Academy(B-award) Awards(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Song(I-award) for(O) A(B-song) Whole(I-song) New(I-song) World(I-song) and(O) receiving(O) nominations(O) for(O) Best(B-award) Song(I-award) ((O) Friend(B-song) Like(I-song) Me(I-song) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Sound(I-award) Editing(I-award) ((O) Mark(B-musical artist) A.(I-musical artist) Mangini(I-musical artist) )(O) ,(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Sound(I-award) Mixing(I-award) ((O) Terry(B-musical artist) Porter(I-musical artist) ,(O) Mel(B-musical artist) Metcalfe(I-musical artist) ,(O) David(B-musical artist) J.(I-musical artist) Hudson(I-musical artist) and(O) Doc(B-musical artist) Kane(I-musical artist) )(O) .(O)"}}
{"id": "89", "dataset": "crossner_music", "split": "test", "label_list": ["country", "music genre", "song", "award", "location", "organization", "person", "musical artist", "album", "band", "event", "musical instrument"], "instance": {"id": "89", "words": ["Just", "like", "in", "Christian", "rock", "and", "other", "Christian", "music", "genres", ",", "some", "artists", "welcome", "being", "called", "Christian", "artists", "while", "others", "do", "not", "want", "to", "be", "labeled", "as", "Christian", "music", ",", "as", "to", "not", "limit", "their", "music", "to", "the", "Christian", "music", "market", "."], "labels": ["O", "O", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, music genre, song, award, location, organization, person, musical artist, album, band, event, musical instrument and O.\nSentence: Just like in Christian rock and other Christian music genres , some artists welcome being called Christian artists while others do not want to be labeled as Christian music , as to not limit their music to the Christian music market .", "prompt_labels": "Just(O) like(O) in(O) Christian(B-music genre) rock(I-music genre) and(O) other(O) Christian(B-music genre) music(I-music genre) genres(O) ,(O) some(O) artists(O) welcome(O) being(O) called(O) Christian(O) artists(O) while(O) others(O) do(O) not(O) want(O) to(O) be(O) labeled(O) as(O) Christian(B-music genre) music(I-music genre) ,(O) as(O) to(O) not(O) limit(O) their(O) music(O) to(O) the(O) Christian(B-music genre) music(I-music genre) market(O) .(O)"}}
{"id": "12", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "person", "musical instrument", "organization", "band", "event", "musical artist", "award", "song", "album", "country", "location"], "instance": {"id": "12", "words": ["Annie", "Hall", "(", "1977", ")", "won", "four", "Academy", "Awards", ",", "including", "Academy", "Award", "for", "Best", "Picture", ",", "Academy", "Award", "for", "Best", "Actress", "for", "Diane", "Keaton", ",", "Academy", "Award", "for", "Best", "Original", "Screenplay", "and", "Academy", "Award", "for", "Best", "Director", "for", "Woody", "Allen", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, person, musical instrument, organization, band, event, musical artist, award, song, album, country, location and O.\nSentence: Annie Hall ( 1977 ) won four Academy Awards , including Academy Award for Best Picture , Academy Award for Best Actress for Diane Keaton , Academy Award for Best Original Screenplay and Academy Award for Best Director for Woody Allen .", "prompt_labels": "Annie(O) Hall(O) ((O) 1977(O) )(O) won(O) four(O) Academy(B-award) Awards(I-award) ,(O) including(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) for(O) Diane(B-person) Keaton(I-person) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Screenplay(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) for(O) Woody(B-person) Allen(I-person) .(O)"}}
{"id": "184", "dataset": "crossner_music", "split": "test", "label_list": ["person", "event", "song", "organization", "band", "country", "musical instrument", "award", "music genre", "musical artist", "location", "album"], "instance": {"id": "184", "words": ["Organised", "by", "the", "European", "Broadcasting", "Union", "(", "EBU", ")", "and", "host", "broadcaster", "Latvijas", "Televīzija", "(", "LTV", ")", ",", "the", "contest", "was", "held", "at", "the", "Skonto", "Hall", ",", "with", "the", "final", "on", "24", "May", "2003", "."], "labels": ["O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, song, organization, band, country, musical instrument, award, music genre, musical artist, location, album and O.\nSentence: Organised by the European Broadcasting Union ( EBU ) and host broadcaster Latvijas Televīzija ( LTV ) , the contest was held at the Skonto Hall , with the final on 24 May 2003 .", "prompt_labels": "Organised(O) by(O) the(O) European(B-organization) Broadcasting(I-organization) Union(I-organization) ((O) EBU(B-organization) )(O) and(O) host(O) broadcaster(O) Latvijas(B-organization) Televīzija(I-organization) ((O) LTV(B-organization) )(O) ,(O) the(O) contest(O) was(O) held(O) at(O) the(O) Skonto(B-location) Hall(I-location) ,(O) with(O) the(O) final(O) on(O) 24(O) May(O) 2003(O) .(O)"}}
{"id": "295", "dataset": "crossner_music", "split": "test", "label_list": ["song", "award", "band", "event", "country", "album", "organization", "person", "music genre", "location", "musical artist", "musical instrument"], "instance": {"id": "295", "words": ["The", "subsequent", "albums", "The", "Clones", "of", "Dr.", "Funkenstein", "(", "1976", ")", ",", "Funkentelechy", "vs.", "the", "Placebo", "Syndrome", "(", "1977", ")", ",", "and", "Motor", "Booty", "Affair", "(", "1978", ")", "all", "reached", "high", "on", "both", "the", "R", "&", "amp", ";", "B", "and", "Pop", "charts", ",", "while", "Funkadelic", "was", "also", "experiencing", "significant", "mainstream", "success", "."], "labels": ["O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "O", "O", "O", "B-band", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, award, band, event, country, album, organization, person, music genre, location, musical artist, musical instrument and O.\nSentence: The subsequent albums The Clones of Dr. Funkenstein ( 1976 ) , Funkentelechy vs. the Placebo Syndrome ( 1977 ) , and Motor Booty Affair ( 1978 ) all reached high on both the R & amp ; B and Pop charts , while Funkadelic was also experiencing significant mainstream success .", "prompt_labels": "The(O) subsequent(O) albums(O) The(B-album) Clones(I-album) of(I-album) Dr.(I-album) Funkenstein(I-album) ((O) 1976(O) )(O) ,(O) Funkentelechy(B-album) vs.(I-album) the(I-album) Placebo(I-album) Syndrome(I-album) ((O) 1977(O) )(O) ,(O) and(O) Motor(B-album) Booty(I-album) Affair(I-album) ((O) 1978(O) )(O) all(O) reached(O) high(O) on(O) both(O) the(O) R(B-music genre) &(I-music genre) amp(I-music genre) ;(I-music genre) B(I-music genre) and(O) Pop(B-music genre) charts(O) ,(O) while(O) Funkadelic(B-band) was(O) also(O) experiencing(O) significant(O) mainstream(O) success(O) .(O)"}}
{"id": "256", "dataset": "crossner_music", "split": "test", "label_list": ["event", "country", "band", "musical instrument", "music genre", "song", "organization", "album", "location", "musical artist", "award", "person"], "instance": {"id": "256", "words": ["During", "the", "course", "of", "her", "career", ",", "Saariaho", "has", "received", "commissions", "from", "the", "Lincoln", "Center", "for", "the", "Kronos", "Quartet", "and", "from", "IRCAM", "for", "the", "Ensemble", "Intercontemporain", ",", "the", "BBC", ",", "the", "New", "York", "Philharmonic", ",", "the", "Salzburg", "Music", "Festival", ",", "the", "Théâtre", "du", "Châtelet", "in", "Paris", ",", "and", "the", "Finnish", "National", "Opera", ",", "among", "others", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-band", "I-band", "O", "O", "B-organization", "O", "O", "B-band", "I-band", "O", "O", "B-organization", "O", "O", "B-band", "I-band", "I-band", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, band, musical instrument, music genre, song, organization, album, location, musical artist, award, person and O.\nSentence: During the course of her career , Saariaho has received commissions from the Lincoln Center for the Kronos Quartet and from IRCAM for the Ensemble Intercontemporain , the BBC , the New York Philharmonic , the Salzburg Music Festival , the Théâtre du Châtelet in Paris , and the Finnish National Opera , among others .", "prompt_labels": "During(O) the(O) course(O) of(O) her(O) career(O) ,(O) Saariaho(B-musical artist) has(O) received(O) commissions(O) from(O) the(O) Lincoln(B-organization) Center(I-organization) for(O) the(O) Kronos(B-band) Quartet(I-band) and(O) from(O) IRCAM(B-organization) for(O) the(O) Ensemble(B-band) Intercontemporain(I-band) ,(O) the(O) BBC(B-organization) ,(O) the(O) New(B-band) York(I-band) Philharmonic(I-band) ,(O) the(O) Salzburg(B-event) Music(I-event) Festival(I-event) ,(O) the(O) Théâtre(B-location) du(I-location) Châtelet(I-location) in(O) Paris(B-location) ,(O) and(O) the(O) Finnish(B-location) National(I-location) Opera(I-location) ,(O) among(O) others(O) .(O)"}}
{"id": "264", "dataset": "crossner_music", "split": "test", "label_list": ["album", "location", "organization", "musical instrument", "country", "band", "musical artist", "event", "person", "music genre", "song", "award"], "instance": {"id": "264", "words": ["Within", "months", ",", "Hendrix", "earned", "three", "UK", "top", "ten", "hits", "with", "the", "Jimi", "Hendrix", "Experience", ":", "Hey", "Joe", ",", "Purple", "Haze", ",", "and", "The", "Wind", "Cries", "Mary", "."], "labels": ["O", "O", "O", "B-musical artist", "O", "O", "B-country", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-song", "I-song", "O", "B-song", "I-song", "O", "O", "B-song", "I-song", "I-song", "I-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, location, organization, musical instrument, country, band, musical artist, event, person, music genre, song, award and O.\nSentence: Within months , Hendrix earned three UK top ten hits with the Jimi Hendrix Experience : Hey Joe , Purple Haze , and The Wind Cries Mary .", "prompt_labels": "Within(O) months(O) ,(O) Hendrix(B-musical artist) earned(O) three(O) UK(B-country) top(O) ten(O) hits(O) with(O) the(B-band) Jimi(I-band) Hendrix(I-band) Experience(I-band) :(O) Hey(B-song) Joe(I-song) ,(O) Purple(B-song) Haze(I-song) ,(O) and(O) The(B-song) Wind(I-song) Cries(I-song) Mary(I-song) .(O)"}}
{"id": "448", "dataset": "crossner_music", "split": "test", "label_list": ["album", "location", "musical instrument", "person", "award", "musical artist", "event", "song", "music genre", "band", "organization", "country"], "instance": {"id": "448", "words": ["By", "the", "early", "20th", "century", ",", "the", "United", "States", "had", "become", "a", "major", "center", "for", "folk", "music", "from", "around", "the", "world", ",", "including", "polka", ",", "Ukrainian", "and", "Polish", "fiddling", ",", "Ashkenazi", ",", "Klezmer", ",", "and", "several", "kinds", "of", "Latin", "music", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, location, musical instrument, person, award, musical artist, event, song, music genre, band, organization, country and O.\nSentence: By the early 20th century , the United States had become a major center for folk music from around the world , including polka , Ukrainian and Polish fiddling , Ashkenazi , Klezmer , and several kinds of Latin music .", "prompt_labels": "By(O) the(O) early(O) 20th(O) century(O) ,(O) the(O) United(B-country) States(I-country) had(O) become(O) a(O) major(O) center(O) for(O) folk(B-music genre) music(I-music genre) from(O) around(O) the(O) world(O) ,(O) including(O) polka(B-music genre) ,(O) Ukrainian(B-music genre) and(O) Polish(B-music genre) fiddling(I-music genre) ,(O) Ashkenazi(B-music genre) ,(O) Klezmer(B-music genre) ,(O) and(O) several(O) kinds(O) of(O) Latin(B-music genre) music(I-music genre) .(O)"}}
{"id": "119", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "song", "musical instrument", "award", "music genre", "person", "event", "location", "organization", "album", "country", "band"], "instance": {"id": "119", "words": ["Dion", "'s", "music", "has", "been", "influenced", "by", "genres", "ranging", "from", "rock", "and", "Rhythm", "and", "blues", "to", "Gospel", "music", "and", "classical", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, song, musical instrument, award, music genre, person, event, location, organization, album, country, band and O.\nSentence: Dion 's music has been influenced by genres ranging from rock and Rhythm and blues to Gospel music and classical .", "prompt_labels": "Dion(B-musical artist) 's(O) music(O) has(O) been(O) influenced(O) by(O) genres(O) ranging(O) from(O) rock(O) and(O) Rhythm(B-music genre) and(I-music genre) blues(I-music genre) to(O) Gospel(B-music genre) music(I-music genre) and(O) classical(B-music genre) .(O)"}}
{"id": "183", "dataset": "crossner_music", "split": "test", "label_list": ["location", "organization", "music genre", "award", "event", "musical instrument", "album", "band", "person", "musical artist", "song", "country"], "instance": {"id": "183", "words": ["In", "addition", "to", "Lady", "Antebellum", ",", "groups", "such", "as", "Herrick", ",", "The", "Quebe", "Sisters", "Band", ",", "Little", "Big", "Town", ",", "The", "Band", "Perry", ",", "Gloriana", ",", "Thompson", "Square", ",", "Eli", "Young", "Band", ",", "Zac", "Brown", "Band", "and", "British", "duo", "The", "Shires", "have", "emerged", "to", "occupy", "a", "large", "portion", "of", "the", "new", "country", "artists", "in", "the", "popular", "scene", "along", "with", "solo", "singers", "Kacey", "Musgraves", "and", "Miranda", "Lambert", "."], "labels": ["O", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "B-band", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, music genre, award, event, musical instrument, album, band, person, musical artist, song, country and O.\nSentence: In addition to Lady Antebellum , groups such as Herrick , The Quebe Sisters Band , Little Big Town , The Band Perry , Gloriana , Thompson Square , Eli Young Band , Zac Brown Band and British duo The Shires have emerged to occupy a large portion of the new country artists in the popular scene along with solo singers Kacey Musgraves and Miranda Lambert .", "prompt_labels": "In(O) addition(O) to(O) Lady(B-band) Antebellum(I-band) ,(O) groups(O) such(O) as(O) Herrick(B-band) ,(O) The(B-band) Quebe(I-band) Sisters(I-band) Band(I-band) ,(O) Little(B-band) Big(I-band) Town(I-band) ,(O) The(B-band) Band(I-band) Perry(I-band) ,(O) Gloriana(B-band) ,(O) Thompson(B-band) Square(I-band) ,(O) Eli(B-band) Young(I-band) Band(I-band) ,(O) Zac(B-band) Brown(I-band) Band(I-band) and(O) British(O) duo(O) The(B-band) Shires(I-band) have(O) emerged(O) to(O) occupy(O) a(O) large(O) portion(O) of(O) the(O) new(O) country(O) artists(O) in(O) the(O) popular(O) scene(O) along(O) with(O) solo(O) singers(O) Kacey(B-musical artist) Musgraves(I-musical artist) and(O) Miranda(B-musical artist) Lambert(I-musical artist) .(O)"}}
{"id": "60", "dataset": "crossner_music", "split": "test", "label_list": ["band", "musical artist", "organization", "country", "musical instrument", "song", "location", "person", "award", "event", "music genre", "album"], "instance": {"id": "60", "words": ["The", "New", "Orleans", "setting", "of", "the", "film", "played", "to", "Newman", "'s", "musical", "strengths", ",", "and", "his", "songs", "contained", "elements", "of", "Cajun", "music", ",", "zydeco", ",", "blues", "and", "Dixieland", "."], "labels": ["O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical artist, organization, country, musical instrument, song, location, person, award, event, music genre, album and O.\nSentence: The New Orleans setting of the film played to Newman 's musical strengths , and his songs contained elements of Cajun music , zydeco , blues and Dixieland .", "prompt_labels": "The(O) New(B-music genre) Orleans(I-music genre) setting(O) of(O) the(O) film(O) played(O) to(O) Newman(B-musical artist) 's(O) musical(O) strengths(O) ,(O) and(O) his(O) songs(O) contained(O) elements(O) of(O) Cajun(B-music genre) music(I-music genre) ,(O) zydeco(B-music genre) ,(O) blues(B-music genre) and(O) Dixieland(B-music genre) .(O)"}}
{"id": "105", "dataset": "crossner_music", "split": "test", "label_list": ["award", "location", "song", "musical instrument", "person", "event", "music genre", "organization", "musical artist", "album", "band", "country"], "instance": {"id": "105", "words": ["The", "Dark", "Star", "St.", "Stephen", "pairing", "was", "taken", "from", "the", "February", "27", ",", "1969", "show", "at", "the", "The", "Fillmore", ";", "The", "Eleven", "and", "Turn", "On", "Your", "Love", "Light", "were", "from", "the", "January", "26", ",", "1969", "show", "at", "the", "Avalon", "Ballroom", ";", "Death", "Don", "'t", "Have", "No", "Mercy", ",", "Feedback", ",", "and", "And", "We", "Bid", "You", "Goodnight", "were", "recorded", "March", "2", ",", "1969", ",", "at", "the", "The", "Fillmore", "."], "labels": ["O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "I-song", "O", "B-song", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, song, musical instrument, person, event, music genre, organization, musical artist, album, band, country and O.\nSentence: The Dark Star St. Stephen pairing was taken from the February 27 , 1969 show at the The Fillmore ; The Eleven and Turn On Your Love Light were from the January 26 , 1969 show at the Avalon Ballroom ; Death Don 't Have No Mercy , Feedback , and And We Bid You Goodnight were recorded March 2 , 1969 , at the The Fillmore .", "prompt_labels": "The(O) Dark(B-song) Star(I-song) St.(I-song) Stephen(I-song) pairing(O) was(O) taken(O) from(O) the(O) February(O) 27(O) ,(O) 1969(O) show(O) at(O) the(O) The(B-location) Fillmore(I-location) ;(O) The(B-song) Eleven(I-song) and(O) Turn(B-song) On(I-song) Your(I-song) Love(I-song) Light(I-song) were(O) from(O) the(O) January(O) 26(O) ,(O) 1969(O) show(O) at(O) the(O) Avalon(B-location) Ballroom(I-location) ;(O) Death(B-song) Don(I-song) 't(I-song) Have(I-song) No(I-song) Mercy(I-song) ,(O) Feedback(B-song) ,(O) and(O) And(O) We(B-song) Bid(I-song) You(I-song) Goodnight(I-song) were(O) recorded(O) March(O) 2(O) ,(O) 1969(O) ,(O) at(O) the(O) The(B-location) Fillmore(I-location) .(O)"}}
{"id": "286", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "band", "country", "music genre", "song", "award", "person", "event", "musical instrument", "musical artist", "album", "location"], "instance": {"id": "286", "words": ["Hunt", "won", "the", "Academy", "Award", "for", "Academy", "Award", "for", "Best", "Actress", "for", "starring", "as", "Carol", "Connelly", "in", "the", "romantic", "comedy", "As", "Good", "as", "It", "Gets", "(", "1997", ")", ",", "while", "her", "portrayal", "of", "Cheryl", "Cohen-Greene", "in", "The", "Sessions", "(", "2012", ")", ",", "gained", "her", "an", "additional", "Academy", "Awards", "nomination", "for", "Academy", "Award", "for", "Best", "Supporting", "Actress", "."], "labels": ["B-person", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, band, country, music genre, song, award, person, event, musical instrument, musical artist, album, location and O.\nSentence: Hunt won the Academy Award for Academy Award for Best Actress for starring as Carol Connelly in the romantic comedy As Good as It Gets ( 1997 ) , while her portrayal of Cheryl Cohen-Greene in The Sessions ( 2012 ) , gained her an additional Academy Awards nomination for Academy Award for Best Supporting Actress .", "prompt_labels": "Hunt(B-person) won(O) the(O) Academy(B-award) Award(I-award) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) for(O) starring(O) as(O) Carol(B-person) Connelly(I-person) in(O) the(O) romantic(O) comedy(O) As(O) Good(O) as(O) It(O) Gets(O) ((O) 1997(O) )(O) ,(O) while(O) her(O) portrayal(O) of(O) Cheryl(B-person) Cohen-Greene(I-person) in(O) The(O) Sessions(O) ((O) 2012(O) )(O) ,(O) gained(O) her(O) an(O) additional(O) Academy(B-award) Awards(I-award) nomination(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actress(I-award) .(O)"}}
{"id": "274", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "music genre", "band", "event", "song", "country", "person", "location", "album", "musical instrument", "award", "musical artist"], "instance": {"id": "274", "words": ["During", "this", "period", ",", "he", "also", "held", "a", "variety", "of", "positions", "at", "the", "Boston", "Symphony", "Orchestra", "'", "s", "summer", "home", "in", "Tanglewood", ",", "serving", "as", "director", "of", "new", "music", "activities", "from", "1965", "to", "1969", "and", "as", "artistic", "director", "of", "the", "Tanglewood", "Music", "Center", "from", "1970", "to", "1984", "and", "creating", "the", "Tanglewood", "Festival", "of", "Contemporary", "Music", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, music genre, band, event, song, country, person, location, album, musical instrument, award, musical artist and O.\nSentence: During this period , he also held a variety of positions at the Boston Symphony Orchestra ' s summer home in Tanglewood , serving as director of new music activities from 1965 to 1969 and as artistic director of the Tanglewood Music Center from 1970 to 1984 and creating the Tanglewood Festival of Contemporary Music .", "prompt_labels": "During(O) this(O) period(O) ,(O) he(O) also(O) held(O) a(O) variety(O) of(O) positions(O) at(O) the(O) Boston(B-band) Symphony(I-band) Orchestra(I-band) '(O) s(O) summer(O) home(O) in(O) Tanglewood(B-location) ,(O) serving(O) as(O) director(O) of(O) new(O) music(O) activities(O) from(O) 1965(O) to(O) 1969(O) and(O) as(O) artistic(O) director(O) of(O) the(O) Tanglewood(B-organization) Music(I-organization) Center(I-organization) from(O) 1970(O) to(O) 1984(O) and(O) creating(O) the(O) Tanglewood(B-event) Festival(I-event) of(I-event) Contemporary(I-event) Music(I-event) .(O)"}}
{"id": "121", "dataset": "crossner_music", "split": "test", "label_list": ["musical artist", "music genre", "band", "album", "event", "musical instrument", "song", "person", "location", "country", "organization", "award"], "instance": {"id": "121", "words": ["New", "songwriting", "and", "recording", "sessions", "took", "place", ",", "and", "during", "October", "and", "December", ",", "they", "released", "the", "singles", "The", "Day", "Before", "You", "Came", "/", "Cassandra", "and", "Under", "Attack", "/", "You", "Owe", "Me", "One", ",", "the", "A-sides", "of", "which", "were", "included", "on", "the", "compilation", "album", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "B-song", "O", "B-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, music genre, band, album, event, musical instrument, song, person, location, country, organization, award and O.\nSentence: New songwriting and recording sessions took place , and during October and December , they released the singles The Day Before You Came / Cassandra and Under Attack / You Owe Me One , the A-sides of which were included on the compilation album .", "prompt_labels": "New(O) songwriting(O) and(O) recording(O) sessions(O) took(O) place(O) ,(O) and(O) during(O) October(O) and(O) December(O) ,(O) they(O) released(O) the(O) singles(O) The(B-song) Day(I-song) Before(I-song) You(I-song) Came(I-song) /(O) Cassandra(B-song) and(O) Under(B-song) Attack(I-song) /(O) You(B-song) Owe(I-song) Me(I-song) One(I-song) ,(O) the(O) A-sides(O) of(O) which(O) were(O) included(O) on(O) the(O) compilation(O) album(O) .(O)"}}
{"id": "272", "dataset": "crossner_music", "split": "test", "label_list": ["award", "song", "album", "country", "music genre", "event", "location", "musical artist", "person", "organization", "band", "musical instrument"], "instance": {"id": "272", "words": ["The", "album", "was", "produced", "Mike", "Chapman", ",", "also", "known", "for", "his", "work", "with", "The", "Sweet", ",", "Mud", ",", "Suzi", "Quatro", ",", "Blondie", ",", "Pat", "Benatar", "and", "The", "Knack", "."], "labels": ["O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, song, album, country, music genre, event, location, musical artist, person, organization, band, musical instrument and O.\nSentence: The album was produced Mike Chapman , also known for his work with The Sweet , Mud , Suzi Quatro , Blondie , Pat Benatar and The Knack .", "prompt_labels": "The(O) album(O) was(O) produced(O) Mike(B-musical artist) Chapman(I-musical artist) ,(O) also(O) known(O) for(O) his(O) work(O) with(O) The(B-band) Sweet(I-band) ,(O) Mud(B-band) ,(O) Suzi(B-musical artist) Quatro(I-musical artist) ,(O) Blondie(B-band) ,(O) Pat(B-musical artist) Benatar(I-musical artist) and(O) The(B-band) Knack(I-band) .(O)"}}
{"id": "64", "dataset": "crossner_music", "split": "test", "label_list": ["location", "award", "band", "person", "musical instrument", "music genre", "song", "musical artist", "country", "album", "organization", "event"], "instance": {"id": "64", "words": ["The", "Police", "won", "a", "number", "of", "music", "awards", ",", "including", "six", "Grammy", "Award", "s", ",", "two", "Brit", "Awards", "-", "winning", "Best", "British", "Group", "once", ",", "an", "MTV", "Video", "Music", "Award", ",", "and", "in", "2003", "were", "inducted", "into", "the", "Rock", "and", "Roll", "Hall", "of", "Fame", "..", "Rolling", "Stone", "."], "labels": ["B-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, band, person, musical instrument, music genre, song, musical artist, country, album, organization, event and O.\nSentence: The Police won a number of music awards , including six Grammy Award s , two Brit Awards - winning Best British Group once , an MTV Video Music Award , and in 2003 were inducted into the Rock and Roll Hall of Fame .. Rolling Stone .", "prompt_labels": "The(B-band) Police(I-band) won(O) a(O) number(O) of(O) music(O) awards(O) ,(O) including(O) six(O) Grammy(B-award) Award(I-award) s(O) ,(O) two(O) Brit(B-award) Awards(I-award) -(O) winning(O) Best(B-award) British(I-award) Group(I-award) once(O) ,(O) an(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) ,(O) and(O) in(O) 2003(O) were(O) inducted(O) into(O) the(O) Rock(B-organization) and(I-organization) Roll(I-organization) Hall(I-organization) of(I-organization) Fame(I-organization) ..(O) Rolling(B-organization) Stone(I-organization) .(O)"}}
{"id": "41", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "band", "event", "location", "album", "organization", "musical instrument", "country", "musical artist", "song", "award", "person"], "instance": {"id": "41", "words": ["Such", "important", "centres", "include", "Mexico", ",", "the", "Andean", "region", ",", "Venezuela", ",", "and", "Paraguay", "."], "labels": ["O", "O", "O", "O", "B-country", "O", "O", "B-location", "O", "O", "B-country", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, band, event, location, album, organization, musical instrument, country, musical artist, song, award, person and O.\nSentence: Such important centres include Mexico , the Andean region , Venezuela , and Paraguay .", "prompt_labels": "Such(O) important(O) centres(O) include(O) Mexico(B-country) ,(O) the(O) Andean(B-location) region(O) ,(O) Venezuela(B-country) ,(O) and(O) Paraguay(B-country) .(O)"}}
{"id": "181", "dataset": "crossner_music", "split": "test", "label_list": ["band", "event", "location", "music genre", "album", "song", "country", "musical artist", "person", "organization", "musical instrument", "award"], "instance": {"id": "181", "words": ["In", "2001", ",", "having", "previously", "won", "an", "Emmy", "Award", ",", "a", "Grammy", "Award", "and", "an", "Academy", "Awards", ",", "he", "joined", "a", "small", "list", "of", "EGOT", "winners", "with", "his", "Tony", "Award", "wins", "for", "The", "Producers", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "B-award", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, event, location, music genre, album, song, country, musical artist, person, organization, musical instrument, award and O.\nSentence: In 2001 , having previously won an Emmy Award , a Grammy Award and an Academy Awards , he joined a small list of EGOT winners with his Tony Award wins for The Producers .", "prompt_labels": "In(O) 2001(O) ,(O) having(O) previously(O) won(O) an(O) Emmy(B-award) Award(I-award) ,(O) a(O) Grammy(B-award) Award(I-award) and(O) an(O) Academy(B-award) Awards(I-award) ,(O) he(O) joined(O) a(O) small(O) list(O) of(O) EGOT(B-award) winners(O) with(O) his(O) Tony(B-award) Award(I-award) wins(O) for(O) The(O) Producers(O) .(O)"}}
{"id": "389", "dataset": "crossner_music", "split": "test", "label_list": ["event", "album", "person", "award", "song", "music genre", "musical artist", "organization", "musical instrument", "country", "band", "location"], "instance": {"id": "389", "words": ["The", "music", "arose", "as", "a", "synthesis", "of", "traditional", "Creole", "music", ",", "some", "Cajun", "music", "influences", ",", "and", "African-American", "traditions", ",", "including", "Rhythm", "and", "blues", ",", "blues", ",", "jazz", ",", "and", "Gospel", "music", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, album, person, award, song, music genre, musical artist, organization, musical instrument, country, band, location and O.\nSentence: The music arose as a synthesis of traditional Creole music , some Cajun music influences , and African-American traditions , including Rhythm and blues , blues , jazz , and Gospel music .", "prompt_labels": "The(O) music(O) arose(O) as(O) a(O) synthesis(O) of(O) traditional(O) Creole(B-music genre) music(I-music genre) ,(O) some(O) Cajun(B-music genre) music(I-music genre) influences(O) ,(O) and(O) African-American(O) traditions(O) ,(O) including(O) Rhythm(B-music genre) and(I-music genre) blues(I-music genre) ,(O) blues(B-music genre) ,(O) jazz(B-music genre) ,(O) and(O) Gospel(B-music genre) music(I-music genre) .(O)"}}
{"id": "68", "dataset": "crossner_music", "split": "test", "label_list": ["song", "event", "organization", "country", "album", "musical instrument", "award", "band", "music genre", "location", "musical artist", "person"], "instance": {"id": "68", "words": ["Dead", "Kennedys", "have", "influenced", "multiple", "acts", "such", "as", "System", "of", "a", "Down", ",", "Green", "Day", ",", "Faith", "No", "More", ",", "Rage", "Against", "the", "Machine", ",", "Anthrax", ",", "Sepultura", ",", "Descendents", ",", "Bad", "Religion", ",", "X", ",", "Minutemen", ",", "The", "Hives", ",", "Saves", "the", "Day", "and", "Screeching", "Weasel", "among", "others", "."], "labels": ["B-band", "I-band", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "O", "B-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, organization, country, album, musical instrument, award, band, music genre, location, musical artist, person and O.\nSentence: Dead Kennedys have influenced multiple acts such as System of a Down , Green Day , Faith No More , Rage Against the Machine , Anthrax , Sepultura , Descendents , Bad Religion , X , Minutemen , The Hives , Saves the Day and Screeching Weasel among others .", "prompt_labels": "Dead(B-band) Kennedys(I-band) have(O) influenced(O) multiple(O) acts(O) such(O) as(O) System(B-band) of(I-band) a(I-band) Down(I-band) ,(O) Green(B-band) Day(I-band) ,(O) Faith(B-band) No(I-band) More(I-band) ,(O) Rage(B-band) Against(I-band) the(I-band) Machine(I-band) ,(O) Anthrax(B-band) ,(O) Sepultura(B-band) ,(O) Descendents(B-band) ,(O) Bad(B-band) Religion(I-band) ,(O) X(B-band) ,(O) Minutemen(B-band) ,(O) The(B-band) Hives(I-band) ,(O) Saves(B-band) the(I-band) Day(I-band) and(O) Screeching(B-band) Weasel(I-band) among(O) others(O) .(O)"}}
{"id": "373", "dataset": "crossner_music", "split": "test", "label_list": ["music genre", "location", "band", "person", "musical instrument", "organization", "event", "musical artist", "award", "album", "country", "song"], "instance": {"id": "373", "words": ["Delta", "blues", "was", "also", "an", "inspiration", "for", "the", "creation", "of", "British", "skiffle", "music", ",", "from", "which", "eventually", "came", "the", "British", "invasion", "bands", ",", "while", "simultaneously", "influencing", "British", "blues", ",", "which", "led", "to", "the", "birth", "of", "early", "hard", "rock", "and", "Heavy", "metal", "music", "."], "labels": ["B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, location, band, person, musical instrument, organization, event, musical artist, award, album, country, song and O.\nSentence: Delta blues was also an inspiration for the creation of British skiffle music , from which eventually came the British invasion bands , while simultaneously influencing British blues , which led to the birth of early hard rock and Heavy metal music .", "prompt_labels": "Delta(B-music genre) blues(I-music genre) was(O) also(O) an(O) inspiration(O) for(O) the(O) creation(O) of(O) British(O) skiffle(B-music genre) music(I-music genre) ,(O) from(O) which(O) eventually(O) came(O) the(O) British(O) invasion(O) bands(O) ,(O) while(O) simultaneously(O) influencing(O) British(B-music genre) blues(I-music genre) ,(O) which(O) led(O) to(O) the(O) birth(O) of(O) early(O) hard(B-music genre) rock(I-music genre) and(O) Heavy(B-music genre) metal(I-music genre) music(I-music genre) .(O)"}}
{"id": "447", "dataset": "crossner_music", "split": "test", "label_list": ["band", "musical instrument", "album", "person", "music genre", "location", "musical artist", "award", "event", "country", "organization", "song"], "instance": {"id": "447", "words": ["From", "the", "1970s", "through", "the", "1990s", ",", "Terry", "performed", "at", "Carnegie", "Hall", ",", "Town", "Hall", ",", "and", "Lincoln", "Center", ",", "toured", "with", "the", "Newport", "Jazz", "All", "Stars", "and", "Jazz", "at", "the", "Philharmonic", ",", "and", "was", "featured", "with", "Skitch", "Henderson", "'", "s", "New", "York", "Pops", "Orchestra", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-location", "I-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical instrument, album, person, music genre, location, musical artist, award, event, country, organization, song and O.\nSentence: From the 1970s through the 1990s , Terry performed at Carnegie Hall , Town Hall , and Lincoln Center , toured with the Newport Jazz All Stars and Jazz at the Philharmonic , and was featured with Skitch Henderson ' s New York Pops Orchestra .", "prompt_labels": "From(O) the(O) 1970s(O) through(O) the(O) 1990s(O) ,(O) Terry(B-musical artist) performed(O) at(O) Carnegie(B-location) Hall(I-location) ,(O) Town(B-location) Hall(I-location) ,(O) and(O) Lincoln(B-location) Center(I-location) ,(O) toured(O) with(O) the(O) Newport(B-event) Jazz(I-event) All(I-event) Stars(I-event) and(O) Jazz(B-event) at(I-event) the(I-event) Philharmonic(I-event) ,(O) and(O) was(O) featured(O) with(O) Skitch(B-musical artist) Henderson(I-musical artist) '(O) s(O) New(B-location) York(I-location) Pops(I-location) Orchestra(I-location) .(O)"}}
{"id": "187", "dataset": "crossner_music", "split": "test", "label_list": ["organization", "band", "musical artist", "song", "event", "country", "award", "album", "music genre", "musical instrument", "location", "person"], "instance": {"id": "187", "words": ["Four", "demos", "for", "the", "album", "were", "recorded", "on", "August", "13", ",", "1990", ";", "Enter", "Sandman", ",", "The", "Unforgiven", ",", "Nothing", "Else", "Matters", "and", "Wherever", "I", "May", "Roam", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "O", "B-song", "I-song", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, band, musical artist, song, event, country, award, album, music genre, musical instrument, location, person and O.\nSentence: Four demos for the album were recorded on August 13 , 1990 ; Enter Sandman , The Unforgiven , Nothing Else Matters and Wherever I May Roam .", "prompt_labels": "Four(O) demos(O) for(O) the(O) album(O) were(O) recorded(O) on(O) August(O) 13(O) ,(O) 1990(O) ;(O) Enter(B-song) Sandman(I-song) ,(O) The(B-song) Unforgiven(I-song) ,(O) Nothing(B-song) Else(I-song) Matters(I-song) and(O) Wherever(B-song) I(I-song) May(I-song) Roam(I-song) .(O)"}}
{"id": "381", "dataset": "crossner_music", "split": "test", "label_list": ["award", "music genre", "song", "musical artist", "event", "person", "location", "musical instrument", "organization", "country", "album", "band"], "instance": {"id": "381", "words": ["She", "also", "serves", "on", "the", "board", "of", "People", "for", "the", "American", "Way", ",", "and", "volunteers", "at", "Amnesty", "International", "and", "Citymeals-on-Wheels", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, music genre, song, musical artist, event, person, location, musical instrument, organization, country, album, band and O.\nSentence: She also serves on the board of People for the American Way , and volunteers at Amnesty International and Citymeals-on-Wheels .", "prompt_labels": "She(O) also(O) serves(O) on(O) the(O) board(O) of(O) People(B-organization) for(I-organization) the(I-organization) American(I-organization) Way(I-organization) ,(O) and(O) volunteers(O) at(O) Amnesty(B-organization) International(I-organization) and(O) Citymeals-on-Wheels(B-organization) .(O)"}}
{"id": "175", "dataset": "crossner_music", "split": "test", "label_list": ["person", "musical artist", "album", "organization", "song", "music genre", "country", "award", "event", "musical instrument", "location", "band"], "instance": {"id": "175", "words": ["In", "the", "early", "2010s", ",", "there", "was", "somewhat", "of", "a", "resurgence", "of", "boy", "band", "popularity", "in", "countries", "where", "the", "trend", "had", "not", "maintained", ",", "with", "the", "emergence", "of", "new", "boy", "bands", "like", "Big", "Time", "Rush", ",", "The", "Wanted", ",", "and", "One", "Direction", "and", "the", "formation", "of", "supergroup", "NKOTBSB", "which", "comprised", "members", "of", "New", "Kids", "on", "the", "Block", "and", "Backstreet", "Boys", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "B-band", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, musical artist, album, organization, song, music genre, country, award, event, musical instrument, location, band and O.\nSentence: In the early 2010s , there was somewhat of a resurgence of boy band popularity in countries where the trend had not maintained , with the emergence of new boy bands like Big Time Rush , The Wanted , and One Direction and the formation of supergroup NKOTBSB which comprised members of New Kids on the Block and Backstreet Boys .", "prompt_labels": "In(O) the(O) early(O) 2010s(O) ,(O) there(O) was(O) somewhat(O) of(O) a(O) resurgence(O) of(O) boy(O) band(O) popularity(O) in(O) countries(O) where(O) the(O) trend(O) had(O) not(O) maintained(O) ,(O) with(O) the(O) emergence(O) of(O) new(O) boy(O) bands(O) like(O) Big(B-band) Time(I-band) Rush(I-band) ,(O) The(B-band) Wanted(I-band) ,(O) and(O) One(B-band) Direction(I-band) and(O) the(O) formation(O) of(O) supergroup(O) NKOTBSB(B-band) which(O) comprised(O) members(O) of(O) New(B-band) Kids(I-band) on(I-band) the(I-band) Block(I-band) and(O) Backstreet(B-band) Boys(I-band) .(O)"}}
{"id": "363", "dataset": "crossner_music", "split": "test", "label_list": ["song", "country", "band", "musical artist", "organization", "award", "event", "person", "music genre", "musical instrument", "location", "album"], "instance": {"id": "363", "words": ["After", "Ice", "Cube", "made", "a", "cameo", "appearance", "in", "Dr.", "Dre", "'s", "Let", "Me", "Ride", "video", "in", "1993", ",", "the", "two", "recorded", "the", "hit", "song", "Natural", "Born", "Killaz", "for", "Snoop", "Doggy", "Dogg", "'", "s", "1994", "short", "film", "and", "soundtrack", "Murder", "Was", "the", "Case", "."], "labels": ["O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, country, band, musical artist, organization, award, event, person, music genre, musical instrument, location, album and O.\nSentence: After Ice Cube made a cameo appearance in Dr. Dre 's Let Me Ride video in 1993 , the two recorded the hit song Natural Born Killaz for Snoop Doggy Dogg ' s 1994 short film and soundtrack Murder Was the Case .", "prompt_labels": "After(O) Ice(B-musical artist) Cube(I-musical artist) made(O) a(O) cameo(O) appearance(O) in(O) Dr.(B-musical artist) Dre(I-musical artist) 's(O) Let(B-song) Me(I-song) Ride(I-song) video(O) in(O) 1993(O) ,(O) the(O) two(O) recorded(O) the(O) hit(O) song(O) Natural(B-song) Born(I-song) Killaz(I-song) for(O) Snoop(B-musical artist) Doggy(I-musical artist) Dogg(I-musical artist) '(O) s(O) 1994(O) short(O) film(O) and(O) soundtrack(O) Murder(B-album) Was(I-album) the(I-album) Case(I-album) .(O)"}}
{"id": "2", "dataset": "crossner_music", "split": "test", "label_list": ["country", "album", "music genre", "song", "person", "event", "musical instrument", "band", "organization", "location", "musical artist", "award"], "instance": {"id": "2", "words": ["Polka", "is", "still", "a", "popular", "genre", "of", "folk", "music", "in", "many", "European", "countries", "and", "is", "performed", "by", "folk", "artists", "in", "Poland", ",", "Latvia", ",", "Lithuania", ",", "Czech", "Republic", ",", "Netherlands", ",", "Croatia", ",", "Slovenia", ",", "Germany", ",", "Hungary", ",", "Austria", ",", "Switzerland", ",", "Italy", ",", "Ukraine", ",", "Belarus", ",", "Russia", "and", "Slovakia", "."], "labels": ["B-music genre", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "I-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, album, music genre, song, person, event, musical instrument, band, organization, location, musical artist, award and O.\nSentence: Polka is still a popular genre of folk music in many European countries and is performed by folk artists in Poland , Latvia , Lithuania , Czech Republic , Netherlands , Croatia , Slovenia , Germany , Hungary , Austria , Switzerland , Italy , Ukraine , Belarus , Russia and Slovakia .", "prompt_labels": "Polka(B-music genre) is(O) still(O) a(O) popular(O) genre(O) of(O) folk(B-music genre) music(I-music genre) in(O) many(O) European(O) countries(O) and(O) is(O) performed(O) by(O) folk(O) artists(O) in(O) Poland(B-country) ,(O) Latvia(B-country) ,(O) Lithuania(B-country) ,(O) Czech(B-country) Republic(I-country) ,(O) Netherlands(B-country) ,(O) Croatia(B-country) ,(O) Slovenia(B-country) ,(O) Germany(B-country) ,(O) Hungary(B-country) ,(O) Austria(B-country) ,(O) Switzerland(B-country) ,(O) Italy(B-country) ,(O) Ukraine(B-country) ,(O) Belarus(B-country) ,(O) Russia(B-country) and(O) Slovakia(B-country) .(O)"}}
{"id": "348", "dataset": "crossner_music", "split": "test", "label_list": ["person", "song", "album", "event", "country", "location", "musical instrument", "musical artist", "organization", "music genre", "award", "band"], "instance": {"id": "348", "words": ["In", "2019", ",", "Cayabyab", "composed", "the", "theme", "song", "for", "the", "2019", "Southeast", "Asian", "Games", ",", "We", "Win", "as", "One", ";", "with", "lyrics", "by", "playwright", "Floy", "Quintos", ",", "and", "sung", "by", "Lea", "Salonga", "."], "labels": ["O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, song, album, event, country, location, musical instrument, musical artist, organization, music genre, award, band and O.\nSentence: In 2019 , Cayabyab composed the theme song for the 2019 Southeast Asian Games , We Win as One ; with lyrics by playwright Floy Quintos , and sung by Lea Salonga .", "prompt_labels": "In(O) 2019(O) ,(O) Cayabyab(B-musical artist) composed(O) the(O) theme(O) song(O) for(O) the(O) 2019(B-event) Southeast(I-event) Asian(I-event) Games(I-event) ,(O) We(B-song) Win(I-song) as(I-song) One(I-song) ;(O) with(O) lyrics(O) by(O) playwright(O) Floy(B-person) Quintos(I-person) ,(O) and(O) sung(O) by(O) Lea(B-musical artist) Salonga(I-musical artist) .(O)"}}
{"id": "285", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "organization", "election", "event", "politician", "person", "political party", "location"], "instance": {"id": "285", "words": ["It", "did", "not", "stand", "candidates", "in", "the", "Māori", "electorates", "in", "the", "2002", "New", "Zealand", "general", "election", ",", "2005", "New", "Zealand", "general", "election", ",", "or", "2008", "New", "Zealand", "general", "elections", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, election, event, politician, person, political party, location and O.\nSentence: It did not stand candidates in the Māori electorates in the 2002 New Zealand general election , 2005 New Zealand general election , or 2008 New Zealand general elections .", "prompt_labels": "It(O) did(O) not(O) stand(O) candidates(O) in(O) the(O) Māori(O) electorates(O) in(O) the(O) 2002(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) 2005(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) or(O) 2008(B-election) New(I-election) Zealand(I-election) general(I-election) elections(I-election) .(O)"}}
{"id": "622", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "location", "political party", "person", "organization", "event", "politician", "country"], "instance": {"id": "622", "words": ["253", "was", "the", "final", "Allies", "of", "World", "War", "II", "attack", "during", "the", "Italian", "Campaign", "in", "the", "final", "stages", "of", "the", "Second", "World", "War", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, political party, person, organization, event, politician, country and O.\nSentence: 253 was the final Allies of World War II attack during the Italian Campaign in the final stages of the Second World War .", "prompt_labels": "253(O) was(O) the(O) final(O) Allies(B-organization) of(I-organization) World(I-organization) War(I-organization) II(I-organization) attack(O) during(O) the(O) Italian(B-event) Campaign(I-event) in(O) the(O) final(O) stages(O) of(O) the(O) Second(B-event) World(I-event) War(I-event) .(O)"}}
{"id": "319", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "location", "person", "election", "political party", "country", "organization", "event"], "instance": {"id": "319", "words": ["In", "1963", "Canadian", "federal", "election", ",", "he", "ran", "unsuccessfully", "as", "a", "Liberal", "Party", "of", "Canada", "in", "the", "riding", "of", "Port", "Arthur", ",", "losing", "to", "the", "New", "Democratic", "Party", "candidate", ",", "Douglas", "Fisher", "."], "labels": ["O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, person, election, political party, country, organization, event and O.\nSentence: In 1963 Canadian federal election , he ran unsuccessfully as a Liberal Party of Canada in the riding of Port Arthur , losing to the New Democratic Party candidate , Douglas Fisher .", "prompt_labels": "In(O) 1963(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) he(O) ran(O) unsuccessfully(O) as(O) a(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) in(O) the(O) riding(O) of(O) Port(B-politician) Arthur(I-politician) ,(O) losing(O) to(O) the(O) New(B-political party) Democratic(I-political party) Party(I-political party) candidate(O) ,(O) Douglas(B-politician) Fisher(I-politician) .(O)"}}
{"id": "155", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "politician", "political party", "event", "location", "organization", "person", "election"], "instance": {"id": "155", "words": ["The", "PSOE", "'s", "motion", "passed", "with", "the", "support", "of", "Unidos", "Podemos", "(", "UP", ")", ",", "Republican", "Left", "of", "Catalonia", "(", "ERC", ")", ",", "Catalan", "European", "Democratic", "Party", "(", "PDeCAT", ")", ",", "Basque", "Nationalist", "Party", "(", "PNV", ")", ",", "Coalició", "Compromís", ",", "EH", "Bildu", "and", "New", "Canaries", "(", "NCa", ")", ",", "bringing", "down", "the", "Rajoy", "government", "."], "labels": ["O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "B-politician", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, politician, political party, event, location, organization, person, election and O.\nSentence: The PSOE 's motion passed with the support of Unidos Podemos ( UP ) , Republican Left of Catalonia ( ERC ) , Catalan European Democratic Party ( PDeCAT ) , Basque Nationalist Party ( PNV ) , Coalició Compromís , EH Bildu and New Canaries ( NCa ) , bringing down the Rajoy government .", "prompt_labels": "The(O) PSOE(B-political party) 's(O) motion(O) passed(O) with(O) the(O) support(O) of(O) Unidos(B-political party) Podemos(I-political party) ((O) UP(B-political party) )(O) ,(O) Republican(B-political party) Left(I-political party) of(I-political party) Catalonia(I-political party) ((O) ERC(B-political party) )(O) ,(O) Catalan(B-political party) European(I-political party) Democratic(I-political party) Party(I-political party) ((O) PDeCAT(B-political party) )(O) ,(O) Basque(B-political party) Nationalist(I-political party) Party(I-political party) ((O) PNV(B-political party) )(O) ,(O) Coalició(B-political party) Compromís(I-political party) ,(O) EH(B-political party) Bildu(I-political party) and(O) New(B-political party) Canaries(I-political party) ((O) NCa(B-political party) )(O) ,(O) bringing(O) down(O) the(O) Rajoy(B-politician) government(O) .(O)"}}
{"id": "268", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "organization", "country", "politician", "election", "event", "political party", "person"], "instance": {"id": "268", "words": ["Departing", "from", "Middlesbrough", "railway", "station", ",", "Northern", "operates", "rail", "services", "throughout", "the", "north-east", "region", "including", "to", "Newcastle", "upon", "Tyne", ",", "Sunderland", ",", "Darlington", ",", "Redcar", "and", "Whitby", ",", "whilst", "TransPennine", "Express", "provides", "direct", "rail", "services", "to", "cities", "such", "as", "Leeds", ",", "York", ",", "Liverpool", "and", "Manchester", "."], "labels": ["O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, country, politician, election, event, political party, person and O.\nSentence: Departing from Middlesbrough railway station , Northern operates rail services throughout the north-east region including to Newcastle upon Tyne , Sunderland , Darlington , Redcar and Whitby , whilst TransPennine Express provides direct rail services to cities such as Leeds , York , Liverpool and Manchester .", "prompt_labels": "Departing(O) from(O) Middlesbrough(B-location) railway(I-location) station(I-location) ,(O) Northern(O) operates(O) rail(O) services(O) throughout(O) the(O) north-east(O) region(O) including(O) to(O) Newcastle(B-location) upon(I-location) Tyne(I-location) ,(O) Sunderland(B-location) ,(O) Darlington(B-location) ,(O) Redcar(B-location) and(O) Whitby(B-location) ,(O) whilst(O) TransPennine(B-organization) Express(I-organization) provides(O) direct(O) rail(O) services(O) to(O) cities(O) such(O) as(O) Leeds(B-location) ,(O) York(B-location) ,(O) Liverpool(B-location) and(O) Manchester(B-location) .(O)"}}
{"id": "307", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "country", "location", "election", "politician", "person", "political party", "event"], "instance": {"id": "307", "words": ["Born", "in", "Pownal", ",", "he", "first", "ran", "for", "public", "office", "in", "the", "1921", "Canadian", "federal", "election", "as", "a", "United", "Farmers", "of", "Canada", "-", "Progressive", "Party", "of", "Canada", "candidate", ",", "but", "failed", "to", "win", "a", "seat", "in", "the", "House", "of", "Commons", "of", "Canada", "."], "labels": ["O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, location, election, politician, person, political party, event and O.\nSentence: Born in Pownal , he first ran for public office in the 1921 Canadian federal election as a United Farmers of Canada - Progressive Party of Canada candidate , but failed to win a seat in the House of Commons of Canada .", "prompt_labels": "Born(O) in(O) Pownal(B-location) ,(O) he(O) first(O) ran(O) for(O) public(O) office(O) in(O) the(O) 1921(B-election) Canadian(I-election) federal(I-election) election(I-election) as(O) a(O) United(B-political party) Farmers(I-political party) of(I-political party) Canada(I-political party) -(O) Progressive(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) candidate(O) ,(O) but(O) failed(O) to(O) win(O) a(O) seat(O) in(O) the(O) House(B-organization) of(I-organization) Commons(I-organization) of(I-organization) Canada(I-organization) .(O)"}}
{"id": "324", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "organization", "election", "person", "political party", "country", "event", "location"], "instance": {"id": "324", "words": ["It", "was", "suspected", "de", "Lille", "would", "either", "revive", "her", "old", "political", "party", ",", "the", "Independent", "Democrats", ",", "or", "either", "join", "the", "African", "National", "Congress", "or", "the", "Economic", "Freedom", "Fighters", "."], "labels": ["O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, election, person, political party, country, event, location and O.\nSentence: It was suspected de Lille would either revive her old political party , the Independent Democrats , or either join the African National Congress or the Economic Freedom Fighters .", "prompt_labels": "It(O) was(O) suspected(O) de(B-politician) Lille(I-politician) would(O) either(O) revive(O) her(O) old(O) political(O) party(O) ,(O) the(O) Independent(B-political party) Democrats(I-political party) ,(O) or(O) either(O) join(O) the(O) African(B-political party) National(I-political party) Congress(I-political party) or(O) the(O) Economic(B-political party) Freedom(I-political party) Fighters(I-political party) .(O)"}}
{"id": "450", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "event", "election", "political party", "organization", "politician", "country", "location"], "instance": {"id": "450", "words": ["The", "UdC", "contested", "the", "2013", "Italian", "general", "election", "as", "part", "of", "the", "With", "Monti", "for", "Italy", "coalition", ",", "alongside", "FLI", "and", "Monti", "'s", "Civic", "Choice", "(", "SC", ")", "."], "labels": ["O", "B-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "O", "B-politician", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, election, political party, organization, politician, country, location and O.\nSentence: The UdC contested the 2013 Italian general election as part of the With Monti for Italy coalition , alongside FLI and Monti 's Civic Choice ( SC ) .", "prompt_labels": "The(O) UdC(B-political party) contested(O) the(O) 2013(B-election) Italian(I-election) general(I-election) election(I-election) as(O) part(O) of(O) the(O) With(B-political party) Monti(I-political party) for(I-political party) Italy(I-political party) coalition(O) ,(O) alongside(O) FLI(B-political party) and(O) Monti(B-politician) 's(O) Civic(B-political party) Choice(I-political party) ((O) SC(B-political party) )(O) .(O)"}}
{"id": "65", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "event", "location", "election", "politician", "person", "political party", "country"], "instance": {"id": "65", "words": ["The", "youngest", "daughter", "of", "Aung", "San", ",", "Father", "of", "the", "Nation", "of", "modern-day", "Myanmar", ",", "and", "Khin", "Kyi", ",", "Aung", "San", "Suu", "Kyi", "was", "born", "in", "Yangon", ",", "British", "rule", "in", "Burma", "."], "labels": ["O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "I-politician", "I-politician", "O", "O", "O", "B-location", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, location, election, politician, person, political party, country and O.\nSentence: The youngest daughter of Aung San , Father of the Nation of modern-day Myanmar , and Khin Kyi , Aung San Suu Kyi was born in Yangon , British rule in Burma .", "prompt_labels": "The(O) youngest(O) daughter(O) of(O) Aung(B-politician) San(I-politician) ,(O) Father(O) of(O) the(O) Nation(O) of(O) modern-day(O) Myanmar(B-country) ,(O) and(O) Khin(B-politician) Kyi(I-politician) ,(O) Aung(B-politician) San(I-politician) Suu(I-politician) Kyi(I-politician) was(O) born(O) in(O) Yangon(B-location) ,(O) British(O) rule(O) in(O) Burma(B-country) .(O)"}}
{"id": "1", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "political party", "location", "event", "country", "politician", "election", "organization"], "instance": {"id": "1", "words": ["Lincoln", "replaced", "Buell", "with", "William", "Rosecrans", ";", "and", "after", "the", "1862", "and", "1863", "United", "States", "House", "of", "Representatives", "elections", "he", "replaced", "McClellan", "with", "Ambrose", "Burnside", "."], "labels": ["B-politician", "O", "B-politician", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-politician", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, location, event, country, politician, election, organization and O.\nSentence: Lincoln replaced Buell with William Rosecrans ; and after the 1862 and 1863 United States House of Representatives elections he replaced McClellan with Ambrose Burnside .", "prompt_labels": "Lincoln(B-politician) replaced(O) Buell(B-politician) with(O) William(B-politician) Rosecrans(I-politician) ;(O) and(O) after(O) the(O) 1862(B-election) and(I-election) 1863(I-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) he(O) replaced(O) McClellan(B-politician) with(O) Ambrose(B-politician) Burnside(I-politician) .(O)"}}
{"id": "16", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "country", "political party", "politician", "election", "location", "person", "organization"], "instance": {"id": "16", "words": ["The", "twelve", "countries", "that", "had", "significant", "interests", "in", "Antarctica", "at", "the", "time", "were", ":", "Argentina", ",", "Australia", ",", "Belgium", ",", "Chile", ",", "France", ",", "Japan", ",", "New", "Zealand", ",", "Norway", ",", "South", "Africa", ",", "the", "Soviet", "Union", ",", "the", "United", "Kingdom", ",", "and", "the", "United", "States", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "I-country", "O", "B-country", "O", "B-country", "I-country", "O", "O", "B-country", "I-country", "O", "O", "B-country", "I-country", "O", "O", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, political party, politician, election, location, person, organization and O.\nSentence: The twelve countries that had significant interests in Antarctica at the time were : Argentina , Australia , Belgium , Chile , France , Japan , New Zealand , Norway , South Africa , the Soviet Union , the United Kingdom , and the United States .", "prompt_labels": "The(O) twelve(O) countries(O) that(O) had(O) significant(O) interests(O) in(O) Antarctica(B-location) at(O) the(O) time(O) were(O) :(O) Argentina(B-country) ,(O) Australia(B-country) ,(O) Belgium(B-country) ,(O) Chile(B-country) ,(O) France(B-country) ,(O) Japan(B-country) ,(O) New(B-country) Zealand(I-country) ,(O) Norway(B-country) ,(O) South(B-country) Africa(I-country) ,(O) the(O) Soviet(B-country) Union(I-country) ,(O) the(O) United(B-country) Kingdom(I-country) ,(O) and(O) the(O) United(B-country) States(I-country) .(O)"}}
{"id": "241", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "event", "location", "country", "politician", "person", "organization", "political party"], "instance": {"id": "241", "words": ["The", "ALP", "was", "defeated", "at", "three", "consecutive", "federal", "elections", "under", "Evatt", "'s", "leadership", ",", "in", "1954", "Australian", "federal", "election", ",", "1955", "Australian", "federal", "election", "and", "1958", "Australian", "federal", "election", "."], "labels": ["O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, location, country, politician, person, organization, political party and O.\nSentence: The ALP was defeated at three consecutive federal elections under Evatt 's leadership , in 1954 Australian federal election , 1955 Australian federal election and 1958 Australian federal election .", "prompt_labels": "The(O) ALP(B-political party) was(O) defeated(O) at(O) three(O) consecutive(O) federal(O) elections(O) under(O) Evatt(B-politician) 's(O) leadership(O) ,(O) in(O) 1954(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) 1955(B-election) Australian(I-election) federal(I-election) election(I-election) and(O) 1958(B-election) Australian(I-election) federal(I-election) election(I-election) .(O)"}}
{"id": "373", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "location", "country", "politician", "organization", "event", "person", "political party"], "instance": {"id": "373", "words": ["The", "amendment", "was", "adopted", "during", "the", "Fianna", "Fáil", "-", "Progressive", "Democrats", "coalition", "government", "led", "by", "Bertie", "Ahern", "but", "had", "been", "first", "drafted", "and", "suggested", "by", "the", "previous", "Fine", "Gael", "-", "Labour", "Party", "-", "Democratic", "Left", "government", "led", "by", "John", "Bruton", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, country, politician, organization, event, person, political party and O.\nSentence: The amendment was adopted during the Fianna Fáil - Progressive Democrats coalition government led by Bertie Ahern but had been first drafted and suggested by the previous Fine Gael - Labour Party - Democratic Left government led by John Bruton .", "prompt_labels": "The(O) amendment(O) was(O) adopted(O) during(O) the(O) Fianna(B-political party) Fáil(I-political party) -(O) Progressive(B-political party) Democrats(I-political party) coalition(O) government(O) led(O) by(O) Bertie(B-politician) Ahern(I-politician) but(O) had(O) been(O) first(O) drafted(O) and(O) suggested(O) by(O) the(O) previous(O) Fine(B-political party) Gael(I-political party) -(O) Labour(B-political party) Party(I-political party) -(O) Democratic(B-political party) Left(I-political party) government(O) led(O) by(O) John(B-politician) Bruton(I-politician) .(O)"}}
{"id": "108", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "political party", "country", "organization", "event", "politician", "person", "election"], "instance": {"id": "108", "words": ["He", "then", "worked", "on", "a", "number", "of", "foreign", "campaigns", ",", "including", "those", "of", "Tony", "Blair", "-", "then", "Prime", "Minister", "of", "the", "United", "Kingdom", "-", "during", "the", "2001", "United", "Kingdom", "general", "election", ";", "Ehud", "Barak", "of", "Israel", "'", "s", "Israeli", "Labor", "Party", "(", "at", "the", "suggestion", "of", "Clinton", ",", "who", "had", "grown", "frustrated", "with", "Benjamin", "Netanyahu", "'", "s", "intransigence", "in", "the", "peace", "process", ")", "in", "the", "1999", "Knesset", "election", ";", "and", "with", "the", "Liberal", "Party", "of", "Canada", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-politician", "I-politician", "O", "B-country", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, country, organization, event, politician, person, election and O.\nSentence: He then worked on a number of foreign campaigns , including those of Tony Blair - then Prime Minister of the United Kingdom - during the 2001 United Kingdom general election ; Ehud Barak of Israel ' s Israeli Labor Party ( at the suggestion of Clinton , who had grown frustrated with Benjamin Netanyahu ' s intransigence in the peace process ) in the 1999 Knesset election ; and with the Liberal Party of Canada .", "prompt_labels": "He(O) then(O) worked(O) on(O) a(O) number(O) of(O) foreign(O) campaigns(O) ,(O) including(O) those(O) of(O) Tony(B-politician) Blair(I-politician) -(O) then(O) Prime(O) Minister(O) of(O) the(O) United(B-country) Kingdom(I-country) -(O) during(O) the(O) 2001(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ;(O) Ehud(B-politician) Barak(I-politician) of(O) Israel(B-country) '(O) s(O) Israeli(B-political party) Labor(I-political party) Party(I-political party) ((O) at(O) the(O) suggestion(O) of(O) Clinton(B-politician) ,(O) who(O) had(O) grown(O) frustrated(O) with(O) Benjamin(B-politician) Netanyahu(I-politician) '(O) s(O) intransigence(O) in(O) the(O) peace(O) process(O) )(O) in(O) the(O) 1999(B-election) Knesset(I-election) election(I-election) ;(O) and(O) with(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) .(O)"}}
{"id": "141", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "person", "election", "event", "organization", "political party", "country", "politician"], "instance": {"id": "141", "words": ["Its", "national", "headquarters", "remain", "in", "New", "York", "City", ",", "but", "today", "it", "has", "regional", "offices", "in", "Atlanta", ",", "Chicago", ",", "Dallas", ",", "Los", "Angeles", ",", "and", "Washington", "."], "labels": ["O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "I-location", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, election, event, organization, political party, country, politician and O.\nSentence: Its national headquarters remain in New York City , but today it has regional offices in Atlanta , Chicago , Dallas , Los Angeles , and Washington .", "prompt_labels": "Its(O) national(O) headquarters(O) remain(O) in(O) New(B-location) York(I-location) City(I-location) ,(O) but(O) today(O) it(O) has(O) regional(O) offices(O) in(O) Atlanta(B-location) ,(O) Chicago(B-location) ,(O) Dallas(B-location) ,(O) Los(B-location) Angeles(I-location) ,(O) and(O) Washington(B-location) .(O)"}}
{"id": "395", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "election", "organization", "political party", "politician", "person", "location", "country"], "instance": {"id": "395", "words": ["The", "election", "also", "coincided", "with", "the", "2004", "United", "Kingdom", "local", "elections", "and", "the", "2004", "London", "Assembly", "election", "and", "2004", "London", "mayoral", "election", "elections", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, organization, political party, politician, person, location, country and O.\nSentence: The election also coincided with the 2004 United Kingdom local elections and the 2004 London Assembly election and 2004 London mayoral election elections .", "prompt_labels": "The(O) election(O) also(O) coincided(O) with(O) the(O) 2004(B-election) United(I-election) Kingdom(I-election) local(I-election) elections(I-election) and(O) the(O) 2004(B-election) London(I-election) Assembly(I-election) election(I-election) and(O) 2004(B-election) London(I-election) mayoral(I-election) election(I-election) elections(O) .(O)"}}
{"id": "419", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "politician", "event", "person", "organization", "political party", "country", "location"], "instance": {"id": "419", "words": ["Although", "several", "public", "opinion", "polls", "predicted", "that", "the", "2006", "Canadian", "federal", "election", "would", "result", "in", "either", "a", "strong", "Conservative", "minority", "or", "a", "slight", "majority", ",", "the", "Liberal", "Party", "of", "Canada", "enjoyed", "a", "last-minute", "surge", "but", "were", "unable", "to", "overtake", "the", "Conservative", "Party", "of", "Canada", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, event, person, organization, political party, country, location and O.\nSentence: Although several public opinion polls predicted that the 2006 Canadian federal election would result in either a strong Conservative minority or a slight majority , the Liberal Party of Canada enjoyed a last-minute surge but were unable to overtake the Conservative Party of Canada .", "prompt_labels": "Although(O) several(O) public(O) opinion(O) polls(O) predicted(O) that(O) the(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) would(O) result(O) in(O) either(O) a(O) strong(O) Conservative(O) minority(O) or(O) a(O) slight(O) majority(O) ,(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) enjoyed(O) a(O) last-minute(O) surge(O) but(O) were(O) unable(O) to(O) overtake(O) the(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) .(O)"}}
{"id": "375", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "location", "country", "election", "organization", "person", "political party", "event"], "instance": {"id": "375", "words": ["Strauss", "writes", "that", ",", "as", "a", "result", "of", "far-right", "involvement", ",", "a", "bizarre", "ideological", "turf", "war", "has", "broken", "out", ",", "whereby", "anti-globalization", "activists", "are", "fighting", "a", "two-front", "battle", ",", "one", "against", "the", "World", "Trade", "Organization", ",", "International", "Monetary", "Fund", "(", "IMF", ")", ",", "and", "World", "Bank", ",", "the", "other", "against", "the", "extremists", "who", "turn", "up", "at", "their", "rallies", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, country, election, organization, person, political party, event and O.\nSentence: Strauss writes that , as a result of far-right involvement , a bizarre ideological turf war has broken out , whereby anti-globalization activists are fighting a two-front battle , one against the World Trade Organization , International Monetary Fund ( IMF ) , and World Bank , the other against the extremists who turn up at their rallies .", "prompt_labels": "Strauss(B-politician) writes(O) that(O) ,(O) as(O) a(O) result(O) of(O) far-right(O) involvement(O) ,(O) a(O) bizarre(O) ideological(O) turf(O) war(O) has(O) broken(O) out(O) ,(O) whereby(O) anti-globalization(O) activists(O) are(O) fighting(O) a(O) two-front(O) battle(O) ,(O) one(O) against(O) the(O) World(B-organization) Trade(I-organization) Organization(I-organization) ,(O) International(B-organization) Monetary(I-organization) Fund(I-organization) ((O) IMF(B-organization) )(O) ,(O) and(O) World(B-organization) Bank(I-organization) ,(O) the(O) other(O) against(O) the(O) extremists(O) who(O) turn(O) up(O) at(O) their(O) rallies(O) .(O)"}}
{"id": "301", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "country", "election", "person", "event", "political party", "politician", "organization"], "instance": {"id": "301", "words": ["The", "Bill", "was", "backed", "by", "the", "Labour", "Party", ",", "the", "Liberal", "Democrats", ",", "Plaid", "Cymru", ",", "the", "Scottish", "National", "Party", "and", "the", "Social", "Democratic", "and", "Labour", "Party", "."], "labels": ["O", "B-politician", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, election, person, event, political party, politician, organization and O.\nSentence: The Bill was backed by the Labour Party , the Liberal Democrats , Plaid Cymru , the Scottish National Party and the Social Democratic and Labour Party .", "prompt_labels": "The(O) Bill(B-politician) was(O) backed(O) by(O) the(O) Labour(B-political party) Party(I-political party) ,(O) the(O) Liberal(B-political party) Democrats(I-political party) ,(O) Plaid(B-political party) Cymru(I-political party) ,(O) the(O) Scottish(B-political party) National(I-political party) Party(I-political party) and(O) the(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) .(O)"}}
{"id": "49", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "politician", "event", "location", "election", "political party", "country", "person"], "instance": {"id": "49", "words": ["The", "Iranic", "peoples", "under", "the", "Medes", ",", "aided", "by", "the", "previous", "Neo-Assyrian", "Empire", "destruction", "of", "the", "hitherto", "dominant", "Elamites", "of", "Ancient", "Iran", ",", "also", "took", "advantage", "of", "the", "upheavals", "in", "Neo-Assyrian", "Empire", "to", "coalesce", "into", "a", "powerful", "Median-dominated", "force", "."], "labels": ["O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, event, location, election, political party, country, person and O.\nSentence: The Iranic peoples under the Medes , aided by the previous Neo-Assyrian Empire destruction of the hitherto dominant Elamites of Ancient Iran , also took advantage of the upheavals in Neo-Assyrian Empire to coalesce into a powerful Median-dominated force .", "prompt_labels": "The(O) Iranic(O) peoples(O) under(O) the(O) Medes(B-country) ,(O) aided(O) by(O) the(O) previous(O) Neo-Assyrian(B-country) Empire(I-country) destruction(O) of(O) the(O) hitherto(O) dominant(O) Elamites(O) of(O) Ancient(B-country) Iran(I-country) ,(O) also(O) took(O) advantage(O) of(O) the(O) upheavals(O) in(O) Neo-Assyrian(B-country) Empire(I-country) to(O) coalesce(O) into(O) a(O) powerful(O) Median-dominated(O) force(O) .(O)"}}
{"id": "644", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "politician", "location", "person", "political party", "organization", "country", "election"], "instance": {"id": "644", "words": ["The", "2018", "North", "Korea-United", "States", "Singapore", "Summit", ",", "commonly", "known", "as", "Singapore", "Summit", ",", "was", "a", "summit", "meeting", "between", "North", "Korean", "Chairman", "Kim", "Jong-un", "and", "U.S.", "President", "Donald", "Trump", ",", "held", "at", "the", "Capella", "Hotel", ",", "Sentosa", ",", "Singapore", "."], "labels": ["O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "B-politician", "I-politician", "O", "B-country", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, location, person, political party, organization, country, election and O.\nSentence: The 2018 North Korea-United States Singapore Summit , commonly known as Singapore Summit , was a summit meeting between North Korean Chairman Kim Jong-un and U.S. President Donald Trump , held at the Capella Hotel , Sentosa , Singapore .", "prompt_labels": "The(O) 2018(B-event) North(I-event) Korea-United(I-event) States(I-event) Singapore(I-event) Summit(I-event) ,(O) commonly(O) known(O) as(O) Singapore(B-event) Summit(I-event) ,(O) was(O) a(O) summit(O) meeting(O) between(O) North(B-country) Korean(I-country) Chairman(O) Kim(B-politician) Jong-un(I-politician) and(O) U.S.(B-country) President(O) Donald(B-politician) Trump(I-politician) ,(O) held(O) at(O) the(O) Capella(B-location) Hotel(I-location) ,(O) Sentosa(B-location) ,(O) Singapore(B-country) .(O)"}}
{"id": "90", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "event", "person", "election", "country", "political party", "organization", "politician"], "instance": {"id": "90", "words": ["Belgium", "assumed", "control", "of", "the", "German", "colonies", "of", "Ruanda-Urundi", "(", "modern-day", "Rwanda", "and", "Burundi", ")", "during", "the", "war", ",", "and", "in", "1924", "the", "League", "of", "Nations", "mandated", "them", "to", "Belgium", "."], "labels": ["B-country", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-country", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, person, election, country, political party, organization, politician and O.\nSentence: Belgium assumed control of the German colonies of Ruanda-Urundi ( modern-day Rwanda and Burundi ) during the war , and in 1924 the League of Nations mandated them to Belgium .", "prompt_labels": "Belgium(B-country) assumed(O) control(O) of(O) the(O) German(O) colonies(O) of(O) Ruanda-Urundi(B-country) ((O) modern-day(O) Rwanda(B-country) and(O) Burundi(B-country) )(O) during(O) the(O) war(O) ,(O) and(O) in(O) 1924(O) the(O) League(B-organization) of(I-organization) Nations(I-organization) mandated(O) them(O) to(O) Belgium(B-country) .(O)"}}
{"id": "547", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "country", "politician", "election", "event", "person", "organization", "location"], "instance": {"id": "547", "words": ["In", "addition", "to", "his", "involvement", "in", "J-PAL", ",", "Olken", "is", "affiliated", "with", "several", "other", "economic", "institutions", ",", "including", "the", "Bureau", "for", "Research", "and", "Economic", "Analysis", "of", "Development", "(", "BREAD", ")", ",", "the", "International", "Growth", "Centre", ",", "the", "Centre", "for", "Economic", "Policy", "Research", "(", "CEPR", ")", ",", "and", "the", "National", "Bureau", "of", "Economic", "Research", "(", "NBER", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, politician, election, event, person, organization, location and O.\nSentence: In addition to his involvement in J-PAL , Olken is affiliated with several other economic institutions , including the Bureau for Research and Economic Analysis of Development ( BREAD ) , the International Growth Centre , the Centre for Economic Policy Research ( CEPR ) , and the National Bureau of Economic Research ( NBER ) .", "prompt_labels": "In(O) addition(O) to(O) his(O) involvement(O) in(O) J-PAL(B-organization) ,(O) Olken(B-person) is(O) affiliated(O) with(O) several(O) other(O) economic(O) institutions(O) ,(O) including(O) the(O) Bureau(B-organization) for(I-organization) Research(I-organization) and(I-organization) Economic(I-organization) Analysis(I-organization) of(I-organization) Development(I-organization) ((O) BREAD(B-organization) )(O) ,(O) the(O) International(B-organization) Growth(I-organization) Centre(I-organization) ,(O) the(O) Centre(B-organization) for(I-organization) Economic(I-organization) Policy(I-organization) Research(I-organization) ((O) CEPR(B-organization) )(O) ,(O) and(O) the(O) National(B-organization) Bureau(I-organization) of(I-organization) Economic(I-organization) Research(I-organization) ((O) NBER(B-organization) )(O) .(O)"}}
{"id": "372", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "political party", "organization", "election", "event", "person", "politician", "location"], "instance": {"id": "372", "words": ["The", "Eleventh", "Amendment", "was", "introduced", "by", "a", "Fianna", "Fáil", "-", "Progressive", "Democrats", "coalition", "government", "and", "was", "also", "supported", "by", "opposition", "parties", "Fine", "Gael", "and", "the", "Labour", "Party", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, organization, election, event, person, politician, location and O.\nSentence: The Eleventh Amendment was introduced by a Fianna Fáil - Progressive Democrats coalition government and was also supported by opposition parties Fine Gael and the Labour Party .", "prompt_labels": "The(O) Eleventh(O) Amendment(O) was(O) introduced(O) by(O) a(O) Fianna(B-political party) Fáil(I-political party) -(O) Progressive(B-political party) Democrats(I-political party) coalition(O) government(O) and(O) was(O) also(O) supported(O) by(O) opposition(O) parties(O) Fine(B-political party) Gael(I-political party) and(O) the(O) Labour(B-political party) Party(I-political party) .(O)"}}
{"id": "290", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "location", "country", "organization", "politician", "person", "election", "political party"], "instance": {"id": "290", "words": ["Redmond", "was", "an", "excellent", "representative", "of", "the", "old", "Ireland", ",", "but", "grew", "increasingly", "out-dated", "as", "he", "paid", "little", "attention", "to", "the", "new", "forces", "attracting", "younger", "Irishman", ",", "such", "as", "Sinn", "Féin", ",", "and", "the", "Ancient", "Order", "of", "Hibernians", "in", "politics", ",", "the", "Gaelic", "Athletic", "Association", "in", "sports", ",", "and", "the", "Gaelic", "League", "in", "cultural", "affairs", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, country, organization, politician, person, election, political party and O.\nSentence: Redmond was an excellent representative of the old Ireland , but grew increasingly out-dated as he paid little attention to the new forces attracting younger Irishman , such as Sinn Féin , and the Ancient Order of Hibernians in politics , the Gaelic Athletic Association in sports , and the Gaelic League in cultural affairs .", "prompt_labels": "Redmond(B-politician) was(O) an(O) excellent(O) representative(O) of(O) the(O) old(O) Ireland(B-country) ,(O) but(O) grew(O) increasingly(O) out-dated(O) as(O) he(O) paid(O) little(O) attention(O) to(O) the(O) new(O) forces(O) attracting(O) younger(O) Irishman(O) ,(O) such(O) as(O) Sinn(B-political party) Féin(I-political party) ,(O) and(O) the(O) Ancient(B-organization) Order(I-organization) of(I-organization) Hibernians(I-organization) in(O) politics(O) ,(O) the(O) Gaelic(B-organization) Athletic(I-organization) Association(I-organization) in(O) sports(O) ,(O) and(O) the(O) Gaelic(B-organization) League(I-organization) in(O) cultural(O) affairs(O) .(O)"}}
{"id": "60", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "politician", "country", "location", "election", "organization", "person", "event"], "instance": {"id": "60", "words": ["Abbas", "II", "Helmy", "Bey", "(", "also", "known", "as", "Abbās", "Ḥilmī", "Pasha", ",", "In", "1914", ",", "after", "the", "Ottoman", "Empire", "joined", "the", "Central", "Powers", "in", "World", "War", "I", ",", "the", "nationalist", "Khedive", "was", "removed", "by", "the", "British", ",", "then", "ruling", "Egypt", ",", "in", "favor", "of", "his", "more", "pro-British", "uncle", ",", "Hussein", "Kamel", ",", "marking", "the", "de", "jure", "end", "of", "Egypt", "'s", "four-century", "era", "as", "a", "province", "of", "the", "Ottoman", "Empire", ",", "which", "had", "begun", "in", "1517", "."], "labels": ["B-person", "I-person", "I-person", "I-person", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "B-country", "I-country", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, country, location, election, organization, person, event and O.\nSentence: Abbas II Helmy Bey ( also known as Abbās Ḥilmī Pasha , In 1914 , after the Ottoman Empire joined the Central Powers in World War I , the nationalist Khedive was removed by the British , then ruling Egypt , in favor of his more pro-British uncle , Hussein Kamel , marking the de jure end of Egypt 's four-century era as a province of the Ottoman Empire , which had begun in 1517 .", "prompt_labels": "Abbas(B-person) II(I-person) Helmy(I-person) Bey(I-person) ((O) also(O) known(O) as(O) Abbās(B-person) Ḥilmī(I-person) Pasha(I-person) ,(O) In(O) 1914(O) ,(O) after(O) the(O) Ottoman(B-country) Empire(I-country) joined(O) the(O) Central(B-country) Powers(I-country) in(O) World(B-event) War(I-event) I(I-event) ,(O) the(O) nationalist(O) Khedive(O) was(O) removed(O) by(O) the(O) British(O) ,(O) then(O) ruling(O) Egypt(B-country) ,(O) in(O) favor(O) of(O) his(O) more(O) pro-British(O) uncle(O) ,(O) Hussein(B-politician) Kamel(I-politician) ,(O) marking(O) the(O) de(O) jure(O) end(O) of(O) Egypt(B-country) 's(O) four-century(O) era(O) as(O) a(O) province(O) of(O) the(O) Ottoman(B-country) Empire(I-country) ,(O) which(O) had(O) begun(O) in(O) 1517(O) .(O)"}}
{"id": "185", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "location", "event", "political party", "organization", "election", "person", "country"], "instance": {"id": "185", "words": ["He", "was", "educated", "at", "Congregation", "of", "Christian", "Brothers", "in", "Synge", "Street", ",", "Dublin", ";", "University", "College", "Dublin", "and", "Harvard", "Business", "School", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-location", "I-location", "O", "B-location", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, event, political party, organization, election, person, country and O.\nSentence: He was educated at Congregation of Christian Brothers in Synge Street , Dublin ; University College Dublin and Harvard Business School .", "prompt_labels": "He(O) was(O) educated(O) at(O) Congregation(B-organization) of(I-organization) Christian(I-organization) Brothers(I-organization) in(O) Synge(B-location) Street(I-location) ,(O) Dublin(B-location) ;(O) University(B-organization) College(I-organization) Dublin(I-organization) and(O) Harvard(B-organization) Business(I-organization) School(I-organization) .(O)"}}
{"id": "150", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "country", "organization", "political party", "location", "election", "person", "politician"], "instance": {"id": "150", "words": ["The", "Anti-Defamation", "League", "has", "accused", "ANSWER", "of", "supporting", "terrorist", "organizations", ",", "such", "as", "Hezbollah", "and", "Hamas", "."], "labels": ["O", "B-organization", "I-organization", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "B-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, organization, political party, location, election, person, politician and O.\nSentence: The Anti-Defamation League has accused ANSWER of supporting terrorist organizations , such as Hezbollah and Hamas .", "prompt_labels": "The(O) Anti-Defamation(B-organization) League(I-organization) has(O) accused(O) ANSWER(B-organization) of(O) supporting(O) terrorist(O) organizations(O) ,(O) such(O) as(O) Hezbollah(B-political party) and(O) Hamas(B-political party) .(O)"}}
{"id": "192", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "political party", "location", "event", "person", "country", "politician", "election"], "instance": {"id": "192", "words": ["ALP", "=", "Australian", "Labor", "Party", ",", "L", "+", "NP", "=", "grouping", "of", "Liberal", "Party", "of", "Australia", "/", "National", "Party", "of", "Australia", "/", "Liberal", "National", "Party", "of", "Queensland", "/", "Country", "Liberal", "Party", "Coalition", "parties", "(", "and", "predecessors", ")", "."], "labels": ["B-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, location, event, person, country, politician, election and O.\nSentence: ALP = Australian Labor Party , L + NP = grouping of Liberal Party of Australia / National Party of Australia / Liberal National Party of Queensland / Country Liberal Party Coalition parties ( and predecessors ) .", "prompt_labels": "ALP(B-political party) =(O) Australian(B-political party) Labor(I-political party) Party(I-political party) ,(O) L(O) +(O) NP(O) =(O) grouping(O) of(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) /(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) /(O) Liberal(B-political party) National(I-political party) Party(I-political party) of(I-political party) Queensland(I-political party) /(O) Country(B-political party) Liberal(I-political party) Party(I-political party) Coalition(O) parties(O) ((O) and(O) predecessors(O) )(O) .(O)"}}
{"id": "144", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "politician", "organization", "country", "political party", "event", "election", "location"], "instance": {"id": "144", "words": ["The", "Communists", "won", "West", "Fife", "in", "1935", "United", "Kingdom", "general", "election", "and", "again", "in", "1945", "United", "Kingdom", "general", "election", "(", "Willie", "Gallacher", ")", "and", "several", "Glasgow", "Labour", "MPs", "joined", "the", "Independent", "Labour", "Party", "in", "the", "1930s", ",", "often", "heavily", "defeating", "the", "official", "Labour", "candidates", "."], "labels": ["O", "O", "O", "B-location", "I-location", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-politician", "I-politician", "O", "O", "O", "B-location", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, organization, country, political party, event, election, location and O.\nSentence: The Communists won West Fife in 1935 United Kingdom general election and again in 1945 United Kingdom general election ( Willie Gallacher ) and several Glasgow Labour MPs joined the Independent Labour Party in the 1930s , often heavily defeating the official Labour candidates .", "prompt_labels": "The(O) Communists(O) won(O) West(B-location) Fife(I-location) in(O) 1935(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) again(O) in(O) 1945(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ((O) Willie(B-politician) Gallacher(I-politician) )(O) and(O) several(O) Glasgow(B-location) Labour(B-political party) MPs(O) joined(O) the(O) Independent(B-political party) Labour(I-political party) Party(I-political party) in(O) the(O) 1930s(O) ,(O) often(O) heavily(O) defeating(O) the(O) official(O) Labour(B-political party) candidates(O) .(O)"}}
{"id": "116", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "organization", "politician", "location", "election", "event", "political party", "person"], "instance": {"id": "116", "words": ["The", "Liberal", "Democrats", "were", "the", "third", "largest", "party", "until", "the", "2015", "United", "Kingdom", "general", "election", "when", "they", "were", "overtaken", "by", "the", "Scottish", "National", "Party", "in", "terms", "of", "seats", "and", "UK", "political", "party", "membership", ",", "and", "by", "the", "UK", "Independence", "Party", "in", "terms", "of", "votes", "."], "labels": ["O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, politician, location, election, event, political party, person and O.\nSentence: The Liberal Democrats were the third largest party until the 2015 United Kingdom general election when they were overtaken by the Scottish National Party in terms of seats and UK political party membership , and by the UK Independence Party in terms of votes .", "prompt_labels": "The(O) Liberal(B-political party) Democrats(I-political party) were(O) the(O) third(O) largest(O) party(O) until(O) the(O) 2015(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) when(O) they(O) were(O) overtaken(O) by(O) the(O) Scottish(B-political party) National(I-political party) Party(I-political party) in(O) terms(O) of(O) seats(O) and(O) UK(O) political(O) party(O) membership(O) ,(O) and(O) by(O) the(O) UK(B-political party) Independence(I-political party) Party(I-political party) in(O) terms(O) of(O) votes(O) .(O)"}}
{"id": "267", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "politician", "country", "event", "election", "organization", "location", "person"], "instance": {"id": "267", "words": ["He", "went", "from", "place", "to", "place", ",", "in", "danger", "of", "his", "life", ",", "denouncing", "the", "errors", "of", "the", "Papacy", "and", "the", "abuses", "in", "the", "churches", "of", "Montrose", ",", "Dundee", "(", "where", "he", "escaped", "an", "attempt", "on", "his", "life", ")", ",", "Ayr", ",", "Perth", ",", "Edinburgh", ",", "Leith", ",", "Haddington", "(", "where", "Knox", "accompanied", "him", ")", "and", "elsewhere", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "O", "B-person", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, country, event, election, organization, location, person and O.\nSentence: He went from place to place , in danger of his life , denouncing the errors of the Papacy and the abuses in the churches of Montrose , Dundee ( where he escaped an attempt on his life ) , Ayr , Perth , Edinburgh , Leith , Haddington ( where Knox accompanied him ) and elsewhere .", "prompt_labels": "He(O) went(O) from(O) place(O) to(O) place(O) ,(O) in(O) danger(O) of(O) his(O) life(O) ,(O) denouncing(O) the(O) errors(O) of(O) the(O) Papacy(O) and(O) the(O) abuses(O) in(O) the(O) churches(O) of(O) Montrose(B-location) ,(O) Dundee(B-location) ((O) where(O) he(O) escaped(O) an(O) attempt(O) on(O) his(O) life(O) )(O) ,(O) Ayr(B-location) ,(O) Perth(B-location) ,(O) Edinburgh(B-location) ,(O) Leith(B-location) ,(O) Haddington(B-location) ((O) where(O) Knox(B-person) accompanied(O) him(O) )(O) and(O) elsewhere(O) .(O)"}}
{"id": "134", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "organization", "country", "person", "location", "election", "politician", "event"], "instance": {"id": "134", "words": ["A", "February", "1974", "United", "Kingdom", "general", "election", "at", "the", "end", "of", "February", "was", "a", "triumph", "for", "the", "United", "Ulster", "Unionist", "Coalition", ",", "in", "which", "the", "bulk", "of", "his", "old", "party", "stood", "as", "Official", "Unionists", "with", "William", "Craig", "'s", "Vanguard", "Unionist", "Progressive", "Party", "and", "Paisley", "'s", "new", "Democratic", "Unionist", "Party", "."], "labels": ["O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "O", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, country, person, location, election, politician, event and O.\nSentence: A February 1974 United Kingdom general election at the end of February was a triumph for the United Ulster Unionist Coalition , in which the bulk of his old party stood as Official Unionists with William Craig 's Vanguard Unionist Progressive Party and Paisley 's new Democratic Unionist Party .", "prompt_labels": "A(O) February(B-election) 1974(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) at(O) the(O) end(O) of(O) February(O) was(O) a(O) triumph(O) for(O) the(O) United(B-political party) Ulster(I-political party) Unionist(I-political party) Coalition(I-political party) ,(O) in(O) which(O) the(O) bulk(O) of(O) his(O) old(O) party(O) stood(O) as(O) Official(O) Unionists(O) with(O) William(B-politician) Craig(I-politician) 's(O) Vanguard(B-political party) Unionist(I-political party) Progressive(I-political party) Party(I-political party) and(O) Paisley(B-politician) 's(O) new(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) .(O)"}}
{"id": "232", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "organization", "political party", "country", "politician", "event", "election", "person"], "instance": {"id": "232", "words": ["Operations", "started", "in", "July", "1933", ",", "initially", "linking", "Cairo", "with", "Alexandria", "and", "Mersa", "Matruh", "using", "de", "Havilland", "DH.84", "Dragon", "equipment", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, political party, country, politician, event, election, person and O.\nSentence: Operations started in July 1933 , initially linking Cairo with Alexandria and Mersa Matruh using de Havilland DH.84 Dragon equipment .", "prompt_labels": "Operations(O) started(O) in(O) July(O) 1933(O) ,(O) initially(O) linking(O) Cairo(B-location) with(O) Alexandria(B-location) and(O) Mersa(B-location) Matruh(I-location) using(O) de(O) Havilland(O) DH.84(O) Dragon(O) equipment(O) .(O)"}}
{"id": "6", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "person", "location", "event", "political party", "country", "organization", "election"], "instance": {"id": "6", "words": ["He", "invited", "the", "chieftains", "of", "the", "former", "satrapy", "of", "Gandhara", "(", "a", "region", "presently", "straddling", "eastern", "Afghanistan", "and", "northern", "Pakistan", ")", ",", "to", "come", "to", "him", "and", "submit", "to", "his", "authority", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, location, event, political party, country, organization, election and O.\nSentence: He invited the chieftains of the former satrapy of Gandhara ( a region presently straddling eastern Afghanistan and northern Pakistan ) , to come to him and submit to his authority .", "prompt_labels": "He(O) invited(O) the(O) chieftains(O) of(O) the(O) former(O) satrapy(O) of(O) Gandhara(B-country) ((O) a(O) region(O) presently(O) straddling(O) eastern(O) Afghanistan(B-country) and(O) northern(O) Pakistan(B-country) )(O) ,(O) to(O) come(O) to(O) him(O) and(O) submit(O) to(O) his(O) authority(O) .(O)"}}
{"id": "121", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "politician", "person", "political party", "country", "organization", "location", "election"], "instance": {"id": "121", "words": ["YIP", "had", "chapters", "all", "over", "the", "US", "and", "in", "other", "countries", ",", "with", "particularly", "active", "groups", "in", "New", "York", "City", ",", "Vancouver", ",", "Washington", "D.C.", ",", "Detroit", ",", "Milwaukee", ",", "Los", "Angeles", ",", "Tucson", ",", "Houston", ",", "Austin", ",", "Columbus", ",", "Dayton", ",", "Chicago", ",", "Berkeley", ",", "San", "Francisco", "and", "Madison", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "B-location", "I-location", "O", "B-location", "O", "B-location", "O", "B-location", "I-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "I-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, person, political party, country, organization, location, election and O.\nSentence: YIP had chapters all over the US and in other countries , with particularly active groups in New York City , Vancouver , Washington D.C. , Detroit , Milwaukee , Los Angeles , Tucson , Houston , Austin , Columbus , Dayton , Chicago , Berkeley , San Francisco and Madison .", "prompt_labels": "YIP(B-person) had(O) chapters(O) all(O) over(O) the(O) US(B-country) and(O) in(O) other(O) countries(O) ,(O) with(O) particularly(O) active(O) groups(O) in(O) New(B-location) York(I-location) City(I-location) ,(O) Vancouver(B-location) ,(O) Washington(B-location) D.C.(I-location) ,(O) Detroit(B-location) ,(O) Milwaukee(B-location) ,(O) Los(B-location) Angeles(I-location) ,(O) Tucson(B-location) ,(O) Houston(B-location) ,(O) Austin(B-location) ,(O) Columbus(B-location) ,(O) Dayton(B-location) ,(O) Chicago(B-location) ,(O) Berkeley(B-location) ,(O) San(B-location) Francisco(I-location) and(O) Madison(B-location) .(O)"}}
{"id": "83", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "election", "location", "event", "person", "political party", "country", "politician"], "instance": {"id": "83", "words": ["On", "the", "Coalition", "side", "of", "politics", ",", "Pru", "Goward", "has", "served", "as", "a", "Minister", "in", "the", "NSW", "state", "Liberal", "Government", ",", "Scott", "Emerson", "and", "Sarah", "Henderson", "all", "held", ",", "or", "hold", ",", "positions", "at", "the", "ABC", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, location, event, person, political party, country, politician and O.\nSentence: On the Coalition side of politics , Pru Goward has served as a Minister in the NSW state Liberal Government , Scott Emerson and Sarah Henderson all held , or hold , positions at the ABC .", "prompt_labels": "On(O) the(O) Coalition(O) side(O) of(O) politics(O) ,(O) Pru(B-politician) Goward(I-politician) has(O) served(O) as(O) a(O) Minister(O) in(O) the(O) NSW(B-political party) state(I-political party) Liberal(I-political party) Government(O) ,(O) Scott(B-politician) Emerson(I-politician) and(O) Sarah(B-politician) Henderson(I-politician) all(O) held(O) ,(O) or(O) hold(O) ,(O) positions(O) at(O) the(O) ABC(B-organization) .(O)"}}
{"id": "542", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "political party", "location", "election", "country", "event", "person", "organization"], "instance": {"id": "542", "words": ["Krasinski", "also", "stars", "in", "the", "series", "as", "the", "title", "character", ",", "making", "him", "the", "fifth", "actor", "to", "portray", "the", "character", "after", "Alec", "Baldwin", ",", "Harrison", "Ford", ",", "Ben", "Affleck", ",", "and", "Chris", "Pine", "from", "the", "film", "series", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, location, election, country, event, person, organization and O.\nSentence: Krasinski also stars in the series as the title character , making him the fifth actor to portray the character after Alec Baldwin , Harrison Ford , Ben Affleck , and Chris Pine from the film series .", "prompt_labels": "Krasinski(B-person) also(O) stars(O) in(O) the(O) series(O) as(O) the(O) title(O) character(O) ,(O) making(O) him(O) the(O) fifth(O) actor(O) to(O) portray(O) the(O) character(O) after(O) Alec(B-person) Baldwin(I-person) ,(O) Harrison(B-person) Ford(I-person) ,(O) Ben(B-person) Affleck(I-person) ,(O) and(O) Chris(B-person) Pine(I-person) from(O) the(O) film(O) series(O) .(O)"}}
{"id": "278", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "political party", "location", "politician", "event", "country", "election", "organization"], "instance": {"id": "278", "words": ["Having", "joined", "the", "New", "Zealand", "National", "Party", "in", "1975", ",", "Shipley", "successfully", "stood", "in", "Ashburton", ",", "a", "safe", "National", "seat", "in", "the", "country", "areas", "surrounding", "Christchurch", ",", "in", "the", "1987", "New", "Zealand", "general", "election", "."], "labels": ["O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, location, politician, event, country, election, organization and O.\nSentence: Having joined the New Zealand National Party in 1975 , Shipley successfully stood in Ashburton , a safe National seat in the country areas surrounding Christchurch , in the 1987 New Zealand general election .", "prompt_labels": "Having(O) joined(O) the(O) New(B-political party) Zealand(I-political party) National(I-political party) Party(I-political party) in(O) 1975(O) ,(O) Shipley(O) successfully(O) stood(O) in(O) Ashburton(B-location) ,(O) a(O) safe(O) National(O) seat(O) in(O) the(O) country(O) areas(O) surrounding(O) Christchurch(B-location) ,(O) in(O) the(O) 1987(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "159", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "event", "election", "political party", "person", "country", "politician", "organization"], "instance": {"id": "159", "words": ["The", "PSOE", ",", "UP", ",", "En", "Comú", "Podem", ",", "Grupo", "Común", "da", "Esquerda", ",", "PNV", ",", "Más", "País", ",", "Compromís", ",", "NCa", ",", "the", "Galician", "Nationalist", "Bloc", "(", "BNG", ")", "and", "Teruel", "Existe", "(", "TE", ")", "voting", "in", "favor", "of", "the", "government", "."], "labels": ["O", "B-political party", "O", "B-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "B-political party", "I-political party", "O", "B-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, election, political party, person, country, politician, organization and O.\nSentence: The PSOE , UP , En Comú Podem , Grupo Común da Esquerda , PNV , Más País , Compromís , NCa , the Galician Nationalist Bloc ( BNG ) and Teruel Existe ( TE ) voting in favor of the government .", "prompt_labels": "The(O) PSOE(B-political party) ,(O) UP(B-political party) ,(O) En(B-political party) Comú(I-political party) Podem(I-political party) ,(O) Grupo(B-political party) Común(I-political party) da(I-political party) Esquerda(I-political party) ,(O) PNV(B-political party) ,(O) Más(B-political party) País(I-political party) ,(O) Compromís(B-political party) ,(O) NCa(B-political party) ,(O) the(O) Galician(B-political party) Nationalist(I-political party) Bloc(I-political party) ((O) BNG(B-political party) )(O) and(O) Teruel(B-political party) Existe(I-political party) ((O) TE(B-political party) )(O) voting(O) in(O) favor(O) of(O) the(O) government(O) .(O)"}}
{"id": "525", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "person", "country", "politician", "location", "political party", "organization", "election"], "instance": {"id": "525", "words": ["Mayor", "Hylan", ",", "an", "ally", "of", "the", "newspaper", "publisher", "William", "Randolph", "Hearst", ",", "was", "unseated", "in", "a", "venomous", "Democratic", "primary", "by", "Gentleman", "Jimmy", "Walker", ",", "the", "Democratic", "party", "leader", "in", "the", "New", "York", "State", "Senate", ",", "who", "had", "been", "recruited", "to", "oppose", "Hylan", "by", "Hearst", "'s", "inveterate", "enemy", ",", "Democratic", "Governor", "Al", "Smith", "."], "labels": ["O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "B-politician", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, country, politician, location, political party, organization, election and O.\nSentence: Mayor Hylan , an ally of the newspaper publisher William Randolph Hearst , was unseated in a venomous Democratic primary by Gentleman Jimmy Walker , the Democratic party leader in the New York State Senate , who had been recruited to oppose Hylan by Hearst 's inveterate enemy , Democratic Governor Al Smith .", "prompt_labels": "Mayor(O) Hylan(B-politician) ,(O) an(O) ally(O) of(O) the(O) newspaper(O) publisher(O) William(B-politician) Randolph(I-politician) Hearst(I-politician) ,(O) was(O) unseated(O) in(O) a(O) venomous(O) Democratic(O) primary(O) by(O) Gentleman(O) Jimmy(B-politician) Walker(I-politician) ,(O) the(O) Democratic(B-political party) party(I-political party) leader(O) in(O) the(O) New(B-organization) York(I-organization) State(I-organization) Senate(I-organization) ,(O) who(O) had(O) been(O) recruited(O) to(O) oppose(O) Hylan(B-politician) by(O) Hearst(B-organization) 's(O) inveterate(O) enemy(O) ,(O) Democratic(O) Governor(O) Al(B-politician) Smith(I-politician) .(O)"}}
{"id": "165", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "political party", "organization", "person", "election", "location", "politician", "country"], "instance": {"id": "165", "words": ["While", "Nemanja", "'s", "envoys", "were", "negotiating", "with", "Berthold", "as", "Barbarossa", "'s", "representative", ",", "Nemanja", "took", "Pernik", ",", "Zemen", ",", "Velbužd", ",", "Žitomisk", ",", "Stobi", ",", "Prizren", "and", "rest", "of", "Kosovo", "and", "Metohija", "and", "even", "Skopje", "."], "labels": ["O", "B-politician", "O", "O", "O", "O", "O", "B-politician", "O", "B-politician", "O", "O", "O", "B-politician", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "O", "O", "B-country", "O", "B-location", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, organization, person, election, location, politician, country and O.\nSentence: While Nemanja 's envoys were negotiating with Berthold as Barbarossa 's representative , Nemanja took Pernik , Zemen , Velbužd , Žitomisk , Stobi , Prizren and rest of Kosovo and Metohija and even Skopje .", "prompt_labels": "While(O) Nemanja(B-politician) 's(O) envoys(O) were(O) negotiating(O) with(O) Berthold(B-politician) as(O) Barbarossa(B-politician) 's(O) representative(O) ,(O) Nemanja(B-politician) took(O) Pernik(B-location) ,(O) Zemen(B-location) ,(O) Velbužd(B-location) ,(O) Žitomisk(B-location) ,(O) Stobi(B-location) ,(O) Prizren(B-location) and(O) rest(O) of(O) Kosovo(B-country) and(O) Metohija(B-location) and(O) even(O) Skopje(B-location) .(O)"}}
{"id": "284", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "organization", "location", "event", "person", "election", "country", "political party"], "instance": {"id": "284", "words": ["Other", "targets", "included", "Birmingham", "and", "Coventry", ",", "and", "strategically", "important", "cities", ",", "such", "as", "the", "naval", "base", "at", "Plymouth", "and", "the", "port", "of", "Kingston", "upon", "Hull", "."], "labels": ["O", "O", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, location, event, person, election, country, political party and O.\nSentence: Other targets included Birmingham and Coventry , and strategically important cities , such as the naval base at Plymouth and the port of Kingston upon Hull .", "prompt_labels": "Other(O) targets(O) included(O) Birmingham(B-location) and(O) Coventry(B-location) ,(O) and(O) strategically(O) important(O) cities(O) ,(O) such(O) as(O) the(O) naval(O) base(O) at(O) Plymouth(B-location) and(O) the(O) port(O) of(O) Kingston(B-location) upon(I-location) Hull(I-location) .(O)"}}
{"id": "531", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "politician", "country", "person", "election", "political party", "location", "event"], "instance": {"id": "531", "words": ["The", "manifesto", "has", "been", "signed", "by", "several", "personalities", "of", "international", "relevance", ",", "which", "include", "the", "following", "five", "Nobel", "Prize", "winners", ":", "Rigoberta", "Menchú", ",", "Desmond", "Tutu", ",", "Adolfo", "Pérez", "Esquivel", ",", "Dario", "Fo", "and", "Jody", "Williams", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, country, person, election, political party, location, event and O.\nSentence: The manifesto has been signed by several personalities of international relevance , which include the following five Nobel Prize winners : Rigoberta Menchú , Desmond Tutu , Adolfo Pérez Esquivel , Dario Fo and Jody Williams .", "prompt_labels": "The(O) manifesto(O) has(O) been(O) signed(O) by(O) several(O) personalities(O) of(O) international(O) relevance(O) ,(O) which(O) include(O) the(O) following(O) five(O) Nobel(O) Prize(O) winners(O) :(O) Rigoberta(B-person) Menchú(I-person) ,(O) Desmond(B-person) Tutu(I-person) ,(O) Adolfo(B-person) Pérez(I-person) Esquivel(I-person) ,(O) Dario(B-person) Fo(I-person) and(O) Jody(B-person) Williams(I-person) .(O)"}}
{"id": "309", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "organization", "event", "location", "country", "election", "person", "politician"], "instance": {"id": "309", "words": ["The", "party", "was", "founded", "on", "5", "January", "1930", "by", "the", "merger", "of", "the", "Hapoel", "Hatzair", "founded", "by", "A.", "D.", "Gordon", "and", "the", "original", "Ahdut", "HaAvoda", "(", "founded", "in", "1919", "from", "the", "right", ",", "more", "moderate", ",", "wing", "of", "the", "Zionist", "socialist", "Poale", "Zion", "led", "by", "David", "Ben-Gurion", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "B-politician", "I-politician", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, event, location, country, election, person, politician and O.\nSentence: The party was founded on 5 January 1930 by the merger of the Hapoel Hatzair founded by A. D. Gordon and the original Ahdut HaAvoda ( founded in 1919 from the right , more moderate , wing of the Zionist socialist Poale Zion led by David Ben-Gurion ) .", "prompt_labels": "The(O) party(O) was(O) founded(O) on(O) 5(O) January(O) 1930(O) by(O) the(O) merger(O) of(O) the(O) Hapoel(B-political party) Hatzair(I-political party) founded(O) by(O) A.(B-politician) D.(I-politician) Gordon(I-politician) and(O) the(O) original(O) Ahdut(B-political party) HaAvoda(I-political party) ((O) founded(O) in(O) 1919(O) from(O) the(O) right(O) ,(O) more(O) moderate(O) ,(O) wing(O) of(O) the(O) Zionist(O) socialist(O) Poale(B-event) Zion(I-event) led(O) by(O) David(B-politician) Ben-Gurion(I-politician) )(O) .(O)"}}
{"id": "254", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "organization", "political party", "location", "politician", "event", "country", "person"], "instance": {"id": "254", "words": ["Examples", "of", "major", "Christian", "democratic", "parties", "include", "the", "Christian", "Democratic", "Union", "of", "Germany", ",", "the", "Austrian", "People", "'s", "Party", ",", "Ireland", "'s", "Fine", "Gael", ",", "the", "Christian", "Democratic", "Party", "of", "Chile", ",", "the", "Aruban", "People", "'s", "Party", ",", "the", "Dutch", "Christian", "Democratic", "Appeal", ",", "the", "Christian", "Democratic", "People", "'s", "Party", "of", "Switzerland", "and", "the", "Spanish", "People", "'s", "Party", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-country", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, political party, location, politician, event, country, person and O.\nSentence: Examples of major Christian democratic parties include the Christian Democratic Union of Germany , the Austrian People 's Party , Ireland 's Fine Gael , the Christian Democratic Party of Chile , the Aruban People 's Party , the Dutch Christian Democratic Appeal , the Christian Democratic People 's Party of Switzerland and the Spanish People 's Party .", "prompt_labels": "Examples(O) of(O) major(O) Christian(O) democratic(O) parties(O) include(O) the(O) Christian(B-political party) Democratic(I-political party) Union(I-political party) of(I-political party) Germany(I-political party) ,(O) the(O) Austrian(B-political party) People(I-political party) 's(I-political party) Party(I-political party) ,(O) Ireland(B-country) 's(O) Fine(B-political party) Gael(I-political party) ,(O) the(O) Christian(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Chile(I-political party) ,(O) the(O) Aruban(B-political party) People(I-political party) 's(I-political party) Party(I-political party) ,(O) the(O) Dutch(O) Christian(B-political party) Democratic(I-political party) Appeal(I-political party) ,(O) the(O) Christian(B-political party) Democratic(I-political party) People(I-political party) 's(I-political party) Party(I-political party) of(I-political party) Switzerland(I-political party) and(O) the(O) Spanish(O) People(B-political party) 's(I-political party) Party(I-political party) .(O)"}}
{"id": "236", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "country", "person", "politician", "location", "political party", "election", "organization"], "instance": {"id": "236", "words": ["Gazprom", "is", "listed", "on", "the", "stock", "markets", "of", "Moscow", ",", "London", ",", "Karachi", ",", "Berlin", ",", "Frankfurt", "and", "Singapore", "."], "labels": ["B-country", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, person, politician, location, political party, election, organization and O.\nSentence: Gazprom is listed on the stock markets of Moscow , London , Karachi , Berlin , Frankfurt and Singapore .", "prompt_labels": "Gazprom(B-country) is(O) listed(O) on(O) the(O) stock(O) markets(O) of(O) Moscow(B-location) ,(O) London(B-location) ,(O) Karachi(B-location) ,(O) Berlin(B-location) ,(O) Frankfurt(B-location) and(O) Singapore(B-country) .(O)"}}
{"id": "489", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "event", "person", "organization", "country", "location", "political party", "politician"], "instance": {"id": "489", "words": ["He", "ran", "for", "a", "seat", "in", "the", "National", "Assembly", "of", "Quebec", "in", "the", "Montreal-based", "district", "of", "Mercier", "against", "Daniel", "Turp", "of", "the", "Parti", "Québécois", "in", "the", "2003", "Quebec", "general", "election", ",", "2007", "Quebec", "general", "election", "and", "2008", "Quebec", "general", "election", "elections", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-location", "I-location", "O", "B-location", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, person, organization, country, location, political party, politician and O.\nSentence: He ran for a seat in the National Assembly of Quebec in the Montreal-based district of Mercier against Daniel Turp of the Parti Québécois in the 2003 Quebec general election , 2007 Quebec general election and 2008 Quebec general election elections .", "prompt_labels": "He(O) ran(O) for(O) a(O) seat(O) in(O) the(O) National(B-organization) Assembly(I-organization) of(I-organization) Quebec(I-organization) in(O) the(O) Montreal-based(B-location) district(I-location) of(O) Mercier(B-location) against(O) Daniel(B-politician) Turp(I-politician) of(O) the(O) Parti(B-political party) Québécois(I-political party) in(O) the(O) 2003(B-election) Quebec(I-election) general(I-election) election(I-election) ,(O) 2007(B-election) Quebec(I-election) general(I-election) election(I-election) and(O) 2008(B-election) Quebec(I-election) general(I-election) election(I-election) elections(O) .(O)"}}
{"id": "637", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "person", "location", "event", "politician", "country", "organization", "election"], "instance": {"id": "637", "words": ["According", "to", "the", "Mueller", "Report", ",", "the", "second", "method", "of", "Russian", "interference", "saw", "the", "Russian", "intelligence", "service", ",", "the", "GRU", ",", "hacking", "into", "email", "accounts", "owned", "by", "volunteers", "and", "employees", "of", "the", "Clinton", "presidential", "campaign", ",", "including", "that", "of", "campaign", "chairman", "John", "Podesta", ",", "and", "also", "hacking", "into", "the", "computer", "networks", "of", "the", "Democratic", "Congressional", "Campaign", "Committee", "(", "DCCC", ")", "and", "the", "Democratic", "National", "Committee", "(", "DNC", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, location, event, politician, country, organization, election and O.\nSentence: According to the Mueller Report , the second method of Russian interference saw the Russian intelligence service , the GRU , hacking into email accounts owned by volunteers and employees of the Clinton presidential campaign , including that of campaign chairman John Podesta , and also hacking into the computer networks of the Democratic Congressional Campaign Committee ( DCCC ) and the Democratic National Committee ( DNC ) .", "prompt_labels": "According(O) to(O) the(O) Mueller(O) Report(O) ,(O) the(O) second(O) method(O) of(O) Russian(O) interference(O) saw(O) the(O) Russian(B-organization) intelligence(I-organization) service(I-organization) ,(O) the(O) GRU(B-organization) ,(O) hacking(O) into(O) email(O) accounts(O) owned(O) by(O) volunteers(O) and(O) employees(O) of(O) the(O) Clinton(B-event) presidential(I-event) campaign(I-event) ,(O) including(O) that(O) of(O) campaign(O) chairman(O) John(B-politician) Podesta(I-politician) ,(O) and(O) also(O) hacking(O) into(O) the(O) computer(O) networks(O) of(O) the(O) Democratic(B-organization) Congressional(I-organization) Campaign(I-organization) Committee(I-organization) ((O) DCCC(B-organization) )(O) and(O) the(O) Democratic(B-organization) National(I-organization) Committee(I-organization) ((O) DNC(B-organization) )(O) .(O)"}}
{"id": "94", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "political party", "event", "election", "location", "organization", "country", "person"], "instance": {"id": "94", "words": ["In", "the", "1996", "United", "States", "presidential", "election", ",", "Clinton", "was", "re-elected", ",", "receiving", "49.2", "percent", "of", "the", "popular", "vote", "over", "Republican", "Bob", "Dole", "(", "40.7", "percent", "of", "the", "popular", "vote", ")", "and", "Reform", "Party", "of", "the", "United", "States", "of", "America", "candidate", "Ross", "Perot", "(", "8.4", "percent", "of", "the", "popular", "vote", ")", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, event, election, location, organization, country, person and O.\nSentence: In the 1996 United States presidential election , Clinton was re-elected , receiving 49.2 percent of the popular vote over Republican Bob Dole ( 40.7 percent of the popular vote ) and Reform Party of the United States of America candidate Ross Perot ( 8.4 percent of the popular vote ) .", "prompt_labels": "In(O) the(O) 1996(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) Clinton(B-politician) was(O) re-elected(O) ,(O) receiving(O) 49.2(O) percent(O) of(O) the(O) popular(O) vote(O) over(O) Republican(O) Bob(B-politician) Dole(I-politician) ((O) 40.7(O) percent(O) of(O) the(O) popular(O) vote(O) )(O) and(O) Reform(B-political party) Party(I-political party) of(I-political party) the(I-political party) United(I-political party) States(I-political party) of(I-political party) America(I-political party) candidate(O) Ross(B-politician) Perot(I-politician) ((O) 8.4(O) percent(O) of(O) the(O) popular(O) vote(O) )(O) .(O)"}}
{"id": "347", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "politician", "country", "election", "location", "organization", "person", "event"], "instance": {"id": "347", "words": ["244", "Among", "the", "most", "important", "are", "World", "Vision", "International", "(", "1950", ")", ",", "Samaritan", "'s", "Purse", "(", "1970", ")", ",", "Mercy", "Ships", "(", "1978", ")", ",", "Prison", "Fellowship", "International", "(", "1979", ")", ",", "International", "Justice", "Mission", "(", "1997", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, country, election, location, organization, person, event and O.\nSentence: 244 Among the most important are World Vision International ( 1950 ) , Samaritan 's Purse ( 1970 ) , Mercy Ships ( 1978 ) , Prison Fellowship International ( 1979 ) , International Justice Mission ( 1997 ) .", "prompt_labels": "244(O) Among(O) the(O) most(O) important(O) are(O) World(B-organization) Vision(I-organization) International(I-organization) ((O) 1950(O) )(O) ,(O) Samaritan(B-organization) 's(I-organization) Purse(I-organization) ((O) 1970(O) )(O) ,(O) Mercy(B-organization) Ships(I-organization) ((O) 1978(O) )(O) ,(O) Prison(B-organization) Fellowship(I-organization) International(I-organization) ((O) 1979(O) )(O) ,(O) International(B-organization) Justice(I-organization) Mission(I-organization) ((O) 1997(O) )(O) .(O)"}}
{"id": "12", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "organization", "politician", "event", "political party", "location", "person", "election"], "instance": {"id": "12", "words": ["The", "most", "significant", "growth", "has", "been", "in", "the", "Melanesian", "countries", "of", "the", "Solomon", "Islands", ",", "Vanuatu", ",", "and", "Papua", "New", "Guinea", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "B-country", "I-country", "O", "B-country", "O", "O", "B-country", "I-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, politician, event, political party, location, person, election and O.\nSentence: The most significant growth has been in the Melanesian countries of the Solomon Islands , Vanuatu , and Papua New Guinea .", "prompt_labels": "The(O) most(O) significant(O) growth(O) has(O) been(O) in(O) the(O) Melanesian(B-location) countries(O) of(O) the(O) Solomon(B-country) Islands(I-country) ,(O) Vanuatu(B-country) ,(O) and(O) Papua(B-country) New(I-country) Guinea(I-country) .(O)"}}
{"id": "589", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "country", "person", "election", "event", "political party", "politician", "organization"], "instance": {"id": "589", "words": ["Nine", "Men", "is", "a", "1943", "United", "Kingdom", "war", "film", ",", "set", "in", "the", "Western", "Desert", "Campaign", "during", "the", "Second", "World", "War", "."], "labels": ["O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, person, election, event, political party, politician, organization and O.\nSentence: Nine Men is a 1943 United Kingdom war film , set in the Western Desert Campaign during the Second World War .", "prompt_labels": "Nine(O) Men(O) is(O) a(O) 1943(O) United(B-event) Kingdom(I-event) war(I-event) film(O) ,(O) set(O) in(O) the(O) Western(B-event) Desert(I-event) Campaign(I-event) during(O) the(O) Second(B-event) World(I-event) War(I-event) .(O)"}}
{"id": "111", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "location", "event", "political party", "organization", "person", "election", "country"], "instance": {"id": "111", "words": ["In", "New", "Zealand", ",", "the", "Māori", "Party", "won", "one", "overhang", "seat", "in", "2005", "New", "Zealand", "general", "election", "and", "2011", "New", "Zealand", "general", "election", ",", "and", "two", "overhang", "seats", "in", "2008", "New", "Zealand", "general", "election", "."], "labels": ["O", "B-country", "I-country", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, event, political party, organization, person, election, country and O.\nSentence: In New Zealand , the Māori Party won one overhang seat in 2005 New Zealand general election and 2011 New Zealand general election , and two overhang seats in 2008 New Zealand general election .", "prompt_labels": "In(O) New(B-country) Zealand(I-country) ,(O) the(O) Māori(B-political party) Party(I-political party) won(O) one(O) overhang(O) seat(O) in(O) 2005(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) and(O) 2011(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) and(O) two(O) overhang(O) seats(O) in(O) 2008(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "561", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "location", "political party", "country", "election", "event", "politician", "person"], "instance": {"id": "561", "words": ["The", "review", "was", "welcomed", "by", "many", "of", "the", "engineering", "institutions", "including", "the", "Institution", "of", "Engineering", "and", "Technology", ",", "the", "Institution", "of", "Mechanical", "Engineers", "and", "the", "Nuclear", "Institute", "-", "many", "of", "whom", "highlighted", "their", "existing", "schemes", "to", "encourage", "women", "and", "young", "people", "into", "the", "industry", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, political party, country, election, event, politician, person and O.\nSentence: The review was welcomed by many of the engineering institutions including the Institution of Engineering and Technology , the Institution of Mechanical Engineers and the Nuclear Institute - many of whom highlighted their existing schemes to encourage women and young people into the industry .", "prompt_labels": "The(O) review(O) was(O) welcomed(O) by(O) many(O) of(O) the(O) engineering(O) institutions(O) including(O) the(O) Institution(B-organization) of(I-organization) Engineering(I-organization) and(I-organization) Technology(I-organization) ,(O) the(O) Institution(B-organization) of(I-organization) Mechanical(I-organization) Engineers(I-organization) and(O) the(O) Nuclear(B-organization) Institute(I-organization) -(O) many(O) of(O) whom(O) highlighted(O) their(O) existing(O) schemes(O) to(O) encourage(O) women(O) and(O) young(O) people(O) into(O) the(O) industry(O) .(O)"}}
{"id": "629", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "political party", "event", "person", "election", "politician", "country", "location"], "instance": {"id": "629", "words": ["In", "August", "1914", ",", "World", "War", "I", "began", "when", "alliance", "obligations", "arising", "from", "the", "war", "between", "Serbia", "and", "Austria-Hungary", "brought", "Germany", "and", "Russia", "to", "war", ",", "while", "Germany", "'s", "invasion", "of", "Belgium", "directly", "triggered", "Britain", "'s", "entry", "."], "labels": ["O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "B-country", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, event, person, election, politician, country, location and O.\nSentence: In August 1914 , World War I began when alliance obligations arising from the war between Serbia and Austria-Hungary brought Germany and Russia to war , while Germany 's invasion of Belgium directly triggered Britain 's entry .", "prompt_labels": "In(O) August(O) 1914(O) ,(O) World(B-event) War(I-event) I(I-event) began(O) when(O) alliance(O) obligations(O) arising(O) from(O) the(O) war(O) between(O) Serbia(B-country) and(O) Austria-Hungary(B-country) brought(O) Germany(B-country) and(O) Russia(B-country) to(O) war(O) ,(O) while(O) Germany(B-event) 's(I-event) invasion(I-event) of(I-event) Belgium(I-event) directly(O) triggered(O) Britain(B-country) 's(O) entry(O) .(O)"}}
{"id": "579", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "organization", "event", "election", "politician", "location", "country", "person"], "instance": {"id": "579", "words": ["During", "his", "premiership", ",", "Renzi", "faced", "several", "challenging", "foreign", "policy", "situations", ",", "such", "as", "the", "European", "debt", "crisis", ",", "the", "civil", "war", "in", "Libya", ",", "the", "Ukrainian", "Crisis", "and", "the", "insurgency", "of", "the", "Islamic", "State", "(", "IS", ")", "in", "the", "Middle", "East", "."], "labels": ["O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "O", "B-country", "O", "O", "B-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "B-country", "O", "O", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, event, election, politician, location, country, person and O.\nSentence: During his premiership , Renzi faced several challenging foreign policy situations , such as the European debt crisis , the civil war in Libya , the Ukrainian Crisis and the insurgency of the Islamic State ( IS ) in the Middle East .", "prompt_labels": "During(O) his(O) premiership(O) ,(O) Renzi(B-politician) faced(O) several(O) challenging(O) foreign(O) policy(O) situations(O) ,(O) such(O) as(O) the(O) European(B-event) debt(I-event) crisis(I-event) ,(O) the(O) civil(B-event) war(I-event) in(O) Libya(B-country) ,(O) the(O) Ukrainian(B-event) Crisis(I-event) and(O) the(O) insurgency(B-event) of(I-event) the(I-event) Islamic(I-event) State(I-event) ((O) IS(B-country) )(O) in(O) the(O) Middle(B-location) East(I-location) .(O)"}}
{"id": "45", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "politician", "location", "event", "election", "organization", "country", "political party"], "instance": {"id": "45", "words": ["Tiglath-Pileser", "III", "conquered", "as", "far", "as", "the", "East", "Mediterranean", ",", "bringing", "the", "Greeks", "of", "Cyprus", ",", "Phoenicia", ",", "Kingdom", "of", "Judah", ",", "Philistia", ",", "Samaria", "and", "the", "whole", "of", "Aramea", "under", "Assyrian", "control", "."], "labels": ["B-politician", "I-politician", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "B-country", "I-country", "I-country", "O", "B-country", "O", "B-country", "I-country", "I-country", "O", "B-country", "O", "B-location", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, location, event, election, organization, country, political party and O.\nSentence: Tiglath-Pileser III conquered as far as the East Mediterranean , bringing the Greeks of Cyprus , Phoenicia , Kingdom of Judah , Philistia , Samaria and the whole of Aramea under Assyrian control .", "prompt_labels": "Tiglath-Pileser(B-politician) III(I-politician) conquered(O) as(O) far(O) as(O) the(O) East(B-location) Mediterranean(I-location) ,(O) bringing(O) the(O) Greeks(B-country) of(I-country) Cyprus(I-country) ,(O) Phoenicia(B-country) ,(O) Kingdom(B-country) of(I-country) Judah(I-country) ,(O) Philistia(B-country) ,(O) Samaria(B-location) and(O) the(O) whole(O) of(O) Aramea(B-country) under(O) Assyrian(B-country) control(O) .(O)"}}
{"id": "167", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "politician", "country", "organization", "person", "location", "political party", "election"], "instance": {"id": "167", "words": ["State", "parties", "which", "exclusively", "contest", "Victorian", "state", "elections", "include", "the", "Aussie", "Battler", "Party", ",", "Hudson", "for", "Northern", "Victoria", ",", "Victorian", "Socialists", ",", "and", "Vote", "1", "Local", "Jobs", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, country, organization, person, location, political party, election and O.\nSentence: State parties which exclusively contest Victorian state elections include the Aussie Battler Party , Hudson for Northern Victoria , Victorian Socialists , and Vote 1 Local Jobs .", "prompt_labels": "State(O) parties(O) which(O) exclusively(O) contest(O) Victorian(O) state(O) elections(O) include(O) the(O) Aussie(B-political party) Battler(I-political party) Party(I-political party) ,(O) Hudson(B-political party) for(I-political party) Northern(I-political party) Victoria(I-political party) ,(O) Victorian(B-political party) Socialists(I-political party) ,(O) and(O) Vote(B-political party) 1(I-political party) Local(I-political party) Jobs(I-political party) .(O)"}}
{"id": "73", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "politician", "political party", "event", "person", "organization", "location", "country"], "instance": {"id": "73", "words": ["In", "India", ",", "Prime", "Ministers", "Indira", "Gandhi", "and", "her", "son", "Rajiv", "Gandhi", "(", "neither", "of", "whom", "were", "related", "to", "Mahatma", "Gandhi", ",", "who", "was", "assassinated", "in", "1948", ")", ",", "were", "assassinated", "in", "1984", "and", "1991", "respectively", "."], "labels": ["O", "B-country", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, political party, event, person, organization, location, country and O.\nSentence: In India , Prime Ministers Indira Gandhi and her son Rajiv Gandhi ( neither of whom were related to Mahatma Gandhi , who was assassinated in 1948 ) , were assassinated in 1984 and 1991 respectively .", "prompt_labels": "In(O) India(B-country) ,(O) Prime(O) Ministers(O) Indira(B-politician) Gandhi(I-politician) and(O) her(O) son(O) Rajiv(B-politician) Gandhi(I-politician) ((O) neither(O) of(O) whom(O) were(O) related(O) to(O) Mahatma(B-politician) Gandhi(I-politician) ,(O) who(O) was(O) assassinated(O) in(O) 1948(O) )(O) ,(O) were(O) assassinated(O) in(O) 1984(O) and(O) 1991(O) respectively(O) .(O)"}}
{"id": "8", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "location", "country", "event", "election", "person", "organization", "politician"], "instance": {"id": "8", "words": ["After", "the", "assassination", "of", "Perdiccas", "in", "321", "BC", ",", "Macedonian", "unity", "collapsed", ",", "and", "40", "years", "of", "war", "between", "The", "Successors", "(", "Diadochi", ")", "ensued", "before", "the", "Hellenistic", "world", "settled", "into", "four", "stable", "power", "blocs", ":", "Ptolemaic", "Kingdom", "Egypt", ",", "Seleucid", "Empire", "Mesopotamia", "and", "Central", "Asia", ",", "Attalid", "dynasty", "Anatolia", ",", "and", "Antigonid", "dynasty", "Macedon", "."], "labels": ["O", "O", "O", "O", "B-person", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "B-country", "O", "B-country", "I-country", "B-location", "O", "B-location", "I-location", "O", "O", "O", "B-location", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, location, country, event, election, person, organization, politician and O.\nSentence: After the assassination of Perdiccas in 321 BC , Macedonian unity collapsed , and 40 years of war between The Successors ( Diadochi ) ensued before the Hellenistic world settled into four stable power blocs : Ptolemaic Kingdom Egypt , Seleucid Empire Mesopotamia and Central Asia , Attalid dynasty Anatolia , and Antigonid dynasty Macedon .", "prompt_labels": "After(O) the(O) assassination(O) of(O) Perdiccas(B-person) in(O) 321(O) BC(O) ,(O) Macedonian(B-country) unity(I-country) collapsed(O) ,(O) and(O) 40(O) years(O) of(O) war(O) between(O) The(O) Successors(O) ((O) Diadochi(O) )(O) ensued(O) before(O) the(O) Hellenistic(O) world(O) settled(O) into(O) four(O) stable(O) power(O) blocs(O) :(O) Ptolemaic(B-country) Kingdom(I-country) Egypt(B-country) ,(O) Seleucid(B-country) Empire(I-country) Mesopotamia(B-location) and(O) Central(B-location) Asia(I-location) ,(O) Attalid(O) dynasty(O) Anatolia(B-location) ,(O) and(O) Antigonid(O) dynasty(O) Macedon(B-country) .(O)"}}
{"id": "569", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "person", "election", "organization", "politician", "political party", "event", "country"], "instance": {"id": "569", "words": ["When", "Sen", "died", "of", "a", "heart", "attack", ",", "Islam", "took", "over", "his", "duties", ",", "managing", "Bangladesh", "'s", "contributions", "to", "the", "International", "Bank", "for", "Reconstruction", "and", "Development", ",", "International", "Development", "Association", ",", "and", "International", "Finance", "Corporation", ",", "the", "former", "two", "of", "which", "were", "both", "previously", "managed", "by", "Sen", "."], "labels": ["O", "B-person", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, election, organization, politician, political party, event, country and O.\nSentence: When Sen died of a heart attack , Islam took over his duties , managing Bangladesh 's contributions to the International Bank for Reconstruction and Development , International Development Association , and International Finance Corporation , the former two of which were both previously managed by Sen .", "prompt_labels": "When(O) Sen(B-person) died(O) of(O) a(O) heart(O) attack(O) ,(O) Islam(B-person) took(O) over(O) his(O) duties(O) ,(O) managing(O) Bangladesh(B-country) 's(O) contributions(O) to(O) the(O) International(B-organization) Bank(I-organization) for(I-organization) Reconstruction(I-organization) and(I-organization) Development(I-organization) ,(O) International(B-organization) Development(I-organization) Association(I-organization) ,(O) and(O) International(B-organization) Finance(I-organization) Corporation(I-organization) ,(O) the(O) former(O) two(O) of(O) which(O) were(O) both(O) previously(O) managed(O) by(O) Sen(B-person) .(O)"}}
{"id": "623", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "election", "country", "event", "organization", "politician", "political party", "person"], "instance": {"id": "623", "words": ["The", "Adriatic", "Campaign", "of", "World", "War", "I", "was", "a", "naval", "campaign", "fought", "between", "the", "Central", "Powers", "and", "the", "Mediterranean", "squadrons", "of", "United", "Kingdom", ",", "France", ",", "the", "Kingdom", "of", "Italy", ",", "Australia", "and", "the", "United", "States", "."], "labels": ["B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "B-country", "O", "O", "B-country", "I-country", "I-country", "O", "B-country", "O", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, election, country, event, organization, politician, political party, person and O.\nSentence: The Adriatic Campaign of World War I was a naval campaign fought between the Central Powers and the Mediterranean squadrons of United Kingdom , France , the Kingdom of Italy , Australia and the United States .", "prompt_labels": "The(B-event) Adriatic(I-event) Campaign(I-event) of(I-event) World(I-event) War(I-event) I(I-event) was(O) a(O) naval(O) campaign(O) fought(O) between(O) the(O) Central(B-country) Powers(I-country) and(O) the(O) Mediterranean(O) squadrons(O) of(O) United(B-country) Kingdom(I-country) ,(O) France(B-country) ,(O) the(O) Kingdom(B-country) of(I-country) Italy(I-country) ,(O) Australia(B-country) and(O) the(O) United(B-country) States(I-country) .(O)"}}
{"id": "292", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "political party", "event", "election", "organization", "politician", "person", "location"], "instance": {"id": "292", "words": ["As", "late", "as", "1970", ",", "most", "Saudis", "lived", "a", "subsistence", "life", "in", "the", "rural", "provinces", ",", "but", "in", "the", "last", "half", "of", "the", "20th", "century", "the", "kingdom", "has", "urbanized", "rapidly.", "about", "80", "%", "of", "Saudis", "live", "in", "urban", "metropolitan", "areas", "-", "specifically", "Riyadh", ",", "Jeddah", ",", "or", "Dammam", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, event, election, organization, politician, person, location and O.\nSentence: As late as 1970 , most Saudis lived a subsistence life in the rural provinces , but in the last half of the 20th century the kingdom has urbanized rapidly. about 80 % of Saudis live in urban metropolitan areas - specifically Riyadh , Jeddah , or Dammam .", "prompt_labels": "As(O) late(O) as(O) 1970(O) ,(O) most(O) Saudis(O) lived(O) a(O) subsistence(O) life(O) in(O) the(O) rural(O) provinces(O) ,(O) but(O) in(O) the(O) last(O) half(O) of(O) the(O) 20th(O) century(O) the(O) kingdom(O) has(O) urbanized(O) rapidly.(O) about(O) 80(O) %(O) of(O) Saudis(O) live(O) in(O) urban(O) metropolitan(O) areas(O) -(O) specifically(O) Riyadh(B-location) ,(O) Jeddah(B-location) ,(O) or(O) Dammam(B-location) .(O)"}}
{"id": "448", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "country", "election", "location", "event", "organization", "politician", "political party"], "instance": {"id": "448", "words": ["Shortly", "afterwards", ",", "when", "Casini", "refused", "to", "merge", "his", "party", "into", "Berlusconi", "'s", "then-new", "political", "movement", ",", "The", "People", "of", "Freedom", "(", "PdL", ")", ",", "the", "UDC", "was", "joined", "by", "The", "Rose", "for", "Italy", "of", "Tabacci", ",", "Baccini", "and", "Savino", "Pezzotta", ",", "as", "well", "as", "by", "two", "leading", "members", "of", "Forza", "Italia", "(", "FI", ")", ",", "Ferdinando", "Adornato", "and", "Angelo", "Sanza", "."], "labels": ["O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "O", "B-politician", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, election, location, event, organization, politician, political party and O.\nSentence: Shortly afterwards , when Casini refused to merge his party into Berlusconi 's then-new political movement , The People of Freedom ( PdL ) , the UDC was joined by The Rose for Italy of Tabacci , Baccini and Savino Pezzotta , as well as by two leading members of Forza Italia ( FI ) , Ferdinando Adornato and Angelo Sanza .", "prompt_labels": "Shortly(O) afterwards(O) ,(O) when(O) Casini(B-politician) refused(O) to(O) merge(O) his(O) party(O) into(O) Berlusconi(B-politician) 's(O) then-new(O) political(O) movement(O) ,(O) The(B-political party) People(I-political party) of(I-political party) Freedom(I-political party) ((O) PdL(B-political party) )(O) ,(O) the(O) UDC(B-political party) was(O) joined(O) by(O) The(B-political party) Rose(I-political party) for(I-political party) Italy(I-political party) of(O) Tabacci(B-politician) ,(O) Baccini(B-politician) and(O) Savino(B-politician) Pezzotta(I-politician) ,(O) as(O) well(O) as(O) by(O) two(O) leading(O) members(O) of(O) Forza(B-political party) Italia(I-political party) ((O) FI(B-political party) )(O) ,(O) Ferdinando(B-politician) Adornato(I-politician) and(O) Angelo(B-politician) Sanza(I-politician) .(O)"}}
{"id": "614", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "election", "location", "event", "person", "political party", "organization", "country"], "instance": {"id": "614", "words": ["Operation", "Cascade", "gave", "the", "Allies", "of", "World", "War", "II", "valuable", "experience", "in", "planning", "deception", "operations", ",", "which", "was", "later", "used", "to", "good", "effect", "in", "the", "deception", "operations", "covering", "the", "invasion", "of", "Europe", "(", "D-Day", "landings", "in", "Normandy", ",", "and", "the", "invasion", "of", "Southern", "France", ")", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, location, event, person, political party, organization, country and O.\nSentence: Operation Cascade gave the Allies of World War II valuable experience in planning deception operations , which was later used to good effect in the deception operations covering the invasion of Europe ( D-Day landings in Normandy , and the invasion of Southern France ) .", "prompt_labels": "Operation(O) Cascade(O) gave(O) the(O) Allies(B-organization) of(I-organization) World(I-organization) War(I-organization) II(I-organization) valuable(O) experience(O) in(O) planning(O) deception(O) operations(O) ,(O) which(O) was(O) later(O) used(O) to(O) good(O) effect(O) in(O) the(O) deception(O) operations(O) covering(O) the(O) invasion(B-event) of(I-event) Europe(I-event) ((O) D-Day(B-event) landings(I-event) in(I-event) Normandy(I-event) ,(O) and(O) the(O) invasion(B-event) of(I-event) Southern(I-event) France(I-event) )(O) .(O)"}}
{"id": "249", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "person", "event", "organization", "political party", "election", "location", "politician"], "instance": {"id": "249", "words": ["He", "first", "defeated", "three-term", "Republican", "incumbent", "Al", "D", "'Amato", "before", "being", "reelected", "in", "2004", "United", "States", "Senate", "election", "in", "New", "York", "with", "71", "percent", "of", "the", "vote", ",", "in", "2010", "United", "States", "Senate", "election", "in", "New", "York", "with", "66", "percent", "of", "the", "vote", ",", "and", "in", "2016", "United", "States", "Senate", "election", "in", "New", "York", "with", "70", "percent", "of", "the", "vote", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, event, organization, political party, election, location, politician and O.\nSentence: He first defeated three-term Republican incumbent Al D 'Amato before being reelected in 2004 United States Senate election in New York with 71 percent of the vote , in 2010 United States Senate election in New York with 66 percent of the vote , and in 2016 United States Senate election in New York with 70 percent of the vote .", "prompt_labels": "He(O) first(O) defeated(O) three-term(O) Republican(O) incumbent(O) Al(B-politician) D(I-politician) 'Amato(I-politician) before(O) being(O) reelected(O) in(O) 2004(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) New(I-election) York(I-election) with(O) 71(O) percent(O) of(O) the(O) vote(O) ,(O) in(O) 2010(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) New(I-election) York(I-election) with(O) 66(O) percent(O) of(O) the(O) vote(O) ,(O) and(O) in(O) 2016(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) New(I-election) York(I-election) with(O) 70(O) percent(O) of(O) the(O) vote(O) .(O)"}}
{"id": "291", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "election", "location", "political party", "person", "country", "politician", "organization"], "instance": {"id": "291", "words": ["Human", "Rights", "organizations", "such", "as", "Amnesty", "International", ",", "Human", "Rights", "Watch", "and", "Freedom", "House", "condemn", "both", "the", "Saudi", "criminal", "justice", "system", "and", "its", "severe", "punishments", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, location, political party, person, country, politician, organization and O.\nSentence: Human Rights organizations such as Amnesty International , Human Rights Watch and Freedom House condemn both the Saudi criminal justice system and its severe punishments .", "prompt_labels": "Human(O) Rights(O) organizations(O) such(O) as(O) Amnesty(B-organization) International(I-organization) ,(O) Human(B-organization) Rights(I-organization) Watch(I-organization) and(O) Freedom(B-organization) House(I-organization) condemn(O) both(O) the(O) Saudi(B-country) criminal(O) justice(O) system(O) and(O) its(O) severe(O) punishments(O) .(O)"}}
{"id": "475", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "location", "event", "country", "organization", "political party", "politician", "person"], "instance": {"id": "475", "words": ["He", "was", "a", "Senior", "Associate", "at", "the", "Carnegie", "Endowment", "for", "International", "Peace", "from", "1992", "to", "1993", ",", "a", "Fellow", "at", "the", "Woodrow", "Wilson", "International", "Center", "for", "Scholars", "from", "1998", "to", "1999", ",", "and", "a", "non-resident", "Senior", "Fellow", "at", "the", "Brookings", "Institution", "from", "1997", "to", "2002", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, event, country, organization, political party, politician, person and O.\nSentence: He was a Senior Associate at the Carnegie Endowment for International Peace from 1992 to 1993 , a Fellow at the Woodrow Wilson International Center for Scholars from 1998 to 1999 , and a non-resident Senior Fellow at the Brookings Institution from 1997 to 2002 .", "prompt_labels": "He(O) was(O) a(O) Senior(O) Associate(O) at(O) the(O) Carnegie(B-organization) Endowment(I-organization) for(I-organization) International(I-organization) Peace(I-organization) from(O) 1992(O) to(O) 1993(O) ,(O) a(O) Fellow(O) at(O) the(O) Woodrow(B-organization) Wilson(I-organization) International(I-organization) Center(I-organization) for(I-organization) Scholars(I-organization) from(O) 1998(O) to(O) 1999(O) ,(O) and(O) a(O) non-resident(O) Senior(O) Fellow(O) at(O) the(O) Brookings(B-organization) Institution(I-organization) from(O) 1997(O) to(O) 2002(O) .(O)"}}
{"id": "474", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "event", "person", "political party", "location", "politician", "country", "organization"], "instance": {"id": "474", "words": ["That", "was", "TRUE", ",", "for", "example", ",", "for", "Synaspismos", ",", "Renewing", "Communist", "Ecological", "Left", ",", "Ecosocialists", "of", "Greece", "and", "Unitary", "Movement", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, person, political party, location, politician, country, organization and O.\nSentence: That was TRUE , for example , for Synaspismos , Renewing Communist Ecological Left , Ecosocialists of Greece and Unitary Movement .", "prompt_labels": "That(O) was(O) TRUE(O) ,(O) for(O) example(O) ,(O) for(O) Synaspismos(B-political party) ,(O) Renewing(B-political party) Communist(I-political party) Ecological(I-political party) Left(I-political party) ,(O) Ecosocialists(B-political party) of(I-political party) Greece(I-political party) and(O) Unitary(B-political party) Movement(I-political party) .(O)"}}
{"id": "252", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "event", "person", "organization", "location", "country", "politician", "election"], "instance": {"id": "252", "words": ["The", "agency", "also", "has", "offices", "in", "Ottawa", ",", "at", "the", "David", "Florida", "Laboratory", ",", "and", "small", "liaison", "offices", "in", "Houston", ";", "Washington", ",", "D.C.", ";", "and", "Paris", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, person, organization, location, country, politician, election and O.\nSentence: The agency also has offices in Ottawa , at the David Florida Laboratory , and small liaison offices in Houston ; Washington , D.C. ; and Paris .", "prompt_labels": "The(O) agency(O) also(O) has(O) offices(O) in(O) Ottawa(B-location) ,(O) at(O) the(O) David(B-location) Florida(I-location) Laboratory(I-location) ,(O) and(O) small(O) liaison(O) offices(O) in(O) Houston(B-location) ;(O) Washington(B-location) ,(I-location) D.C.(I-location) ;(O) and(O) Paris(B-location) .(O)"}}
{"id": "400", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "election", "political party", "location", "organization", "event", "country", "person"], "instance": {"id": "400", "words": ["Olympic", "athlete", "Bob", "Richards", "(", "1984", "United", "States", "presidential", "election", ")", ",", "David", "Duke", "(", "a", "founder", "of", "the", "Knights", "of", "the", "Ku", "Klux", "Klan", "and", "a", "future", "Louisiana", "state", "representative", ",", "1988", "United", "States", "presidential", "election", ")", "and", "former", "Green", "Beret", "Bo", "Gritz", "(", "1992", "United", "States", "presidential", "election", ")", "were", "the", "Populist", "Party", "'s", "only", "three", "presidential", "candidates", "."], "labels": ["O", "O", "B-politician", "I-politician", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-location", "I-location", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-organization", "I-organization", "B-politician", "I-politician", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, political party, location, organization, event, country, person and O.\nSentence: Olympic athlete Bob Richards ( 1984 United States presidential election ) , David Duke ( a founder of the Knights of the Ku Klux Klan and a future Louisiana state representative , 1988 United States presidential election ) and former Green Beret Bo Gritz ( 1992 United States presidential election ) were the Populist Party 's only three presidential candidates .", "prompt_labels": "Olympic(O) athlete(O) Bob(B-politician) Richards(I-politician) ((O) 1984(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) )(O) ,(O) David(B-politician) Duke(I-politician) ((O) a(O) founder(O) of(O) the(O) Knights(B-organization) of(I-organization) the(I-organization) Ku(I-organization) Klux(I-organization) Klan(I-organization) and(O) a(O) future(O) Louisiana(B-location) state(I-location) representative(O) ,(O) 1988(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) )(O) and(O) former(O) Green(B-organization) Beret(I-organization) Bo(B-politician) Gritz(I-politician) ((O) 1992(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) )(O) were(O) the(O) Populist(B-political party) Party(I-political party) 's(O) only(O) three(O) presidential(O) candidates(O) .(O)"}}
{"id": "264", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "organization", "politician", "election", "person", "political party", "country", "event"], "instance": {"id": "264", "words": ["Cantwell", "was", "reelected", "in", "2006", "United", "States", "Senate", "election", "in", "Washington", ",", "2012", "United", "States", "Senate", "election", "in", "Washington", ",", "and", "2018", "United", "States", "Senate", "election", "in", "Washington", "."], "labels": ["B-politician", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, politician, election, person, political party, country, event and O.\nSentence: Cantwell was reelected in 2006 United States Senate election in Washington , 2012 United States Senate election in Washington , and 2018 United States Senate election in Washington .", "prompt_labels": "Cantwell(B-politician) was(O) reelected(O) in(O) 2006(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Washington(I-election) ,(O) 2012(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Washington(I-election) ,(O) and(O) 2018(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Washington(I-election) .(O)"}}
{"id": "399", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "country", "event", "person", "election", "organization", "politician", "location"], "instance": {"id": "399", "words": ["The", "Democratic", "Party", "made", "steady", "gains", "there", ",", "however", ",", "and", "in", "1992", "United", "States", "presidential", "election", ",", "1996", "United", "States", "presidential", "election", ",", "2004", "United", "States", "presidential", "election", ",", "2008", "United", "States", "presidential", "election", "and", "2012", "United", "States", "presidential", "election", "all", "eleven", "Northeastern", "states", ",", "from", "Maryland", "to", "Maine", ",", "voted", "for", "the", "Democrats", "except", "New", "Hampshire", "leaned", "more", "Republican", "."], "labels": ["O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, event, person, election, organization, politician, location and O.\nSentence: The Democratic Party made steady gains there , however , and in 1992 United States presidential election , 1996 United States presidential election , 2004 United States presidential election , 2008 United States presidential election and 2012 United States presidential election all eleven Northeastern states , from Maryland to Maine , voted for the Democrats except New Hampshire leaned more Republican .", "prompt_labels": "The(O) Democratic(B-political party) Party(I-political party) made(O) steady(O) gains(O) there(O) ,(O) however(O) ,(O) and(O) in(O) 1992(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) 1996(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) 2004(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) 2008(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) 2012(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) all(O) eleven(O) Northeastern(O) states(O) ,(O) from(O) Maryland(B-location) to(O) Maine(B-location) ,(O) voted(O) for(O) the(O) Democrats(O) except(O) New(B-location) Hampshire(I-location) leaned(O) more(O) Republican(B-political party) .(O)"}}
{"id": "263", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "election", "location", "political party", "politician", "organization", "person", "event"], "instance": {"id": "263", "words": ["Born", "and", "raised", "in", "Indianapolis", ",", "Indiana", ",", "Cantwell", "graduated", "from", "Miami", "University", "before", "moving", "to", "Seattle", "to", "work", "on", "Alan", "Cranston", "'", "s", "1984", "Democratic", "Party", "presidential", "primaries", "."], "labels": ["O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-politician", "O", "O", "B-organization", "I-organization", "O", "O", "O", "B-location", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, location, political party, politician, organization, person, event and O.\nSentence: Born and raised in Indianapolis , Indiana , Cantwell graduated from Miami University before moving to Seattle to work on Alan Cranston ' s 1984 Democratic Party presidential primaries .", "prompt_labels": "Born(O) and(O) raised(O) in(O) Indianapolis(B-location) ,(O) Indiana(B-location) ,(O) Cantwell(B-politician) graduated(O) from(O) Miami(B-organization) University(I-organization) before(O) moving(O) to(O) Seattle(B-location) to(O) work(O) on(O) Alan(B-politician) Cranston(I-politician) '(O) s(O) 1984(B-election) Democratic(I-election) Party(I-election) presidential(I-election) primaries(I-election) .(O)"}}
{"id": "530", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "location", "country", "organization", "election", "person", "politician", "event"], "instance": {"id": "530", "words": ["The", "series", "was", "written", "by", "Russell", "T", "Davies", "and", "directed", "by", "Stephen", "Frears", ",", "with", "Hugh", "Grant", "starring", "as", "Thorpe", "and", "Ben", "Whishaw", "as", "Scott", "."], "labels": ["O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "O", "B-person", "O", "B-person", "I-person", "O", "B-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, location, country, organization, election, person, politician, event and O.\nSentence: The series was written by Russell T Davies and directed by Stephen Frears , with Hugh Grant starring as Thorpe and Ben Whishaw as Scott .", "prompt_labels": "The(O) series(O) was(O) written(O) by(O) Russell(B-person) T(I-person) Davies(I-person) and(O) directed(O) by(O) Stephen(B-person) Frears(I-person) ,(O) with(O) Hugh(B-person) Grant(I-person) starring(O) as(O) Thorpe(B-person) and(O) Ben(B-person) Whishaw(I-person) as(O) Scott(B-person) .(O)"}}
{"id": "328", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "person", "event", "political party", "organization", "politician", "location", "election"], "instance": {"id": "328", "words": ["In", "2007", ",", "Cox", "campaigned", "for", "Labour", "in", "the", "run-up", "to", "that", "year", "'s", "2007", "Scottish", "Parliament", "election", ".", "However", ",", "Cox", "endorsed", "the", "Scottish", "National", "Party", "in", "the", "2011", "Scottish", "Parliament", "election", ",", "only", "because", "of", "their", "higher", "education", "policy", "."], "labels": ["O", "O", "O", "B-politician", "O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-politician", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, event, political party, organization, politician, location, election and O.\nSentence: In 2007 , Cox campaigned for Labour in the run-up to that year 's 2007 Scottish Parliament election . However , Cox endorsed the Scottish National Party in the 2011 Scottish Parliament election , only because of their higher education policy .", "prompt_labels": "In(O) 2007(O) ,(O) Cox(B-politician) campaigned(O) for(O) Labour(B-political party) in(O) the(O) run-up(O) to(O) that(O) year(O) 's(O) 2007(B-election) Scottish(I-election) Parliament(I-election) election(I-election) .(O) However(O) ,(O) Cox(B-politician) endorsed(O) the(O) Scottish(B-political party) National(I-political party) Party(I-political party) in(O) the(O) 2011(B-election) Scottish(I-election) Parliament(I-election) election(I-election) ,(O) only(O) because(O) of(O) their(O) higher(O) education(O) policy(O) .(O)"}}
{"id": "25", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "event", "country", "politician", "location", "political party", "election", "organization"], "instance": {"id": "25", "words": ["Jackson", "'s", "state", "department", "was", "active", "and", "successful", "at", "making", "trade", "agreements", "with", "Russia", ",", "Spain", ",", "Turkey", ",", "United", "Kingdom", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, country, politician, location, political party, election, organization and O.\nSentence: Jackson 's state department was active and successful at making trade agreements with Russia , Spain , Turkey , United Kingdom .", "prompt_labels": "Jackson(B-politician) 's(O) state(O) department(O) was(O) active(O) and(O) successful(O) at(O) making(O) trade(O) agreements(O) with(O) Russia(B-country) ,(O) Spain(B-country) ,(O) Turkey(B-country) ,(O) United(B-country) Kingdom(I-country) .(O)"}}
{"id": "440", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "person", "event", "organization", "location", "election", "country", "political party"], "instance": {"id": "440", "words": ["He", "was", "the", "Minister", "of", "Finance", "(", "1976-1984", ")", ",", "Minister", "of", "International", "Trade", "and", "Industry", "(", "1984-1987", ")", ",", "former", "chairman", "of", "Asian", "Development", "Bank", ",", "former", "chairman", "of", "Islamic", "Development", "Bank", ",", "founding", "Chairman", "and", "Chief", "Executive", "of", "Malaysian", "oil", "company", ",", "PETRONAS", ",", "and", "chairman", "of", "the", "33rd", "Board", "of", "Governors", "of", "the", "World", "Bank", "and", "IMF", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, event, organization, location, election, country, political party and O.\nSentence: He was the Minister of Finance ( 1976-1984 ) , Minister of International Trade and Industry ( 1984-1987 ) , former chairman of Asian Development Bank , former chairman of Islamic Development Bank , founding Chairman and Chief Executive of Malaysian oil company , PETRONAS , and chairman of the 33rd Board of Governors of the World Bank and IMF .", "prompt_labels": "He(O) was(O) the(O) Minister(O) of(O) Finance(O) ((O) 1976-1984(O) )(O) ,(O) Minister(O) of(O) International(O) Trade(O) and(O) Industry(O) ((O) 1984-1987(O) )(O) ,(O) former(O) chairman(O) of(O) Asian(B-organization) Development(I-organization) Bank(I-organization) ,(O) former(O) chairman(O) of(O) Islamic(B-organization) Development(I-organization) Bank(I-organization) ,(O) founding(O) Chairman(O) and(O) Chief(O) Executive(O) of(O) Malaysian(B-organization) oil(I-organization) company(I-organization) ,(O) PETRONAS(B-organization) ,(O) and(O) chairman(O) of(O) the(O) 33rd(O) Board(O) of(O) Governors(O) of(O) the(O) World(B-organization) Bank(I-organization) and(O) IMF(B-organization) .(O)"}}
{"id": "157", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "event", "political party", "election", "person", "organization", "politician", "country"], "instance": {"id": "157", "words": ["The", "PSOE", "finished", "first", "in", "terms", "of", "votes", "and", "seats", "in", "every", "region", "except", "for", "2019", "Cantabrian", "regional", "election", ",", "where", "the", "Regionalist", "Party", "of", "Cantabria", "(", "PRC", ")", "finished", "first", "and", "the", "PSOE", "third", "behind", "the", "PP", ",", "and", "2019", "Navarrese", "regional", "election", ",", "where", "the", "conservative", "regionalist", "Navarra", "Suma", "finished", "first", "and", "the", "Socialist", "Party", "of", "Navarre", "finished", "second", "."], "labels": ["O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "B-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, political party, election, person, organization, politician, country and O.\nSentence: The PSOE finished first in terms of votes and seats in every region except for 2019 Cantabrian regional election , where the Regionalist Party of Cantabria ( PRC ) finished first and the PSOE third behind the PP , and 2019 Navarrese regional election , where the conservative regionalist Navarra Suma finished first and the Socialist Party of Navarre finished second .", "prompt_labels": "The(O) PSOE(B-political party) finished(O) first(O) in(O) terms(O) of(O) votes(O) and(O) seats(O) in(O) every(O) region(O) except(O) for(O) 2019(B-election) Cantabrian(I-election) regional(I-election) election(I-election) ,(O) where(O) the(O) Regionalist(B-political party) Party(I-political party) of(I-political party) Cantabria(I-political party) ((O) PRC(B-political party) )(O) finished(O) first(O) and(O) the(O) PSOE(B-political party) third(O) behind(O) the(O) PP(B-political party) ,(O) and(O) 2019(B-election) Navarrese(I-election) regional(I-election) election(I-election) ,(O) where(O) the(O) conservative(O) regionalist(O) Navarra(B-political party) Suma(I-political party) finished(O) first(O) and(O) the(O) Socialist(B-political party) Party(I-political party) of(I-political party) Navarre(I-political party) finished(O) second(O) .(O)"}}
{"id": "587", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "person", "event", "political party", "location", "country", "politician", "election"], "instance": {"id": "587", "words": ["Corsi", "had", "scheduled", "a", "press", "conference", "to", "announce", "the", "claim", "that", "Obama", "had", "raised", "a", "million", "dollars", "for", "the", "election", "campaign", "of", "Kenyan", "prime", "minister", "Raila", "Odinga", ",", "and", "had", "helped", "run", "Odinga", "'s", "campaign", "as", "a", "strategist", ",", "including", "setting", "the", "stage", "for", "the", "campaign", "of", "violence", "and", "bloodshed", "that", "had", "brought", "Odinga", "to", "power", "after", "a", "disputed", "election", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, event, political party, location, country, politician, election and O.\nSentence: Corsi had scheduled a press conference to announce the claim that Obama had raised a million dollars for the election campaign of Kenyan prime minister Raila Odinga , and had helped run Odinga 's campaign as a strategist , including setting the stage for the campaign of violence and bloodshed that had brought Odinga to power after a disputed election .", "prompt_labels": "Corsi(B-person) had(O) scheduled(O) a(O) press(O) conference(O) to(O) announce(O) the(O) claim(O) that(O) Obama(B-politician) had(O) raised(O) a(O) million(O) dollars(O) for(O) the(O) election(B-event) campaign(I-event) of(I-event) Kenyan(I-event) prime(O) minister(O) Raila(B-politician) Odinga(I-politician) ,(O) and(O) had(O) helped(O) run(O) Odinga(O) 's(B-politician) campaign(O) as(O) a(O) strategist(O) ,(O) including(O) setting(O) the(O) stage(O) for(O) the(O) campaign(O) of(O) violence(O) and(O) bloodshed(O) that(O) had(O) brought(O) Odinga(B-politician) to(O) power(O) after(O) a(O) disputed(O) election(O) .(O)"}}
{"id": "182", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "location", "event", "organization", "election", "person", "politician", "country"], "instance": {"id": "182", "words": ["It", "was", "eclipsed", "by", "parties", "like", "the", "(", "relatively", "moderate", ")", "National", "Peasants", "'", "Party", "and", "its", "more", "radical", "Romanian", "Front", "offshoot", ",", "the", "National-Christian", "Defense", "League", "(", "LANC", ")", "and", "the", "Iron", "Guard", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, location, event, organization, election, person, politician, country and O.\nSentence: It was eclipsed by parties like the ( relatively moderate ) National Peasants ' Party and its more radical Romanian Front offshoot , the National-Christian Defense League ( LANC ) and the Iron Guard .", "prompt_labels": "It(O) was(O) eclipsed(O) by(O) parties(O) like(O) the(O) ((O) relatively(O) moderate(O) )(O) National(B-political party) Peasants(I-political party) '(I-political party) Party(I-political party) and(O) its(O) more(O) radical(O) Romanian(B-political party) Front(I-political party) offshoot(O) ,(O) the(O) National-Christian(B-political party) Defense(I-political party) League(I-political party) ((O) LANC(B-political party) )(O) and(O) the(O) Iron(B-political party) Guard(I-political party) .(O)"}}
{"id": "331", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "country", "organization", "location", "politician", "political party", "person", "election"], "instance": {"id": "331", "words": ["No", "Montreal-area", "riding", "has", "elected", "a", "Conservative", "or", "any", "member", "of", "the", "party", "'s", "predecessors", "-", "the", "Progressive", "Conservative", "Party", "of", "Canada", ",", "the", "Canadian", "Alliance", "and", "the", "Reform", "Party", "of", "Canada", "-", "since", "1988", "Canadian", "federal", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, organization, location, politician, political party, person, election and O.\nSentence: No Montreal-area riding has elected a Conservative or any member of the party 's predecessors - the Progressive Conservative Party of Canada , the Canadian Alliance and the Reform Party of Canada - since 1988 Canadian federal election .", "prompt_labels": "No(O) Montreal-area(O) riding(O) has(O) elected(O) a(O) Conservative(O) or(O) any(O) member(O) of(O) the(O) party(O) 's(O) predecessors(O) -(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) the(O) Canadian(B-political party) Alliance(I-political party) and(O) the(O) Reform(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) -(O) since(O) 1988(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)"}}
{"id": "320", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "politician", "event", "organization", "person", "election", "country", "political party"], "instance": {"id": "320", "words": ["Scottish", "Labour", ",", "the", "Scottish", "National", "Party", ",", "Liberal", "Democrats", ",", "and", "Scottish", "Greens", "campaigned", "for", "a", "Yes", "vote", "for", "both", "proposals", "whilst", "the", "Conservatives", "opposed", "both", "proposals", "."], "labels": ["B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, event, organization, person, election, country, political party and O.\nSentence: Scottish Labour , the Scottish National Party , Liberal Democrats , and Scottish Greens campaigned for a Yes vote for both proposals whilst the Conservatives opposed both proposals .", "prompt_labels": "Scottish(B-political party) Labour(I-political party) ,(O) the(O) Scottish(B-political party) National(I-political party) Party(I-political party) ,(O) Liberal(B-political party) Democrats(I-political party) ,(O) and(O) Scottish(B-political party) Greens(I-political party) campaigned(O) for(O) a(O) Yes(O) vote(O) for(O) both(O) proposals(O) whilst(O) the(O) Conservatives(B-political party) opposed(O) both(O) proposals(O) .(O)"}}
{"id": "131", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "country", "politician", "event", "location", "person", "organization", "election"], "instance": {"id": "131", "words": ["Ewing", "became", "active", "in", "campaigning", "for", "Scottish", "independence", "through", "her", "membership", "of", "the", "Glasgow", "University", "Scottish", "Nationalist", "Association", ",", "and", "came", "to", "prominence", "in", "1967", "when", "she", "won", "the", "1967", "Hamilton", "by-election", "as", "the", "Scottish", "National", "Party", "(", "SNP", ")", "candidate", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, politician, event, location, person, organization, election and O.\nSentence: Ewing became active in campaigning for Scottish independence through her membership of the Glasgow University Scottish Nationalist Association , and came to prominence in 1967 when she won the 1967 Hamilton by-election as the Scottish National Party ( SNP ) candidate .", "prompt_labels": "Ewing(B-politician) became(O) active(O) in(O) campaigning(O) for(O) Scottish(O) independence(O) through(O) her(O) membership(O) of(O) the(O) Glasgow(B-organization) University(I-organization) Scottish(I-organization) Nationalist(I-organization) Association(I-organization) ,(O) and(O) came(O) to(O) prominence(O) in(O) 1967(O) when(O) she(O) won(O) the(O) 1967(B-election) Hamilton(I-election) by-election(I-election) as(O) the(O) Scottish(B-political party) National(I-political party) Party(I-political party) ((O) SNP(B-political party) )(O) candidate(O) .(O)"}}
{"id": "186", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "person", "organization", "country", "location", "political party", "event", "politician"], "instance": {"id": "186", "words": ["He", "entered", "the", "British", "Diplomatic", "Service", "in", "1883", ",", "and", "served", "in", "minor", "positions", "at", "embassies", "in", "Berlin", ",", "Rome", ",", "Athens", "and", "Paris", "."], "labels": ["O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, organization, country, location, political party, event, politician and O.\nSentence: He entered the British Diplomatic Service in 1883 , and served in minor positions at embassies in Berlin , Rome , Athens and Paris .", "prompt_labels": "He(O) entered(O) the(O) British(B-organization) Diplomatic(I-organization) Service(I-organization) in(O) 1883(O) ,(O) and(O) served(O) in(O) minor(O) positions(O) at(O) embassies(O) in(O) Berlin(B-location) ,(O) Rome(B-location) ,(O) Athens(B-location) and(O) Paris(B-location) .(O)"}}
{"id": "529", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "political party", "election", "politician", "person", "event", "location", "organization"], "instance": {"id": "529", "words": ["It", "is", "based", "upon", "the", "1980", "Iranian", "Embassy", "siege", "in", "London", "and", "stars", "Jamie", "Bell", ",", "Abbie", "Cornish", ",", "Mark", "Strong", "and", "Martin", "Shaw", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "B-location", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, election, politician, person, event, location, organization and O.\nSentence: It is based upon the 1980 Iranian Embassy siege in London and stars Jamie Bell , Abbie Cornish , Mark Strong and Martin Shaw .", "prompt_labels": "It(O) is(O) based(O) upon(O) the(O) 1980(O) Iranian(B-event) Embassy(I-event) siege(I-event) in(O) London(B-location) and(O) stars(O) Jamie(B-person) Bell(I-person) ,(O) Abbie(B-person) Cornish(I-person) ,(O) Mark(B-person) Strong(I-person) and(O) Martin(B-person) Shaw(I-person) .(O)"}}
{"id": "234", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "election", "political party", "location", "person", "event", "country", "politician"], "instance": {"id": "234", "words": ["Since", "2011", ",", "they", "are", "part", "of", "the", "Coalició", "Compromís", "coalition", ",", "which", "won", "six", "seats", "in", "the", "2011", "Valencian", "regional", "election", "s", "and", "19", "in", "the", "2015", "Valencian", "regional", "election", ",", "becoming", "the", "third", "largest", "party", "in", "the", "regional", "parliament", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, political party, location, person, event, country, politician and O.\nSentence: Since 2011 , they are part of the Coalició Compromís coalition , which won six seats in the 2011 Valencian regional election s and 19 in the 2015 Valencian regional election , becoming the third largest party in the regional parliament .", "prompt_labels": "Since(O) 2011(O) ,(O) they(O) are(O) part(O) of(O) the(O) Coalició(B-political party) Compromís(I-political party) coalition(O) ,(O) which(O) won(O) six(O) seats(O) in(O) the(O) 2011(B-election) Valencian(I-election) regional(I-election) election(I-election) s(O) and(O) 19(O) in(O) the(O) 2015(B-election) Valencian(I-election) regional(I-election) election(I-election) ,(O) becoming(O) the(O) third(O) largest(O) party(O) in(O) the(O) regional(O) parliament(O) .(O)"}}
{"id": "86", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "person", "country", "location", "political party", "election", "organization", "politician"], "instance": {"id": "86", "words": ["Fini", "was", "succeeded", "by", "Ignazio", "La", "Russa", ",", "who", "managed", "the", "merger", "of", "the", "party", "with", "Forza", "Italia", "(", "FI", ")", "into", "The", "People", "of", "Freedom", "(", "PdL", ")", "in", "2009", "."], "labels": ["B-politician", "O", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, country, location, political party, election, organization, politician and O.\nSentence: Fini was succeeded by Ignazio La Russa , who managed the merger of the party with Forza Italia ( FI ) into The People of Freedom ( PdL ) in 2009 .", "prompt_labels": "Fini(B-politician) was(O) succeeded(O) by(O) Ignazio(B-politician) La(I-politician) Russa(I-politician) ,(O) who(O) managed(O) the(O) merger(O) of(O) the(O) party(O) with(O) Forza(B-political party) Italia(I-political party) ((O) FI(B-political party) )(O) into(O) The(B-political party) People(I-political party) of(I-political party) Freedom(I-political party) ((O) PdL(B-political party) )(O) in(O) 2009(O) .(O)"}}
{"id": "552", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "location", "country", "organization", "political party", "person", "event", "election"], "instance": {"id": "552", "words": ["Speakers", "included", "Michael", "Moore", ",", "Whoopi", "Goldberg", ",", "Padma", "Lakshmi", ",", "Amy", "Schumer", ",", "Laura", "Benanti", ",", "Amber", "Tamblyn", ",", "Patricia", "Arquette", ",", "Rosie", "Perez", ",", "Piper", "Perabo", ",", "Drew", "Barrymore", ",", "and", "singers", "Cyndi", "Lauper", "and", "Halsey"], "labels": ["O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "B-person"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, country, organization, political party, person, event, election and O.\nSentence: Speakers included Michael Moore , Whoopi Goldberg , Padma Lakshmi , Amy Schumer , Laura Benanti , Amber Tamblyn , Patricia Arquette , Rosie Perez , Piper Perabo , Drew Barrymore , and singers Cyndi Lauper and Halsey", "prompt_labels": "Speakers(O) included(O) Michael(B-person) Moore(I-person) ,(O) Whoopi(B-person) Goldberg(I-person) ,(O) Padma(B-person) Lakshmi(I-person) ,(O) Amy(B-person) Schumer(I-person) ,(O) Laura(B-person) Benanti(I-person) ,(O) Amber(B-person) Tamblyn(I-person) ,(O) Patricia(B-person) Arquette(I-person) ,(O) Rosie(B-person) Perez(I-person) ,(O) Piper(B-person) Perabo(I-person) ,(O) Drew(B-person) Barrymore(I-person) ,(O) and(O) singers(O) Cyndi(B-person) Lauper(I-person) and(O) Halsey(B-person)"}}
{"id": "300", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "country", "organization", "politician", "location", "election", "political party", "event"], "instance": {"id": "300", "words": ["Carstairs", "led", "the", "Liberal", "Party", "to", "a", "dramatic", "resurgence", "in", "the", "1988", "Manitoba", "general", "election", ",", "which", "saw", "the", "election", "of", "a", "Progressive", "Conservative", "Party", "of", "Manitoba", "minority", "government", "under", "Gary", "Filmon", "and", "the", "reduction", "of", "the", "New", "Democratic", "Party", "of", "Manitoba", "from", "government", "to", "third", "party", "status", "."], "labels": ["B-politician", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, organization, politician, location, election, political party, event and O.\nSentence: Carstairs led the Liberal Party to a dramatic resurgence in the 1988 Manitoba general election , which saw the election of a Progressive Conservative Party of Manitoba minority government under Gary Filmon and the reduction of the New Democratic Party of Manitoba from government to third party status .", "prompt_labels": "Carstairs(B-politician) led(O) the(O) Liberal(B-political party) Party(I-political party) to(O) a(O) dramatic(O) resurgence(O) in(O) the(O) 1988(B-election) Manitoba(I-election) general(I-election) election(I-election) ,(O) which(O) saw(O) the(O) election(O) of(O) a(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Manitoba(I-political party) minority(O) government(O) under(O) Gary(B-politician) Filmon(I-politician) and(O) the(O) reduction(O) of(O) the(O) New(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Manitoba(I-political party) from(O) government(O) to(O) third(O) party(O) status(O) .(O)"}}
{"id": "312", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "organization", "election", "person", "country", "location", "event", "political party"], "instance": {"id": "312", "words": ["Ben-Gurion", "again", "formed", "the", "government", "with", "the", "support", "of", "Mizrachi", ",", "Hapoel", "HaMizrachi", ",", "Agudat", "Yisrael", ",", "Poalei", "Agudat", "Yisrael", "and", "the", "three", "Israeli", "Arab", "parties", "associated", "with", "Mapai", ",", "the", "Democratic", "List", "for", "Israeli", "Arabs", ",", "Progress", "and", "Work", "and", "Agriculture", "and", "Development", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, election, person, country, location, event, political party and O.\nSentence: Ben-Gurion again formed the government with the support of Mizrachi , Hapoel HaMizrachi , Agudat Yisrael , Poalei Agudat Yisrael and the three Israeli Arab parties associated with Mapai , the Democratic List for Israeli Arabs , Progress and Work and Agriculture and Development .", "prompt_labels": "Ben-Gurion(B-politician) again(O) formed(O) the(O) government(O) with(O) the(O) support(O) of(O) Mizrachi(B-organization) ,(O) Hapoel(B-political party) HaMizrachi(I-political party) ,(O) Agudat(B-political party) Yisrael(I-political party) ,(O) Poalei(B-political party) Agudat(I-political party) Yisrael(I-political party) and(O) the(O) three(O) Israeli(O) Arab(O) parties(O) associated(O) with(O) Mapai(B-political party) ,(O) the(O) Democratic(B-political party) List(I-political party) for(I-political party) Israeli(I-political party) Arabs(I-political party) ,(O) Progress(B-political party) and(I-political party) Work(I-political party) and(O) Agriculture(B-political party) and(I-political party) Development(I-political party) .(O)"}}
{"id": "113", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "election", "location", "politician", "organization", "country", "person", "political party"], "instance": {"id": "113", "words": ["Reid", "adopted", "a", "strategy", "of", "trying", "to", "reorient", "the", "party", "system", "along", "Australian", "Labor", "Party", "vs", "non-Labor", "lines", "-", "prior", "to", "the", "1906", "Australian", "federal", "election", ",", "he", "renamed", "his", "Free", "Trade", "Party", "to", "the", "Anti-Socialist", "Party", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, location, politician, organization, country, person, political party and O.\nSentence: Reid adopted a strategy of trying to reorient the party system along Australian Labor Party vs non-Labor lines - prior to the 1906 Australian federal election , he renamed his Free Trade Party to the Anti-Socialist Party .", "prompt_labels": "Reid(B-politician) adopted(O) a(O) strategy(O) of(O) trying(O) to(O) reorient(O) the(O) party(O) system(O) along(O) Australian(B-political party) Labor(I-political party) Party(I-political party) vs(O) non-Labor(O) lines(O) -(O) prior(O) to(O) the(O) 1906(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) he(O) renamed(O) his(O) Free(B-political party) Trade(I-political party) Party(I-political party) to(O) the(O) Anti-Socialist(B-political party) Party(I-political party) .(O)"}}
{"id": "76", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "location", "event", "organization", "politician", "person", "election", "political party"], "instance": {"id": "76", "words": ["The", "Kingdom", "of", "Hungary", "and", "the", "First", "Austrian", "Republic", "were", "treated", "as", "its", "successors", "de", "jure", ",", "whereas", "the", "independence", "of", "the", "West", "Slavs", "and", "South", "Slavs", "of", "the", "Empire", "as", "the", "First", "Czechoslovak", "Republic", ",", "the", "Second", "Polish", "Republic", "and", "the", "Kingdom", "of", "Yugoslavia", ",", "respectively", ",", "and", "most", "of", "the", "territorial", "demands", "of", "the", "Kingdom", "of", "Romania", "were", "also", "recognized", "by", "the", "victorious", "powers", "in", "1920", "."], "labels": ["O", "B-country", "I-country", "I-country", "O", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "I-country", "O", "O", "B-country", "I-country", "I-country", "O", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, event, organization, politician, person, election, political party and O.\nSentence: The Kingdom of Hungary and the First Austrian Republic were treated as its successors de jure , whereas the independence of the West Slavs and South Slavs of the Empire as the First Czechoslovak Republic , the Second Polish Republic and the Kingdom of Yugoslavia , respectively , and most of the territorial demands of the Kingdom of Romania were also recognized by the victorious powers in 1920 .", "prompt_labels": "The(O) Kingdom(B-country) of(I-country) Hungary(I-country) and(O) the(O) First(B-country) Austrian(I-country) Republic(I-country) were(O) treated(O) as(O) its(O) successors(O) de(O) jure(O) ,(O) whereas(O) the(O) independence(O) of(O) the(O) West(O) Slavs(O) and(O) South(O) Slavs(O) of(O) the(O) Empire(O) as(O) the(O) First(B-country) Czechoslovak(I-country) Republic(I-country) ,(O) the(O) Second(B-country) Polish(I-country) Republic(I-country) and(O) the(O) Kingdom(B-country) of(I-country) Yugoslavia(I-country) ,(O) respectively(O) ,(O) and(O) most(O) of(O) the(O) territorial(O) demands(O) of(O) the(O) Kingdom(B-country) of(I-country) Romania(I-country) were(O) also(O) recognized(O) by(O) the(O) victorious(O) powers(O) in(O) 1920(O) .(O)"}}
{"id": "82", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "person", "event", "location", "country", "election", "organization", "politician"], "instance": {"id": "82", "words": ["A", "2013", "University", "of", "the", "Sunshine", "Coast", "study", "of", "the", "voting", "intentions", "of", "journalists", "found", "that", "73.6", "per", "cent", "of", "ABC", "journalists", "supported", "Australian", "Labor", "Party", "or", "Australian", "Greens", "-", "with", "41", "%", "supporting", "the", "Greens", "(", "whereas", "only", "around", "10", "%", "of", "people", "in", "the", "general", "population", "voted", "Green", ")", "."], "labels": ["O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, event, location, country, election, organization, politician and O.\nSentence: A 2013 University of the Sunshine Coast study of the voting intentions of journalists found that 73.6 per cent of ABC journalists supported Australian Labor Party or Australian Greens - with 41 % supporting the Greens ( whereas only around 10 % of people in the general population voted Green ) .", "prompt_labels": "A(O) 2013(O) University(B-organization) of(I-organization) the(I-organization) Sunshine(I-organization) Coast(I-organization) study(O) of(O) the(O) voting(O) intentions(O) of(O) journalists(O) found(O) that(O) 73.6(O) per(O) cent(O) of(O) ABC(B-organization) journalists(O) supported(O) Australian(B-political party) Labor(I-political party) Party(I-political party) or(O) Australian(B-political party) Greens(I-political party) -(O) with(O) 41(O) %(O) supporting(O) the(O) Greens(B-political party) ((O) whereas(O) only(O) around(O) 10(O) %(O) of(O) people(O) in(O) the(O) general(O) population(O) voted(O) Green(B-political party) )(O) .(O)"}}
{"id": "177", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "organization", "politician", "event", "political party", "location", "country", "person"], "instance": {"id": "177", "words": ["The", "organization", "has", "local", "chapters", "in", "Fredericton", ",", "Montreal", ",", "Ottawa", ",", "Toronto", ",", "Hamilton", ",", "Edmonton", ",", "Calgary", "and", "Vancouver", ",", "plus", "international", "chapters", "in", "the", "US", "and", "the", "UK", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, politician, event, political party, location, country, person and O.\nSentence: The organization has local chapters in Fredericton , Montreal , Ottawa , Toronto , Hamilton , Edmonton , Calgary and Vancouver , plus international chapters in the US and the UK .", "prompt_labels": "The(O) organization(O) has(O) local(O) chapters(O) in(O) Fredericton(B-location) ,(O) Montreal(B-location) ,(O) Ottawa(B-location) ,(O) Toronto(B-location) ,(O) Hamilton(B-location) ,(O) Edmonton(B-location) ,(O) Calgary(B-location) and(O) Vancouver(B-location) ,(O) plus(O) international(O) chapters(O) in(O) the(O) US(B-country) and(O) the(O) UK(B-country) .(O)"}}
{"id": "240", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "politician", "organization", "country", "election", "person", "location", "event"], "instance": {"id": "240", "words": ["At", "the", "2003", "election", ",", "the", "Democratic", "Unionist", "Party", "and", "Sinn", "Féin", "displaced", "the", "more", "moderate", "Ulster", "Unionist", "Party", "and", "Social", "Democratic", "and", "Labour", "Party", "as", "the", "largest", "parties", "in", "the", "unionist", "and", "nationalist", "blocks", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, organization, country, election, person, location, event and O.\nSentence: At the 2003 election , the Democratic Unionist Party and Sinn Féin displaced the more moderate Ulster Unionist Party and Social Democratic and Labour Party as the largest parties in the unionist and nationalist blocks .", "prompt_labels": "At(O) the(O) 2003(O) election(O) ,(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) and(O) Sinn(B-political party) Féin(I-political party) displaced(O) the(O) more(O) moderate(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) and(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) as(O) the(O) largest(O) parties(O) in(O) the(O) unionist(O) and(O) nationalist(O) blocks(O) .(O)"}}
{"id": "110", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "event", "country", "person", "organization", "political party", "location", "politician"], "instance": {"id": "110", "words": ["MacKay", "was", "the", "final", "leader", "of", "the", "Progressive", "Conservative", "Party", "of", "Canada", "(", "PC", "Party", ")", ",", "and", "he", "agreed", "to", "merge", "the", "party", "with", "Stephen", "Harper", "'s", "Canadian", "Alliance", "in", "2003", ",", "forming", "the", "Conservative", "Party", "of", "Canada", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, country, person, organization, political party, location, politician and O.\nSentence: MacKay was the final leader of the Progressive Conservative Party of Canada ( PC Party ) , and he agreed to merge the party with Stephen Harper 's Canadian Alliance in 2003 , forming the Conservative Party of Canada .", "prompt_labels": "MacKay(B-politician) was(O) the(O) final(O) leader(O) of(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) ((O) PC(B-political party) Party(I-political party) )(O) ,(O) and(O) he(O) agreed(O) to(O) merge(O) the(O) party(O) with(O) Stephen(B-politician) Harper(I-politician) 's(O) Canadian(B-political party) Alliance(I-political party) in(O) 2003(O) ,(O) forming(O) the(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) .(O)"}}
{"id": "335", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "person", "election", "politician", "organization", "event", "political party", "location"], "instance": {"id": "335", "words": ["Both", "losses", "(", "1974", "Canadian", "federal", "election", "and", "1993", "Canadian", "federal", "election", ")", "have", "come", "at", "the", "hands", "of", "Liberal", "Party", "of", "Canada", "candidates", "who", "failed", "to", "retain", "the", "seat", "at", "the", "next", "election", "."], "labels": ["O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, election, politician, organization, event, political party, location and O.\nSentence: Both losses ( 1974 Canadian federal election and 1993 Canadian federal election ) have come at the hands of Liberal Party of Canada candidates who failed to retain the seat at the next election .", "prompt_labels": "Both(O) losses(O) ((O) 1974(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 1993(B-election) Canadian(I-election) federal(I-election) election(I-election) )(O) have(O) come(O) at(O) the(O) hands(O) of(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) candidates(O) who(O) failed(O) to(O) retain(O) the(O) seat(O) at(O) the(O) next(O) election(O) .(O)"}}
{"id": "270", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "event", "organization", "person", "politician", "location", "election", "country"], "instance": {"id": "270", "words": ["For", "the", "June", "1946", "French", "legislative", "election", ",", "the", "Rally", "of", "Left", "Republicans", "(", ",", "RGR", ")", ",", "which", "encompassed", "the", "Radical-Socialist", "Party", ",", "the", "Democratic", "and", "Socialist", "Union", "of", "the", "Resistance", "and", "other", "conservative", "parties", ",", "unsuccessfully", "attempted", "to", "oppose", "the", "Christian", "democrat", "and", "socialist", "Popular", "Republican", "Movement", "alliance", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, organization, person, politician, location, election, country and O.\nSentence: For the June 1946 French legislative election , the Rally of Left Republicans ( , RGR ) , which encompassed the Radical-Socialist Party , the Democratic and Socialist Union of the Resistance and other conservative parties , unsuccessfully attempted to oppose the Christian democrat and socialist Popular Republican Movement alliance .", "prompt_labels": "For(O) the(O) June(B-election) 1946(I-election) French(I-election) legislative(I-election) election(I-election) ,(O) the(O) Rally(B-political party) of(I-political party) Left(I-political party) Republicans(I-political party) ((O) ,(O) RGR(B-political party) )(O) ,(O) which(O) encompassed(O) the(O) Radical-Socialist(B-political party) Party(I-political party) ,(O) the(O) Democratic(B-political party) and(I-political party) Socialist(I-political party) Union(I-political party) of(I-political party) the(I-political party) Resistance(I-political party) and(O) other(O) conservative(O) parties(O) ,(O) unsuccessfully(O) attempted(O) to(O) oppose(O) the(O) Christian(O) democrat(O) and(O) socialist(O) Popular(B-political party) Republican(I-political party) Movement(I-political party) alliance(O) .(O)"}}
{"id": "95", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "organization", "politician", "location", "person", "election", "political party", "country"], "instance": {"id": "95", "words": ["He", "brought", "Israeli", "prime", "minister", "Ehud", "Barak", "and", "Palestinian", "Authority", "chairman", "Yasser", "Arafat", "together", "at", "Camp", "David", "for", "the", "Camp", "David", "Summit", "in", "July", "2000", ",", "which", "lasted", "14", "days", "."], "labels": ["O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, politician, location, person, election, political party, country and O.\nSentence: He brought Israeli prime minister Ehud Barak and Palestinian Authority chairman Yasser Arafat together at Camp David for the Camp David Summit in July 2000 , which lasted 14 days .", "prompt_labels": "He(O) brought(O) Israeli(O) prime(O) minister(O) Ehud(B-politician) Barak(I-politician) and(O) Palestinian(O) Authority(O) chairman(O) Yasser(B-politician) Arafat(I-politician) together(O) at(O) Camp(B-location) David(I-location) for(O) the(O) Camp(O) David(O) Summit(O) in(O) July(O) 2000(O) ,(O) which(O) lasted(O) 14(O) days(O) .(O)"}}
{"id": "200", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "organization", "political party", "country", "election", "person", "location", "politician"], "instance": {"id": "200", "words": ["It", "also", "became", "a", "member", "of", "the", "March", "14", "Alliance", ",", "along", "with", "the", "Future", "Movement", ",", "Progressive", "Socialist", "Party", ",", "Lebanese", "Forces", "and", "other", "minor", "parties", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, political party, country, election, person, location, politician and O.\nSentence: It also became a member of the March 14 Alliance , along with the Future Movement , Progressive Socialist Party , Lebanese Forces and other minor parties .", "prompt_labels": "It(O) also(O) became(O) a(O) member(O) of(O) the(O) March(B-political party) 14(I-political party) Alliance(I-political party) ,(O) along(O) with(O) the(O) Future(B-political party) Movement(I-political party) ,(O) Progressive(B-political party) Socialist(I-political party) Party(I-political party) ,(O) Lebanese(B-political party) Forces(I-political party) and(O) other(O) minor(O) parties(O) .(O)"}}
{"id": "407", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "election", "person", "event", "location", "political party", "organization", "politician"], "instance": {"id": "407", "words": ["He", "stood", "for", "election", "to", "Parliament", "at", "the", "1987", "United", "Kingdom", "general", "election", ",", "the", "1992", "United", "Kingdom", "general", "election", "and", "1997", "United", "Kingdom", "general", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, person, event, location, political party, organization, politician and O.\nSentence: He stood for election to Parliament at the 1987 United Kingdom general election , the 1992 United Kingdom general election and 1997 United Kingdom general election .", "prompt_labels": "He(O) stood(O) for(O) election(O) to(O) Parliament(O) at(O) the(O) 1987(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1992(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1997(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "523", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "event", "politician", "political party", "location", "election", "person", "organization"], "instance": {"id": "523", "words": [")", "Christine", "Quinn", ",", "the", "Speaker", "of", "the", "New", "York", "City", "Council", ",", "came", "in", "third", ",", "with", "15.7", "%", ",", "while", "none", "of", "the", "other", "candidates", ",", "including", "City", "Comptroller", "John", "Liu", "and", "former", "Congressman", "Anthony", "Weiner", ",", "won", "as", "much", "as", "10", "%", "."], "labels": ["O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, politician, political party, location, election, person, organization and O.\nSentence: ) Christine Quinn , the Speaker of the New York City Council , came in third , with 15.7 % , while none of the other candidates , including City Comptroller John Liu and former Congressman Anthony Weiner , won as much as 10 % .", "prompt_labels": ")(O) Christine(B-politician) Quinn(I-politician) ,(O) the(O) Speaker(O) of(O) the(O) New(B-organization) York(I-organization) City(I-organization) Council(I-organization) ,(O) came(O) in(O) third(O) ,(O) with(O) 15.7(O) %(O) ,(O) while(O) none(O) of(O) the(O) other(O) candidates(O) ,(O) including(O) City(O) Comptroller(O) John(B-politician) Liu(I-politician) and(O) former(O) Congressman(O) Anthony(B-politician) Weiner(I-politician) ,(O) won(O) as(O) much(O) as(O) 10(O) %(O) .(O)"}}
{"id": "98", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "politician", "event", "organization", "country", "political party", "person", "location"], "instance": {"id": "98", "words": ["In", "the", "last", "2014", "European", "Parliament", "election", ",", "the", "Portuguese", "Communist", "Party", "won", "three", "seats", "and", "the", "Left", "Bloc", "won", "one", "seat", "."], "labels": ["O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, event, organization, country, political party, person, location and O.\nSentence: In the last 2014 European Parliament election , the Portuguese Communist Party won three seats and the Left Bloc won one seat .", "prompt_labels": "In(O) the(O) last(O) 2014(B-election) European(I-election) Parliament(I-election) election(I-election) ,(O) the(O) Portuguese(B-political party) Communist(I-political party) Party(I-political party) won(O) three(O) seats(O) and(O) the(O) Left(B-political party) Bloc(I-political party) won(O) one(O) seat(O) .(O)"}}
{"id": "567", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "political party", "organization", "person", "country", "event", "election", "location"], "instance": {"id": "567", "words": ["Corin", "met", "with", "fellow", "students", "at", "Stoneman", "Douglas", ",", "including", "David", "Hogg", ",", "Emma", "González", "and", "Cameron", "Kasky", ",", "at", "Kasky", "'s", "house", ";", "they", "formed", "the", "Never", "Again", "MSD", "movement", "during", "these", "meetings", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, organization, person, country, event, election, location and O.\nSentence: Corin met with fellow students at Stoneman Douglas , including David Hogg , Emma González and Cameron Kasky , at Kasky 's house ; they formed the Never Again MSD movement during these meetings .", "prompt_labels": "Corin(B-person) met(O) with(O) fellow(O) students(O) at(O) Stoneman(B-location) Douglas(I-location) ,(O) including(O) David(B-person) Hogg(I-person) ,(O) Emma(B-person) González(I-person) and(O) Cameron(B-person) Kasky(I-person) ,(O) at(O) Kasky(B-person) 's(O) house(O) ;(O) they(O) formed(O) the(O) Never(B-organization) Again(I-organization) MSD(I-organization) movement(O) during(O) these(O) meetings(O) .(O)"}}
{"id": "574", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "location", "election", "politician", "person", "political party", "country", "event"], "instance": {"id": "574", "words": ["In", "this", "interview", "conducted", "in", "Cambridge", ",", "Massachusetts", "on", "February", "12", ",", "2004", ",", "Chomsky", "starts", "by", "responding", "to", "Robert", "McNamara", "'", "s", "comments", "about", "the", "World", "War", "II", "firebombing", "of", "Tokyo", "in", "The", "Fog", "of", "War", "by", "pointing", "out", "that", "definition", "of", "a", "war", "crime", "at", "the", "Nuremberg", "Trials", "was", "anything", "the", "enemy", "did", "that", "the", "Allies", "didn", "'t", "do", "and", "goes", "on", "to", "point", "out", "that", "this", "logic", "is", "central", "to", "the", "Bush", "doctrine", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, election, politician, person, political party, country, event and O.\nSentence: In this interview conducted in Cambridge , Massachusetts on February 12 , 2004 , Chomsky starts by responding to Robert McNamara ' s comments about the World War II firebombing of Tokyo in The Fog of War by pointing out that definition of a war crime at the Nuremberg Trials was anything the enemy did that the Allies didn 't do and goes on to point out that this logic is central to the Bush doctrine .", "prompt_labels": "In(O) this(O) interview(O) conducted(O) in(O) Cambridge(B-organization) ,(O) Massachusetts(B-organization) on(O) February(O) 12(O) ,(O) 2004(O) ,(O) Chomsky(O) starts(O) by(O) responding(O) to(O) Robert(B-politician) McNamara(I-politician) '(O) s(O) comments(O) about(O) the(O) World(B-event) War(I-event) II(I-event) firebombing(B-event) of(I-event) Tokyo(I-event) in(O) The(O) Fog(O) of(O) War(O) by(O) pointing(O) out(O) that(O) definition(O) of(O) a(O) war(O) crime(O) at(O) the(O) Nuremberg(B-event) Trials(I-event) was(O) anything(O) the(O) enemy(O) did(O) that(O) the(O) Allies(O) didn(O) 't(O) do(O) and(O) goes(O) on(O) to(O) point(O) out(O) that(O) this(O) logic(O) is(O) central(O) to(O) the(O) Bush(B-politician) doctrine(I-politician) .(O)"}}
{"id": "218", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "event", "organization", "politician", "election", "political party", "country", "person"], "instance": {"id": "218", "words": ["He", "was", "educated", "at", "the", "University", "of", "Basel", ",", "University", "of", "Heidelberg", ",", "University", "of", "Bern", ",", "and", "University", "of", "Paris", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, organization, politician, election, political party, country, person and O.\nSentence: He was educated at the University of Basel , University of Heidelberg , University of Bern , and University of Paris .", "prompt_labels": "He(O) was(O) educated(O) at(O) the(O) University(B-organization) of(I-organization) Basel(I-organization) ,(O) University(B-organization) of(I-organization) Heidelberg(I-organization) ,(O) University(B-organization) of(I-organization) Bern(I-organization) ,(O) and(O) University(B-organization) of(I-organization) Paris(I-organization) .(O)"}}
{"id": "332", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "event", "election", "political party", "organization", "country", "location", "person"], "instance": {"id": "332", "words": ["In", "1980", "the", "party", "merged", "with", "the", "Anti-Revolutionary", "Party", "(", "ARP", ")", "and", "the", "Christian", "Historical", "Union", "(", "CHU", ")", "to", "form", "the", "Christian", "Democratic", "Appeal", "(", "CDA", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, election, political party, organization, country, location, person and O.\nSentence: In 1980 the party merged with the Anti-Revolutionary Party ( ARP ) and the Christian Historical Union ( CHU ) to form the Christian Democratic Appeal ( CDA ) .", "prompt_labels": "In(O) 1980(O) the(O) party(O) merged(O) with(O) the(O) Anti-Revolutionary(B-political party) Party(I-political party) ((O) ARP(B-political party) )(O) and(O) the(O) Christian(B-political party) Historical(I-political party) Union(I-political party) ((O) CHU(B-political party) )(O) to(O) form(O) the(O) Christian(B-political party) Democratic(I-political party) Appeal(I-political party) ((O) CDA(B-political party) )(O) .(O)"}}
{"id": "194", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "location", "politician", "election", "country", "organization", "political party", "event"], "instance": {"id": "194", "words": ["Throughout", "the", "2000s", "(", "decade", ")", ",", "Thai", "aggressively", "continued", "its", "route", "network", "expansion", "with", "new", "services", "to", "Chengdu", ",", "Busan", ",", "Chennai", ",", "Xiamen", ",", "Milan", ",", "Moscow", ",", "Islamabad", ",", "Hyderabad", ",", "Johannesburg", "(", "later", "suspended", ")", "and", "Oslo", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, politician, election, country, organization, political party, event and O.\nSentence: Throughout the 2000s ( decade ) , Thai aggressively continued its route network expansion with new services to Chengdu , Busan , Chennai , Xiamen , Milan , Moscow , Islamabad , Hyderabad , Johannesburg ( later suspended ) and Oslo .", "prompt_labels": "Throughout(O) the(O) 2000s(O) ((O) decade(O) )(O) ,(O) Thai(O) aggressively(O) continued(O) its(O) route(O) network(O) expansion(O) with(O) new(O) services(O) to(O) Chengdu(B-location) ,(O) Busan(B-location) ,(O) Chennai(B-location) ,(O) Xiamen(B-location) ,(O) Milan(B-location) ,(O) Moscow(B-location) ,(O) Islamabad(B-location) ,(O) Hyderabad(B-location) ,(O) Johannesburg(B-location) ((O) later(O) suspended(O) )(O) and(O) Oslo(B-location) .(O)"}}
{"id": "484", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "political party", "location", "organization", "country", "event", "person", "politician"], "instance": {"id": "484", "words": ["He", "joined", "Mel", "Hurtig", "'", "s", "National", "Party", "of", "Canada", "in", "1993", ",", "and", "later", "campaigned", "for", "the", "Green", "Party", "of", "Canada", "and", "the", "Alberta", "Greens", "."], "labels": ["O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, location, organization, country, event, person, politician and O.\nSentence: He joined Mel Hurtig ' s National Party of Canada in 1993 , and later campaigned for the Green Party of Canada and the Alberta Greens .", "prompt_labels": "He(O) joined(O) Mel(B-politician) Hurtig(I-politician) '(O) s(O) National(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) in(O) 1993(O) ,(O) and(O) later(O) campaigned(O) for(O) the(O) Green(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) the(O) Alberta(B-political party) Greens(I-political party) .(O)"}}
{"id": "84", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "location", "politician", "person", "event", "election", "organization", "country"], "instance": {"id": "84", "words": ["This", "school", "initially", "consisted", "of", "nearly", "200", "faculty", "members", "and", "Ph.D.", "students", "from", "the", "Vrije", "Universiteit", ",", "University", "of", "Amsterdam", ",", "Delft", "University", "of", "Technology", ",", "and", "Leiden", "University", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, location, politician, person, event, election, organization, country and O.\nSentence: This school initially consisted of nearly 200 faculty members and Ph.D. students from the Vrije Universiteit , University of Amsterdam , Delft University of Technology , and Leiden University .", "prompt_labels": "This(O) school(O) initially(O) consisted(O) of(O) nearly(O) 200(O) faculty(O) members(O) and(O) Ph.D.(O) students(O) from(O) the(O) Vrije(B-organization) Universiteit(I-organization) ,(O) University(B-organization) of(I-organization) Amsterdam(I-organization) ,(O) Delft(B-organization) University(I-organization) of(I-organization) Technology(I-organization) ,(O) and(O) Leiden(B-organization) University(I-organization) .(O)"}}
{"id": "483", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "politician", "organization", "country", "political party", "person", "election", "location"], "instance": {"id": "483", "words": ["Party", "leader", "Peter", "Dunne", "was", "re-elected", "into", "the", "Ōhariū", "seat", "in", "2008", "New", "Zealand", "general", "election", ",", "2011", "New", "Zealand", "general", "election", ",", "and", "2014", "New", "Zealand", "general", "election", ",", "becoming", "a", "coalition", "partner", "with", "National", ",", "despite", "receiving", "under", "1", "%", "of", "the", "party", "vote", "."], "labels": ["O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-location", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, organization, country, political party, person, election, location and O.\nSentence: Party leader Peter Dunne was re-elected into the Ōhariū seat in 2008 New Zealand general election , 2011 New Zealand general election , and 2014 New Zealand general election , becoming a coalition partner with National , despite receiving under 1 % of the party vote .", "prompt_labels": "Party(O) leader(O) Peter(B-politician) Dunne(I-politician) was(O) re-elected(O) into(O) the(O) Ōhariū(B-location) seat(O) in(O) 2008(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) 2011(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) and(O) 2014(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) becoming(O) a(O) coalition(O) partner(O) with(O) National(O) ,(O) despite(O) receiving(O) under(O) 1(O) %(O) of(O) the(O) party(O) vote(O) .(O)"}}
{"id": "490", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "organization", "location", "person", "country", "election", "political party", "politician"], "instance": {"id": "490", "words": ["She", "had", "previously", "been", "the", "party", "'s", "candidate", "for", "Newmarket", "-", "Aurora", "in", "the", "2004", "Canadian", "federal", "election", ",", "losing", "narrowly", "to", "Conservative", "Party", "of", "Canada", "candidate", "Belinda", "Stronach", ",", "and", "the", "first", "declared", "candidate", "for", "the", "2006", "Liberal", "Party", "of", "Canada", "leadership", "election", "to", "succeed", "Paul", "Martin", "in", "2006", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-politician", "I-politician", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, location, person, country, election, political party, politician and O.\nSentence: She had previously been the party 's candidate for Newmarket - Aurora in the 2004 Canadian federal election , losing narrowly to Conservative Party of Canada candidate Belinda Stronach , and the first declared candidate for the 2006 Liberal Party of Canada leadership election to succeed Paul Martin in 2006 .", "prompt_labels": "She(O) had(O) previously(O) been(O) the(O) party(O) 's(O) candidate(O) for(O) Newmarket(B-location) -(I-location) Aurora(I-location) in(O) the(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) losing(O) narrowly(O) to(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) candidate(O) Belinda(B-politician) Stronach(I-politician) ,(O) and(O) the(O) first(O) declared(O) candidate(O) for(O) the(O) 2006(B-election) Liberal(I-election) Party(I-election) of(I-election) Canada(I-election) leadership(I-election) election(I-election) to(O) succeed(O) Paul(B-politician) Martin(I-politician) in(O) 2006(O) .(O)"}}
{"id": "39", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "organization", "person", "location", "politician", "election", "country", "event"], "instance": {"id": "39", "words": ["During", "the", "1988", "United", "States", "presidential", "election", ",", "Vice", "President", "George", "H.", "W.", "Bush", "noted", "that", "his", "opponent", "Massachusetts", "Governor", "Michael", "Dukakis", "had", "described", "himself", "as", "a", "card-carrying", "member", "of", "the", "ACLU", "and", "used", "that", "as", "evidence", "that", "Dukakis", "was", "a", "strong", ",", "passionate", "liberal", "and", "out", "of", "the", "mainstream", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-politician", "I-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, person, location, politician, election, country, event and O.\nSentence: During the 1988 United States presidential election , Vice President George H. W. Bush noted that his opponent Massachusetts Governor Michael Dukakis had described himself as a card-carrying member of the ACLU and used that as evidence that Dukakis was a strong , passionate liberal and out of the mainstream .", "prompt_labels": "During(O) the(O) 1988(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) Vice(O) President(O) George(B-politician) H.(I-politician) W.(I-politician) Bush(I-politician) noted(O) that(O) his(O) opponent(O) Massachusetts(O) Governor(O) Michael(B-politician) Dukakis(I-politician) had(O) described(O) himself(O) as(O) a(O) card-carrying(O) member(O) of(O) the(O) ACLU(B-organization) and(O) used(O) that(O) as(O) evidence(O) that(O) Dukakis(B-politician) was(O) a(O) strong(O) ,(O) passionate(O) liberal(O) and(O) out(O) of(O) the(O) mainstream(O) .(O)"}}
{"id": "442", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "election", "person", "country", "event", "politician", "location", "organization"], "instance": {"id": "442", "words": ["At", "its", "founding", "congress", ",", "the", "NPSI", "decided", "to", "enter", "the", "centre-right", "House", "of", "Freedoms", "(", "CdL", ")", "coalition", "led", "by", "Silvio", "Berlusconi", "(", "a", "former", "friend", "of", "Bettino", "Craxi", ")", "as", "the", "centre-left", "was", "considered", "too", "compromised", "with", "the", "Mani", "pulite", "investigation", ",", "upon", "which", "the", "old", "Italian", "Socialist", "Party", "was", "disbanded", "while", "the", "ex", "Democratic", "Party", "of", "the", "Left", "were", "not", "touched", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, person, country, event, politician, location, organization and O.\nSentence: At its founding congress , the NPSI decided to enter the centre-right House of Freedoms ( CdL ) coalition led by Silvio Berlusconi ( a former friend of Bettino Craxi ) as the centre-left was considered too compromised with the Mani pulite investigation , upon which the old Italian Socialist Party was disbanded while the ex Democratic Party of the Left were not touched .", "prompt_labels": "At(O) its(O) founding(O) congress(O) ,(O) the(O) NPSI(B-organization) decided(O) to(O) enter(O) the(O) centre-right(O) House(B-political party) of(I-political party) Freedoms(I-political party) ((O) CdL(B-political party) )(O) coalition(O) led(O) by(O) Silvio(B-politician) Berlusconi(I-politician) ((O) a(O) former(O) friend(O) of(O) Bettino(B-politician) Craxi(I-politician) )(O) as(O) the(O) centre-left(O) was(O) considered(O) too(O) compromised(O) with(O) the(O) Mani(B-organization) pulite(I-organization) investigation(O) ,(O) upon(O) which(O) the(O) old(O) Italian(B-political party) Socialist(I-political party) Party(I-political party) was(O) disbanded(O) while(O) the(O) ex(O) Democratic(B-political party) Party(I-political party) of(I-political party) the(I-political party) Left(I-political party) were(O) not(O) touched(O) .(O)"}}
{"id": "516", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "election", "location", "country", "politician", "event", "political party", "organization"], "instance": {"id": "516", "words": ["Republican", "incumbent", "Bob", "Packwood", "was", "re-elected", "to", "a", "third", "term", ",", "defeating", "Democratic", "state", "senator", "Ted", "Kulongoski", "and", "Libertarian", "Tonie", "Nathan", "."], "labels": ["O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, location, country, politician, event, political party, organization and O.\nSentence: Republican incumbent Bob Packwood was re-elected to a third term , defeating Democratic state senator Ted Kulongoski and Libertarian Tonie Nathan .", "prompt_labels": "Republican(O) incumbent(O) Bob(B-politician) Packwood(I-politician) was(O) re-elected(O) to(O) a(O) third(O) term(O) ,(O) defeating(O) Democratic(O) state(O) senator(O) Ted(B-politician) Kulongoski(I-politician) and(O) Libertarian(O) Tonie(B-politician) Nathan(I-politician) .(O)"}}
{"id": "343", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "country", "organization", "event", "election", "location", "person", "politician"], "instance": {"id": "343", "words": ["In", "2003", ",", "Kaczyński", "co-founded", "the", "Law", "and", "Justice", "party", ",", "after", "splitting", "from", "the", "Solidarity", "Electoral", "Action", "and", "the", "Christian", "National", "Union", ",", "along", "with", "his", "brother", "."], "labels": ["O", "O", "O", "B-politician", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, organization, event, election, location, person, politician and O.\nSentence: In 2003 , Kaczyński co-founded the Law and Justice party , after splitting from the Solidarity Electoral Action and the Christian National Union , along with his brother .", "prompt_labels": "In(O) 2003(O) ,(O) Kaczyński(B-politician) co-founded(O) the(O) Law(B-political party) and(I-political party) Justice(I-political party) party(O) ,(O) after(O) splitting(O) from(O) the(O) Solidarity(B-political party) Electoral(I-political party) Action(I-political party) and(O) the(O) Christian(B-political party) National(I-political party) Union(I-political party) ,(O) along(O) with(O) his(O) brother(O) .(O)"}}
{"id": "99", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "country", "politician", "location", "event", "organization", "person", "election"], "instance": {"id": "99", "words": ["In", "the", "2019", "European", "Parliament", "election", "in", "Portugal", ",", "Left", "Bloc", "took", "9.83", "%", "and", "gained", "1", "seat", ",", "Portuguese", "Communist", "Party", "working", "in", "coalition", "with", "Ecologist", "Party", ",", "The", "Greens", "took", "6.88", "%", "and", "2", "seats", "and", "National", "Renovator", "Party", "(", "PNR", ")", "polled", "just", "0.49", "%", ",", "with", "no", "seats", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-country", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, politician, location, event, organization, person, election and O.\nSentence: In the 2019 European Parliament election in Portugal , Left Bloc took 9.83 % and gained 1 seat , Portuguese Communist Party working in coalition with Ecologist Party , The Greens took 6.88 % and 2 seats and National Renovator Party ( PNR ) polled just 0.49 % , with no seats .", "prompt_labels": "In(O) the(O) 2019(B-election) European(I-election) Parliament(I-election) election(I-election) in(O) Portugal(B-country) ,(O) Left(B-political party) Bloc(I-political party) took(O) 9.83(O) %(O) and(O) gained(O) 1(O) seat(O) ,(O) Portuguese(B-political party) Communist(I-political party) Party(I-political party) working(O) in(O) coalition(O) with(O) Ecologist(B-political party) Party(I-political party) ,(O) The(B-political party) Greens(I-political party) took(O) 6.88(O) %(O) and(O) 2(O) seats(O) and(O) National(B-political party) Renovator(I-political party) Party(I-political party) ((O) PNR(B-political party) )(O) polled(O) just(O) 0.49(O) %(O) ,(O) with(O) no(O) seats(O) .(O)"}}
{"id": "191", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "election", "politician", "event", "political party", "location", "country", "person"], "instance": {"id": "191", "words": ["This", "tends", "to", "lead", "to", "the", "chamber", "being", "dominated", "by", "two", "major", "political", "groups", ",", "the", "centre-right", "Coalition", "(", "consisting", "of", "the", "Liberal", "Party", "of", "Australia", "and", "National", "Party", "of", "Australia", "Parties", ")", "and", "the", "centre-left", "Australian", "Labor", "Party", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, politician, event, political party, location, country, person and O.\nSentence: This tends to lead to the chamber being dominated by two major political groups , the centre-right Coalition ( consisting of the Liberal Party of Australia and National Party of Australia Parties ) and the centre-left Australian Labor Party .", "prompt_labels": "This(O) tends(O) to(O) lead(O) to(O) the(O) chamber(O) being(O) dominated(O) by(O) two(O) major(O) political(O) groups(O) ,(O) the(O) centre-right(O) Coalition(O) ((O) consisting(O) of(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) and(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) Parties(O) )(O) and(O) the(O) centre-left(O) Australian(B-political party) Labor(I-political party) Party(I-political party) .(O)"}}
{"id": "184", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "organization", "event", "politician", "country", "political party", "location", "election"], "instance": {"id": "184", "words": ["Some", "international", "routes", "operate", "from", "Chengdu", ",", "Chongqing", ",", "Dalian", ",", "Hangzhou", ",", "Kunming", "and", "Xiamen", "."], "labels": ["O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, event, politician, country, political party, location, election and O.\nSentence: Some international routes operate from Chengdu , Chongqing , Dalian , Hangzhou , Kunming and Xiamen .", "prompt_labels": "Some(O) international(O) routes(O) operate(O) from(O) Chengdu(B-location) ,(O) Chongqing(B-location) ,(O) Dalian(B-location) ,(O) Hangzhou(B-location) ,(O) Kunming(B-location) and(O) Xiamen(B-location) .(O)"}}
{"id": "102", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "country", "organization", "politician", "election", "person", "location", "political party"], "instance": {"id": "102", "words": ["After", "the", "1921", "Canadian", "federal", "election", ",", "Progressive", "Party", "of", "Canada", "leader", "Thomas", "Crerar", "was", "considering", "a", "merger", "of", "his", "party", "with", "the", "Liberal", "Party", "of", "Canada", "and", "asked", "Greenfield", "to", "join", "him", "as", "Alberta", "'s", "representative", "in", "the", "federal", "cabinet", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-politician", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, organization, politician, election, person, location, political party and O.\nSentence: After the 1921 Canadian federal election , Progressive Party of Canada leader Thomas Crerar was considering a merger of his party with the Liberal Party of Canada and asked Greenfield to join him as Alberta 's representative in the federal cabinet .", "prompt_labels": "After(O) the(O) 1921(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) Progressive(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) leader(O) Thomas(B-politician) Crerar(I-politician) was(O) considering(O) a(O) merger(O) of(O) his(O) party(O) with(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) asked(O) Greenfield(B-politician) to(O) join(O) him(O) as(O) Alberta(B-location) 's(O) representative(O) in(O) the(O) federal(O) cabinet(O) .(O)"}}
{"id": "3", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "political party", "person", "politician", "event", "organization", "country", "location"], "instance": {"id": "3", "words": ["Many", "of", "the", "most", "prominent", "national", "leaders", ",", "such", "as", "Washington", ",", "John", "Adams", ",", "John", "Hancock", ",", "and", "Benjamin", "Franklin", ",", "retired", "from", "public", "life", ",", "served", "as", "foreign", "delegates", ",", "or", "held", "office", "in", "state", "governments", ";", "and", "for", "the", "general", "public", ",", "local", "government", "and", "self-rule", "seemed", "quite", "satisfactory", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, person, politician, event, organization, country, location and O.\nSentence: Many of the most prominent national leaders , such as Washington , John Adams , John Hancock , and Benjamin Franklin , retired from public life , served as foreign delegates , or held office in state governments ; and for the general public , local government and self-rule seemed quite satisfactory .", "prompt_labels": "Many(O) of(O) the(O) most(O) prominent(O) national(O) leaders(O) ,(O) such(O) as(O) Washington(B-politician) ,(O) John(B-politician) Adams(I-politician) ,(O) John(B-politician) Hancock(I-politician) ,(O) and(O) Benjamin(B-politician) Franklin(I-politician) ,(O) retired(O) from(O) public(O) life(O) ,(O) served(O) as(O) foreign(O) delegates(O) ,(O) or(O) held(O) office(O) in(O) state(O) governments(O) ;(O) and(O) for(O) the(O) general(O) public(O) ,(O) local(O) government(O) and(O) self-rule(O) seemed(O) quite(O) satisfactory(O) .(O)"}}
{"id": "586", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "person", "political party", "location", "organization", "election", "country", "event"], "instance": {"id": "586", "words": ["On", "the", "second", "day", "of", "the", "Summit", ",", "President", "Obama", "announced", "that", "the", "next", "summit", "meeting", "about", "this", "subject", "will", "be", "in", "South", "Korea", ";", "see", "2012", "Nuclear", "Security", "Summit", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-event", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, political party, location, organization, election, country, event and O.\nSentence: On the second day of the Summit , President Obama announced that the next summit meeting about this subject will be in South Korea ; see 2012 Nuclear Security Summit .", "prompt_labels": "On(O) the(O) second(O) day(O) of(O) the(O) Summit(B-event) ,(O) President(O) Obama(B-politician) announced(O) that(O) the(O) next(O) summit(O) meeting(O) about(O) this(O) subject(O) will(O) be(O) in(O) South(B-country) Korea(I-country) ;(O) see(O) 2012(B-event) Nuclear(I-event) Security(I-event) Summit(I-event) .(O)"}}
{"id": "500", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "organization", "political party", "election", "location", "country", "politician", "event"], "instance": {"id": "500", "words": ["In", "the", "last", "days", "of", "the", "campaign", ",", "D", "'Amato", "campaigned", "with", "popular", "Governor", "George", "Pataki", ",", "who", "was", "also", "running", "for", "re-election", ",", "and", "was", "also", "supported", "by", "New", "York", "City", "Mayor", "Rudy", "Giuliani", "and", "former", "Mayor", "Ed", "Koch", "(", "a", "Democrat", ")"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-politician", "I-politician", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, political party, election, location, country, politician, event and O.\nSentence: In the last days of the campaign , D 'Amato campaigned with popular Governor George Pataki , who was also running for re-election , and was also supported by New York City Mayor Rudy Giuliani and former Mayor Ed Koch ( a Democrat )", "prompt_labels": "In(O) the(O) last(O) days(O) of(O) the(O) campaign(O) ,(O) D(B-politician) 'Amato(I-politician) campaigned(O) with(O) popular(O) Governor(O) George(B-politician) Pataki(I-politician) ,(O) who(O) was(O) also(O) running(O) for(O) re-election(O) ,(O) and(O) was(O) also(O) supported(O) by(O) New(B-location) York(I-location) City(I-location) Mayor(O) Rudy(B-politician) Giuliani(I-politician) and(O) former(O) Mayor(O) Ed(B-politician) Koch(I-politician) ((O) a(O) Democrat(O) )(O)"}}
{"id": "72", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "event", "politician", "political party", "organization", "country", "person", "location"], "instance": {"id": "72", "words": ["During", "the", "1930s", "and", "1940s", ",", "Joseph", "Stalin", "'", "s", "NKVD", "carried", "out", "numerous", "assassinations", "outside", "of", "the", "Soviet", "Union", ",", "such", "as", "the", "killings", "of", "Organization", "of", "Ukrainian", "Nationalists", "leader", "Yevhen", "Konovalets", ",", "Ignace", "Poretsky", ",", "Fourth", "International", "secretary", "Rudolf", "Klement", ",", "Leon", "Trotsky", ",", "and", "the", "Workers", "'", "Party", "of", "Marxist", "Unification", "(", "POUM", ")", "leadership", "in", "Catalonia", "Michael", "Ellman", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "B-organization", "I-organization", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-location", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, politician, political party, organization, country, person, location and O.\nSentence: During the 1930s and 1940s , Joseph Stalin ' s NKVD carried out numerous assassinations outside of the Soviet Union , such as the killings of Organization of Ukrainian Nationalists leader Yevhen Konovalets , Ignace Poretsky , Fourth International secretary Rudolf Klement , Leon Trotsky , and the Workers ' Party of Marxist Unification ( POUM ) leadership in Catalonia Michael Ellman .", "prompt_labels": "During(O) the(O) 1930s(O) and(O) 1940s(O) ,(O) Joseph(B-politician) Stalin(I-politician) '(O) s(O) NKVD(B-organization) carried(O) out(O) numerous(O) assassinations(O) outside(O) of(O) the(O) Soviet(B-country) Union(I-country) ,(O) such(O) as(O) the(O) killings(O) of(O) Organization(B-political party) of(I-political party) Ukrainian(I-political party) Nationalists(I-political party) leader(O) Yevhen(B-politician) Konovalets(I-politician) ,(O) Ignace(B-politician) Poretsky(I-politician) ,(O) Fourth(B-organization) International(I-organization) secretary(O) Rudolf(B-politician) Klement(I-politician) ,(O) Leon(B-politician) Trotsky(I-politician) ,(O) and(O) the(O) Workers(B-political party) '(I-political party) Party(I-political party) of(I-political party) Marxist(I-political party) Unification(I-political party) ((O) POUM(B-political party) )(O) leadership(O) in(O) Catalonia(B-location) Michael(B-politician) Ellman(I-politician) .(O)"}}
{"id": "59", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "event", "country", "election", "organization", "person", "location", "political party"], "instance": {"id": "59", "words": ["Early", "loans", "went", "largely", "to", "Indonesia", ",", "Thailand", ",", "Malaysia", ",", "South", "Korea", "and", "the", "Philippines", ";", "these", "nations", "accounted", "for", "78.48", "%", "of", "the", "total", "ADB", "loans", "between", "1967", "and", "1972", "."], "labels": ["O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "I-country", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, country, election, organization, person, location, political party and O.\nSentence: Early loans went largely to Indonesia , Thailand , Malaysia , South Korea and the Philippines ; these nations accounted for 78.48 % of the total ADB loans between 1967 and 1972 .", "prompt_labels": "Early(O) loans(O) went(O) largely(O) to(O) Indonesia(B-country) ,(O) Thailand(B-country) ,(O) Malaysia(B-country) ,(O) South(B-country) Korea(I-country) and(O) the(O) Philippines(B-country) ;(O) these(O) nations(O) accounted(O) for(O) 78.48(O) %(O) of(O) the(O) total(O) ADB(B-organization) loans(O) between(O) 1967(O) and(O) 1972(O) .(O)"}}
{"id": "97", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "politician", "organization", "country", "location", "political party", "event", "election"], "instance": {"id": "97", "words": ["The", "main", "Eurosceptic", "parties", "in", "Portugal", "are", "National", "Renovator", "Party", "(", "PNR", ")", ",", "Portuguese", "Communist", "Party", "(", "PCP", ")", ",", "and", "Left", "Bloc", "(", "BE", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-country", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, organization, country, location, political party, event, election and O.\nSentence: The main Eurosceptic parties in Portugal are National Renovator Party ( PNR ) , Portuguese Communist Party ( PCP ) , and Left Bloc ( BE ) .", "prompt_labels": "The(O) main(O) Eurosceptic(O) parties(O) in(O) Portugal(B-country) are(O) National(B-political party) Renovator(I-political party) Party(I-political party) ((O) PNR(B-political party) )(O) ,(O) Portuguese(B-political party) Communist(I-political party) Party(I-political party) ((O) PCP(B-political party) )(O) ,(O) and(O) Left(B-political party) Bloc(I-political party) ((O) BE(B-political party) )(O) .(O)"}}
{"id": "148", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "political party", "organization", "event", "location", "country", "election", "politician"], "instance": {"id": "148", "words": ["He", "was", "put", "in", "charge", "of", "governing", "Lingling", "(", "present", "day", "Yongzhou", ",", "Hunan", ")", ",", "Guiyang", "and", "Changsha", "commanderies", "and", "collecting", "taxes", "to", "fund", "the", "military", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, organization, event, location, country, election, politician and O.\nSentence: He was put in charge of governing Lingling ( present day Yongzhou , Hunan ) , Guiyang and Changsha commanderies and collecting taxes to fund the military .", "prompt_labels": "He(O) was(O) put(O) in(O) charge(O) of(O) governing(O) Lingling(O) ((O) present(O) day(O) Yongzhou(B-location) ,(O) Hunan(B-location) )(O) ,(O) Guiyang(B-location) and(O) Changsha(B-location) commanderies(O) and(O) collecting(O) taxes(O) to(O) fund(O) the(O) military(O) .(O)"}}
{"id": "363", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "event", "person", "location", "election", "politician", "country", "political party"], "instance": {"id": "363", "words": ["Initially", ",", "Reform", "BC", ",", "the", "British", "Columbia", "Social", "Credit", "Party", ",", "the", "British", "Columbia", "Party", ",", "and", "the", "Family", "Coalition", "Party", "of", "British", "Columbia", "had", "joined", "under", "the", "BC", "Unity", "umbrella", "."], "labels": ["O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, person, location, election, politician, country, political party and O.\nSentence: Initially , Reform BC , the British Columbia Social Credit Party , the British Columbia Party , and the Family Coalition Party of British Columbia had joined under the BC Unity umbrella .", "prompt_labels": "Initially(O) ,(O) Reform(B-political party) BC(I-political party) ,(O) the(O) British(B-political party) Columbia(I-political party) Social(I-political party) Credit(I-political party) Party(I-political party) ,(O) the(O) British(B-political party) Columbia(I-political party) Party(I-political party) ,(O) and(O) the(O) Family(B-political party) Coalition(I-political party) Party(I-political party) of(I-political party) British(I-political party) Columbia(I-political party) had(O) joined(O) under(O) the(O) BC(B-organization) Unity(I-organization) umbrella(O) .(O)"}}
{"id": "47", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "location", "election", "organization", "person", "politician", "political party", "country"], "instance": {"id": "47", "words": ["The", "Scythians", "and", "Cimmerians", "took", "advantage", "of", "the", "bitter", "fighting", "among", "the", "Assyrians", "to", "raid", "Neo-Assyrian", "Empire", "colonies", ",", "with", "hordes", "of", "horse-borne", "marauders", "ravaging", "parts", "of", "Asia", "Minor", "and", "the", "Caucasus", ",", "where", "the", "vassal", "kings", "of", "Urartu", "and", "Lydia", "begged", "their", "Neo-Assyrian", "Empire", "overlord", "for", "help", "in", "vain", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, election, organization, person, politician, political party, country and O.\nSentence: The Scythians and Cimmerians took advantage of the bitter fighting among the Assyrians to raid Neo-Assyrian Empire colonies , with hordes of horse-borne marauders ravaging parts of Asia Minor and the Caucasus , where the vassal kings of Urartu and Lydia begged their Neo-Assyrian Empire overlord for help in vain .", "prompt_labels": "The(O) Scythians(O) and(O) Cimmerians(O) took(O) advantage(O) of(O) the(O) bitter(O) fighting(O) among(O) the(O) Assyrians(O) to(O) raid(O) Neo-Assyrian(B-country) Empire(I-country) colonies(O) ,(O) with(O) hordes(O) of(O) horse-borne(O) marauders(O) ravaging(O) parts(O) of(O) Asia(B-location) Minor(I-location) and(O) the(O) Caucasus(B-location) ,(O) where(O) the(O) vassal(O) kings(O) of(O) Urartu(B-country) and(O) Lydia(B-country) begged(O) their(O) Neo-Assyrian(B-country) Empire(I-country) overlord(O) for(O) help(O) in(O) vain(O) .(O)"}}
{"id": "364", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "political party", "person", "location", "politician", "country", "election", "organization"], "instance": {"id": "364", "words": ["She", "was", "a", "candidate", "for", "the", "McGillicuddy", "Serious", "Party", "in", "the", "1993", "New", "Zealand", "general", "election", ",", "for", "the", "Aotearoa", "Legalise", "Cannabis", "Party", "in", "the", "1996", "New", "Zealand", "general", "election", "and", "for", "McGillicuddy", "Serious", "again", "in", "the", "1999", "New", "Zealand", "general", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-political party", "I-political party", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, person, location, politician, country, election, organization and O.\nSentence: She was a candidate for the McGillicuddy Serious Party in the 1993 New Zealand general election , for the Aotearoa Legalise Cannabis Party in the 1996 New Zealand general election and for McGillicuddy Serious again in the 1999 New Zealand general election .", "prompt_labels": "She(O) was(O) a(O) candidate(O) for(O) the(O) McGillicuddy(B-political party) Serious(I-political party) Party(I-political party) in(O) the(O) 1993(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) for(O) the(O) Aotearoa(B-political party) Legalise(I-political party) Cannabis(I-political party) Party(I-political party) in(O) the(O) 1996(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) and(O) for(O) McGillicuddy(B-political party) Serious(I-political party) again(O) in(O) the(O) 1999(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "327", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "person", "politician", "country", "political party", "election", "organization", "location"], "instance": {"id": "327", "words": ["Goldstone", "serves", "on", "the", "Board", "of", "Directors", "of", "several", "nonprofit", "organisations", "that", "promote", "justice", ",", "including", "Physicians", "for", "Human", "Rights", ",", "the", "International", "Center", "for", "Transitional", "Justice", ",", "the", "Institute", "for", "Justice", "and", "Reconciliation", ",", "the", "South", "African", "Legal", "Services", "Foundation", ",", "the", "Brandeis", "University", "Center", "for", "Ethics", ",", "Justice", ",", "and", "Public", "Life", ",", "Human", "Rights", "Watch", ",", "and", "the", "Center", "for", "Economic", "and", "Social", "Rights", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, politician, country, political party, election, organization, location and O.\nSentence: Goldstone serves on the Board of Directors of several nonprofit organisations that promote justice , including Physicians for Human Rights , the International Center for Transitional Justice , the Institute for Justice and Reconciliation , the South African Legal Services Foundation , the Brandeis University Center for Ethics , Justice , and Public Life , Human Rights Watch , and the Center for Economic and Social Rights .", "prompt_labels": "Goldstone(B-politician) serves(O) on(O) the(O) Board(O) of(O) Directors(O) of(O) several(O) nonprofit(O) organisations(O) that(O) promote(O) justice(O) ,(O) including(O) Physicians(B-organization) for(I-organization) Human(I-organization) Rights(I-organization) ,(O) the(O) International(B-organization) Center(I-organization) for(I-organization) Transitional(I-organization) Justice(I-organization) ,(O) the(O) Institute(B-organization) for(I-organization) Justice(I-organization) and(I-organization) Reconciliation(I-organization) ,(O) the(O) South(B-organization) African(I-organization) Legal(I-organization) Services(I-organization) Foundation(I-organization) ,(O) the(O) Brandeis(B-organization) University(I-organization) Center(I-organization) for(I-organization) Ethics(I-organization) ,(I-organization) Justice(I-organization) ,(I-organization) and(I-organization) Public(I-organization) Life(I-organization) ,(O) Human(B-organization) Rights(I-organization) Watch(I-organization) ,(O) and(O) the(O) Center(B-organization) for(I-organization) Economic(I-organization) and(I-organization) Social(I-organization) Rights(I-organization) .(O)"}}
{"id": "298", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "event", "politician", "election", "organization", "political party", "country", "location"], "instance": {"id": "298", "words": ["The", "ruling", "coalition", ",", "consisting", "of", "the", "Social", "Democratic", "Party", "of", "Croatia", "(", "SDP", ")", ",", "Croatian", "People", "'s", "Party", "(", "HNS", ")", ",", "Croatian", "Peasant", "Party", "(", "HSS", ")", ",", "Party", "of", "Liberal", "Democrats", "(", "Libra", ")", "and", "the", "Liberal", "Party", "(", "LS", ")", "did", "not", "contest", "the", "elections", "as", "a", "single", "bloc", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, politician, election, organization, political party, country, location and O.\nSentence: The ruling coalition , consisting of the Social Democratic Party of Croatia ( SDP ) , Croatian People 's Party ( HNS ) , Croatian Peasant Party ( HSS ) , Party of Liberal Democrats ( Libra ) and the Liberal Party ( LS ) did not contest the elections as a single bloc .", "prompt_labels": "The(O) ruling(O) coalition(O) ,(O) consisting(O) of(O) the(O) Social(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Croatia(I-political party) ((O) SDP(B-political party) )(O) ,(O) Croatian(B-political party) People(I-political party) 's(I-political party) Party(I-political party) ((O) HNS(B-political party) )(O) ,(O) Croatian(B-political party) Peasant(I-political party) Party(I-political party) ((O) HSS(B-political party) )(O) ,(O) Party(B-political party) of(I-political party) Liberal(I-political party) Democrats(I-political party) ((O) Libra(B-political party) )(O) and(O) the(O) Liberal(B-political party) Party(I-political party) ((O) LS(B-political party) )(O) did(O) not(O) contest(O) the(O) elections(O) as(O) a(O) single(O) bloc(O) .(O)"}}
{"id": "289", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "politician", "organization", "country", "person", "election", "location", "event"], "instance": {"id": "289", "words": ["The", "Imperial", "Court", "in", "Kyoto", "was", "the", "nominal", "ruling", "government", "of", "Japan", "from", "794", "AD", "until", "the", "Meiji", "period", "(", "1868-1912", ")", ",", "after", "which", "the", "court", "was", "moved", "from", "Kyoto", "(", "formerly", "Heian-kyō", ")", "to", "Tokyo", "(", "formerly", "Edo", ")", "and", "integrated", "into", "the", "Meiji", "government", "."], "labels": ["O", "B-location", "I-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "B-location", "O", "O", "B-location", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, organization, country, person, election, location, event and O.\nSentence: The Imperial Court in Kyoto was the nominal ruling government of Japan from 794 AD until the Meiji period ( 1868-1912 ) , after which the court was moved from Kyoto ( formerly Heian-kyō ) to Tokyo ( formerly Edo ) and integrated into the Meiji government .", "prompt_labels": "The(O) Imperial(B-location) Court(I-location) in(O) Kyoto(B-location) was(O) the(O) nominal(O) ruling(O) government(O) of(O) Japan(B-country) from(O) 794(O) AD(O) until(O) the(O) Meiji(O) period(O) ((O) 1868-1912(O) )(O) ,(O) after(O) which(O) the(O) court(O) was(O) moved(O) from(O) Kyoto(B-location) ((O) formerly(O) Heian-kyō(B-location) )(O) to(O) Tokyo(B-location) ((O) formerly(O) Edo(B-location) )(O) and(O) integrated(O) into(O) the(O) Meiji(O) government(O) .(O)"}}
{"id": "405", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "politician", "event", "election", "person", "political party", "location", "country"], "instance": {"id": "405", "words": ["He", "was", "elected", "for", "the", "UK", "Independence", "Party", "in", "the", "European", "elections", "of", "2004", "European", "Parliament", "election", "in", "the", "United", "Kingdom", "and", "2009", "European", "Parliament", "election", "in", "the", "United", "Kingdom", ",", "representing", "UKIP", "until", "September", "2013", ",", "when", "UKIP", "withdrew", "the", "party", "whip", "from", "him", "."], "labels": ["O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-political party", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, event, election, person, political party, location, country and O.\nSentence: He was elected for the UK Independence Party in the European elections of 2004 European Parliament election in the United Kingdom and 2009 European Parliament election in the United Kingdom , representing UKIP until September 2013 , when UKIP withdrew the party whip from him .", "prompt_labels": "He(O) was(O) elected(O) for(O) the(O) UK(B-political party) Independence(I-political party) Party(I-political party) in(O) the(O) European(O) elections(O) of(O) 2004(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) the(I-election) United(I-election) Kingdom(I-election) and(O) 2009(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) the(I-election) United(I-election) Kingdom(I-election) ,(O) representing(O) UKIP(B-political party) until(O) September(O) 2013(O) ,(O) when(O) UKIP(B-political party) withdrew(O) the(O) party(O) whip(O) from(O) him(O) .(O)"}}
{"id": "188", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "organization", "event", "location", "political party", "election", "politician", "person"], "instance": {"id": "188", "words": ["After", "serving", "two", "terms", "in", "the", "New", "Hampshire", "Senate", ",", "Shaheen", "was", "elected", "governor", "in", "1996", "New", "Hampshire", "gubernatorial", "election", "and", "reelected", "in", "1998", "New", "Hampshire", "gubernatorial", "election", "and", "2000", "New", "Hampshire", "gubernatorial", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-politician", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, event, location, political party, election, politician, person and O.\nSentence: After serving two terms in the New Hampshire Senate , Shaheen was elected governor in 1996 New Hampshire gubernatorial election and reelected in 1998 New Hampshire gubernatorial election and 2000 New Hampshire gubernatorial election .", "prompt_labels": "After(O) serving(O) two(O) terms(O) in(O) the(O) New(B-organization) Hampshire(I-organization) Senate(I-organization) ,(O) Shaheen(B-politician) was(O) elected(O) governor(O) in(O) 1996(B-election) New(I-election) Hampshire(I-election) gubernatorial(I-election) election(I-election) and(O) reelected(O) in(O) 1998(B-election) New(I-election) Hampshire(I-election) gubernatorial(I-election) election(I-election) and(O) 2000(B-election) New(I-election) Hampshire(I-election) gubernatorial(I-election) election(I-election) .(O)"}}
{"id": "112", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "election", "person", "country", "politician", "event", "political party", "organization"], "instance": {"id": "112", "words": ["There", "have", "been", "five", "elections", "to", "the", "Assembly", ",", "in", "1999", "National", "Assembly", "for", "Wales", "election", ",", "2003", "National", "Assembly", "for", "Wales", "election", ",", "2007", "National", "Assembly", "for", "Wales", "election", ",", "2011", "National", "Assembly", "for", "Wales", "election", "and", "2016", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, election, person, country, politician, event, political party, organization and O.\nSentence: There have been five elections to the Assembly , in 1999 National Assembly for Wales election , 2003 National Assembly for Wales election , 2007 National Assembly for Wales election , 2011 National Assembly for Wales election and 2016 .", "prompt_labels": "There(O) have(O) been(O) five(O) elections(O) to(O) the(O) Assembly(O) ,(O) in(O) 1999(B-election) National(I-election) Assembly(I-election) for(I-election) Wales(I-election) election(I-election) ,(O) 2003(B-election) National(I-election) Assembly(I-election) for(I-election) Wales(I-election) election(I-election) ,(O) 2007(B-election) National(I-election) Assembly(I-election) for(I-election) Wales(I-election) election(I-election) ,(O) 2011(B-election) National(I-election) Assembly(I-election) for(I-election) Wales(I-election) election(I-election) and(O) 2016(O) .(O)"}}
{"id": "133", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "political party", "person", "event", "election", "country", "politician", "organization"], "instance": {"id": "133", "words": ["Gold", "coins", "bearing", "portraits", "of", "Chandragupta", "and", "Kumaradevi", "have", "been", "discovered", "at", "Mathura", ",", "Ayodhya", ",", "Lucknow", ",", "Sitapur", ",", "Tanda", ",", "Ghazipur", ",", "and", "Varanasi", "in", "Uttar", "Pradesh", ";", "Bayana", "in", "Rajasthan", ";", "and", "Hajipur", "in", "Bihar", "."], "labels": ["O", "O", "O", "O", "O", "B-person", "O", "B-person", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "O", "B-location", "O", "B-location", "B-location", "O", "B-location", "O", "B-location", "O", "O", "B-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, person, event, election, country, politician, organization and O.\nSentence: Gold coins bearing portraits of Chandragupta and Kumaradevi have been discovered at Mathura , Ayodhya , Lucknow , Sitapur , Tanda , Ghazipur , and Varanasi in Uttar Pradesh ; Bayana in Rajasthan ; and Hajipur in Bihar .", "prompt_labels": "Gold(O) coins(O) bearing(O) portraits(O) of(O) Chandragupta(B-person) and(O) Kumaradevi(B-person) have(O) been(O) discovered(O) at(O) Mathura(B-location) ,(O) Ayodhya(B-location) ,(O) Lucknow(B-location) ,(O) Sitapur(B-location) ,(O) Tanda(B-location) ,(O) Ghazipur(B-location) ,(O) and(O) Varanasi(B-location) in(O) Uttar(B-location) Pradesh(B-location) ;(O) Bayana(B-location) in(O) Rajasthan(B-location) ;(O) and(O) Hajipur(B-location) in(O) Bihar(B-location) .(O)"}}
{"id": "178", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "politician", "event", "political party", "organization", "country", "election", "location"], "instance": {"id": "178", "words": ["Other", "minor", "parties", "included", "the", "Green", "Party", "of", "Canada", "which", "ran", "79", "candidates", ",", "Libertarian", "Party", "of", "Canada", ",", "the", "Marxist-Leninist", "Party", "of", "Canada", "and", "the", "Christian", "Heritage", "Party", "of", "Canada", ",", "which", "was", "mainly", "dedicated", "to", "opposing", "abortion", "."], "labels": ["O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, event, political party, organization, country, election, location and O.\nSentence: Other minor parties included the Green Party of Canada which ran 79 candidates , Libertarian Party of Canada , the Marxist-Leninist Party of Canada and the Christian Heritage Party of Canada , which was mainly dedicated to opposing abortion .", "prompt_labels": "Other(O) minor(O) parties(O) included(O) the(O) Green(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) which(O) ran(O) 79(O) candidates(O) ,(O) Libertarian(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) the(O) Marxist-Leninist(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) the(O) Christian(B-political party) Heritage(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) which(O) was(O) mainly(O) dedicated(O) to(O) opposing(O) abortion(O) .(O)"}}
{"id": "396", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "organization", "political party", "country", "politician", "person", "event", "location"], "instance": {"id": "396", "words": ["The", "United", "Kingdom", "local", "elections", "of", "2004", "were", "held", "on", "10", "June", ",", "as", "part", "of", "the", "2004", "set", "of", "elections", "along", "with", "the", "2004", "European", "Parliament", "election", "in", "the", "United", "Kingdom", "and", "the", "London", "2004", "London", "mayoral", "election", "and", "2004", "London", "Assembly", "election", "elections", "."], "labels": ["O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-location", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, political party, country, politician, person, event, location and O.\nSentence: The United Kingdom local elections of 2004 were held on 10 June , as part of the 2004 set of elections along with the 2004 European Parliament election in the United Kingdom and the London 2004 London mayoral election and 2004 London Assembly election elections .", "prompt_labels": "The(O) United(B-election) Kingdom(I-election) local(I-election) elections(I-election) of(I-election) 2004(I-election) were(O) held(O) on(O) 10(O) June(O) ,(O) as(O) part(O) of(O) the(O) 2004(O) set(O) of(O) elections(O) along(O) with(O) the(O) 2004(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) the(I-election) United(I-election) Kingdom(I-election) and(O) the(O) London(B-location) 2004(B-election) London(I-election) mayoral(I-election) election(I-election) and(O) 2004(B-election) London(I-election) Assembly(I-election) election(I-election) elections(O) .(O)"}}
{"id": "389", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "event", "organization", "country", "location", "politician", "political party", "person"], "instance": {"id": "389", "words": ["Condon", "ran", "unsuccessfully", "as", "a", "New", "Democratic", "Party", "candidate", "for", "the", "House", "of", "Commons", "of", "Canada", "in", "the", "1980", "Canadian", "federal", "election", "and", "1984", "Canadian", "federal", "election", "in", "the", "riding", "of", "Grand", "Falls", "-", "White", "Bay", "-", "Labrador", "."], "labels": ["B-politician", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, organization, country, location, politician, political party, person and O.\nSentence: Condon ran unsuccessfully as a New Democratic Party candidate for the House of Commons of Canada in the 1980 Canadian federal election and 1984 Canadian federal election in the riding of Grand Falls - White Bay - Labrador .", "prompt_labels": "Condon(B-politician) ran(O) unsuccessfully(O) as(O) a(O) New(B-political party) Democratic(I-political party) Party(I-political party) candidate(O) for(O) the(O) House(B-organization) of(I-organization) Commons(I-organization) of(I-organization) Canada(I-organization) in(O) the(O) 1980(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 1984(B-election) Canadian(I-election) federal(I-election) election(I-election) in(O) the(O) riding(O) of(O) Grand(B-location) Falls(I-location) -(O) White(B-location) Bay(I-location) -(O) Labrador(B-location) .(O)"}}
{"id": "53", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "person", "political party", "location", "politician", "event", "organization", "election"], "instance": {"id": "53", "words": ["He", "subsequently", "became", "the", "de", "facto", "leader", "of", "Afghanistan", "'s", "Uzbek", "community", ",", "controlling", "the", "country", "'s", "northern", "provinces", "and", "Mazar-i-Sharif", ",", "effectively", "creating", "his", "own", "proto-state", "with", "an", "army", "of", "up", "to", "40,000", "men", "with", "tanks", "supplied", "by", "Uzbekistan", "and", "Russia", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, political party, location, politician, event, organization, election and O.\nSentence: He subsequently became the de facto leader of Afghanistan 's Uzbek community , controlling the country 's northern provinces and Mazar-i-Sharif , effectively creating his own proto-state with an army of up to 40,000 men with tanks supplied by Uzbekistan and Russia .", "prompt_labels": "He(O) subsequently(O) became(O) the(O) de(O) facto(O) leader(O) of(O) Afghanistan(B-country) 's(O) Uzbek(O) community(O) ,(O) controlling(O) the(O) country(O) 's(O) northern(O) provinces(O) and(O) Mazar-i-Sharif(B-location) ,(O) effectively(O) creating(O) his(O) own(O) proto-state(O) with(O) an(O) army(O) of(O) up(O) to(O) 40,000(O) men(O) with(O) tanks(O) supplied(O) by(O) Uzbekistan(B-country) and(O) Russia(B-country) .(O)"}}
{"id": "17", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "politician", "country", "person", "event", "election", "location", "political party"], "instance": {"id": "17", "words": ["The", "Battle", "of", "Qarqar", "is", "mentioned", "in", "extra-biblical", "records", ",", "and", "was", "perhaps", "at", "Apamea", ",", "where", "Shalmaneser", "III", "of", "Assyria", "fought", "a", "great", "confederation", "of", "princes", "from", "Cilicia", ",", "Northern", "Syria", ",", "Israel", ",", "Ammon", ",", "and", "the", "tribes", "of", "the", "Syrian", "desert", "(", "853", "BC", ")", ",", "including", "Ahab", "the", "Israelite", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "B-politician", "I-politician", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, country, person, event, election, location, political party and O.\nSentence: The Battle of Qarqar is mentioned in extra-biblical records , and was perhaps at Apamea , where Shalmaneser III of Assyria fought a great confederation of princes from Cilicia , Northern Syria , Israel , Ammon , and the tribes of the Syrian desert ( 853 BC ) , including Ahab the Israelite .", "prompt_labels": "The(O) Battle(O) of(O) Qarqar(O) is(O) mentioned(O) in(O) extra-biblical(O) records(O) ,(O) and(O) was(O) perhaps(O) at(O) Apamea(B-location) ,(O) where(O) Shalmaneser(B-politician) III(I-politician) of(O) Assyria(B-country) fought(O) a(O) great(O) confederation(O) of(O) princes(O) from(O) Cilicia(B-country) ,(O) Northern(O) Syria(B-country) ,(O) Israel(B-country) ,(O) Ammon(B-country) ,(O) and(O) the(O) tribes(O) of(O) the(O) Syrian(O) desert(O) ((O) 853(O) BC(O) )(O) ,(O) including(O) Ahab(B-politician) the(O) Israelite(O) .(O)"}}
{"id": "91", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "person", "event", "politician", "location", "political party", "election", "country"], "instance": {"id": "91", "words": ["Belgium", "became", "one", "of", "the", "six", "founding", "members", "of", "the", "European", "Coal", "and", "Steel", "Community", "in", "1951", "and", "of", "the", "European", "Atomic", "Energy", "Community", "and", "European", "Economic", "Community", ",", "established", "in", "1957", "."], "labels": ["B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, event, politician, location, political party, election, country and O.\nSentence: Belgium became one of the six founding members of the European Coal and Steel Community in 1951 and of the European Atomic Energy Community and European Economic Community , established in 1957 .", "prompt_labels": "Belgium(B-country) became(O) one(O) of(O) the(O) six(O) founding(O) members(O) of(O) the(O) European(B-organization) Coal(I-organization) and(I-organization) Steel(I-organization) Community(I-organization) in(O) 1951(O) and(O) of(O) the(O) European(B-organization) Atomic(I-organization) Energy(I-organization) Community(I-organization) and(O) European(B-organization) Economic(I-organization) Community(I-organization) ,(O) established(O) in(O) 1957(O) .(O)"}}
{"id": "521", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "election", "country", "event", "location", "person", "political party", "organization"], "instance": {"id": "521", "words": ["Green", "deputies", "Noël", "Mamère", ",", "Martine", "Billard", "and", "Yves", "Cochet", "on", "September", "10", ",", "2003", "requested", "a", "Parliamentary", "Commission", "on", "the", "role", "of", "France", "in", "the", "support", "of", "military", "regimes", "in", "Latin", "America", "from", "1973", "to", "1984", "before", "the", "Foreign", "Affairs", "Commission", "of", "the", "National", "Assembly", ",", "presided", "by", "Edouard", "Balladur", "."], "labels": ["O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, country, event, location, person, political party, organization and O.\nSentence: Green deputies Noël Mamère , Martine Billard and Yves Cochet on September 10 , 2003 requested a Parliamentary Commission on the role of France in the support of military regimes in Latin America from 1973 to 1984 before the Foreign Affairs Commission of the National Assembly , presided by Edouard Balladur .", "prompt_labels": "Green(O) deputies(O) Noël(B-politician) Mamère(I-politician) ,(O) Martine(B-politician) Billard(I-politician) and(O) Yves(B-politician) Cochet(I-politician) on(O) September(O) 10(O) ,(O) 2003(O) requested(O) a(O) Parliamentary(O) Commission(O) on(O) the(O) role(O) of(O) France(B-country) in(O) the(O) support(O) of(O) military(O) regimes(O) in(O) Latin(B-location) America(I-location) from(O) 1973(O) to(O) 1984(O) before(O) the(O) Foreign(B-organization) Affairs(I-organization) Commission(I-organization) of(I-organization) the(I-organization) National(I-organization) Assembly(I-organization) ,(O) presided(O) by(O) Edouard(B-politician) Balladur(I-politician) .(O)"}}
{"id": "42", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "political party", "country", "politician", "election", "organization", "person", "location"], "instance": {"id": "42", "words": ["At", "its", "peak", ",", "the", "Neo-Assyrian", "Empire", "of", "911", "to", "609", "BC", "stretched", "from", "eastern", "Libya", "and", "Cyprus", "in", "the", "East", "Mediterranean", "to", "Iran", ",", "and", "from", "present-day", "Armenia", "and", "Azerbaijan", "in", "the", "Transcaucasia", "to", "the", "Arabian", "Peninsula", "."], "labels": ["O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "B-location", "I-location", "O", "B-country", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "B-location", "O", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, country, politician, election, organization, person, location and O.\nSentence: At its peak , the Neo-Assyrian Empire of 911 to 609 BC stretched from eastern Libya and Cyprus in the East Mediterranean to Iran , and from present-day Armenia and Azerbaijan in the Transcaucasia to the Arabian Peninsula .", "prompt_labels": "At(O) its(O) peak(O) ,(O) the(O) Neo-Assyrian(B-country) Empire(I-country) of(O) 911(O) to(O) 609(O) BC(O) stretched(O) from(O) eastern(O) Libya(B-country) and(O) Cyprus(B-country) in(O) the(O) East(B-location) Mediterranean(I-location) to(O) Iran(B-country) ,(O) and(O) from(O) present-day(O) Armenia(B-country) and(O) Azerbaijan(B-country) in(O) the(O) Transcaucasia(B-location) to(O) the(O) Arabian(B-location) Peninsula(I-location) .(O)"}}
{"id": "434", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "organization", "location", "person", "country", "politician", "election", "event"], "instance": {"id": "434", "words": ["In", "the", "1993", "New", "Zealand", "general", "election", ",", "however", ",", "the", "new", "New", "Zealand", "First", "party", ",", "led", "by", "the", "part-Māori", "Winston", "Peters", "-", "who", "himself", "held", "the", "general", "seat", "of", "Tauranga", "from", "1984", "to", "2005", "-", "gained", "the", "Northern", "Māori", "seat", "(", "electing", "Tau", "Henare", "to", "Parliament", ")", ",", "and", "in", "the", "1996", "New", "Zealand", "general", "election", "New", "Zealand", "First", "captured", "all", "the", "Māori", "electorates", "for", "one", "electoral", "term", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, location, person, country, politician, election, event and O.\nSentence: In the 1993 New Zealand general election , however , the new New Zealand First party , led by the part-Māori Winston Peters - who himself held the general seat of Tauranga from 1984 to 2005 - gained the Northern Māori seat ( electing Tau Henare to Parliament ) , and in the 1996 New Zealand general election New Zealand First captured all the Māori electorates for one electoral term .", "prompt_labels": "In(O) the(O) 1993(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) however(O) ,(O) the(O) new(O) New(B-political party) Zealand(I-political party) First(I-political party) party(O) ,(O) led(O) by(O) the(O) part-Māori(O) Winston(B-politician) Peters(I-politician) -(O) who(O) himself(O) held(O) the(O) general(O) seat(O) of(O) Tauranga(B-location) from(O) 1984(O) to(O) 2005(O) -(O) gained(O) the(O) Northern(O) Māori(O) seat(O) ((O) electing(O) Tau(B-politician) Henare(I-politician) to(O) Parliament(O) )(O) ,(O) and(O) in(O) the(O) 1996(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) New(B-country) Zealand(I-country) First(O) captured(O) all(O) the(O) Māori(O) electorates(O) for(O) one(O) electoral(O) term(O) .(O)"}}
{"id": "627", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "organization", "country", "politician", "location", "election", "person", "event"], "instance": {"id": "627", "words": ["Datong", "-Puzhou", "Campaign", "(", "同蒲战役", ")", "is", "a", "campaign", "Communist", "Party", "of", "China", "fought", "against", "the", "Kuomintang", "during", "the", "Chinese", "Civil", "War", "in", "the", "post-", "World", "War", "II", "era", "in", "Shanxi", ",", "and", "resulted", "in", "communist", "victory", "."], "labels": ["B-event", "I-event", "I-event", "O", "B-event", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, country, politician, location, election, person, event and O.\nSentence: Datong -Puzhou Campaign ( 同蒲战役 ) is a campaign Communist Party of China fought against the Kuomintang during the Chinese Civil War in the post- World War II era in Shanxi , and resulted in communist victory .", "prompt_labels": "Datong(B-event) -Puzhou(I-event) Campaign(I-event) ((O) 同蒲战役(B-event) )(O) is(O) a(O) campaign(O) Communist(B-political party) Party(I-political party) of(I-political party) China(I-political party) fought(O) against(O) the(O) Kuomintang(B-political party) during(O) the(O) Chinese(B-event) Civil(I-event) War(I-event) in(O) the(O) post-(B-event) World(I-event) War(I-event) II(I-event) era(O) in(O) Shanxi(B-location) ,(O) and(O) resulted(O) in(O) communist(O) victory(O) .(O)"}}
{"id": "46", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "event", "political party", "location", "organization", "country", "politician", "election"], "instance": {"id": "46", "words": ["Ultimately", ",", "Assyria", "conquered", "Babylonia", ",", "Chaldea", ",", "Elam", ",", "Media", ",", "Persia", ",", "Urartu", "(", "Armenia", ")", ",", "Phoenicia", ",", "Aramea", "/", "Syria", ",", "Phrygia", ",", "the", "Neo-Hittite", "States", ",", "the", "Hurrian", "lands", ",", "Arabia", ",", "Gutium", ",", "Israel", ",", "Kingdom", "of", "Judah", ",", "and", "others", "."], "labels": ["O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "O", "B-country", "I-country", "O", "O", "B-location", "I-location", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, political party, location, organization, country, politician, election and O.\nSentence: Ultimately , Assyria conquered Babylonia , Chaldea , Elam , Media , Persia , Urartu ( Armenia ) , Phoenicia , Aramea / Syria , Phrygia , the Neo-Hittite States , the Hurrian lands , Arabia , Gutium , Israel , Kingdom of Judah , and others .", "prompt_labels": "Ultimately(O) ,(O) Assyria(B-country) conquered(O) Babylonia(B-country) ,(O) Chaldea(B-country) ,(O) Elam(B-country) ,(O) Media(B-country) ,(O) Persia(B-country) ,(O) Urartu(B-country) ((O) Armenia(B-country) )(O) ,(O) Phoenicia(B-country) ,(O) Aramea(B-country) /(O) Syria(B-country) ,(O) Phrygia(B-country) ,(O) the(O) Neo-Hittite(B-country) States(I-country) ,(O) the(O) Hurrian(B-location) lands(I-location) ,(O) Arabia(B-country) ,(O) Gutium(B-country) ,(O) Israel(B-country) ,(O) Kingdom(B-country) of(I-country) Judah(I-country) ,(O) and(O) others(O) .(O)"}}
{"id": "534", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "event", "country", "organization", "politician", "political party", "location", "person"], "instance": {"id": "534", "words": ["Guests", "have", "included", "Jordan", "Peterson", ",", "Coleman", "Hughes", ",", "James", "Damore", ",", "Lindsay", "Shepherd", ",", "Susan", "Bradley", ",", "Ed", "the", "Sock", ",", "Adrienne", "Batra", ",", "Steven", "Pinker", ",", "Bill", "Kristol", ",", "Michael", "Shermer", ",", "Matthew", "Goodwin", ",", "Irshad", "Manji", ",", "Roger", "Scruton", ",", "Claire", "Fox", ",", "Francis", "Fukuyama", ",", "Peter", "Boghossian", ",", "Douglas", "Murray", ",", "Brian", "C.", "Kalt", ",", "and", "David", "Frum", "."], "labels": ["O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, country, organization, politician, political party, location, person and O.\nSentence: Guests have included Jordan Peterson , Coleman Hughes , James Damore , Lindsay Shepherd , Susan Bradley , Ed the Sock , Adrienne Batra , Steven Pinker , Bill Kristol , Michael Shermer , Matthew Goodwin , Irshad Manji , Roger Scruton , Claire Fox , Francis Fukuyama , Peter Boghossian , Douglas Murray , Brian C. Kalt , and David Frum .", "prompt_labels": "Guests(O) have(O) included(O) Jordan(B-person) Peterson(I-person) ,(O) Coleman(B-person) Hughes(I-person) ,(O) James(B-person) Damore(I-person) ,(O) Lindsay(B-person) Shepherd(I-person) ,(O) Susan(B-person) Bradley(I-person) ,(O) Ed(B-person) the(I-person) Sock(I-person) ,(O) Adrienne(B-person) Batra(I-person) ,(O) Steven(B-person) Pinker(I-person) ,(O) Bill(B-person) Kristol(I-person) ,(O) Michael(B-person) Shermer(I-person) ,(O) Matthew(B-person) Goodwin(I-person) ,(O) Irshad(B-person) Manji(I-person) ,(O) Roger(B-person) Scruton(I-person) ,(O) Claire(B-person) Fox(I-person) ,(O) Francis(B-person) Fukuyama(I-person) ,(O) Peter(B-person) Boghossian(I-person) ,(O) Douglas(B-person) Murray(I-person) ,(O) Brian(B-person) C.(I-person) Kalt(I-person) ,(O) and(O) David(B-person) Frum(I-person) .(O)"}}
{"id": "162", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "political party", "organization", "politician", "person", "event", "location", "country"], "instance": {"id": "162", "words": ["In", "1642", "parliament", "seized", "control", "of", "the", "Tower", "mint", "and", "after", "Charles", "I", "tried", "to", "arrest", "the", "Five", "Members", "he", "was", "forced", "to", "flee", "London", ",", "establishing", "at", "least", "16", "emergency", "mints", "across", "the", "British", "Isles", "in", "Colchester", ",", "Chester", ",", "Cork", ",", "Edinburgh", ",", "Dublin", ",", "Exeter", ",", "Salisbury", ",", "parts", "of", "Cornwall", "including", "Truro", ",", "Weymouth", ",", "Worcester", ",", "York", ",", "Carlisle", ",", "Newark", ",", "Pontefract", "and", "Scarborough", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, organization, politician, person, event, location, country and O.\nSentence: In 1642 parliament seized control of the Tower mint and after Charles I tried to arrest the Five Members he was forced to flee London , establishing at least 16 emergency mints across the British Isles in Colchester , Chester , Cork , Edinburgh , Dublin , Exeter , Salisbury , parts of Cornwall including Truro , Weymouth , Worcester , York , Carlisle , Newark , Pontefract and Scarborough .", "prompt_labels": "In(O) 1642(O) parliament(O) seized(O) control(O) of(O) the(O) Tower(O) mint(O) and(O) after(O) Charles(B-person) I(I-person) tried(O) to(O) arrest(O) the(O) Five(O) Members(O) he(O) was(O) forced(O) to(O) flee(O) London(B-location) ,(O) establishing(O) at(O) least(O) 16(O) emergency(O) mints(O) across(O) the(O) British(B-location) Isles(I-location) in(O) Colchester(B-location) ,(O) Chester(B-location) ,(O) Cork(B-location) ,(O) Edinburgh(B-location) ,(O) Dublin(B-location) ,(O) Exeter(B-location) ,(O) Salisbury(B-location) ,(O) parts(O) of(O) Cornwall(B-location) including(O) Truro(B-location) ,(O) Weymouth(B-location) ,(O) Worcester(B-location) ,(O) York(B-location) ,(O) Carlisle(B-location) ,(O) Newark(B-location) ,(O) Pontefract(B-location) and(O) Scarborough(B-location) .(O)"}}
{"id": "51", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "person", "politician", "election", "organization", "location", "event", "country"], "instance": {"id": "51", "words": ["In", "1991", ",", "President", "George", "H.", "W.", "Bush", "nominated", "Clarence", "Thomas", ",", "a", "federal", "Circuit", "Judge", ",", "to", "succeed", "retiring", "Associate", "Supreme", "Court", "Justice", "Thurgood", "Marshall", "."], "labels": ["O", "O", "O", "O", "B-politician", "I-politician", "I-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, politician, election, organization, location, event, country and O.\nSentence: In 1991 , President George H. W. Bush nominated Clarence Thomas , a federal Circuit Judge , to succeed retiring Associate Supreme Court Justice Thurgood Marshall .", "prompt_labels": "In(O) 1991(O) ,(O) President(O) George(B-politician) H.(I-politician) W.(I-politician) Bush(I-politician) nominated(O) Clarence(B-politician) Thomas(I-politician) ,(O) a(O) federal(O) Circuit(O) Judge(O) ,(O) to(O) succeed(O) retiring(O) Associate(O) Supreme(O) Court(O) Justice(O) Thurgood(B-politician) Marshall(I-politician) .(O)"}}
{"id": "317", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "politician", "country", "election", "political party", "event", "person", "location"], "instance": {"id": "317", "words": ["As", "the", "Hawke", "Labor", "opposition", "defeated", "the", "Fraser", "-", "Doug", "Anthony", "Liberal", "Party", "of", "Australia", "-", "National", "Party", "of", "Australia", "coalition", ",", "Labor", "retained", "its", "four", "Senate", "seats", "in", "New", "South", "Wales", ",", "with", "Richardson", "polling", "the", "third", "highest", "quota", "at", "the", "1983", "Australian", "federal", "election", "."], "labels": ["O", "O", "B-political party", "I-political party", "O", "O", "O", "B-location", "O", "B-politician", "I-politician", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, country, election, political party, event, person, location and O.\nSentence: As the Hawke Labor opposition defeated the Fraser - Doug Anthony Liberal Party of Australia - National Party of Australia coalition , Labor retained its four Senate seats in New South Wales , with Richardson polling the third highest quota at the 1983 Australian federal election .", "prompt_labels": "As(O) the(O) Hawke(B-political party) Labor(I-political party) opposition(O) defeated(O) the(O) Fraser(B-location) -(O) Doug(B-politician) Anthony(I-politician) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) -(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) coalition(O) ,(O) Labor(B-political party) retained(O) its(O) four(O) Senate(O) seats(O) in(O) New(B-location) South(I-location) Wales(I-location) ,(O) with(O) Richardson(B-politician) polling(O) the(O) third(O) highest(O) quota(O) at(O) the(O) 1983(B-election) Australian(I-election) federal(I-election) election(I-election) .(O)"}}
{"id": "435", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "organization", "location", "country", "event", "person", "political party", "politician"], "instance": {"id": "435", "words": ["In", "the", "2009", "Portuguese", "legislative", "election", ",", "the", "party", "won", "21", "seats", ",", "its", "most", "since", "the", "1985", "Portuguese", "legislative", "election", ",", "and", "increased", "it", "to", "24", "2011", "Portuguese", "legislative", "election", ",", "leading", "to", "it", "forming", "a", "coalition", "government", "with", "the", "PSD", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, location, country, event, person, political party, politician and O.\nSentence: In the 2009 Portuguese legislative election , the party won 21 seats , its most since the 1985 Portuguese legislative election , and increased it to 24 2011 Portuguese legislative election , leading to it forming a coalition government with the PSD .", "prompt_labels": "In(O) the(O) 2009(B-election) Portuguese(I-election) legislative(I-election) election(I-election) ,(O) the(O) party(O) won(O) 21(O) seats(O) ,(O) its(O) most(O) since(O) the(O) 1985(B-election) Portuguese(I-election) legislative(I-election) election(I-election) ,(O) and(O) increased(O) it(O) to(O) 24(O) 2011(B-election) Portuguese(I-election) legislative(I-election) election(I-election) ,(O) leading(O) to(O) it(O) forming(O) a(O) coalition(O) government(O) with(O) the(O) PSD(B-organization) .(O)"}}
{"id": "519", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "politician", "person", "election", "country", "organization", "political party", "location"], "instance": {"id": "519", "words": ["Incumbent", "Democrat", "Lee", "Metcalf", ",", "who", "was", "first", "elected", "to", "the", "Senate", "in", "1960", "United", "States", "Senate", "election", "in", "Montana", "and", "was", "re-elected", "in", "1966", "United", "States", "Senate", "election", "in", "Montana", ",", "ran", "for", "re-election", "."], "labels": ["O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, person, election, country, organization, political party, location and O.\nSentence: Incumbent Democrat Lee Metcalf , who was first elected to the Senate in 1960 United States Senate election in Montana and was re-elected in 1966 United States Senate election in Montana , ran for re-election .", "prompt_labels": "Incumbent(O) Democrat(O) Lee(B-politician) Metcalf(I-politician) ,(O) who(O) was(O) first(O) elected(O) to(O) the(O) Senate(B-organization) in(O) 1960(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Montana(I-election) and(O) was(O) re-elected(O) in(O) 1966(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Montana(I-election) ,(O) ran(O) for(O) re-election(O) .(O)"}}
{"id": "271", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "event", "location", "election", "political party", "country", "person", "organization"], "instance": {"id": "271", "words": ["Stabenow", "has", "received", "low", "scores", "from", "free-market", "groups", "(", "Competitive", "Enterprise", "Institute", ",", "2013", ",", "0", "%", ";", "American", "Conservative", "Union", ",", "2016", ",", "0", "%", ";", "Americans", "for", "Prosperity", ",", "2015-16", ",", "0", "%", ")", "and", "high", "scores", "from", "fiscally", "liberal", "groups", "(", "Progressive", "Punch", ",", "2015", ",", "92", "%", ";", "NETWORK", ",", "A", "National", "Catholic", "Social", "Justice", "Lobby", ",", "2012", ",", "91", "%", ")", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, location, election, political party, country, person, organization and O.\nSentence: Stabenow has received low scores from free-market groups ( Competitive Enterprise Institute , 2013 , 0 % ; American Conservative Union , 2016 , 0 % ; Americans for Prosperity , 2015-16 , 0 % ) and high scores from fiscally liberal groups ( Progressive Punch , 2015 , 92 % ; NETWORK , A National Catholic Social Justice Lobby , 2012 , 91 % ) .", "prompt_labels": "Stabenow(B-politician) has(O) received(O) low(O) scores(O) from(O) free-market(O) groups(O) ((O) Competitive(B-organization) Enterprise(I-organization) Institute(I-organization) ,(O) 2013(O) ,(O) 0(O) %(O) ;(O) American(B-organization) Conservative(I-organization) Union(I-organization) ,(O) 2016(O) ,(O) 0(O) %(O) ;(O) Americans(B-organization) for(I-organization) Prosperity(I-organization) ,(O) 2015-16(O) ,(O) 0(O) %(O) )(O) and(O) high(O) scores(O) from(O) fiscally(O) liberal(O) groups(O) ((O) Progressive(B-organization) Punch(I-organization) ,(O) 2015(O) ,(O) 92(O) %(O) ;(O) NETWORK(B-organization) ,(I-organization) A(I-organization) National(I-organization) Catholic(I-organization) Social(I-organization) Justice(I-organization) Lobby(I-organization) ,(O) 2012(O) ,(O) 91(O) %(O) )(O) .(O)"}}
{"id": "262", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "person", "location", "event", "election", "organization", "politician", "political party"], "instance": {"id": "262", "words": ["The", "NF", "was", "founded", "by", "A.", "K.", "Chesterton", ",", "formerly", "of", "the", "British", "Union", "of", "Fascists", ",", "as", "a", "merger", "between", "his", "League", "of", "Empire", "Loyalists", "and", "the", "British", "National", "Party", "."], "labels": ["O", "B-political party", "O", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, location, event, election, organization, politician, political party and O.\nSentence: The NF was founded by A. K. Chesterton , formerly of the British Union of Fascists , as a merger between his League of Empire Loyalists and the British National Party .", "prompt_labels": "The(O) NF(B-political party) was(O) founded(O) by(O) A.(B-politician) K.(I-politician) Chesterton(I-politician) ,(O) formerly(O) of(O) the(O) British(B-political party) Union(I-political party) of(I-political party) Fascists(I-political party) ,(O) as(O) a(O) merger(O) between(O) his(O) League(B-political party) of(I-political party) Empire(I-political party) Loyalists(I-political party) and(O) the(O) British(B-political party) National(I-political party) Party(I-political party) .(O)"}}
{"id": "456", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "political party", "election", "politician", "location", "person", "organization", "event"], "instance": {"id": "456", "words": ["However", ",", "several", "groups", ",", "including", "a", "large", "portion", "of", "the", "New", "Italian", "Socialist", "Party", ",", "The", "Italian", "Socialists", ",", "Democracy", "and", "Socialism", "and", "the", "Association", "for", "the", "Rose", "in", "the", "Fist", ",", "decided", "to", "join", "forces", "with", "the", "SDI", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, election, politician, location, person, organization, event and O.\nSentence: However , several groups , including a large portion of the New Italian Socialist Party , The Italian Socialists , Democracy and Socialism and the Association for the Rose in the Fist , decided to join forces with the SDI .", "prompt_labels": "However(O) ,(O) several(O) groups(O) ,(O) including(O) a(O) large(O) portion(O) of(O) the(O) New(B-political party) Italian(I-political party) Socialist(I-political party) Party(I-political party) ,(O) The(B-political party) Italian(I-political party) Socialists(I-political party) ,(O) Democracy(B-political party) and(I-political party) Socialism(I-political party) and(O) the(O) Association(B-political party) for(I-political party) the(I-political party) Rose(I-political party) in(I-political party) the(I-political party) Fist(I-political party) ,(O) decided(O) to(O) join(O) forces(O) with(O) the(O) SDI(B-political party) .(O)"}}
{"id": "33", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "organization", "election", "person", "political party", "politician", "location", "country"], "instance": {"id": "33", "words": ["CNN", "report", "has", "revealed", "that", "Saudi", "Arabia", "and", "the", "United", "Arab", "Emirates", "(", "UAE", ")", "have", "been", "handing", "out", "sophisticated", "American-made", "weapons", "to", "al-Qaeda-linked", "fighters", "in", "Yemen", "."], "labels": ["B-organization", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "B-country", "I-country", "I-country", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, election, person, political party, politician, location, country and O.\nSentence: CNN report has revealed that Saudi Arabia and the United Arab Emirates ( UAE ) have been handing out sophisticated American-made weapons to al-Qaeda-linked fighters in Yemen .", "prompt_labels": "CNN(B-organization) report(O) has(O) revealed(O) that(O) Saudi(B-country) Arabia(I-country) and(O) the(O) United(B-country) Arab(I-country) Emirates(I-country) ((O) UAE(B-country) )(O) have(O) been(O) handing(O) out(O) sophisticated(O) American-made(O) weapons(O) to(O) al-Qaeda-linked(B-organization) fighters(O) in(O) Yemen(B-country) .(O)"}}
{"id": "145", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "country", "political party", "election", "organization", "person", "location", "event"], "instance": {"id": "145", "words": ["The", "2017", "United", "Kingdom", "general", "election", "produced", "a", "mixed", "result", "for", "the", "party", "as", "it", "gained", "six", "seat", "and", "increased", "its", "vote", "by", "2.8", "%", "but", "the", "party", "came", "in", "third", "behind", "the", "Scottish", "National", "Party", "and", "Scottish", "Conservatives", "."], "labels": ["O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, political party, election, organization, person, location, event and O.\nSentence: The 2017 United Kingdom general election produced a mixed result for the party as it gained six seat and increased its vote by 2.8 % but the party came in third behind the Scottish National Party and Scottish Conservatives .", "prompt_labels": "The(O) 2017(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) produced(O) a(O) mixed(O) result(O) for(O) the(O) party(O) as(O) it(O) gained(O) six(O) seat(O) and(O) increased(O) its(O) vote(O) by(O) 2.8(O) %(O) but(O) the(O) party(O) came(O) in(O) third(O) behind(O) the(O) Scottish(B-political party) National(I-political party) Party(I-political party) and(O) Scottish(B-political party) Conservatives(I-political party) .(O)"}}
{"id": "345", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "country", "location", "politician", "political party", "person", "election", "event"], "instance": {"id": "345", "words": ["Throughout", "his", "political", "career", "he", "has", "been", "variously", "associated", "with", "conservative", "groups", ",", "including", "the", "Association", "of", "Christian", "Parent", "Controlled", "Schools", ",", "Salt", "Shakers", ",", "Focus", "on", "the", "Family", ",", "Lyons", "Forum", ",", "Endeavour", "Forum", ",", "Family", "Council", "of", "Victoria", ",", "Fatherhood", "Foundation", ",", "Australian", "Christian", "Lobby", ",", "Australian", "Family", "Association", "and", "Right", "to", "Life", "Australia", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, location, politician, political party, person, election, event and O.\nSentence: Throughout his political career he has been variously associated with conservative groups , including the Association of Christian Parent Controlled Schools , Salt Shakers , Focus on the Family , Lyons Forum , Endeavour Forum , Family Council of Victoria , Fatherhood Foundation , Australian Christian Lobby , Australian Family Association and Right to Life Australia .", "prompt_labels": "Throughout(O) his(O) political(O) career(O) he(O) has(O) been(O) variously(O) associated(O) with(O) conservative(O) groups(O) ,(O) including(O) the(O) Association(B-organization) of(I-organization) Christian(I-organization) Parent(I-organization) Controlled(I-organization) Schools(I-organization) ,(O) Salt(B-organization) Shakers(I-organization) ,(O) Focus(B-organization) on(I-organization) the(I-organization) Family(I-organization) ,(O) Lyons(B-organization) Forum(I-organization) ,(O) Endeavour(B-organization) Forum(I-organization) ,(O) Family(B-organization) Council(I-organization) of(I-organization) Victoria(I-organization) ,(O) Fatherhood(B-organization) Foundation(I-organization) ,(O) Australian(B-organization) Christian(I-organization) Lobby(I-organization) ,(O) Australian(B-organization) Family(I-organization) Association(I-organization) and(O) Right(B-organization) to(I-organization) Life(I-organization) Australia(I-organization) .(O)"}}
{"id": "61", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "country", "politician", "political party", "election", "location", "person", "organization"], "instance": {"id": "61", "words": ["Abbas", "II", "(", "full", "name", ":", "Abbas", "Hilmy", ")", ",", "the", "great-great-grandson", "of", "Muhammad", "Ali", ",", "was", "born", "in", "Alexandria", ",", "Egypt", "on", "14", "July", "1874", "."], "labels": ["B-person", "I-person", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-location", "O", "B-country", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, politician, political party, election, location, person, organization and O.\nSentence: Abbas II ( full name : Abbas Hilmy ) , the great-great-grandson of Muhammad Ali , was born in Alexandria , Egypt on 14 July 1874 .", "prompt_labels": "Abbas(B-person) II(I-person) ((O) full(O) name(O) :(O) Abbas(B-person) Hilmy(I-person) )(O) ,(O) the(O) great-great-grandson(O) of(O) Muhammad(B-person) Ali(I-person) ,(O) was(O) born(O) in(O) Alexandria(B-location) ,(O) Egypt(B-country) on(O) 14(O) July(O) 1874(O) .(O)"}}
{"id": "648", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "politician", "organization", "location", "event", "person", "country", "political party"], "instance": {"id": "648", "words": ["War", "returned", "in", "September", "1939", "when", "the", "French", "and", "British", "governments", "declared", "war", "on", "Nazi", "Germany", "in", "response", "to", "the", "German", "invasion", "of", "Poland", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, organization, location, event, person, country, political party and O.\nSentence: War returned in September 1939 when the French and British governments declared war on Nazi Germany in response to the German invasion of Poland .", "prompt_labels": "War(O) returned(O) in(O) September(O) 1939(O) when(O) the(O) French(O) and(O) British(O) governments(O) declared(O) war(O) on(O) Nazi(B-country) Germany(I-country) in(O) response(O) to(O) the(O) German(B-event) invasion(I-event) of(I-event) Poland(I-event) .(O)"}}
{"id": "528", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "politician", "event", "political party", "organization", "location", "election", "country"], "instance": {"id": "528", "words": ["Director", "Michael", "Moore", "partnered", "with", "producers", "Harvey", "Weinstein", "and", "Bob", "Weinstein", "in", "May", "2017", "to", "produce", "and", "distribute", "Fahrenheit", "11", "/", "9", "."], "labels": ["O", "B-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, event, political party, organization, location, election, country and O.\nSentence: Director Michael Moore partnered with producers Harvey Weinstein and Bob Weinstein in May 2017 to produce and distribute Fahrenheit 11 / 9 .", "prompt_labels": "Director(O) Michael(B-person) Moore(I-person) partnered(O) with(O) producers(O) Harvey(B-person) Weinstein(I-person) and(O) Bob(B-person) Weinstein(I-person) in(O) May(O) 2017(O) to(O) produce(O) and(O) distribute(O) Fahrenheit(O) 11(O) /(O) 9(O) .(O)"}}
{"id": "296", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "location", "country", "event", "political party", "organization", "politician", "election"], "instance": {"id": "296", "words": ["The", "strictness", "of", "these", "qualifications", "led", "to", "the", "1999", "Singaporean", "presidential", "election", ",", "2005", "Singaporean", "presidential", "election", ",", "and", "2017", "Singaporean", "presidential", "election", "being", "walkovers", "as", "only", "one", "candidate", "had", "qualified", "on", "nomination", "day", ".."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, country, event, political party, organization, politician, election and O.\nSentence: The strictness of these qualifications led to the 1999 Singaporean presidential election , 2005 Singaporean presidential election , and 2017 Singaporean presidential election being walkovers as only one candidate had qualified on nomination day ..", "prompt_labels": "The(O) strictness(O) of(O) these(O) qualifications(O) led(O) to(O) the(O) 1999(B-election) Singaporean(I-election) presidential(I-election) election(I-election) ,(O) 2005(B-election) Singaporean(I-election) presidential(I-election) election(I-election) ,(O) and(O) 2017(B-election) Singaporean(I-election) presidential(I-election) election(I-election) being(O) walkovers(O) as(O) only(O) one(O) candidate(O) had(O) qualified(O) on(O) nomination(O) day(O) ..(O)"}}
{"id": "636", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "election", "event", "politician", "country", "organization", "location", "person"], "instance": {"id": "636", "words": ["Brătianu", ",", "to", "seek", "a", "rapprochement", "with", "the", "former", "war", "enemy", ";", "during", "World", "War", "II", ",", "PNL", "leaders", "supported", "Romania", "'s", "participation", "in", "the", "Axis", "-led", "invasion", "of", "the", "Soviet", "Union", ",", "while", "maintaining", "contacts", "with", "the", "Western", "Allies", "of", "World", "War", "II", ",", "ultimately", "backing", "the", "realignment", "with", "the", "latter", "in", "August", "1944", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, event, politician, country, organization, location, person and O.\nSentence: Brătianu , to seek a rapprochement with the former war enemy ; during World War II , PNL leaders supported Romania 's participation in the Axis -led invasion of the Soviet Union , while maintaining contacts with the Western Allies of World War II , ultimately backing the realignment with the latter in August 1944 .", "prompt_labels": "Brătianu(B-politician) ,(O) to(O) seek(O) a(O) rapprochement(O) with(O) the(O) former(O) war(O) enemy(O) ;(O) during(O) World(B-event) War(I-event) II(I-event) ,(O) PNL(O) leaders(O) supported(O) Romania(B-country) 's(O) participation(O) in(O) the(O) Axis(B-event) -led(I-event) invasion(I-event) of(O) the(O) Soviet(B-country) Union(I-country) ,(O) while(O) maintaining(O) contacts(O) with(O) the(O) Western(O) Allies(B-organization) of(I-organization) World(I-organization) War(I-organization) II(I-organization) ,(O) ultimately(O) backing(O) the(O) realignment(O) with(O) the(O) latter(O) in(O) August(O) 1944(O) .(O)"}}
{"id": "453", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "location", "politician", "person", "country", "political party", "organization", "event"], "instance": {"id": "453", "words": ["Starting", "in", "1992", ",", "he", "served", "as", "a", "member", "of", "Meretz", ",", "a", "dovish", "left", "wing", "party", "which", "resulted", "from", "the", "merger", "of", "Mapam", ",", "Ratz", "and", "Shinui", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "B-political party", "O", "B-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, politician, person, country, political party, organization, event and O.\nSentence: Starting in 1992 , he served as a member of Meretz , a dovish left wing party which resulted from the merger of Mapam , Ratz and Shinui .", "prompt_labels": "Starting(O) in(O) 1992(O) ,(O) he(O) served(O) as(O) a(O) member(O) of(O) Meretz(B-political party) ,(O) a(O) dovish(O) left(O) wing(O) party(O) which(O) resulted(O) from(O) the(O) merger(O) of(O) Mapam(B-political party) ,(O) Ratz(B-political party) and(O) Shinui(B-political party) .(O)"}}
{"id": "559", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "politician", "election", "political party", "person", "location", "organization", "event"], "instance": {"id": "559", "words": ["Omar", "Chaparro", ",", "Angélica", "Vale", ",", "Eduardo", "Manzano", ",", "and", "Jaime", "Maussan", "."], "labels": ["B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, politician, election, political party, person, location, organization, event and O.\nSentence: Omar Chaparro , Angélica Vale , Eduardo Manzano , and Jaime Maussan .", "prompt_labels": "Omar(B-person) Chaparro(I-person) ,(O) Angélica(B-person) Vale(I-person) ,(O) Eduardo(B-person) Manzano(I-person) ,(O) and(O) Jaime(B-person) Maussan(I-person) .(O)"}}
{"id": "216", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "person", "location", "event", "country", "political party", "politician", "election"], "instance": {"id": "216", "words": ["The", "commission", "set", "up", "eight", "regional", "offices", "in", "Jalalabad", ",", "Herat", ",", "Kunduz", ",", "Kabul", ",", "Gardez", ",", "Kandahar", ",", "Mazar-i-Sharif", "and", "Bamyan", "as", "well", "as", "in", "the", "Pakistan", "i", "cities", "of", "Peshawar", "and", "Quetta", ",", "and", "in", "the", "Iran", "ian", "cities", "of", "Tehran", "and", "Mashhad", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, location, event, country, political party, politician, election and O.\nSentence: The commission set up eight regional offices in Jalalabad , Herat , Kunduz , Kabul , Gardez , Kandahar , Mazar-i-Sharif and Bamyan as well as in the Pakistan i cities of Peshawar and Quetta , and in the Iran ian cities of Tehran and Mashhad .", "prompt_labels": "The(O) commission(O) set(O) up(O) eight(O) regional(O) offices(O) in(O) Jalalabad(B-location) ,(O) Herat(B-location) ,(O) Kunduz(B-location) ,(O) Kabul(B-location) ,(O) Gardez(B-location) ,(O) Kandahar(B-location) ,(O) Mazar-i-Sharif(B-location) and(O) Bamyan(B-location) as(O) well(O) as(O) in(O) the(O) Pakistan(B-country) i(O) cities(O) of(O) Peshawar(B-location) and(O) Quetta(B-location) ,(O) and(O) in(O) the(O) Iran(B-country) ian(O) cities(O) of(O) Tehran(B-location) and(O) Mashhad(B-location) .(O)"}}
{"id": "393", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "political party", "politician", "country", "organization", "location", "election", "event"], "instance": {"id": "393", "words": ["The", "Radicals", "continued", "to", "participate", "in", "elections", "through", "the", "Antiprohibitionists", "on", "Drugs", "list", ",", "the", "Rainbow", "Greens", ",", "the", "Pannella", "List", ",", "the", "Bonino", "List", "and", "the", "Bonino-Pannella", "List", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, politician, country, organization, location, election, event and O.\nSentence: The Radicals continued to participate in elections through the Antiprohibitionists on Drugs list , the Rainbow Greens , the Pannella List , the Bonino List and the Bonino-Pannella List .", "prompt_labels": "The(O) Radicals(O) continued(O) to(O) participate(O) in(O) elections(O) through(O) the(O) Antiprohibitionists(B-political party) on(I-political party) Drugs(I-political party) list(O) ,(O) the(O) Rainbow(B-political party) Greens(I-political party) ,(O) the(O) Pannella(B-political party) List(I-political party) ,(O) the(O) Bonino(B-political party) List(I-political party) and(O) the(O) Bonino-Pannella(B-political party) List(I-political party) .(O)"}}
{"id": "214", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "politician", "person", "location", "country", "election", "organization", "event"], "instance": {"id": "214", "words": ["Spivak", "has", "received", "11", "honorary", "doctorates", "from", "the", "University", "of", "Toronto", ",", "University", "of", "London", ",", "Oberlin", "College", ",", "Rovira", "i", "Virgili", "University", ",", "Rabindra", "Bharati", "University", ",", "National", "University", "of", "General", "San", "Martín", ",", "University", "of", "St", "Andrews", ",", "Université", "de", "Vincennes", "à", "Saint-Denis", ",", "Presidency", "University", ",", "Yale", "University", ",", "and", "University", "of", "Ghana", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, person, location, country, election, organization, event and O.\nSentence: Spivak has received 11 honorary doctorates from the University of Toronto , University of London , Oberlin College , Rovira i Virgili University , Rabindra Bharati University , National University of General San Martín , University of St Andrews , Université de Vincennes à Saint-Denis , Presidency University , Yale University , and University of Ghana .", "prompt_labels": "Spivak(B-person) has(O) received(O) 11(O) honorary(O) doctorates(O) from(O) the(O) University(B-organization) of(I-organization) Toronto(I-organization) ,(O) University(B-organization) of(I-organization) London(I-organization) ,(O) Oberlin(B-organization) College(I-organization) ,(O) Rovira(B-organization) i(I-organization) Virgili(I-organization) University(I-organization) ,(O) Rabindra(B-organization) Bharati(I-organization) University(I-organization) ,(O) National(B-organization) University(I-organization) of(I-organization) General(I-organization) San(I-organization) Martín(I-organization) ,(O) University(B-organization) of(I-organization) St(I-organization) Andrews(I-organization) ,(O) Université(B-organization) de(I-organization) Vincennes(I-organization) à(I-organization) Saint-Denis(I-organization) ,(O) Presidency(B-organization) University(I-organization) ,(O) Yale(B-organization) University(I-organization) ,(O) and(O) University(B-organization) of(I-organization) Ghana(I-organization) .(O)"}}
{"id": "353", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "location", "organization", "politician", "political party", "election", "event", "country"], "instance": {"id": "353", "words": ["In", "Belfast", "North", "at", "the", "2017", "United", "Kingdom", "general", "election", ",", "the", "Social", "Democratic", "and", "Labour", "Party", "(", "SDLP", ")", "were", "criticised", "for", "standing", "Martin", "McAuley", "as", "a", "paper", "candidate", "-", "a", "deliberately", "weak", "candidate", ",", "a", "26-year-old", "with", "little", "experience", ",", "on", "the", "assumption", "that", "nationalist", "and", "Catholic", "voters", "would", "instead", "vote", "for", "Sinn", "Féin", ",", "instead", "of", "the", "unionist", "candidate", "Nigel", "Dodds", "(", "Democratic", "Unionist", "Party", ")", "."], "labels": ["O", "B-location", "I-location", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "B-political party", "I-political party", "I-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, organization, politician, political party, election, event, country and O.\nSentence: In Belfast North at the 2017 United Kingdom general election , the Social Democratic and Labour Party ( SDLP ) were criticised for standing Martin McAuley as a paper candidate - a deliberately weak candidate , a 26-year-old with little experience , on the assumption that nationalist and Catholic voters would instead vote for Sinn Féin , instead of the unionist candidate Nigel Dodds ( Democratic Unionist Party ) .", "prompt_labels": "In(O) Belfast(B-location) North(I-location) at(O) the(O) 2017(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) ((O) SDLP(B-political party) )(O) were(O) criticised(O) for(O) standing(O) Martin(B-politician) McAuley(I-politician) as(O) a(O) paper(O) candidate(O) -(O) a(O) deliberately(O) weak(O) candidate(O) ,(O) a(O) 26-year-old(O) with(O) little(O) experience(O) ,(O) on(O) the(O) assumption(O) that(O) nationalist(O) and(O) Catholic(O) voters(O) would(O) instead(O) vote(O) for(O) Sinn(B-political party) Féin(I-political party) ,(O) instead(O) of(O) the(O) unionist(O) candidate(O) Nigel(B-politician) Dodds(I-politician) ((O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) )(O) .(O)"}}
{"id": "386", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "location", "person", "country", "election", "event", "organization", "political party"], "instance": {"id": "386", "words": ["Future", "New", "Zealand", "joined", "with", "the", "United", "New", "Zealand", "to", "form", "a", "coalition", "known", "as", "United", "Future", "New", "Zealand", "in", "November", "2000", "and", "contested", "the", "2002", "New", "Zealand", "general", "election", "as", "such", "."], "labels": ["B-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, person, country, election, event, organization, political party and O.\nSentence: Future New Zealand joined with the United New Zealand to form a coalition known as United Future New Zealand in November 2000 and contested the 2002 New Zealand general election as such .", "prompt_labels": "Future(B-political party) New(I-political party) Zealand(I-political party) joined(O) with(O) the(O) United(B-political party) New(I-political party) Zealand(I-political party) to(O) form(O) a(O) coalition(O) known(O) as(O) United(B-political party) Future(I-political party) New(I-political party) Zealand(I-political party) in(O) November(O) 2000(O) and(O) contested(O) the(O) 2002(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) as(O) such(O) .(O)"}}
{"id": "114", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "person", "political party", "organization", "country", "election", "location", "event"], "instance": {"id": "114", "words": ["Born", "in", "Hanover", "to", "Carl", "Hugenberg", ",", "a", "royal", "Kingdom", "of", "Hanover", "official", "who", "in", "1867", "entered", "the", "Prussian", "Landtag", "as", "a", "member", "of", "the", "National", "Liberal", "Party", ",", "he", "studied", "law", "in", "University", "of", "Göttingen", ",", "Heidelberg", ",", "and", "Humboldt", "University", "of", "Berlin", ",", "as", "well", "as", "economics", "in", "University", "of", "Strasbourg", "."], "labels": ["O", "O", "B-location", "O", "B-person", "I-person", "O", "O", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, political party, organization, country, election, location, event and O.\nSentence: Born in Hanover to Carl Hugenberg , a royal Kingdom of Hanover official who in 1867 entered the Prussian Landtag as a member of the National Liberal Party , he studied law in University of Göttingen , Heidelberg , and Humboldt University of Berlin , as well as economics in University of Strasbourg .", "prompt_labels": "Born(O) in(O) Hanover(B-location) to(O) Carl(B-person) Hugenberg(I-person) ,(O) a(O) royal(O) Kingdom(B-country) of(I-country) Hanover(I-country) official(O) who(O) in(O) 1867(O) entered(O) the(O) Prussian(B-organization) Landtag(I-organization) as(O) a(O) member(O) of(O) the(O) National(B-political party) Liberal(I-political party) Party(I-political party) ,(O) he(O) studied(O) law(O) in(O) University(B-organization) of(I-organization) Göttingen(I-organization) ,(O) Heidelberg(B-location) ,(O) and(O) Humboldt(B-organization) University(I-organization) of(I-organization) Berlin(I-organization) ,(O) as(O) well(O) as(O) economics(O) in(O) University(B-organization) of(I-organization) Strasbourg(I-organization) .(O)"}}
{"id": "381", "dataset": "crossner_politics", "split": "test", "label_list": ["location", "event", "person", "election", "political party", "country", "politician", "organization"], "instance": {"id": "381", "words": ["Dion", "would", "hold", "the", "riding", "in", "1997", "Canadian", "federal", "election", ",", "and", "was", "reelected", "again", "in", "the", "2000", "Canadian", "federal", "election", ",", "2004", "Canadian", "federal", "election", ",", "2006", "Canadian", "federal", "election", ",", "2008", "Canadian", "federal", "election", ",", "2011", "Canadian", "federal", "election", ",", "and", "2015", "Canadian", "federal", "election", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, person, election, political party, country, politician, organization and O.\nSentence: Dion would hold the riding in 1997 Canadian federal election , and was reelected again in the 2000 Canadian federal election , 2004 Canadian federal election , 2006 Canadian federal election , 2008 Canadian federal election , 2011 Canadian federal election , and 2015 Canadian federal election .", "prompt_labels": "Dion(B-politician) would(O) hold(O) the(O) riding(O) in(O) 1997(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) and(O) was(O) reelected(O) again(O) in(O) the(O) 2000(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2008(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2011(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) and(O) 2015(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)"}}
{"id": "485", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "politician", "person", "event", "country", "political party", "location", "election"], "instance": {"id": "485", "words": ["In", "the", "2006", "Canadian", "federal", "election", "in", "Canada", ",", "the", "Liberal", "Party", "of", "Canada", "used", "attack", "ads", "against", "Conservative", "Party", "of", "Canada", "leader", "Stephen", "Harper", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-country", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, person, event, country, political party, location, election and O.\nSentence: In the 2006 Canadian federal election in Canada , the Liberal Party of Canada used attack ads against Conservative Party of Canada leader Stephen Harper .", "prompt_labels": "In(O) the(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) in(O) Canada(B-country) ,(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) used(O) attack(O) ads(O) against(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) leader(O) Stephen(B-politician) Harper(I-politician) .(O)"}}
{"id": "206", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "politician", "country", "event", "political party", "person", "organization", "location"], "instance": {"id": "206", "words": ["In", "Alberta", ",", "relations", "became", "strained", "between", "the", "federal", "Conservative", "Party", "and", "the", "Progressive", "Conservative", "Association", "of", "Alberta", "prior", "to", "its", "provincial", "loss", "in", "2015", "Alberta", "general", "election", "and", "eventually", "emergence", "with", "the", "Wildrose", "Party", "into", "the", "United", "Conservative", "Party", "in", "2017", "."], "labels": ["O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, country, event, political party, person, organization, location and O.\nSentence: In Alberta , relations became strained between the federal Conservative Party and the Progressive Conservative Association of Alberta prior to its provincial loss in 2015 Alberta general election and eventually emergence with the Wildrose Party into the United Conservative Party in 2017 .", "prompt_labels": "In(O) Alberta(B-location) ,(O) relations(O) became(O) strained(O) between(O) the(O) federal(O) Conservative(O) Party(O) and(O) the(O) Progressive(B-political party) Conservative(I-political party) Association(I-political party) of(I-political party) Alberta(I-political party) prior(O) to(O) its(O) provincial(O) loss(O) in(O) 2015(B-election) Alberta(I-election) general(I-election) election(I-election) and(O) eventually(O) emergence(O) with(O) the(O) Wildrose(B-political party) Party(I-political party) into(O) the(O) United(B-political party) Conservative(I-political party) Party(I-political party) in(O) 2017(O) .(O)"}}
{"id": "382", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "organization", "politician", "event", "country", "location", "political party", "election"], "instance": {"id": "382", "words": ["These", "agreements", "set", "the", "policies", "for", "the", "Bank", "for", "International", "Settlements", "(", "BIS", ")", ",", "International", "Monetary", "Fund", "(", "IMF", ")", ",", "and", "World", "Bank", ",", "the", "so-called", "Bretton", "Woods", "Institutions", ",", "launched", "in", "the", "late", "1940s", "for", "the", "last", "two", "(", "the", "BIS", "was", "founded", "in", "1930", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "B-organization", "B-organization", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, politician, event, country, location, political party, election and O.\nSentence: These agreements set the policies for the Bank for International Settlements ( BIS ) , International Monetary Fund ( IMF ) , and World Bank , the so-called Bretton Woods Institutions , launched in the late 1940s for the last two ( the BIS was founded in 1930 ) .", "prompt_labels": "These(O) agreements(O) set(O) the(O) policies(O) for(O) the(O) Bank(B-organization) for(I-organization) International(I-organization) Settlements(I-organization) ((O) BIS(B-organization) )(O) ,(O) International(B-organization) Monetary(I-organization) Fund(I-organization) ((O) IMF(B-organization) )(O) ,(O) and(O) World(B-organization) Bank(I-organization) ,(O) the(O) so-called(O) Bretton(B-organization) Woods(B-organization) Institutions(B-organization) ,(O) launched(O) in(O) the(O) late(O) 1940s(O) for(O) the(O) last(O) two(O) ((O) the(O) BIS(B-organization) was(O) founded(O) in(O) 1930(O) )(O) .(O)"}}
{"id": "443", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "event", "political party", "location", "election", "person", "organization", "country"], "instance": {"id": "443", "words": ["In", "June", "2007", ",", "the", "NPSI", "split", "among", "those", "who", "wanted", "to", "participate", "to", "the", "foundation", "of", "a", "joint", "Socialist", "Party", "along", "with", "the", "Italian", "Democratic", "Socialists", "of", "Enrico", "Boselli", ",", "The", "Italian", "Socialists", "of", "Bobo", "Craxi", "and", "the", "Association", "for", "the", "Rose", "in", "the", "Fist", "of", "Lanfranco", "Turci", "and", "those", "who", "wanted", "to", "maintain", "the", "allegiance", "to", "the", "House", "of", "Freedoms", "coalition", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O", "B-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, political party, location, election, person, organization, country and O.\nSentence: In June 2007 , the NPSI split among those who wanted to participate to the foundation of a joint Socialist Party along with the Italian Democratic Socialists of Enrico Boselli , The Italian Socialists of Bobo Craxi and the Association for the Rose in the Fist of Lanfranco Turci and those who wanted to maintain the allegiance to the House of Freedoms coalition .", "prompt_labels": "In(O) June(O) 2007(O) ,(O) the(O) NPSI(B-organization) split(O) among(O) those(O) who(O) wanted(O) to(O) participate(O) to(O) the(O) foundation(O) of(O) a(O) joint(O) Socialist(O) Party(O) along(O) with(O) the(O) Italian(B-political party) Democratic(I-political party) Socialists(I-political party) of(O) Enrico(B-politician) Boselli(I-politician) ,(O) The(B-political party) Italian(I-political party) Socialists(I-political party) of(O) Bobo(B-politician) Craxi(I-politician) and(O) the(O) Association(B-political party) for(I-political party) the(I-political party) Rose(I-political party) in(I-political party) the(I-political party) Fist(I-political party) of(O) Lanfranco(B-politician) Turci(I-politician) and(O) those(O) who(O) wanted(O) to(O) maintain(O) the(O) allegiance(O) to(O) the(O) House(B-political party) of(I-political party) Freedoms(I-political party) coalition(O) .(O)"}}
{"id": "477", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "organization", "politician", "person", "location", "election", "country", "event"], "instance": {"id": "477", "words": ["The", "decision", "to", "form", "the", "Royal", "Commission", "was", "taken", "by", "the", "Fourth", "Labour", "government", ",", "after", "the", "Labour", "party", "had", "received", "more", "votes", ",", "yet", "won", "fewer", "seats", "than", "the", "New", "Zealand", "National", "Party", "in", "both", "the", "1978", "New", "Zealand", "general", "election", "and", "1981", "New", "Zealand", "general", "election", "elections", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, politician, person, location, election, country, event and O.\nSentence: The decision to form the Royal Commission was taken by the Fourth Labour government , after the Labour party had received more votes , yet won fewer seats than the New Zealand National Party in both the 1978 New Zealand general election and 1981 New Zealand general election elections .", "prompt_labels": "The(O) decision(O) to(O) form(O) the(O) Royal(B-organization) Commission(I-organization) was(O) taken(O) by(O) the(O) Fourth(B-organization) Labour(I-organization) government(I-organization) ,(O) after(O) the(O) Labour(B-political party) party(I-political party) had(O) received(O) more(O) votes(O) ,(O) yet(O) won(O) fewer(O) seats(O) than(O) the(O) New(B-political party) Zealand(I-political party) National(I-political party) Party(I-political party) in(O) both(O) the(O) 1978(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) and(O) 1981(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) elections(O) .(O)"}}
{"id": "371", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "political party", "country", "election", "location", "organization", "event", "politician"], "instance": {"id": "371", "words": ["He", "was", "re-elected", "in", "1923", "United", "Kingdom", "general", "election", ",", "1924", "United", "Kingdom", "general", "election", "and", "1929", "United", "Kingdom", "general", "election", "."], "labels": ["O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, country, election, location, organization, event, politician and O.\nSentence: He was re-elected in 1923 United Kingdom general election , 1924 United Kingdom general election and 1929 United Kingdom general election .", "prompt_labels": "He(O) was(O) re-elected(O) in(O) 1923(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 1924(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1929(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "215", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "event", "politician", "political party", "location", "country", "election", "person"], "instance": {"id": "215", "words": ["The", "UFP", "presented", "itself", "as", "an", "alternative", "to", "the", "main", "three", "parties", "in", "Québec", ":", "the", "centre-left", "Parti", "Québécois", ",", "the", "centre-right", "Quebec", "Liberal", "Party", ",", "and", "the", "conservative", "Action", "démocratique", "du", "Québec", "/", "Equipe", "Mario", "Dumont", ",", "saying", "that", "all", "three", "are", "but", "different", "faces", "of", "the", "same", "right-wing", "ideology", "called", "neoliberalism", "."], "labels": ["O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, politician, political party, location, country, election, person and O.\nSentence: The UFP presented itself as an alternative to the main three parties in Québec : the centre-left Parti Québécois , the centre-right Quebec Liberal Party , and the conservative Action démocratique du Québec / Equipe Mario Dumont , saying that all three are but different faces of the same right-wing ideology called neoliberalism .", "prompt_labels": "The(O) UFP(B-political party) presented(O) itself(O) as(O) an(O) alternative(O) to(O) the(O) main(O) three(O) parties(O) in(O) Québec(B-location) :(O) the(O) centre-left(O) Parti(B-political party) Québécois(I-political party) ,(O) the(O) centre-right(O) Quebec(B-political party) Liberal(I-political party) Party(I-political party) ,(O) and(O) the(O) conservative(O) Action(B-political party) démocratique(I-political party) du(I-political party) Québec(I-political party) /(O) Equipe(O) Mario(B-politician) Dumont(I-politician) ,(O) saying(O) that(O) all(O) three(O) are(O) but(O) different(O) faces(O) of(O) the(O) same(O) right-wing(O) ideology(O) called(O) neoliberalism(O) .(O)"}}
{"id": "370", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "event", "person", "location", "organization", "political party", "election", "country"], "instance": {"id": "370", "words": ["Seven", "parties", "contested", "the", "elections", "as", "the", "Mgwirizano", "Coalition", ";", "the", "Republican", "Party", ",", "the", "People", "'s", "Progressive", "Movement", ",", "the", "Movement", "for", "Genuine", "Democratic", "Change", ",", "the", "People", "'s", "Transformation", "Party", ",", "the", "Malawi", "Forum", "for", "Unity", "and", "Development", ",", "the", "National", "Unity", "Party", "and", "the", "Malawi", "Democratic", "Party", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, person, location, organization, political party, election, country and O.\nSentence: Seven parties contested the elections as the Mgwirizano Coalition ; the Republican Party , the People 's Progressive Movement , the Movement for Genuine Democratic Change , the People 's Transformation Party , the Malawi Forum for Unity and Development , the National Unity Party and the Malawi Democratic Party .", "prompt_labels": "Seven(O) parties(O) contested(O) the(O) elections(O) as(O) the(O) Mgwirizano(B-political party) Coalition(I-political party) ;(O) the(O) Republican(B-political party) Party(I-political party) ,(O) the(O) People(B-political party) 's(I-political party) Progressive(I-political party) Movement(I-political party) ,(O) the(O) Movement(B-political party) for(I-political party) Genuine(I-political party) Democratic(I-political party) Change(I-political party) ,(O) the(O) People(B-political party) 's(I-political party) Transformation(I-political party) Party(I-political party) ,(O) the(O) Malawi(B-political party) Forum(I-political party) for(I-political party) Unity(I-political party) and(I-political party) Development(I-political party) ,(O) the(O) National(B-political party) Unity(I-political party) Party(I-political party) and(O) the(O) Malawi(B-political party) Democratic(I-political party) Party(I-political party) .(O)"}}
{"id": "57", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "election", "location", "organization", "political party", "person", "politician", "event"], "instance": {"id": "57", "words": ["In", "2019", "Widdecombe", "appeared", "on", "the", "celebrity", "version", "of", "The", "Crystal", "Maze", ",", "where", "alongside", "Sunetra", "Sarker", ",", "Wes", "Nelson", ",", "Matthew", "Wright", "and", "Nikki", "Sanderson", ",", "she", "won", "money", "for", "the", "Stand", "Up", "to", "Cancer", "initiative", "."], "labels": ["O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, location, organization, political party, person, politician, event and O.\nSentence: In 2019 Widdecombe appeared on the celebrity version of The Crystal Maze , where alongside Sunetra Sarker , Wes Nelson , Matthew Wright and Nikki Sanderson , she won money for the Stand Up to Cancer initiative .", "prompt_labels": "In(O) 2019(O) Widdecombe(B-politician) appeared(O) on(O) the(O) celebrity(O) version(O) of(O) The(O) Crystal(O) Maze(O) ,(O) where(O) alongside(O) Sunetra(B-person) Sarker(I-person) ,(O) Wes(B-person) Nelson(I-person) ,(O) Matthew(B-person) Wright(I-person) and(O) Nikki(B-person) Sanderson(I-person) ,(O) she(O) won(O) money(O) for(O) the(O) Stand(O) Up(O) to(O) Cancer(O) initiative(O) .(O)"}}
{"id": "146", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "person", "politician", "event", "location", "election", "political party", "organization"], "instance": {"id": "146", "words": ["At", "the", "four", "subsequent", "UK", "elections", "(", "2001", "United", "Kingdom", "general", "election", ",", "2005", "United", "Kingdom", "general", "election", ",", "2010", "United", "Kingdom", "general", "election", "and", "2015", "United", "Kingdom", "general", "election", ")", "the", "Conservatives", "won", "only", "one", "Scottish", "seat", "."], "labels": ["O", "O", "O", "O", "B-country", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, politician, event, location, election, political party, organization and O.\nSentence: At the four subsequent UK elections ( 2001 United Kingdom general election , 2005 United Kingdom general election , 2010 United Kingdom general election and 2015 United Kingdom general election ) the Conservatives won only one Scottish seat .", "prompt_labels": "At(O) the(O) four(O) subsequent(O) UK(B-country) elections(O) ((O) 2001(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 2005(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 2010(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 2015(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) )(O) the(O) Conservatives(O) won(O) only(O) one(O) Scottish(O) seat(O) .(O)"}}
{"id": "384", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "country", "politician", "event", "organization", "political party", "location", "election"], "instance": {"id": "384", "words": ["If", "serving", "on", "a", "deputation", ",", "they", "may", "be", "employed", "in", "intergovernmental", "organisation", "s", "such", "as", "the", "World", "Bank", ",", "the", "International", "Monetary", "Fund", ",", "the", "Asian", "Development", "Bank", ",", "the", "Asian", "Infrastructure", "Investment", "Bank", ",", "or", "the", "United", "Nations", ",", "or", "its", "agencies", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, politician, event, organization, political party, location, election and O.\nSentence: If serving on a deputation , they may be employed in intergovernmental organisation s such as the World Bank , the International Monetary Fund , the Asian Development Bank , the Asian Infrastructure Investment Bank , or the United Nations , or its agencies .", "prompt_labels": "If(O) serving(O) on(O) a(O) deputation(O) ,(O) they(O) may(O) be(O) employed(O) in(O) intergovernmental(O) organisation(O) s(O) such(O) as(O) the(O) World(B-organization) Bank(I-organization) ,(O) the(O) International(B-organization) Monetary(I-organization) Fund(I-organization) ,(O) the(O) Asian(B-organization) Development(I-organization) Bank(I-organization) ,(O) the(O) Asian(B-organization) Infrastructure(I-organization) Investment(I-organization) Bank(I-organization) ,(O) or(O) the(O) United(B-country) Nations(I-country) ,(O) or(O) its(O) agencies(O) .(O)"}}
{"id": "225", "dataset": "crossner_politics", "split": "test", "label_list": ["organization", "location", "event", "political party", "election", "person", "country", "politician"], "instance": {"id": "225", "words": ["The", "National", "Union", "party", "won", "seven", "seats", ",", "and", "was", "included", "in", "Ariel", "Sharon", "'s", "coalition", ",", "alongside", "Likud", ",", "Shinui", ",", "the", "National", "Religious", "Party", ",", "and", "Yisrael", "BaAliyah", "."], "labels": ["O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, event, political party, election, person, country, politician and O.\nSentence: The National Union party won seven seats , and was included in Ariel Sharon 's coalition , alongside Likud , Shinui , the National Religious Party , and Yisrael BaAliyah .", "prompt_labels": "The(O) National(B-political party) Union(I-political party) party(I-political party) won(O) seven(O) seats(O) ,(O) and(O) was(O) included(O) in(O) Ariel(B-politician) Sharon(I-politician) 's(O) coalition(O) ,(O) alongside(O) Likud(B-political party) ,(O) Shinui(B-political party) ,(O) the(O) National(B-political party) Religious(I-political party) Party(I-political party) ,(O) and(O) Yisrael(B-political party) BaAliyah(I-political party) .(O)"}}
{"id": "9", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "organization", "location", "event", "political party", "election", "person", "politician"], "instance": {"id": "9", "words": ["Some", "of", "the", "most", "pronounced", "effects", "of", "Hellenization", "can", "be", "seen", "in", "Afghanistan", "and", "India", ",", "in", "the", "region", "of", "the", "relatively", "late-rising", "Greco-Bactrian", "Kingdom", "(", "250-125", "BC", ")", "(", "in", "modern", "Afghanistan", ",", "Pakistan", ",", "and", "Tajikistan", ")", "and", "the", "Indo-Greek", "Kingdom", "(", "180", "BC", "-", "10", "AD", ")", "in", "modern", "Afghanistan", "and", "India.", "and", "created", "a", "culture", "of", "Greco-Buddhist", "art", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "B-country", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, location, event, political party, election, person, politician and O.\nSentence: Some of the most pronounced effects of Hellenization can be seen in Afghanistan and India , in the region of the relatively late-rising Greco-Bactrian Kingdom ( 250-125 BC ) ( in modern Afghanistan , Pakistan , and Tajikistan ) and the Indo-Greek Kingdom ( 180 BC - 10 AD ) in modern Afghanistan and India. and created a culture of Greco-Buddhist art .", "prompt_labels": "Some(O) of(O) the(O) most(O) pronounced(O) effects(O) of(O) Hellenization(O) can(O) be(O) seen(O) in(O) Afghanistan(B-country) and(O) India(B-country) ,(O) in(O) the(O) region(O) of(O) the(O) relatively(O) late-rising(O) Greco-Bactrian(B-country) Kingdom(I-country) ((O) 250-125(O) BC(O) )(O) ((O) in(O) modern(O) Afghanistan(B-country) ,(O) Pakistan(B-country) ,(O) and(O) Tajikistan(B-country) )(O) and(O) the(O) Indo-Greek(B-country) Kingdom(I-country) ((O) 180(O) BC(O) -(O) 10(O) AD(O) )(O) in(O) modern(O) Afghanistan(B-country) and(O) India.(B-country) and(O) created(O) a(O) culture(O) of(O) Greco-Buddhist(O) art(O) .(O)"}}
{"id": "275", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "event", "political party", "person", "organization", "country", "election", "location"], "instance": {"id": "275", "words": ["After", "his", "junior", "year", "at", "Harvard", "College", ",", "he", "spent", "three", "years", "studying", "Japanese", "at", "the", "International", "Christian", "University", "in", "Tokyo", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, event, political party, person, organization, country, election, location and O.\nSentence: After his junior year at Harvard College , he spent three years studying Japanese at the International Christian University in Tokyo .", "prompt_labels": "After(O) his(O) junior(O) year(O) at(O) Harvard(B-organization) College(I-organization) ,(O) he(O) spent(O) three(O) years(O) studying(O) Japanese(O) at(O) the(O) International(B-organization) Christian(I-organization) University(I-organization) in(O) Tokyo(B-location) .(O)"}}
{"id": "527", "dataset": "crossner_politics", "split": "test", "label_list": ["political party", "organization", "politician", "country", "election", "person", "location", "event"], "instance": {"id": "527", "words": ["He", "was", "elected", "to", "the", "House", "of", "Commons", "of", "Canada", "in", "the", "2000", "Canadian", "federal", "election", "as", "the", "Liberal", "Party", "of", "Canada", "MP", "for", "York", "South", "-", "Weston", "defeating", "Independent", "MP", "(", "and", "former", "Liberal", ")", "John", "Nunziata", "by", "1,497", "votes", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, politician, country, election, person, location, event and O.\nSentence: He was elected to the House of Commons of Canada in the 2000 Canadian federal election as the Liberal Party of Canada MP for York South - Weston defeating Independent MP ( and former Liberal ) John Nunziata by 1,497 votes .", "prompt_labels": "He(O) was(O) elected(O) to(O) the(O) House(B-organization) of(I-organization) Commons(I-organization) of(I-organization) Canada(I-organization) in(O) the(O) 2000(B-election) Canadian(I-election) federal(I-election) election(I-election) as(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) MP(O) for(O) York(B-location) South(I-location) -(I-location) Weston(I-location) defeating(O) Independent(O) MP(O) ((O) and(O) former(O) Liberal(O) )(O) John(B-politician) Nunziata(I-politician) by(O) 1,497(O) votes(O) .(O)"}}
{"id": "590", "dataset": "crossner_politics", "split": "test", "label_list": ["country", "election", "organization", "event", "politician", "location", "person", "political party"], "instance": {"id": "590", "words": ["The", "air", "raid", "was", "part", "of", "the", "Allies", "of", "World", "War", "II", "'", "aerial", "campaign", "against", "the", "Home", "Islands", "of", "the", "Empire", "of", "Japan", "during", "World", "War", "II", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "I-country", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, organization, event, politician, location, person, political party and O.\nSentence: The air raid was part of the Allies of World War II ' aerial campaign against the Home Islands of the Empire of Japan during World War II .", "prompt_labels": "The(O) air(O) raid(O) was(O) part(O) of(O) the(O) Allies(B-country) of(O) World(B-event) War(I-event) II(I-event) '(O) aerial(O) campaign(O) against(O) the(O) Home(O) Islands(O) of(O) the(O) Empire(B-country) of(I-country) Japan(I-country) during(O) World(B-event) War(I-event) II(I-event) .(O)"}}
{"id": "223", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "election", "organization", "politician", "political party", "location", "person", "country"], "instance": {"id": "223", "words": ["He", "was", "re-elected", "in", "the", "elections", "of", "1966", "Manitoba", "general", "election", ",", "1969", "Manitoba", "general", "election", ",", "1973", "Manitoba", "general", "election", "and", "1977", "Manitoba", "general", "election", ",", "each", "time", "by", "a", "significant", "margin", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, organization, politician, political party, location, person, country and O.\nSentence: He was re-elected in the elections of 1966 Manitoba general election , 1969 Manitoba general election , 1973 Manitoba general election and 1977 Manitoba general election , each time by a significant margin .", "prompt_labels": "He(O) was(O) re-elected(O) in(O) the(O) elections(O) of(O) 1966(B-election) Manitoba(I-election) general(I-election) election(I-election) ,(O) 1969(B-election) Manitoba(I-election) general(I-election) election(I-election) ,(O) 1973(B-election) Manitoba(I-election) general(I-election) election(I-election) and(O) 1977(B-election) Manitoba(I-election) general(I-election) election(I-election) ,(O) each(O) time(O) by(O) a(O) significant(O) margin(O) .(O)"}}
{"id": "468", "dataset": "crossner_politics", "split": "test", "label_list": ["person", "political party", "event", "organization", "country", "election", "politician", "location"], "instance": {"id": "468", "words": ["Resulting", "from", "this", "move", "was", "targeting", "by", "the", "New", "Democracy", "Party", ",", "the", "Popular", "Orthodox", "Rally", ",", "PASOK", "and", "the", "Communist", "Party", "of", "Greece", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, event, organization, country, election, politician, location and O.\nSentence: Resulting from this move was targeting by the New Democracy Party , the Popular Orthodox Rally , PASOK and the Communist Party of Greece .", "prompt_labels": "Resulting(O) from(O) this(O) move(O) was(O) targeting(O) by(O) the(O) New(B-political party) Democracy(I-political party) Party(I-political party) ,(O) the(O) Popular(B-political party) Orthodox(I-political party) Rally(I-political party) ,(O) PASOK(B-political party) and(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Greece(I-political party) .(O)"}}
{"id": "515", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "location", "person", "organization", "political party", "event", "election", "country"], "instance": {"id": "515", "words": ["Incumbent", "Democrat", "Alan", "Cranston", "easily", "won", "re-election", "to", "a", "third", "term", "over", "Paul", "Gann", ",", "political", "activist", ",", "even", "as", "the", "state", "'s", "former", "Republican", "governor", ",", "Ronald", "Reagan", ",", "claimed", "a", "landslide", "victory", "in", "the", "1980", "United", "States", "presidential", "election", "."], "labels": ["O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, person, organization, political party, event, election, country and O.\nSentence: Incumbent Democrat Alan Cranston easily won re-election to a third term over Paul Gann , political activist , even as the state 's former Republican governor , Ronald Reagan , claimed a landslide victory in the 1980 United States presidential election .", "prompt_labels": "Incumbent(O) Democrat(O) Alan(B-politician) Cranston(I-politician) easily(O) won(O) re-election(O) to(O) a(O) third(O) term(O) over(O) Paul(B-politician) Gann(I-politician) ,(O) political(O) activist(O) ,(O) even(O) as(O) the(O) state(O) 's(O) former(O) Republican(O) governor(O) ,(O) Ronald(B-politician) Reagan(I-politician) ,(O) claimed(O) a(O) landslide(O) victory(O) in(O) the(O) 1980(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) .(O)"}}
{"id": "425", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "location", "event", "organization", "politician", "political party", "person", "country"], "instance": {"id": "425", "words": ["O", "'Connor", "continued", "to", "be", "re-elected", "in", "Liverpool", "under", "this", "label", "unopposed", "in", "the", "1918", "United", "Kingdom", "general", "election", ",", "1922", "United", "Kingdom", "general", "election", ",", "1923", "United", "Kingdom", "general", "election", ",", "1924", "United", "Kingdom", "general", "election", "and", "1929", "United", "Kingdom", "general", "election", "general", "elections", "."], "labels": ["B-politician", "I-politician", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, event, organization, politician, political party, person, country and O.\nSentence: O 'Connor continued to be re-elected in Liverpool under this label unopposed in the 1918 United Kingdom general election , 1922 United Kingdom general election , 1923 United Kingdom general election , 1924 United Kingdom general election and 1929 United Kingdom general election general elections .", "prompt_labels": "O(B-politician) 'Connor(I-politician) continued(O) to(O) be(O) re-elected(O) in(O) Liverpool(B-location) under(O) this(O) label(O) unopposed(O) in(O) the(O) 1918(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 1922(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 1923(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 1924(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1929(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) general(O) elections(O) .(O)"}}
{"id": "93", "dataset": "crossner_politics", "split": "test", "label_list": ["election", "location", "organization", "country", "event", "person", "politician", "political party"], "instance": {"id": "93", "words": ["In", "addition", ",", "he", "secured", "the", "release", "of", "two", "American", "journalists", "imprisoned", "by", "North", "Korea", ",", "visiting", "the", "capital", "Pyongyang", "in", "2009", "and", "negotiating", "their", "release", "with", "then-North", "Korean", "leader", "Kim", "Jong-il", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, organization, country, event, person, politician, political party and O.\nSentence: In addition , he secured the release of two American journalists imprisoned by North Korea , visiting the capital Pyongyang in 2009 and negotiating their release with then-North Korean leader Kim Jong-il .", "prompt_labels": "In(O) addition(O) ,(O) he(O) secured(O) the(O) release(O) of(O) two(O) American(O) journalists(O) imprisoned(O) by(O) North(B-country) Korea(I-country) ,(O) visiting(O) the(O) capital(O) Pyongyang(B-location) in(O) 2009(O) and(O) negotiating(O) their(O) release(O) with(O) then-North(O) Korean(O) leader(O) Kim(B-politician) Jong-il(I-politician) .(O)"}}
{"id": "11", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "organization", "political party", "location", "person", "election", "country", "event"], "instance": {"id": "11", "words": ["In", "May", "2000", ",", "Andranik", "Margaryan", "replaced", "Aram", "Sargsyan", "(", "a", "brother", "of", "assassinated", "Vazgen", "Sargsyan", ")", "as", "Prime", "Minister", "."], "labels": ["O", "O", "O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, political party, location, person, election, country, event and O.\nSentence: In May 2000 , Andranik Margaryan replaced Aram Sargsyan ( a brother of assassinated Vazgen Sargsyan ) as Prime Minister .", "prompt_labels": "In(O) May(O) 2000(O) ,(O) Andranik(B-politician) Margaryan(I-politician) replaced(O) Aram(B-politician) Sargsyan(I-politician) ((O) a(O) brother(O) of(O) assassinated(O) Vazgen(B-politician) Sargsyan(I-politician) )(O) as(O) Prime(O) Minister(O) .(O)"}}
{"id": "239", "dataset": "crossner_politics", "split": "test", "label_list": ["event", "country", "person", "political party", "politician", "organization", "location", "election"], "instance": {"id": "239", "words": ["Attempts", "to", "secure", "its", "operation", "on", "a", "permanent", "basis", "had", "been", "frustrated", "by", "disagreements", "between", "the", "two", "main", "unionist", "parties", "(", "the", "Democratic", "Unionist", "Party", "(", "DUP", ")", "and", "the", "Ulster", "Unionist", "Party", ")", "and", "Sinn", "Féin", ",", "the", "largest", "nationalist", "party", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, person, political party, politician, organization, location, election and O.\nSentence: Attempts to secure its operation on a permanent basis had been frustrated by disagreements between the two main unionist parties ( the Democratic Unionist Party ( DUP ) and the Ulster Unionist Party ) and Sinn Féin , the largest nationalist party .", "prompt_labels": "Attempts(O) to(O) secure(O) its(O) operation(O) on(O) a(O) permanent(O) basis(O) had(O) been(O) frustrated(O) by(O) disagreements(O) between(O) the(O) two(O) main(O) unionist(O) parties(O) ((O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) ((O) DUP(B-political party) )(O) and(O) the(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) )(O) and(O) Sinn(B-political party) Féin(I-political party) ,(O) the(O) largest(O) nationalist(O) party(O) .(O)"}}
{"id": "462", "dataset": "crossner_politics", "split": "test", "label_list": ["politician", "country", "person", "political party", "event", "organization", "location", "election"], "instance": {"id": "462", "words": ["Between", "the", "1935", "Canadian", "federal", "election", "and", "1958", "Canadian", "federal", "election", "elections", ",", "the", "top", "ranking", "was", "consistently", "held", "by", "either", "the", "Co-operative", "Commonwealth", "Federation", "or", "the", "Labor-Progressive", "Party", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, person, political party, event, organization, location, election and O.\nSentence: Between the 1935 Canadian federal election and 1958 Canadian federal election elections , the top ranking was consistently held by either the Co-operative Commonwealth Federation or the Labor-Progressive Party .", "prompt_labels": "Between(O) the(O) 1935(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 1958(B-election) Canadian(I-election) federal(I-election) election(I-election) elections(O) ,(O) the(O) top(O) ranking(O) was(O) consistently(O) held(O) by(O) either(O) the(O) Co-operative(B-political party) Commonwealth(I-political party) Federation(I-political party) or(O) the(O) Labor-Progressive(B-political party) Party(I-political party) .(O)"}}
{"id": "420", "dataset": "crossner_science", "split": "test", "label_list": ["scientist", "enzyme", "location", "university", "astronomical object", "academic journal", "country", "event", "protein", "discipline", "chemical compound", "chemical element", "award", "organization", "theory", "person"], "instance": {"id": "420", "words": ["He", "became", "an", "expert", "on", "the", "electrical", "conductivity", "of", "gases", ",", "the", "properties", "of", "ion", "s", ",", "and", "the", "behavior", "of", "atmospheric", "electricity", ",", "publishing", "in", "journals", "including", "the", "Physical", "Review", ",", "Journal", "of", "Applied", "Physics", ",", "Journal", "of", "Chemical", "Physics", ",", "and", "the", "Journal", "of", "Atmospheric", "Electricity", "and", "Terrestrial", "Magnetism", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, enzyme, location, university, astronomical object, academic journal, country, event, protein, discipline, chemical compound, chemical element, award, organization, theory, person and O.\nSentence: He became an expert on the electrical conductivity of gases , the properties of ion s , and the behavior of atmospheric electricity , publishing in journals including the Physical Review , Journal of Applied Physics , Journal of Chemical Physics , and the Journal of Atmospheric Electricity and Terrestrial Magnetism .", "prompt_labels": "He(O) became(O) an(O) expert(O) on(O) the(O) electrical(O) conductivity(O) of(O) gases(O) ,(O) the(O) properties(O) of(O) ion(O) s(O) ,(O) and(O) the(O) behavior(O) of(O) atmospheric(O) electricity(O) ,(O) publishing(O) in(O) journals(O) including(O) the(O) Physical(B-academic journal) Review(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Applied(I-academic journal) Physics(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Chemical(I-academic journal) Physics(I-academic journal) ,(O) and(O) the(O) Journal(B-academic journal) of(I-academic journal) Atmospheric(I-academic journal) Electricity(I-academic journal) and(I-academic journal) Terrestrial(I-academic journal) Magnetism(I-academic journal) .(O)"}}
{"id": "122", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "theory", "astronomical object", "enzyme", "university", "country", "award", "chemical element", "chemical compound", "person", "location", "discipline", "event", "organization", "scientist", "protein"], "instance": {"id": "122", "words": ["The", "ten", "attendees", "were", "conference", "organizer", "J.", "Peter", "Pearman", ",", "Frank", "Drake", ",", "Philip", "Morrison", ",", "businessman", "and", "radio", "amateur", "Dana", "Atchley", ",", "chemist", "Melvin", "Calvin", ",", "astronomer", "Su-Shu", "Huang", ",", "neuroscientist", "John", "C.", "Lilly", ",", "inventor", "Barney", "Oliver", ",", "astronomer", "Carl", "Sagan", "and", "radio-astronomer", "Otto", "Struve", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, theory, astronomical object, enzyme, university, country, award, chemical element, chemical compound, person, location, discipline, event, organization, scientist, protein and O.\nSentence: The ten attendees were conference organizer J. Peter Pearman , Frank Drake , Philip Morrison , businessman and radio amateur Dana Atchley , chemist Melvin Calvin , astronomer Su-Shu Huang , neuroscientist John C. Lilly , inventor Barney Oliver , astronomer Carl Sagan and radio-astronomer Otto Struve .", "prompt_labels": "The(O) ten(O) attendees(O) were(O) conference(O) organizer(O) J.(B-scientist) Peter(I-scientist) Pearman(I-scientist) ,(O) Frank(B-scientist) Drake(I-scientist) ,(O) Philip(B-scientist) Morrison(I-scientist) ,(O) businessman(O) and(O) radio(O) amateur(O) Dana(B-person) Atchley(I-person) ,(O) chemist(O) Melvin(B-scientist) Calvin(I-scientist) ,(O) astronomer(O) Su-Shu(B-scientist) Huang(I-scientist) ,(O) neuroscientist(O) John(B-scientist) C.(I-scientist) Lilly(I-scientist) ,(O) inventor(O) Barney(B-scientist) Oliver(I-scientist) ,(O) astronomer(O) Carl(B-scientist) Sagan(I-scientist) and(O) radio-astronomer(O) Otto(B-scientist) Struve(I-scientist) .(O)"}}
{"id": "275", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "academic journal", "scientist", "theory", "discipline", "enzyme", "protein", "country", "award", "event", "organization", "university", "chemical element", "chemical compound", "location", "person"], "instance": {"id": "275", "words": ["Whereas", "the", "xenon", "fluorides", "are", "well", "characterized", ",", "with", "the", "exception", "of", "dichloride", "Xenon", "dichloride", "and", "Xenon", "tetrachloride", ",", "the", "other", "halides", "are", "not", "known", "."], "labels": ["O", "O", "B-chemical compound", "I-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, academic journal, scientist, theory, discipline, enzyme, protein, country, award, event, organization, university, chemical element, chemical compound, location, person and O.\nSentence: Whereas the xenon fluorides are well characterized , with the exception of dichloride Xenon dichloride and Xenon tetrachloride , the other halides are not known .", "prompt_labels": "Whereas(O) the(O) xenon(B-chemical compound) fluorides(I-chemical compound) are(O) well(O) characterized(O) ,(O) with(O) the(O) exception(O) of(O) dichloride(B-chemical compound) Xenon(B-chemical compound) dichloride(I-chemical compound) and(O) Xenon(B-chemical compound) tetrachloride(I-chemical compound) ,(O) the(O) other(O) halides(O) are(O) not(O) known(O) .(O)"}}
{"id": "424", "dataset": "crossner_science", "split": "test", "label_list": ["university", "theory", "protein", "country", "enzyme", "location", "person", "organization", "scientist", "award", "astronomical object", "chemical element", "academic journal", "discipline", "chemical compound", "event"], "instance": {"id": "424", "words": ["Taurasi", "also", "received", "many", "personal", "accolades", "at", "UConn", "including", "the", "2003", "and", "2004", "Naismith", "College", "Player", "of", "the", "Year", "awards", ",", "the", "2003", "Wade", "Trophy", ",", "the", "2003", "and", "2004", "Honda", "Sports", "Award", "and", "the", "2003", "Associated", "Press", "Player", "of", "the", "Year", "award", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "B-university", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, theory, protein, country, enzyme, location, person, organization, scientist, award, astronomical object, chemical element, academic journal, discipline, chemical compound, event and O.\nSentence: Taurasi also received many personal accolades at UConn including the 2003 and 2004 Naismith College Player of the Year awards , the 2003 Wade Trophy , the 2003 and 2004 Honda Sports Award and the 2003 Associated Press Player of the Year award .", "prompt_labels": "Taurasi(B-person) also(O) received(O) many(O) personal(O) accolades(O) at(O) UConn(B-university) including(O) the(O) 2003(O) and(O) 2004(O) Naismith(B-award) College(I-award) Player(I-award) of(I-award) the(I-award) Year(I-award) awards(I-award) ,(O) the(O) 2003(O) Wade(B-award) Trophy(I-award) ,(O) the(O) 2003(O) and(O) 2004(O) Honda(B-award) Sports(I-award) Award(I-award) and(O) the(O) 2003(O) Associated(B-award) Press(I-award) Player(I-award) of(I-award) the(I-award) Year(I-award) award(I-award) .(O)"}}
{"id": "234", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "discipline", "university", "enzyme", "theory", "chemical compound", "academic journal", "chemical element", "organization", "event", "country", "protein", "scientist", "person", "location", "award"], "instance": {"id": "234", "words": ["He", "was", "a", "member", "of", "the", "Royal", "Swedish", "Academy", "of", "Sciences", "from", "1836", "and", "a", "foreign", "member", "of", "the", "Royal", "Society", "of", "London", "from", "1837", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, discipline, university, enzyme, theory, chemical compound, academic journal, chemical element, organization, event, country, protein, scientist, person, location, award and O.\nSentence: He was a member of the Royal Swedish Academy of Sciences from 1836 and a foreign member of the Royal Society of London from 1837 .", "prompt_labels": "He(O) was(O) a(O) member(O) of(O) the(O) Royal(B-organization) Swedish(I-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) from(O) 1836(O) and(O) a(O) foreign(O) member(O) of(O) the(O) Royal(B-organization) Society(I-organization) of(I-organization) London(I-organization) from(O) 1837(O) .(O)"}}
{"id": "286", "dataset": "crossner_science", "split": "test", "label_list": ["chemical compound", "enzyme", "discipline", "organization", "country", "academic journal", "protein", "award", "location", "theory", "university", "chemical element", "astronomical object", "scientist", "person", "event"], "instance": {"id": "286", "words": ["Palsson", "serves", "on", "the", "editorial", "board", "of", "several", "scientific", "journals", "including", "Annals", "of", "Biomedical", "Engineering", ",", "Biotechnology", "and", "Bioengineering", ",", "Metabolic", "Engineering", "and", "Molecular", "Systems", "Biology", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, enzyme, discipline, organization, country, academic journal, protein, award, location, theory, university, chemical element, astronomical object, scientist, person, event and O.\nSentence: Palsson serves on the editorial board of several scientific journals including Annals of Biomedical Engineering , Biotechnology and Bioengineering , Metabolic Engineering and Molecular Systems Biology .", "prompt_labels": "Palsson(B-scientist) serves(O) on(O) the(O) editorial(O) board(O) of(O) several(O) scientific(O) journals(O) including(O) Annals(B-academic journal) of(I-academic journal) Biomedical(I-academic journal) Engineering(I-academic journal) ,(O) Biotechnology(B-academic journal) and(I-academic journal) Bioengineering(I-academic journal) ,(O) Metabolic(B-academic journal) Engineering(I-academic journal) and(O) Molecular(B-academic journal) Systems(I-academic journal) Biology(I-academic journal) .(O)"}}
{"id": "505", "dataset": "crossner_science", "split": "test", "label_list": ["organization", "discipline", "enzyme", "location", "protein", "event", "chemical compound", "academic journal", "award", "scientist", "person", "country", "chemical element", "astronomical object", "university", "theory"], "instance": {"id": "505", "words": ["Some", "notable", "examples", "include", "Journal", "of", "Computational", "Biology", "and", "PLOS", "Computational", "Biology", "."], "labels": ["O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, discipline, enzyme, location, protein, event, chemical compound, academic journal, award, scientist, person, country, chemical element, astronomical object, university, theory and O.\nSentence: Some notable examples include Journal of Computational Biology and PLOS Computational Biology .", "prompt_labels": "Some(O) notable(O) examples(O) include(O) Journal(B-academic journal) of(I-academic journal) Computational(I-academic journal) Biology(I-academic journal) and(O) PLOS(B-academic journal) Computational(I-academic journal) Biology(I-academic journal) .(O)"}}
{"id": "133", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "astronomical object", "award", "person", "discipline", "event", "academic journal", "location", "scientist", "country", "protein", "chemical element", "chemical compound", "enzyme", "organization", "university"], "instance": {"id": "133", "words": ["This", "led", "to", "efforts", "such", "as", "Optical", "Gravitational", "Lensing", "Experiment", ",", "or", "OGLE", ",", "that", "have", "characterized", "hundreds", "of", "such", "events", ",", "including", "those", "of", "OGLE-2016-BLG-1190Lb", "and", "OGLE-2016-BLG-1195Lb", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, astronomical object, award, person, discipline, event, academic journal, location, scientist, country, protein, chemical element, chemical compound, enzyme, organization, university and O.\nSentence: This led to efforts such as Optical Gravitational Lensing Experiment , or OGLE , that have characterized hundreds of such events , including those of OGLE-2016-BLG-1190Lb and OGLE-2016-BLG-1195Lb .", "prompt_labels": "This(O) led(O) to(O) efforts(O) such(O) as(O) Optical(B-organization) Gravitational(I-organization) Lensing(I-organization) Experiment(I-organization) ,(O) or(O) OGLE(B-organization) ,(O) that(O) have(O) characterized(O) hundreds(O) of(O) such(O) events(O) ,(O) including(O) those(O) of(O) OGLE-2016-BLG-1190Lb(B-astronomical object) and(O) OGLE-2016-BLG-1195Lb(B-astronomical object) .(O)"}}
{"id": "318", "dataset": "crossner_science", "split": "test", "label_list": ["person", "chemical compound", "protein", "event", "university", "theory", "country", "discipline", "scientist", "enzyme", "chemical element", "organization", "academic journal", "astronomical object", "award", "location"], "instance": {"id": "318", "words": ["The", "term", "can", "also", "be", "used", "to", "describe", "the", "motion", "of", "a", "satellite", "across", "its", "parent", "planet", ",", "for", "instance", "one", "of", "the", "Galilean", "satellites", "(", "Io", ",", "Europa", ",", "Ganymede", ",", "Callisto", ")", "across", "Jupiter", ",", "as", "seen", "from", "Earth", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O", "O", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, chemical compound, protein, event, university, theory, country, discipline, scientist, enzyme, chemical element, organization, academic journal, astronomical object, award, location and O.\nSentence: The term can also be used to describe the motion of a satellite across its parent planet , for instance one of the Galilean satellites ( Io , Europa , Ganymede , Callisto ) across Jupiter , as seen from Earth .", "prompt_labels": "The(O) term(O) can(O) also(O) be(O) used(O) to(O) describe(O) the(O) motion(O) of(O) a(O) satellite(O) across(O) its(O) parent(O) planet(O) ,(O) for(O) instance(O) one(O) of(O) the(O) Galilean(O) satellites(O) ((O) Io(B-astronomical object) ,(O) Europa(B-astronomical object) ,(O) Ganymede(B-astronomical object) ,(O) Callisto(B-astronomical object) )(O) across(O) Jupiter(B-astronomical object) ,(O) as(O) seen(O) from(O) Earth(B-astronomical object) .(O)"}}
{"id": "388", "dataset": "crossner_science", "split": "test", "label_list": ["person", "chemical compound", "organization", "university", "astronomical object", "chemical element", "country", "award", "theory", "discipline", "enzyme", "academic journal", "location", "scientist", "event", "protein"], "instance": {"id": "388", "words": ["The", "invitees", "included", "Walther", "Bothe", ",", "Siegfried", "Flügge", ",", "Hans", "Geiger", ",", "Otto", "Hahn", ",", "Paul", "Harteck", ",", "Gerhard", "Hoffmann", ",", "Josef", "Mattauch", ",", "and", "Georg", "Stetter", "."], "labels": ["O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, chemical compound, organization, university, astronomical object, chemical element, country, award, theory, discipline, enzyme, academic journal, location, scientist, event, protein and O.\nSentence: The invitees included Walther Bothe , Siegfried Flügge , Hans Geiger , Otto Hahn , Paul Harteck , Gerhard Hoffmann , Josef Mattauch , and Georg Stetter .", "prompt_labels": "The(O) invitees(O) included(O) Walther(B-scientist) Bothe(I-scientist) ,(O) Siegfried(B-scientist) Flügge(I-scientist) ,(O) Hans(B-scientist) Geiger(I-scientist) ,(O) Otto(B-scientist) Hahn(I-scientist) ,(O) Paul(B-scientist) Harteck(I-scientist) ,(O) Gerhard(B-scientist) Hoffmann(I-scientist) ,(O) Josef(B-scientist) Mattauch(I-scientist) ,(O) and(O) Georg(B-scientist) Stetter(I-scientist) .(O)"}}
{"id": "533", "dataset": "crossner_science", "split": "test", "label_list": ["scientist", "chemical element", "protein", "astronomical object", "university", "person", "award", "theory", "discipline", "enzyme", "academic journal", "organization", "location", "country", "event", "chemical compound"], "instance": {"id": "533", "words": ["An", "E3", "uibiquitin", "ligase", "complex", "composed", "of", "Cullin", "(", "CUL3", ")", ",", "speckle-type", "POZ", "protein", "(", "SPOP", ")", ",", "and", "RING-box", "protein", "1", "(", "RBX1", ")", "has", "been", "shown", "to", "mark", "the", "Bmi-1", "with", "ubiquitin", "(", "a", "process", "known", "as", "ubiquitin", "ation", ")", "."], "labels": ["O", "B-protein", "I-protein", "I-protein", "I-protein", "O", "O", "B-protein", "O", "B-protein", "O", "O", "B-protein", "I-protein", "I-protein", "O", "B-protein", "O", "O", "O", "B-protein", "I-protein", "O", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, chemical element, protein, astronomical object, university, person, award, theory, discipline, enzyme, academic journal, organization, location, country, event, chemical compound and O.\nSentence: An E3 uibiquitin ligase complex composed of Cullin ( CUL3 ) , speckle-type POZ protein ( SPOP ) , and RING-box protein 1 ( RBX1 ) has been shown to mark the Bmi-1 with ubiquitin ( a process known as ubiquitin ation ) .", "prompt_labels": "An(O) E3(B-protein) uibiquitin(I-protein) ligase(I-protein) complex(I-protein) composed(O) of(O) Cullin(B-protein) ((O) CUL3(B-protein) )(O) ,(O) speckle-type(B-protein) POZ(I-protein) protein(I-protein) ((O) SPOP(B-protein) )(O) ,(O) and(O) RING-box(B-protein) protein(I-protein) 1(O) ((O) RBX1(B-protein) )(O) has(O) been(O) shown(O) to(O) mark(O) the(O) Bmi-1(B-protein) with(O) ubiquitin(B-protein) ((O) a(O) process(O) known(O) as(O) ubiquitin(O) ation(O) )(O) .(O)"}}
{"id": "271", "dataset": "crossner_science", "split": "test", "label_list": ["person", "protein", "discipline", "chemical compound", "organization", "location", "chemical element", "university", "theory", "scientist", "event", "award", "academic journal", "country", "astronomical object", "enzyme"], "instance": {"id": "271", "words": ["Max", "Planck", ",", "Albert", "Einstein", ",", "and", "Niels", "Bohr", "postulated", "the", "occurrence", "of", "energy", "in", "discrete", "quantities", "(", "quanta", ")", "in", "order", "to", "explain", "phenomena", "such", "as", "the", "spectrum", "of", "black-body", "radiation", ",", "the", "photoelectric", "effect", ",", "and", "the", "stability", "and", "spectra", "of", "atoms", "."], "labels": ["B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-theory", "I-theory", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, protein, discipline, chemical compound, organization, location, chemical element, university, theory, scientist, event, award, academic journal, country, astronomical object, enzyme and O.\nSentence: Max Planck , Albert Einstein , and Niels Bohr postulated the occurrence of energy in discrete quantities ( quanta ) in order to explain phenomena such as the spectrum of black-body radiation , the photoelectric effect , and the stability and spectra of atoms .", "prompt_labels": "Max(B-scientist) Planck(I-scientist) ,(O) Albert(B-scientist) Einstein(I-scientist) ,(O) and(O) Niels(B-scientist) Bohr(I-scientist) postulated(O) the(O) occurrence(O) of(O) energy(O) in(O) discrete(O) quantities(O) ((O) quanta(O) )(O) in(O) order(O) to(O) explain(O) phenomena(O) such(O) as(O) the(O) spectrum(O) of(O) black-body(O) radiation(O) ,(O) the(O) photoelectric(B-theory) effect(I-theory) ,(O) and(O) the(O) stability(O) and(O) spectra(O) of(O) atoms(O) .(O)"}}
{"id": "410", "dataset": "crossner_science", "split": "test", "label_list": ["award", "person", "country", "discipline", "organization", "university", "academic journal", "astronomical object", "chemical compound", "event", "chemical element", "location", "enzyme", "theory", "protein", "scientist"], "instance": {"id": "410", "words": ["They", "also", "noticed", "the", "similarity", "between", "this", "test", "theory", "and", "Lorentz", "ether", "theory", "of", "Hendrik", "Lorentz", ",", "Joseph", "Larmor", "and", "Henri", "Poincaré", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-theory", "I-theory", "I-theory", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, country, discipline, organization, university, academic journal, astronomical object, chemical compound, event, chemical element, location, enzyme, theory, protein, scientist and O.\nSentence: They also noticed the similarity between this test theory and Lorentz ether theory of Hendrik Lorentz , Joseph Larmor and Henri Poincaré .", "prompt_labels": "They(O) also(O) noticed(O) the(O) similarity(O) between(O) this(O) test(O) theory(O) and(O) Lorentz(B-theory) ether(I-theory) theory(I-theory) of(O) Hendrik(B-scientist) Lorentz(I-scientist) ,(O) Joseph(B-scientist) Larmor(I-scientist) and(O) Henri(B-scientist) Poincaré(I-scientist) .(O)"}}
{"id": "477", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "scientist", "country", "chemical compound", "location", "organization", "discipline", "award", "theory", "protein", "astronomical object", "university", "event", "chemical element", "person", "academic journal"], "instance": {"id": "477", "words": ["In", "1893", "he", "received", "the", "Royal", "Society", "'", "s", "Copley", "Medal", ",", "then", "the", "most", "prestigious", "scientific", "prize", "in", "the", "world", ",", "for", "his", "researches", "and", "discoveries", "in", "physical", "science", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "I-discipline", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, scientist, country, chemical compound, location, organization, discipline, award, theory, protein, astronomical object, university, event, chemical element, person, academic journal and O.\nSentence: In 1893 he received the Royal Society ' s Copley Medal , then the most prestigious scientific prize in the world , for his researches and discoveries in physical science .", "prompt_labels": "In(O) 1893(O) he(O) received(O) the(O) Royal(B-organization) Society(I-organization) '(O) s(O) Copley(B-award) Medal(I-award) ,(O) then(O) the(O) most(O) prestigious(O) scientific(O) prize(O) in(O) the(O) world(O) ,(O) for(O) his(O) researches(O) and(O) discoveries(O) in(O) physical(B-discipline) science(I-discipline) .(O)"}}
{"id": "398", "dataset": "crossner_science", "split": "test", "label_list": ["country", "theory", "astronomical object", "event", "organization", "protein", "scientist", "chemical compound", "academic journal", "discipline", "person", "enzyme", "award", "university", "chemical element", "location"], "instance": {"id": "398", "words": ["The", "boy", "was", "sent", "to", "school", "at", "the", "Lycée", "Louis-le-Grand", ",", "part", "of", "the", "University", "of", "Paris", ",", "and", "it", "was", "decided", "that", "he", "would", "carry", "on", "the", "family", "martial", "tradition.", "The", "comte", ",", "the", "boy", "'s", "great-grandfather", ",", "enrolled", "the", "boy", "in", "a", "program", "to", "train", "future", "Musketeers", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, theory, astronomical object, event, organization, protein, scientist, chemical compound, academic journal, discipline, person, enzyme, award, university, chemical element, location and O.\nSentence: The boy was sent to school at the Lycée Louis-le-Grand , part of the University of Paris , and it was decided that he would carry on the family martial tradition. The comte , the boy 's great-grandfather , enrolled the boy in a program to train future Musketeers .", "prompt_labels": "The(O) boy(O) was(O) sent(O) to(O) school(O) at(O) the(O) Lycée(B-organization) Louis-le-Grand(I-organization) ,(O) part(O) of(O) the(O) University(B-university) of(I-university) Paris(I-university) ,(O) and(O) it(O) was(O) decided(O) that(O) he(O) would(O) carry(O) on(O) the(O) family(O) martial(O) tradition.(O) The(O) comte(O) ,(O) the(O) boy(O) 's(O) great-grandfather(O) ,(O) enrolled(O) the(O) boy(O) in(O) a(O) program(O) to(O) train(O) future(O) Musketeers(O) .(O)"}}
{"id": "319", "dataset": "crossner_science", "split": "test", "label_list": ["location", "person", "scientist", "protein", "organization", "country", "theory", "university", "enzyme", "discipline", "event", "award", "astronomical object", "academic journal", "chemical element", "chemical compound"], "instance": {"id": "319", "words": ["For", "the", "Good", "Times", "(", "Ray", "Price", ")", "won", "Song", "of", "the", "Year", "in", "1970", "from", "the", "Academy", "of", "Country", "Music", ",", "while", "Sunday", "Morning", "Coming", "Down", "(", "Johnny", "Cash", ")", "won", "the", "same", "award", "from", "the", "Academy", "'s", "rival", ",", "the", "Country", "Music", "Association", ",", "in", "the", "same", "year", "."], "labels": ["O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, scientist, protein, organization, country, theory, university, enzyme, discipline, event, award, astronomical object, academic journal, chemical element, chemical compound and O.\nSentence: For the Good Times ( Ray Price ) won Song of the Year in 1970 from the Academy of Country Music , while Sunday Morning Coming Down ( Johnny Cash ) won the same award from the Academy 's rival , the Country Music Association , in the same year .", "prompt_labels": "For(O) the(O) Good(O) Times(O) ((O) Ray(B-person) Price(I-person) )(O) won(O) Song(B-award) of(I-award) the(I-award) Year(I-award) in(O) 1970(O) from(O) the(O) Academy(B-organization) of(I-organization) Country(I-organization) Music(I-organization) ,(O) while(O) Sunday(O) Morning(O) Coming(O) Down(O) ((O) Johnny(B-person) Cash(I-person) )(O) won(O) the(O) same(O) award(O) from(O) the(O) Academy(O) 's(O) rival(O) ,(O) the(O) Country(B-organization) Music(I-organization) Association(I-organization) ,(O) in(O) the(O) same(O) year(O) .(O)"}}
{"id": "447", "dataset": "crossner_science", "split": "test", "label_list": ["award", "person", "scientist", "discipline", "protein", "university", "chemical compound", "theory", "location", "enzyme", "organization", "astronomical object", "event", "academic journal", "chemical element", "country"], "instance": {"id": "447", "words": ["The", "group", "included", "the", "physicists", "Walther", "Bothe", ",", "Robert", "Döpel", ",", "Hans", "Geiger", ",", "Wolfgang", "Gentner", ",", "Wilhelm", "Hanle", ",", "Gerhard", "Hoffmann", ",", "and", "Joos", "."], "labels": ["O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, scientist, discipline, protein, university, chemical compound, theory, location, enzyme, organization, astronomical object, event, academic journal, chemical element, country and O.\nSentence: The group included the physicists Walther Bothe , Robert Döpel , Hans Geiger , Wolfgang Gentner , Wilhelm Hanle , Gerhard Hoffmann , and Joos .", "prompt_labels": "The(O) group(O) included(O) the(O) physicists(O) Walther(B-scientist) Bothe(I-scientist) ,(O) Robert(B-scientist) Döpel(I-scientist) ,(O) Hans(B-scientist) Geiger(I-scientist) ,(O) Wolfgang(B-scientist) Gentner(I-scientist) ,(O) Wilhelm(B-scientist) Hanle(I-scientist) ,(O) Gerhard(B-scientist) Hoffmann(I-scientist) ,(O) and(O) Joos(B-scientist) .(O)"}}
{"id": "148", "dataset": "crossner_science", "split": "test", "label_list": ["scientist", "astronomical object", "person", "chemical compound", "enzyme", "academic journal", "theory", "location", "award", "discipline", "organization", "country", "university", "chemical element", "protein", "event"], "instance": {"id": "148", "words": ["In", "addition", "to", "the", "lighthouse", ",", "there", "are", "four", "other", "sites", "in", "Point", "Loma", "listed", "on", "the", "National", "Register", "of", "Historic", "Places", ":", "Cabrillo", "National", "Monument", ",", "the", "Marine", "Corps", "Recruit", "Depot", "San", "Diego", ",", "Naval", "Training", "Center", "San", "Diego", ",", "and", "Rosecroft", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "I-location", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, astronomical object, person, chemical compound, enzyme, academic journal, theory, location, award, discipline, organization, country, university, chemical element, protein, event and O.\nSentence: In addition to the lighthouse , there are four other sites in Point Loma listed on the National Register of Historic Places : Cabrillo National Monument , the Marine Corps Recruit Depot San Diego , Naval Training Center San Diego , and Rosecroft .", "prompt_labels": "In(O) addition(O) to(O) the(O) lighthouse(O) ,(O) there(O) are(O) four(O) other(O) sites(O) in(O) Point(B-location) Loma(I-location) listed(O) on(O) the(O) National(O) Register(O) of(O) Historic(O) Places(O) :(O) Cabrillo(B-location) National(I-location) Monument(I-location) ,(O) the(O) Marine(B-location) Corps(I-location) Recruit(I-location) Depot(I-location) San(I-location) Diego(I-location) ,(O) Naval(B-location) Training(I-location) Center(I-location) San(I-location) Diego(I-location) ,(O) and(O) Rosecroft(B-location) .(O)"}}
{"id": "49", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "country", "discipline", "theory", "protein", "scientist", "academic journal", "award", "university", "chemical element", "location", "organization", "chemical compound", "enzyme", "person", "event"], "instance": {"id": "49", "words": ["During", "his", "academic", "career", "he", "served", "on", "the", "editorial", "boards", "of", "the", "Journal", "of", "Rational", "Mechanics", "and", "Analysis", ",", "Archive", "for", "Rational", "Mechanics", "and", "Analysis", ",", "Journal", "of", "Elasticity", ",", "and", "the", "International", "Journal", "of", "Solids", "and", "Structures", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, country, discipline, theory, protein, scientist, academic journal, award, university, chemical element, location, organization, chemical compound, enzyme, person, event and O.\nSentence: During his academic career he served on the editorial boards of the Journal of Rational Mechanics and Analysis , Archive for Rational Mechanics and Analysis , Journal of Elasticity , and the International Journal of Solids and Structures .", "prompt_labels": "During(O) his(O) academic(O) career(O) he(O) served(O) on(O) the(O) editorial(O) boards(O) of(O) the(O) Journal(B-academic journal) of(I-academic journal) Rational(I-academic journal) Mechanics(I-academic journal) and(I-academic journal) Analysis(I-academic journal) ,(O) Archive(B-academic journal) for(I-academic journal) Rational(I-academic journal) Mechanics(I-academic journal) and(I-academic journal) Analysis(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Elasticity(I-academic journal) ,(O) and(O) the(O) International(B-academic journal) Journal(I-academic journal) of(I-academic journal) Solids(I-academic journal) and(I-academic journal) Structures(I-academic journal) .(O)"}}
{"id": "411", "dataset": "crossner_science", "split": "test", "label_list": ["organization", "theory", "event", "scientist", "chemical compound", "enzyme", "discipline", "country", "university", "award", "academic journal", "chemical element", "location", "protein", "astronomical object", "person"], "instance": {"id": "411", "words": ["It", "was", "founded", "by", "William", "Bateson", "and", "Edith", "Rebecca", "Saunders", "in", "1919", "and", "celebrates", "its", "centenary", "year", "in", "2019", "."], "labels": ["O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, theory, event, scientist, chemical compound, enzyme, discipline, country, university, award, academic journal, chemical element, location, protein, astronomical object, person and O.\nSentence: It was founded by William Bateson and Edith Rebecca Saunders in 1919 and celebrates its centenary year in 2019 .", "prompt_labels": "It(O) was(O) founded(O) by(O) William(B-scientist) Bateson(I-scientist) and(O) Edith(B-scientist) Rebecca(I-scientist) Saunders(I-scientist) in(O) 1919(O) and(O) celebrates(O) its(O) centenary(O) year(O) in(O) 2019(O) .(O)"}}
{"id": "330", "dataset": "crossner_science", "split": "test", "label_list": ["award", "discipline", "academic journal", "protein", "country", "theory", "enzyme", "astronomical object", "location", "chemical compound", "organization", "event", "chemical element", "university", "person", "scientist"], "instance": {"id": "330", "words": ["The", "NLR", "proteins", "are", "not", "linear", ":", "the", "NB-ARC", "domain", "is", "sandwiched", "in", "between", "the", "Leucine-rich", "repeat", "and", "Toll-Interleukin", "receptor", "/", "CC", "domains", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O", "B-protein", "I-protein", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, discipline, academic journal, protein, country, theory, enzyme, astronomical object, location, chemical compound, organization, event, chemical element, university, person, scientist and O.\nSentence: The NLR proteins are not linear : the NB-ARC domain is sandwiched in between the Leucine-rich repeat and Toll-Interleukin receptor / CC domains .", "prompt_labels": "The(O) NLR(O) proteins(O) are(O) not(O) linear(O) :(O) the(O) NB-ARC(B-protein) domain(I-protein) is(O) sandwiched(O) in(O) between(O) the(O) Leucine-rich(B-protein) repeat(I-protein) and(O) Toll-Interleukin(B-protein) receptor(I-protein) /(O) CC(O) domains(O) .(O)"}}
{"id": "199", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "chemical compound", "event", "protein", "country", "award", "discipline", "organization", "person", "astronomical object", "location", "scientist", "university", "enzyme", "chemical element", "academic journal"], "instance": {"id": "199", "words": ["He", "was", "appointed", "as", "assistant", "to", "the", "astronomer", "Giovanni", "Battista", "Donati", "in", "1862", ",", "professor", "at", "the", "technological", "institute", "of", "Bologna", "in", "1864", ",", "professor", "of", "physics", "at", "the", "University", "of", "Cagliari", "in", "1873", ",", "and", ",", "finally", ",", "successor", "to", "his", "father", "in", "1881", "in", "the", "chair", "of", "technological", "physics", "at", "the", "University", "of", "Pisa", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "O", "O", "O", "B-discipline", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "I-discipline", "O", "O", "B-university", "I-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, chemical compound, event, protein, country, award, discipline, organization, person, astronomical object, location, scientist, university, enzyme, chemical element, academic journal and O.\nSentence: He was appointed as assistant to the astronomer Giovanni Battista Donati in 1862 , professor at the technological institute of Bologna in 1864 , professor of physics at the University of Cagliari in 1873 , and , finally , successor to his father in 1881 in the chair of technological physics at the University of Pisa .", "prompt_labels": "He(O) was(O) appointed(O) as(O) assistant(O) to(O) the(O) astronomer(O) Giovanni(B-scientist) Battista(I-scientist) Donati(I-scientist) in(O) 1862(O) ,(O) professor(O) at(O) the(O) technological(B-location) institute(I-location) of(I-location) Bologna(I-location) in(O) 1864(O) ,(O) professor(O) of(O) physics(B-discipline) at(O) the(O) University(B-university) of(I-university) Cagliari(I-university) in(O) 1873(O) ,(O) and(O) ,(O) finally(O) ,(O) successor(O) to(O) his(O) father(O) in(O) 1881(O) in(O) the(O) chair(O) of(O) technological(B-discipline) physics(I-discipline) at(O) the(O) University(B-university) of(I-university) Pisa(I-university) .(O)"}}
{"id": "393", "dataset": "crossner_science", "split": "test", "label_list": ["university", "chemical element", "person", "organization", "astronomical object", "scientist", "academic journal", "country", "award", "location", "discipline", "protein", "event", "chemical compound", "theory", "enzyme"], "instance": {"id": "393", "words": ["He", "served", "on", "the", "editorial", "boards", "of", "several", "scientific", "journals", ",", "including", "NeuroImage", ",", "European", "Journal", "of", "Neuroscience", ",", "European", "Journal", "of", "Physiology", ",", "NeuroReport", ",", "Physiological", "Reviews", ",", "Journal", "of", "Motor", "Behavior", ",", "Perception", ",", "Cognition", ",", "Behavioural", "Brain", "Research", ",", "and", "Experimental", "Brain", "Research", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "O", "B-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, chemical element, person, organization, astronomical object, scientist, academic journal, country, award, location, discipline, protein, event, chemical compound, theory, enzyme and O.\nSentence: He served on the editorial boards of several scientific journals , including NeuroImage , European Journal of Neuroscience , European Journal of Physiology , NeuroReport , Physiological Reviews , Journal of Motor Behavior , Perception , Cognition , Behavioural Brain Research , and Experimental Brain Research .", "prompt_labels": "He(O) served(O) on(O) the(O) editorial(O) boards(O) of(O) several(O) scientific(O) journals(O) ,(O) including(O) NeuroImage(B-academic journal) ,(O) European(B-academic journal) Journal(I-academic journal) of(I-academic journal) Neuroscience(I-academic journal) ,(O) European(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physiology(I-academic journal) ,(O) NeuroReport(B-academic journal) ,(O) Physiological(B-academic journal) Reviews(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Motor(I-academic journal) Behavior(I-academic journal) ,(O) Perception(B-academic journal) ,(O) Cognition(B-academic journal) ,(O) Behavioural(B-academic journal) Brain(I-academic journal) Research(I-academic journal) ,(O) and(O) Experimental(B-academic journal) Brain(I-academic journal) Research(I-academic journal) .(O)"}}
{"id": "13", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "scientist", "chemical compound", "academic journal", "protein", "organization", "award", "theory", "astronomical object", "person", "location", "university", "country", "event", "discipline", "enzyme"], "instance": {"id": "13", "words": ["Most", "widely", "used", "to", "assess", "potential", "synthetic", "lethal", "interactions", "is", "using", "siRNA", "and", "CRISPR", "to", "modify", "target", "genes", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, scientist, chemical compound, academic journal, protein, organization, award, theory, astronomical object, person, location, university, country, event, discipline, enzyme and O.\nSentence: Most widely used to assess potential synthetic lethal interactions is using siRNA and CRISPR to modify target genes .", "prompt_labels": "Most(O) widely(O) used(O) to(O) assess(O) potential(O) synthetic(O) lethal(O) interactions(O) is(O) using(O) siRNA(O) and(O) CRISPR(O) to(O) modify(O) target(O) genes(O) .(O)"}}
{"id": "87", "dataset": "crossner_science", "split": "test", "label_list": ["discipline", "enzyme", "country", "chemical compound", "astronomical object", "award", "event", "scientist", "academic journal", "organization", "protein", "person", "theory", "location", "university", "chemical element"], "instance": {"id": "87", "words": ["In", "recognition", "of", "his", "contribution", "to", "the", "creation", "of", "modern", "electrical", "science", ",", "an", "international", "convention", ",", "signed", "at", "the", "1881", "International", "Exposition", "of", "Electricity", ",", "established", "the", "ampere", "as", "a", "standard", "unit", "of", "electrical", "measurement", ",", "along", "with", "the", "coulomb", ",", "volt", ",", "ohm", ",", "and", "watt", ",", "which", "are", "named", ",", "respectively", ",", "after", "Ampère", "'s", "contemporaries", "Charles-Augustin", "de", "Coulomb", "of", "France", ",", "Alessandro", "Volta", "of", "Italy", ",", "Georg", "Ohm", "of", "Germany", ",", "and", "James", "Watt", "of", "Scotland", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "I-discipline", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-country", "O", "B-scientist", "I-scientist", "O", "B-country", "O", "B-scientist", "I-scientist", "O", "B-country", "O", "O", "B-scientist", "I-scientist", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, enzyme, country, chemical compound, astronomical object, award, event, scientist, academic journal, organization, protein, person, theory, location, university, chemical element and O.\nSentence: In recognition of his contribution to the creation of modern electrical science , an international convention , signed at the 1881 International Exposition of Electricity , established the ampere as a standard unit of electrical measurement , along with the coulomb , volt , ohm , and watt , which are named , respectively , after Ampère 's contemporaries Charles-Augustin de Coulomb of France , Alessandro Volta of Italy , Georg Ohm of Germany , and James Watt of Scotland .", "prompt_labels": "In(O) recognition(O) of(O) his(O) contribution(O) to(O) the(O) creation(O) of(O) modern(O) electrical(B-discipline) science(I-discipline) ,(O) an(O) international(O) convention(O) ,(O) signed(O) at(O) the(O) 1881(B-event) International(I-event) Exposition(I-event) of(I-event) Electricity(I-event) ,(O) established(O) the(O) ampere(O) as(O) a(O) standard(O) unit(O) of(O) electrical(O) measurement(O) ,(O) along(O) with(O) the(O) coulomb(O) ,(O) volt(O) ,(O) ohm(O) ,(O) and(O) watt(O) ,(O) which(O) are(O) named(O) ,(O) respectively(O) ,(O) after(O) Ampère(B-scientist) 's(O) contemporaries(O) Charles-Augustin(B-scientist) de(I-scientist) Coulomb(I-scientist) of(O) France(B-country) ,(O) Alessandro(B-scientist) Volta(I-scientist) of(O) Italy(B-country) ,(O) Georg(B-scientist) Ohm(I-scientist) of(O) Germany(B-country) ,(O) and(O) James(B-scientist) Watt(I-scientist) of(O) Scotland(B-country) .(O)"}}
{"id": "182", "dataset": "crossner_science", "split": "test", "label_list": ["location", "university", "astronomical object", "award", "chemical element", "organization", "scientist", "enzyme", "country", "protein", "person", "academic journal", "theory", "chemical compound", "event", "discipline"], "instance": {"id": "182", "words": ["A", "pupil", "of", "Peter", "Gustav", "Lejeune", "Dirichlet", ",", "Gabriel", "Lamé", "and", "Augustin-Louis", "Cauchy", "Bjerknes", "worked", "for", "the", "rest", "of", "his", "life", "in", "the", "field", "of", "hydrodynamics", "."], "labels": ["O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, university, astronomical object, award, chemical element, organization, scientist, enzyme, country, protein, person, academic journal, theory, chemical compound, event, discipline and O.\nSentence: A pupil of Peter Gustav Lejeune Dirichlet , Gabriel Lamé and Augustin-Louis Cauchy Bjerknes worked for the rest of his life in the field of hydrodynamics .", "prompt_labels": "A(O) pupil(O) of(O) Peter(B-scientist) Gustav(I-scientist) Lejeune(I-scientist) Dirichlet(I-scientist) ,(O) Gabriel(B-scientist) Lamé(I-scientist) and(O) Augustin-Louis(B-scientist) Cauchy(I-scientist) Bjerknes(I-scientist) worked(O) for(O) the(O) rest(O) of(O) his(O) life(O) in(O) the(O) field(O) of(O) hydrodynamics(B-discipline) .(O)"}}
{"id": "222", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "country", "theory", "university", "award", "location", "organization", "scientist", "protein", "discipline", "chemical compound", "person", "academic journal", "event", "enzyme", "astronomical object"], "instance": {"id": "222", "words": ["The", "lithophile", "elements", "include", ":", "Al", ",", "Boron", ",", "Barium", ",", "Beryllium", ",", "Bromine", ",", "Calcium", ",", "Chlorine", ",", "Cr", ",", "Caesium", ",", "Fluorine", ",", "Iodine", ",", "Hf", ",", "Potassium", ",", "Lithium", ",", "Magnesium", ",", "Sodium", ",", "Niobium", ",", "O", ",", "P", ",", "Rubidium", ",", "Sc", ",", "Si", ",", "Strontium", ",", "Ta", ",", "Th", ",", "Ti", ",", "U", ",", "V", ",", "Y", ",", "Zirconium", ",", "W", "and", "the", "lanthanides", "."], "labels": ["O", "O", "O", "O", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "O", "B-chemical element", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, country, theory, university, award, location, organization, scientist, protein, discipline, chemical compound, person, academic journal, event, enzyme, astronomical object and O.\nSentence: The lithophile elements include : Al , Boron , Barium , Beryllium , Bromine , Calcium , Chlorine , Cr , Caesium , Fluorine , Iodine , Hf , Potassium , Lithium , Magnesium , Sodium , Niobium , O , P , Rubidium , Sc , Si , Strontium , Ta , Th , Ti , U , V , Y , Zirconium , W and the lanthanides .", "prompt_labels": "The(O) lithophile(O) elements(O) include(O) :(O) Al(B-chemical element) ,(O) Boron(B-chemical element) ,(O) Barium(B-chemical element) ,(O) Beryllium(B-chemical element) ,(O) Bromine(B-chemical element) ,(O) Calcium(B-chemical element) ,(O) Chlorine(B-chemical element) ,(O) Cr(B-chemical element) ,(O) Caesium(B-chemical element) ,(O) Fluorine(B-chemical element) ,(O) Iodine(B-chemical element) ,(O) Hf(B-chemical element) ,(O) Potassium(B-chemical element) ,(O) Lithium(B-chemical element) ,(O) Magnesium(B-chemical element) ,(O) Sodium(B-chemical element) ,(O) Niobium(B-chemical element) ,(O) O(B-chemical element) ,(O) P(B-chemical element) ,(O) Rubidium(B-chemical element) ,(O) Sc(B-chemical element) ,(O) Si(B-chemical element) ,(O) Strontium(B-chemical element) ,(O) Ta(B-chemical element) ,(O) Th(B-chemical element) ,(O) Ti(B-chemical element) ,(O) U(B-chemical element) ,(O) V(B-chemical element) ,(O) Y(B-chemical element) ,(O) Zirconium(B-chemical element) ,(O) W(B-chemical element) and(O) the(O) lanthanides(B-chemical element) .(O)"}}
{"id": "98", "dataset": "crossner_science", "split": "test", "label_list": ["location", "theory", "person", "chemical compound", "country", "university", "academic journal", "protein", "event", "organization", "chemical element", "astronomical object", "scientist", "discipline", "enzyme", "award"], "instance": {"id": "98", "words": ["He", "was", "a", "life", "member", "of", "the", "Franklin", "Institute", ",", "the", "National", "Academy", "of", "Engineering", "and", "the", "Society", "for", "Advancement", "of", "Management", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, theory, person, chemical compound, country, university, academic journal, protein, event, organization, chemical element, astronomical object, scientist, discipline, enzyme, award and O.\nSentence: He was a life member of the Franklin Institute , the National Academy of Engineering and the Society for Advancement of Management .", "prompt_labels": "He(O) was(O) a(O) life(O) member(O) of(O) the(O) Franklin(B-organization) Institute(I-organization) ,(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Engineering(I-organization) and(O) the(O) Society(B-organization) for(I-organization) Advancement(I-organization) of(I-organization) Management(I-organization) .(O)"}}
{"id": "112", "dataset": "crossner_science", "split": "test", "label_list": ["event", "organization", "award", "theory", "academic journal", "chemical compound", "chemical element", "location", "protein", "person", "enzyme", "discipline", "scientist", "country", "astronomical object", "university"], "instance": {"id": "112", "words": ["These", "experimental", "and", "mathematical", "analyses", "were", "applied", "to", "several", "areas", "of", "geophysics", ":", "Earth", "'s", "shape", ",", "density", ",", "and", "gravity", "field", "(", "Pierre", "Bouguer", ",", "Alexis", "Clairaut", "and", "Henry", "Cavendish", ")", ",", "Earth", "'s", "magnetic", "field", "(", "Alexander", "von", "Humboldt", ",", "Edmund", "Halley", "and", "Carl", "Friedrich", "Gauss", ")", ",", "seismology", "(", "John", "Milne", "and", "Robert", "Mallet", ")", ",", "and", "the", "Earth", "'s", "age", ",", "heat", "and", "radioactivity", "(", "Arthur", "Holmes", "and", "William", "Thomson", ",", "1st", "Baron", "Kelvin", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-astronomical object", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "B-discipline", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "I-scientist", "I-scientist", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, award, theory, academic journal, chemical compound, chemical element, location, protein, person, enzyme, discipline, scientist, country, astronomical object, university and O.\nSentence: These experimental and mathematical analyses were applied to several areas of geophysics : Earth 's shape , density , and gravity field ( Pierre Bouguer , Alexis Clairaut and Henry Cavendish ) , Earth 's magnetic field ( Alexander von Humboldt , Edmund Halley and Carl Friedrich Gauss ) , seismology ( John Milne and Robert Mallet ) , and the Earth 's age , heat and radioactivity ( Arthur Holmes and William Thomson , 1st Baron Kelvin ) .", "prompt_labels": "These(O) experimental(O) and(O) mathematical(O) analyses(O) were(O) applied(O) to(O) several(O) areas(O) of(O) geophysics(B-discipline) :(O) Earth(B-astronomical object) 's(O) shape(O) ,(O) density(O) ,(O) and(O) gravity(O) field(O) ((O) Pierre(B-scientist) Bouguer(I-scientist) ,(O) Alexis(B-scientist) Clairaut(I-scientist) and(O) Henry(B-scientist) Cavendish(I-scientist) )(O) ,(O) Earth(B-astronomical object) 's(O) magnetic(O) field(O) ((O) Alexander(B-scientist) von(I-scientist) Humboldt(I-scientist) ,(O) Edmund(B-scientist) Halley(I-scientist) and(O) Carl(B-scientist) Friedrich(I-scientist) Gauss(I-scientist) )(O) ,(O) seismology(B-discipline) ((O) John(B-scientist) Milne(I-scientist) and(O) Robert(B-scientist) Mallet(I-scientist) )(O) ,(O) and(O) the(O) Earth(B-astronomical object) 's(O) age(O) ,(O) heat(O) and(O) radioactivity(O) ((O) Arthur(B-scientist) Holmes(I-scientist) and(O) William(B-scientist) Thomson(I-scientist) ,(I-scientist) 1st(I-scientist) Baron(I-scientist) Kelvin(I-scientist) )(O) .(O)"}}
{"id": "501", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "country", "award", "protein", "organization", "person", "theory", "location", "discipline", "chemical element", "academic journal", "university", "chemical compound", "enzyme", "event", "scientist"], "instance": {"id": "501", "words": ["Several", "DNA", "markers", ",", "belonging", "to", "the", "nuclear", ",", "mitochondrial", ",", "and", "chloroplast", "genomes", "(", "RuBisCO", ",", "COI", ",", "ITS", "+", "5.8S", ",", "SSU", ",", "18S", "...", ")", ",", "have", "been", "designed", "and", "successfully", "used", "for", "diatoms", "identification", "with", "NGS", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, country, award, protein, organization, person, theory, location, discipline, chemical element, academic journal, university, chemical compound, enzyme, event, scientist and O.\nSentence: Several DNA markers , belonging to the nuclear , mitochondrial , and chloroplast genomes ( RuBisCO , COI , ITS + 5.8S , SSU , 18S ... ) , have been designed and successfully used for diatoms identification with NGS .", "prompt_labels": "Several(O) DNA(O) markers(O) ,(O) belonging(O) to(O) the(O) nuclear(O) ,(O) mitochondrial(O) ,(O) and(O) chloroplast(O) genomes(O) ((O) RuBisCO(B-enzyme) ,(O) COI(O) ,(O) ITS(O) +(O) 5.8S(O) ,(O) SSU(O) ,(O) 18S(O) ...(O) )(O) ,(O) have(O) been(O) designed(O) and(O) successfully(O) used(O) for(O) diatoms(O) identification(O) with(O) NGS(O) .(O)"}}
{"id": "95", "dataset": "crossner_science", "split": "test", "label_list": ["chemical compound", "organization", "scientist", "university", "chemical element", "discipline", "astronomical object", "event", "protein", "theory", "location", "person", "country", "award", "enzyme", "academic journal"], "instance": {"id": "95", "words": ["Reverse", "transcriptase", "protocols", "at", "the", "time", "had", "difficulties", "with", "the", "secondary", "structure", "of", "mRNA", ",", "leading", "to", "abbreviated", "cDNAs", "that", "were", "difficult", "to", "align", "and", "invited", "further", "complications", "in", "downstream", "analysis", "."], "labels": ["B-enzyme", "I-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, organization, scientist, university, chemical element, discipline, astronomical object, event, protein, theory, location, person, country, award, enzyme, academic journal and O.\nSentence: Reverse transcriptase protocols at the time had difficulties with the secondary structure of mRNA , leading to abbreviated cDNAs that were difficult to align and invited further complications in downstream analysis .", "prompt_labels": "Reverse(B-enzyme) transcriptase(I-enzyme) protocols(O) at(O) the(O) time(O) had(O) difficulties(O) with(O) the(O) secondary(O) structure(O) of(O) mRNA(O) ,(O) leading(O) to(O) abbreviated(O) cDNAs(O) that(O) were(O) difficult(O) to(O) align(O) and(O) invited(O) further(O) complications(O) in(O) downstream(O) analysis(O) .(O)"}}
{"id": "272", "dataset": "crossner_science", "split": "test", "label_list": ["scientist", "theory", "chemical compound", "enzyme", "protein", "country", "award", "astronomical object", "person", "event", "university", "organization", "academic journal", "chemical element", "discipline", "location"], "instance": {"id": "272", "words": ["The", "introduction", "of", "the", "Others", "featured", "Tom", ",", "a.k.a.", "Mr.", "Friendly", "(", "M.", "C.", "Gainey", ")", ",", "and", "Ethan", "Rom", "(", "William", "Mapother", ")", ",", "all", "of", "whom", "have", "been", "shown", "in", "both", "flashbacks", "and", "the", "ongoing", "story", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, theory, chemical compound, enzyme, protein, country, award, astronomical object, person, event, university, organization, academic journal, chemical element, discipline, location and O.\nSentence: The introduction of the Others featured Tom , a.k.a. Mr. Friendly ( M. C. Gainey ) , and Ethan Rom ( William Mapother ) , all of whom have been shown in both flashbacks and the ongoing story .", "prompt_labels": "The(O) introduction(O) of(O) the(O) Others(O) featured(O) Tom(B-person) ,(O) a.k.a.(O) Mr.(O) Friendly(O) ((O) M.(B-person) C.(I-person) Gainey(I-person) )(O) ,(O) and(O) Ethan(B-person) Rom(I-person) ((O) William(B-person) Mapother(I-person) )(O) ,(O) all(O) of(O) whom(O) have(O) been(O) shown(O) in(O) both(O) flashbacks(O) and(O) the(O) ongoing(O) story(O) .(O)"}}
{"id": "6", "dataset": "crossner_science", "split": "test", "label_list": ["scientist", "organization", "chemical compound", "person", "award", "university", "country", "protein", "enzyme", "astronomical object", "event", "theory", "chemical element", "academic journal", "location", "discipline"], "instance": {"id": "6", "words": ["In", "February", "2016", ",", "he", "was", "one", "of", "the", "four", "scientists", "of", "LIGO", "Scientific", "Collaboration", "/", "Virgo", "interferometer", "collaboration", "presenting", "at", "the", "press", "conference", "for", "the", "announcement", "that", "the", "first", "direct", "gravitational", "wave", "observation", "had", "been", "made", "in", "September", "2015", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, organization, chemical compound, person, award, university, country, protein, enzyme, astronomical object, event, theory, chemical element, academic journal, location, discipline and O.\nSentence: In February 2016 , he was one of the four scientists of LIGO Scientific Collaboration / Virgo interferometer collaboration presenting at the press conference for the announcement that the first direct gravitational wave observation had been made in September 2015 .", "prompt_labels": "In(O) February(O) 2016(O) ,(O) he(O) was(O) one(O) of(O) the(O) four(O) scientists(O) of(O) LIGO(B-organization) Scientific(I-organization) Collaboration(I-organization) /(O) Virgo(B-organization) interferometer(I-organization) collaboration(O) presenting(O) at(O) the(O) press(O) conference(O) for(O) the(O) announcement(O) that(O) the(O) first(B-event) direct(I-event) gravitational(I-event) wave(I-event) observation(I-event) had(O) been(O) made(O) in(O) September(O) 2015(O) .(O)"}}
{"id": "444", "dataset": "crossner_science", "split": "test", "label_list": ["event", "chemical compound", "scientist", "academic journal", "enzyme", "protein", "chemical element", "astronomical object", "discipline", "country", "person", "university", "organization", "location", "theory", "award"], "instance": {"id": "444", "words": ["He", "was", "elected", "President", "of", "the", "International", "Council", "for", "Science", "in", "2008", ",", "to", "start", "his", "term", "in", "2011", "..", "International", "Council", "for", "Science", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, chemical compound, scientist, academic journal, enzyme, protein, chemical element, astronomical object, discipline, country, person, university, organization, location, theory, award and O.\nSentence: He was elected President of the International Council for Science in 2008 , to start his term in 2011 .. International Council for Science .", "prompt_labels": "He(O) was(O) elected(O) President(O) of(O) the(O) International(B-organization) Council(I-organization) for(I-organization) Science(I-organization) in(O) 2008(O) ,(O) to(O) start(O) his(O) term(O) in(O) 2011(O) ..(O) International(B-organization) Council(I-organization) for(I-organization) Science(I-organization) .(O)"}}
{"id": "485", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "location", "university", "scientist", "award", "person", "chemical compound", "astronomical object", "event", "discipline", "theory", "academic journal", "country", "enzyme", "protein", "organization"], "instance": {"id": "485", "words": ["Starling", "'s", "successor", ",", "Archibald", "Hill", ",", "fostered", "the", "career", "of", "Bernard", "Katz", ",", "whose", "long", "association", "with", "UCL", "began", "in", "1935", "."], "labels": ["B-scientist", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "B-university", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, location, university, scientist, award, person, chemical compound, astronomical object, event, discipline, theory, academic journal, country, enzyme, protein, organization and O.\nSentence: Starling 's successor , Archibald Hill , fostered the career of Bernard Katz , whose long association with UCL began in 1935 .", "prompt_labels": "Starling(B-scientist) 's(O) successor(O) ,(O) Archibald(B-scientist) Hill(I-scientist) ,(O) fostered(O) the(O) career(O) of(O) Bernard(B-scientist) Katz(I-scientist) ,(O) whose(O) long(O) association(O) with(O) UCL(B-university) began(O) in(O) 1935(O) .(O)"}}
{"id": "191", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "organization", "event", "university", "chemical element", "location", "country", "enzyme", "protein", "scientist", "discipline", "academic journal", "award", "theory", "chemical compound", "person"], "instance": {"id": "191", "words": ["Comparative", "table", "of", "countries", "with", "very", "high", "human", "development", "(", "same", "or", "higher", "than", "0.8", ")", ",", "according", "to", "UNDP", ";", "OECD", "members", ";", "advanced", "economies", ",", "according", "to", "the", "International", "Monetary", "Fund", ";", "high", "income", "economies", ",", "according", "to", "the", "World", "Bank", "and", "income", "per", "capita", "(", "purchasing", "power", "parity", ")", "higher", "than", "$", "22,000", ",", "according", "to", "the", "International", "Monetary", "Fund", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, organization, event, university, chemical element, location, country, enzyme, protein, scientist, discipline, academic journal, award, theory, chemical compound, person and O.\nSentence: Comparative table of countries with very high human development ( same or higher than 0.8 ) , according to UNDP ; OECD members ; advanced economies , according to the International Monetary Fund ; high income economies , according to the World Bank and income per capita ( purchasing power parity ) higher than $ 22,000 , according to the International Monetary Fund .", "prompt_labels": "Comparative(O) table(O) of(O) countries(O) with(O) very(O) high(O) human(O) development(O) ((O) same(O) or(O) higher(O) than(O) 0.8(O) )(O) ,(O) according(O) to(O) UNDP(B-organization) ;(O) OECD(B-organization) members(O) ;(O) advanced(O) economies(O) ,(O) according(O) to(O) the(O) International(B-organization) Monetary(I-organization) Fund(I-organization) ;(O) high(O) income(O) economies(O) ,(O) according(O) to(O) the(O) World(B-organization) Bank(I-organization) and(O) income(O) per(O) capita(O) ((O) purchasing(O) power(O) parity(O) )(O) higher(O) than(O) $(O) 22,000(O) ,(O) according(O) to(O) the(O) International(B-organization) Monetary(I-organization) Fund(I-organization) .(O)"}}
{"id": "511", "dataset": "crossner_science", "split": "test", "label_list": ["event", "chemical compound", "person", "enzyme", "university", "astronomical object", "academic journal", "location", "chemical element", "award", "scientist", "discipline", "organization", "theory", "protein", "country"], "instance": {"id": "511", "words": ["In", "1972", ",", "LaHaye", "helped", "establish", "the", "Institute", "for", "Creation", "Research", "at", "San", "Diego", "Christian", "College", "in", "El", "Cajon", ",", "California", ",", "along", "with", "Henry", "M.", "Morris", "."], "labels": ["O", "O", "O", "B-person", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-university", "I-university", "I-university", "I-university", "O", "B-location", "I-location", "O", "B-location", "O", "O", "O", "B-person", "I-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, chemical compound, person, enzyme, university, astronomical object, academic journal, location, chemical element, award, scientist, discipline, organization, theory, protein, country and O.\nSentence: In 1972 , LaHaye helped establish the Institute for Creation Research at San Diego Christian College in El Cajon , California , along with Henry M. Morris .", "prompt_labels": "In(O) 1972(O) ,(O) LaHaye(B-person) helped(O) establish(O) the(O) Institute(B-organization) for(I-organization) Creation(I-organization) Research(I-organization) at(O) San(B-university) Diego(I-university) Christian(I-university) College(I-university) in(O) El(B-location) Cajon(I-location) ,(O) California(B-location) ,(O) along(O) with(O) Henry(B-person) M.(I-person) Morris(I-person) .(O)"}}
{"id": "218", "dataset": "crossner_science", "split": "test", "label_list": ["scientist", "award", "location", "academic journal", "discipline", "protein", "person", "organization", "theory", "astronomical object", "university", "chemical element", "country", "chemical compound", "event", "enzyme"], "instance": {"id": "218", "words": ["2060", "Chiron", "is", "a", "small", "Solar", "System", "body", "in", "the", "outer", "Solar", "System", ",", "orbit", "ing", "the", "Sun", "between", "Saturn", "and", "Uranus", "."], "labels": ["B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, award, location, academic journal, discipline, protein, person, organization, theory, astronomical object, university, chemical element, country, chemical compound, event, enzyme and O.\nSentence: 2060 Chiron is a small Solar System body in the outer Solar System , orbit ing the Sun between Saturn and Uranus .", "prompt_labels": "2060(B-astronomical object) Chiron(I-astronomical object) is(O) a(O) small(O) Solar(O) System(O) body(O) in(O) the(O) outer(O) Solar(O) System(O) ,(O) orbit(O) ing(O) the(O) Sun(B-astronomical object) between(O) Saturn(B-astronomical object) and(O) Uranus(B-astronomical object) .(O)"}}
{"id": "344", "dataset": "crossner_science", "split": "test", "label_list": ["university", "person", "theory", "chemical element", "chemical compound", "award", "organization", "location", "scientist", "enzyme", "event", "protein", "academic journal", "astronomical object", "discipline", "country"], "instance": {"id": "344", "words": ["Dyson", "presented", "his", "observations", "of", "the", "solar", "eclipse", "of", "May", "29", ",", "1919", "to", "a", "joint", "meeting", "of", "the", "Royal", "Society", "and", "Royal", "Astronomical", "Society", "on", "6", "November", "1919", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, person, theory, chemical element, chemical compound, award, organization, location, scientist, enzyme, event, protein, academic journal, astronomical object, discipline, country and O.\nSentence: Dyson presented his observations of the solar eclipse of May 29 , 1919 to a joint meeting of the Royal Society and Royal Astronomical Society on 6 November 1919 .", "prompt_labels": "Dyson(B-scientist) presented(O) his(O) observations(O) of(O) the(O) solar(B-event) eclipse(I-event) of(I-event) May(I-event) 29(I-event) ,(I-event) 1919(I-event) to(O) a(O) joint(O) meeting(O) of(O) the(O) Royal(B-organization) Society(I-organization) and(O) Royal(B-organization) Astronomical(I-organization) Society(I-organization) on(O) 6(O) November(O) 1919(O) .(O)"}}
{"id": "208", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "university", "country", "enzyme", "discipline", "protein", "astronomical object", "scientist", "theory", "event", "chemical element", "location", "chemical compound", "person", "organization", "award"], "instance": {"id": "208", "words": ["Flügge", "also", "extended", "Niels", "Bohr", "and", "John", "Archibald", "Wheeler", "theory", "of", "nuclear", "fission", "published", "in", "1939", ".", "Niels", "Bohr", "and", "J.", "A.", "Wheeler", "Mechanism", "of", "nuclear", "fission", ",", "Phys", "."], "labels": ["B-scientist", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "B-theory", "I-theory", "I-theory", "I-theory", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "O", "B-discipline", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, university, country, enzyme, discipline, protein, astronomical object, scientist, theory, event, chemical element, location, chemical compound, person, organization, award and O.\nSentence: Flügge also extended Niels Bohr and John Archibald Wheeler theory of nuclear fission published in 1939 . Niels Bohr and J. A. Wheeler Mechanism of nuclear fission , Phys .", "prompt_labels": "Flügge(B-scientist) also(O) extended(O) Niels(B-scientist) Bohr(I-scientist) and(O) John(B-scientist) Archibald(I-scientist) Wheeler(I-scientist) theory(B-theory) of(I-theory) nuclear(I-theory) fission(I-theory) published(O) in(O) 1939(O) .(O) Niels(B-scientist) Bohr(I-scientist) and(O) J.(B-scientist) A.(I-scientist) Wheeler(I-scientist) Mechanism(O) of(O) nuclear(O) fission(O) ,(O) Phys(B-discipline) .(O)"}}
{"id": "85", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "astronomical object", "theory", "organization", "scientist", "university", "country", "enzyme", "person", "location", "protein", "award", "discipline", "chemical compound", "event", "chemical element"], "instance": {"id": "85", "words": ["Binding", "of", "these", "proteins", "recruit", "Histone", "deacetylase", "enzyme", "which", "initiate", "chromatin", "remodeling", "such", "that", "the", "DNA", "becoming", "less", "accessible", "to", "transcriptional", "machinery", ",", "such", "as", "RNA", "polymerase", ",", "effectively", "repressing", "gene", "expression", "."], "labels": ["O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "I-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, astronomical object, theory, organization, scientist, university, country, enzyme, person, location, protein, award, discipline, chemical compound, event, chemical element and O.\nSentence: Binding of these proteins recruit Histone deacetylase enzyme which initiate chromatin remodeling such that the DNA becoming less accessible to transcriptional machinery , such as RNA polymerase , effectively repressing gene expression .", "prompt_labels": "Binding(O) of(O) these(O) proteins(O) recruit(O) Histone(B-enzyme) deacetylase(I-enzyme) enzyme(I-enzyme) which(O) initiate(O) chromatin(O) remodeling(O) such(O) that(O) the(O) DNA(O) becoming(O) less(O) accessible(O) to(O) transcriptional(O) machinery(O) ,(O) such(O) as(O) RNA(B-enzyme) polymerase(I-enzyme) ,(O) effectively(O) repressing(O) gene(O) expression(O) .(O)"}}
{"id": "129", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "chemical compound", "scientist", "award", "organization", "astronomical object", "theory", "protein", "university", "academic journal", "country", "person", "discipline", "chemical element", "location", "event"], "instance": {"id": "129", "words": ["CCR3", "is", "a", "receptor", "for", "multiple", "inflammatory", "/", "inducible", "CC", "chemokines", ",", "including", "CCL11", ",", "CCL26", ",", "CCL7", ",", "CCL13", ",", "CCL15", ",", "CCL24", "and", "CCL5", "that", "attract", "eosinophils", ",", "and", "CCL28", "that", "attracts", "B", "and", "T", "lymphocyte", "s", "to", "mucosal", "tissue", "s.ref", "name", "=", "Youn", "/", "It", "is", "most", "highly", "expressed", "in", "both", "eosinophils", "and", "basophils", ",", "but", "can", "also", "be", "found", "in", "Th1", "and", "Th2", "cells", "and", "airway", "epithelial", "cells", "."], "labels": ["B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "O", "O", "O", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical compound, scientist, award, organization, astronomical object, theory, protein, university, academic journal, country, person, discipline, chemical element, location, event and O.\nSentence: CCR3 is a receptor for multiple inflammatory / inducible CC chemokines , including CCL11 , CCL26 , CCL7 , CCL13 , CCL15 , CCL24 and CCL5 that attract eosinophils , and CCL28 that attracts B and T lymphocyte s to mucosal tissue s.ref name = Youn / It is most highly expressed in both eosinophils and basophils , but can also be found in Th1 and Th2 cells and airway epithelial cells .", "prompt_labels": "CCR3(B-protein) is(O) a(O) receptor(O) for(O) multiple(O) inflammatory(O) /(O) inducible(O) CC(B-protein) chemokines(I-protein) ,(O) including(O) CCL11(B-protein) ,(O) CCL26(B-protein) ,(O) CCL7(B-protein) ,(O) CCL13(B-protein) ,(O) CCL15(B-protein) ,(O) CCL24(B-protein) and(O) CCL5(B-protein) that(O) attract(O) eosinophils(O) ,(O) and(O) CCL28(B-protein) that(O) attracts(O) B(O) and(O) T(O) lymphocyte(O) s(O) to(O) mucosal(O) tissue(O) s.ref(O) name(O) =(O) Youn(O) /(O) It(O) is(O) most(O) highly(O) expressed(O) in(O) both(O) eosinophils(O) and(O) basophils(O) ,(O) but(O) can(O) also(O) be(O) found(O) in(O) Th1(O) and(O) Th2(O) cells(O) and(O) airway(O) epithelial(O) cells(O) .(O)"}}
{"id": "90", "dataset": "crossner_science", "split": "test", "label_list": ["location", "chemical element", "scientist", "discipline", "protein", "country", "university", "event", "chemical compound", "enzyme", "academic journal", "theory", "astronomical object", "award", "person", "organization"], "instance": {"id": "90", "words": ["50000", "Quaoar", "and", "90377", "Sedna", "are", "two", "Solar", "System", "objects", "discovered", "in", "this", "way", "by", "Michael", "E.", "Brown", "and", "others", "at", "Caltech", "using", "the", "Palomar", "Observatory", "'", "s", "Samuel", "Oschin", "telescope", "of", "and", "the", "Palomar-Quest", "large-area", "CCD", "camera", "."], "labels": ["B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "O", "B-university", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, chemical element, scientist, discipline, protein, country, university, event, chemical compound, enzyme, academic journal, theory, astronomical object, award, person, organization and O.\nSentence: 50000 Quaoar and 90377 Sedna are two Solar System objects discovered in this way by Michael E. Brown and others at Caltech using the Palomar Observatory ' s Samuel Oschin telescope of and the Palomar-Quest large-area CCD camera .", "prompt_labels": "50000(B-astronomical object) Quaoar(I-astronomical object) and(O) 90377(B-astronomical object) Sedna(I-astronomical object) are(O) two(O) Solar(O) System(O) objects(O) discovered(O) in(O) this(O) way(O) by(O) Michael(B-scientist) E.(I-scientist) Brown(I-scientist) and(O) others(O) at(O) Caltech(B-university) using(O) the(O) Palomar(B-location) Observatory(I-location) '(O) s(O) Samuel(O) Oschin(O) telescope(O) of(O) and(O) the(O) Palomar-Quest(O) large-area(O) CCD(O) camera(O) .(O)"}}
{"id": "114", "dataset": "crossner_science", "split": "test", "label_list": ["chemical compound", "person", "discipline", "award", "academic journal", "chemical element", "event", "theory", "scientist", "enzyme", "university", "organization", "protein", "astronomical object", "location", "country"], "instance": {"id": "114", "words": ["Computer", "programs", "that", "solve", "these", "problems", "have", "been", "used", "to", "research", "a", "broad", "range", "of", "scientific", "topics", "from", "Adenosine", "diphosphate", "to", "breast", "cancer", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, person, discipline, award, academic journal, chemical element, event, theory, scientist, enzyme, university, organization, protein, astronomical object, location, country and O.\nSentence: Computer programs that solve these problems have been used to research a broad range of scientific topics from Adenosine diphosphate to breast cancer .", "prompt_labels": "Computer(O) programs(O) that(O) solve(O) these(O) problems(O) have(O) been(O) used(O) to(O) research(O) a(O) broad(O) range(O) of(O) scientific(O) topics(O) from(O) Adenosine(B-chemical compound) diphosphate(I-chemical compound) to(O) breast(O) cancer(O) .(O)"}}
{"id": "362", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "award", "theory", "enzyme", "scientist", "organization", "chemical compound", "astronomical object", "university", "discipline", "chemical element", "person", "event", "protein", "country", "location"], "instance": {"id": "362", "words": ["Under", "typical", "dark", "sky", "conditions", "Uranus", "(", "magnitude", "+", "5.8", ")", "can", "be", "seen", "as", "well", "with", "averted", "vision", ",", "as", "can", "the", "asteroid", "4", "Vesta", "at", "its", "brighter", "oppositions", "."], "labels": ["O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, award, theory, enzyme, scientist, organization, chemical compound, astronomical object, university, discipline, chemical element, person, event, protein, country, location and O.\nSentence: Under typical dark sky conditions Uranus ( magnitude + 5.8 ) can be seen as well with averted vision , as can the asteroid 4 Vesta at its brighter oppositions .", "prompt_labels": "Under(O) typical(O) dark(O) sky(O) conditions(O) Uranus(B-astronomical object) ((O) magnitude(O) +(O) 5.8(O) )(O) can(O) be(O) seen(O) as(O) well(O) with(O) averted(O) vision(O) ,(O) as(O) can(O) the(O) asteroid(O) 4(B-astronomical object) Vesta(I-astronomical object) at(O) its(O) brighter(O) oppositions(O) .(O)"}}
{"id": "414", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "organization", "astronomical object", "university", "theory", "award", "event", "scientist", "discipline", "chemical compound", "location", "chemical element", "academic journal", "person", "country", "protein"], "instance": {"id": "414", "words": ["Szilard", "initially", "attended", "Budapest", "University", "of", "Technology", "and", "Economics", "in", "Budapest", ",", "but", "his", "engineering", "studies", "were", "interrupted", "by", "service", "in", "the", "Austro-Hungarian", "Army", "during", "World", "War", "I.", "He", "left", "Hungary", "for", "Germany", "in", "1919", ",", "enrolling", "at", "Technical", "University", "of", "Berlin", ",", "but", "became", "bored", "with", "engineering", "and", "transferred", "to", "Humboldt", "University", "of", "Berlin", ",", "where", "he", "studied", "physics", "."], "labels": ["B-scientist", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "I-university", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-event", "I-event", "I-event", "O", "O", "B-country", "O", "B-country", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "O", "O", "O", "B-discipline", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, organization, astronomical object, university, theory, award, event, scientist, discipline, chemical compound, location, chemical element, academic journal, person, country, protein and O.\nSentence: Szilard initially attended Budapest University of Technology and Economics in Budapest , but his engineering studies were interrupted by service in the Austro-Hungarian Army during World War I. He left Hungary for Germany in 1919 , enrolling at Technical University of Berlin , but became bored with engineering and transferred to Humboldt University of Berlin , where he studied physics .", "prompt_labels": "Szilard(B-scientist) initially(O) attended(O) Budapest(B-university) University(I-university) of(I-university) Technology(I-university) and(I-university) Economics(I-university) in(O) Budapest(B-location) ,(O) but(O) his(O) engineering(O) studies(O) were(O) interrupted(O) by(O) service(O) in(O) the(O) Austro-Hungarian(B-organization) Army(I-organization) during(O) World(B-event) War(I-event) I.(I-event) He(O) left(O) Hungary(B-country) for(O) Germany(B-country) in(O) 1919(O) ,(O) enrolling(O) at(O) Technical(B-university) University(I-university) of(I-university) Berlin(I-university) ,(O) but(O) became(O) bored(O) with(O) engineering(O) and(O) transferred(O) to(O) Humboldt(B-university) University(I-university) of(I-university) Berlin(I-university) ,(O) where(O) he(O) studied(O) physics(B-discipline) .(O)"}}
{"id": "248", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "organization", "location", "award", "university", "theory", "astronomical object", "event", "chemical element", "enzyme", "scientist", "person", "discipline", "protein", "country", "chemical compound"], "instance": {"id": "248", "words": ["This", "culminates", "in", "the", "translocation", "of", "the", "NF-κB", "transcription", "factor", "Relish", ",", "leading", "to", "production", "of", "antimicrobial", "peptides", "and", "other", "effectors", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, organization, location, award, university, theory, astronomical object, event, chemical element, enzyme, scientist, person, discipline, protein, country, chemical compound and O.\nSentence: This culminates in the translocation of the NF-κB transcription factor Relish , leading to production of antimicrobial peptides and other effectors .", "prompt_labels": "This(O) culminates(O) in(O) the(O) translocation(O) of(O) the(O) NF-κB(B-protein) transcription(O) factor(O) Relish(O) ,(O) leading(O) to(O) production(O) of(O) antimicrobial(O) peptides(O) and(O) other(O) effectors(O) .(O)"}}
{"id": "32", "dataset": "crossner_science", "split": "test", "label_list": ["discipline", "academic journal", "chemical compound", "event", "chemical element", "university", "astronomical object", "enzyme", "theory", "protein", "location", "scientist", "award", "country", "organization", "person"], "instance": {"id": "32", "words": ["He", "is", "the", "recipient", "of", "a", "Lifetime", "Achievement", "Award", "from", "the", "The", "Recording", "Academy", "and", "a", "Lifetime", "Achievement", "Award", "from", "the", "Rhythm", "and", "Blues", "Foundation", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, academic journal, chemical compound, event, chemical element, university, astronomical object, enzyme, theory, protein, location, scientist, award, country, organization, person and O.\nSentence: He is the recipient of a Lifetime Achievement Award from the The Recording Academy and a Lifetime Achievement Award from the Rhythm and Blues Foundation .", "prompt_labels": "He(O) is(O) the(O) recipient(O) of(O) a(O) Lifetime(B-award) Achievement(I-award) Award(I-award) from(I-award) the(I-award) The(I-award) Recording(I-award) Academy(I-award) and(O) a(O) Lifetime(B-award) Achievement(I-award) Award(I-award) from(I-award) the(I-award) Rhythm(I-award) and(I-award) Blues(I-award) Foundation(I-award) .(O)"}}
{"id": "124", "dataset": "crossner_science", "split": "test", "label_list": ["organization", "protein", "theory", "chemical element", "country", "enzyme", "astronomical object", "event", "discipline", "scientist", "location", "chemical compound", "person", "university", "award", "academic journal"], "instance": {"id": "124", "words": ["Recognition", "of", "Kilby", "'s", "outstanding", "achievements", "have", "been", "made", "by", "the", "Institute", "of", "Electrical", "and", "Electronics", "Engineers", "(", "IEEE", ")", ",", "including", "the", "election", "to", "IEEE", "Fellow", "in", "1966", ",", "the", "IEEE", "David", "Sarnoff", "Award", "in", "1966", ",", "was", "created", "in", "1995", "."], "labels": ["O", "O", "B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, protein, theory, chemical element, country, enzyme, astronomical object, event, discipline, scientist, location, chemical compound, person, university, award, academic journal and O.\nSentence: Recognition of Kilby 's outstanding achievements have been made by the Institute of Electrical and Electronics Engineers ( IEEE ) , including the election to IEEE Fellow in 1966 , the IEEE David Sarnoff Award in 1966 , was created in 1995 .", "prompt_labels": "Recognition(O) of(O) Kilby(B-scientist) 's(O) outstanding(O) achievements(O) have(O) been(O) made(O) by(O) the(O) Institute(B-organization) of(I-organization) Electrical(I-organization) and(I-organization) Electronics(I-organization) Engineers(I-organization) ((O) IEEE(B-organization) )(O) ,(O) including(O) the(O) election(O) to(O) IEEE(B-award) Fellow(I-award) in(O) 1966(O) ,(O) the(O) IEEE(B-award) David(I-award) Sarnoff(I-award) Award(I-award) in(O) 1966(O) ,(O) was(O) created(O) in(O) 1995(O) .(O)"}}
{"id": "471", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "astronomical object", "protein", "scientist", "organization", "university", "theory", "person", "chemical compound", "event", "country", "academic journal", "award", "discipline", "location", "enzyme"], "instance": {"id": "471", "words": ["Oxidants", "are", "usually", "chemical", "substances", "with", "elements", "in", "high", "oxidation", "states", ",", "or", "else", "highly", "electronegative", "elements", "(", "Osub2", "/", "sub", ",", "Fluorine", ",", "Chlorine", ",", "Bromine", ")", "that", "can", "gain", "extra", "electrons", "by", "oxidizing", "another", "substance", "."], "labels": ["B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, astronomical object, protein, scientist, organization, university, theory, person, chemical compound, event, country, academic journal, award, discipline, location, enzyme and O.\nSentence: Oxidants are usually chemical substances with elements in high oxidation states , or else highly electronegative elements ( Osub2 / sub , Fluorine , Chlorine , Bromine ) that can gain extra electrons by oxidizing another substance .", "prompt_labels": "Oxidants(B-chemical compound) are(O) usually(O) chemical(O) substances(O) with(O) elements(O) in(O) high(O) oxidation(O) states(O) ,(O) or(O) else(O) highly(O) electronegative(O) elements(O) ((O) Osub2(B-chemical compound) /(I-chemical compound) sub(I-chemical compound) ,(O) Fluorine(B-chemical element) ,(O) Chlorine(B-chemical element) ,(O) Bromine(B-chemical element) )(O) that(O) can(O) gain(O) extra(O) electrons(O) by(O) oxidizing(O) another(O) substance(O) .(O)"}}
{"id": "19", "dataset": "crossner_science", "split": "test", "label_list": ["person", "chemical compound", "organization", "location", "protein", "theory", "country", "academic journal", "scientist", "chemical element", "award", "event", "university", "enzyme", "astronomical object", "discipline"], "instance": {"id": "19", "words": ["It", "displays", "the", "mean", "time", ",", "sidereal", ",", "(", "or", "star", ")", ",", "time", "and", "the", "motions", "of", "the", "sun", ",", "moon", "and", "the", "five", "then-known", "planets", "Venus", ",", "Mars", ",", "Saturn", ",", "Mercury", ",", "and", "Jupiter", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, chemical compound, organization, location, protein, theory, country, academic journal, scientist, chemical element, award, event, university, enzyme, astronomical object, discipline and O.\nSentence: It displays the mean time , sidereal , ( or star ) , time and the motions of the sun , moon and the five then-known planets Venus , Mars , Saturn , Mercury , and Jupiter .", "prompt_labels": "It(O) displays(O) the(O) mean(O) time(O) ,(O) sidereal(O) ,(O) ((O) or(O) star(O) )(O) ,(O) time(O) and(O) the(O) motions(O) of(O) the(O) sun(B-astronomical object) ,(O) moon(B-astronomical object) and(O) the(O) five(O) then-known(O) planets(O) Venus(B-astronomical object) ,(O) Mars(B-astronomical object) ,(O) Saturn(B-astronomical object) ,(O) Mercury(B-astronomical object) ,(O) and(O) Jupiter(B-astronomical object) .(O)"}}
{"id": "185", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "astronomical object", "academic journal", "chemical compound", "event", "discipline", "chemical element", "scientist", "country", "theory", "university", "organization", "location", "enzyme", "person", "award"], "instance": {"id": "185", "words": ["Esaki", "moved", "back", "to", "Japan", "in", "1992", ",", "subsequently", ",", "he", "served", "as", "president", "of", "the", "University", "of", "Tsukuba", "and", "Shibaura", "Institute", "of", "Technology", "."], "labels": ["B-scientist", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "B-university", "I-university", "I-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, astronomical object, academic journal, chemical compound, event, discipline, chemical element, scientist, country, theory, university, organization, location, enzyme, person, award and O.\nSentence: Esaki moved back to Japan in 1992 , subsequently , he served as president of the University of Tsukuba and Shibaura Institute of Technology .", "prompt_labels": "Esaki(B-scientist) moved(O) back(O) to(O) Japan(B-country) in(O) 1992(O) ,(O) subsequently(O) ,(O) he(O) served(O) as(O) president(O) of(O) the(O) University(B-university) of(I-university) Tsukuba(I-university) and(O) Shibaura(B-university) Institute(I-university) of(I-university) Technology(I-university) .(O)"}}
{"id": "211", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "person", "award", "organization", "academic journal", "university", "scientist", "astronomical object", "location", "discipline", "chemical compound", "event", "theory", "country", "enzyme", "protein"], "instance": {"id": "211", "words": ["Employing", "a", "starting", "feedstock", "mixture", "of", "1-Butene", "and", "2-Butene", ",", "which", "is", "known", "as", "Raffinate-2", ",", "produces", "phenol", "and", "a", "mixture", "of", "acetone", "and", "butanone", "instead", "of", "only", "phenol", "and", "acetone", "in", "the", "original", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, person, award, organization, academic journal, university, scientist, astronomical object, location, discipline, chemical compound, event, theory, country, enzyme, protein and O.\nSentence: Employing a starting feedstock mixture of 1-Butene and 2-Butene , which is known as Raffinate-2 , produces phenol and a mixture of acetone and butanone instead of only phenol and acetone in the original .", "prompt_labels": "Employing(O) a(O) starting(O) feedstock(O) mixture(O) of(O) 1-Butene(B-chemical compound) and(O) 2-Butene(B-chemical compound) ,(O) which(O) is(O) known(O) as(O) Raffinate-2(O) ,(O) produces(O) phenol(B-chemical compound) and(O) a(O) mixture(O) of(O) acetone(B-chemical compound) and(O) butanone(B-chemical compound) instead(O) of(O) only(O) phenol(B-chemical compound) and(O) acetone(B-chemical compound) in(O) the(O) original(O) .(O)"}}
{"id": "443", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "organization", "event", "award", "person", "location", "university", "chemical element", "discipline", "protein", "country", "enzyme", "academic journal", "theory", "scientist", "chemical compound"], "instance": {"id": "443", "words": ["There", "are", "four", "community", "colleges", "in", "the", "valley", ";", "Los", "Angeles", "Valley", "College", "in", "Valley", "Glen", ",", "Los", "Angeles", "Mission", "College", "in", "Sylmar", ",", "Los", "Angeles", "Pierce", "College", "in", "Woodland", "Hills", "and", "Glendale", "Community", "College", "in", "the", "College", "Hills", "neighborhood", "of", "Glendale", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "B-location", "I-location", "O", "B-university", "I-university", "I-university", "I-university", "O", "B-location", "O", "B-university", "I-university", "I-university", "I-university", "O", "B-location", "I-location", "O", "B-university", "I-university", "I-university", "O", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, organization, event, award, person, location, university, chemical element, discipline, protein, country, enzyme, academic journal, theory, scientist, chemical compound and O.\nSentence: There are four community colleges in the valley ; Los Angeles Valley College in Valley Glen , Los Angeles Mission College in Sylmar , Los Angeles Pierce College in Woodland Hills and Glendale Community College in the College Hills neighborhood of Glendale .", "prompt_labels": "There(O) are(O) four(O) community(O) colleges(O) in(O) the(O) valley(O) ;(O) Los(B-university) Angeles(I-university) Valley(I-university) College(I-university) in(O) Valley(B-location) Glen(I-location) ,(O) Los(B-university) Angeles(I-university) Mission(I-university) College(I-university) in(O) Sylmar(B-location) ,(O) Los(B-university) Angeles(I-university) Pierce(I-university) College(I-university) in(O) Woodland(B-location) Hills(I-location) and(O) Glendale(B-university) Community(I-university) College(I-university) in(O) the(O) College(B-location) Hills(I-location) neighborhood(I-location) of(I-location) Glendale(I-location) .(O)"}}
{"id": "207", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "location", "chemical compound", "enzyme", "university", "discipline", "chemical element", "theory", "scientist", "person", "protein", "country", "event", "organization", "astronomical object", "award"], "instance": {"id": "207", "words": ["Bangladesh", "is", "a", "member", "of", "the", "UN", ",", "World", "Trade", "Organization", ",", "International", "Monetary", "Fund", ",", "the", "World", "Bank", ",", "Asian", "Development", "Bank", ",", "OIC", ",", "Islamic", "Development", "Bank", ",", "SAARC", ",", "BIMSTEC", "and", "the", "Islamic", "Military", "Counter", "Terrorism", "Coalition", "."], "labels": ["B-country", "O", "O", "O", "O", "O", "B-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "B-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, location, chemical compound, enzyme, university, discipline, chemical element, theory, scientist, person, protein, country, event, organization, astronomical object, award and O.\nSentence: Bangladesh is a member of the UN , World Trade Organization , International Monetary Fund , the World Bank , Asian Development Bank , OIC , Islamic Development Bank , SAARC , BIMSTEC and the Islamic Military Counter Terrorism Coalition .", "prompt_labels": "Bangladesh(B-country) is(O) a(O) member(O) of(O) the(O) UN(B-organization) ,(O) World(B-organization) Trade(I-organization) Organization(I-organization) ,(O) International(B-organization) Monetary(I-organization) Fund(I-organization) ,(O) the(O) World(B-organization) Bank(I-organization) ,(O) Asian(B-organization) Development(I-organization) Bank(I-organization) ,(O) OIC(B-organization) ,(O) Islamic(B-organization) Development(I-organization) Bank(I-organization) ,(O) SAARC(B-organization) ,(O) BIMSTEC(B-organization) and(O) the(O) Islamic(B-organization) Military(I-organization) Counter(I-organization) Terrorism(I-organization) Coalition(I-organization) .(O)"}}
{"id": "60", "dataset": "crossner_science", "split": "test", "label_list": ["location", "person", "protein", "country", "discipline", "enzyme", "chemical element", "organization", "scientist", "university", "event", "chemical compound", "academic journal", "astronomical object", "award", "theory"], "instance": {"id": "60", "words": ["In", "2007", "the", "British", "Biochemical", "Society", "was", "given", "a", "grant", "by", "the", "Wellcome", "Trust", "to", "catalogue", "and", "preserve", "the", "35", "laboratory", "notebooks", "in", "which", "Sanger", "recorded", "his", "research", "from", "1944", "to", "1983", "."], "labels": ["O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, protein, country, discipline, enzyme, chemical element, organization, scientist, university, event, chemical compound, academic journal, astronomical object, award, theory and O.\nSentence: In 2007 the British Biochemical Society was given a grant by the Wellcome Trust to catalogue and preserve the 35 laboratory notebooks in which Sanger recorded his research from 1944 to 1983 .", "prompt_labels": "In(O) 2007(O) the(O) British(B-organization) Biochemical(I-organization) Society(I-organization) was(O) given(O) a(O) grant(O) by(O) the(O) Wellcome(B-organization) Trust(I-organization) to(O) catalogue(O) and(O) preserve(O) the(O) 35(O) laboratory(O) notebooks(O) in(O) which(O) Sanger(B-scientist) recorded(O) his(O) research(O) from(O) 1944(O) to(O) 1983(O) .(O)"}}
{"id": "35", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "chemical element", "organization", "theory", "astronomical object", "enzyme", "discipline", "location", "chemical compound", "protein", "scientist", "award", "university", "country", "person", "event"], "instance": {"id": "35", "words": ["The", "specific", "ligand", "of", "this", "receptor", "is", "CCL25", "To", "note", ",", "the", "chemokine", "binding", "protein", "D6", "had", "previously", "been", "named", "CCR9", ",", "but", "this", "molecule", "is", "a", "scavenger", "receptor", "not", "a", "TRUE", "(", "signaling", ")", "chemokine", "receptor", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O", "O", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, chemical element, organization, theory, astronomical object, enzyme, discipline, location, chemical compound, protein, scientist, award, university, country, person, event and O.\nSentence: The specific ligand of this receptor is CCL25 To note , the chemokine binding protein D6 had previously been named CCR9 , but this molecule is a scavenger receptor not a TRUE ( signaling ) chemokine receptor .", "prompt_labels": "The(O) specific(O) ligand(O) of(O) this(O) receptor(O) is(O) CCL25(B-protein) To(O) note(O) ,(O) the(O) chemokine(O) binding(O) protein(O) D6(B-protein) had(O) previously(O) been(O) named(O) CCR9(B-protein) ,(O) but(O) this(O) molecule(O) is(O) a(O) scavenger(O) receptor(O) not(O) a(O) TRUE(O) ((O) signaling(O) )(O) chemokine(O) receptor(O) .(O)"}}
{"id": "56", "dataset": "crossner_science", "split": "test", "label_list": ["organization", "protein", "academic journal", "theory", "university", "award", "chemical compound", "discipline", "astronomical object", "chemical element", "enzyme", "location", "country", "scientist", "event", "person"], "instance": {"id": "56", "words": ["Lexell", "calculated", "its", "orbit", ",", "showed", "that", "the", "comet", "had", "had", "a", "much", "larger", "perihelion", "before", "the", "encounter", "with", "Jupiter", "in", "1767", "and", "predicted", "that", "after", "encountering", "Jupiter", "again", "in", "1779", "it", "would", "be", "altogether", "expelled", "from", "the", "inner", "Solar", "System", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, protein, academic journal, theory, university, award, chemical compound, discipline, astronomical object, chemical element, enzyme, location, country, scientist, event, person and O.\nSentence: Lexell calculated its orbit , showed that the comet had had a much larger perihelion before the encounter with Jupiter in 1767 and predicted that after encountering Jupiter again in 1779 it would be altogether expelled from the inner Solar System .", "prompt_labels": "Lexell(B-scientist) calculated(O) its(O) orbit(O) ,(O) showed(O) that(O) the(O) comet(O) had(O) had(O) a(O) much(O) larger(O) perihelion(O) before(O) the(O) encounter(O) with(O) Jupiter(B-astronomical object) in(O) 1767(O) and(O) predicted(O) that(O) after(O) encountering(O) Jupiter(B-astronomical object) again(O) in(O) 1779(O) it(O) would(O) be(O) altogether(O) expelled(O) from(O) the(O) inner(O) Solar(O) System(O) .(O)"}}
{"id": "11", "dataset": "crossner_science", "split": "test", "label_list": ["event", "scientist", "award", "chemical element", "chemical compound", "enzyme", "protein", "discipline", "academic journal", "organization", "astronomical object", "person", "university", "country", "theory", "location"], "instance": {"id": "11", "words": ["Khan", "'s", "research", "focus", "is", "on", "developing", "new", "inhibitors", "against", "multidrug", "resistant", "clinical", "strains", "with", "special", "interest", "on", "extended", "spectrum", "beta", "lactamases", "(", "ESBL", ")", "such", "as", "NDM-1", "and", "Beta-lactamase", ",", "using", "QSARR", "modeling", "and", "structure-based", "virtual", "screening", "methods", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "I-enzyme", "I-enzyme", "O", "B-enzyme", "O", "O", "O", "B-enzyme", "O", "B-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, scientist, award, chemical element, chemical compound, enzyme, protein, discipline, academic journal, organization, astronomical object, person, university, country, theory, location and O.\nSentence: Khan 's research focus is on developing new inhibitors against multidrug resistant clinical strains with special interest on extended spectrum beta lactamases ( ESBL ) such as NDM-1 and Beta-lactamase , using QSARR modeling and structure-based virtual screening methods .", "prompt_labels": "Khan(B-scientist) 's(O) research(O) focus(O) is(O) on(O) developing(O) new(O) inhibitors(O) against(O) multidrug(O) resistant(O) clinical(O) strains(O) with(O) special(O) interest(O) on(O) extended(B-enzyme) spectrum(I-enzyme) beta(I-enzyme) lactamases(I-enzyme) ((O) ESBL(B-enzyme) )(O) such(O) as(O) NDM-1(B-enzyme) and(O) Beta-lactamase(B-enzyme) ,(O) using(O) QSARR(O) modeling(O) and(O) structure-based(O) virtual(O) screening(O) methods(O) .(O)"}}
{"id": "261", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "country", "award", "enzyme", "academic journal", "chemical compound", "university", "location", "protein", "discipline", "organization", "astronomical object", "chemical element", "scientist", "event", "person"], "instance": {"id": "261", "words": ["In", "the", "Greek-influenced", "tradition", ",", "Neptune", "is", "the", "brother", "of", "Jupiter", "and", "Pluto", ";", "the", "brothers", "preside", "over", "the", "realms", "of", "Heaven", ",", "the", "earthly", "world", ",", "and", "the", "Underworld", ".About", "the", "relationship", "of", "the", "lord", "of", "our", "earthly", "world", "with", "water", "(", "s", ")", "Bloch", ",", "p", "."], "labels": ["O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "B-person", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, country, award, enzyme, academic journal, chemical compound, university, location, protein, discipline, organization, astronomical object, chemical element, scientist, event, person and O.\nSentence: In the Greek-influenced tradition , Neptune is the brother of Jupiter and Pluto ; the brothers preside over the realms of Heaven , the earthly world , and the Underworld .About the relationship of the lord of our earthly world with water ( s ) Bloch , p .", "prompt_labels": "In(O) the(O) Greek-influenced(O) tradition(O) ,(O) Neptune(B-person) is(O) the(O) brother(O) of(O) Jupiter(B-person) and(O) Pluto(B-person) ;(O) the(O) brothers(O) preside(O) over(O) the(O) realms(O) of(O) Heaven(O) ,(O) the(O) earthly(O) world(O) ,(O) and(O) the(O) Underworld(O) .About(O) the(O) relationship(O) of(O) the(O) lord(O) of(O) our(O) earthly(O) world(O) with(O) water(O) ((O) s(O) )(O) Bloch(B-person) ,(O) p(O) .(O)"}}
{"id": "193", "dataset": "crossner_science", "split": "test", "label_list": ["event", "location", "award", "person", "enzyme", "academic journal", "protein", "chemical compound", "country", "theory", "chemical element", "organization", "university", "discipline", "astronomical object", "scientist"], "instance": {"id": "193", "words": ["The", "alkaline", "earth", "metals", "are", "named", "after", "their", "oxide", "s", ",", "the", "alkaline", "earths", ",", "whose", "old-fashioned", "names", "were", "beryllia", ",", "Magnesium", "oxide", ",", "Calcium", "oxide", ",", "strontia", ",", "and", "baryta", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O", "O", "B-chemical compound", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, award, person, enzyme, academic journal, protein, chemical compound, country, theory, chemical element, organization, university, discipline, astronomical object, scientist and O.\nSentence: The alkaline earth metals are named after their oxide s , the alkaline earths , whose old-fashioned names were beryllia , Magnesium oxide , Calcium oxide , strontia , and baryta .", "prompt_labels": "The(O) alkaline(O) earth(O) metals(O) are(O) named(O) after(O) their(O) oxide(B-chemical compound) s(O) ,(O) the(O) alkaline(O) earths(O) ,(O) whose(O) old-fashioned(O) names(O) were(O) beryllia(B-chemical compound) ,(O) Magnesium(B-chemical compound) oxide(I-chemical compound) ,(O) Calcium(B-chemical compound) oxide(I-chemical compound) ,(O) strontia(B-chemical compound) ,(O) and(O) baryta(B-chemical compound) .(O)"}}
{"id": "164", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "organization", "scientist", "country", "enzyme", "discipline", "event", "protein", "astronomical object", "university", "person", "location", "award", "academic journal", "theory", "chemical compound"], "instance": {"id": "164", "words": ["It", "stars", "Vincent", "Gallo", "and", "Maria", "de", "Medeiros", ",", "and", "was", "directed", "by", "Spanish", "filmmaker", "and", "actress", "María", "Lidón", "(", "credited", "in", "the", "English", "version", "of", "the", "movie", "as", "Luna", ")", ",", "with", "screenplay", "by", "Spanish", "science", "fiction", "author", "Juan", "Miguel", "Aguilera", "."], "labels": ["O", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, organization, scientist, country, enzyme, discipline, event, protein, astronomical object, university, person, location, award, academic journal, theory, chemical compound and O.\nSentence: It stars Vincent Gallo and Maria de Medeiros , and was directed by Spanish filmmaker and actress María Lidón ( credited in the English version of the movie as Luna ) , with screenplay by Spanish science fiction author Juan Miguel Aguilera .", "prompt_labels": "It(O) stars(O) Vincent(B-person) Gallo(I-person) and(O) Maria(B-person) de(I-person) Medeiros(I-person) ,(O) and(O) was(O) directed(O) by(O) Spanish(O) filmmaker(O) and(O) actress(O) María(B-person) Lidón(I-person) ((O) credited(O) in(O) the(O) English(O) version(O) of(O) the(O) movie(O) as(O) Luna(O) )(O) ,(O) with(O) screenplay(O) by(O) Spanish(O) science(O) fiction(O) author(O) Juan(B-person) Miguel(I-person) Aguilera(I-person) .(O)"}}
{"id": "161", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "country", "academic journal", "award", "discipline", "enzyme", "university", "chemical compound", "location", "astronomical object", "theory", "organization", "protein", "scientist", "person", "event"], "instance": {"id": "161", "words": ["In", "1896", ",", "he", "translated", ",", "from", "German", ",", "Heimat", ",", "a", "play", "in", "four", "acts", "by", "Hermann", "Sudermann", ",", "renamed", "Magda", "and", "played", "by", "Henry", "Stephenson", "and", "Charles", "Waldron", "in", "a", "Broadway", "theatre", "production", "in", "New", "York", "City", ",", "New", "York", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "B-person", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, country, academic journal, award, discipline, enzyme, university, chemical compound, location, astronomical object, theory, organization, protein, scientist, person, event and O.\nSentence: In 1896 , he translated , from German , Heimat , a play in four acts by Hermann Sudermann , renamed Magda and played by Henry Stephenson and Charles Waldron in a Broadway theatre production in New York City , New York .", "prompt_labels": "In(O) 1896(O) ,(O) he(O) translated(O) ,(O) from(O) German(O) ,(O) Heimat(O) ,(O) a(O) play(O) in(O) four(O) acts(O) by(O) Hermann(B-person) Sudermann(I-person) ,(O) renamed(O) Magda(B-person) and(O) played(O) by(O) Henry(B-person) Stephenson(I-person) and(O) Charles(B-person) Waldron(I-person) in(O) a(O) Broadway(O) theatre(O) production(O) in(O) New(B-location) York(I-location) City(I-location) ,(O) New(B-location) York(I-location) .(O)"}}
{"id": "255", "dataset": "crossner_science", "split": "test", "label_list": ["person", "protein", "scientist", "event", "academic journal", "country", "chemical element", "organization", "enzyme", "university", "theory", "discipline", "award", "astronomical object", "location", "chemical compound"], "instance": {"id": "255", "words": ["2016", "he", "has", "been", "awarded", "the", "Kavli", "Prize", "in", "Nanoscience", "together", "with", "Gerd", "Binnig", "and", "Calvin", "Quate", "for", "the", "Scanning", "Force", "Microscope", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "B-discipline", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, protein, scientist, event, academic journal, country, chemical element, organization, enzyme, university, theory, discipline, award, astronomical object, location, chemical compound and O.\nSentence: 2016 he has been awarded the Kavli Prize in Nanoscience together with Gerd Binnig and Calvin Quate for the Scanning Force Microscope .", "prompt_labels": "2016(O) he(O) has(O) been(O) awarded(O) the(O) Kavli(B-award) Prize(I-award) in(O) Nanoscience(B-discipline) together(O) with(O) Gerd(B-scientist) Binnig(I-scientist) and(O) Calvin(B-scientist) Quate(I-scientist) for(O) the(O) Scanning(O) Force(O) Microscope(O) .(O)"}}
{"id": "134", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "location", "academic journal", "award", "person", "event", "theory", "discipline", "chemical compound", "chemical element", "university", "enzyme", "astronomical object", "country", "scientist", "organization"], "instance": {"id": "134", "words": ["Topics", "included", "matter", "creation", "and", "annihilation", ",", "the", "fundamental", "interaction", "s", ",", "elementary", "particle", "s", "and", "their", "currents", ",", "hadron", "ic", "and", "lepton", "ic", "physics", ",", "and", "the", "parton", "model", ",", "published", "in", "professional", "peer-reviewed", "scientific", "journal", "s", "including", "Nuclear", "Physics", "B", ",", "Australian", "Journal", "of", "Physics", ",", "Nuovo", "Cimento", ",", "and", "Physical", "Review", "D", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, location, academic journal, award, person, event, theory, discipline, chemical compound, chemical element, university, enzyme, astronomical object, country, scientist, organization and O.\nSentence: Topics included matter creation and annihilation , the fundamental interaction s , elementary particle s and their currents , hadron ic and lepton ic physics , and the parton model , published in professional peer-reviewed scientific journal s including Nuclear Physics B , Australian Journal of Physics , Nuovo Cimento , and Physical Review D .", "prompt_labels": "Topics(O) included(O) matter(O) creation(O) and(O) annihilation(O) ,(O) the(O) fundamental(O) interaction(O) s(O) ,(O) elementary(O) particle(O) s(O) and(O) their(O) currents(O) ,(O) hadron(O) ic(O) and(O) lepton(O) ic(O) physics(B-discipline) ,(O) and(O) the(O) parton(O) model(O) ,(O) published(O) in(O) professional(O) peer-reviewed(O) scientific(O) journal(O) s(O) including(O) Nuclear(B-academic journal) Physics(I-academic journal) B(I-academic journal) ,(O) Australian(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physics(I-academic journal) ,(O) Nuovo(B-academic journal) Cimento(I-academic journal) ,(O) and(O) Physical(B-academic journal) Review(I-academic journal) D(I-academic journal) .(O)"}}
{"id": "435", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "person", "scientist", "location", "chemical element", "award", "organization", "university", "event", "enzyme", "academic journal", "country", "discipline", "theory", "astronomical object", "chemical compound"], "instance": {"id": "435", "words": ["Sydney", "Goldstein", ",", "who", "also", "wrote", "the", "Royal", "Society", "memoir", "for", "Kármán", ",", "reviewed", "the", "autobiography", "Sydney", "Goldstein", "(", "1968", ")", "Journal", "of", "Fluid", "Mechanics", "33", "(", "2", ")", "and", "remembered", "an", "eminent", "engineer", "and", "scientist", ",", "warm-hearted", "and", "witty", ",", "much", "traveled", ",", "well-known", "by", "many", ",", "devoted", "to", "international", "collaboration", ",", "who", ",", "in", "his", "own", "words", ",", "as", "a", "scientist", "found", "the", "military", "'", "the", "most", "comfortable", "group", "to", "deal", "with", "'", "."], "labels": ["B-scientist", "I-scientist", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-scientist", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, person, scientist, location, chemical element, award, organization, university, event, enzyme, academic journal, country, discipline, theory, astronomical object, chemical compound and O.\nSentence: Sydney Goldstein , who also wrote the Royal Society memoir for Kármán , reviewed the autobiography Sydney Goldstein ( 1968 ) Journal of Fluid Mechanics 33 ( 2 ) and remembered an eminent engineer and scientist , warm-hearted and witty , much traveled , well-known by many , devoted to international collaboration , who , in his own words , as a scientist found the military ' the most comfortable group to deal with ' .", "prompt_labels": "Sydney(B-scientist) Goldstein(I-scientist) ,(O) who(O) also(O) wrote(O) the(O) Royal(B-organization) Society(I-organization) memoir(O) for(O) Kármán(B-scientist) ,(O) reviewed(O) the(O) autobiography(O) Sydney(B-scientist) Goldstein(I-scientist) ((O) 1968(O) )(O) Journal(B-academic journal) of(I-academic journal) Fluid(I-academic journal) Mechanics(I-academic journal) 33(O) ((O) 2(O) )(O) and(O) remembered(O) an(O) eminent(O) engineer(O) and(O) scientist(O) ,(O) warm-hearted(O) and(O) witty(O) ,(O) much(O) traveled(O) ,(O) well-known(O) by(O) many(O) ,(O) devoted(O) to(O) international(O) collaboration(O) ,(O) who(O) ,(O) in(O) his(O) own(O) words(O) ,(O) as(O) a(O) scientist(O) found(O) the(O) military(O) '(O) the(O) most(O) comfortable(O) group(O) to(O) deal(O) with(O) '(O) .(O)"}}
{"id": "9", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "chemical compound", "person", "discipline", "astronomical object", "protein", "location", "enzyme", "scientist", "theory", "organization", "country", "university", "event", "chemical element", "award"], "instance": {"id": "9", "words": ["That", "is", ",", "cell", "stimulation", "causes", "DHA", "to", "be", "released", "from", "the", "sn-2", "position", "of", "their", "membrane-bound", "cellular", "phospholipid", "pools", "through", "the", "action", "of", "a", "Phospholipase", "A2", "-type", "enzyme", "and", "the", "subsequent", "attack", "of", "the", "released", "DHA", "by", "CYP450", "epoxidases", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "I-enzyme", "I-enzyme", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "B-enzyme", "I-enzyme", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, chemical compound, person, discipline, astronomical object, protein, location, enzyme, scientist, theory, organization, country, university, event, chemical element, award and O.\nSentence: That is , cell stimulation causes DHA to be released from the sn-2 position of their membrane-bound cellular phospholipid pools through the action of a Phospholipase A2 -type enzyme and the subsequent attack of the released DHA by CYP450 epoxidases .", "prompt_labels": "That(O) is(O) ,(O) cell(O) stimulation(O) causes(O) DHA(B-chemical compound) to(O) be(O) released(O) from(O) the(O) sn-2(O) position(O) of(O) their(O) membrane-bound(O) cellular(O) phospholipid(O) pools(O) through(O) the(O) action(O) of(O) a(O) Phospholipase(B-enzyme) A2(I-enzyme) -type(I-enzyme) enzyme(I-enzyme) and(O) the(O) subsequent(O) attack(O) of(O) the(O) released(O) DHA(B-chemical compound) by(O) CYP450(B-enzyme) epoxidases(I-enzyme) .(O)"}}
{"id": "92", "dataset": "crossner_science", "split": "test", "label_list": ["organization", "award", "academic journal", "enzyme", "chemical element", "discipline", "location", "theory", "university", "event", "chemical compound", "person", "scientist", "astronomical object", "protein", "country"], "instance": {"id": "92", "words": ["The", "German", "university", "system", "allowed", "students", "to", "move", "easily", "from", "one", "university", "to", "another", ",", "so", "he", "spent", "summer", "semesters", "at", "Heidelberg", "University", "in", "1902", "and", "the", "University", "of", "Zurich", "in", "1903", "."], "labels": ["B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, academic journal, enzyme, chemical element, discipline, location, theory, university, event, chemical compound, person, scientist, astronomical object, protein, country and O.\nSentence: The German university system allowed students to move easily from one university to another , so he spent summer semesters at Heidelberg University in 1902 and the University of Zurich in 1903 .", "prompt_labels": "The(B-university) German(I-university) university(I-university) system(O) allowed(O) students(O) to(O) move(O) easily(O) from(O) one(O) university(O) to(O) another(O) ,(O) so(O) he(O) spent(O) summer(O) semesters(O) at(O) Heidelberg(B-university) University(I-university) in(O) 1902(O) and(O) the(O) University(B-university) of(I-university) Zurich(I-university) in(O) 1903(O) .(O)"}}
{"id": "38", "dataset": "crossner_science", "split": "test", "label_list": ["university", "chemical element", "protein", "scientist", "organization", "country", "event", "person", "award", "enzyme", "academic journal", "astronomical object", "location", "theory", "chemical compound", "discipline"], "instance": {"id": "38", "words": ["Currently", ",", "three", "major", "groups", "of", "DNA", "binding", "proteins", "have", "been", "predominantly", "used", "for", "epigenome", "editing", ":", "Zinc", "finger", "proteins", ",", "Transcription", "Activator-Like", "Effectors", "(", "TALEs", ")", "and", "nuclease", "deficient", "Cas9", "fusions", "(", "CRISPR", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "I-protein", "O", "B-protein", "I-protein", "I-protein", "O", "B-protein", "O", "O", "O", "O", "B-protein", "O", "O", "B-protein", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, chemical element, protein, scientist, organization, country, event, person, award, enzyme, academic journal, astronomical object, location, theory, chemical compound, discipline and O.\nSentence: Currently , three major groups of DNA binding proteins have been predominantly used for epigenome editing : Zinc finger proteins , Transcription Activator-Like Effectors ( TALEs ) and nuclease deficient Cas9 fusions ( CRISPR ) .", "prompt_labels": "Currently(O) ,(O) three(O) major(O) groups(O) of(O) DNA(O) binding(O) proteins(O) have(O) been(O) predominantly(O) used(O) for(O) epigenome(O) editing(O) :(O) Zinc(B-protein) finger(I-protein) proteins(I-protein) ,(O) Transcription(B-protein) Activator-Like(I-protein) Effectors(I-protein) ((O) TALEs(B-protein) )(O) and(O) nuclease(O) deficient(O) Cas9(B-protein) fusions(O) ((O) CRISPR(B-protein) )(O) .(O)"}}
{"id": "231", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "enzyme", "event", "university", "academic journal", "chemical element", "person", "organization", "theory", "discipline", "scientist", "protein", "award", "country", "chemical compound", "location"], "instance": {"id": "231", "words": ["Her", "children", "were", "Jupiter", ",", "Neptune", ",", "Pluto", ",", "Juno", ",", "Ceres", ",", "and", "Vesta", "."], "labels": ["O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, enzyme, event, university, academic journal, chemical element, person, organization, theory, discipline, scientist, protein, award, country, chemical compound, location and O.\nSentence: Her children were Jupiter , Neptune , Pluto , Juno , Ceres , and Vesta .", "prompt_labels": "Her(O) children(O) were(O) Jupiter(B-astronomical object) ,(O) Neptune(B-astronomical object) ,(O) Pluto(B-astronomical object) ,(O) Juno(B-astronomical object) ,(O) Ceres(B-astronomical object) ,(O) and(O) Vesta(B-astronomical object) .(O)"}}
{"id": "487", "dataset": "crossner_science", "split": "test", "label_list": ["scientist", "location", "theory", "protein", "enzyme", "discipline", "person", "award", "academic journal", "event", "university", "chemical compound", "country", "astronomical object", "organization", "chemical element"], "instance": {"id": "487", "words": ["Though", "Mendeleev", "was", "widely", "honored", "by", "scientific", "organizations", "all", "over", "Europe", ",", "including", "(", "in", "1882", ")", "the", "Davy", "Medal", "from", "the", "Royal", "Society", "of", "London", "(", "which", "later", "also", "awarded", "him", "the", "Copley", "Medal", "in", "1905", ")", ","], "labels": ["O", "B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, location, theory, protein, enzyme, discipline, person, award, academic journal, event, university, chemical compound, country, astronomical object, organization, chemical element and O.\nSentence: Though Mendeleev was widely honored by scientific organizations all over Europe , including ( in 1882 ) the Davy Medal from the Royal Society of London ( which later also awarded him the Copley Medal in 1905 ) ,", "prompt_labels": "Though(O) Mendeleev(B-scientist) was(O) widely(O) honored(O) by(O) scientific(O) organizations(O) all(O) over(O) Europe(B-location) ,(O) including(O) ((O) in(O) 1882(O) )(O) the(O) Davy(B-award) Medal(I-award) from(I-award) the(I-award) Royal(I-award) Society(I-award) of(I-award) London(I-award) ((O) which(O) later(O) also(O) awarded(O) him(O) the(O) Copley(B-award) Medal(I-award) in(O) 1905(O) )(O) ,(O)"}}
{"id": "282", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "discipline", "theory", "astronomical object", "chemical element", "scientist", "chemical compound", "country", "organization", "location", "university", "protein", "person", "academic journal", "award", "event"], "instance": {"id": "282", "words": ["Other", "work", "included", "an", "improved", "determination", "of", "the", "rotation", "period", "of", "Mars", ",", "the", "discovery", "that", "the", "Martian", "polar", "caps", "vary", "seasonally", ",", "the", "discovery", "of", "Titania", "and", "Oberon", "(", "moons", "of", "Uranus", ")", "and", "Enceladus", "and", "Mimas", "(", "moons", "of", "Saturn", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "B-astronomical object", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, discipline, theory, astronomical object, chemical element, scientist, chemical compound, country, organization, location, university, protein, person, academic journal, award, event and O.\nSentence: Other work included an improved determination of the rotation period of Mars , the discovery that the Martian polar caps vary seasonally , the discovery of Titania and Oberon ( moons of Uranus ) and Enceladus and Mimas ( moons of Saturn ) .", "prompt_labels": "Other(O) work(O) included(O) an(O) improved(O) determination(O) of(O) the(O) rotation(O) period(O) of(O) Mars(B-astronomical object) ,(O) the(O) discovery(O) that(O) the(O) Martian(O) polar(O) caps(O) vary(O) seasonally(O) ,(O) the(O) discovery(O) of(O) Titania(B-astronomical object) and(O) Oberon(B-astronomical object) ((O) moons(O) of(O) Uranus(B-astronomical object) )(O) and(O) Enceladus(B-astronomical object) and(O) Mimas(B-astronomical object) ((O) moons(O) of(O) Saturn(B-astronomical object) )(O) .(O)"}}
{"id": "335", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "chemical element", "event", "astronomical object", "organization", "university", "scientist", "location", "enzyme", "theory", "award", "person", "chemical compound", "academic journal", "country", "discipline"], "instance": {"id": "335", "words": ["A", "second", "meeting", "was", "held", "soon", "thereafter", "and", "included", "Klaus", "Clusius", ",", "Robert", "Döpel", ",", "Werner", "Heisenberg", ",", "and", "Carl", "Friedrich", "von", "Weizsäcker", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, chemical element, event, astronomical object, organization, university, scientist, location, enzyme, theory, award, person, chemical compound, academic journal, country, discipline and O.\nSentence: A second meeting was held soon thereafter and included Klaus Clusius , Robert Döpel , Werner Heisenberg , and Carl Friedrich von Weizsäcker .", "prompt_labels": "A(O) second(O) meeting(O) was(O) held(O) soon(O) thereafter(O) and(O) included(O) Klaus(B-scientist) Clusius(I-scientist) ,(O) Robert(B-scientist) Döpel(I-scientist) ,(O) Werner(B-scientist) Heisenberg(I-scientist) ,(O) and(O) Carl(B-scientist) Friedrich(I-scientist) von(I-scientist) Weizsäcker(I-scientist) .(O)"}}
{"id": "380", "dataset": "crossner_science", "split": "test", "label_list": ["university", "academic journal", "chemical compound", "event", "location", "country", "award", "chemical element", "theory", "astronomical object", "organization", "protein", "person", "discipline", "scientist", "enzyme"], "instance": {"id": "380", "words": ["There", "are", "two", "voltage-gated", "calcium", "channels", "within", "cardiac", "muscle", ":", "L-type", "calcium", "channel", "s", "(", "'", "L", "'", "for", "Long-lasting", ")", "and", "T-type", "calcium", "channel", "s", "(", "'", "T", "'", "for", "Transient", ",", "i.e.", "short", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, academic journal, chemical compound, event, location, country, award, chemical element, theory, astronomical object, organization, protein, person, discipline, scientist, enzyme and O.\nSentence: There are two voltage-gated calcium channels within cardiac muscle : L-type calcium channel s ( ' L ' for Long-lasting ) and T-type calcium channel s ( ' T ' for Transient , i.e. short ) .", "prompt_labels": "There(O) are(O) two(O) voltage-gated(O) calcium(O) channels(O) within(O) cardiac(O) muscle(O) :(O) L-type(O) calcium(O) channel(O) s(O) ((O) '(O) L(O) '(O) for(O) Long-lasting(O) )(O) and(O) T-type(O) calcium(O) channel(O) s(O) ((O) '(O) T(O) '(O) for(O) Transient(O) ,(O) i.e.(O) short(O) )(O) .(O)"}}
{"id": "42", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "chemical compound", "scientist", "discipline", "organization", "chemical element", "award", "theory", "event", "academic journal", "university", "enzyme", "location", "person", "country", "astronomical object"], "instance": {"id": "42", "words": ["He", "was", "a", "Fellow", "of", "the", "Royal", "Society", "and", "a", "foreign", "member", "of", "the", "Royal", "Swedish", "Academy", "of", "Sciences", "."], "labels": ["O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, chemical compound, scientist, discipline, organization, chemical element, award, theory, event, academic journal, university, enzyme, location, person, country, astronomical object and O.\nSentence: He was a Fellow of the Royal Society and a foreign member of the Royal Swedish Academy of Sciences .", "prompt_labels": "He(O) was(O) a(O) Fellow(B-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) and(O) a(O) foreign(O) member(O) of(O) the(O) Royal(B-organization) Swedish(I-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) .(O)"}}
{"id": "204", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "discipline", "enzyme", "scientist", "university", "person", "event", "academic journal", "country", "chemical compound", "organization", "award", "astronomical object", "theory", "location", "protein"], "instance": {"id": "204", "words": ["In", "addition", ",", "scientific", "results", "obtained", "using", "HAARP", "are", "routinely", "published", "in", "major", "research", "journals", "(", "such", "as", "Geophysical", "Research", "Letters", "and", "Journal", "of", "Geophysical", "Research", ")", ",", "written", "both", "by", "university", "scientists", "(", "American", "and", "foreign", ")", "and", "by", "U.S.", "Department", "of", "Defense", "research", "lab", "scientists", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, discipline, enzyme, scientist, university, person, event, academic journal, country, chemical compound, organization, award, astronomical object, theory, location, protein and O.\nSentence: In addition , scientific results obtained using HAARP are routinely published in major research journals ( such as Geophysical Research Letters and Journal of Geophysical Research ) , written both by university scientists ( American and foreign ) and by U.S. Department of Defense research lab scientists .", "prompt_labels": "In(O) addition(O) ,(O) scientific(O) results(O) obtained(O) using(O) HAARP(O) are(O) routinely(O) published(O) in(O) major(O) research(O) journals(O) ((O) such(O) as(O) Geophysical(B-academic journal) Research(I-academic journal) Letters(I-academic journal) and(O) Journal(B-academic journal) of(I-academic journal) Geophysical(I-academic journal) Research(I-academic journal) )(O) ,(O) written(O) both(O) by(O) university(O) scientists(O) ((O) American(O) and(O) foreign(O) )(O) and(O) by(O) U.S.(B-organization) Department(I-organization) of(I-organization) Defense(I-organization) research(I-organization) lab(I-organization) scientists(O) .(O)"}}
{"id": "24", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "academic journal", "chemical element", "discipline", "university", "award", "organization", "chemical compound", "theory", "enzyme", "astronomical object", "country", "event", "location", "person", "scientist"], "instance": {"id": "24", "words": ["She", "has", "won", "five", "Grammy", "Awards", ",", "including", "Grammy", "Award", "for", "Album", "of", "the", "Year", "and", "Grammy", "Award", "for", "Record", "of", "the", "Year", "."], "labels": ["O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, academic journal, chemical element, discipline, university, award, organization, chemical compound, theory, enzyme, astronomical object, country, event, location, person, scientist and O.\nSentence: She has won five Grammy Awards , including Grammy Award for Album of the Year and Grammy Award for Record of the Year .", "prompt_labels": "She(O) has(O) won(O) five(O) Grammy(B-award) Awards(I-award) ,(O) including(O) Grammy(B-award) Award(I-award) for(I-award) Album(I-award) of(I-award) the(I-award) Year(I-award) and(O) Grammy(B-award) Award(I-award) for(I-award) Record(I-award) of(I-award) the(I-award) Year(I-award) .(O)"}}
{"id": "395", "dataset": "crossner_science", "split": "test", "label_list": ["university", "academic journal", "scientist", "discipline", "protein", "chemical compound", "country", "astronomical object", "theory", "location", "organization", "person", "chemical element", "enzyme", "event", "award"], "instance": {"id": "395", "words": ["Additionally", ",", "Histone", "Deacetylase", "A6", "and", "A19", "activity", "contributes", "to", "silencing", "of", "Cytochrome", "P450", "707A", "and", "activation", "of", "NINE-CIS-EPOXYCAROTENOID", "DIOXYGENASE", "4", ",", "9", "."], "labels": ["O", "O", "B-enzyme", "I-enzyme", "I-enzyme", "O", "B-enzyme", "O", "O", "O", "O", "O", "B-protein", "I-protein", "I-protein", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, academic journal, scientist, discipline, protein, chemical compound, country, astronomical object, theory, location, organization, person, chemical element, enzyme, event, award and O.\nSentence: Additionally , Histone Deacetylase A6 and A19 activity contributes to silencing of Cytochrome P450 707A and activation of NINE-CIS-EPOXYCAROTENOID DIOXYGENASE 4 , 9 .", "prompt_labels": "Additionally(O) ,(O) Histone(B-enzyme) Deacetylase(I-enzyme) A6(I-enzyme) and(O) A19(B-enzyme) activity(O) contributes(O) to(O) silencing(O) of(O) Cytochrome(B-protein) P450(I-protein) 707A(I-protein) and(O) activation(O) of(O) NINE-CIS-EPOXYCAROTENOID(B-enzyme) DIOXYGENASE(I-enzyme) 4(O) ,(O) 9(O) .(O)"}}
{"id": "532", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "discipline", "protein", "enzyme", "chemical element", "scientist", "university", "location", "award", "event", "theory", "country", "academic journal", "person", "chemical compound", "organization"], "instance": {"id": "532", "words": ["More", "recently", ",", "some", "of", "the", "top", "awards", "for", "geophysicists", "are", "the", "Vetlesen", "Prize", "(", "intended", "to", "be", "the", "equivalent", "of", "a", "Nobel", "Prize", "for", "geology", "or", "geophysics", ")", ";", "the", "William", "Bowie", "Medal", "(", "the", "top", "award", "of", "the", "American", "Geophysical", "Union", ")", ";", "and", "the", "Crafoord", "Prize", "for", "geosciences", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "B-discipline", "O", "B-discipline", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-award", "I-award", "O", "B-discipline", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, discipline, protein, enzyme, chemical element, scientist, university, location, award, event, theory, country, academic journal, person, chemical compound, organization and O.\nSentence: More recently , some of the top awards for geophysicists are the Vetlesen Prize ( intended to be the equivalent of a Nobel Prize for geology or geophysics ) ; the William Bowie Medal ( the top award of the American Geophysical Union ) ; and the Crafoord Prize for geosciences .", "prompt_labels": "More(O) recently(O) ,(O) some(O) of(O) the(O) top(O) awards(O) for(O) geophysicists(O) are(O) the(O) Vetlesen(B-award) Prize(I-award) ((O) intended(O) to(O) be(O) the(O) equivalent(O) of(O) a(O) Nobel(B-award) Prize(I-award) for(O) geology(B-discipline) or(O) geophysics(B-discipline) )(O) ;(O) the(O) William(B-award) Bowie(I-award) Medal(I-award) ((O) the(O) top(O) award(O) of(O) the(O) American(B-organization) Geophysical(I-organization) Union(I-organization) )(O) ;(O) and(O) the(O) Crafoord(B-award) Prize(I-award) for(O) geosciences(B-discipline) .(O)"}}
{"id": "381", "dataset": "crossner_science", "split": "test", "label_list": ["location", "protein", "organization", "theory", "person", "academic journal", "enzyme", "award", "chemical compound", "scientist", "country", "astronomical object", "event", "university", "chemical element", "discipline"], "instance": {"id": "381", "words": ["In", "1855", ",", "he", "received", "the", "Copley", "Medal", "of", "the", "Royal", "Society", "for", "his", "'", "very", "remarkable", "experimental", "researches", "'", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, protein, organization, theory, person, academic journal, enzyme, award, chemical compound, scientist, country, astronomical object, event, university, chemical element, discipline and O.\nSentence: In 1855 , he received the Copley Medal of the Royal Society for his ' very remarkable experimental researches ' .", "prompt_labels": "In(O) 1855(O) ,(O) he(O) received(O) the(O) Copley(B-award) Medal(I-award) of(O) the(O) Royal(B-organization) Society(I-organization) for(O) his(O) '(O) very(O) remarkable(O) experimental(O) researches(O) '(O) .(O)"}}
{"id": "21", "dataset": "crossner_science", "split": "test", "label_list": ["event", "astronomical object", "protein", "university", "organization", "award", "theory", "scientist", "person", "chemical compound", "country", "location", "academic journal", "chemical element", "enzyme", "discipline"], "instance": {"id": "21", "words": ["In", "the", "second", "season", ",", "married", "couple", "Rose", "Nadler", "(", "L.", "Scott", "Caldwell", ")", "and", "Bernard", "Nadler", "(", "Sam", "Anderson", ")", ",", "separated", "on", "opposite", "sides", "of", "the", "island", "(", "she", "with", "the", "main", "characters", ",", "he", "with", "the", "tail", "section", "survivors", ")", ",", "were", "featured", "in", "a", "flashback", "episode", "after", "being", "reunited", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, astronomical object, protein, university, organization, award, theory, scientist, person, chemical compound, country, location, academic journal, chemical element, enzyme, discipline and O.\nSentence: In the second season , married couple Rose Nadler ( L. Scott Caldwell ) and Bernard Nadler ( Sam Anderson ) , separated on opposite sides of the island ( she with the main characters , he with the tail section survivors ) , were featured in a flashback episode after being reunited .", "prompt_labels": "In(O) the(O) second(O) season(O) ,(O) married(O) couple(O) Rose(B-person) Nadler(I-person) ((O) L.(B-person) Scott(I-person) Caldwell(I-person) )(O) and(O) Bernard(B-person) Nadler(I-person) ((O) Sam(B-person) Anderson(I-person) )(O) ,(O) separated(O) on(O) opposite(O) sides(O) of(O) the(O) island(O) ((O) she(O) with(O) the(O) main(O) characters(O) ,(O) he(O) with(O) the(O) tail(O) section(O) survivors(O) )(O) ,(O) were(O) featured(O) in(O) a(O) flashback(O) episode(O) after(O) being(O) reunited(O) .(O)"}}
{"id": "451", "dataset": "crossner_science", "split": "test", "label_list": ["chemical compound", "enzyme", "award", "protein", "academic journal", "astronomical object", "discipline", "scientist", "event", "organization", "person", "theory", "university", "location", "country", "chemical element"], "instance": {"id": "451", "words": ["On", "February", "12", ",", "2005", ",", "he", "was", "given", "a", "Grammy", "Lifetime", "Achievement", "Award", "by", "The", "Recording", "Academy", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, enzyme, award, protein, academic journal, astronomical object, discipline, scientist, event, organization, person, theory, university, location, country, chemical element and O.\nSentence: On February 12 , 2005 , he was given a Grammy Lifetime Achievement Award by The Recording Academy .", "prompt_labels": "On(O) February(O) 12(O) ,(O) 2005(O) ,(O) he(O) was(O) given(O) a(O) Grammy(B-award) Lifetime(I-award) Achievement(I-award) Award(I-award) by(O) The(B-organization) Recording(I-organization) Academy(I-organization) .(O)"}}
{"id": "7", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "organization", "astronomical object", "university", "chemical compound", "person", "country", "scientist", "enzyme", "award", "theory", "event", "protein", "location", "chemical element", "discipline"], "instance": {"id": "7", "words": ["Research", "indicates", "that", "the", "majority", "of", "variation", "in", "coat", "growth", "pattern", ",", "length", "and", "curl", "can", "be", "attributed", "to", "mutations", "in", "four", "genes", ",", "the", "R-spondin-2", "gene", "or", "RSPO2", ",", "the", "fibroblast", "growth", "factor-5", "gene", "or", "FGF5", ",", "the", "KRT71", "gene", "or", "KRT71", "and", "the", "melanocortin", "5", "receptor", "gene", "(", "MC5R", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O", "B-protein", "O", "O", "B-protein", "I-protein", "I-protein", "O", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "I-protein", "O", "O", "B-protein", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, organization, astronomical object, university, chemical compound, person, country, scientist, enzyme, award, theory, event, protein, location, chemical element, discipline and O.\nSentence: Research indicates that the majority of variation in coat growth pattern , length and curl can be attributed to mutations in four genes , the R-spondin-2 gene or RSPO2 , the fibroblast growth factor-5 gene or FGF5 , the KRT71 gene or KRT71 and the melanocortin 5 receptor gene ( MC5R ) .", "prompt_labels": "Research(O) indicates(O) that(O) the(O) majority(O) of(O) variation(O) in(O) coat(O) growth(O) pattern(O) ,(O) length(O) and(O) curl(O) can(O) be(O) attributed(O) to(O) mutations(O) in(O) four(O) genes(O) ,(O) the(O) R-spondin-2(B-protein) gene(O) or(O) RSPO2(B-protein) ,(O) the(O) fibroblast(B-protein) growth(I-protein) factor-5(I-protein) gene(O) or(O) FGF5(B-protein) ,(O) the(O) KRT71(O) gene(O) or(O) KRT71(O) and(O) the(O) melanocortin(B-protein) 5(I-protein) receptor(I-protein) gene(O) ((O) MC5R(B-protein) )(O) .(O)"}}
{"id": "345", "dataset": "crossner_science", "split": "test", "label_list": ["event", "location", "protein", "person", "theory", "discipline", "astronomical object", "university", "scientist", "enzyme", "academic journal", "country", "chemical compound", "organization", "chemical element", "award"], "instance": {"id": "345", "words": ["The", "discovery", "of", "the", "asteroid", "3", "Juno", "by", "Karl", "Ludwig", "Harding", "and", "4", "Vesta", "by", "Olbers", ",", "buttressed", "his", "hypothesis", "."], "labels": ["O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-astronomical object", "I-astronomical object", "O", "B-scientist", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, protein, person, theory, discipline, astronomical object, university, scientist, enzyme, academic journal, country, chemical compound, organization, chemical element, award and O.\nSentence: The discovery of the asteroid 3 Juno by Karl Ludwig Harding and 4 Vesta by Olbers , buttressed his hypothesis .", "prompt_labels": "The(O) discovery(O) of(O) the(O) asteroid(O) 3(B-astronomical object) Juno(I-astronomical object) by(O) Karl(B-scientist) Ludwig(I-scientist) Harding(I-scientist) and(O) 4(B-astronomical object) Vesta(I-astronomical object) by(O) Olbers(B-scientist) ,(O) buttressed(O) his(O) hypothesis(O) .(O)"}}
{"id": "387", "dataset": "crossner_science", "split": "test", "label_list": ["country", "chemical element", "enzyme", "award", "location", "chemical compound", "scientist", "organization", "event", "academic journal", "person", "astronomical object", "discipline", "protein", "theory", "university"], "instance": {"id": "387", "words": ["Another", "absolute", "instrumental", "approach", "was", "also", "possible", "with", "the", "development", "of", "light", "scattering", "theory", "by", "Einstein", ",", "Raman", ",", "Peter", "Debye", ",", "Bruno", "H.", "Zimm", ",", "and", "others", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-theory", "I-theory", "I-theory", "O", "B-scientist", "O", "B-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, chemical element, enzyme, award, location, chemical compound, scientist, organization, event, academic journal, person, astronomical object, discipline, protein, theory, university and O.\nSentence: Another absolute instrumental approach was also possible with the development of light scattering theory by Einstein , Raman , Peter Debye , Bruno H. Zimm , and others .", "prompt_labels": "Another(O) absolute(O) instrumental(O) approach(O) was(O) also(O) possible(O) with(O) the(O) development(O) of(O) light(B-theory) scattering(I-theory) theory(I-theory) by(O) Einstein(B-scientist) ,(O) Raman(B-scientist) ,(O) Peter(B-scientist) Debye(I-scientist) ,(O) Bruno(B-scientist) H.(I-scientist) Zimm(I-scientist) ,(O) and(O) others(O) .(O)"}}
{"id": "509", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "chemical compound", "protein", "award", "organization", "chemical element", "astronomical object", "event", "enzyme", "university", "scientist", "location", "person", "academic journal", "country", "discipline"], "instance": {"id": "509", "words": ["Two", "American", "explorers", "claimed", "to", "reach", "the", "North", "Pole", ";", "Frederick", "Cook", "in", "1908", "and", "Robert", "Peary", "in", "1909", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, chemical compound, protein, award, organization, chemical element, astronomical object, event, enzyme, university, scientist, location, person, academic journal, country, discipline and O.\nSentence: Two American explorers claimed to reach the North Pole ; Frederick Cook in 1908 and Robert Peary in 1909 .", "prompt_labels": "Two(O) American(O) explorers(O) claimed(O) to(O) reach(O) the(O) North(B-location) Pole(I-location) ;(O) Frederick(B-person) Cook(I-person) in(O) 1908(O) and(O) Robert(B-person) Peary(I-person) in(O) 1909(O) .(O)"}}
{"id": "385", "dataset": "crossner_science", "split": "test", "label_list": ["person", "award", "enzyme", "country", "chemical compound", "astronomical object", "university", "theory", "scientist", "organization", "chemical element", "location", "event", "academic journal", "protein", "discipline"], "instance": {"id": "385", "words": ["García", "studied", "law", ",", "first", "at", "the", "Pontifical", "Catholic", "University", "of", "Peru", "-although", "the", "official", "records", "of", "his", "tenure", "in", "this", "university", "were", "never", "found-", "and", "later", "earning", "a", "law", "degree", "from", "the", "National", "University", "of", "San", "Marcos", "in", "1971", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, enzyme, country, chemical compound, astronomical object, university, theory, scientist, organization, chemical element, location, event, academic journal, protein, discipline and O.\nSentence: García studied law , first at the Pontifical Catholic University of Peru -although the official records of his tenure in this university were never found- and later earning a law degree from the National University of San Marcos in 1971 .", "prompt_labels": "García(B-person) studied(O) law(O) ,(O) first(O) at(O) the(O) Pontifical(B-university) Catholic(I-university) University(I-university) of(I-university) Peru(I-university) -although(O) the(O) official(O) records(O) of(O) his(O) tenure(O) in(O) this(O) university(O) were(O) never(O) found-(O) and(O) later(O) earning(O) a(O) law(O) degree(O) from(O) the(O) National(B-university) University(I-university) of(I-university) San(I-university) Marcos(I-university) in(O) 1971(O) .(O)"}}
{"id": "202", "dataset": "crossner_science", "split": "test", "label_list": ["event", "theory", "enzyme", "chemical compound", "chemical element", "discipline", "country", "scientist", "university", "astronomical object", "protein", "person", "location", "academic journal", "organization", "award"], "instance": {"id": "202", "words": ["Marsden", "was", "discovered", "on", "24", "March", "1971", ",", "by", "Dutch", "astronomer", "couple", "Ingrid", "van", "Houten-Groeneveld", "and", "Cornelis", "van", "Houten", "at", "Leiden", ",", "on", "photographic", "plates", "taken", "by", "Dutch-American", "astronomer", "Tom", "Gehrels", "at", "Palomar", "Observatory", ",", "California", "."], "labels": ["B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-location", "I-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, theory, enzyme, chemical compound, chemical element, discipline, country, scientist, university, astronomical object, protein, person, location, academic journal, organization, award and O.\nSentence: Marsden was discovered on 24 March 1971 , by Dutch astronomer couple Ingrid van Houten-Groeneveld and Cornelis van Houten at Leiden , on photographic plates taken by Dutch-American astronomer Tom Gehrels at Palomar Observatory , California .", "prompt_labels": "Marsden(B-astronomical object) was(O) discovered(O) on(O) 24(O) March(O) 1971(O) ,(O) by(O) Dutch(O) astronomer(O) couple(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Cornelis(B-scientist) van(I-scientist) Houten(I-scientist) at(O) Leiden(B-location) ,(O) on(O) photographic(O) plates(O) taken(O) by(O) Dutch-American(O) astronomer(O) Tom(B-scientist) Gehrels(I-scientist) at(O) Palomar(B-location) Observatory(I-location) ,(O) California(B-location) .(O)"}}
{"id": "75", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "award", "scientist", "discipline", "astronomical object", "chemical compound", "enzyme", "chemical element", "university", "protein", "organization", "academic journal", "location", "person", "event", "country"], "instance": {"id": "75", "words": ["This", "unification", ",", "which", "was", "observed", "by", "Michael", "Faraday", ",", "extended", "by", "James", "Clerk", "Maxwell", ",", "and", "partially", "reformulated", "by", "Oliver", "Heaviside", "and", "Heinrich", "Hertz", ",", "is", "one", "of", "the", "key", "accomplishments", "of", "19th-century", "mathematical", "physics", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "I-discipline", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, award, scientist, discipline, astronomical object, chemical compound, enzyme, chemical element, university, protein, organization, academic journal, location, person, event, country and O.\nSentence: This unification , which was observed by Michael Faraday , extended by James Clerk Maxwell , and partially reformulated by Oliver Heaviside and Heinrich Hertz , is one of the key accomplishments of 19th-century mathematical physics .", "prompt_labels": "This(O) unification(O) ,(O) which(O) was(O) observed(O) by(O) Michael(B-scientist) Faraday(I-scientist) ,(O) extended(O) by(O) James(B-scientist) Clerk(I-scientist) Maxwell(I-scientist) ,(O) and(O) partially(O) reformulated(O) by(O) Oliver(B-scientist) Heaviside(I-scientist) and(O) Heinrich(B-scientist) Hertz(I-scientist) ,(O) is(O) one(O) of(O) the(O) key(O) accomplishments(O) of(O) 19th-century(O) mathematical(B-discipline) physics(I-discipline) .(O)"}}
{"id": "267", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "scientist", "event", "protein", "theory", "enzyme", "organization", "award", "country", "university", "discipline", "astronomical object", "location", "chemical compound", "person", "academic journal"], "instance": {"id": "267", "words": ["He", "calculated", "the", "orbit", "s", "of", "the", "moons", "of", "Uranus", "and", "Saturn", ",", "obtaining", "the", "first", "value", "for", "Uranus", "'", "mass", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, scientist, event, protein, theory, enzyme, organization, award, country, university, discipline, astronomical object, location, chemical compound, person, academic journal and O.\nSentence: He calculated the orbit s of the moons of Uranus and Saturn , obtaining the first value for Uranus ' mass .", "prompt_labels": "He(O) calculated(O) the(O) orbit(O) s(O) of(O) the(O) moons(O) of(O) Uranus(B-astronomical object) and(O) Saturn(B-astronomical object) ,(O) obtaining(O) the(O) first(O) value(O) for(O) Uranus(B-astronomical object) '(O) mass(O) .(O)"}}
{"id": "416", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "person", "event", "location", "academic journal", "theory", "protein", "organization", "country", "enzyme", "university", "award", "chemical compound", "astronomical object", "discipline", "scientist"], "instance": {"id": "416", "words": ["In", "2002", ",", "it", "received", "nominations", "for", "Saturn", "Award", "for", "Best", "Actress", "on", "Television", "(", "Black", ")", ",", "Saturn", "Award", "for", "Best", "Supporting", "Actor", "on", "Television", "(", "Anthony", "Simcoe", "as", "the", "Luxan", "warrior", "Ka", "D", "'Argo", ")", ",", "and", "Saturn", "Award", "for", "Best", "Supporting", "Actress", "on", "Television", "(", "Gigi", "Edgley", "as", "the", "Nebari", "rogue", "Chiana", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "O", "O", "O", "B-person", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, person, event, location, academic journal, theory, protein, organization, country, enzyme, university, award, chemical compound, astronomical object, discipline, scientist and O.\nSentence: In 2002 , it received nominations for Saturn Award for Best Actress on Television ( Black ) , Saturn Award for Best Supporting Actor on Television ( Anthony Simcoe as the Luxan warrior Ka D 'Argo ) , and Saturn Award for Best Supporting Actress on Television ( Gigi Edgley as the Nebari rogue Chiana ) .", "prompt_labels": "In(O) 2002(O) ,(O) it(O) received(O) nominations(O) for(O) Saturn(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) on(I-award) Television(I-award) ((O) Black(O) )(O) ,(O) Saturn(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actor(I-award) on(I-award) Television(I-award) ((O) Anthony(B-person) Simcoe(I-person) as(O) the(O) Luxan(O) warrior(O) Ka(B-person) D(I-person) 'Argo(I-person) )(O) ,(O) and(O) Saturn(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actress(I-award) on(I-award) Television(I-award) ((O) Gigi(B-person) Edgley(I-person) as(O) the(O) Nebari(O) rogue(O) Chiana(B-person) )(O) .(O)"}}
{"id": "500", "dataset": "crossner_science", "split": "test", "label_list": ["event", "astronomical object", "location", "person", "award", "organization", "university", "chemical element", "chemical compound", "scientist", "country", "academic journal", "enzyme", "protein", "discipline", "theory"], "instance": {"id": "500", "words": ["He", "has", "collaborated", "with", ",", "among", "others", ",", "Luciano", "Pietronero", ",", "Benoit", "Mandelbrot", ",", "Betz", "Halloran", ",", "Ira", "Longini", ",", "and", "David", "Lazer", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, astronomical object, location, person, award, organization, university, chemical element, chemical compound, scientist, country, academic journal, enzyme, protein, discipline, theory and O.\nSentence: He has collaborated with , among others , Luciano Pietronero , Benoit Mandelbrot , Betz Halloran , Ira Longini , and David Lazer .", "prompt_labels": "He(O) has(O) collaborated(O) with(O) ,(O) among(O) others(O) ,(O) Luciano(B-scientist) Pietronero(I-scientist) ,(O) Benoit(B-scientist) Mandelbrot(I-scientist) ,(O) Betz(B-scientist) Halloran(I-scientist) ,(O) Ira(B-scientist) Longini(I-scientist) ,(O) and(O) David(B-scientist) Lazer(I-scientist) .(O)"}}
{"id": "284", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "theory", "enzyme", "academic journal", "astronomical object", "organization", "country", "university", "scientist", "event", "chemical compound", "award", "person", "chemical element", "location", "discipline"], "instance": {"id": "284", "words": ["The", "visitor", "starts", "with", "the", "sun", "on", "the", "eastern", "summit", "of", "the", "Hill", ",", "and", "following", "the", "trail", "to", "the", "west", "from", "the", "Sun", ",", "in", "the", "direction", "of", "Mills", "Observatory", ",", "he", "or", "she", "will", "encounter", "another", "eight", "rocks", "representing", "the", "planets", "Mercury", ",", "Venus", ",", "Earth", ",", "Mars", ",", "Jupiter", ",", "Saturn", ",", "Uranus", "and", "Neptune", "."], "labels": ["O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, theory, enzyme, academic journal, astronomical object, organization, country, university, scientist, event, chemical compound, award, person, chemical element, location, discipline and O.\nSentence: The visitor starts with the sun on the eastern summit of the Hill , and following the trail to the west from the Sun , in the direction of Mills Observatory , he or she will encounter another eight rocks representing the planets Mercury , Venus , Earth , Mars , Jupiter , Saturn , Uranus and Neptune .", "prompt_labels": "The(O) visitor(O) starts(O) with(O) the(O) sun(B-astronomical object) on(O) the(O) eastern(O) summit(O) of(O) the(O) Hill(B-location) ,(O) and(O) following(O) the(O) trail(O) to(O) the(O) west(O) from(O) the(O) Sun(B-astronomical object) ,(O) in(O) the(O) direction(O) of(O) Mills(B-location) Observatory(I-location) ,(O) he(O) or(O) she(O) will(O) encounter(O) another(O) eight(O) rocks(O) representing(O) the(O) planets(O) Mercury(B-astronomical object) ,(O) Venus(B-astronomical object) ,(O) Earth(B-astronomical object) ,(O) Mars(B-astronomical object) ,(O) Jupiter(B-astronomical object) ,(O) Saturn(B-astronomical object) ,(O) Uranus(B-astronomical object) and(O) Neptune(B-astronomical object) .(O)"}}
{"id": "109", "dataset": "crossner_science", "split": "test", "label_list": ["scientist", "academic journal", "discipline", "person", "enzyme", "chemical element", "astronomical object", "award", "country", "theory", "event", "location", "university", "chemical compound", "organization", "protein"], "instance": {"id": "109", "words": ["Juilfs", "was", "a", "theoretical", "physics", "assistant", "from", "1938", "to", "1945", "at", "the", "Kaiser-Wilhelm", "Institut", "für", "Physik", "(", "KWIP", ",", "Kaiser", "Wilhelm", "Institute", "for", "Physics", ";", "today", ",", "the", "Max-Planck", "Institut", "für", "Physik", ")", ",", "first", "for", "Max", "von", "Laue", "and", "from", "1943", "to", "Werner", "Heisenberg", "and", "Hentschel", ",", "1996", ",", "Appendix", "F", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, academic journal, discipline, person, enzyme, chemical element, astronomical object, award, country, theory, event, location, university, chemical compound, organization, protein and O.\nSentence: Juilfs was a theoretical physics assistant from 1938 to 1945 at the Kaiser-Wilhelm Institut für Physik ( KWIP , Kaiser Wilhelm Institute for Physics ; today , the Max-Planck Institut für Physik ) , first for Max von Laue and from 1943 to Werner Heisenberg and Hentschel , 1996 , Appendix F .", "prompt_labels": "Juilfs(B-scientist) was(O) a(O) theoretical(O) physics(O) assistant(O) from(O) 1938(O) to(O) 1945(O) at(O) the(O) Kaiser-Wilhelm(B-organization) Institut(I-organization) für(I-organization) Physik(I-organization) ((O) KWIP(B-organization) ,(O) Kaiser(B-organization) Wilhelm(I-organization) Institute(I-organization) for(I-organization) Physics(I-organization) ;(O) today(O) ,(O) the(O) Max-Planck(B-organization) Institut(I-organization) für(I-organization) Physik(I-organization) )(O) ,(O) first(O) for(O) Max(B-scientist) von(I-scientist) Laue(I-scientist) and(O) from(O) 1943(O) to(O) Werner(B-scientist) Heisenberg(I-scientist) and(O) Hentschel(B-scientist) ,(O) 1996(O) ,(O) Appendix(O) F(O) .(O)"}}
{"id": "356", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "country", "award", "location", "chemical element", "university", "event", "scientist", "theory", "person", "enzyme", "chemical compound", "protein", "academic journal", "organization", "discipline"], "instance": {"id": "356", "words": ["The", "gamma", "pseudogene", "contains", "an", "inverted", "long", "interspersed", "nuclear", "element", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, country, award, location, chemical element, university, event, scientist, theory, person, enzyme, chemical compound, protein, academic journal, organization, discipline and O.\nSentence: The gamma pseudogene contains an inverted long interspersed nuclear element .", "prompt_labels": "The(O) gamma(O) pseudogene(O) contains(O) an(O) inverted(O) long(O) interspersed(O) nuclear(O) element(O) .(O)"}}
{"id": "291", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "location", "scientist", "award", "academic journal", "university", "country", "organization", "discipline", "astronomical object", "chemical compound", "protein", "theory", "person", "enzyme", "event"], "instance": {"id": "291", "words": ["She", "played", "a", "substantial", "role", "in", "the", "discovery", "of", "germ", "cell", "migratory", "pathways", "(", "namely", "those", "involving", "gap", "junction", "s", ",", "G", "protein-coupled", "receptor", "like", "Tre-1", ",", "and", "isoprenoids", ")", ",", "particularly", "those", "concerning", "migration", "into", "the", "ovaries", "and", "testis", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "I-protein", "O", "B-protein", "O", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, location, scientist, award, academic journal, university, country, organization, discipline, astronomical object, chemical compound, protein, theory, person, enzyme, event and O.\nSentence: She played a substantial role in the discovery of germ cell migratory pathways ( namely those involving gap junction s , G protein-coupled receptor like Tre-1 , and isoprenoids ) , particularly those concerning migration into the ovaries and testis .", "prompt_labels": "She(O) played(O) a(O) substantial(O) role(O) in(O) the(O) discovery(O) of(O) germ(O) cell(O) migratory(O) pathways(O) ((O) namely(O) those(O) involving(O) gap(O) junction(O) s(O) ,(O) G(B-protein) protein-coupled(I-protein) receptor(I-protein) like(O) Tre-1(B-protein) ,(O) and(O) isoprenoids(B-protein) )(O) ,(O) particularly(O) those(O) concerning(O) migration(O) into(O) the(O) ovaries(O) and(O) testis(O) .(O)"}}
{"id": "309", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "university", "chemical compound", "country", "organization", "chemical element", "enzyme", "discipline", "event", "scientist", "astronomical object", "location", "person", "award", "theory", "protein"], "instance": {"id": "309", "words": ["Typical", "COC", "material", "will", "have", "a", "higher", "modulus", "than", "HDPE", "and", "Polypropylene", ",", "similar", "to", "Polyethylene", "terephthalate", "or", "PC", "."], "labels": ["O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, university, chemical compound, country, organization, chemical element, enzyme, discipline, event, scientist, astronomical object, location, person, award, theory, protein and O.\nSentence: Typical COC material will have a higher modulus than HDPE and Polypropylene , similar to Polyethylene terephthalate or PC .", "prompt_labels": "Typical(O) COC(B-chemical compound) material(O) will(O) have(O) a(O) higher(O) modulus(O) than(O) HDPE(B-chemical compound) and(O) Polypropylene(B-chemical compound) ,(O) similar(O) to(O) Polyethylene(B-chemical compound) terephthalate(I-chemical compound) or(O) PC(B-chemical compound) .(O)"}}
{"id": "355", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "chemical element", "protein", "discipline", "organization", "event", "award", "university", "scientist", "academic journal", "chemical compound", "enzyme", "location", "astronomical object", "person", "country"], "instance": {"id": "355", "words": ["Aluminium", "hydroxide", ",", "ferrous", "hydroxide", ",", "Hydroxide", ","], "labels": ["B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, chemical element, protein, discipline, organization, event, award, university, scientist, academic journal, chemical compound, enzyme, location, astronomical object, person, country and O.\nSentence: Aluminium hydroxide , ferrous hydroxide , Hydroxide ,", "prompt_labels": "Aluminium(B-chemical compound) hydroxide(I-chemical compound) ,(O) ferrous(B-chemical compound) hydroxide(I-chemical compound) ,(O) Hydroxide(B-chemical compound) ,(O)"}}
{"id": "527", "dataset": "crossner_science", "split": "test", "label_list": ["location", "chemical compound", "country", "astronomical object", "person", "event", "chemical element", "organization", "discipline", "university", "enzyme", "academic journal", "theory", "award", "protein", "scientist"], "instance": {"id": "527", "words": ["The", "journal", "has", "been", "cited", "most", "often", "by", "the", "following", "journals", "Journal", "of", "Clinical", "Pathology", ",", "Histopathology", ",", "Archives", "of", "Pathology", ";", "Laboratory", "Medicine", ",", "Human", "Pathology", ",", "and", "the", "World", "Journal", "of", "Gastroenterology", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, chemical compound, country, astronomical object, person, event, chemical element, organization, discipline, university, enzyme, academic journal, theory, award, protein, scientist and O.\nSentence: The journal has been cited most often by the following journals Journal of Clinical Pathology , Histopathology , Archives of Pathology ; Laboratory Medicine , Human Pathology , and the World Journal of Gastroenterology .", "prompt_labels": "The(O) journal(O) has(O) been(O) cited(O) most(O) often(O) by(O) the(O) following(O) journals(O) Journal(B-academic journal) of(I-academic journal) Clinical(I-academic journal) Pathology(I-academic journal) ,(O) Histopathology(B-academic journal) ,(O) Archives(B-academic journal) of(I-academic journal) Pathology(I-academic journal) ;(O) Laboratory(B-academic journal) Medicine(I-academic journal) ,(O) Human(B-academic journal) Pathology(I-academic journal) ,(O) and(O) the(O) World(B-academic journal) Journal(I-academic journal) of(I-academic journal) Gastroenterology(I-academic journal) .(O)"}}
{"id": "375", "dataset": "crossner_science", "split": "test", "label_list": ["country", "scientist", "chemical compound", "person", "university", "theory", "protein", "award", "chemical element", "organization", "academic journal", "discipline", "event", "enzyme", "location", "astronomical object"], "instance": {"id": "375", "words": ["DSBs", "can", "be", "artificially", "induced", "using", "genome", "editing", "technologies", "such", "as", "CRISPR", "or", "TALEN", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, scientist, chemical compound, person, university, theory, protein, award, chemical element, organization, academic journal, discipline, event, enzyme, location, astronomical object and O.\nSentence: DSBs can be artificially induced using genome editing technologies such as CRISPR or TALEN .", "prompt_labels": "DSBs(O) can(O) be(O) artificially(O) induced(O) using(O) genome(O) editing(O) technologies(O) such(O) as(O) CRISPR(O) or(O) TALEN(O) .(O)"}}
{"id": "180", "dataset": "crossner_science", "split": "test", "label_list": ["chemical compound", "discipline", "chemical element", "award", "protein", "astronomical object", "university", "organization", "country", "enzyme", "theory", "person", "academic journal", "location", "scientist", "event"], "instance": {"id": "180", "words": ["The", "International", "Astronomical", "Union", "names", "all", "colles", "(", "small", "hills", ")", "on", "Saturn", "'", "s", "moon", "Titan", "after", "characters", "in", "Tolkien", "'s", "work", "."], "labels": ["O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "B-astronomical object", "O", "O", "O", "B-scientist", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, discipline, chemical element, award, protein, astronomical object, university, organization, country, enzyme, theory, person, academic journal, location, scientist, event and O.\nSentence: The International Astronomical Union names all colles ( small hills ) on Saturn ' s moon Titan after characters in Tolkien 's work .", "prompt_labels": "The(O) International(B-organization) Astronomical(I-organization) Union(I-organization) names(O) all(O) colles(O) ((O) small(O) hills(O) )(O) on(O) Saturn(B-astronomical object) '(O) s(O) moon(O) Titan(B-astronomical object) after(O) characters(O) in(O) Tolkien(B-scientist) 's(O) work(O) .(O)"}}
{"id": "201", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "event", "discipline", "protein", "chemical element", "academic journal", "theory", "university", "country", "location", "enzyme", "chemical compound", "award", "person", "scientist", "organization"], "instance": {"id": "201", "words": ["In", "addition", ",", "he", "is", "the", "founding", "editor", "of", "Theoretical", "Population", "Biology", "(", "1971-2013", ")", "and", "an", "associate", "editor", "of", "Genetics", ",", "Human", "Genetics", ",", "Annals", "of", "Human", "Genetics", ",", "Annals", "of", "Human", "Biology", ",", "and", "Complexity", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "B-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, event, discipline, protein, chemical element, academic journal, theory, university, country, location, enzyme, chemical compound, award, person, scientist, organization and O.\nSentence: In addition , he is the founding editor of Theoretical Population Biology ( 1971-2013 ) and an associate editor of Genetics , Human Genetics , Annals of Human Genetics , Annals of Human Biology , and Complexity .", "prompt_labels": "In(O) addition(O) ,(O) he(O) is(O) the(O) founding(O) editor(O) of(O) Theoretical(B-academic journal) Population(I-academic journal) Biology(I-academic journal) ((O) 1971-2013(O) )(O) and(O) an(O) associate(O) editor(O) of(O) Genetics(B-academic journal) ,(O) Human(B-academic journal) Genetics(I-academic journal) ,(O) Annals(B-academic journal) of(I-academic journal) Human(I-academic journal) Genetics(I-academic journal) ,(O) Annals(B-academic journal) of(I-academic journal) Human(I-academic journal) Biology(I-academic journal) ,(O) and(O) Complexity(B-academic journal) .(O)"}}
{"id": "427", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "academic journal", "award", "person", "protein", "event", "location", "country", "enzyme", "scientist", "chemical element", "theory", "discipline", "university", "chemical compound", "organization"], "instance": {"id": "427", "words": ["This", "includes", "Pluto", ",", "which", "is", "constrained", "in", "its", "orbit", "by", "the", "gravity", "of", "Neptune", "and", "shares", "its", "orbital", "neighbourhood", "with", "many", "Kuiper", "belt", "objects", "."], "labels": ["O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, academic journal, award, person, protein, event, location, country, enzyme, scientist, chemical element, theory, discipline, university, chemical compound, organization and O.\nSentence: This includes Pluto , which is constrained in its orbit by the gravity of Neptune and shares its orbital neighbourhood with many Kuiper belt objects .", "prompt_labels": "This(O) includes(O) Pluto(B-astronomical object) ,(O) which(O) is(O) constrained(O) in(O) its(O) orbit(O) by(O) the(O) gravity(O) of(O) Neptune(B-astronomical object) and(O) shares(O) its(O) orbital(O) neighbourhood(O) with(O) many(O) Kuiper(O) belt(O) objects(O) .(O)"}}
{"id": "91", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "scientist", "chemical compound", "theory", "university", "organization", "person", "enzyme", "award", "event", "country", "location", "chemical element", "protein", "discipline", "academic journal"], "instance": {"id": "91", "words": ["He", "represented", "his", "country", "at", "the", "2017", "World", "Championships", "in", "Athletics", "without", "reaching", "the", "semifinals", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, scientist, chemical compound, theory, university, organization, person, enzyme, award, event, country, location, chemical element, protein, discipline, academic journal and O.\nSentence: He represented his country at the 2017 World Championships in Athletics without reaching the semifinals .", "prompt_labels": "He(O) represented(O) his(O) country(O) at(O) the(O) 2017(B-event) World(I-event) Championships(I-event) in(I-event) Athletics(I-event) without(O) reaching(O) the(O) semifinals(O) .(O)"}}
{"id": "178", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "discipline", "organization", "astronomical object", "scientist", "location", "academic journal", "chemical compound", "award", "university", "country", "person", "chemical element", "enzyme", "theory", "event"], "instance": {"id": "178", "words": ["Arizona", "State", "University", "offers", "undergraduate", "student", "housing", "on", "four", "metropolitan", "Phoenix", "campuses", "(", "Arizona", "State", "University", "Tempe", "campus", ",", "Arizona", "State", "University", "Polytechnic", "campus", ",", "Arizona", "State", "University", "Downtown", "Phoenix", "campus", ",", "and", "Arizona", "State", "University", "West", "campus", ")", ",", "plus", "the", "ASU", "Colleges", "at", "Lake", "Havasu", "City", "."], "labels": ["B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "I-university", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, discipline, organization, astronomical object, scientist, location, academic journal, chemical compound, award, university, country, person, chemical element, enzyme, theory, event and O.\nSentence: Arizona State University offers undergraduate student housing on four metropolitan Phoenix campuses ( Arizona State University Tempe campus , Arizona State University Polytechnic campus , Arizona State University Downtown Phoenix campus , and Arizona State University West campus ) , plus the ASU Colleges at Lake Havasu City .", "prompt_labels": "Arizona(B-university) State(I-university) University(I-university) offers(O) undergraduate(O) student(O) housing(O) on(O) four(O) metropolitan(O) Phoenix(B-location) campuses(I-location) ((O) Arizona(B-university) State(I-university) University(I-university) Tempe(I-university) campus(I-university) ,(O) Arizona(B-university) State(I-university) University(I-university) Polytechnic(I-university) campus(I-university) ,(O) Arizona(B-university) State(I-university) University(I-university) Downtown(I-university) Phoenix(I-university) campus(I-university) ,(O) and(O) Arizona(B-university) State(I-university) University(I-university) West(I-university) campus(I-university) )(O) ,(O) plus(O) the(O) ASU(B-university) Colleges(I-university) at(I-university) Lake(I-university) Havasu(I-university) City(I-university) .(O)"}}
{"id": "252", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "university", "astronomical object", "theory", "protein", "award", "event", "enzyme", "chemical compound", "organization", "chemical element", "scientist", "location", "person", "discipline", "country"], "instance": {"id": "252", "words": ["Furry", "made", "important", "contributions", "to", "the", "early", "development", "of", "Quantum", "Field", "Theory", "with", "J.", "Robert", "Oppenheimer", ",", "Vladimir", "Fock", ",", "and", "others", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "B-theory", "I-theory", "I-theory", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, university, astronomical object, theory, protein, award, event, enzyme, chemical compound, organization, chemical element, scientist, location, person, discipline, country and O.\nSentence: Furry made important contributions to the early development of Quantum Field Theory with J. Robert Oppenheimer , Vladimir Fock , and others .", "prompt_labels": "Furry(B-scientist) made(O) important(O) contributions(O) to(O) the(O) early(O) development(O) of(O) Quantum(B-theory) Field(I-theory) Theory(I-theory) with(O) J.(B-scientist) Robert(I-scientist) Oppenheimer(I-scientist) ,(O) Vladimir(B-scientist) Fock(I-scientist) ,(O) and(O) others(O) .(O)"}}
{"id": "329", "dataset": "crossner_science", "split": "test", "label_list": ["location", "person", "organization", "country", "chemical compound", "academic journal", "theory", "scientist", "protein", "award", "university", "chemical element", "event", "discipline", "astronomical object", "enzyme"], "instance": {"id": "329", "words": [";", "the", "most", "exclusive", "sports", "clubs", ":", "CCI", ",", "The", "Willingdon", "Sports", "Club", "as", "well", "as", "Bombay", "Gymkhana", "and", "the", "most", "expensive", "hospitals", ":", "Breach", "Candy", "Hospital", ",", "Bombay", "Hospital", ",", "Jaslok", "Hospital", "and", "Hurkisondas", "Hospital", ";", "in", "the", "nation", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, organization, country, chemical compound, academic journal, theory, scientist, protein, award, university, chemical element, event, discipline, astronomical object, enzyme and O.\nSentence: ; the most exclusive sports clubs : CCI , The Willingdon Sports Club as well as Bombay Gymkhana and the most expensive hospitals : Breach Candy Hospital , Bombay Hospital , Jaslok Hospital and Hurkisondas Hospital ; in the nation .", "prompt_labels": ";(O) the(O) most(O) exclusive(O) sports(O) clubs(O) :(O) CCI(B-organization) ,(O) The(B-organization) Willingdon(I-organization) Sports(I-organization) Club(I-organization) as(O) well(O) as(O) Bombay(B-organization) Gymkhana(I-organization) and(O) the(O) most(O) expensive(O) hospitals(O) :(O) Breach(B-location) Candy(I-location) Hospital(I-location) ,(O) Bombay(B-location) Hospital(I-location) ,(O) Jaslok(B-location) Hospital(I-location) and(O) Hurkisondas(B-organization) Hospital(I-organization) ;(O) in(O) the(O) nation(O) .(O)"}}
{"id": "314", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "chemical element", "country", "chemical compound", "organization", "event", "award", "person", "scientist", "university", "enzyme", "academic journal", "location", "discipline", "astronomical object", "protein"], "instance": {"id": "314", "words": ["He", "was", "named", "one", "of", "the", "most", "valued", "reviewers", "of", "2010", "by", "the", "Editors", "of", "Elsevier", "and", "Nuclear", "Physics", "Other", "journals", "for", "which", "Dr.", "Poenaru", "peer", "review", "ed", "articles", "include", "Physical", "Review", "Letters", ",", "Physical", "Review", "C", ",", "Journal", "of", "Physics", "G", ":", "Nuclear", "and", "Particle", "Physics", "and", "Canadian", "Journal", "of", "Physics", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "B-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "B-scientist", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, chemical element, country, chemical compound, organization, event, award, person, scientist, university, enzyme, academic journal, location, discipline, astronomical object, protein and O.\nSentence: He was named one of the most valued reviewers of 2010 by the Editors of Elsevier and Nuclear Physics Other journals for which Dr. Poenaru peer review ed articles include Physical Review Letters , Physical Review C , Journal of Physics G : Nuclear and Particle Physics and Canadian Journal of Physics .", "prompt_labels": "He(O) was(O) named(O) one(O) of(O) the(O) most(O) valued(O) reviewers(O) of(O) 2010(O) by(O) the(O) Editors(O) of(O) Elsevier(B-organization) and(O) Nuclear(B-academic journal) Physics(I-academic journal) Other(O) journals(O) for(O) which(O) Dr.(O) Poenaru(B-scientist) peer(O) review(O) ed(O) articles(O) include(O) Physical(B-academic journal) Review(I-academic journal) Letters(I-academic journal) ,(O) Physical(B-academic journal) Review(I-academic journal) C(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Physics(I-academic journal) G(I-academic journal) :(O) Nuclear(B-academic journal) and(I-academic journal) Particle(I-academic journal) Physics(I-academic journal) and(O) Canadian(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physics(I-academic journal) .(O)"}}
{"id": "113", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "event", "person", "location", "country", "scientist", "protein", "organization", "award", "chemical compound", "astronomical object", "discipline", "academic journal", "chemical element", "university", "theory"], "instance": {"id": "113", "words": ["The", "three", "required", "enzyme", "activities", "are", ":", "exonuclease", ",", "DNA", "polymerase", ",", "and", "DNA", "ligase", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-enzyme", "O", "B-enzyme", "I-enzyme", "O", "O", "B-enzyme", "I-enzyme", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, event, person, location, country, scientist, protein, organization, award, chemical compound, astronomical object, discipline, academic journal, chemical element, university, theory and O.\nSentence: The three required enzyme activities are : exonuclease , DNA polymerase , and DNA ligase .", "prompt_labels": "The(O) three(O) required(O) enzyme(O) activities(O) are(O) :(O) exonuclease(B-enzyme) ,(O) DNA(B-enzyme) polymerase(I-enzyme) ,(O) and(O) DNA(B-enzyme) ligase(I-enzyme) .(O)"}}
{"id": "486", "dataset": "crossner_science", "split": "test", "label_list": ["university", "scientist", "location", "protein", "chemical element", "academic journal", "enzyme", "event", "theory", "award", "country", "organization", "discipline", "person", "chemical compound", "astronomical object"], "instance": {"id": "486", "words": ["It", "was", "discovered", "on", "19", "September", "1973", ",", "by", "Dutch", "astronomers", "Ingrid", "van", "Houten-Groeneveld", "and", "Cornelis", "van", "Houten", "at", "Leiden", ",", "on", "photographic", "plates", "taken", "by", "Tom", "Gehrels", "at", "the", "Palomar", "Observatory", "in", "California", ",", "United", "States", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "B-location", "I-location", "O", "B-location", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, scientist, location, protein, chemical element, academic journal, enzyme, event, theory, award, country, organization, discipline, person, chemical compound, astronomical object and O.\nSentence: It was discovered on 19 September 1973 , by Dutch astronomers Ingrid van Houten-Groeneveld and Cornelis van Houten at Leiden , on photographic plates taken by Tom Gehrels at the Palomar Observatory in California , United States .", "prompt_labels": "It(O) was(O) discovered(O) on(O) 19(O) September(O) 1973(O) ,(O) by(O) Dutch(O) astronomers(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Cornelis(B-scientist) van(I-scientist) Houten(I-scientist) at(O) Leiden(B-location) ,(O) on(O) photographic(O) plates(O) taken(O) by(O) Tom(B-scientist) Gehrels(I-scientist) at(O) the(O) Palomar(B-location) Observatory(I-location) in(O) California(B-location) ,(O) United(B-country) States(I-country) .(O)"}}
{"id": "126", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "academic journal", "university", "organization", "discipline", "protein", "chemical compound", "astronomical object", "country", "person", "event", "location", "chemical element", "award", "scientist", "enzyme"], "instance": {"id": "126", "words": ["Beyond", "the", "Earth", "'s", "atmosphere", ",", "thunderstorms", "have", "also", "been", "observed", "on", "the", "planets", "of", "Jupiter", ",", "Saturn", ",", "Neptune", ",", "and", ",", "probably", ",", "Venus", "."], "labels": ["O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, academic journal, university, organization, discipline, protein, chemical compound, astronomical object, country, person, event, location, chemical element, award, scientist, enzyme and O.\nSentence: Beyond the Earth 's atmosphere , thunderstorms have also been observed on the planets of Jupiter , Saturn , Neptune , and , probably , Venus .", "prompt_labels": "Beyond(O) the(O) Earth(B-astronomical object) 's(O) atmosphere(O) ,(O) thunderstorms(O) have(O) also(O) been(O) observed(O) on(O) the(O) planets(O) of(O) Jupiter(B-astronomical object) ,(O) Saturn(B-astronomical object) ,(O) Neptune(B-astronomical object) ,(O) and(O) ,(O) probably(O) ,(O) Venus(B-astronomical object) .(O)"}}
{"id": "364", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "chemical compound", "university", "country", "organization", "discipline", "theory", "chemical element", "person", "protein", "scientist", "event", "location", "academic journal", "award", "astronomical object"], "instance": {"id": "364", "words": ["The", "2nd", "group", "of", "anions", "consist", "of", "Chloride", ",", "Bromide", ",", "Iodide", ",", "Nitrate", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical compound, university, country, organization, discipline, theory, chemical element, person, protein, scientist, event, location, academic journal, award, astronomical object and O.\nSentence: The 2nd group of anions consist of Chloride , Bromide , Iodide , Nitrate .", "prompt_labels": "The(O) 2nd(O) group(O) of(O) anions(O) consist(O) of(O) Chloride(B-chemical compound) ,(O) Bromide(B-chemical compound) ,(O) Iodide(B-chemical compound) ,(O) Nitrate(B-chemical compound) .(O)"}}
{"id": "450", "dataset": "crossner_science", "split": "test", "label_list": ["discipline", "event", "person", "chemical compound", "location", "enzyme", "astronomical object", "award", "university", "organization", "country", "academic journal", "chemical element", "scientist", "theory", "protein"], "instance": {"id": "450", "words": ["Wilson", "won", "gold", "medals", "in", "the", "800", "metres", "at", "both", "the", "2011", "World", "Youth", "Championships", "in", "Athletics", "and", "the", "2012", "World", "Junior", "Championships", "in", "Athletics", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, event, person, chemical compound, location, enzyme, astronomical object, award, university, organization, country, academic journal, chemical element, scientist, theory, protein and O.\nSentence: Wilson won gold medals in the 800 metres at both the 2011 World Youth Championships in Athletics and the 2012 World Junior Championships in Athletics .", "prompt_labels": "Wilson(B-person) won(O) gold(O) medals(O) in(O) the(O) 800(O) metres(O) at(O) both(O) the(O) 2011(B-event) World(I-event) Youth(I-event) Championships(I-event) in(I-event) Athletics(I-event) and(O) the(O) 2012(B-event) World(I-event) Junior(I-event) Championships(I-event) in(I-event) Athletics(I-event) .(O)"}}
{"id": "366", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "protein", "location", "award", "country", "chemical compound", "person", "scientist", "theory", "astronomical object", "enzyme", "discipline", "academic journal", "organization", "university", "event"], "instance": {"id": "366", "words": ["It", "works", "as", "a", "transmitter", "from", "and", "to", "the", "tight", "junction", ",", "because", "of", "its", "association", "with", "signaling", "molecules", "(", "Phosphoinositide", "3-kinase", ",", "Protein", "kinase", "C", ",", "YES", ",", "Protein", "phosphatase", "2", ",", "1", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "B-enzyme", "I-enzyme", "I-enzyme", "O", "B-enzyme", "O", "B-enzyme", "I-enzyme", "I-enzyme", "I-enzyme", "I-enzyme", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, protein, location, award, country, chemical compound, person, scientist, theory, astronomical object, enzyme, discipline, academic journal, organization, university, event and O.\nSentence: It works as a transmitter from and to the tight junction , because of its association with signaling molecules ( Phosphoinositide 3-kinase , Protein kinase C , YES , Protein phosphatase 2 , 1 ) .", "prompt_labels": "It(O) works(O) as(O) a(O) transmitter(O) from(O) and(O) to(O) the(O) tight(O) junction(O) ,(O) because(O) of(O) its(O) association(O) with(O) signaling(O) molecules(O) ((O) Phosphoinositide(B-enzyme) 3-kinase(I-enzyme) ,(O) Protein(B-enzyme) kinase(I-enzyme) C(I-enzyme) ,(O) YES(B-enzyme) ,(O) Protein(B-enzyme) phosphatase(I-enzyme) 2(I-enzyme) ,(I-enzyme) 1(I-enzyme) )(O) .(O)"}}
{"id": "390", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "theory", "chemical compound", "protein", "chemical element", "award", "astronomical object", "location", "person", "country", "university", "discipline", "event", "scientist", "enzyme", "organization"], "instance": {"id": "390", "words": ["This", "was", "shown", "by", "Artur", "Avila", "and", "Svetlana", "Jitomirskaya", "solving", "the", "by-then", "famous", "ten", "martini", "problem", "with", "respect", "to", "the", "parameters", ")", "."], "labels": ["O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, theory, chemical compound, protein, chemical element, award, astronomical object, location, person, country, university, discipline, event, scientist, enzyme, organization and O.\nSentence: This was shown by Artur Avila and Svetlana Jitomirskaya solving the by-then famous ten martini problem with respect to the parameters ) .", "prompt_labels": "This(O) was(O) shown(O) by(O) Artur(B-scientist) Avila(I-scientist) and(O) Svetlana(B-scientist) Jitomirskaya(I-scientist) solving(O) the(O) by-then(O) famous(O) ten(O) martini(O) problem(O) with(O) respect(O) to(O) the(O) parameters(O) )(O) .(O)"}}
{"id": "348", "dataset": "crossner_science", "split": "test", "label_list": ["country", "chemical element", "person", "location", "scientist", "event", "discipline", "theory", "protein", "organization", "chemical compound", "academic journal", "enzyme", "award", "astronomical object", "university"], "instance": {"id": "348", "words": ["LINE1", "transposable", "elements", "have", "been", "identified", "as", "targets", "for", "ionizing", "radiation", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, chemical element, person, location, scientist, event, discipline, theory, protein, organization, chemical compound, academic journal, enzyme, award, astronomical object, university and O.\nSentence: LINE1 transposable elements have been identified as targets for ionizing radiation .", "prompt_labels": "LINE1(O) transposable(O) elements(O) have(O) been(O) identified(O) as(O) targets(O) for(O) ionizing(O) radiation(O) .(O)"}}
{"id": "108", "dataset": "crossner_science", "split": "test", "label_list": ["person", "university", "country", "discipline", "chemical element", "theory", "enzyme", "academic journal", "chemical compound", "location", "organization", "scientist", "protein", "astronomical object", "award", "event"], "instance": {"id": "108", "words": ["Additionally", ",", "she", "has", "received", "five", "Academy", "of", "Country", "Music", "Awards", ",", "six", "Country", "Music", "Association", "Awards", ",", "and", "a", "Grammy", "Award", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, university, country, discipline, chemical element, theory, enzyme, academic journal, chemical compound, location, organization, scientist, protein, astronomical object, award, event and O.\nSentence: Additionally , she has received five Academy of Country Music Awards , six Country Music Association Awards , and a Grammy Award .", "prompt_labels": "Additionally(O) ,(O) she(O) has(O) received(O) five(O) Academy(B-award) of(I-award) Country(I-award) Music(I-award) Awards(I-award) ,(O) six(O) Country(B-award) Music(I-award) Association(I-award) Awards(I-award) ,(O) and(O) a(O) Grammy(B-award) Award(I-award) .(O)"}}
{"id": "495", "dataset": "crossner_science", "split": "test", "label_list": ["country", "location", "theory", "enzyme", "organization", "chemical element", "discipline", "academic journal", "person", "astronomical object", "chemical compound", "protein", "university", "event", "award", "scientist"], "instance": {"id": "495", "words": ["Amphitrite", "(", "minor", "planet", "designation", ":", "29", "Amphridite", ")", "is", "one", "of", "the", "largest", "S-type", "asteroid", "s", ",", "approximately", "in", "diameter", ",", "and", "probably", "third", "largest", "after", "15", "Eunomia", "and", "3", "Juno", ",", "although", "7", "Iris", "and", "532", "Herculina", "are", "similar", "in", "size", "."], "labels": ["B-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, theory, enzyme, organization, chemical element, discipline, academic journal, person, astronomical object, chemical compound, protein, university, event, award, scientist and O.\nSentence: Amphitrite ( minor planet designation : 29 Amphridite ) is one of the largest S-type asteroid s , approximately in diameter , and probably third largest after 15 Eunomia and 3 Juno , although 7 Iris and 532 Herculina are similar in size .", "prompt_labels": "Amphitrite(B-astronomical object) ((O) minor(O) planet(O) designation(O) :(O) 29(B-astronomical object) Amphridite(I-astronomical object) )(O) is(O) one(O) of(O) the(O) largest(O) S-type(O) asteroid(O) s(O) ,(O) approximately(O) in(O) diameter(O) ,(O) and(O) probably(O) third(O) largest(O) after(O) 15(B-astronomical object) Eunomia(I-astronomical object) and(O) 3(B-astronomical object) Juno(I-astronomical object) ,(O) although(O) 7(B-astronomical object) Iris(I-astronomical object) and(O) 532(B-astronomical object) Herculina(I-astronomical object) are(O) similar(O) in(O) size(O) .(O)"}}
{"id": "132", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "enzyme", "astronomical object", "theory", "award", "event", "scientist", "chemical compound", "academic journal", "person", "location", "country", "university", "chemical element", "organization", "discipline"], "instance": {"id": "132", "words": ["Camilla", "is", "the", "6th", "trinary", "asteroid", "that", "has", "been", "discovered", "in", "the", "asteroid", "belt", ",", "after", "87", "Sylvia", ",", "45", "Eugenia", ",", "216", "Kleopatra", ",", "93", "Minerva", "and", "130", "Elektra", "."], "labels": ["B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, enzyme, astronomical object, theory, award, event, scientist, chemical compound, academic journal, person, location, country, university, chemical element, organization, discipline and O.\nSentence: Camilla is the 6th trinary asteroid that has been discovered in the asteroid belt , after 87 Sylvia , 45 Eugenia , 216 Kleopatra , 93 Minerva and 130 Elektra .", "prompt_labels": "Camilla(B-astronomical object) is(O) the(O) 6th(O) trinary(O) asteroid(O) that(O) has(O) been(O) discovered(O) in(O) the(O) asteroid(O) belt(O) ,(O) after(O) 87(B-astronomical object) Sylvia(I-astronomical object) ,(O) 45(B-astronomical object) Eugenia(I-astronomical object) ,(O) 216(B-astronomical object) Kleopatra(I-astronomical object) ,(O) 93(B-astronomical object) Minerva(I-astronomical object) and(O) 130(B-astronomical object) Elektra(I-astronomical object) .(O)"}}
{"id": "391", "dataset": "crossner_science", "split": "test", "label_list": ["award", "enzyme", "astronomical object", "discipline", "organization", "protein", "theory", "event", "scientist", "university", "country", "person", "academic journal", "chemical compound", "location", "chemical element"], "instance": {"id": "391", "words": ["Uranus", "and", "4", "Vesta", "had", "most", "probably", "been", "seen", "but", "could", "not", "be", "recognized", "as", "planets", "because", "they", "appear", "so", "faint", "even", "at", "maximum", "brightness", ";", "Uranus", "'", "magnitude", "varies", "from", "+", "5.3supm", "/", "sup", "to", "+", "5.9supm", "/", "sup", ",", "and", "Vesta", "'s", "from", "+", "5.2supm", "/", "sup", "to", "+", "8.5supm", "/", "sup", "(", "so", "that", "it", "is", "only", "visible", "near", "its", "opposition", "dates", ")", "."], "labels": ["B-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, enzyme, astronomical object, discipline, organization, protein, theory, event, scientist, university, country, person, academic journal, chemical compound, location, chemical element and O.\nSentence: Uranus and 4 Vesta had most probably been seen but could not be recognized as planets because they appear so faint even at maximum brightness ; Uranus ' magnitude varies from + 5.3supm / sup to + 5.9supm / sup , and Vesta 's from + 5.2supm / sup to + 8.5supm / sup ( so that it is only visible near its opposition dates ) .", "prompt_labels": "Uranus(B-astronomical object) and(O) 4(B-astronomical object) Vesta(I-astronomical object) had(O) most(O) probably(O) been(O) seen(O) but(O) could(O) not(O) be(O) recognized(O) as(O) planets(O) because(O) they(O) appear(O) so(O) faint(O) even(O) at(O) maximum(O) brightness(O) ;(O) Uranus(B-astronomical object) '(O) magnitude(O) varies(O) from(O) +(O) 5.3supm(O) /(O) sup(O) to(O) +(O) 5.9supm(O) /(O) sup(O) ,(O) and(O) Vesta(B-astronomical object) 's(O) from(O) +(O) 5.2supm(O) /(O) sup(O) to(O) +(O) 8.5supm(O) /(O) sup(O) ((O) so(O) that(O) it(O) is(O) only(O) visible(O) near(O) its(O) opposition(O) dates(O) )(O) .(O)"}}
{"id": "481", "dataset": "crossner_science", "split": "test", "label_list": ["country", "chemical compound", "theory", "astronomical object", "chemical element", "award", "university", "discipline", "academic journal", "scientist", "person", "organization", "event", "enzyme", "location", "protein"], "instance": {"id": "481", "words": ["This", "would", "be", "achieved", "using", "biotechnology", "methods", "to", "confer", "to", "them", "the", "capacity", "to", "alter", "nucleotides", "in", "the", "chromosomes", "of", "infected", "individuals", "through", "sequence-specific", "editing", "systems", "like", "CRISPR", ",", "ZFN", "or", "TALEN", "."], "labels": ["O", "O", "O", "O", "O", "B-discipline", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, chemical compound, theory, astronomical object, chemical element, award, university, discipline, academic journal, scientist, person, organization, event, enzyme, location, protein and O.\nSentence: This would be achieved using biotechnology methods to confer to them the capacity to alter nucleotides in the chromosomes of infected individuals through sequence-specific editing systems like CRISPR , ZFN or TALEN .", "prompt_labels": "This(O) would(O) be(O) achieved(O) using(O) biotechnology(B-discipline) methods(O) to(O) confer(O) to(O) them(O) the(O) capacity(O) to(O) alter(O) nucleotides(B-chemical compound) in(O) the(O) chromosomes(O) of(O) infected(O) individuals(O) through(O) sequence-specific(O) editing(O) systems(O) like(O) CRISPR(O) ,(O) ZFN(O) or(O) TALEN(O) .(O)"}}
{"id": "456", "dataset": "crossner_science", "split": "test", "label_list": ["event", "location", "country", "protein", "scientist", "academic journal", "enzyme", "discipline", "person", "astronomical object", "chemical element", "university", "award", "theory", "organization", "chemical compound"], "instance": {"id": "456", "words": ["A", "conjunction", "of", "Venus", "and", "Jupiter", "occurred", "on", "1", "December", "2008", ",", "and", "several", "hours", "later", "both", "planets", "separately", "reached", "conjunction", "with", "the", "crescent", "Moon", "."], "labels": ["O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, country, protein, scientist, academic journal, enzyme, discipline, person, astronomical object, chemical element, university, award, theory, organization, chemical compound and O.\nSentence: A conjunction of Venus and Jupiter occurred on 1 December 2008 , and several hours later both planets separately reached conjunction with the crescent Moon .", "prompt_labels": "A(O) conjunction(O) of(O) Venus(B-astronomical object) and(O) Jupiter(B-astronomical object) occurred(O) on(O) 1(O) December(O) 2008(O) ,(O) and(O) several(O) hours(O) later(O) both(O) planets(O) separately(O) reached(O) conjunction(O) with(O) the(O) crescent(O) Moon(B-astronomical object) .(O)"}}
{"id": "374", "dataset": "crossner_science", "split": "test", "label_list": ["chemical compound", "event", "scientist", "enzyme", "chemical element", "location", "person", "country", "protein", "organization", "university", "award", "theory", "academic journal", "discipline", "astronomical object"], "instance": {"id": "374", "words": ["Assisted", "by", "activists", "from", "Southern", "Christian", "Leadership", "Conference", ",", "Congress", "of", "Racial", "Equality", ",", "SNCC", ",", "and", "the", "NAACP", ",", "African", "Americans", "and", "supporters", "took", "a", "stand", "to", "fight", "segregation", "through", "nonviolence", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, event, scientist, enzyme, chemical element, location, person, country, protein, organization, university, award, theory, academic journal, discipline, astronomical object and O.\nSentence: Assisted by activists from Southern Christian Leadership Conference , Congress of Racial Equality , SNCC , and the NAACP , African Americans and supporters took a stand to fight segregation through nonviolence .", "prompt_labels": "Assisted(O) by(O) activists(O) from(O) Southern(B-organization) Christian(I-organization) Leadership(I-organization) Conference(I-organization) ,(O) Congress(B-organization) of(I-organization) Racial(I-organization) Equality(I-organization) ,(O) SNCC(B-organization) ,(O) and(O) the(O) NAACP(B-organization) ,(O) African(O) Americans(O) and(O) supporters(O) took(O) a(O) stand(O) to(O) fight(O) segregation(O) through(O) nonviolence(O) .(O)"}}
{"id": "136", "dataset": "crossner_science", "split": "test", "label_list": ["award", "person", "theory", "country", "academic journal", "discipline", "chemical element", "location", "organization", "scientist", "protein", "university", "astronomical object", "event", "enzyme", "chemical compound"], "instance": {"id": "136", "words": ["It", "can", "simulate", "BOD", "/", "DO", ",", "Ammonia", ",", "Nitrate", ",", "Eutrophication", ",", "heavy", "metals", "and", "Wetland", "s", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, theory, country, academic journal, discipline, chemical element, location, organization, scientist, protein, university, astronomical object, event, enzyme, chemical compound and O.\nSentence: It can simulate BOD / DO , Ammonia , Nitrate , Eutrophication , heavy metals and Wetland s .", "prompt_labels": "It(O) can(O) simulate(O) BOD(O) /(O) DO(O) ,(O) Ammonia(B-chemical compound) ,(O) Nitrate(B-chemical compound) ,(O) Eutrophication(B-chemical compound) ,(O) heavy(O) metals(O) and(O) Wetland(O) s(O) .(O)"}}
{"id": "350", "dataset": "crossner_science", "split": "test", "label_list": ["country", "discipline", "location", "event", "award", "academic journal", "enzyme", "university", "astronomical object", "scientist", "protein", "chemical element", "organization", "theory", "person", "chemical compound"], "instance": {"id": "350", "words": ["Chinese", "Physics", "Letters", "is", "a", "part", "of", "a", "small", "group", "of", "four", "journals", "from", "the", "Chinese", "Physical", "Society", ",", "the", "other", "three", "are", ":", "Communications", "in", "Theoretical", "Physics", "(", "in", "English", ",", "subtitled", "Chinese", "Physics", "A", ")", ",", "Chinese", "Physics", "B", "(", "in", "English", ")", ",", "and", "Chinese", "Physics", "C", "(", "in", "English", ")", "."], "labels": ["B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, discipline, location, event, award, academic journal, enzyme, university, astronomical object, scientist, protein, chemical element, organization, theory, person, chemical compound and O.\nSentence: Chinese Physics Letters is a part of a small group of four journals from the Chinese Physical Society , the other three are : Communications in Theoretical Physics ( in English , subtitled Chinese Physics A ) , Chinese Physics B ( in English ) , and Chinese Physics C ( in English ) .", "prompt_labels": "Chinese(B-academic journal) Physics(I-academic journal) Letters(I-academic journal) is(O) a(O) part(O) of(O) a(O) small(O) group(O) of(O) four(O) journals(O) from(O) the(O) Chinese(B-organization) Physical(I-organization) Society(I-organization) ,(O) the(O) other(O) three(O) are(O) :(O) Communications(B-academic journal) in(I-academic journal) Theoretical(I-academic journal) Physics(I-academic journal) ((O) in(O) English(O) ,(O) subtitled(O) Chinese(B-academic journal) Physics(I-academic journal) A(I-academic journal) )(O) ,(O) Chinese(B-academic journal) Physics(I-academic journal) B(I-academic journal) ((O) in(O) English(O) )(O) ,(O) and(O) Chinese(B-academic journal) Physics(I-academic journal) C(I-academic journal) ((O) in(O) English(O) )(O) .(O)"}}
{"id": "205", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "chemical element", "country", "chemical compound", "award", "person", "location", "scientist", "academic journal", "university", "discipline", "astronomical object", "theory", "enzyme", "organization", "event"], "instance": {"id": "205", "words": ["His", "work", "has", "appeared", "in", "leading", "scientific", "journals", "such", "as", "Nature", ",", "Science", ",", "Nature", "Genetics", ",", "Molecular", "Biology", "and", "Evolution", ",", "Journal", "of", "Evolutionary", "Biology", ",", "as", "well", "as", "popular", "magazines", "such", "as", "Scientific", "American", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "O", "B-academic journal", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, chemical element, country, chemical compound, award, person, location, scientist, academic journal, university, discipline, astronomical object, theory, enzyme, organization, event and O.\nSentence: His work has appeared in leading scientific journals such as Nature , Science , Nature Genetics , Molecular Biology and Evolution , Journal of Evolutionary Biology , as well as popular magazines such as Scientific American .", "prompt_labels": "His(O) work(O) has(O) appeared(O) in(O) leading(O) scientific(O) journals(O) such(O) as(O) Nature(B-academic journal) ,(O) Science(B-academic journal) ,(O) Nature(B-academic journal) Genetics(I-academic journal) ,(O) Molecular(B-academic journal) Biology(I-academic journal) and(I-academic journal) Evolution(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Evolutionary(I-academic journal) Biology(I-academic journal) ,(O) as(O) well(O) as(O) popular(O) magazines(O) such(O) as(O) Scientific(B-academic journal) American(I-academic journal) .(O)"}}
{"id": "467", "dataset": "crossner_science", "split": "test", "label_list": ["chemical compound", "organization", "chemical element", "theory", "enzyme", "country", "award", "astronomical object", "scientist", "event", "discipline", "location", "university", "academic journal", "protein", "person"], "instance": {"id": "467", "words": ["Friedmann", "Peak", ",", "in", "the", "Darwin", "Mountains", "of", "Antarctica", ",", "where", "she", "co-discovered", "endolith", "ic", "microorganisms", "in", "the", "Beacon", "sandstone", ",", "is", "named", "after", "her", "."], "labels": ["B-location", "I-location", "O", "O", "O", "B-location", "I-location", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, organization, chemical element, theory, enzyme, country, award, astronomical object, scientist, event, discipline, location, university, academic journal, protein, person and O.\nSentence: Friedmann Peak , in the Darwin Mountains of Antarctica , where she co-discovered endolith ic microorganisms in the Beacon sandstone , is named after her .", "prompt_labels": "Friedmann(B-location) Peak(I-location) ,(O) in(O) the(O) Darwin(B-location) Mountains(I-location) of(O) Antarctica(B-country) ,(O) where(O) she(O) co-discovered(O) endolith(O) ic(O) microorganisms(O) in(O) the(O) Beacon(B-location) sandstone(I-location) ,(O) is(O) named(O) after(O) her(O) .(O)"}}
{"id": "170", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "location", "country", "organization", "event", "chemical element", "university", "astronomical object", "chemical compound", "protein", "discipline", "academic journal", "scientist", "enzyme", "award", "person"], "instance": {"id": "170", "words": ["In", "December", "2015", ",", "it", "was", "recognized", "as", "one", "of", "four", "new", "elements", "by", "the", "Joint", "Working", "Party", "of", "international", "scientific", "bodies", "International", "Union", "of", "Pure", "and", "Applied", "Chemistry", "and", "International", "Union", "of", "Pure", "and", "Applied", "Physics", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, location, country, organization, event, chemical element, university, astronomical object, chemical compound, protein, discipline, academic journal, scientist, enzyme, award, person and O.\nSentence: In December 2015 , it was recognized as one of four new elements by the Joint Working Party of international scientific bodies International Union of Pure and Applied Chemistry and International Union of Pure and Applied Physics .", "prompt_labels": "In(O) December(O) 2015(O) ,(O) it(O) was(O) recognized(O) as(O) one(O) of(O) four(O) new(O) elements(O) by(O) the(O) Joint(B-organization) Working(I-organization) Party(I-organization) of(O) international(O) scientific(O) bodies(O) International(B-organization) Union(I-organization) of(I-organization) Pure(I-organization) and(I-organization) Applied(I-organization) Chemistry(I-organization) and(O) International(B-organization) Union(I-organization) of(I-organization) Pure(I-organization) and(I-organization) Applied(I-organization) Physics(I-organization) .(O)"}}
{"id": "172", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "chemical element", "discipline", "event", "scientist", "university", "award", "chemical compound", "astronomical object", "academic journal", "country", "protein", "organization", "location", "theory", "person"], "instance": {"id": "172", "words": ["The", "invitees", "included", "Walther", "Bothe", ",", "Siegfried", "Flügge", ",", "Hans", "Geiger", ",", "Otto", "Hahn", ",", "Paul", "Harteck", ",", "Gerhard", "Hoffmann", ",", "Josef", "Mattauch", ",", "and", "Georg", "Stetter", "."], "labels": ["O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical element, discipline, event, scientist, university, award, chemical compound, astronomical object, academic journal, country, protein, organization, location, theory, person and O.\nSentence: The invitees included Walther Bothe , Siegfried Flügge , Hans Geiger , Otto Hahn , Paul Harteck , Gerhard Hoffmann , Josef Mattauch , and Georg Stetter .", "prompt_labels": "The(O) invitees(O) included(O) Walther(B-scientist) Bothe(I-scientist) ,(O) Siegfried(B-scientist) Flügge(I-scientist) ,(O) Hans(B-scientist) Geiger(I-scientist) ,(O) Otto(B-scientist) Hahn(I-scientist) ,(O) Paul(B-scientist) Harteck(I-scientist) ,(O) Gerhard(B-scientist) Hoffmann(I-scientist) ,(O) Josef(B-scientist) Mattauch(I-scientist) ,(O) and(O) Georg(B-scientist) Stetter(I-scientist) .(O)"}}
{"id": "65", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "astronomical object", "discipline", "organization", "location", "academic journal", "chemical element", "award", "event", "person", "enzyme", "country", "protein", "university", "scientist", "chemical compound"], "instance": {"id": "65", "words": ["The", "song", "was", "also", "nominated", "for", "a", "Golden", "Globe", "Award", "for", "Best", "Original", "Song", "and", "an", "Academy", "Award", "for", "Best", "Original", "Song", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, astronomical object, discipline, organization, location, academic journal, chemical element, award, event, person, enzyme, country, protein, university, scientist, chemical compound and O.\nSentence: The song was also nominated for a Golden Globe Award for Best Original Song and an Academy Award for Best Original Song .", "prompt_labels": "The(O) song(O) was(O) also(O) nominated(O) for(O) a(O) Golden(B-award) Globe(I-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Song(I-award) and(O) an(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Song(I-award) .(O)"}}
{"id": "278", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "location", "astronomical object", "academic journal", "scientist", "event", "discipline", "protein", "chemical compound", "organization", "award", "country", "person", "theory", "chemical element", "university"], "instance": {"id": "278", "words": ["Donna", "J.", "Haraway", "(", "born", "September", "6", ",", "1944", ")", "is", "an", "American", "Professor", "Emerita", "in", "the", "History", "of", "Consciousness", "Department", "and", "Feminist", "Studies", "Department", "at", "the", "University", "of", "California", ",", "Santa", "Cruz", ",", "United", "States", "."], "labels": ["B-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-university", "I-university", "I-university", "O", "B-location", "I-location", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, location, astronomical object, academic journal, scientist, event, discipline, protein, chemical compound, organization, award, country, person, theory, chemical element, university and O.\nSentence: Donna J. Haraway ( born September 6 , 1944 ) is an American Professor Emerita in the History of Consciousness Department and Feminist Studies Department at the University of California , Santa Cruz , United States .", "prompt_labels": "Donna(B-scientist) J.(I-scientist) Haraway(I-scientist) ((O) born(O) September(O) 6(O) ,(O) 1944(O) )(O) is(O) an(O) American(O) Professor(B-award) Emerita(I-award) in(O) the(O) History(B-organization) of(I-organization) Consciousness(I-organization) Department(I-organization) and(O) Feminist(B-organization) Studies(I-organization) Department(I-organization) at(O) the(O) University(B-university) of(I-university) California(I-university) ,(O) Santa(B-location) Cruz(I-location) ,(O) United(B-country) States(I-country) .(O)"}}
{"id": "466", "dataset": "crossner_science", "split": "test", "label_list": ["discipline", "theory", "enzyme", "protein", "event", "scientist", "astronomical object", "chemical compound", "chemical element", "academic journal", "country", "award", "organization", "person", "location", "university"], "instance": {"id": "466", "words": ["Her", "published", "work", "includes", "biographies", "of", "Marie", "Curie", ",", "Alexander", "Graham", "Bell", ",", "Niels", "Bohr", ",", "Isaac", "Newton", ",", "Albert", "Einstein", ",", "Linus", "Pauling", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, theory, enzyme, protein, event, scientist, astronomical object, chemical compound, chemical element, academic journal, country, award, organization, person, location, university and O.\nSentence: Her published work includes biographies of Marie Curie , Alexander Graham Bell , Niels Bohr , Isaac Newton , Albert Einstein , Linus Pauling .", "prompt_labels": "Her(O) published(O) work(O) includes(O) biographies(O) of(O) Marie(B-scientist) Curie(I-scientist) ,(O) Alexander(B-scientist) Graham(I-scientist) Bell(I-scientist) ,(O) Niels(B-scientist) Bohr(I-scientist) ,(O) Isaac(B-scientist) Newton(I-scientist) ,(O) Albert(B-scientist) Einstein(I-scientist) ,(O) Linus(B-scientist) Pauling(I-scientist) .(O)"}}
{"id": "536", "dataset": "crossner_science", "split": "test", "label_list": ["event", "enzyme", "discipline", "award", "location", "theory", "person", "organization", "country", "academic journal", "protein", "university", "astronomical object", "chemical element", "scientist", "chemical compound"], "instance": {"id": "536", "words": ["He", "was", "a", "member", "of", "the", "Experimental", "Aircraft", "Association", ",", "the", "Space", "Pioneers", ",", "the", "Confederate", "Air", "Force", ",", "the", "Order", "of", "Daedalians", ",", "the", "National", "Rifle", "Association", "of", "America", ",", "the", "Veterans", "of", "Foreign", "Wars", ",", "and", "the", "Fraternal", "Order", "of", "Eagles", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-country", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, enzyme, discipline, award, location, theory, person, organization, country, academic journal, protein, university, astronomical object, chemical element, scientist, chemical compound and O.\nSentence: He was a member of the Experimental Aircraft Association , the Space Pioneers , the Confederate Air Force , the Order of Daedalians , the National Rifle Association of America , the Veterans of Foreign Wars , and the Fraternal Order of Eagles .", "prompt_labels": "He(O) was(O) a(O) member(O) of(O) the(O) Experimental(B-organization) Aircraft(I-organization) Association(I-organization) ,(O) the(O) Space(B-organization) Pioneers(I-organization) ,(O) the(O) Confederate(B-organization) Air(I-organization) Force(I-organization) ,(O) the(O) Order(B-organization) of(I-organization) Daedalians(I-organization) ,(O) the(O) National(B-organization) Rifle(I-organization) Association(I-organization) of(O) America(B-country) ,(O) the(O) Veterans(B-organization) of(I-organization) Foreign(I-organization) Wars(I-organization) ,(O) and(O) the(O) Fraternal(B-organization) Order(I-organization) of(I-organization) Eagles(I-organization) .(O)"}}
{"id": "287", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "award", "organization", "location", "chemical compound", "person", "country", "discipline", "scientist", "academic journal", "event", "university", "chemical element", "protein", "theory", "enzyme"], "instance": {"id": "287", "words": ["Of", "the", "sympatric", "species", ",", "they", "all", "contain", "Egg", "lysin", "that", "drives", "gamete", "isolation", ",", "but", "the", "allopatric", "species", "does", "not", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, award, organization, location, chemical compound, person, country, discipline, scientist, academic journal, event, university, chemical element, protein, theory, enzyme and O.\nSentence: Of the sympatric species , they all contain Egg lysin that drives gamete isolation , but the allopatric species does not .", "prompt_labels": "Of(O) the(O) sympatric(O) species(O) ,(O) they(O) all(O) contain(O) Egg(B-protein) lysin(I-protein) that(O) drives(O) gamete(O) isolation(O) ,(O) but(O) the(O) allopatric(O) species(O) does(O) not(O) .(O)"}}
{"id": "346", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "event", "academic journal", "award", "discipline", "location", "university", "organization", "country", "scientist", "person", "protein", "chemical element", "chemical compound", "enzyme", "astronomical object"], "instance": {"id": "346", "words": ["During", "their", "maturation", "in", "the", "thymus", ",", "they", "undergo", "a", "process", "called", "V", "(", "D", ")", "J", "recombination", "which", "conducts", "the", "development", "of", "T-cell", "receptor", "(", "TCR", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O", "B-protein", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, event, academic journal, award, discipline, location, university, organization, country, scientist, person, protein, chemical element, chemical compound, enzyme, astronomical object and O.\nSentence: During their maturation in the thymus , they undergo a process called V ( D ) J recombination which conducts the development of T-cell receptor ( TCR ) .", "prompt_labels": "During(O) their(O) maturation(O) in(O) the(O) thymus(O) ,(O) they(O) undergo(O) a(O) process(O) called(O) V(O) ((O) D(O) )(O) J(O) recombination(O) which(O) conducts(O) the(O) development(O) of(O) T-cell(B-protein) receptor(I-protein) ((O) TCR(B-protein) )(O) .(O)"}}
{"id": "200", "dataset": "crossner_science", "split": "test", "label_list": ["discipline", "event", "organization", "protein", "person", "scientist", "theory", "enzyme", "chemical compound", "country", "award", "astronomical object", "university", "chemical element", "academic journal", "location"], "instance": {"id": "200", "words": ["The", "Arts", "Centre", "Melbourne", "(", "which", "includes", "the", "State", "Theatre", ",", "Hamer", "Hall", ",", "the", "Playhouse", "and", "the", "Fairfax", "Studio", ")", ",", "and", "the", "Melbourne", "Recital", "Centre", "are", "located", "just", "to", "the", "south", "of", "the", "CBD", ",", "with", "the", "Sidney", "Myer", "Music", "Bowl", "in", "parklands", "to", "the", "east", "."], "labels": ["O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "O", "B-location", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "B-location", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, event, organization, protein, person, scientist, theory, enzyme, chemical compound, country, award, astronomical object, university, chemical element, academic journal, location and O.\nSentence: The Arts Centre Melbourne ( which includes the State Theatre , Hamer Hall , the Playhouse and the Fairfax Studio ) , and the Melbourne Recital Centre are located just to the south of the CBD , with the Sidney Myer Music Bowl in parklands to the east .", "prompt_labels": "The(O) Arts(B-organization) Centre(I-organization) Melbourne(I-organization) ((O) which(O) includes(O) the(O) State(B-location) Theatre(I-location) ,(O) Hamer(B-location) Hall(I-location) ,(O) the(O) Playhouse(B-location) and(O) the(O) Fairfax(B-location) Studio(I-location) )(O) ,(O) and(O) the(O) Melbourne(B-location) Recital(I-location) Centre(I-location) are(O) located(O) just(O) to(O) the(O) south(O) of(O) the(O) CBD(B-location) ,(O) with(O) the(O) Sidney(B-location) Myer(I-location) Music(I-location) Bowl(I-location) in(O) parklands(B-location) to(O) the(O) east(O) .(O)"}}
{"id": "230", "dataset": "crossner_science", "split": "test", "label_list": ["university", "organization", "enzyme", "scientist", "discipline", "chemical compound", "astronomical object", "chemical element", "location", "person", "theory", "academic journal", "country", "award", "protein", "event"], "instance": {"id": "230", "words": ["His", "research", "in", "cognitive", "psychology", "has", "won", "the", "Early", "Career", "Award", "(", "1984", ")", "and", "Boyd", "McCandless", "Award", "(", "1986", ")", "from", "the", "American", "Psychological", "Association", ",", "the", "Troland", "Research", "Award", "(", "1993", ")", "from", "the", "National", "Academy", "of", "Sciences", ",", "the", "Henry", "Dale", "Prize", "(", "2004", ")", "from", "the", "Royal", "Institution", "of", "Great", "Britain", ",", "and", "the", "George", "Miller", "Prize", "(", "2010", ")", "from", "the", "Cognitive", "Neuroscience", "Society", "."], "labels": ["O", "O", "O", "B-discipline", "I-discipline", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, organization, enzyme, scientist, discipline, chemical compound, astronomical object, chemical element, location, person, theory, academic journal, country, award, protein, event and O.\nSentence: His research in cognitive psychology has won the Early Career Award ( 1984 ) and Boyd McCandless Award ( 1986 ) from the American Psychological Association , the Troland Research Award ( 1993 ) from the National Academy of Sciences , the Henry Dale Prize ( 2004 ) from the Royal Institution of Great Britain , and the George Miller Prize ( 2010 ) from the Cognitive Neuroscience Society .", "prompt_labels": "His(O) research(O) in(O) cognitive(B-discipline) psychology(I-discipline) has(O) won(O) the(O) Early(B-award) Career(I-award) Award(I-award) ((O) 1984(O) )(O) and(O) Boyd(B-award) McCandless(I-award) Award(I-award) ((O) 1986(O) )(O) from(O) the(O) American(B-organization) Psychological(I-organization) Association(I-organization) ,(O) the(O) Troland(B-award) Research(I-award) Award(I-award) ((O) 1993(O) )(O) from(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) the(O) Henry(B-award) Dale(I-award) Prize(I-award) ((O) 2004(O) )(O) from(O) the(O) Royal(B-organization) Institution(I-organization) of(I-organization) Great(I-organization) Britain(I-organization) ,(O) and(O) the(O) George(B-award) Miller(I-award) Prize(I-award) ((O) 2010(O) )(O) from(O) the(O) Cognitive(B-organization) Neuroscience(I-organization) Society(I-organization) .(O)"}}
{"id": "232", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "chemical compound", "academic journal", "location", "enzyme", "award", "university", "event", "organization", "discipline", "theory", "person", "astronomical object", "scientist", "country", "chemical element"], "instance": {"id": "232", "words": ["The", "episode", "was", "written", "by", "series", "creator", "Shonda", "Rhimes", "and", "directed", "by", "co-executive", "producer", "Peter", "Horton", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, chemical compound, academic journal, location, enzyme, award, university, event, organization, discipline, theory, person, astronomical object, scientist, country, chemical element and O.\nSentence: The episode was written by series creator Shonda Rhimes and directed by co-executive producer Peter Horton .", "prompt_labels": "The(O) episode(O) was(O) written(O) by(O) series(O) creator(O) Shonda(B-person) Rhimes(I-person) and(O) directed(O) by(O) co-executive(O) producer(O) Peter(B-person) Horton(I-person) .(O)"}}
{"id": "123", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "person", "theory", "enzyme", "university", "location", "organization", "protein", "country", "discipline", "scientist", "chemical compound", "academic journal", "event", "chemical element", "award"], "instance": {"id": "123", "words": ["The", "planets", "of", "the", "Solar", "System", "are", "divided", "into", "two", "groups", ":", "the", "four", "inner", "planets", "are", "the", "terrestrial", "planet", "s", "(", "Mercury", ",", "Venus", ",", "Earth", "and", "Mars", ")", ",", "with", "relatively", "small", "sizes", "and", "rocky", "surfaces", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, person, theory, enzyme, university, location, organization, protein, country, discipline, scientist, chemical compound, academic journal, event, chemical element, award and O.\nSentence: The planets of the Solar System are divided into two groups : the four inner planets are the terrestrial planet s ( Mercury , Venus , Earth and Mars ) , with relatively small sizes and rocky surfaces .", "prompt_labels": "The(O) planets(O) of(O) the(O) Solar(O) System(O) are(O) divided(O) into(O) two(O) groups(O) :(O) the(O) four(O) inner(O) planets(O) are(O) the(O) terrestrial(O) planet(O) s(O) ((O) Mercury(B-astronomical object) ,(O) Venus(B-astronomical object) ,(O) Earth(B-astronomical object) and(O) Mars(B-astronomical object) )(O) ,(O) with(O) relatively(O) small(O) sizes(O) and(O) rocky(O) surfaces(O) .(O)"}}
{"id": "151", "dataset": "crossner_science", "split": "test", "label_list": ["award", "enzyme", "chemical element", "academic journal", "country", "university", "discipline", "theory", "chemical compound", "scientist", "organization", "astronomical object", "location", "person", "protein", "event"], "instance": {"id": "151", "words": ["The", "story", "instead", "focuses", "on", "the", "Doctor", "'s", "companion", ",", "Donna", "Noble", "(", "Catherine", "Tate", ")", "and", "her", "encounters", "with", "former", "companion", "Rose", "Tyler", "(", "Billie", "Piper", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, enzyme, chemical element, academic journal, country, university, discipline, theory, chemical compound, scientist, organization, astronomical object, location, person, protein, event and O.\nSentence: The story instead focuses on the Doctor 's companion , Donna Noble ( Catherine Tate ) and her encounters with former companion Rose Tyler ( Billie Piper ) .", "prompt_labels": "The(O) story(O) instead(O) focuses(O) on(O) the(O) Doctor(O) 's(O) companion(O) ,(O) Donna(B-person) Noble(I-person) ((O) Catherine(B-person) Tate(I-person) )(O) and(O) her(O) encounters(O) with(O) former(O) companion(O) Rose(B-person) Tyler(I-person) ((O) Billie(B-person) Piper(I-person) )(O) .(O)"}}
{"id": "280", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "university", "discipline", "chemical element", "person", "location", "chemical compound", "award", "organization", "theory", "event", "country", "enzyme", "astronomical object", "scientist", "academic journal"], "instance": {"id": "280", "words": ["His", "research", "group", "'s", "developments", "in", "the", "solar", "and", "plasmonics", "field", "have", "been", "featured", "in", "Scientific", "American", "and", "in", "research", "papers", "in", "Science", ",", "Nature", "Materials", ",", "Nature", "Photonics", "and", "Advanced", "Materials", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-discipline", "I-discipline", "I-discipline", "I-discipline", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, university, discipline, chemical element, person, location, chemical compound, award, organization, theory, event, country, enzyme, astronomical object, scientist, academic journal and O.\nSentence: His research group 's developments in the solar and plasmonics field have been featured in Scientific American and in research papers in Science , Nature Materials , Nature Photonics and Advanced Materials .", "prompt_labels": "His(O) research(O) group(O) 's(O) developments(O) in(O) the(O) solar(B-discipline) and(I-discipline) plasmonics(I-discipline) field(I-discipline) have(O) been(O) featured(O) in(O) Scientific(O) American(O) and(O) in(O) research(O) papers(O) in(O) Science(B-academic journal) ,(O) Nature(B-academic journal) Materials(I-academic journal) ,(O) Nature(B-academic journal) Photonics(I-academic journal) and(O) Advanced(B-academic journal) Materials(I-academic journal) .(O)"}}
{"id": "503", "dataset": "crossner_science", "split": "test", "label_list": ["university", "event", "organization", "country", "chemical element", "award", "discipline", "theory", "chemical compound", "enzyme", "astronomical object", "scientist", "person", "location", "protein", "academic journal"], "instance": {"id": "503", "words": ["The", "2013", "Nobel", "Peace", "Prize", "was", "awarded", "to", "the", "organization", "because", "it", "had", ",", "with", "the", "Chemical", "Weapons", "Convention", ",", "defined", "the", "use", "of", "chemical", "weapons", "as", "a", "taboo", "under", "international", "law", "according", "to", "Thorbjørn", "Jagland", ",", "Chairman", "of", "the", "Norwegian", "Nobel", "Committee", "."], "labels": ["O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, event, organization, country, chemical element, award, discipline, theory, chemical compound, enzyme, astronomical object, scientist, person, location, protein, academic journal and O.\nSentence: The 2013 Nobel Peace Prize was awarded to the organization because it had , with the Chemical Weapons Convention , defined the use of chemical weapons as a taboo under international law according to Thorbjørn Jagland , Chairman of the Norwegian Nobel Committee .", "prompt_labels": "The(O) 2013(B-award) Nobel(I-award) Peace(I-award) Prize(I-award) was(O) awarded(O) to(O) the(O) organization(O) because(O) it(O) had(O) ,(O) with(O) the(O) Chemical(O) Weapons(O) Convention(O) ,(O) defined(O) the(O) use(O) of(O) chemical(O) weapons(O) as(O) a(O) taboo(O) under(O) international(O) law(O) according(O) to(O) Thorbjørn(B-person) Jagland(I-person) ,(O) Chairman(O) of(O) the(O) Norwegian(B-organization) Nobel(I-organization) Committee(I-organization) .(O)"}}
{"id": "421", "dataset": "crossner_science", "split": "test", "label_list": ["discipline", "scientist", "event", "person", "award", "chemical compound", "academic journal", "protein", "organization", "country", "theory", "university", "astronomical object", "enzyme", "chemical element", "location"], "instance": {"id": "421", "words": ["Interaction", "of", "PAMPs", "with", "PRPs", "(", "pattern-recognition", "proteins", ")", "activates", "a", "series", "of", "Serine", "protease", "and", "those", "proteolytically", "cleave", "the", "prophenoloxidase", "(", "proPO", ")", "zymogen", "and", "activate", "phenoxidase", "(", "PO", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, scientist, event, person, award, chemical compound, academic journal, protein, organization, country, theory, university, astronomical object, enzyme, chemical element, location and O.\nSentence: Interaction of PAMPs with PRPs ( pattern-recognition proteins ) activates a series of Serine protease and those proteolytically cleave the prophenoloxidase ( proPO ) zymogen and activate phenoxidase ( PO ) .", "prompt_labels": "Interaction(O) of(O) PAMPs(O) with(O) PRPs(O) ((O) pattern-recognition(O) proteins(O) )(O) activates(O) a(O) series(O) of(O) Serine(B-enzyme) protease(I-enzyme) and(O) those(O) proteolytically(O) cleave(O) the(O) prophenoloxidase(O) ((O) proPO(O) )(O) zymogen(O) and(O) activate(O) phenoxidase(O) ((O) PO(O) )(O) .(O)"}}
{"id": "331", "dataset": "crossner_science", "split": "test", "label_list": ["discipline", "astronomical object", "scientist", "chemical compound", "academic journal", "chemical element", "country", "person", "event", "organization", "protein", "enzyme", "university", "award", "location", "theory"], "instance": {"id": "331", "words": ["In", "season", "three", ",", "two", "actors", "were", "promoted", "from", "recurring", "to", "starring", "roles", ":", "Henry", "Ian", "Cusick", "as", "former", "Scottish", "soldier", "Desmond", "Hume", ";", "and", "Michael", "Emerson", "as", "the", "manipulative", "leader", "of", "the", "Others", ",", "Ben", "Linus", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, astronomical object, scientist, chemical compound, academic journal, chemical element, country, person, event, organization, protein, enzyme, university, award, location, theory and O.\nSentence: In season three , two actors were promoted from recurring to starring roles : Henry Ian Cusick as former Scottish soldier Desmond Hume ; and Michael Emerson as the manipulative leader of the Others , Ben Linus .", "prompt_labels": "In(O) season(O) three(O) ,(O) two(O) actors(O) were(O) promoted(O) from(O) recurring(O) to(O) starring(O) roles(O) :(O) Henry(B-person) Ian(I-person) Cusick(I-person) as(O) former(O) Scottish(O) soldier(O) Desmond(B-person) Hume(I-person) ;(O) and(O) Michael(B-person) Emerson(I-person) as(O) the(O) manipulative(O) leader(O) of(O) the(O) Others(O) ,(O) Ben(B-person) Linus(I-person) .(O)"}}
{"id": "121", "dataset": "crossner_science", "split": "test", "label_list": ["person", "enzyme", "academic journal", "astronomical object", "protein", "event", "university", "chemical compound", "country", "discipline", "location", "organization", "scientist", "award", "theory", "chemical element"], "instance": {"id": "121", "words": ["Lucifer", "was", "the", "name", "used", "by", "the", "Romans", "for", "the", "Venus", "and", "the", "goddess", "Venus", "."], "labels": ["B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, enzyme, academic journal, astronomical object, protein, event, university, chemical compound, country, discipline, location, organization, scientist, award, theory, chemical element and O.\nSentence: Lucifer was the name used by the Romans for the Venus and the goddess Venus .", "prompt_labels": "Lucifer(B-astronomical object) was(O) the(O) name(O) used(O) by(O) the(O) Romans(O) for(O) the(O) Venus(B-astronomical object) and(O) the(O) goddess(O) Venus(B-astronomical object) .(O)"}}
{"id": "217", "dataset": "crossner_science", "split": "test", "label_list": ["university", "astronomical object", "person", "location", "country", "protein", "award", "event", "organization", "chemical element", "academic journal", "theory", "chemical compound", "discipline", "enzyme", "scientist"], "instance": {"id": "217", "words": ["Witten", "has", "been", "honored", "with", "numerous", "awards", "including", "a", "MacArthur", "Grant", "(", "1982", ")", ",", "the", "Fields", "Medal", "(", "1990", ")", ",", "the", "Nemmers", "Prize", "in", "Mathematics", "(", "2000", ")", ",", "the", "National", "Medal", "of", "Science", ",", "retrieved", "2013-09-01", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, astronomical object, person, location, country, protein, award, event, organization, chemical element, academic journal, theory, chemical compound, discipline, enzyme, scientist and O.\nSentence: Witten has been honored with numerous awards including a MacArthur Grant ( 1982 ) , the Fields Medal ( 1990 ) , the Nemmers Prize in Mathematics ( 2000 ) , the National Medal of Science , retrieved 2013-09-01 .", "prompt_labels": "Witten(B-scientist) has(O) been(O) honored(O) with(O) numerous(O) awards(O) including(O) a(O) MacArthur(B-award) Grant(I-award) ((O) 1982(O) )(O) ,(O) the(O) Fields(B-award) Medal(I-award) ((O) 1990(O) )(O) ,(O) the(O) Nemmers(B-award) Prize(I-award) in(I-award) Mathematics(I-award) ((O) 2000(O) )(O) ,(O) the(O) National(B-award) Medal(I-award) of(I-award) Science(I-award) ,(O) retrieved(O) 2013-09-01(O) .(O)"}}
{"id": "349", "dataset": "crossner_science", "split": "test", "label_list": ["location", "protein", "enzyme", "university", "event", "discipline", "astronomical object", "country", "theory", "academic journal", "award", "scientist", "person", "organization", "chemical element", "chemical compound"], "instance": {"id": "349", "words": ["Staudinger", ",", "who", "initially", "wanted", "to", "become", "a", "botanist", ",", "studied", "chemistry", "at", "the", "Martin", "Luther", "University", "of", "Halle-Wittenberg", ",", "at", "the", "Technische", "Universität", "Darmstadt", "and", "at", "the", "Ludwig", "Maximilian", "University", "of", "Munich", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, protein, enzyme, university, event, discipline, astronomical object, country, theory, academic journal, award, scientist, person, organization, chemical element, chemical compound and O.\nSentence: Staudinger , who initially wanted to become a botanist , studied chemistry at the Martin Luther University of Halle-Wittenberg , at the Technische Universität Darmstadt and at the Ludwig Maximilian University of Munich .", "prompt_labels": "Staudinger(B-scientist) ,(O) who(O) initially(O) wanted(O) to(O) become(O) a(O) botanist(O) ,(O) studied(O) chemistry(B-discipline) at(O) the(O) Martin(B-university) Luther(I-university) University(I-university) of(I-university) Halle-Wittenberg(I-university) ,(O) at(O) the(O) Technische(B-university) Universität(I-university) Darmstadt(I-university) and(O) at(O) the(O) Ludwig(B-university) Maximilian(I-university) University(I-university) of(I-university) Munich(I-university) .(O)"}}
{"id": "316", "dataset": "crossner_science", "split": "test", "label_list": ["astronomical object", "award", "country", "chemical element", "academic journal", "scientist", "chemical compound", "protein", "theory", "university", "enzyme", "location", "organization", "discipline", "event", "person"], "instance": {"id": "316", "words": ["52872", "Okyrhoe", "is", "a", "centaur", "orbiting", "in", "the", "outer", "Solar", "System", "between", "Jupiter", "and", "Saturn", "."], "labels": ["B-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, award, country, chemical element, academic journal, scientist, chemical compound, protein, theory, university, enzyme, location, organization, discipline, event, person and O.\nSentence: 52872 Okyrhoe is a centaur orbiting in the outer Solar System between Jupiter and Saturn .", "prompt_labels": "52872(B-academic journal) Okyrhoe(I-academic journal) is(O) a(O) centaur(O) orbiting(O) in(O) the(O) outer(O) Solar(O) System(O) between(O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) .(O)"}}
{"id": "194", "dataset": "crossner_science", "split": "test", "label_list": ["event", "astronomical object", "organization", "protein", "enzyme", "scientist", "discipline", "person", "location", "university", "award", "theory", "country", "chemical compound", "chemical element", "academic journal"], "instance": {"id": "194", "words": ["The", "Copenhagen", "interpretation", "is", "an", "expression", "of", "the", "meaning", "of", "quantum", "mechanics", "that", "was", "largely", "devised", "from", "1925", "to", "1927", "by", "Niels", "Bohr", "and", "Werner", "Heisenberg", "."], "labels": ["O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "I-discipline", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, astronomical object, organization, protein, enzyme, scientist, discipline, person, location, university, award, theory, country, chemical compound, chemical element, academic journal and O.\nSentence: The Copenhagen interpretation is an expression of the meaning of quantum mechanics that was largely devised from 1925 to 1927 by Niels Bohr and Werner Heisenberg .", "prompt_labels": "The(O) Copenhagen(B-location) interpretation(O) is(O) an(O) expression(O) of(O) the(O) meaning(O) of(O) quantum(B-discipline) mechanics(I-discipline) that(O) was(O) largely(O) devised(O) from(O) 1925(O) to(O) 1927(O) by(O) Niels(B-scientist) Bohr(I-scientist) and(O) Werner(B-scientist) Heisenberg(I-scientist) .(O)"}}
{"id": "166", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "protein", "astronomical object", "chemical compound", "academic journal", "discipline", "location", "theory", "event", "scientist", "person", "award", "enzyme", "organization", "university", "country"], "instance": {"id": "166", "words": ["As", "demonstrated", "by", "Temin", "and", "Baltimore", ",", "who", "shared", "the", "Nobel", "Prize", "in", "Physiology", "or", "Medicine", "with", "Dulbecco", ",", "the", "transfer", "of", "viral", "genes", "to", "the", "cell", "is", "mediated", "by", "an", "enzyme", "called", "reverse", "transcriptase", "(", "or", ",", "more", "precisely", ",", "RNA-dependent", "DNA", "polymerase", ")", ",", "which", "replicates", "the", "viral", "genome", "(", "in", "this", "case", "made", "of", "RNA", ")", "into", "DNA", ",", "which", "is", "later", "incorporated", "in", "the", "host", "genome", "."], "labels": ["O", "O", "O", "B-scientist", "O", "B-scientist", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "I-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, protein, astronomical object, chemical compound, academic journal, discipline, location, theory, event, scientist, person, award, enzyme, organization, university, country and O.\nSentence: As demonstrated by Temin and Baltimore , who shared the Nobel Prize in Physiology or Medicine with Dulbecco , the transfer of viral genes to the cell is mediated by an enzyme called reverse transcriptase ( or , more precisely , RNA-dependent DNA polymerase ) , which replicates the viral genome ( in this case made of RNA ) into DNA , which is later incorporated in the host genome .", "prompt_labels": "As(O) demonstrated(O) by(O) Temin(B-scientist) and(O) Baltimore(B-scientist) ,(O) who(O) shared(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Physiology(I-award) or(I-award) Medicine(I-award) with(O) Dulbecco(B-scientist) ,(O) the(O) transfer(O) of(O) viral(O) genes(O) to(O) the(O) cell(O) is(O) mediated(O) by(O) an(O) enzyme(O) called(O) reverse(B-enzyme) transcriptase(I-enzyme) ((O) or(O) ,(O) more(O) precisely(O) ,(O) RNA-dependent(B-enzyme) DNA(I-enzyme) polymerase(I-enzyme) )(O) ,(O) which(O) replicates(O) the(O) viral(O) genome(O) ((O) in(O) this(O) case(O) made(O) of(O) RNA(O) )(O) into(O) DNA(O) ,(O) which(O) is(O) later(O) incorporated(O) in(O) the(O) host(O) genome(O) .(O)"}}
{"id": "77", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "event", "scientist", "chemical compound", "academic journal", "astronomical object", "chemical element", "university", "organization", "theory", "location", "country", "discipline", "award", "protein", "person"], "instance": {"id": "77", "words": ["Historian", "J.", "G.", "M.", "Ramsey", "reported", "a", "conference", "between", "militia", "commander", "John", "Sevier", "and", "Cherokee", "Chief", "Hanging", "Maw", "held", "at", "the", "original", "Citico", "in", "1782", "in", "which", "the", "two", "sides", "agreed", "to", "a", "truce", "."], "labels": ["O", "B-person", "I-person", "I-person", "I-person", "O", "O", "O", "O", "B-organization", "O", "B-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, event, scientist, chemical compound, academic journal, astronomical object, chemical element, university, organization, theory, location, country, discipline, award, protein, person and O.\nSentence: Historian J. G. M. Ramsey reported a conference between militia commander John Sevier and Cherokee Chief Hanging Maw held at the original Citico in 1782 in which the two sides agreed to a truce .", "prompt_labels": "Historian(O) J.(B-person) G.(I-person) M.(I-person) Ramsey(I-person) reported(O) a(O) conference(O) between(O) militia(B-organization) commander(O) John(B-person) Sevier(I-person) and(O) Cherokee(O) Chief(O) Hanging(B-person) Maw(I-person) held(O) at(O) the(O) original(O) Citico(B-location) in(O) 1782(O) in(O) which(O) the(O) two(O) sides(O) agreed(O) to(O) a(O) truce(O) .(O)"}}
{"id": "189", "dataset": "crossner_science", "split": "test", "label_list": ["award", "scientist", "chemical compound", "person", "event", "university", "protein", "academic journal", "chemical element", "location", "theory", "organization", "country", "enzyme", "astronomical object", "discipline"], "instance": {"id": "189", "words": ["He", "was", "awarded", "the", "Louisa", "Gross", "Horwitz", "Prize", "from", "Columbia", "University", "in", "1991", ",", "the", "Louis-Jeantet", "Prize", "for", "Medicine", "in", "1993", ",", "the", "Otto", "Warburg", "Medal", "in", "1999", "and", "half", "of", "the", "Nobel", "Prize", "in", "Chemistry", "in", "2002", "for", "his", "development", "of", "nuclear", "magnetic", "resonance", "spectroscopy", "for", "determining", "the", "three-dimensional", "structure", "of", "biological", "macromolecules", "in", "solution", "."], "labels": ["O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "B-university", "I-university", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, scientist, chemical compound, person, event, university, protein, academic journal, chemical element, location, theory, organization, country, enzyme, astronomical object, discipline and O.\nSentence: He was awarded the Louisa Gross Horwitz Prize from Columbia University in 1991 , the Louis-Jeantet Prize for Medicine in 1993 , the Otto Warburg Medal in 1999 and half of the Nobel Prize in Chemistry in 2002 for his development of nuclear magnetic resonance spectroscopy for determining the three-dimensional structure of biological macromolecules in solution .", "prompt_labels": "He(O) was(O) awarded(O) the(O) Louisa(B-award) Gross(I-award) Horwitz(I-award) Prize(I-award) from(O) Columbia(B-university) University(I-university) in(O) 1991(O) ,(O) the(O) Louis-Jeantet(B-award) Prize(I-award) for(I-award) Medicine(I-award) in(O) 1993(O) ,(O) the(O) Otto(B-award) Warburg(I-award) Medal(I-award) in(O) 1999(O) and(O) half(O) of(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Chemistry(I-award) in(O) 2002(O) for(O) his(O) development(O) of(O) nuclear(B-award) magnetic(I-award) resonance(I-award) spectroscopy(I-award) for(O) determining(O) the(O) three-dimensional(O) structure(O) of(O) biological(O) macromolecules(O) in(O) solution(O) .(O)"}}
{"id": "130", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "discipline", "award", "academic journal", "organization", "theory", "protein", "person", "scientist", "chemical element", "location", "country", "chemical compound", "university", "astronomical object", "event"], "instance": {"id": "130", "words": ["On", "11", "September", "2010", ",", "Jones", "performed", "for", "an", "audience", "of", "50,000", "at", "the", "Help", "for", "Heroes", "charity", "concert", "at", "Twickenham", "Stadium", "performing", "Strange", "Things", "Are", "Happening", "Every", "Day", "and", "his", "hit", "Green", ",", "Green", "Grass", "of", "Home", "."], "labels": ["O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "B-event", "I-event", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, discipline, award, academic journal, organization, theory, protein, person, scientist, chemical element, location, country, chemical compound, university, astronomical object, event and O.\nSentence: On 11 September 2010 , Jones performed for an audience of 50,000 at the Help for Heroes charity concert at Twickenham Stadium performing Strange Things Are Happening Every Day and his hit Green , Green Grass of Home .", "prompt_labels": "On(O) 11(O) September(O) 2010(O) ,(O) Jones(B-person) performed(O) for(O) an(O) audience(O) of(O) 50,000(O) at(O) the(O) Help(B-organization) for(I-organization) Heroes(I-organization) charity(B-event) concert(I-event) at(O) Twickenham(B-location) Stadium(I-location) performing(O) Strange(O) Things(O) Are(O) Happening(O) Every(O) Day(O) and(O) his(O) hit(O) Green(O) ,(O) Green(O) Grass(O) of(O) Home(O) .(O)"}}
{"id": "520", "dataset": "crossner_science", "split": "test", "label_list": ["person", "country", "theory", "enzyme", "chemical compound", "academic journal", "chemical element", "scientist", "protein", "university", "organization", "location", "astronomical object", "event", "award", "discipline"], "instance": {"id": "520", "words": ["This", "interaction", "stabilizes", "both", "TOC1", "and", "PRR5", "and", "prevents", "their", "degradation", "by", "the", "F-box", "protein", "ZEITLUPE", "(", "ZTL", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "B-protein", "O", "B-protein", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, theory, enzyme, chemical compound, academic journal, chemical element, scientist, protein, university, organization, location, astronomical object, event, award, discipline and O.\nSentence: This interaction stabilizes both TOC1 and PRR5 and prevents their degradation by the F-box protein ZEITLUPE ( ZTL ) .", "prompt_labels": "This(O) interaction(O) stabilizes(O) both(O) TOC1(O) and(O) PRR5(O) and(O) prevents(O) their(O) degradation(O) by(O) the(O) F-box(B-protein) protein(I-protein) ZEITLUPE(B-protein) ((O) ZTL(B-protein) )(O) .(O)"}}
{"id": "368", "dataset": "crossner_science", "split": "test", "label_list": ["scientist", "university", "theory", "protein", "event", "person", "chemical compound", "discipline", "enzyme", "astronomical object", "chemical element", "country", "organization", "location", "award", "academic journal"], "instance": {"id": "368", "words": ["However", ",", "various", "histone", "modifications", "are", "placed", "by", "epigenetic", "modifiers", "such", "as", "DNA", "methyltransferase", "in", "neurons", "and", "these", "marks", "regulate", "gene", "expression", "throughout", "the", "life", "span", "of", "the", "neuron", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, university, theory, protein, event, person, chemical compound, discipline, enzyme, astronomical object, chemical element, country, organization, location, award, academic journal and O.\nSentence: However , various histone modifications are placed by epigenetic modifiers such as DNA methyltransferase in neurons and these marks regulate gene expression throughout the life span of the neuron .", "prompt_labels": "However(O) ,(O) various(O) histone(O) modifications(O) are(O) placed(O) by(O) epigenetic(O) modifiers(O) such(O) as(O) DNA(B-protein) methyltransferase(I-protein) in(O) neurons(O) and(O) these(O) marks(O) regulate(O) gene(O) expression(O) throughout(O) the(O) life(O) span(O) of(O) the(O) neuron(O) .(O)"}}
{"id": "187", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "chemical compound", "theory", "university", "organization", "event", "enzyme", "award", "protein", "astronomical object", "chemical element", "location", "discipline", "country", "scientist", "person"], "instance": {"id": "187", "words": ["The", "quarry", "is", "at", "Herniss", ",", "to", "the", "north", "of", "the", "A", "394", "road", ",", "between", "Rame", "and", "Longdowns", "."], "labels": ["O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, chemical compound, theory, university, organization, event, enzyme, award, protein, astronomical object, chemical element, location, discipline, country, scientist, person and O.\nSentence: The quarry is at Herniss , to the north of the A 394 road , between Rame and Longdowns .", "prompt_labels": "The(O) quarry(O) is(O) at(O) Herniss(B-location) ,(O) to(O) the(O) north(O) of(O) the(O) A(B-location) 394(I-location) road(I-location) ,(O) between(O) Rame(B-location) and(O) Longdowns(B-location) .(O)"}}
{"id": "171", "dataset": "crossner_science", "split": "test", "label_list": ["location", "award", "theory", "person", "country", "university", "academic journal", "scientist", "chemical element", "protein", "discipline", "enzyme", "astronomical object", "event", "organization", "chemical compound"], "instance": {"id": "171", "words": ["Since", "1993", "the", "Milutin", "Milankovic", "Medal", "has", "been", "awarded", "by", "the", "European", "Geophysical", "Society", "(", "called", "the", "European", "Geosciences", "Union", "since", "2003", ")", "for", "contributions", "in", "the"], "labels": ["O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, theory, person, country, university, academic journal, scientist, chemical element, protein, discipline, enzyme, astronomical object, event, organization, chemical compound and O.\nSentence: Since 1993 the Milutin Milankovic Medal has been awarded by the European Geophysical Society ( called the European Geosciences Union since 2003 ) for contributions in the", "prompt_labels": "Since(O) 1993(O) the(O) Milutin(B-award) Milankovic(I-award) Medal(I-award) has(O) been(O) awarded(O) by(O) the(O) European(B-organization) Geophysical(I-organization) Society(I-organization) ((O) called(O) the(O) European(B-organization) Geosciences(I-organization) Union(I-organization) since(O) 2003(O) )(O) for(O) contributions(O) in(O) the(O)"}}
{"id": "288", "dataset": "crossner_science", "split": "test", "label_list": ["event", "theory", "country", "academic journal", "person", "scientist", "university", "organization", "chemical element", "award", "location", "chemical compound", "astronomical object", "discipline", "protein", "enzyme"], "instance": {"id": "288", "words": ["As", "examples", ",", "for", "Greeks", ",", "Constantine", "XI", "Palaiologos", "and", "Kolokotronis", ";", "and", "for", "Serbs", ",", "Miloš", "Obilić", "and", "Tzar", "Lazar", ";", "for", "Montenegrins", ",", "Đurađ", "I", "Balšić", "and", "Ivan", "Crnojević", ";", "for", "Albanians", ",", "George", "Kastrioti", "Skanderbeg", ";", "for", "ethnic", "Macedonians", ",", "Nikola", "Karev", "for", "Bulgarians", ",", "Vasil", "Levski", ",", "Georgi", "Sava", "Rakovski", "and", "Hristo", "Botev", "and", "for", "Croats", ",", "Nikola", "Šubić", "Zrinjski", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "B-person", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, theory, country, academic journal, person, scientist, university, organization, chemical element, award, location, chemical compound, astronomical object, discipline, protein, enzyme and O.\nSentence: As examples , for Greeks , Constantine XI Palaiologos and Kolokotronis ; and for Serbs , Miloš Obilić and Tzar Lazar ; for Montenegrins , Đurađ I Balšić and Ivan Crnojević ; for Albanians , George Kastrioti Skanderbeg ; for ethnic Macedonians , Nikola Karev for Bulgarians , Vasil Levski , Georgi Sava Rakovski and Hristo Botev and for Croats , Nikola Šubić Zrinjski .", "prompt_labels": "As(O) examples(O) ,(O) for(O) Greeks(O) ,(O) Constantine(B-person) XI(I-person) Palaiologos(I-person) and(O) Kolokotronis(B-person) ;(O) and(O) for(O) Serbs(O) ,(O) Miloš(B-person) Obilić(I-person) and(O) Tzar(B-person) Lazar(I-person) ;(O) for(O) Montenegrins(O) ,(O) Đurađ(B-person) I(I-person) Balšić(I-person) and(O) Ivan(B-person) Crnojević(I-person) ;(O) for(O) Albanians(O) ,(O) George(B-person) Kastrioti(I-person) Skanderbeg(I-person) ;(O) for(O) ethnic(O) Macedonians(O) ,(O) Nikola(B-person) Karev(I-person) for(O) Bulgarians(O) ,(O) Vasil(B-person) Levski(I-person) ,(O) Georgi(B-person) Sava(I-person) Rakovski(I-person) and(O) Hristo(B-person) Botev(I-person) and(O) for(O) Croats(O) ,(O) Nikola(B-person) Šubić(I-person) Zrinjski(I-person) .(O)"}}
{"id": "257", "dataset": "crossner_science", "split": "test", "label_list": ["organization", "astronomical object", "person", "discipline", "chemical element", "location", "country", "theory", "protein", "chemical compound", "scientist", "academic journal", "university", "enzyme", "event", "award"], "instance": {"id": "257", "words": ["In", "1978", ",", "he", "was", "also", "heavily", "influenced", "by", "the", "visit", "of", "Simon", "van", "der", "Meer", "and", "Carlo", "Rubbia", "to", "the", "laboratory", ",", "speaking", "on", "the", "possibilities", "of", "stochastic", "phase", "space", "cooling", "of", "antiproton", "beams", "and", "the", "exciting", "possibilities", "with", "proton-antiproton", "collisions", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, astronomical object, person, discipline, chemical element, location, country, theory, protein, chemical compound, scientist, academic journal, university, enzyme, event, award and O.\nSentence: In 1978 , he was also heavily influenced by the visit of Simon van der Meer and Carlo Rubbia to the laboratory , speaking on the possibilities of stochastic phase space cooling of antiproton beams and the exciting possibilities with proton-antiproton collisions .", "prompt_labels": "In(O) 1978(O) ,(O) he(O) was(O) also(O) heavily(O) influenced(O) by(O) the(O) visit(O) of(O) Simon(B-scientist) van(I-scientist) der(I-scientist) Meer(I-scientist) and(O) Carlo(B-scientist) Rubbia(I-scientist) to(O) the(O) laboratory(O) ,(O) speaking(O) on(O) the(O) possibilities(O) of(O) stochastic(O) phase(O) space(O) cooling(O) of(O) antiproton(O) beams(O) and(O) the(O) exciting(O) possibilities(O) with(O) proton-antiproton(O) collisions(O) .(O)"}}
{"id": "534", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "award", "scientist", "organization", "location", "academic journal", "astronomical object", "protein", "enzyme", "university", "chemical element", "person", "chemical compound", "country", "discipline", "event"], "instance": {"id": "534", "words": ["He", "then", "stated", "that", ",", "based", "on", "his", "data", "on", "various", "comet", "s", ",", "the", "size", "of", "the", "Solar", "system", "can", "be", "100", "AU", "or", "even", "more", ",", "and", "that", "it", "could", "be", "other", "planet", "s", "there", "that", "perturb", "the", "orbit", "of", "Uranus", "(", "although", "the", "position", "of", "the", "eventual", "Neptune", "was", "not", "calculated", "until", "much", "later", "by", "Urbain", "Le", "Verrier", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, award, scientist, organization, location, academic journal, astronomical object, protein, enzyme, university, chemical element, person, chemical compound, country, discipline, event and O.\nSentence: He then stated that , based on his data on various comet s , the size of the Solar system can be 100 AU or even more , and that it could be other planet s there that perturb the orbit of Uranus ( although the position of the eventual Neptune was not calculated until much later by Urbain Le Verrier ) .", "prompt_labels": "He(O) then(O) stated(O) that(O) ,(O) based(O) on(O) his(O) data(O) on(O) various(O) comet(O) s(O) ,(O) the(O) size(O) of(O) the(O) Solar(O) system(O) can(O) be(O) 100(O) AU(O) or(O) even(O) more(O) ,(O) and(O) that(O) it(O) could(O) be(O) other(O) planet(O) s(O) there(O) that(O) perturb(O) the(O) orbit(O) of(O) Uranus(B-astronomical object) ((O) although(O) the(O) position(O) of(O) the(O) eventual(O) Neptune(B-astronomical object) was(O) not(O) calculated(O) until(O) much(O) later(O) by(O) Urbain(B-scientist) Le(I-scientist) Verrier(I-scientist) )(O) .(O)"}}
{"id": "99", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "theory", "event", "discipline", "academic journal", "chemical element", "country", "person", "astronomical object", "scientist", "chemical compound", "university", "protein", "organization", "award", "location"], "instance": {"id": "99", "words": ["The", "principal", "candidates", "for", "the", "origin", "of", "Mars", "'", "methane", "include", "non-biological", "processes", "such", "as", "Properties", "of", "water", "-rock", "reactions", ",", "radiolysis", "of", "water", ",", "and", "pyrite", "formation", ",", "all", "of", "which", "produce", "Hydrogen", "that", "could", "then", "generate", "methane", "and", "other", "hydrocarbons", "via", "Fischer-Tropsch", "synthesis", "with", "Carbon", "monoxide", "and", "COsub2", "/", "sub", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical element", "O", "O", "O", "O", "B-chemical compound", "O", "O", "B-chemical compound", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, theory, event, discipline, academic journal, chemical element, country, person, astronomical object, scientist, chemical compound, university, protein, organization, award, location and O.\nSentence: The principal candidates for the origin of Mars ' methane include non-biological processes such as Properties of water -rock reactions , radiolysis of water , and pyrite formation , all of which produce Hydrogen that could then generate methane and other hydrocarbons via Fischer-Tropsch synthesis with Carbon monoxide and COsub2 / sub .", "prompt_labels": "The(O) principal(O) candidates(O) for(O) the(O) origin(O) of(O) Mars(B-astronomical object) '(O) methane(B-chemical compound) include(O) non-biological(O) processes(O) such(O) as(O) Properties(O) of(O) water(O) -rock(O) reactions(O) ,(O) radiolysis(O) of(O) water(O) ,(O) and(O) pyrite(O) formation(O) ,(O) all(O) of(O) which(O) produce(O) Hydrogen(B-chemical element) that(O) could(O) then(O) generate(O) methane(B-chemical compound) and(O) other(O) hydrocarbons(B-chemical compound) via(O) Fischer-Tropsch(O) synthesis(O) with(O) Carbon(B-chemical compound) monoxide(I-chemical compound) and(O) COsub2(B-chemical compound) /(I-chemical compound) sub(I-chemical compound) .(O)"}}
{"id": "297", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "university", "organization", "chemical compound", "location", "astronomical object", "protein", "scientist", "theory", "award", "enzyme", "event", "country", "person", "academic journal", "discipline"], "instance": {"id": "297", "words": ["The", "rings", "of", "Uranus", "are", "a", "system", "of", "ring", "s", "around", "the", "planet", "Uranus", ",", "intermediate", "in", "complexity", "between", "the", "more", "extensive", "set", "around", "Saturn", "and", "the", "simpler", "systems", "around", "Jupiter", "and", "Neptune", "."], "labels": ["O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, university, organization, chemical compound, location, astronomical object, protein, scientist, theory, award, enzyme, event, country, person, academic journal, discipline and O.\nSentence: The rings of Uranus are a system of ring s around the planet Uranus , intermediate in complexity between the more extensive set around Saturn and the simpler systems around Jupiter and Neptune .", "prompt_labels": "The(O) rings(O) of(O) Uranus(B-astronomical object) are(O) a(O) system(O) of(O) ring(O) s(O) around(O) the(O) planet(O) Uranus(B-astronomical object) ,(O) intermediate(O) in(O) complexity(O) between(O) the(O) more(O) extensive(O) set(O) around(O) Saturn(B-astronomical object) and(O) the(O) simpler(O) systems(O) around(O) Jupiter(B-astronomical object) and(O) Neptune(B-astronomical object) .(O)"}}
{"id": "308", "dataset": "crossner_science", "split": "test", "label_list": ["award", "chemical element", "discipline", "university", "enzyme", "organization", "person", "country", "location", "protein", "chemical compound", "academic journal", "astronomical object", "event", "theory", "scientist"], "instance": {"id": "308", "words": ["Jared", "Diamond", "describes", "an", "Evil", "Quartet", "of", "habitat", "destruction", ",", "overkill", ",", "introduced", "species", "and", "secondary", "extinctions.small", "which", "has", "been", "adopted", "by", "major", "international", "conservation", "organizations", "such", "as", "the", "US", "Nature", "Conservancy", ",", "the", "World", "Wildlife", "Fund", ",", "Conservation", "International", "and", "BirdLife", "International", "."], "labels": ["B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, chemical element, discipline, university, enzyme, organization, person, country, location, protein, chemical compound, academic journal, astronomical object, event, theory, scientist and O.\nSentence: Jared Diamond describes an Evil Quartet of habitat destruction , overkill , introduced species and secondary extinctions.small which has been adopted by major international conservation organizations such as the US Nature Conservancy , the World Wildlife Fund , Conservation International and BirdLife International .", "prompt_labels": "Jared(B-scientist) Diamond(I-scientist) describes(O) an(O) Evil(O) Quartet(O) of(O) habitat(O) destruction(O) ,(O) overkill(O) ,(O) introduced(O) species(O) and(O) secondary(O) extinctions.small(O) which(O) has(O) been(O) adopted(O) by(O) major(O) international(O) conservation(O) organizations(O) such(O) as(O) the(O) US(B-organization) Nature(I-organization) Conservancy(I-organization) ,(O) the(O) World(B-organization) Wildlife(I-organization) Fund(I-organization) ,(O) Conservation(B-organization) International(I-organization) and(O) BirdLife(B-organization) International(I-organization) .(O)"}}
{"id": "209", "dataset": "crossner_science", "split": "test", "label_list": ["chemical compound", "chemical element", "discipline", "location", "enzyme", "country", "scientist", "academic journal", "award", "astronomical object", "protein", "theory", "organization", "person", "event", "university"], "instance": {"id": "209", "words": ["His", "work", "won", "him", "the", "Rumford", "Medal", "of", "the", "Royal", "Society", "in", "1838", ",", "and", "in", "1843", "he", "received", "its", "Royal", "Medal", "for", "a", "paper", "on", "the", "Transparency", "of", "the", "Atmosphere", "and", "the", "Laws", "of", "Extinction", "of", "the", "Sun", "'s", "Rays", "passing", "through", "it", "."], "labels": ["O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, chemical element, discipline, location, enzyme, country, scientist, academic journal, award, astronomical object, protein, theory, organization, person, event, university and O.\nSentence: His work won him the Rumford Medal of the Royal Society in 1838 , and in 1843 he received its Royal Medal for a paper on the Transparency of the Atmosphere and the Laws of Extinction of the Sun 's Rays passing through it .", "prompt_labels": "His(O) work(O) won(O) him(O) the(O) Rumford(B-award) Medal(I-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) in(O) 1838(O) ,(O) and(O) in(O) 1843(O) he(O) received(O) its(O) Royal(B-award) Medal(I-award) for(O) a(O) paper(O) on(O) the(O) Transparency(O) of(O) the(O) Atmosphere(O) and(O) the(O) Laws(O) of(O) Extinction(O) of(O) the(O) Sun(B-astronomical object) 's(O) Rays(O) passing(O) through(O) it(O) .(O)"}}
{"id": "510", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "protein", "astronomical object", "university", "country", "person", "academic journal", "theory", "chemical compound", "scientist", "discipline", "organization", "chemical element", "award", "location", "event"], "instance": {"id": "510", "words": ["A", "number", "of", "Pluto", "-as-planet", "proponents", ",", "in", "particular", "Alan", "Stern", ",", "head", "of", "NASA", "'s", "New", "Horizons", "mission", "to", "Pluto", ",", "have", "circulated", "a", "petition", "among", "astronomers", "to", "alter", "the", "definition", "."], "labels": ["O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, protein, astronomical object, university, country, person, academic journal, theory, chemical compound, scientist, discipline, organization, chemical element, award, location, event and O.\nSentence: A number of Pluto -as-planet proponents , in particular Alan Stern , head of NASA 's New Horizons mission to Pluto , have circulated a petition among astronomers to alter the definition .", "prompt_labels": "A(O) number(O) of(O) Pluto(B-astronomical object) -as-planet(O) proponents(O) ,(O) in(O) particular(O) Alan(B-scientist) Stern(I-scientist) ,(O) head(O) of(O) NASA(B-organization) 's(O) New(O) Horizons(O) mission(O) to(O) Pluto(B-astronomical object) ,(O) have(O) circulated(O) a(O) petition(O) among(O) astronomers(O) to(O) alter(O) the(O) definition(O) .(O)"}}
{"id": "15", "dataset": "crossner_science", "split": "test", "label_list": ["event", "organization", "award", "country", "enzyme", "scientist", "university", "academic journal", "location", "theory", "chemical compound", "discipline", "protein", "person", "astronomical object", "chemical element"], "instance": {"id": "15", "words": ["Cory", "'s", "work", "has", "been", "published", "in", "research", "journals", "including", "Blood", "(", "journal", ")", ",", "The", "EMBO", "Journal", ",", "Nature", "(", "journal", ")", ",", "Cell", "Death", "&", "amp", ";", "Differentiation", ",", "and", "Proceedings", "of", "the", "National", "Academy", "of", "Sciences", "of", "the", "United", "States", "of", "America", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, award, country, enzyme, scientist, university, academic journal, location, theory, chemical compound, discipline, protein, person, astronomical object, chemical element and O.\nSentence: Cory 's work has been published in research journals including Blood ( journal ) , The EMBO Journal , Nature ( journal ) , Cell Death & amp ; Differentiation , and Proceedings of the National Academy of Sciences of the United States of America .", "prompt_labels": "Cory(B-scientist) 's(O) work(O) has(O) been(O) published(O) in(O) research(O) journals(O) including(O) Blood(B-academic journal) ((O) journal(O) )(O) ,(O) The(B-academic journal) EMBO(I-academic journal) Journal(I-academic journal) ,(O) Nature(B-academic journal) ((O) journal(O) )(O) ,(O) Cell(B-academic journal) Death(I-academic journal) &(I-academic journal) amp(I-academic journal) ;(I-academic journal) Differentiation(I-academic journal) ,(O) and(O) Proceedings(B-academic journal) of(I-academic journal) the(I-academic journal) National(I-academic journal) Academy(I-academic journal) of(I-academic journal) Sciences(I-academic journal) of(I-academic journal) the(I-academic journal) United(I-academic journal) States(I-academic journal) of(I-academic journal) America(I-academic journal) .(O)"}}
{"id": "115", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "country", "location", "organization", "university", "discipline", "protein", "scientist", "award", "theory", "enzyme", "chemical element", "astronomical object", "chemical compound", "person", "event"], "instance": {"id": "115", "words": ["Observations", "are", "currently", "being", "coordinated", "by", "the", "Association", "of", "Lunar", "and", "Planetary", "Observers", "and", "the", "British", "Astronomical", "Association", "to", "re-observe", "sites", "where", "transient", "lunar", "phenomena", "were", "reported", "in", "the", "past", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, country, location, organization, university, discipline, protein, scientist, award, theory, enzyme, chemical element, astronomical object, chemical compound, person, event and O.\nSentence: Observations are currently being coordinated by the Association of Lunar and Planetary Observers and the British Astronomical Association to re-observe sites where transient lunar phenomena were reported in the past .", "prompt_labels": "Observations(O) are(O) currently(O) being(O) coordinated(O) by(O) the(O) Association(B-organization) of(I-organization) Lunar(I-organization) and(I-organization) Planetary(I-organization) Observers(I-organization) and(O) the(O) British(B-organization) Astronomical(I-organization) Association(I-organization) to(O) re-observe(O) sites(O) where(O) transient(O) lunar(O) phenomena(O) were(O) reported(O) in(O) the(O) past(O) .(O)"}}
{"id": "313", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "scientist", "enzyme", "award", "event", "country", "location", "chemical compound", "protein", "academic journal", "astronomical object", "organization", "university", "person", "discipline", "chemical element"], "instance": {"id": "313", "words": ["These", "include", "the", "TRUE", "Blue", "Crew", ",", "Antipodean", "Resistance", ",", "the", "Australian", "Defence", "League", ",", "National", "Action", "(", "Australia", ")", ",", "the", "Q", "Society", ",", "Reclaim", "Australia", "and", "the", "Lads", "Society", "(", "formerly", "United", "Patriots", "Front", ")", "."], "labels": ["O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-country", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, scientist, enzyme, award, event, country, location, chemical compound, protein, academic journal, astronomical object, organization, university, person, discipline, chemical element and O.\nSentence: These include the TRUE Blue Crew , Antipodean Resistance , the Australian Defence League , National Action ( Australia ) , the Q Society , Reclaim Australia and the Lads Society ( formerly United Patriots Front ) .", "prompt_labels": "These(O) include(O) the(O) TRUE(B-organization) Blue(I-organization) Crew(I-organization) ,(O) Antipodean(B-organization) Resistance(I-organization) ,(O) the(O) Australian(B-organization) Defence(I-organization) League(I-organization) ,(O) National(B-organization) Action(I-organization) ((O) Australia(B-country) )(O) ,(O) the(O) Q(B-organization) Society(I-organization) ,(O) Reclaim(B-organization) Australia(I-organization) and(O) the(O) Lads(B-organization) Society(I-organization) ((O) formerly(O) United(B-organization) Patriots(I-organization) Front(I-organization) )(O) .(O)"}}
{"id": "0", "dataset": "crossner_science", "split": "test", "label_list": ["scientist", "organization", "university", "country", "academic journal", "theory", "chemical compound", "chemical element", "location", "discipline", "person", "astronomical object", "event", "protein", "enzyme", "award"], "instance": {"id": "0", "words": ["ESA", "'s", "Advanced", "Concepts", "Team", "also", "demonstrated", "theoretically", "that", "a", "deflection", "of", "99942", "Apophis", "could", "be", "achieved", "by", "sending", "a", "spacecraft", "weighing", "less", "than", "a", "tonne", "to", "impact", "against", "the", "asteroid", "."], "labels": ["B-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, organization, university, country, academic journal, theory, chemical compound, chemical element, location, discipline, person, astronomical object, event, protein, enzyme, award and O.\nSentence: ESA 's Advanced Concepts Team also demonstrated theoretically that a deflection of 99942 Apophis could be achieved by sending a spacecraft weighing less than a tonne to impact against the asteroid .", "prompt_labels": "ESA(B-organization) 's(O) Advanced(B-organization) Concepts(I-organization) Team(I-organization) also(O) demonstrated(O) theoretically(O) that(O) a(O) deflection(O) of(O) 99942(B-astronomical object) Apophis(I-astronomical object) could(O) be(O) achieved(O) by(O) sending(O) a(O) spacecraft(O) weighing(O) less(O) than(O) a(O) tonne(O) to(O) impact(O) against(O) the(O) asteroid(O) .(O)"}}
{"id": "50", "dataset": "crossner_science", "split": "test", "label_list": ["award", "astronomical object", "theory", "country", "location", "protein", "person", "organization", "discipline", "event", "chemical compound", "chemical element", "enzyme", "academic journal", "university", "scientist"], "instance": {"id": "50", "words": ["DNA", "methyltransferase", "is", "the", "enzyme", "involved", "in", "the", "maintenance", "of", "DNA", "methylation", "marks", "."], "labels": ["B-enzyme", "I-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, astronomical object, theory, country, location, protein, person, organization, discipline, event, chemical compound, chemical element, enzyme, academic journal, university, scientist and O.\nSentence: DNA methyltransferase is the enzyme involved in the maintenance of DNA methylation marks .", "prompt_labels": "DNA(B-enzyme) methyltransferase(I-enzyme) is(O) the(O) enzyme(O) involved(O) in(O) the(O) maintenance(O) of(O) DNA(O) methylation(O) marks(O) .(O)"}}
{"id": "306", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "theory", "university", "discipline", "scientist", "protein", "person", "chemical element", "astronomical object", "organization", "location", "event", "chemical compound", "award", "country", "academic journal"], "instance": {"id": "306", "words": ["The", "journal", "is", "published", "by", "Taylor", "and", "Francis", "on", "behalf", "of", "the", "American", "Association", "of", "Geographers", ",", "of", "which", "it", "is", "one", "of", "four", "official", "journals", ",", "the", "others", "being", "The", "Professional", "Geographer", ",", "AAG", "Review", "of", "Books", ",", "GeoHumanities", ",", "and", "African", "Geographical", "Review", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, theory, university, discipline, scientist, protein, person, chemical element, astronomical object, organization, location, event, chemical compound, award, country, academic journal and O.\nSentence: The journal is published by Taylor and Francis on behalf of the American Association of Geographers , of which it is one of four official journals , the others being The Professional Geographer , AAG Review of Books , GeoHumanities , and African Geographical Review .", "prompt_labels": "The(O) journal(O) is(O) published(O) by(O) Taylor(B-organization) and(I-organization) Francis(I-organization) on(O) behalf(O) of(O) the(O) American(B-organization) Association(I-organization) of(I-organization) Geographers(I-organization) ,(O) of(O) which(O) it(O) is(O) one(O) of(O) four(O) official(O) journals(O) ,(O) the(O) others(O) being(O) The(B-academic journal) Professional(I-academic journal) Geographer(I-academic journal) ,(O) AAG(B-academic journal) Review(I-academic journal) of(I-academic journal) Books(I-academic journal) ,(O) GeoHumanities(B-academic journal) ,(O) and(O) African(B-academic journal) Geographical(I-academic journal) Review(I-academic journal) .(O)"}}
{"id": "44", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "astronomical object", "academic journal", "scientist", "award", "protein", "event", "university", "person", "enzyme", "organization", "chemical compound", "location", "theory", "country", "discipline"], "instance": {"id": "44", "words": ["Non-governmental", "agencies", "such", "as", "the", "International", "Planned", "Parenthood", "Federation", "and", "Marie", "Stopes", "International", "provide", "contraceptive", "advice", "for", "young", "women", "worldwide", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, astronomical object, academic journal, scientist, award, protein, event, university, person, enzyme, organization, chemical compound, location, theory, country, discipline and O.\nSentence: Non-governmental agencies such as the International Planned Parenthood Federation and Marie Stopes International provide contraceptive advice for young women worldwide .", "prompt_labels": "Non-governmental(O) agencies(O) such(O) as(O) the(O) International(B-organization) Planned(I-organization) Parenthood(I-organization) Federation(I-organization) and(O) Marie(B-organization) Stopes(I-organization) International(I-organization) provide(O) contraceptive(O) advice(O) for(O) young(O) women(O) worldwide(O) .(O)"}}
{"id": "307", "dataset": "crossner_science", "split": "test", "label_list": ["event", "discipline", "organization", "award", "scientist", "theory", "location", "academic journal", "chemical compound", "chemical element", "person", "country", "protein", "astronomical object", "university", "enzyme"], "instance": {"id": "307", "words": ["The", "scientists", "reported", "that", "genome", "editing", "by", "CRISPR", "induced", "DNA", "damage", "response", "and", "the", "cell", "cycle", "stopped", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, discipline, organization, award, scientist, theory, location, academic journal, chemical compound, chemical element, person, country, protein, astronomical object, university, enzyme and O.\nSentence: The scientists reported that genome editing by CRISPR induced DNA damage response and the cell cycle stopped .", "prompt_labels": "The(O) scientists(O) reported(O) that(O) genome(O) editing(O) by(O) CRISPR(O) induced(O) DNA(O) damage(O) response(O) and(O) the(O) cell(O) cycle(O) stopped(O) .(O)"}}
{"id": "542", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "university", "protein", "chemical compound", "event", "astronomical object", "award", "enzyme", "country", "chemical element", "academic journal", "person", "organization", "location", "scientist", "discipline"], "instance": {"id": "542", "words": ["Mattauch", "and", "Fritz", "Strassmann", "actively", "supported", "the", "proposed", "appointment", "of", "Lise", "Meitner", "as", "head", "of", "the", "physics", "department", "of", "the", "University", "of", "Mainz", "."], "labels": ["B-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, university, protein, chemical compound, event, astronomical object, award, enzyme, country, chemical element, academic journal, person, organization, location, scientist, discipline and O.\nSentence: Mattauch and Fritz Strassmann actively supported the proposed appointment of Lise Meitner as head of the physics department of the University of Mainz .", "prompt_labels": "Mattauch(B-scientist) and(O) Fritz(B-scientist) Strassmann(I-scientist) actively(O) supported(O) the(O) proposed(O) appointment(O) of(O) Lise(B-scientist) Meitner(I-scientist) as(O) head(O) of(O) the(O) physics(B-organization) department(I-organization) of(I-organization) the(I-organization) University(I-organization) of(I-organization) Mainz(I-organization) .(O)"}}
{"id": "73", "dataset": "crossner_science", "split": "test", "label_list": ["award", "organization", "chemical element", "enzyme", "academic journal", "protein", "chemical compound", "astronomical object", "country", "discipline", "theory", "event", "scientist", "location", "person", "university"], "instance": {"id": "73", "words": ["On", "11", "August", "2014", ",", "astronomers", "released", "studies", ",", "using", "the", "Atacama", "Large", "Millimeter", "/", "Submillimeter", "Array", "(", "ALMA", ")", "for", "the", "first", "time", ",", "that", "detailed", "the", "distribution", "of", "HCN", ",", "Hydrogen", "isocyanide", ",", "Formaldehyde", ",", "and", "dust", "inside", "the", "comae", "of", "comet", "s", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, chemical element, enzyme, academic journal, protein, chemical compound, astronomical object, country, discipline, theory, event, scientist, location, person, university and O.\nSentence: On 11 August 2014 , astronomers released studies , using the Atacama Large Millimeter / Submillimeter Array ( ALMA ) for the first time , that detailed the distribution of HCN , Hydrogen isocyanide , Formaldehyde , and dust inside the comae of comet s .", "prompt_labels": "On(O) 11(O) August(O) 2014(O) ,(O) astronomers(O) released(O) studies(O) ,(O) using(O) the(O) Atacama(O) Large(O) Millimeter(O) /(O) Submillimeter(O) Array(O) ((O) ALMA(O) )(O) for(O) the(O) first(O) time(O) ,(O) that(O) detailed(O) the(O) distribution(O) of(O) HCN(O) ,(O) Hydrogen(B-chemical compound) isocyanide(I-chemical compound) ,(O) Formaldehyde(B-chemical compound) ,(O) and(O) dust(O) inside(O) the(O) comae(O) of(O) comet(O) s(O) .(O)"}}
{"id": "540", "dataset": "crossner_science", "split": "test", "label_list": ["chemical compound", "protein", "scientist", "university", "theory", "event", "person", "astronomical object", "award", "enzyme", "location", "organization", "academic journal", "country", "discipline", "chemical element"], "instance": {"id": "540", "words": ["Current", "research", "in", "the", "Liang", "Tong", "'s", "lab", "focuses", "on", "enzymes", "involved", "in", "fatty", "acid", "metabolism", ",", "including", "Acetyl-CoA", "carboxylase", ",", "carnitine", "acyltransferase", ",", "AMP-activated", "protein", "kinase", ",", "and", "others", "."], "labels": ["O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "B-enzyme", "I-enzyme", "O", "B-enzyme", "I-enzyme", "I-enzyme", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, protein, scientist, university, theory, event, person, astronomical object, award, enzyme, location, organization, academic journal, country, discipline, chemical element and O.\nSentence: Current research in the Liang Tong 's lab focuses on enzymes involved in fatty acid metabolism , including Acetyl-CoA carboxylase , carnitine acyltransferase , AMP-activated protein kinase , and others .", "prompt_labels": "Current(O) research(O) in(O) the(O) Liang(B-scientist) Tong(I-scientist) 's(O) lab(O) focuses(O) on(O) enzymes(O) involved(O) in(O) fatty(B-chemical compound) acid(I-chemical compound) metabolism(O) ,(O) including(O) Acetyl-CoA(B-enzyme) carboxylase(I-enzyme) ,(O) carnitine(B-enzyme) acyltransferase(I-enzyme) ,(O) AMP-activated(B-enzyme) protein(I-enzyme) kinase(I-enzyme) ,(O) and(O) others(O) .(O)"}}
{"id": "323", "dataset": "crossner_science", "split": "test", "label_list": ["location", "event", "scientist", "country", "theory", "award", "academic journal", "astronomical object", "enzyme", "chemical compound", "organization", "university", "discipline", "protein", "person", "chemical element"], "instance": {"id": "323", "words": ["The", "use", "of", "1,4-butanediol", "as", "an", "intelligent", "cosubstrate", "has", "also", "been", "validated", "in", "non-aqueous", "media", "using", "a", "commercial", "Alcohol", "dehydrogenase", ".Kara", "S", ",", "Spickermann", "D", ",", "Weckbecker", "A", ",", "Leggewie", "C", ",", "Arends", "IWCE", ",", "Hollmann", "F", "(", "2014", ")", "Bioreductions", "Catalyzed", "by", "an", "Alcohol", "Dehydrogenase", "in", "Non-aqueous", "Media", "."], "labels": ["O", "O", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, scientist, country, theory, award, academic journal, astronomical object, enzyme, chemical compound, organization, university, discipline, protein, person, chemical element and O.\nSentence: The use of 1,4-butanediol as an intelligent cosubstrate has also been validated in non-aqueous media using a commercial Alcohol dehydrogenase .Kara S , Spickermann D , Weckbecker A , Leggewie C , Arends IWCE , Hollmann F ( 2014 ) Bioreductions Catalyzed by an Alcohol Dehydrogenase in Non-aqueous Media .", "prompt_labels": "The(O) use(O) of(O) 1,4-butanediol(B-chemical compound) as(O) an(O) intelligent(O) cosubstrate(O) has(O) also(O) been(O) validated(O) in(O) non-aqueous(O) media(O) using(O) a(O) commercial(O) Alcohol(B-enzyme) dehydrogenase(I-enzyme) .Kara(B-scientist) S(I-scientist) ,(O) Spickermann(B-scientist) D(I-scientist) ,(O) Weckbecker(B-scientist) A(I-scientist) ,(O) Leggewie(B-scientist) C(I-scientist) ,(O) Arends(B-scientist) IWCE(I-scientist) ,(O) Hollmann(B-scientist) F(I-scientist) ((O) 2014(O) )(O) Bioreductions(O) Catalyzed(O) by(O) an(O) Alcohol(B-enzyme) Dehydrogenase(I-enzyme) in(O) Non-aqueous(O) Media(O) .(O)"}}
{"id": "343", "dataset": "crossner_science", "split": "test", "label_list": ["chemical element", "protein", "scientist", "chemical compound", "location", "discipline", "enzyme", "theory", "organization", "country", "astronomical object", "person", "award", "academic journal", "university", "event"], "instance": {"id": "343", "words": ["Declaring", "an", "arsenal", "of", "39,967", "tons", "of", "chemical", "weapons", "in", "1997", ",", "by", "far", "the", "largest", "arsenal", ",", "consisting", "of", "blister", "agents", ":", "Lewisite", ",", "Sulfur", "mustard", ",", "Lewisite-mustard", "mix", ",", "and", "nerve", "agents", ":", "Sarin", ",", "Soman", ",", "and", "VX", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O", "O", "B-chemical compound", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, protein, scientist, chemical compound, location, discipline, enzyme, theory, organization, country, astronomical object, person, award, academic journal, university, event and O.\nSentence: Declaring an arsenal of 39,967 tons of chemical weapons in 1997 , by far the largest arsenal , consisting of blister agents : Lewisite , Sulfur mustard , Lewisite-mustard mix , and nerve agents : Sarin , Soman , and VX .", "prompt_labels": "Declaring(O) an(O) arsenal(O) of(O) 39,967(O) tons(O) of(O) chemical(O) weapons(O) in(O) 1997(O) ,(O) by(O) far(O) the(O) largest(O) arsenal(O) ,(O) consisting(O) of(O) blister(B-chemical compound) agents(I-chemical compound) :(O) Lewisite(B-chemical compound) ,(O) Sulfur(B-chemical compound) mustard(I-chemical compound) ,(O) Lewisite-mustard(B-chemical compound) mix(I-chemical compound) ,(O) and(O) nerve(B-chemical compound) agents(I-chemical compound) :(O) Sarin(B-chemical compound) ,(O) Soman(B-chemical compound) ,(O) and(O) VX(B-chemical compound) .(O)"}}
{"id": "150", "dataset": "crossner_science", "split": "test", "label_list": ["event", "chemical element", "academic journal", "organization", "country", "astronomical object", "award", "discipline", "enzyme", "location", "chemical compound", "person", "university", "theory", "protein", "scientist"], "instance": {"id": "150", "words": ["With", "the", "first", "group", "interacts", "one", "plaque", "protein", "without", "PDZ", "domain", ",", "called", "cingulin", ",", "which", "plays", "a", "key", "role", "in", "the", "cell", "adhesion", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O", "O", "O", "O", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, chemical element, academic journal, organization, country, astronomical object, award, discipline, enzyme, location, chemical compound, person, university, theory, protein, scientist and O.\nSentence: With the first group interacts one plaque protein without PDZ domain , called cingulin , which plays a key role in the cell adhesion .", "prompt_labels": "With(O) the(O) first(O) group(O) interacts(O) one(O) plaque(B-protein) protein(I-protein) without(O) PDZ(O) domain(O) ,(O) called(O) cingulin(B-protein) ,(O) which(O) plays(O) a(O) key(O) role(O) in(O) the(O) cell(O) adhesion(O) .(O)"}}
{"id": "210", "dataset": "crossner_science", "split": "test", "label_list": ["location", "protein", "award", "scientist", "theory", "chemical compound", "event", "country", "university", "astronomical object", "organization", "discipline", "academic journal", "chemical element", "enzyme", "person"], "instance": {"id": "210", "words": ["The", "Msub2", "/", "sub", "muscarinic", "receptors", "decrease", "the", "heart", "rate", "by", "inhibiting", "depolarization", "of", "the", "sinoatrial", "node", "via", "Gi", "alpha", "subunit", "-", "G", "protein-coupled", "receptor", "and", "through", "modulation", "of", "muscarinic", "potassium", "channels", "."], "labels": ["O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "I-chemical compound", "I-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "I-protein", "O", "B-protein", "I-protein", "I-protein", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, protein, award, scientist, theory, chemical compound, event, country, university, astronomical object, organization, discipline, academic journal, chemical element, enzyme, person and O.\nSentence: The Msub2 / sub muscarinic receptors decrease the heart rate by inhibiting depolarization of the sinoatrial node via Gi alpha subunit - G protein-coupled receptor and through modulation of muscarinic potassium channels .", "prompt_labels": "The(O) Msub2(B-chemical compound) /(I-chemical compound) sub(I-chemical compound) muscarinic(I-chemical compound) receptors(I-chemical compound) decrease(O) the(O) heart(O) rate(O) by(O) inhibiting(O) depolarization(O) of(O) the(O) sinoatrial(O) node(O) via(O) Gi(B-protein) alpha(I-protein) subunit(I-protein) -(O) G(B-protein) protein-coupled(I-protein) receptor(I-protein) and(O) through(O) modulation(O) of(O) muscarinic(O) potassium(O) channels(O) .(O)"}}
{"id": "285", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "chemical element", "academic journal", "astronomical object", "scientist", "event", "location", "person", "award", "discipline", "university", "protein", "country", "theory", "organization", "chemical compound"], "instance": {"id": "285", "words": ["Simpson", "'s", "other", "great", "passions", "were", "astronomy", "(", "he", "was", "a", "member", "of", "the", "British", "Astronomical", "Association", "and", "-", "unusually", "for", "an", "amateur", "-", "was", "made", "a", "Fellow", "of", "the", "Royal", "Astronomical", "Society", ")", "and", "pacifism", ",", "specifically", "addressed", "in", "the", "title", "of", "his", "Tenth", "String", "Quartet", ",", "For", "Peace", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "B-discipline", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical element, academic journal, astronomical object, scientist, event, location, person, award, discipline, university, protein, country, theory, organization, chemical compound and O.\nSentence: Simpson 's other great passions were astronomy ( he was a member of the British Astronomical Association and - unusually for an amateur - was made a Fellow of the Royal Astronomical Society ) and pacifism , specifically addressed in the title of his Tenth String Quartet , For Peace .", "prompt_labels": "Simpson(B-person) 's(O) other(O) great(O) passions(O) were(O) astronomy(B-discipline) ((O) he(O) was(O) a(O) member(O) of(O) the(O) British(B-organization) Astronomical(I-organization) Association(I-organization) and(O) -(O) unusually(O) for(O) an(O) amateur(O) -(O) was(O) made(O) a(O) Fellow(B-award) of(I-award) the(I-award) Royal(I-award) Astronomical(I-award) Society(I-award) )(O) and(O) pacifism(O) ,(O) specifically(O) addressed(O) in(O) the(O) title(O) of(O) his(O) Tenth(O) String(O) Quartet(O) ,(O) For(O) Peace(O) .(O)"}}
{"id": "336", "dataset": "crossner_science", "split": "test", "label_list": ["award", "chemical compound", "scientist", "chemical element", "location", "event", "university", "person", "country", "discipline", "astronomical object", "theory", "organization", "academic journal", "enzyme", "protein"], "instance": {"id": "336", "words": ["The", "conclusion", "of", "the", "scientists", "was", "that", "further", "effort", "was", "needed", "in", "to", "improve", "the", "precision", "and", "efficiency", "of", "CRISPR", "gene", "editing", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, chemical compound, scientist, chemical element, location, event, university, person, country, discipline, astronomical object, theory, organization, academic journal, enzyme, protein and O.\nSentence: The conclusion of the scientists was that further effort was needed in to improve the precision and efficiency of CRISPR gene editing .", "prompt_labels": "The(O) conclusion(O) of(O) the(O) scientists(O) was(O) that(O) further(O) effort(O) was(O) needed(O) in(O) to(O) improve(O) the(O) precision(O) and(O) efficiency(O) of(O) CRISPR(O) gene(O) editing(O) .(O)"}}
{"id": "322", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "organization", "chemical element", "astronomical object", "university", "scientist", "event", "location", "country", "enzyme", "person", "chemical compound", "theory", "protein", "award", "discipline"], "instance": {"id": "322", "words": ["They", "include", "the", "N-terminal", "scaffold", "attachment", "factor", "-A", "/", "B", ",", "acinus", "and", "PIAS", "(", "SAP", ")", "domain", ",", "the", "Proline", "-", "Isoleucine", "-", "Asparagine", "-Ile-", "Threonine", "(", "PINIT", ")", "motif", ",", "the", "RING", "finger", "domain", "-", "Zinc", "finger", "-like", "zinc", "-binding", "domain", "(", "RLD", ")", ",", "the", "highly", "acidic", "domain", "(", "AD", ")", ",", "the", "SUMO-interacting", "motif", "(", "SIM", ")", ",", "and", "the", "serine", "/", "threonine", "-rich", "C-terminal", "region", "(", "S", "/", "T", ")", "."], "labels": ["O", "O", "O", "B-protein", "I-protein", "I-protein", "I-protein", "I-protein", "I-protein", "I-protein", "O", "O", "O", "B-protein", "I-protein", "I-protein", "I-protein", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O", "O", "O", "O", "B-protein", "I-protein", "I-protein", "O", "B-protein", "I-protein", "I-protein", "I-protein", "I-protein", "I-protein", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O", "B-protein", "O", "O", "O", "O", "B-protein", "I-protein", "I-protein", "I-protein", "I-protein", "I-protein", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, organization, chemical element, astronomical object, university, scientist, event, location, country, enzyme, person, chemical compound, theory, protein, award, discipline and O.\nSentence: They include the N-terminal scaffold attachment factor -A / B , acinus and PIAS ( SAP ) domain , the Proline - Isoleucine - Asparagine -Ile- Threonine ( PINIT ) motif , the RING finger domain - Zinc finger -like zinc -binding domain ( RLD ) , the highly acidic domain ( AD ) , the SUMO-interacting motif ( SIM ) , and the serine / threonine -rich C-terminal region ( S / T ) .", "prompt_labels": "They(O) include(O) the(O) N-terminal(B-protein) scaffold(I-protein) attachment(I-protein) factor(I-protein) -A(I-protein) /(I-protein) B(I-protein) ,(O) acinus(O) and(O) PIAS(B-protein) ((I-protein) SAP(I-protein) )(I-protein) domain(O) ,(O) the(O) Proline(B-chemical compound) -(O) Isoleucine(B-chemical compound) -(O) Asparagine(B-chemical compound) -Ile-(O) Threonine(B-chemical compound) ((O) PINIT(B-chemical compound) )(O) motif(O) ,(O) the(O) RING(B-protein) finger(I-protein) domain(I-protein) -(O) Zinc(B-protein) finger(I-protein) -like(I-protein) zinc(I-protein) -binding(I-protein) domain(I-protein) ((O) RLD(B-protein) )(O) ,(O) the(O) highly(O) acidic(O) domain(O) ((O) AD(O) )(O) ,(O) the(O) SUMO-interacting(B-protein) motif(I-protein) ((O) SIM(B-protein) )(O) ,(O) and(O) the(O) serine(B-protein) /(I-protein) threonine(I-protein) -rich(I-protein) C-terminal(I-protein) region(I-protein) ((O) S(O) /(O) T(O) )(O) .(O)"}}
{"id": "535", "dataset": "crossner_science", "split": "test", "label_list": ["chemical compound", "award", "organization", "discipline", "country", "location", "chemical element", "academic journal", "protein", "university", "enzyme", "astronomical object", "person", "scientist", "theory", "event"], "instance": {"id": "535", "words": ["At", "Institute", "G", "were", "Heinz", "Barwich", ",", "Werner", "Hartmann", ",", "and", "Justus", "Mühlenpfordt", "."], "labels": ["O", "B-organization", "I-organization", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, award, organization, discipline, country, location, chemical element, academic journal, protein, university, enzyme, astronomical object, person, scientist, theory, event and O.\nSentence: At Institute G were Heinz Barwich , Werner Hartmann , and Justus Mühlenpfordt .", "prompt_labels": "At(O) Institute(B-organization) G(I-organization) were(O) Heinz(B-scientist) Barwich(I-scientist) ,(O) Werner(B-scientist) Hartmann(I-scientist) ,(O) and(O) Justus(B-scientist) Mühlenpfordt(I-scientist) .(O)"}}
{"id": "82", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "university", "astronomical object", "award", "discipline", "event", "location", "theory", "chemical element", "chemical compound", "organization", "person", "country", "academic journal", "scientist", "enzyme"], "instance": {"id": "82", "words": ["Woese", "was", "a", "MacArthur", "Fellow", "in", "1984", ",", "was", "made", "a", "member", "of", "the", "National", "Academy", "of", "Sciences", "in", "1988", ",", "received", "the", "Leeuwenhoek", "Medal", "(", "microbiology", "'s", "highest", "honor", ")", "in", "1992", ",", "the", "Selman", "A.", "Waksman", "Award", "in", "Microbiology", "in", "1995", "from", "the", "National", "Academy", "of", "Sciences", ","], "labels": ["B-scientist", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "B-discipline", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, university, astronomical object, award, discipline, event, location, theory, chemical element, chemical compound, organization, person, country, academic journal, scientist, enzyme and O.\nSentence: Woese was a MacArthur Fellow in 1984 , was made a member of the National Academy of Sciences in 1988 , received the Leeuwenhoek Medal ( microbiology 's highest honor ) in 1992 , the Selman A. Waksman Award in Microbiology in 1995 from the National Academy of Sciences ,", "prompt_labels": "Woese(B-scientist) was(O) a(O) MacArthur(B-award) Fellow(I-award) in(O) 1984(O) ,(O) was(O) made(O) a(O) member(O) of(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) in(O) 1988(O) ,(O) received(O) the(O) Leeuwenhoek(B-award) Medal(I-award) ((O) microbiology(B-discipline) 's(O) highest(O) honor(O) )(O) in(O) 1992(O) ,(O) the(O) Selman(B-award) A.(I-award) Waksman(I-award) Award(I-award) in(I-award) Microbiology(I-award) in(O) 1995(O) from(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O)"}}
{"id": "250", "dataset": "crossner_science", "split": "test", "label_list": ["protein", "event", "organization", "enzyme", "theory", "academic journal", "chemical element", "location", "country", "scientist", "chemical compound", "discipline", "university", "award", "astronomical object", "person"], "instance": {"id": "250", "words": ["The", "first", "one", ",", "Annales", "de", "physique", ",", "was", "latterly", "published", "by", "EDP", "Sciences", "under", "the", "same", "name", "up", "to", "2009", ",", "when", "it", "became", "integrated", "in", "the", "European", "Physical", "Journal", "series", "as", "the", "European", "Physical", "Journal", "H", "."], "labels": ["O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, event, organization, enzyme, theory, academic journal, chemical element, location, country, scientist, chemical compound, discipline, university, award, astronomical object, person and O.\nSentence: The first one , Annales de physique , was latterly published by EDP Sciences under the same name up to 2009 , when it became integrated in the European Physical Journal series as the European Physical Journal H .", "prompt_labels": "The(O) first(O) one(O) ,(O) Annales(B-academic journal) de(I-academic journal) physique(I-academic journal) ,(O) was(O) latterly(O) published(O) by(O) EDP(B-organization) Sciences(I-organization) under(O) the(O) same(O) name(O) up(O) to(O) 2009(O) ,(O) when(O) it(O) became(O) integrated(O) in(O) the(O) European(B-academic journal) Physical(I-academic journal) Journal(I-academic journal) series(O) as(O) the(O) European(B-academic journal) Physical(I-academic journal) Journal(I-academic journal) H(I-academic journal) .(O)"}}
{"id": "378", "dataset": "crossner_science", "split": "test", "label_list": ["theory", "scientist", "enzyme", "country", "astronomical object", "organization", "award", "protein", "university", "event", "discipline", "academic journal", "chemical element", "chemical compound", "person", "location"], "instance": {"id": "378", "words": ["In", "1968", "and", "1970", ",", "Roger", "Penrose", ",", "Stephen", "Hawking", ",", "and", "George", "F.", "R.", "Ellis", "published", "papers", "where", "they", "showed", "that", "mathematical", "singularities", "were", "an", "inevitable", "initial", "condition", "of", "relativistic", "models", "of", "the", "Big", "Bang", "."], "labels": ["O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-theory", "I-theory", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, scientist, enzyme, country, astronomical object, organization, award, protein, university, event, discipline, academic journal, chemical element, chemical compound, person, location and O.\nSentence: In 1968 and 1970 , Roger Penrose , Stephen Hawking , and George F. R. Ellis published papers where they showed that mathematical singularities were an inevitable initial condition of relativistic models of the Big Bang .", "prompt_labels": "In(O) 1968(O) and(O) 1970(O) ,(O) Roger(B-scientist) Penrose(I-scientist) ,(O) Stephen(B-scientist) Hawking(I-scientist) ,(O) and(O) George(B-scientist) F.(I-scientist) R.(I-scientist) Ellis(I-scientist) published(O) papers(O) where(O) they(O) showed(O) that(O) mathematical(O) singularities(O) were(O) an(O) inevitable(O) initial(O) condition(O) of(O) relativistic(O) models(O) of(O) the(O) Big(B-theory) Bang(I-theory) .(O)"}}
{"id": "144", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "university", "theory", "country", "astronomical object", "chemical compound", "location", "discipline", "person", "scientist", "protein", "event", "academic journal", "chemical element", "award", "organization"], "instance": {"id": "144", "words": ["In", "2006", ",", "his", "work", "with", "WITNESS", "and", "his", "long-standing", "support", "of", "peace", "and", "human", "rights", "causes", "was", "recognised", "by", "the", "Nobel", "Peace", "Prize", "Laureates", "with", "the", "Man", "of", "Peace", "award", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, university, theory, country, astronomical object, chemical compound, location, discipline, person, scientist, protein, event, academic journal, chemical element, award, organization and O.\nSentence: In 2006 , his work with WITNESS and his long-standing support of peace and human rights causes was recognised by the Nobel Peace Prize Laureates with the Man of Peace award .", "prompt_labels": "In(O) 2006(O) ,(O) his(O) work(O) with(O) WITNESS(B-organization) and(O) his(O) long-standing(O) support(O) of(O) peace(O) and(O) human(O) rights(O) causes(O) was(O) recognised(O) by(O) the(O) Nobel(B-award) Peace(I-award) Prize(I-award) Laureates(O) with(O) the(O) Man(B-award) of(I-award) Peace(I-award) award(I-award) .(O)"}}
{"id": "34", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "chemical element", "discipline", "enzyme", "location", "person", "scientist", "chemical compound", "event", "country", "theory", "astronomical object", "university", "organization", "award", "protein"], "instance": {"id": "34", "words": ["He", "has", "chaired", "the", "Southern", "Growth", "Policies", "Board", ",", "the", "Southern", "Regional", "Education", "Board", ",", "the", "Southern", "Technology", "Council", ",", "the", "Interstate", "Oil", "and", "Gas", "Compact", "Commission", ",", "and", "the", "Education", "Commission", "of", "the", "States", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, chemical element, discipline, enzyme, location, person, scientist, chemical compound, event, country, theory, astronomical object, university, organization, award, protein and O.\nSentence: He has chaired the Southern Growth Policies Board , the Southern Regional Education Board , the Southern Technology Council , the Interstate Oil and Gas Compact Commission , and the Education Commission of the States .", "prompt_labels": "He(O) has(O) chaired(O) the(O) Southern(B-organization) Growth(I-organization) Policies(I-organization) Board(I-organization) ,(O) the(O) Southern(B-organization) Regional(I-organization) Education(I-organization) Board(I-organization) ,(O) the(B-organization) Southern(I-organization) Technology(I-organization) Council(I-organization) ,(O) the(O) Interstate(B-organization) Oil(I-organization) and(I-organization) Gas(I-organization) Compact(I-organization) Commission(I-organization) ,(O) and(O) the(O) Education(B-organization) Commission(I-organization) of(I-organization) the(I-organization) States(I-organization) .(O)"}}
{"id": "80", "dataset": "crossner_science", "split": "test", "label_list": ["country", "person", "academic journal", "university", "theory", "chemical compound", "location", "organization", "scientist", "chemical element", "event", "award", "protein", "enzyme", "astronomical object", "discipline"], "instance": {"id": "80", "words": ["The", "album", "earned", "Usher", "numerous", "awards", ",", "including", "two", "Grammy", "Award", "s", ",", "three", "Billboard", "Music", "Awards", "and", "a", "BET", "Awards", "."], "labels": ["O", "O", "O", "B-person", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, academic journal, university, theory, chemical compound, location, organization, scientist, chemical element, event, award, protein, enzyme, astronomical object, discipline and O.\nSentence: The album earned Usher numerous awards , including two Grammy Award s , three Billboard Music Awards and a BET Awards .", "prompt_labels": "The(O) album(O) earned(O) Usher(B-person) numerous(O) awards(O) ,(O) including(O) two(O) Grammy(B-award) Award(I-award) s(O) ,(O) three(O) Billboard(B-award) Music(I-award) Awards(I-award) and(O) a(O) BET(B-award) Awards(I-award) .(O)"}}
{"id": "146", "dataset": "crossner_science", "split": "test", "label_list": ["academic journal", "award", "theory", "organization", "person", "country", "chemical element", "protein", "discipline", "chemical compound", "location", "astronomical object", "scientist", "university", "event", "enzyme"], "instance": {"id": "146", "words": ["Alessandra", "Becatti", "(", "born", "30", "April", "1965", ")", "is", "an", "Italy", "female", "retired", "heptathlete", ",", "which", "participated", "at", "the", "1987", "World", "Championships", "in", "Athletics", "."], "labels": ["B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, award, theory, organization, person, country, chemical element, protein, discipline, chemical compound, location, astronomical object, scientist, university, event, enzyme and O.\nSentence: Alessandra Becatti ( born 30 April 1965 ) is an Italy female retired heptathlete , which participated at the 1987 World Championships in Athletics .", "prompt_labels": "Alessandra(B-person) Becatti(I-person) ((O) born(O) 30(O) April(O) 1965(O) )(O) is(O) an(O) Italy(B-country) female(O) retired(O) heptathlete(O) ,(O) which(O) participated(O) at(O) the(O) 1987(B-event) World(I-event) Championships(I-event) in(I-event) Athletics(I-event) .(O)"}}
{"id": "479", "dataset": "crossner_science", "split": "test", "label_list": ["award", "discipline", "theory", "astronomical object", "enzyme", "protein", "scientist", "event", "university", "academic journal", "location", "chemical element", "person", "organization", "country", "chemical compound"], "instance": {"id": "479", "words": ["The", "resultant", "increase", "in", "Aryl", "hydrocarbon", "receptor", "repressor", "expression", "could", "lead", "to", "a", "decrease", "in", "the", "body", "'s", "ability", "to", "break", "down", "carcinogens", ",", "increasing", "the", "risk", "of", "cancer", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, discipline, theory, astronomical object, enzyme, protein, scientist, event, university, academic journal, location, chemical element, person, organization, country, chemical compound and O.\nSentence: The resultant increase in Aryl hydrocarbon receptor repressor expression could lead to a decrease in the body 's ability to break down carcinogens , increasing the risk of cancer .", "prompt_labels": "The(O) resultant(O) increase(O) in(O) Aryl(O) hydrocarbon(O) receptor(O) repressor(O) expression(O) could(O) lead(O) to(O) a(O) decrease(O) in(O) the(O) body(O) 's(O) ability(O) to(O) break(O) down(O) carcinogens(O) ,(O) increasing(O) the(O) risk(O) of(O) cancer(O) .(O)"}}
{"id": "290", "dataset": "crossner_science", "split": "test", "label_list": ["country", "protein", "enzyme", "discipline", "award", "person", "university", "chemical element", "organization", "location", "chemical compound", "event", "academic journal", "scientist", "theory", "astronomical object"], "instance": {"id": "290", "words": ["The", "concept", "of", "détournement", "has", "had", "a", "popular", "influence", "amongst", "contemporary", "radicals", ",", "and", "the", "technique", "can", "be", "seen", "in", "action", "in", "the", "present", "day", "when", "looking", "at", "the", "work", "of", "Culture", "Jammer", "s", "including", "the", "Cacophony", "Society", ",", "Billboard", "Liberation", "Front", ",", "monochrom", ",", "Occupy", "Movements", "and", "Adbusters", ",", "whose", "subvertisements", "detourn", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-event", "I-event", "I-event", "O", "B-organization", "O", "B-event", "I-event", "O", "B-organization", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, protein, enzyme, discipline, award, person, university, chemical element, organization, location, chemical compound, event, academic journal, scientist, theory, astronomical object and O.\nSentence: The concept of détournement has had a popular influence amongst contemporary radicals , and the technique can be seen in action in the present day when looking at the work of Culture Jammer s including the Cacophony Society , Billboard Liberation Front , monochrom , Occupy Movements and Adbusters , whose subvertisements detourn .", "prompt_labels": "The(O) concept(O) of(O) détournement(O) has(O) had(O) a(O) popular(O) influence(O) amongst(O) contemporary(O) radicals(O) ,(O) and(O) the(O) technique(O) can(O) be(O) seen(O) in(O) action(O) in(O) the(O) present(O) day(O) when(O) looking(O) at(O) the(O) work(O) of(O) Culture(O) Jammer(O) s(O) including(O) the(O) Cacophony(B-organization) Society(I-organization) ,(O) Billboard(B-event) Liberation(I-event) Front(I-event) ,(O) monochrom(B-organization) ,(O) Occupy(B-event) Movements(I-event) and(O) Adbusters(B-organization) ,(O) whose(O) subvertisements(O) detourn(O) .(O)"}}
{"id": "401", "dataset": "crossner_science", "split": "test", "label_list": ["university", "scientist", "award", "enzyme", "protein", "organization", "chemical element", "academic journal", "astronomical object", "person", "country", "theory", "location", "discipline", "event", "chemical compound"], "instance": {"id": "401", "words": ["The", "transform", "provides", "an", "alternative", "approach", "to", "analytic", "wave", "front", "set", "s", "of", "distributions", ",", "developed", "independently", "by", "the", "Japanese", "mathematicians", "Mikio", "Sato", ",", "Masaki", "Kashiwara", "and", "Takahiro", "Kawai", "in", "their", "approach", "to", "microlocal", "analysis", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, scientist, award, enzyme, protein, organization, chemical element, academic journal, astronomical object, person, country, theory, location, discipline, event, chemical compound and O.\nSentence: The transform provides an alternative approach to analytic wave front set s of distributions , developed independently by the Japanese mathematicians Mikio Sato , Masaki Kashiwara and Takahiro Kawai in their approach to microlocal analysis .", "prompt_labels": "The(O) transform(O) provides(O) an(O) alternative(O) approach(O) to(O) analytic(O) wave(O) front(O) set(O) s(O) of(O) distributions(O) ,(O) developed(O) independently(O) by(O) the(O) Japanese(O) mathematicians(O) Mikio(B-scientist) Sato(I-scientist) ,(O) Masaki(B-scientist) Kashiwara(I-scientist) and(O) Takahiro(B-scientist) Kawai(I-scientist) in(O) their(O) approach(O) to(O) microlocal(O) analysis(O) .(O)"}}
{"id": "167", "dataset": "crossner_science", "split": "test", "label_list": ["country", "astronomical object", "academic journal", "enzyme", "event", "person", "organization", "university", "protein", "theory", "location", "chemical element", "discipline", "chemical compound", "award", "scientist"], "instance": {"id": "167", "words": ["In", "June", "2011", ",", "during", "the", "Second", "CoRoT", "Symposium", ",", "the", "probe", "added", "ten", "new", "objects", "to", "the", "Exoplanet", "catalogue", ":", "CoRoT-16b", ",", "CoRoT-17b", ",", "CoRoT-18b", ",", "CoRoT-19b", ",", "CoRoT-20b", ",", "CoRoT-21b", ",", "CoRoT-22b", ",", "CoRoT-23b", ",", "CoRoT-24b", ",", "CoRoT-24c", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, astronomical object, academic journal, enzyme, event, person, organization, university, protein, theory, location, chemical element, discipline, chemical compound, award, scientist and O.\nSentence: In June 2011 , during the Second CoRoT Symposium , the probe added ten new objects to the Exoplanet catalogue : CoRoT-16b , CoRoT-17b , CoRoT-18b , CoRoT-19b , CoRoT-20b , CoRoT-21b , CoRoT-22b , CoRoT-23b , CoRoT-24b , CoRoT-24c .", "prompt_labels": "In(O) June(O) 2011(O) ,(O) during(O) the(O) Second(B-event) CoRoT(I-event) Symposium(I-event) ,(O) the(O) probe(O) added(O) ten(O) new(O) objects(O) to(O) the(O) Exoplanet(O) catalogue(O) :(O) CoRoT-16b(B-astronomical object) ,(O) CoRoT-17b(B-astronomical object) ,(O) CoRoT-18b(B-astronomical object) ,(O) CoRoT-19b(B-astronomical object) ,(O) CoRoT-20b(B-astronomical object) ,(O) CoRoT-21b(B-astronomical object) ,(O) CoRoT-22b(B-astronomical object) ,(O) CoRoT-23b(B-astronomical object) ,(O) CoRoT-24b(B-astronomical object) ,(O) CoRoT-24c(B-astronomical object) .(O)"}}
{"id": "537", "dataset": "crossner_science", "split": "test", "label_list": ["event", "protein", "discipline", "scientist", "award", "organization", "location", "country", "chemical compound", "university", "chemical element", "theory", "astronomical object", "person", "academic journal", "enzyme"], "instance": {"id": "537", "words": ["Overall", "Bamberga", "is", "the", "tenth-brightest", "main-belt", "asteroid", "after", ",", "in", "order", ",", "4", "Vesta", ",", "2", "Pallas", ",", "Ceres", ",", "7", "Iris", ",", "6", "Hebe", ",", "3", "Juno", ",", "18", "Melpomene", ",", "15", "Eunomia", "and", "8", "Flora", "."], "labels": ["O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, protein, discipline, scientist, award, organization, location, country, chemical compound, university, chemical element, theory, astronomical object, person, academic journal, enzyme and O.\nSentence: Overall Bamberga is the tenth-brightest main-belt asteroid after , in order , 4 Vesta , 2 Pallas , Ceres , 7 Iris , 6 Hebe , 3 Juno , 18 Melpomene , 15 Eunomia and 8 Flora .", "prompt_labels": "Overall(O) Bamberga(B-astronomical object) is(O) the(O) tenth-brightest(O) main-belt(O) asteroid(O) after(O) ,(O) in(O) order(O) ,(O) 4(B-astronomical object) Vesta(I-astronomical object) ,(O) 2(B-astronomical object) Pallas(I-astronomical object) ,(O) Ceres(B-astronomical object) ,(O) 7(B-astronomical object) Iris(I-astronomical object) ,(O) 6(B-astronomical object) Hebe(I-astronomical object) ,(O) 3(B-astronomical object) Juno(I-astronomical object) ,(O) 18(B-astronomical object) Melpomene(I-astronomical object) ,(O) 15(B-astronomical object) Eunomia(I-astronomical object) and(O) 8(B-astronomical object) Flora(I-astronomical object) .(O)"}}
{"id": "62", "dataset": "crossner_science", "split": "test", "label_list": ["enzyme", "location", "person", "protein", "discipline", "scientist", "university", "country", "award", "chemical element", "academic journal", "organization", "astronomical object", "event", "theory", "chemical compound"], "instance": {"id": "62", "words": ["On", "August", "11", ",", "2014", ",", "astronomers", "released", "studies", ",", "using", "the", "Atacama", "Large", "Millimeter", "/", "Submillimeter", "Array", "(", "ALMA", ")", "for", "the", "first", "time", ",", "that", "detailed", "the", "distribution", "of", "Hydrogen", "cyanide", ",", "Hydrogen", "isocyanide", ",", "Formaldehyde", ",", "and", "dust", "inside", "the", "comae", "of", "comet", "s", "C", "/", "2012", "F6", "(", "Lemmon", ")", "and", "C", "/", "2012", "S1", "(", "ISON", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "I-astronomical object", "I-astronomical object", "I-astronomical object", "I-astronomical object", "O", "B-astronomical object", "O", "O", "B-astronomical object", "I-astronomical object", "I-astronomical object", "I-astronomical object", "O", "B-astronomical object", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, location, person, protein, discipline, scientist, university, country, award, chemical element, academic journal, organization, astronomical object, event, theory, chemical compound and O.\nSentence: On August 11 , 2014 , astronomers released studies , using the Atacama Large Millimeter / Submillimeter Array ( ALMA ) for the first time , that detailed the distribution of Hydrogen cyanide , Hydrogen isocyanide , Formaldehyde , and dust inside the comae of comet s C / 2012 F6 ( Lemmon ) and C / 2012 S1 ( ISON ) .", "prompt_labels": "On(O) August(O) 11(O) ,(O) 2014(O) ,(O) astronomers(O) released(O) studies(O) ,(O) using(O) the(O) Atacama(O) Large(O) Millimeter(O) /(O) Submillimeter(O) Array(O) ((O) ALMA(O) )(O) for(O) the(O) first(O) time(O) ,(O) that(O) detailed(O) the(O) distribution(O) of(O) Hydrogen(B-chemical compound) cyanide(I-chemical compound) ,(O) Hydrogen(B-chemical compound) isocyanide(I-chemical compound) ,(O) Formaldehyde(B-chemical compound) ,(O) and(O) dust(O) inside(O) the(O) comae(O) of(O) comet(B-astronomical object) s(I-astronomical object) C(I-astronomical object) /(I-astronomical object) 2012(I-astronomical object) F6(I-astronomical object) ((O) Lemmon(B-astronomical object) )(O) and(O) C(B-astronomical object) /(I-astronomical object) 2012(I-astronomical object) S1(I-astronomical object) ((O) ISON(B-astronomical object) )(O) .(O)"}}
{"id": "320", "dataset": "crossner_science", "split": "test", "label_list": ["location", "astronomical object", "theory", "university", "academic journal", "scientist", "person", "chemical compound", "enzyme", "protein", "event", "chemical element", "organization", "discipline", "award", "country"], "instance": {"id": "320", "words": ["It", "was", "discovered", "during", "the", "Palomar-Leiden", "survey", "on", "24", "September", "1960", ",", "by", "Ingrid", "van", "Houten-Groeneveld", "and", "Cornelis", "van", "Houten", "at", "Leiden", ",", "and", "Tom", "Gehrels", "at", "Palomar", "Observatory", "in", "California", ",", "United", "States", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-location", "O", "O", "B-scientist", "I-scientist", "O", "B-location", "I-location", "O", "B-location", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, astronomical object, theory, university, academic journal, scientist, person, chemical compound, enzyme, protein, event, chemical element, organization, discipline, award, country and O.\nSentence: It was discovered during the Palomar-Leiden survey on 24 September 1960 , by Ingrid van Houten-Groeneveld and Cornelis van Houten at Leiden , and Tom Gehrels at Palomar Observatory in California , United States .", "prompt_labels": "It(O) was(O) discovered(O) during(O) the(O) Palomar-Leiden(O) survey(O) on(O) 24(O) September(O) 1960(O) ,(O) by(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Cornelis(B-scientist) van(I-scientist) Houten(I-scientist) at(O) Leiden(B-location) ,(O) and(O) Tom(B-scientist) Gehrels(I-scientist) at(O) Palomar(B-location) Observatory(I-location) in(O) California(B-location) ,(O) United(B-country) States(I-country) .(O)"}}
{"id": "516", "dataset": "crossner_science", "split": "test", "label_list": ["discipline", "location", "scientist", "academic journal", "astronomical object", "theory", "university", "protein", "country", "event", "organization", "chemical compound", "enzyme", "person", "chemical element", "award"], "instance": {"id": "516", "words": ["The", "first", "molecular", "condensates", "were", "created", "in", "November", "2003", "by", "the", "groups", "of", "Rudolf", "Grimm", "at", "the", "University", "of", "Innsbruck", ",", "Deborah", "S.", "Jin", "at", "the", "University", "of", "Colorado", "at", "Boulder", "and", "Wolfgang", "Ketterle", "at", "MIT", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "B-university", "I-university", "I-university", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "O", "B-scientist", "I-scientist", "O", "B-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, location, scientist, academic journal, astronomical object, theory, university, protein, country, event, organization, chemical compound, enzyme, person, chemical element, award and O.\nSentence: The first molecular condensates were created in November 2003 by the groups of Rudolf Grimm at the University of Innsbruck , Deborah S. Jin at the University of Colorado at Boulder and Wolfgang Ketterle at MIT .", "prompt_labels": "The(O) first(O) molecular(O) condensates(O) were(O) created(O) in(O) November(O) 2003(O) by(O) the(O) groups(O) of(O) Rudolf(B-scientist) Grimm(I-scientist) at(O) the(O) University(B-university) of(I-university) Innsbruck(I-university) ,(O) Deborah(B-scientist) S.(I-scientist) Jin(I-scientist) at(O) the(O) University(B-university) of(I-university) Colorado(I-university) at(I-university) Boulder(I-university) and(O) Wolfgang(B-scientist) Ketterle(I-scientist) at(O) MIT(B-university) .(O)"}}
{"id": "294", "dataset": "crossner_science", "split": "test", "label_list": ["scientist", "discipline", "country", "theory", "organization", "astronomical object", "protein", "event", "enzyme", "award", "location", "university", "academic journal", "person", "chemical compound", "chemical element"], "instance": {"id": "294", "words": ["Galaxies", "and", "protostar", "s", "usually", "show", "differential", "rotation", ";", "examples", "in", "the", "Solar", "System", "include", "the", "Sun", ",", "Jupiter", "and", "Saturn", "."], "labels": ["B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, discipline, country, theory, organization, astronomical object, protein, event, enzyme, award, location, university, academic journal, person, chemical compound, chemical element and O.\nSentence: Galaxies and protostar s usually show differential rotation ; examples in the Solar System include the Sun , Jupiter and Saturn .", "prompt_labels": "Galaxies(B-astronomical object) and(O) protostar(B-astronomical object) s(O) usually(O) show(O) differential(O) rotation(O) ;(O) examples(O) in(O) the(O) Solar(O) System(O) include(O) the(O) Sun(B-astronomical object) ,(O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) .(O)"}}
{"id": "115", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "song", "title", "average ratings", "actor", "year", "review", "genre", "character", "director", "trailer", "plot"], "instance": {"id": "115", "words": ["what", "is", "puss", "in", "boots", "about"], "labels": ["O", "O", "B-title", "I-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, song, title, average ratings, actor, year, review, genre, character, director, trailer, plot and O.\nSentence: what is puss in boots about", "prompt_labels": "what(O) is(O) puss(B-title) in(I-title) boots(I-title) about(O)"}}
{"id": "1599", "dataset": "mit-movie", "split": "test", "label_list": ["review", "average ratings", "genre", "plot", "song", "trailer", "actor", "character", "rating", "year", "director", "title"], "instance": {"id": "1599", "words": ["list", "a", "pg", "13", "biography", "film", "that", "centers", "on", "mafia", "directed", "by", "david", "green", "for", "the", "past", "five", "years"], "labels": ["O", "O", "B-rating", "I-rating", "B-genre", "O", "O", "O", "O", "B-plot", "O", "O", "B-director", "I-director", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, average ratings, genre, plot, song, trailer, actor, character, rating, year, director, title and O.\nSentence: list a pg 13 biography film that centers on mafia directed by david green for the past five years", "prompt_labels": "list(O) a(O) pg(B-rating) 13(I-rating) biography(B-genre) film(O) that(O) centers(O) on(O) mafia(B-plot) directed(O) by(O) david(B-director) green(I-director) for(O) the(O) past(B-year) five(I-year) years(I-year)"}}
{"id": "465", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "title", "character", "song", "director", "review", "plot", "rating", "year", "trailer", "actor", "genre"], "instance": {"id": "465", "words": ["find", "me", "the", "review", "of", "black", "swan"], "labels": ["O", "O", "O", "B-average ratings", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, title, character, song, director, review, plot, rating, year, trailer, actor, genre and O.\nSentence: find me the review of black swan", "prompt_labels": "find(O) me(O) the(O) review(B-average ratings) of(O) black(B-title) swan(I-title)"}}
{"id": "185", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "song", "genre", "plot", "review", "title", "director", "rating", "year", "actor", "character", "average ratings"], "instance": {"id": "185", "words": ["who", "played", "as", "princess", "fiona", "in", "shrek"], "labels": ["O", "O", "O", "B-character", "I-character", "O", "B-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, song, genre, plot, review, title, director, rating, year, actor, character, average ratings and O.\nSentence: who played as princess fiona in shrek", "prompt_labels": "who(O) played(O) as(O) princess(B-character) fiona(I-character) in(O) shrek(B-title)"}}
{"id": "648", "dataset": "mit-movie", "split": "test", "label_list": ["review", "average ratings", "character", "actor", "title", "genre", "rating", "song", "plot", "trailer", "director", "year"], "instance": {"id": "648", "words": ["what", "year", "did", "scott", "pilgrim", "vs", "the", "world", "come", "out"], "labels": ["O", "O", "O", "B-title", "I-title", "I-title", "I-title", "I-title", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, average ratings, character, actor, title, genre, rating, song, plot, trailer, director, year and O.\nSentence: what year did scott pilgrim vs the world come out", "prompt_labels": "what(O) year(O) did(O) scott(B-title) pilgrim(I-title) vs(I-title) the(I-title) world(I-title) come(O) out(O)"}}
{"id": "2302", "dataset": "mit-movie", "split": "test", "label_list": ["character", "rating", "title", "plot", "genre", "director", "actor", "song", "year", "review", "trailer", "average ratings"], "instance": {"id": "2302", "words": ["who", "directed", "crimson", "rivers", "2", "angels", "of", "the", "apocalypse"], "labels": ["O", "O", "B-title", "I-title", "I-title", "I-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, rating, title, plot, genre, director, actor, song, year, review, trailer, average ratings and O.\nSentence: who directed crimson rivers 2 angels of the apocalypse", "prompt_labels": "who(O) directed(O) crimson(B-title) rivers(I-title) 2(I-title) angels(I-title) of(I-title) the(I-title) apocalypse(I-title)"}}
{"id": "880", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "title", "review", "plot", "character", "average ratings", "actor", "year", "trailer", "song", "genre", "director"], "instance": {"id": "880", "words": ["what", "movies", "from", "the", "90s", "did", "jeremy", "irons", "star", "in"], "labels": ["O", "O", "O", "O", "B-year", "O", "B-actor", "I-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, title, review, plot, character, average ratings, actor, year, trailer, song, genre, director and O.\nSentence: what movies from the 90s did jeremy irons star in", "prompt_labels": "what(O) movies(O) from(O) the(O) 90s(B-year) did(O) jeremy(B-actor) irons(I-actor) star(O) in(O)"}}
{"id": "1133", "dataset": "mit-movie", "split": "test", "label_list": ["director", "trailer", "actor", "song", "character", "year", "rating", "title", "genre", "review", "plot", "average ratings"], "instance": {"id": "1133", "words": ["did", "david", "fincher", "direct", "a", "movie", "about", "war", "in", "the", "1990", "s"], "labels": ["O", "B-director", "I-director", "O", "O", "O", "O", "B-genre", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, trailer, actor, song, character, year, rating, title, genre, review, plot, average ratings and O.\nSentence: did david fincher direct a movie about war in the 1990 s", "prompt_labels": "did(O) david(B-director) fincher(I-director) direct(O) a(O) movie(O) about(O) war(B-genre) in(O) the(O) 1990(B-year) s(I-year)"}}
{"id": "1946", "dataset": "mit-movie", "split": "test", "label_list": ["year", "director", "review", "title", "song", "genre", "rating", "plot", "actor", "average ratings", "character", "trailer"], "instance": {"id": "1946", "words": ["what", "critically", "acclaimed", "movies", "starred", "ben", "stiller", "or", "his", "parents", "in", "the", "last", "five", "decades"], "labels": ["O", "B-average ratings", "I-average ratings", "O", "O", "B-actor", "I-actor", "O", "O", "O", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, review, title, song, genre, rating, plot, actor, average ratings, character, trailer and O.\nSentence: what critically acclaimed movies starred ben stiller or his parents in the last five decades", "prompt_labels": "what(O) critically(B-average ratings) acclaimed(I-average ratings) movies(O) starred(O) ben(B-actor) stiller(I-actor) or(O) his(O) parents(O) in(O) the(O) last(B-year) five(I-year) decades(I-year)"}}
{"id": "9", "dataset": "mit-movie", "split": "test", "label_list": ["character", "actor", "song", "genre", "director", "year", "title", "review", "average ratings", "trailer", "rating", "plot"], "instance": {"id": "9", "words": ["what", "is", "the", "most", "current", "movie", "featuring", "mat", "damon"], "labels": ["O", "O", "O", "O", "B-year", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, actor, song, genre, director, year, title, review, average ratings, trailer, rating, plot and O.\nSentence: what is the most current movie featuring mat damon", "prompt_labels": "what(O) is(O) the(O) most(O) current(B-year) movie(O) featuring(O) mat(B-actor) damon(I-actor)"}}
{"id": "1510", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "year", "character", "title", "average ratings", "trailer", "review", "plot", "director", "actor", "genre", "song"], "instance": {"id": "1510", "words": ["is", "there", "a", "movie", "called", "black", "rain"], "labels": ["O", "O", "O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, year, character, title, average ratings, trailer, review, plot, director, actor, genre, song and O.\nSentence: is there a movie called black rain", "prompt_labels": "is(O) there(O) a(O) movie(O) called(O) black(B-title) rain(I-title)"}}
{"id": "2213", "dataset": "mit-movie", "split": "test", "label_list": ["review", "actor", "title", "trailer", "director", "character", "genre", "average ratings", "plot", "song", "year", "rating"], "instance": {"id": "2213", "words": ["what", "was", "the", "hoyt", "yeatman", "disney", "wildlife", "film", "that", "averaged", "seven", "out", "of", "ten"], "labels": ["O", "O", "O", "B-director", "I-director", "B-genre", "B-plot", "O", "O", "O", "B-average ratings", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, actor, title, trailer, director, character, genre, average ratings, plot, song, year, rating and O.\nSentence: what was the hoyt yeatman disney wildlife film that averaged seven out of ten", "prompt_labels": "what(O) was(O) the(O) hoyt(B-director) yeatman(I-director) disney(B-genre) wildlife(B-plot) film(O) that(O) averaged(O) seven(B-average ratings) out(O) of(O) ten(O)"}}
{"id": "499", "dataset": "mit-movie", "split": "test", "label_list": ["director", "actor", "song", "trailer", "character", "genre", "rating", "review", "average ratings", "title", "plot", "year"], "instance": {"id": "499", "words": ["did", "hans", "zimmer", "write", "the", "music", "for", "pirates", "of", "the", "carribean"], "labels": ["O", "B-song", "I-song", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, song, trailer, character, genre, rating, review, average ratings, title, plot, year and O.\nSentence: did hans zimmer write the music for pirates of the carribean", "prompt_labels": "did(O) hans(B-song) zimmer(I-song) write(O) the(O) music(O) for(O) pirates(B-song) of(I-song) the(I-song) carribean(I-song)"}}
{"id": "333", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "plot", "title", "year", "rating", "genre", "review", "character", "average ratings", "song", "trailer", "director"], "instance": {"id": "333", "words": ["find", "romantic", "comedies", "starring", "jennifer", "lopez"], "labels": ["O", "B-genre", "I-genre", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, plot, title, year, rating, genre, review, character, average ratings, song, trailer, director and O.\nSentence: find romantic comedies starring jennifer lopez", "prompt_labels": "find(O) romantic(B-genre) comedies(I-genre) starring(O) jennifer(B-actor) lopez(I-actor)"}}
{"id": "755", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "review", "year", "plot", "title", "rating", "director", "average ratings", "character", "genre", "song", "trailer"], "instance": {"id": "755", "words": ["what", "is", "the", "best", "movie", "of", "all", "time", "rated", "by", "viewers"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, review, year, plot, title, rating, director, average ratings, character, genre, song, trailer and O.\nSentence: what is the best movie of all time rated by viewers", "prompt_labels": "what(O) is(O) the(O) best(O) movie(O) of(O) all(O) time(O) rated(O) by(O) viewers(B-average ratings)"}}
{"id": "1064", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "rating", "title", "character", "plot", "song", "director", "review", "year", "actor", "genre", "average ratings"], "instance": {"id": "1064", "words": ["a", "must", "see", "romance", "film", "from", "1990", "with", "a", "pg", "13", "rating"], "labels": ["O", "B-average ratings", "I-average ratings", "B-genre", "O", "O", "B-year", "O", "O", "B-rating", "I-rating", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, rating, title, character, plot, song, director, review, year, actor, genre, average ratings and O.\nSentence: a must see romance film from 1990 with a pg 13 rating", "prompt_labels": "a(O) must(B-average ratings) see(I-average ratings) romance(B-genre) film(O) from(O) 1990(B-year) with(O) a(O) pg(B-rating) 13(I-rating) rating(O)"}}
{"id": "197", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "trailer", "genre", "song", "character", "director", "actor", "title", "review", "average ratings", "year", "plot"], "instance": {"id": "197", "words": ["name", "a", "kirk", "douglas", "science", "fiction", "film"], "labels": ["O", "O", "B-actor", "I-actor", "B-genre", "I-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, trailer, genre, song, character, director, actor, title, review, average ratings, year, plot and O.\nSentence: name a kirk douglas science fiction film", "prompt_labels": "name(O) a(O) kirk(B-actor) douglas(I-actor) science(B-genre) fiction(I-genre) film(O)"}}
{"id": "795", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "year", "rating", "genre", "character", "title", "review", "director", "trailer", "song", "average ratings", "plot"], "instance": {"id": "795", "words": ["who", "was", "the", "director", "of", "target", "with", "gene", "hackman"], "labels": ["O", "O", "O", "O", "O", "B-title", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, year, rating, genre, character, title, review, director, trailer, song, average ratings, plot and O.\nSentence: who was the director of target with gene hackman", "prompt_labels": "who(O) was(O) the(O) director(O) of(O) target(B-title) with(O) gene(B-actor) hackman(I-actor)"}}
{"id": "1355", "dataset": "mit-movie", "split": "test", "label_list": ["review", "genre", "actor", "average ratings", "rating", "plot", "title", "director", "character", "year", "song", "trailer"], "instance": {"id": "1355", "words": ["im", "looking", "for", "a", "pg", "13", "documentary", "directed", "by", "shuhei", "morita"], "labels": ["O", "O", "O", "O", "B-rating", "I-rating", "B-genre", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, genre, actor, average ratings, rating, plot, title, director, character, year, song, trailer and O.\nSentence: im looking for a pg 13 documentary directed by shuhei morita", "prompt_labels": "im(O) looking(O) for(O) a(O) pg(B-rating) 13(I-rating) documentary(B-genre) directed(O) by(O) shuhei(B-director) morita(I-director)"}}
{"id": "610", "dataset": "mit-movie", "split": "test", "label_list": ["review", "song", "director", "actor", "genre", "year", "rating", "trailer", "character", "title", "average ratings", "plot"], "instance": {"id": "610", "words": ["find", "the", "air", "bud", "movie", "about", "baseball"], "labels": ["O", "O", "B-character", "I-character", "O", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, director, actor, genre, year, rating, trailer, character, title, average ratings, plot and O.\nSentence: find the air bud movie about baseball", "prompt_labels": "find(O) the(O) air(B-character) bud(I-character) movie(O) about(O) baseball(B-plot)"}}
{"id": "242", "dataset": "mit-movie", "split": "test", "label_list": ["review", "trailer", "average ratings", "character", "year", "plot", "genre", "song", "director", "title", "actor", "rating"], "instance": {"id": "242", "words": ["how", "many", "movies", "has", "clint", "eastwood", "directed", "since", "1995"], "labels": ["O", "O", "O", "O", "B-director", "I-director", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, trailer, average ratings, character, year, plot, genre, song, director, title, actor, rating and O.\nSentence: how many movies has clint eastwood directed since 1995", "prompt_labels": "how(O) many(O) movies(O) has(O) clint(B-director) eastwood(I-director) directed(O) since(B-year) 1995(I-year)"}}
{"id": "977", "dataset": "mit-movie", "split": "test", "label_list": ["review", "year", "character", "rating", "trailer", "plot", "average ratings", "genre", "director", "title", "song", "actor"], "instance": {"id": "977", "words": ["is", "there", "a", "documentary", "about", "a", "kiss", "tribute", "band"], "labels": ["O", "O", "O", "B-genre", "B-plot", "I-plot", "I-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, year, character, rating, trailer, plot, average ratings, genre, director, title, song, actor and O.\nSentence: is there a documentary about a kiss tribute band", "prompt_labels": "is(O) there(O) a(O) documentary(B-genre) about(B-plot) a(I-plot) kiss(I-plot) tribute(I-plot) band(I-plot)"}}
{"id": "781", "dataset": "mit-movie", "split": "test", "label_list": ["character", "actor", "genre", "review", "year", "song", "average ratings", "trailer", "title", "director", "plot", "rating"], "instance": {"id": "781", "words": ["what", "live", "action", "movie", "based", "on", "action", "figure", "toys", "was", "released", "in", "2009"], "labels": ["O", "B-genre", "I-genre", "O", "O", "O", "B-plot", "I-plot", "I-plot", "O", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, actor, genre, review, year, song, average ratings, trailer, title, director, plot, rating and O.\nSentence: what live action movie based on action figure toys was released in 2009", "prompt_labels": "what(O) live(B-genre) action(I-genre) movie(O) based(O) on(O) action(B-plot) figure(I-plot) toys(I-plot) was(O) released(O) in(O) 2009(B-year)"}}
{"id": "1441", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "year", "plot", "actor", "title", "character", "song", "director", "rating", "average ratings", "review", "genre"], "instance": {"id": "1441", "words": ["is", "quentin", "tarantino", "planning", "on", "working", "on", "any", "animated", "films", "soon"], "labels": ["O", "B-director", "I-director", "O", "O", "O", "O", "O", "B-genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, year, plot, actor, title, character, song, director, rating, average ratings, review, genre and O.\nSentence: is quentin tarantino planning on working on any animated films soon", "prompt_labels": "is(O) quentin(B-director) tarantino(I-director) planning(O) on(O) working(O) on(O) any(O) animated(B-genre) films(O) soon(O)"}}
{"id": "1189", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "title", "review", "year", "actor", "rating", "director", "trailer", "song", "character", "plot", "genre"], "instance": {"id": "1189", "words": ["do", "you", "have", "colossus", "the", "forbin", "project"], "labels": ["O", "O", "O", "B-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, title, review, year, actor, rating, director, trailer, song, character, plot, genre and O.\nSentence: do you have colossus the forbin project", "prompt_labels": "do(O) you(O) have(O) colossus(B-title) the(I-title) forbin(I-title) project(I-title)"}}
{"id": "2046", "dataset": "mit-movie", "split": "test", "label_list": ["song", "actor", "trailer", "plot", "average ratings", "title", "genre", "year", "review", "rating", "character", "director"], "instance": {"id": "2046", "words": ["what", "is", "a", "well", "rated", "action", "movie", "about", "prisoners"], "labels": ["O", "O", "O", "B-average ratings", "I-average ratings", "B-genre", "O", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, actor, trailer, plot, average ratings, title, genre, year, review, rating, character, director and O.\nSentence: what is a well rated action movie about prisoners", "prompt_labels": "what(O) is(O) a(O) well(B-average ratings) rated(I-average ratings) action(B-genre) movie(O) about(O) prisoners(B-plot)"}}
{"id": "956", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "song", "review", "trailer", "plot", "character", "actor", "year", "rating", "average ratings", "title", "director"], "instance": {"id": "956", "words": ["looking", "for", "the", "movie", "with", "a", "black", "man", "as", "an", "alien", "directed", "by", "john", "sayles"], "labels": ["O", "O", "O", "O", "O", "O", "B-plot", "I-plot", "I-plot", "I-plot", "I-plot", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, song, review, trailer, plot, character, actor, year, rating, average ratings, title, director and O.\nSentence: looking for the movie with a black man as an alien directed by john sayles", "prompt_labels": "looking(O) for(O) the(O) movie(O) with(O) a(O) black(B-plot) man(I-plot) as(I-plot) an(I-plot) alien(I-plot) directed(O) by(O) john(B-director) sayles(I-director)"}}
{"id": "1278", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "genre", "average ratings", "character", "trailer", "rating", "title", "plot", "song", "review", "director", "year"], "instance": {"id": "1278", "words": ["has", "michael", "keusch", "directed", "any", "rated", "r", "musicals", "in", "the", "past", "ten", "decades"], "labels": ["O", "B-director", "I-director", "O", "O", "O", "B-rating", "B-genre", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, genre, average ratings, character, trailer, rating, title, plot, song, review, director, year and O.\nSentence: has michael keusch directed any rated r musicals in the past ten decades", "prompt_labels": "has(O) michael(B-director) keusch(I-director) directed(O) any(O) rated(O) r(B-rating) musicals(B-genre) in(O) the(O) past(B-year) ten(I-year) decades(I-year)"}}
{"id": "1107", "dataset": "mit-movie", "split": "test", "label_list": ["character", "director", "review", "genre", "actor", "average ratings", "year", "title", "song", "plot", "rating", "trailer"], "instance": {"id": "1107", "words": ["can", "you", "help", "me", "find", "some", "good", "comedy", "movies"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, director, review, genre, actor, average ratings, year, title, song, plot, rating, trailer and O.\nSentence: can you help me find some good comedy movies", "prompt_labels": "can(O) you(O) help(O) me(O) find(O) some(O) good(O) comedy(B-genre) movies(O)"}}
{"id": "2293", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "actor", "rating", "character", "review", "plot", "title", "year", "song", "trailer", "average ratings", "director"], "instance": {"id": "2293", "words": ["which", "drama", "movie", "released", "in", "the", "past", "decade", "starred", "phil", "hartman"], "labels": ["O", "B-genre", "O", "O", "O", "O", "B-year", "I-year", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, actor, rating, character, review, plot, title, year, song, trailer, average ratings, director and O.\nSentence: which drama movie released in the past decade starred phil hartman", "prompt_labels": "which(O) drama(B-genre) movie(O) released(O) in(O) the(O) past(B-year) decade(I-year) starred(O) phil(B-actor) hartman(I-actor)"}}
{"id": "835", "dataset": "mit-movie", "split": "test", "label_list": ["title", "review", "song", "character", "year", "trailer", "actor", "genre", "director", "plot", "rating", "average ratings"], "instance": {"id": "835", "words": ["is", "there", "a", "sequal", "to", "crash", "of", "the", "titans"], "labels": ["O", "O", "O", "O", "O", "B-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, review, song, character, year, trailer, actor, genre, director, plot, rating, average ratings and O.\nSentence: is there a sequal to crash of the titans", "prompt_labels": "is(O) there(O) a(O) sequal(O) to(O) crash(B-title) of(I-title) the(I-title) titans(I-title)"}}
{"id": "639", "dataset": "mit-movie", "split": "test", "label_list": ["song", "year", "genre", "title", "actor", "average ratings", "review", "plot", "character", "rating", "trailer", "director"], "instance": {"id": "639", "words": ["find", "me", "a", "movie", "with", "the", "song", "a", "whole", "new", "world"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, year, genre, title, actor, average ratings, review, plot, character, rating, trailer, director and O.\nSentence: find me a movie with the song a whole new world", "prompt_labels": "find(O) me(O) a(O) movie(O) with(O) the(O) song(O) a(B-song) whole(I-song) new(I-song) world(I-song)"}}
{"id": "1431", "dataset": "mit-movie", "split": "test", "label_list": ["review", "rating", "character", "trailer", "song", "average ratings", "genre", "actor", "title", "year", "director", "plot"], "instance": {"id": "1431", "words": ["is", "david", "lean", "a", "highly", "recommended", "director"], "labels": ["O", "B-director", "I-director", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, rating, character, trailer, song, average ratings, genre, actor, title, year, director, plot and O.\nSentence: is david lean a highly recommended director", "prompt_labels": "is(O) david(B-director) lean(I-director) a(O) highly(O) recommended(O) director(O)"}}
{"id": "1911", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "director", "actor", "rating", "year", "average ratings", "review", "title", "genre", "plot", "song", "character"], "instance": {"id": "1911", "words": ["what", "are", "some", "must", "see", "westerns", "from", "the", "1950", "s"], "labels": ["O", "O", "O", "B-average ratings", "I-average ratings", "B-genre", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, director, actor, rating, year, average ratings, review, title, genre, plot, song, character and O.\nSentence: what are some must see westerns from the 1950 s", "prompt_labels": "what(O) are(O) some(O) must(B-average ratings) see(I-average ratings) westerns(B-genre) from(O) the(O) 1950(B-year) s(I-year)"}}
{"id": "1258", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "director", "trailer", "rating", "song", "genre", "average ratings", "title", "review", "actor", "year", "character"], "instance": {"id": "1258", "words": ["has", "charlton", "heston", "ever", "been", "a", "voice", "in", "an", "animated", "movie"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "O", "O", "O", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, director, trailer, rating, song, genre, average ratings, title, review, actor, year, character and O.\nSentence: has charlton heston ever been a voice in an animated movie", "prompt_labels": "has(O) charlton(B-actor) heston(I-actor) ever(O) been(O) a(O) voice(O) in(O) an(O) animated(B-genre) movie(O)"}}
{"id": "1851", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "actor", "average ratings", "rating", "director", "plot", "genre", "character", "year", "song", "title", "review"], "instance": {"id": "1851", "words": ["what", "pg", "13", "war", "movie", "has", "spencer", "tracy", "starred", "in"], "labels": ["O", "B-rating", "I-rating", "B-genre", "O", "O", "B-actor", "I-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, actor, average ratings, rating, director, plot, genre, character, year, song, title, review and O.\nSentence: what pg 13 war movie has spencer tracy starred in", "prompt_labels": "what(O) pg(B-rating) 13(I-rating) war(B-genre) movie(O) has(O) spencer(B-actor) tracy(I-actor) starred(O) in(O)"}}
{"id": "114", "dataset": "mit-movie", "split": "test", "label_list": ["title", "review", "year", "director", "actor", "character", "song", "plot", "trailer", "genre", "rating", "average ratings"], "instance": {"id": "114", "words": ["what", "movie", "won", "the", "most", "awards", "in", "2005"], "labels": ["O", "O", "O", "O", "B-average ratings", "I-average ratings", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, review, year, director, actor, character, song, plot, trailer, genre, rating, average ratings and O.\nSentence: what movie won the most awards in 2005", "prompt_labels": "what(O) movie(O) won(O) the(O) most(B-average ratings) awards(I-average ratings) in(O) 2005(B-year)"}}
{"id": "870", "dataset": "mit-movie", "split": "test", "label_list": ["title", "average ratings", "rating", "actor", "plot", "director", "review", "trailer", "song", "character", "year", "genre"], "instance": {"id": "870", "words": ["where", "was", "red", "tails", "filmed"], "labels": ["O", "O", "B-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, average ratings, rating, actor, plot, director, review, trailer, song, character, year, genre and O.\nSentence: where was red tails filmed", "prompt_labels": "where(O) was(O) red(B-title) tails(I-title) filmed(O)"}}
{"id": "400", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "plot", "average ratings", "review", "rating", "trailer", "director", "year", "actor", "title", "song", "character"], "instance": {"id": "400", "words": ["which", "movies", "about", "the", "mob", "are", "rated", "r"], "labels": ["O", "O", "O", "O", "B-plot", "O", "B-rating", "I-rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, plot, average ratings, review, rating, trailer, director, year, actor, title, song, character and O.\nSentence: which movies about the mob are rated r", "prompt_labels": "which(O) movies(O) about(O) the(O) mob(B-plot) are(O) rated(B-rating) r(I-rating)"}}
{"id": "1124", "dataset": "mit-movie", "split": "test", "label_list": ["review", "plot", "genre", "actor", "rating", "director", "trailer", "song", "title", "character", "average ratings", "year"], "instance": {"id": "1124", "words": ["details", "about", "scooby", "doo", "and", "the", "loch", "ness", "monster", "kids", "movie"], "labels": ["O", "O", "B-title", "I-title", "I-title", "I-title", "I-title", "I-title", "I-title", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, genre, actor, rating, director, trailer, song, title, character, average ratings, year and O.\nSentence: details about scooby doo and the loch ness monster kids movie", "prompt_labels": "details(O) about(O) scooby(B-title) doo(I-title) and(I-title) the(I-title) loch(I-title) ness(I-title) monster(I-title) kids(O) movie(O)"}}
{"id": "735", "dataset": "mit-movie", "split": "test", "label_list": ["character", "average ratings", "trailer", "title", "actor", "year", "plot", "review", "song", "rating", "director", "genre"], "instance": {"id": "735", "words": ["find", "a", "scary", "movie", "directed", "by", "stephen", "spillburg"], "labels": ["O", "O", "B-genre", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, average ratings, trailer, title, actor, year, plot, review, song, rating, director, genre and O.\nSentence: find a scary movie directed by stephen spillburg", "prompt_labels": "find(O) a(O) scary(B-genre) movie(O) directed(O) by(O) stephen(B-director) spillburg(I-director)"}}
{"id": "1336", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "year", "average ratings", "review", "character", "genre", "title", "plot", "actor", "song", "rating", "director"], "instance": {"id": "1336", "words": ["i", "would", "like", "an", "emotional", "type", "movie"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, year, average ratings, review, character, genre, title, plot, actor, song, rating, director and O.\nSentence: i would like an emotional type movie", "prompt_labels": "i(O) would(O) like(O) an(O) emotional(B-genre) type(O) movie(O)"}}
{"id": "1315", "dataset": "mit-movie", "split": "test", "label_list": ["character", "title", "year", "trailer", "actor", "genre", "review", "director", "song", "average ratings", "rating", "plot"], "instance": {"id": "1315", "words": ["i", "am", "looking", "for", "an", "arnold", "schwarzenegger", "action", "movie", "from", "the", "past", "five", "years"], "labels": ["O", "O", "O", "O", "O", "B-actor", "I-actor", "B-genre", "O", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, title, year, trailer, actor, genre, review, director, song, average ratings, rating, plot and O.\nSentence: i am looking for an arnold schwarzenegger action movie from the past five years", "prompt_labels": "i(O) am(O) looking(O) for(O) an(O) arnold(B-actor) schwarzenegger(I-actor) action(B-genre) movie(O) from(O) the(O) past(B-year) five(I-year) years(I-year)"}}
{"id": "1545", "dataset": "mit-movie", "split": "test", "label_list": ["character", "song", "review", "year", "trailer", "title", "plot", "average ratings", "rating", "genre", "director", "actor"], "instance": {"id": "1545", "words": ["is", "there", "a", "western", "movie", "starring", "charlton", "heston"], "labels": ["O", "O", "O", "B-genre", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, song, review, year, trailer, title, plot, average ratings, rating, genre, director, actor and O.\nSentence: is there a western movie starring charlton heston", "prompt_labels": "is(O) there(O) a(O) western(B-genre) movie(O) starring(O) charlton(B-actor) heston(I-actor)"}}
{"id": "1529", "dataset": "mit-movie", "split": "test", "label_list": ["year", "character", "actor", "plot", "rating", "review", "director", "title", "genre", "trailer", "average ratings", "song"], "instance": {"id": "1529", "words": ["is", "there", "a", "musical", "movie", "which", "director", "is", "francis", "ford", "coppola"], "labels": ["O", "O", "O", "B-genre", "O", "O", "O", "O", "B-director", "I-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, character, actor, plot, rating, review, director, title, genre, trailer, average ratings, song and O.\nSentence: is there a musical movie which director is francis ford coppola", "prompt_labels": "is(O) there(O) a(O) musical(B-genre) movie(O) which(O) director(O) is(O) francis(B-director) ford(I-director) coppola(I-director)"}}
{"id": "384", "dataset": "mit-movie", "split": "test", "label_list": ["character", "trailer", "rating", "actor", "year", "title", "review", "average ratings", "director", "song", "plot", "genre"], "instance": {"id": "384", "words": ["who", "was", "the", "cast", "of", "terminator"], "labels": ["O", "O", "O", "B-actor", "O", "B-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, trailer, rating, actor, year, title, review, average ratings, director, song, plot, genre and O.\nSentence: who was the cast of terminator", "prompt_labels": "who(O) was(O) the(O) cast(B-actor) of(O) terminator(B-title)"}}
{"id": "1593", "dataset": "mit-movie", "split": "test", "label_list": ["director", "plot", "year", "trailer", "genre", "rating", "average ratings", "character", "actor", "title", "song", "review"], "instance": {"id": "1593", "words": ["list", "a", "nc", "17", "rated", "military", "films", "directed", "by", "zelda", "barron", "in", "1980", "that", "was", "ok"], "labels": ["O", "O", "B-rating", "I-rating", "O", "B-genre", "O", "O", "O", "B-director", "I-director", "O", "B-year", "O", "O", "B-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, plot, year, trailer, genre, rating, average ratings, character, actor, title, song, review and O.\nSentence: list a nc 17 rated military films directed by zelda barron in 1980 that was ok", "prompt_labels": "list(O) a(O) nc(B-rating) 17(I-rating) rated(O) military(B-genre) films(O) directed(O) by(O) zelda(B-director) barron(I-director) in(O) 1980(B-year) that(O) was(O) ok(B-average ratings)"}}
{"id": "147", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "title", "trailer", "rating", "character", "year", "actor", "song", "review", "director", "average ratings", "genre"], "instance": {"id": "147", "words": ["what", "is", "a", "recent", "george", "clooney", "movie", "with", "high", "viewers", "rating"], "labels": ["O", "O", "O", "O", "B-actor", "I-actor", "O", "O", "B-average ratings", "I-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, title, trailer, rating, character, year, actor, song, review, director, average ratings, genre and O.\nSentence: what is a recent george clooney movie with high viewers rating", "prompt_labels": "what(O) is(O) a(O) recent(O) george(B-actor) clooney(I-actor) movie(O) with(O) high(B-average ratings) viewers(I-average ratings) rating(I-average ratings)"}}
{"id": "1300", "dataset": "mit-movie", "split": "test", "label_list": ["song", "title", "character", "trailer", "plot", "average ratings", "rating", "review", "year", "genre", "director", "actor"], "instance": {"id": "1300", "words": ["how", "did", "you", "like", "4", "months", "3", "weeks", "and", "2", "days"], "labels": ["O", "O", "O", "O", "B-title", "I-title", "I-title", "I-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, title, character, trailer, plot, average ratings, rating, review, year, genre, director, actor and O.\nSentence: how did you like 4 months 3 weeks and 2 days", "prompt_labels": "how(O) did(O) you(O) like(O) 4(B-title) months(I-title) 3(I-title) weeks(I-title) and(I-title) 2(I-title) days(I-title)"}}
{"id": "1807", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "plot", "review", "average ratings", "rating", "trailer", "title", "song", "director", "actor", "character", "year"], "instance": {"id": "1807", "words": ["was", "that", "michael", "bergin", "scary", "pg", "13", "movie", "made", "in", "the", "2000", "s", "all", "right"], "labels": ["O", "O", "B-actor", "I-actor", "B-genre", "B-rating", "I-rating", "O", "O", "O", "O", "B-year", "I-year", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, plot, review, average ratings, rating, trailer, title, song, director, actor, character, year and O.\nSentence: was that michael bergin scary pg 13 movie made in the 2000 s all right", "prompt_labels": "was(O) that(O) michael(B-actor) bergin(I-actor) scary(B-genre) pg(B-rating) 13(I-rating) movie(O) made(O) in(O) the(O) 2000(B-year) s(I-year) all(B-average ratings) right(I-average ratings)"}}
{"id": "401", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "director", "plot", "year", "title", "trailer", "average ratings", "review", "song", "actor", "character", "rating"], "instance": {"id": "401", "words": ["who", "acted", "in", "the", "title", "role", "of", "michael", "collins"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, director, plot, year, title, trailer, average ratings, review, song, actor, character, rating and O.\nSentence: who acted in the title role of michael collins", "prompt_labels": "who(O) acted(O) in(O) the(O) title(O) role(O) of(O) michael(B-title) collins(I-title)"}}
{"id": "864", "dataset": "mit-movie", "split": "test", "label_list": ["character", "average ratings", "genre", "plot", "rating", "director", "song", "title", "year", "trailer", "actor", "review"], "instance": {"id": "864", "words": ["show", "me", "harry", "potter", "movies", "from", "the", "2000s"], "labels": ["O", "O", "B-character", "I-character", "O", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, average ratings, genre, plot, rating, director, song, title, year, trailer, actor, review and O.\nSentence: show me harry potter movies from the 2000s", "prompt_labels": "show(O) me(O) harry(B-character) potter(I-character) movies(O) from(O) the(O) 2000s(B-year)"}}
{"id": "1853", "dataset": "mit-movie", "split": "test", "label_list": ["character", "actor", "trailer", "review", "average ratings", "year", "plot", "song", "director", "genre", "rating", "title"], "instance": {"id": "1853", "words": ["what", "pg", "13", "james", "bedford", "mystery", "got", "two", "thumbs", "up", "in", "the", "1980", "s"], "labels": ["O", "B-rating", "I-rating", "B-director", "I-director", "B-genre", "O", "B-average ratings", "I-average ratings", "I-average ratings", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, actor, trailer, review, average ratings, year, plot, song, director, genre, rating, title and O.\nSentence: what pg 13 james bedford mystery got two thumbs up in the 1980 s", "prompt_labels": "what(O) pg(B-rating) 13(I-rating) james(B-director) bedford(I-director) mystery(B-genre) got(O) two(B-average ratings) thumbs(I-average ratings) up(I-average ratings) in(O) the(O) 1980(B-year) s(I-year)"}}
{"id": "2321", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "actor", "director", "title", "character", "year", "review", "trailer", "average ratings", "genre", "song", "plot"], "instance": {"id": "2321", "words": ["who", "starred", "in", "bridesmaids"], "labels": ["O", "O", "O", "B-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, actor, director, title, character, year, review, trailer, average ratings, genre, song, plot and O.\nSentence: who starred in bridesmaids", "prompt_labels": "who(O) starred(O) in(O) bridesmaids(B-title)"}}
{"id": "1454", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "rating", "year", "character", "title", "director", "actor", "genre", "trailer", "review", "plot", "song"], "instance": {"id": "1454", "words": ["is", "there", "a", "bill", "paxton", "movie", "in", "the", "last", "seven", "decades"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, rating, year, character, title, director, actor, genre, trailer, review, plot, song and O.\nSentence: is there a bill paxton movie in the last seven decades", "prompt_labels": "is(O) there(O) a(O) bill(B-actor) paxton(I-actor) movie(O) in(O) the(O) last(B-year) seven(I-year) decades(I-year)"}}
{"id": "1209", "dataset": "mit-movie", "split": "test", "label_list": ["director", "review", "rating", "genre", "character", "plot", "trailer", "average ratings", "actor", "year", "title", "song"], "instance": {"id": "1209", "words": ["do", "you", "have", "the", "movie", "key", "largo"], "labels": ["O", "O", "O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, review, rating, genre, character, plot, trailer, average ratings, actor, year, title, song and O.\nSentence: do you have the movie key largo", "prompt_labels": "do(O) you(O) have(O) the(O) movie(O) key(B-title) largo(I-title)"}}
{"id": "549", "dataset": "mit-movie", "split": "test", "label_list": ["character", "year", "trailer", "actor", "director", "song", "review", "plot", "average ratings", "genre", "title", "rating"], "instance": {"id": "549", "words": ["what", "movie", "had", "jim", "carrey", "as", "a", "dr", "seuss", "character"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O", "O", "B-character", "I-character", "I-character"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, year, trailer, actor, director, song, review, plot, average ratings, genre, title, rating and O.\nSentence: what movie had jim carrey as a dr seuss character", "prompt_labels": "what(O) movie(O) had(O) jim(B-actor) carrey(I-actor) as(O) a(O) dr(B-character) seuss(I-character) character(I-character)"}}
{"id": "2171", "dataset": "mit-movie", "split": "test", "label_list": ["year", "genre", "actor", "rating", "review", "song", "plot", "character", "average ratings", "director", "trailer", "title"], "instance": {"id": "2171", "words": ["what", "rated", "r", "horror", "is", "there", "involving", "a", "shower", "murder"], "labels": ["O", "O", "B-rating", "B-genre", "O", "O", "O", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, genre, actor, rating, review, song, plot, character, average ratings, director, trailer, title and O.\nSentence: what rated r horror is there involving a shower murder", "prompt_labels": "what(O) rated(O) r(B-rating) horror(B-genre) is(O) there(O) involving(O) a(O) shower(B-plot) murder(I-plot)"}}
{"id": "421", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "actor", "trailer", "genre", "song", "title", "year", "character", "review", "plot", "director", "rating"], "instance": {"id": "421", "words": ["what", "is", "a", "rated", "r", "film", "that", "features", "katherine", "heigl"], "labels": ["O", "O", "O", "B-rating", "I-rating", "O", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, actor, trailer, genre, song, title, year, character, review, plot, director, rating and O.\nSentence: what is a rated r film that features katherine heigl", "prompt_labels": "what(O) is(O) a(O) rated(B-rating) r(I-rating) film(O) that(O) features(O) katherine(B-actor) heigl(I-actor)"}}
{"id": "2349", "dataset": "mit-movie", "split": "test", "label_list": ["director", "average ratings", "trailer", "year", "plot", "character", "rating", "review", "genre", "actor", "title", "song"], "instance": {"id": "2349", "words": ["did", "the", "shining", "scare", "charlize", "theron", "when", "she", "was", "a", "kid"], "labels": ["O", "B-title", "I-title", "O", "B-actor", "I-actor", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, average ratings, trailer, year, plot, character, rating, review, genre, actor, title, song and O.\nSentence: did the shining scare charlize theron when she was a kid", "prompt_labels": "did(O) the(B-title) shining(I-title) scare(O) charlize(B-actor) theron(I-actor) when(O) she(O) was(O) a(O) kid(O)"}}
{"id": "1309", "dataset": "mit-movie", "split": "test", "label_list": ["character", "genre", "title", "year", "trailer", "director", "review", "actor", "plot", "song", "average ratings", "rating"], "instance": {"id": "1309", "words": ["i", "am", "interested", "in", "a", "pg", "13", "fantasy", "movie", "that", "was", "directed", "by", "dusty", "nelson", "and", "has", "an", "average", "rating", "of", "seven", "stars"], "labels": ["O", "O", "O", "O", "O", "B-rating", "I-rating", "B-genre", "O", "O", "O", "O", "O", "B-director", "I-director", "O", "O", "O", "O", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, genre, title, year, trailer, director, review, actor, plot, song, average ratings, rating and O.\nSentence: i am interested in a pg 13 fantasy movie that was directed by dusty nelson and has an average rating of seven stars", "prompt_labels": "i(O) am(O) interested(O) in(O) a(O) pg(B-rating) 13(I-rating) fantasy(B-genre) movie(O) that(O) was(O) directed(O) by(O) dusty(B-director) nelson(I-director) and(O) has(O) an(O) average(O) rating(O) of(O) seven(B-average ratings) stars(I-average ratings)"}}
{"id": "2164", "dataset": "mit-movie", "split": "test", "label_list": ["director", "song", "character", "trailer", "title", "review", "genre", "rating", "average ratings", "actor", "plot", "year"], "instance": {"id": "2164", "words": ["what", "mystery", "starring", "jane", "seymour", "is", "about", "lies"], "labels": ["O", "B-genre", "O", "B-actor", "I-actor", "O", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, song, character, trailer, title, review, genre, rating, average ratings, actor, plot, year and O.\nSentence: what mystery starring jane seymour is about lies", "prompt_labels": "what(O) mystery(B-genre) starring(O) jane(B-actor) seymour(I-actor) is(O) about(O) lies(B-plot)"}}
{"id": "2165", "dataset": "mit-movie", "split": "test", "label_list": ["year", "plot", "average ratings", "director", "song", "title", "trailer", "character", "review", "actor", "rating", "genre"], "instance": {"id": "2165", "words": ["what", "nuclear", "war", "movies", "are", "rated", "pg", "13"], "labels": ["O", "B-plot", "I-plot", "O", "O", "O", "B-rating", "I-rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, plot, average ratings, director, song, title, trailer, character, review, actor, rating, genre and O.\nSentence: what nuclear war movies are rated pg 13", "prompt_labels": "what(O) nuclear(B-plot) war(I-plot) movies(O) are(O) rated(O) pg(B-rating) 13(I-rating)"}}
{"id": "36", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "rating", "trailer", "character", "average ratings", "year", "review", "director", "genre", "actor", "song", "title"], "instance": {"id": "36", "words": ["what", "is", "brad", "pitts", "first", "movie"], "labels": ["O", "O", "B-actor", "I-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, rating, trailer, character, average ratings, year, review, director, genre, actor, song, title and O.\nSentence: what is brad pitts first movie", "prompt_labels": "what(O) is(O) brad(B-actor) pitts(I-actor) first(O) movie(O)"}}
{"id": "1516", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "song", "year", "trailer", "genre", "rating", "actor", "character", "plot", "title", "director", "review"], "instance": {"id": "1516", "words": ["is", "there", "a", "movie", "called", "the", "night", "on", "earth"], "labels": ["O", "O", "O", "O", "O", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, song, year, trailer, genre, rating, actor, character, plot, title, director, review and O.\nSentence: is there a movie called the night on earth", "prompt_labels": "is(O) there(O) a(O) movie(O) called(O) the(O) night(B-title) on(I-title) earth(I-title)"}}
{"id": "180", "dataset": "mit-movie", "split": "test", "label_list": ["character", "title", "review", "trailer", "rating", "year", "plot", "genre", "average ratings", "song", "director", "actor"], "instance": {"id": "180", "words": ["what", "movie", "stars", "reese", "witherspoon", "in", "2004"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, title, review, trailer, rating, year, plot, genre, average ratings, song, director, actor and O.\nSentence: what movie stars reese witherspoon in 2004", "prompt_labels": "what(O) movie(O) stars(O) reese(B-actor) witherspoon(I-actor) in(O) 2004(B-year)"}}
{"id": "1960", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "title", "plot", "rating", "year", "genre", "average ratings", "character", "director", "actor", "song", "review"], "instance": {"id": "1960", "words": ["what", "good", "crime", "movie", "revolves", "around", "crime", "bosses"], "labels": ["O", "O", "B-genre", "O", "O", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, title, plot, rating, year, genre, average ratings, character, director, actor, song, review and O.\nSentence: what good crime movie revolves around crime bosses", "prompt_labels": "what(O) good(O) crime(B-genre) movie(O) revolves(O) around(O) crime(B-plot) bosses(I-plot)"}}
{"id": "1465", "dataset": "mit-movie", "split": "test", "label_list": ["year", "actor", "rating", "song", "director", "average ratings", "trailer", "title", "review", "character", "plot", "genre"], "instance": {"id": "1465", "words": ["is", "there", "a", "pg", "13", "mockumentary", "that", "is", "liked", "by", "many", "and", "starring", "nicolas", "cage", "from", "the", "last", "nine", "years"], "labels": ["O", "O", "O", "B-rating", "I-rating", "B-genre", "O", "O", "B-average ratings", "I-average ratings", "I-average ratings", "O", "O", "B-actor", "I-actor", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, actor, rating, song, director, average ratings, trailer, title, review, character, plot, genre and O.\nSentence: is there a pg 13 mockumentary that is liked by many and starring nicolas cage from the last nine years", "prompt_labels": "is(O) there(O) a(O) pg(B-rating) 13(I-rating) mockumentary(B-genre) that(O) is(O) liked(B-average ratings) by(I-average ratings) many(I-average ratings) and(O) starring(O) nicolas(B-actor) cage(I-actor) from(O) the(O) last(B-year) nine(I-year) years(I-year)"}}
{"id": "1128", "dataset": "mit-movie", "split": "test", "label_list": ["review", "director", "plot", "song", "actor", "trailer", "year", "genre", "character", "rating", "average ratings", "title"], "instance": {"id": "1128", "words": ["did", "brian", "de", "palma", "direct", "any", "movies", "for", "children"], "labels": ["O", "B-director", "I-director", "I-director", "O", "O", "O", "O", "B-genre"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, director, plot, song, actor, trailer, year, genre, character, rating, average ratings, title and O.\nSentence: did brian de palma direct any movies for children", "prompt_labels": "did(O) brian(B-director) de(I-director) palma(I-director) direct(O) any(O) movies(O) for(O) children(B-genre)"}}
{"id": "1784", "dataset": "mit-movie", "split": "test", "label_list": ["year", "song", "rating", "character", "average ratings", "plot", "genre", "actor", "trailer", "director", "title", "review"], "instance": {"id": "1784", "words": ["unrated", "mystery", "in", "the", "past", "seven", "years", "that", "has", "four", "stars"], "labels": ["B-rating", "B-genre", "O", "O", "B-year", "I-year", "I-year", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, song, rating, character, average ratings, plot, genre, actor, trailer, director, title, review and O.\nSentence: unrated mystery in the past seven years that has four stars", "prompt_labels": "unrated(B-rating) mystery(B-genre) in(O) the(O) past(B-year) seven(I-year) years(I-year) that(O) has(O) four(B-average ratings) stars(I-average ratings)"}}
{"id": "1215", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "rating", "actor", "song", "review", "year", "plot", "director", "average ratings", "character", "genre", "title"], "instance": {"id": "1215", "words": ["do", "you", "know", "of", "the", "r", "rated", "movie", "starring", "mark", "dacascos", "in", "a", "war", "movie", "during", "the", "vietcong", "vietnam", "that", "was", "liked", "by", "many"], "labels": ["O", "O", "O", "O", "O", "B-rating", "O", "O", "O", "B-actor", "I-actor", "O", "O", "B-genre", "O", "O", "O", "B-plot", "I-plot", "O", "O", "B-average ratings", "I-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, rating, actor, song, review, year, plot, director, average ratings, character, genre, title and O.\nSentence: do you know of the r rated movie starring mark dacascos in a war movie during the vietcong vietnam that was liked by many", "prompt_labels": "do(O) you(O) know(O) of(O) the(O) r(B-rating) rated(O) movie(O) starring(O) mark(B-actor) dacascos(I-actor) in(O) a(O) war(B-genre) movie(O) during(O) the(O) vietcong(B-plot) vietnam(I-plot) that(O) was(O) liked(B-average ratings) by(I-average ratings) many(I-average ratings)"}}
{"id": "2206", "dataset": "mit-movie", "split": "test", "label_list": ["song", "year", "rating", "character", "plot", "actor", "average ratings", "director", "title", "review", "genre", "trailer"], "instance": {"id": "2206", "words": ["what", "was", "a", "very", "popular", "science", "fiction", "movie", "that", "came", "out", "in", "the", "last", "nine", "years", "and", "was", "directed", "by", "rusty", "cundieff"], "labels": ["O", "O", "O", "B-average ratings", "I-average ratings", "B-genre", "I-genre", "O", "O", "O", "O", "O", "O", "B-year", "I-year", "I-year", "O", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, year, rating, character, plot, actor, average ratings, director, title, review, genre, trailer and O.\nSentence: what was a very popular science fiction movie that came out in the last nine years and was directed by rusty cundieff", "prompt_labels": "what(O) was(O) a(O) very(B-average ratings) popular(I-average ratings) science(B-genre) fiction(I-genre) movie(O) that(O) came(O) out(O) in(O) the(O) last(B-year) nine(I-year) years(I-year) and(O) was(O) directed(O) by(O) rusty(B-director) cundieff(I-director)"}}
{"id": "642", "dataset": "mit-movie", "split": "test", "label_list": ["song", "actor", "title", "director", "character", "plot", "year", "average ratings", "review", "trailer", "rating", "genre"], "instance": {"id": "642", "words": ["find", "me", "a", "movie", "about", "serial", "killers"], "labels": ["O", "O", "O", "O", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, actor, title, director, character, plot, year, average ratings, review, trailer, rating, genre and O.\nSentence: find me a movie about serial killers", "prompt_labels": "find(O) me(O) a(O) movie(O) about(O) serial(B-plot) killers(I-plot)"}}
{"id": "1519", "dataset": "mit-movie", "split": "test", "label_list": ["song", "character", "rating", "actor", "year", "trailer", "title", "genre", "director", "average ratings", "plot", "review"], "instance": {"id": "1519", "words": ["is", "there", "a", "movie", "from", "the", "1990", "s", "about", "romance", "and", "rated", "r", "starring", "julie", "andrews", "and", "has", "excellent", "ratings"], "labels": ["O", "O", "O", "O", "O", "O", "B-year", "I-year", "O", "B-genre", "O", "O", "B-rating", "O", "B-actor", "I-actor", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, character, rating, actor, year, trailer, title, genre, director, average ratings, plot, review and O.\nSentence: is there a movie from the 1990 s about romance and rated r starring julie andrews and has excellent ratings", "prompt_labels": "is(O) there(O) a(O) movie(O) from(O) the(O) 1990(B-year) s(I-year) about(O) romance(B-genre) and(O) rated(O) r(B-rating) starring(O) julie(B-actor) andrews(I-actor) and(O) has(O) excellent(B-average ratings) ratings(I-average ratings)"}}
{"id": "2104", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "genre", "director", "year", "title", "actor", "average ratings", "character", "review", "song", "plot", "rating"], "instance": {"id": "2104", "words": ["what", "is", "the", "name", "of", "the", "unrated", "military", "movie", "about", "d", "day", "released", "in", "2010", "directed", "by", "peter", "kuran"], "labels": ["O", "O", "O", "O", "O", "O", "B-rating", "B-genre", "O", "O", "B-plot", "I-plot", "O", "O", "B-year", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, genre, director, year, title, actor, average ratings, character, review, song, plot, rating and O.\nSentence: what is the name of the unrated military movie about d day released in 2010 directed by peter kuran", "prompt_labels": "what(O) is(O) the(O) name(O) of(O) the(O) unrated(B-rating) military(B-genre) movie(O) about(O) d(B-plot) day(I-plot) released(O) in(O) 2010(B-year) directed(O) by(O) peter(B-director) kuran(I-director)"}}
{"id": "1537", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "year", "rating", "director", "title", "trailer", "song", "average ratings", "review", "genre", "character", "plot"], "instance": {"id": "1537", "words": ["is", "there", "a", "short", "dean", "stockwell", "movie", "rated", "nc", "17", "from", "the", "past", "eight", "decades"], "labels": ["O", "O", "O", "B-genre", "B-actor", "I-actor", "O", "O", "B-rating", "I-rating", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, year, rating, director, title, trailer, song, average ratings, review, genre, character, plot and O.\nSentence: is there a short dean stockwell movie rated nc 17 from the past eight decades", "prompt_labels": "is(O) there(O) a(O) short(B-genre) dean(B-actor) stockwell(I-actor) movie(O) rated(O) nc(B-rating) 17(I-rating) from(O) the(O) past(B-year) eight(I-year) decades(I-year)"}}
{"id": "2174", "dataset": "mit-movie", "split": "test", "label_list": ["song", "director", "character", "title", "average ratings", "genre", "rating", "trailer", "plot", "actor", "year", "review"], "instance": {"id": "2174", "words": ["what", "rated", "r", "movie", "has", "gary", "david", "goldberg", "directed", "about", "satire", "in", "the", "past", "seven", "years"], "labels": ["O", "O", "B-rating", "O", "O", "B-director", "I-director", "I-director", "O", "O", "B-plot", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, director, character, title, average ratings, genre, rating, trailer, plot, actor, year, review and O.\nSentence: what rated r movie has gary david goldberg directed about satire in the past seven years", "prompt_labels": "what(O) rated(O) r(B-rating) movie(O) has(O) gary(B-director) david(I-director) goldberg(I-director) directed(O) about(O) satire(B-plot) in(O) the(O) past(B-year) seven(I-year) years(I-year)"}}
{"id": "2110", "dataset": "mit-movie", "split": "test", "label_list": ["title", "director", "character", "average ratings", "genre", "plot", "year", "trailer", "review", "song", "actor", "rating"], "instance": {"id": "2110", "words": ["what", "is", "the", "plot", "of", "the", "movie", "us", "now"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, director, character, average ratings, genre, plot, year, trailer, review, song, actor, rating and O.\nSentence: what is the plot of the movie us now", "prompt_labels": "what(O) is(O) the(O) plot(O) of(O) the(O) movie(O) us(B-title) now(I-title)"}}
{"id": "2016", "dataset": "mit-movie", "split": "test", "label_list": ["character", "review", "trailer", "actor", "title", "average ratings", "song", "director", "genre", "rating", "plot", "year"], "instance": {"id": "2016", "words": ["what", "is", "a", "good", "comedy", "that", "was", "made", "in", "the", "1970", "s", "besides", "caddyshack"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O", "O", "O", "O", "B-year", "I-year", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, review, trailer, actor, title, average ratings, song, director, genre, rating, plot, year and O.\nSentence: what is a good comedy that was made in the 1970 s besides caddyshack", "prompt_labels": "what(O) is(O) a(O) good(O) comedy(B-genre) that(O) was(O) made(O) in(O) the(O) 1970(B-year) s(I-year) besides(O) caddyshack(O)"}}
{"id": "861", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "actor", "average ratings", "review", "plot", "trailer", "song", "rating", "title", "character", "year", "director"], "instance": {"id": "861", "words": ["find", "meg", "ryan", "films", "from", "the", "1990s", "with", "angels"], "labels": ["O", "O", "O", "O", "O", "O", "B-year", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, actor, average ratings, review, plot, trailer, song, rating, title, character, year, director and O.\nSentence: find meg ryan films from the 1990s with angels", "prompt_labels": "find(O) meg(O) ryan(O) films(O) from(O) the(O) 1990s(B-year) with(O) angels(O)"}}
{"id": "1249", "dataset": "mit-movie", "split": "test", "label_list": ["year", "character", "plot", "review", "actor", "genre", "song", "trailer", "director", "rating", "average ratings", "title"], "instance": {"id": "1249", "words": ["find", "me", "a", "fantasy", "movie", "made", "withing", "the", "past", "eight", "decades"], "labels": ["O", "O", "O", "B-genre", "O", "O", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, character, plot, review, actor, genre, song, trailer, director, rating, average ratings, title and O.\nSentence: find me a fantasy movie made withing the past eight decades", "prompt_labels": "find(O) me(O) a(O) fantasy(B-genre) movie(O) made(O) withing(O) the(O) past(B-year) eight(I-year) decades(I-year)"}}
{"id": "1306", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "director", "year", "character", "trailer", "plot", "average ratings", "rating", "actor", "review", "title", "song"], "instance": {"id": "1306", "words": ["how", "many", "films", "featured", "stacey", "dash", "in", "1950"], "labels": ["O", "O", "O", "O", "B-actor", "I-actor", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, director, year, character, trailer, plot, average ratings, rating, actor, review, title, song and O.\nSentence: how many films featured stacey dash in 1950", "prompt_labels": "how(O) many(O) films(O) featured(O) stacey(B-actor) dash(I-actor) in(O) 1950(B-year)"}}
{"id": "996", "dataset": "mit-movie", "split": "test", "label_list": ["review", "genre", "director", "character", "plot", "actor", "trailer", "average ratings", "title", "rating", "year", "song"], "instance": {"id": "996", "words": ["find", "a", "movie", "with", "natalie", "wood", "and", "christopher", "walken"], "labels": ["O", "O", "O", "O", "B-actor", "I-actor", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, genre, director, character, plot, actor, trailer, average ratings, title, rating, year, song and O.\nSentence: find a movie with natalie wood and christopher walken", "prompt_labels": "find(O) a(O) movie(O) with(O) natalie(B-actor) wood(I-actor) and(O) christopher(B-actor) walken(I-actor)"}}
{"id": "719", "dataset": "mit-movie", "split": "test", "label_list": ["song", "trailer", "actor", "title", "plot", "average ratings", "director", "character", "rating", "review", "year", "genre"], "instance": {"id": "719", "words": ["what", "year", "was", "the", "last", "james", "bond", "film", "released"], "labels": ["O", "B-year", "O", "O", "O", "B-title", "I-title", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, trailer, actor, title, plot, average ratings, director, character, rating, review, year, genre and O.\nSentence: what year was the last james bond film released", "prompt_labels": "what(O) year(B-year) was(O) the(O) last(O) james(B-title) bond(I-title) film(O) released(O)"}}
{"id": "81", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "actor", "song", "year", "director", "plot", "average ratings", "genre", "review", "trailer", "character", "title"], "instance": {"id": "81", "words": ["i", "would", "like", "a", "list", "of", "movies", "about", "dancing", "from", "the", "past", "10", "years"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-plot", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, actor, song, year, director, plot, average ratings, genre, review, trailer, character, title and O.\nSentence: i would like a list of movies about dancing from the past 10 years", "prompt_labels": "i(O) would(O) like(O) a(O) list(O) of(O) movies(O) about(O) dancing(B-plot) from(O) the(O) past(B-year) 10(I-year) years(I-year)"}}
{"id": "1148", "dataset": "mit-movie", "split": "test", "label_list": ["character", "average ratings", "plot", "director", "rating", "genre", "year", "actor", "song", "trailer", "review", "title"], "instance": {"id": "1148", "words": ["did", "jerry", "obach", "do", "any", "shorts"], "labels": ["O", "B-actor", "I-actor", "O", "O", "B-genre"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, average ratings, plot, director, rating, genre, year, actor, song, trailer, review, title and O.\nSentence: did jerry obach do any shorts", "prompt_labels": "did(O) jerry(B-actor) obach(I-actor) do(O) any(O) shorts(B-genre)"}}
{"id": "2178", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "average ratings", "year", "trailer", "actor", "song", "plot", "director", "review", "title", "character", "genre"], "instance": {"id": "2178", "words": ["what", "recent", "action", "movies", "received", "seven", "stars", "and", "above"], "labels": ["O", "O", "B-genre", "O", "O", "B-average ratings", "I-average ratings", "I-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, average ratings, year, trailer, actor, song, plot, director, review, title, character, genre and O.\nSentence: what recent action movies received seven stars and above", "prompt_labels": "what(O) recent(O) action(B-genre) movies(O) received(O) seven(B-average ratings) stars(I-average ratings) and(I-average ratings) above(I-average ratings)"}}
{"id": "1208", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "character", "plot", "review", "trailer", "average ratings", "actor", "director", "year", "title", "genre", "song"], "instance": {"id": "1208", "words": ["do", "you", "have", "the", "gangster", "film", "from", "the", "past", "four", "years", "directed", "by", "rob", "hardy", "about", "a", "missing", "prisoner"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O", "O", "B-year", "I-year", "I-year", "O", "O", "B-director", "I-director", "O", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, character, plot, review, trailer, average ratings, actor, director, year, title, genre, song and O.\nSentence: do you have the gangster film from the past four years directed by rob hardy about a missing prisoner", "prompt_labels": "do(O) you(O) have(O) the(O) gangster(B-genre) film(O) from(O) the(O) past(B-year) four(I-year) years(I-year) directed(O) by(O) rob(B-director) hardy(I-director) about(O) a(O) missing(B-plot) prisoner(I-plot)"}}
{"id": "1620", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "average ratings", "song", "trailer", "year", "review", "plot", "title", "actor", "genre", "director", "character"], "instance": {"id": "1620", "words": ["list", "a", "highly", "liked", "horror", "film", "that", "is", "rated", "r"], "labels": ["O", "O", "B-average ratings", "I-average ratings", "B-genre", "O", "O", "O", "O", "B-rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, average ratings, song, trailer, year, review, plot, title, actor, genre, director, character and O.\nSentence: list a highly liked horror film that is rated r", "prompt_labels": "list(O) a(O) highly(B-average ratings) liked(I-average ratings) horror(B-genre) film(O) that(O) is(O) rated(O) r(B-rating)"}}
{"id": "1098", "dataset": "mit-movie", "split": "test", "label_list": ["review", "actor", "rating", "year", "trailer", "character", "average ratings", "title", "plot", "song", "director", "genre"], "instance": {"id": "1098", "words": ["are", "there", "any", "scary", "g", "rated", "films", "starring", "kelly", "ripa", "from", "the", "1990", "s"], "labels": ["O", "O", "O", "B-genre", "B-rating", "O", "O", "O", "B-actor", "I-actor", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, actor, rating, year, trailer, character, average ratings, title, plot, song, director, genre and O.\nSentence: are there any scary g rated films starring kelly ripa from the 1990 s", "prompt_labels": "are(O) there(O) any(O) scary(B-genre) g(B-rating) rated(O) films(O) starring(O) kelly(B-actor) ripa(I-actor) from(O) the(O) 1990(B-year) s(I-year)"}}
{"id": "1869", "dataset": "mit-movie", "split": "test", "label_list": ["review", "character", "rating", "song", "actor", "director", "year", "title", "trailer", "genre", "average ratings", "plot"], "instance": {"id": "1869", "words": ["what", "r", "rated", "howard", "stern", "movies", "are", "there", "from", "the", "past", "two", "decades"], "labels": ["O", "B-rating", "O", "B-actor", "I-actor", "O", "O", "O", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, character, rating, song, actor, director, year, title, trailer, genre, average ratings, plot and O.\nSentence: what r rated howard stern movies are there from the past two decades", "prompt_labels": "what(O) r(B-rating) rated(O) howard(B-actor) stern(I-actor) movies(O) are(O) there(O) from(O) the(O) past(B-year) two(I-year) decades(I-year)"}}
{"id": "8", "dataset": "mit-movie", "split": "test", "label_list": ["director", "genre", "trailer", "year", "rating", "plot", "title", "actor", "song", "average ratings", "review", "character"], "instance": {"id": "8", "words": ["find", "me", "science", "fiction", "movies", "since", "2005"], "labels": ["O", "O", "B-genre", "I-genre", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, genre, trailer, year, rating, plot, title, actor, song, average ratings, review, character and O.\nSentence: find me science fiction movies since 2005", "prompt_labels": "find(O) me(O) science(B-genre) fiction(I-genre) movies(O) since(B-year) 2005(I-year)"}}
{"id": "1822", "dataset": "mit-movie", "split": "test", "label_list": ["song", "rating", "genre", "title", "trailer", "plot", "average ratings", "review", "character", "director", "actor", "year"], "instance": {"id": "1822", "words": ["what", "1950", "s", "war", "movies", "are", "about", "snipers"], "labels": ["O", "B-year", "I-year", "B-genre", "O", "O", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, rating, genre, title, trailer, plot, average ratings, review, character, director, actor, year and O.\nSentence: what 1950 s war movies are about snipers", "prompt_labels": "what(O) 1950(B-year) s(I-year) war(B-genre) movies(O) are(O) about(O) snipers(B-plot)"}}
{"id": "1532", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "song", "genre", "title", "director", "rating", "average ratings", "character", "actor", "review", "trailer", "year"], "instance": {"id": "1532", "words": ["is", "there", "a", "really", "good", "love", "movie", "about", "falling", "in", "love", "in", "the", "1940", "era"], "labels": ["O", "O", "O", "B-average ratings", "I-average ratings", "B-genre", "O", "O", "B-plot", "I-plot", "I-plot", "O", "O", "B-year", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, song, genre, title, director, rating, average ratings, character, actor, review, trailer, year and O.\nSentence: is there a really good love movie about falling in love in the 1940 era", "prompt_labels": "is(O) there(O) a(O) really(B-average ratings) good(I-average ratings) love(B-genre) movie(O) about(O) falling(B-plot) in(I-plot) love(I-plot) in(O) the(O) 1940(B-year) era(O)"}}
{"id": "901", "dataset": "mit-movie", "split": "test", "label_list": ["title", "director", "rating", "year", "average ratings", "review", "actor", "trailer", "song", "plot", "character", "genre"], "instance": {"id": "901", "words": ["when", "is", "the", "last", "season", "of", "desperate", "housewives", "going", "to", "air", "in", "europe"], "labels": ["O", "O", "O", "O", "O", "O", "B-title", "I-title", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, director, rating, year, average ratings, review, actor, trailer, song, plot, character, genre and O.\nSentence: when is the last season of desperate housewives going to air in europe", "prompt_labels": "when(O) is(O) the(O) last(O) season(O) of(O) desperate(B-title) housewives(I-title) going(O) to(O) air(O) in(O) europe(O)"}}
{"id": "166", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "character", "plot", "review", "trailer", "genre", "director", "title", "rating", "actor", "year", "song"], "instance": {"id": "166", "words": ["what", "was", "johnny", "depps", "first", "movie"], "labels": ["O", "O", "B-actor", "I-actor", "B-year", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, character, plot, review, trailer, genre, director, title, rating, actor, year, song and O.\nSentence: what was johnny depps first movie", "prompt_labels": "what(O) was(O) johnny(B-actor) depps(I-actor) first(B-year) movie(O)"}}
{"id": "224", "dataset": "mit-movie", "split": "test", "label_list": ["title", "trailer", "plot", "director", "song", "character", "genre", "review", "actor", "rating", "average ratings", "year"], "instance": {"id": "224", "words": ["who", "was", "the", "actor", "in", "the", "movie", "teen", "wolf"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, trailer, plot, director, song, character, genre, review, actor, rating, average ratings, year and O.\nSentence: who was the actor in the movie teen wolf", "prompt_labels": "who(O) was(O) the(O) actor(O) in(O) the(O) movie(O) teen(B-title) wolf(I-title)"}}
{"id": "1405", "dataset": "mit-movie", "split": "test", "label_list": ["song", "review", "rating", "director", "character", "plot", "genre", "actor", "average ratings", "title", "trailer", "year"], "instance": {"id": "1405", "words": ["in", "the", "1980", "s", "what", "military", "soldier", "movies", "came", "out"], "labels": ["O", "O", "B-year", "I-year", "O", "B-genre", "B-plot", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, review, rating, director, character, plot, genre, actor, average ratings, title, trailer, year and O.\nSentence: in the 1980 s what military soldier movies came out", "prompt_labels": "in(O) the(O) 1980(B-year) s(I-year) what(O) military(B-genre) soldier(B-plot) movies(O) came(O) out(O)"}}
{"id": "1053", "dataset": "mit-movie", "split": "test", "label_list": ["director", "trailer", "genre", "year", "plot", "actor", "rating", "song", "review", "title", "average ratings", "character"], "instance": {"id": "1053", "words": ["what", "was", "the", "name", "of", "the", "stepmother", "played", "by", "allison", "janney", "in", "juno"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-actor", "I-actor", "O", "B-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, trailer, genre, year, plot, actor, rating, song, review, title, average ratings, character and O.\nSentence: what was the name of the stepmother played by allison janney in juno", "prompt_labels": "what(O) was(O) the(O) name(O) of(O) the(O) stepmother(O) played(O) by(O) allison(B-actor) janney(I-actor) in(O) juno(B-title)"}}
{"id": "274", "dataset": "mit-movie", "split": "test", "label_list": ["title", "genre", "plot", "rating", "actor", "trailer", "character", "year", "song", "average ratings", "review", "director"], "instance": {"id": "274", "words": ["show", "me", "the", "half", "baked", "cover"], "labels": ["O", "O", "O", "B-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, genre, plot, rating, actor, trailer, character, year, song, average ratings, review, director and O.\nSentence: show me the half baked cover", "prompt_labels": "show(O) me(O) the(O) half(B-title) baked(I-title) cover(O)"}}
{"id": "2301", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "year", "average ratings", "review", "title", "trailer", "genre", "character", "plot", "actor", "song", "director"], "instance": {"id": "2301", "words": ["who", "directed", "but", "im", "a", "cheerleader"], "labels": ["O", "O", "B-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, year, average ratings, review, title, trailer, genre, character, plot, actor, song, director and O.\nSentence: who directed but im a cheerleader", "prompt_labels": "who(O) directed(O) but(B-title) im(I-title) a(I-title) cheerleader(I-title)"}}
{"id": "1631", "dataset": "mit-movie", "split": "test", "label_list": ["review", "trailer", "rating", "year", "average ratings", "plot", "title", "actor", "song", "genre", "character", "director"], "instance": {"id": "1631", "words": ["list", "a", "must", "see", "thriller", "made", "last", "year", "but", "is", "pg", "13"], "labels": ["O", "O", "B-average ratings", "I-average ratings", "B-genre", "O", "B-year", "I-year", "O", "O", "B-rating", "I-rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, trailer, rating, year, average ratings, plot, title, actor, song, genre, character, director and O.\nSentence: list a must see thriller made last year but is pg 13", "prompt_labels": "list(O) a(O) must(B-average ratings) see(I-average ratings) thriller(B-genre) made(O) last(B-year) year(I-year) but(O) is(O) pg(B-rating) 13(I-rating)"}}
{"id": "398", "dataset": "mit-movie", "split": "test", "label_list": ["review", "director", "song", "title", "rating", "trailer", "character", "actor", "average ratings", "genre", "plot", "year"], "instance": {"id": "398", "words": ["list", "all", "versions", "of", "south", "pacific"], "labels": ["O", "O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, director, song, title, rating, trailer, character, actor, average ratings, genre, plot, year and O.\nSentence: list all versions of south pacific", "prompt_labels": "list(O) all(O) versions(O) of(O) south(B-title) pacific(I-title)"}}
{"id": "1717", "dataset": "mit-movie", "split": "test", "label_list": ["character", "title", "average ratings", "song", "rating", "year", "trailer", "review", "actor", "director", "plot", "genre"], "instance": {"id": "1717", "words": ["name", "a", "2010", "s", "drama"], "labels": ["O", "O", "B-year", "I-year", "B-genre"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, title, average ratings, song, rating, year, trailer, review, actor, director, plot, genre and O.\nSentence: name a 2010 s drama", "prompt_labels": "name(O) a(O) 2010(B-year) s(I-year) drama(B-genre)"}}
{"id": "1270", "dataset": "mit-movie", "split": "test", "label_list": ["title", "trailer", "song", "genre", "year", "review", "director", "rating", "character", "plot", "actor", "average ratings"], "instance": {"id": "1270", "words": ["has", "james", "dean", "acted", "in", "any", "drama", "films"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, trailer, song, genre, year, review, director, rating, character, plot, actor, average ratings and O.\nSentence: has james dean acted in any drama films", "prompt_labels": "has(O) james(B-actor) dean(I-actor) acted(O) in(O) any(O) drama(B-genre) films(O)"}}
{"id": "1250", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "plot", "review", "trailer", "actor", "rating", "director", "character", "title", "average ratings", "year", "song"], "instance": {"id": "1250", "words": ["frank", "darabont", "made", "good", "movies", "around", "1950", "s", "any", "good", "war", "films", "by", "him"], "labels": ["B-director", "I-director", "O", "O", "O", "O", "B-year", "I-year", "O", "O", "B-genre", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, plot, review, trailer, actor, rating, director, character, title, average ratings, year, song and O.\nSentence: frank darabont made good movies around 1950 s any good war films by him", "prompt_labels": "frank(B-director) darabont(I-director) made(O) good(O) movies(O) around(O) 1950(B-year) s(I-year) any(O) good(O) war(B-genre) films(O) by(O) him(O)"}}
{"id": "1617", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "actor", "character", "title", "average ratings", "plot", "director", "review", "genre", "year", "song", "trailer"], "instance": {"id": "1617", "words": ["list", "a", "gangster", "film", "with", "a", "nc", "17", "rating", "directed", "by", "sean", "ellis", "in", "the", "year", "2010"], "labels": ["O", "O", "B-genre", "O", "O", "O", "B-rating", "I-rating", "O", "O", "O", "B-director", "I-director", "O", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, actor, character, title, average ratings, plot, director, review, genre, year, song, trailer and O.\nSentence: list a gangster film with a nc 17 rating directed by sean ellis in the year 2010", "prompt_labels": "list(O) a(O) gangster(B-genre) film(O) with(O) a(O) nc(B-rating) 17(I-rating) rating(O) directed(O) by(O) sean(B-director) ellis(I-director) in(O) the(O) year(O) 2010(B-year)"}}
{"id": "601", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "actor", "song", "title", "review", "character", "genre", "average ratings", "year", "trailer", "rating", "director"], "instance": {"id": "601", "words": ["name", "the", "movie", "in", "which", "clint", "eastwood", "sings"], "labels": ["O", "O", "O", "O", "O", "B-actor", "I-actor", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, actor, song, title, review, character, genre, average ratings, year, trailer, rating, director and O.\nSentence: name the movie in which clint eastwood sings", "prompt_labels": "name(O) the(O) movie(O) in(O) which(O) clint(B-actor) eastwood(I-actor) sings(B-plot)"}}
{"id": "606", "dataset": "mit-movie", "split": "test", "label_list": ["character", "director", "average ratings", "rating", "plot", "year", "title", "song", "review", "trailer", "genre", "actor"], "instance": {"id": "606", "words": ["are", "you", "able", "to", "name", "an", "anime", "film", "that", "is", "in", "black", "and", "white"], "labels": ["O", "O", "O", "O", "O", "O", "B-genre", "I-genre", "O", "O", "O", "B-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, director, average ratings, rating, plot, year, title, song, review, trailer, genre, actor and O.\nSentence: are you able to name an anime film that is in black and white", "prompt_labels": "are(O) you(O) able(O) to(O) name(O) an(O) anime(B-genre) film(I-genre) that(O) is(O) in(O) black(B-plot) and(I-plot) white(I-plot)"}}
{"id": "1353", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "character", "director", "year", "song", "review", "average ratings", "plot", "title", "genre", "rating", "actor"], "instance": {"id": "1353", "words": ["im", "looking", "for", "a", "kate", "jackson", "sci", "fi", "film", "from", "2000", "that", "got", "excellent", "ratings"], "labels": ["O", "O", "O", "O", "B-actor", "I-actor", "B-genre", "I-genre", "O", "O", "B-year", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, character, director, year, song, review, average ratings, plot, title, genre, rating, actor and O.\nSentence: im looking for a kate jackson sci fi film from 2000 that got excellent ratings", "prompt_labels": "im(O) looking(O) for(O) a(O) kate(B-actor) jackson(I-actor) sci(B-genre) fi(I-genre) film(O) from(O) 2000(B-year) that(O) got(O) excellent(B-average ratings) ratings(I-average ratings)"}}
{"id": "2211", "dataset": "mit-movie", "split": "test", "label_list": ["song", "average ratings", "plot", "trailer", "actor", "year", "director", "rating", "title", "character", "genre", "review"], "instance": {"id": "2211", "words": ["what", "was", "that", "unrated", "spaghetti", "western", "with", "reese", "witherspoon", "that", "had", "a", "five", "star", "rating", "average", "and", "came", "out", "in", "the", "2000", "s"], "labels": ["O", "O", "O", "B-rating", "B-genre", "I-genre", "O", "B-actor", "I-actor", "O", "O", "O", "B-average ratings", "I-average ratings", "O", "O", "O", "O", "O", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, average ratings, plot, trailer, actor, year, director, rating, title, character, genre, review and O.\nSentence: what was that unrated spaghetti western with reese witherspoon that had a five star rating average and came out in the 2000 s", "prompt_labels": "what(O) was(O) that(O) unrated(B-rating) spaghetti(B-genre) western(I-genre) with(O) reese(B-actor) witherspoon(I-actor) that(O) had(O) a(O) five(B-average ratings) star(I-average ratings) rating(O) average(O) and(O) came(O) out(O) in(O) the(O) 2000(B-year) s(I-year)"}}
{"id": "625", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "character", "rating", "plot", "review", "genre", "director", "year", "song", "average ratings", "actor", "title"], "instance": {"id": "625", "words": ["how", "many", "movies", "were", "released", "in", "the", "year", "2009"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, character, rating, plot, review, genre, director, year, song, average ratings, actor, title and O.\nSentence: how many movies were released in the year 2009", "prompt_labels": "how(O) many(O) movies(O) were(O) released(O) in(O) the(O) year(O) 2009(B-year)"}}
{"id": "1338", "dataset": "mit-movie", "split": "test", "label_list": ["title", "rating", "genre", "song", "trailer", "director", "average ratings", "character", "plot", "actor", "year", "review"], "instance": {"id": "1338", "words": ["i", "would", "like", "the", "title", "of", "the", "pg", "13", "rated", "movie", "with", "dan", "haggerty", "that", "was", "rated", "well"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-rating", "I-rating", "O", "O", "O", "B-actor", "I-actor", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, rating, genre, song, trailer, director, average ratings, character, plot, actor, year, review and O.\nSentence: i would like the title of the pg 13 rated movie with dan haggerty that was rated well", "prompt_labels": "i(O) would(O) like(O) the(O) title(O) of(O) the(O) pg(B-rating) 13(I-rating) rated(O) movie(O) with(O) dan(B-actor) haggerty(I-actor) that(O) was(O) rated(B-average ratings) well(I-average ratings)"}}
{"id": "1058", "dataset": "mit-movie", "split": "test", "label_list": ["character", "review", "title", "song", "average ratings", "rating", "trailer", "plot", "year", "actor", "director", "genre"], "instance": {"id": "1058", "words": ["a", "highly", "liked", "teen", "movie", "with", "jim", "carrey", "please"], "labels": ["O", "B-average ratings", "I-average ratings", "B-genre", "O", "O", "B-actor", "I-actor", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, review, title, song, average ratings, rating, trailer, plot, year, actor, director, genre and O.\nSentence: a highly liked teen movie with jim carrey please", "prompt_labels": "a(O) highly(B-average ratings) liked(I-average ratings) teen(B-genre) movie(O) with(O) jim(B-actor) carrey(I-actor) please(O)"}}
{"id": "773", "dataset": "mit-movie", "split": "test", "label_list": ["song", "title", "trailer", "actor", "genre", "director", "plot", "year", "rating", "review", "average ratings", "character"], "instance": {"id": "773", "words": ["what", "film", "directed", "by", "zack", "snyder", "shows", "the", "forces", "of", "king", "leonidas"], "labels": ["O", "O", "O", "O", "B-actor", "I-actor", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, title, trailer, actor, genre, director, plot, year, rating, review, average ratings, character and O.\nSentence: what film directed by zack snyder shows the forces of king leonidas", "prompt_labels": "what(O) film(O) directed(O) by(O) zack(B-actor) snyder(I-actor) shows(O) the(O) forces(O) of(O) king(O) leonidas(O)"}}
{"id": "2060", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "song", "title", "rating", "trailer", "plot", "actor", "year", "review", "genre", "character", "director"], "instance": {"id": "2060", "words": ["what", "is", "the", "fists", "of", "fury"], "labels": ["O", "O", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, song, title, rating, trailer, plot, actor, year, review, genre, character, director and O.\nSentence: what is the fists of fury", "prompt_labels": "what(O) is(O) the(O) fists(B-title) of(I-title) fury(I-title)"}}
{"id": "1699", "dataset": "mit-movie", "split": "test", "label_list": ["character", "genre", "title", "rating", "plot", "song", "review", "average ratings", "actor", "director", "year", "trailer"], "instance": {"id": "1699", "words": ["list", "some", "movies", "featuring", "graphic", "sexual", "horror"], "labels": ["O", "O", "O", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, genre, title, rating, plot, song, review, average ratings, actor, director, year, trailer and O.\nSentence: list some movies featuring graphic sexual horror", "prompt_labels": "list(O) some(O) movies(O) featuring(O) graphic(B-title) sexual(I-title) horror(I-title)"}}
{"id": "1040", "dataset": "mit-movie", "split": "test", "label_list": ["title", "average ratings", "rating", "director", "plot", "genre", "review", "year", "character", "song", "trailer", "actor"], "instance": {"id": "1040", "words": ["show", "me", "an", "lee", "meriweather", "film", "about", "a", "torch", "singer"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O", "O", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, average ratings, rating, director, plot, genre, review, year, character, song, trailer, actor and O.\nSentence: show me an lee meriweather film about a torch singer", "prompt_labels": "show(O) me(O) an(O) lee(B-actor) meriweather(I-actor) film(O) about(O) a(O) torch(B-plot) singer(I-plot)"}}
{"id": "200", "dataset": "mit-movie", "split": "test", "label_list": ["director", "title", "actor", "review", "genre", "plot", "rating", "year", "trailer", "song", "character", "average ratings"], "instance": {"id": "200", "words": ["who", "was", "the", "male", "lead", "in", "gone", "with", "the", "wind"], "labels": ["O", "O", "O", "O", "O", "O", "B-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, title, actor, review, genre, plot, rating, year, trailer, song, character, average ratings and O.\nSentence: who was the male lead in gone with the wind", "prompt_labels": "who(O) was(O) the(O) male(O) lead(O) in(O) gone(B-title) with(I-title) the(I-title) wind(I-title)"}}
{"id": "651", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "average ratings", "song", "director", "plot", "character", "year", "title", "review", "actor", "rating", "trailer"], "instance": {"id": "651", "words": ["find", "a", "comedy", "starring", "john", "ritter"], "labels": ["O", "O", "B-genre", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, average ratings, song, director, plot, character, year, title, review, actor, rating, trailer and O.\nSentence: find a comedy starring john ritter", "prompt_labels": "find(O) a(O) comedy(B-genre) starring(O) john(B-actor) ritter(I-actor)"}}
{"id": "1207", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "genre", "actor", "review", "song", "title", "year", "plot", "average ratings", "rating", "character", "director"], "instance": {"id": "1207", "words": ["do", "you", "have", "the", "film", "the", "loss", "of", "sexual", "innocence"], "labels": ["O", "O", "O", "O", "O", "B-title", "I-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, genre, actor, review, song, title, year, plot, average ratings, rating, character, director and O.\nSentence: do you have the film the loss of sexual innocence", "prompt_labels": "do(O) you(O) have(O) the(O) film(O) the(B-title) loss(I-title) of(I-title) sexual(I-title) innocence(I-title)"}}
{"id": "1391", "dataset": "mit-movie", "split": "test", "label_list": ["character", "title", "song", "trailer", "genre", "rating", "review", "actor", "director", "plot", "year", "average ratings"], "instance": {"id": "1391", "words": ["im", "want", "to", "watch", "an", "all", "right", "western", "from", "1980", "where", "they", "search", "for", "gold"], "labels": ["O", "O", "O", "O", "O", "B-average ratings", "I-average ratings", "B-genre", "O", "B-year", "O", "O", "B-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, title, song, trailer, genre, rating, review, actor, director, plot, year, average ratings and O.\nSentence: im want to watch an all right western from 1980 where they search for gold", "prompt_labels": "im(O) want(O) to(O) watch(O) an(O) all(B-average ratings) right(I-average ratings) western(B-genre) from(O) 1980(B-year) where(O) they(O) search(B-plot) for(I-plot) gold(I-plot)"}}
{"id": "1511", "dataset": "mit-movie", "split": "test", "label_list": ["year", "song", "plot", "actor", "title", "genre", "average ratings", "rating", "character", "director", "review", "trailer"], "instance": {"id": "1511", "words": ["is", "there", "a", "movie", "called", "manfast"], "labels": ["O", "O", "O", "O", "O", "B-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, song, plot, actor, title, genre, average ratings, rating, character, director, review, trailer and O.\nSentence: is there a movie called manfast", "prompt_labels": "is(O) there(O) a(O) movie(O) called(O) manfast(B-title)"}}
{"id": "1890", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "director", "review", "year", "title", "average ratings", "trailer", "character", "rating", "song", "actor", "plot"], "instance": {"id": "1890", "words": ["what", "are", "some", "pg", "13", "thrillers"], "labels": ["O", "O", "O", "B-rating", "I-rating", "B-genre"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, director, review, year, title, average ratings, trailer, character, rating, song, actor, plot and O.\nSentence: what are some pg 13 thrillers", "prompt_labels": "what(O) are(O) some(O) pg(B-rating) 13(I-rating) thrillers(B-genre)"}}
{"id": "2344", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "review", "song", "trailer", "plot", "average ratings", "year", "actor", "director", "rating", "title", "character"], "instance": {"id": "2344", "words": ["did", "charles", "haid", "have", "a", "war", "movie", "in", "the", "last", "three", "years"], "labels": ["O", "B-director", "I-director", "O", "O", "B-genre", "O", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, review, song, trailer, plot, average ratings, year, actor, director, rating, title, character and O.\nSentence: did charles haid have a war movie in the last three years", "prompt_labels": "did(O) charles(B-director) haid(I-director) have(O) a(O) war(B-genre) movie(O) in(O) the(O) last(B-year) three(I-year) years(I-year)"}}
{"id": "517", "dataset": "mit-movie", "split": "test", "label_list": ["song", "trailer", "director", "character", "average ratings", "rating", "review", "plot", "title", "year", "genre", "actor"], "instance": {"id": "517", "words": ["what", "character", "did", "morgan", "freeman", "play", "in", "shawshank", "redemption"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, trailer, director, character, average ratings, rating, review, plot, title, year, genre, actor and O.\nSentence: what character did morgan freeman play in shawshank redemption", "prompt_labels": "what(O) character(O) did(O) morgan(B-actor) freeman(I-actor) play(O) in(O) shawshank(B-title) redemption(I-title)"}}
{"id": "1132", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "genre", "director", "year", "trailer", "average ratings", "plot", "actor", "character", "song", "review", "title"], "instance": {"id": "1132", "words": ["did", "damon", "wayons", "star", "in", "an", "unrated", "nine", "stars", "movie", "in", "1940"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "B-rating", "B-average ratings", "I-average ratings", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, genre, director, year, trailer, average ratings, plot, actor, character, song, review, title and O.\nSentence: did damon wayons star in an unrated nine stars movie in 1940", "prompt_labels": "did(O) damon(B-actor) wayons(I-actor) star(O) in(O) an(O) unrated(B-rating) nine(B-average ratings) stars(I-average ratings) movie(O) in(O) 1940(B-year)"}}
{"id": "404", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "title", "year", "rating", "review", "song", "trailer", "genre", "actor", "character", "director", "plot"], "instance": {"id": "404", "words": ["did", "jessica", "alba", "make", "any", "new", "movies"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, title, year, rating, review, song, trailer, genre, actor, character, director, plot and O.\nSentence: did jessica alba make any new movies", "prompt_labels": "did(O) jessica(B-actor) alba(I-actor) make(O) any(O) new(O) movies(O)"}}
{"id": "2219", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "trailer", "plot", "character", "review", "song", "director", "title", "actor", "average ratings", "rating", "year"], "instance": {"id": "2219", "words": ["what", "was", "the", "first", "crime", "movie", "that", "marlon", "brando", "was", "in", "was", "it", "the", "godfather"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O", "B-actor", "I-actor", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, trailer, plot, character, review, song, director, title, actor, average ratings, rating, year and O.\nSentence: what was the first crime movie that marlon brando was in was it the godfather", "prompt_labels": "what(O) was(O) the(O) first(O) crime(B-genre) movie(O) that(O) marlon(B-actor) brando(I-actor) was(O) in(O) was(O) it(O) the(O) godfather(O)"}}
{"id": "68", "dataset": "mit-movie", "split": "test", "label_list": ["year", "plot", "average ratings", "trailer", "actor", "song", "rating", "director", "title", "review", "character", "genre"], "instance": {"id": "68", "words": ["the", "new", "batman", "movie", "looks", "epic"], "labels": ["O", "O", "B-title", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, plot, average ratings, trailer, actor, song, rating, director, title, review, character, genre and O.\nSentence: the new batman movie looks epic", "prompt_labels": "the(O) new(O) batman(B-title) movie(O) looks(O) epic(O)"}}
{"id": "2339", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "song", "review", "director", "trailer", "character", "actor", "rating", "year", "genre", "plot", "title"], "instance": {"id": "2339", "words": ["who", "was", "the", "main", "character", "in", "the", "movie", "the", "hills", "have", "eyes"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, song, review, director, trailer, character, actor, rating, year, genre, plot, title and O.\nSentence: who was the main character in the movie the hills have eyes", "prompt_labels": "who(O) was(O) the(O) main(O) character(O) in(O) the(O) movie(O) the(B-title) hills(I-title) have(I-title) eyes(I-title)"}}
{"id": "1676", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "genre", "rating", "plot", "director", "title", "trailer", "review", "year", "character", "song", "actor"], "instance": {"id": "1676", "words": ["list", "an", "independent", "movie"], "labels": ["O", "O", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, genre, rating, plot, director, title, trailer, review, year, character, song, actor and O.\nSentence: list an independent movie", "prompt_labels": "list(O) an(O) independent(B-genre) movie(O)"}}
{"id": "507", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "average ratings", "rating", "song", "title", "year", "trailer", "character", "director", "plot", "review", "genre"], "instance": {"id": "507", "words": ["in", "what", "year", "did", "the", "film", "vegas", "vacation", "come", "out"], "labels": ["O", "O", "O", "O", "O", "O", "B-title", "I-title", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, average ratings, rating, song, title, year, trailer, character, director, plot, review, genre and O.\nSentence: in what year did the film vegas vacation come out", "prompt_labels": "in(O) what(O) year(O) did(O) the(O) film(O) vegas(B-title) vacation(I-title) come(O) out(O)"}}
{"id": "1186", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "year", "character", "rating", "song", "title", "average ratings", "director", "genre", "review", "plot", "actor"], "instance": {"id": "1186", "words": ["do", "you", "carry", "the", "movie", "the", "a", "team"], "labels": ["O", "O", "O", "O", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, year, character, rating, song, title, average ratings, director, genre, review, plot, actor and O.\nSentence: do you carry the movie the a team", "prompt_labels": "do(O) you(O) carry(O) the(O) movie(O) the(B-title) a(I-title) team(I-title)"}}
{"id": "971", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "trailer", "song", "director", "character", "year", "average ratings", "rating", "review", "title", "actor", "plot"], "instance": {"id": "971", "words": ["what", "is", "the", "scariest", "horror", "movie", "from", "the", "90s"], "labels": ["O", "O", "O", "B-genre", "I-genre", "O", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, trailer, song, director, character, year, average ratings, rating, review, title, actor, plot and O.\nSentence: what is the scariest horror movie from the 90s", "prompt_labels": "what(O) is(O) the(O) scariest(B-genre) horror(I-genre) movie(O) from(O) the(O) 90s(B-year)"}}
{"id": "2342", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "rating", "review", "plot", "year", "actor", "genre", "song", "average ratings", "director", "character", "title"], "instance": {"id": "2342", "words": ["would", "you", "be", "able", "to", "help", "me", "find", "the", "cheetah", "girls", "2", "movie"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-title", "I-title", "I-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, rating, review, plot, year, actor, genre, song, average ratings, director, character, title and O.\nSentence: would you be able to help me find the cheetah girls 2 movie", "prompt_labels": "would(O) you(O) be(O) able(O) to(O) help(O) me(O) find(O) the(B-title) cheetah(I-title) girls(I-title) 2(I-title) movie(O)"}}
{"id": "758", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "genre", "song", "average ratings", "trailer", "year", "actor", "title", "director", "review", "character", "plot"], "instance": {"id": "758", "words": ["what", "was", "the", "best", "football", "movie", "ever", "made"], "labels": ["O", "O", "O", "O", "B-genre", "I-genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, genre, song, average ratings, trailer, year, actor, title, director, review, character, plot and O.\nSentence: what was the best football movie ever made", "prompt_labels": "what(O) was(O) the(O) best(O) football(B-genre) movie(I-genre) ever(O) made(O)"}}
{"id": "2086", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "average ratings", "character", "plot", "trailer", "director", "title", "song", "year", "rating", "genre", "review"], "instance": {"id": "2086", "words": ["what", "is", "the", "movie", "the", "shunning", "about"], "labels": ["O", "O", "O", "O", "B-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, average ratings, character, plot, trailer, director, title, song, year, rating, genre, review and O.\nSentence: what is the movie the shunning about", "prompt_labels": "what(O) is(O) the(O) movie(O) the(B-title) shunning(I-title) about(O)"}}
{"id": "315", "dataset": "mit-movie", "split": "test", "label_list": ["song", "review", "title", "trailer", "director", "plot", "rating", "average ratings", "year", "character", "genre", "actor"], "instance": {"id": "315", "words": ["name", "the", "theme", "song", "for", "2001", "a", "space", "odyssey"], "labels": ["O", "O", "B-song", "I-song", "O", "B-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, review, title, trailer, director, plot, rating, average ratings, year, character, genre, actor and O.\nSentence: name the theme song for 2001 a space odyssey", "prompt_labels": "name(O) the(O) theme(B-song) song(I-song) for(O) 2001(B-title) a(I-title) space(I-title) odyssey(I-title)"}}
{"id": "774", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "director", "genre", "character", "plot", "year", "actor", "rating", "average ratings", "review", "song", "title"], "instance": {"id": "774", "words": ["what", "is", "the", "best", "movie", "trailer"], "labels": ["O", "O", "O", "B-trailer", "I-trailer", "I-trailer"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, director, genre, character, plot, year, actor, rating, average ratings, review, song, title and O.\nSentence: what is the best movie trailer", "prompt_labels": "what(O) is(O) the(O) best(B-trailer) movie(I-trailer) trailer(I-trailer)"}}
{"id": "2438", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "year", "character", "song", "actor", "review", "plot", "genre", "title", "director", "rating", "average ratings"], "instance": {"id": "2438", "words": ["play", "the", "trailer", "dark", "of", "the", "moon"], "labels": ["O", "O", "B-trailer", "B-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, year, character, song, actor, review, plot, genre, title, director, rating, average ratings and O.\nSentence: play the trailer dark of the moon", "prompt_labels": "play(O) the(O) trailer(B-trailer) dark(B-title) of(I-title) the(I-title) moon(I-title)"}}
{"id": "1557", "dataset": "mit-movie", "split": "test", "label_list": ["song", "rating", "character", "plot", "director", "actor", "year", "title", "review", "trailer", "average ratings", "genre"], "instance": {"id": "1557", "words": ["is", "there", "any", "r", "rated", "history", "movie", "filmed", "in", "the", "year", "1950", "which", "received", "a", "rating", "average", "of", "seven"], "labels": ["O", "O", "O", "B-rating", "O", "B-genre", "O", "O", "O", "O", "O", "B-year", "O", "O", "O", "O", "O", "O", "B-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, rating, character, plot, director, actor, year, title, review, trailer, average ratings, genre and O.\nSentence: is there any r rated history movie filmed in the year 1950 which received a rating average of seven", "prompt_labels": "is(O) there(O) any(O) r(B-rating) rated(O) history(B-genre) movie(O) filmed(O) in(O) the(O) year(O) 1950(B-year) which(O) received(O) a(O) rating(O) average(O) of(O) seven(B-average ratings)"}}
{"id": "156", "dataset": "mit-movie", "split": "test", "label_list": ["year", "trailer", "average ratings", "plot", "director", "character", "rating", "review", "genre", "song", "title", "actor"], "instance": {"id": "156", "words": ["who", "directed", "310", "to", "yuma"], "labels": ["O", "B-director", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, trailer, average ratings, plot, director, character, rating, review, genre, song, title, actor and O.\nSentence: who directed 310 to yuma", "prompt_labels": "who(O) directed(B-director) 310(B-title) to(I-title) yuma(I-title)"}}
{"id": "1174", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "actor", "character", "trailer", "rating", "plot", "director", "review", "year", "genre", "song", "title"], "instance": {"id": "1174", "words": ["did", "steven", "spielberg", "direct", "wall", "e"], "labels": ["O", "B-director", "I-director", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, actor, character, trailer, rating, plot, director, review, year, genre, song, title and O.\nSentence: did steven spielberg direct wall e", "prompt_labels": "did(O) steven(B-director) spielberg(I-director) direct(O) wall(B-title) e(I-title)"}}
{"id": "226", "dataset": "mit-movie", "split": "test", "label_list": ["character", "average ratings", "trailer", "rating", "plot", "title", "song", "review", "genre", "year", "director", "actor"], "instance": {"id": "226", "words": ["when", "did", "underworld", "come", "out"], "labels": ["O", "O", "B-title", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, average ratings, trailer, rating, plot, title, song, review, genre, year, director, actor and O.\nSentence: when did underworld come out", "prompt_labels": "when(O) did(O) underworld(B-title) come(O) out(O)"}}
{"id": "1829", "dataset": "mit-movie", "split": "test", "label_list": ["title", "rating", "year", "character", "review", "trailer", "song", "average ratings", "plot", "actor", "genre", "director"], "instance": {"id": "1829", "words": ["what", "1990", "s", "animation", "film", "was", "rated", "g"], "labels": ["O", "B-year", "I-year", "B-genre", "O", "O", "O", "B-rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, rating, year, character, review, trailer, song, average ratings, plot, actor, genre, director and O.\nSentence: what 1990 s animation film was rated g", "prompt_labels": "what(O) 1990(B-year) s(I-year) animation(B-genre) film(O) was(O) rated(O) g(B-rating)"}}
{"id": "1948", "dataset": "mit-movie", "split": "test", "label_list": ["review", "year", "character", "trailer", "plot", "genre", "song", "director", "title", "average ratings", "rating", "actor"], "instance": {"id": "1948", "words": ["what", "decent", "movie", "released", "last", "year", "was", "a", "pg", "13", "fantasy", "that", "involved", "elves"], "labels": ["O", "B-average ratings", "O", "O", "B-year", "I-year", "O", "O", "B-rating", "I-rating", "B-genre", "O", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, year, character, trailer, plot, genre, song, director, title, average ratings, rating, actor and O.\nSentence: what decent movie released last year was a pg 13 fantasy that involved elves", "prompt_labels": "what(O) decent(B-average ratings) movie(O) released(O) last(B-year) year(I-year) was(O) a(O) pg(B-rating) 13(I-rating) fantasy(B-genre) that(O) involved(O) elves(B-plot)"}}
{"id": "854", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "review", "title", "genre", "actor", "average ratings", "year", "plot", "song", "character", "director", "rating"], "instance": {"id": "854", "words": ["when", "is", "american", "reunion", "being", "released"], "labels": ["O", "O", "B-title", "I-title", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, review, title, genre, actor, average ratings, year, plot, song, character, director, rating and O.\nSentence: when is american reunion being released", "prompt_labels": "when(O) is(O) american(B-title) reunion(I-title) being(O) released(O)"}}
{"id": "697", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "review", "genre", "rating", "average ratings", "character", "year", "plot", "actor", "title", "director", "song"], "instance": {"id": "697", "words": ["show", "me", "films", "with", "wyatt", "earp", "from", "the", "1980s"], "labels": ["O", "O", "O", "O", "B-character", "I-character", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, review, genre, rating, average ratings, character, year, plot, actor, title, director, song and O.\nSentence: show me films with wyatt earp from the 1980s", "prompt_labels": "show(O) me(O) films(O) with(O) wyatt(B-character) earp(I-character) from(O) the(O) 1980s(B-year)"}}
{"id": "1406", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "character", "average ratings", "review", "song", "title", "trailer", "genre", "director", "rating", "year", "plot"], "instance": {"id": "1406", "words": ["in", "the", "1990", "s", "was", "there", "an", "r", "rated", "adventure", "movie", "that", "had", "an", "average", "rating", "of", "eight"], "labels": ["O", "O", "B-year", "I-year", "O", "O", "O", "B-rating", "O", "B-genre", "O", "O", "O", "O", "O", "O", "O", "B-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, character, average ratings, review, song, title, trailer, genre, director, rating, year, plot and O.\nSentence: in the 1990 s was there an r rated adventure movie that had an average rating of eight", "prompt_labels": "in(O) the(O) 1990(B-year) s(I-year) was(O) there(O) an(O) r(B-rating) rated(O) adventure(B-genre) movie(O) that(O) had(O) an(O) average(O) rating(O) of(O) eight(B-average ratings)"}}
{"id": "174", "dataset": "mit-movie", "split": "test", "label_list": ["director", "rating", "title", "average ratings", "trailer", "plot", "review", "genre", "actor", "year", "character", "song"], "instance": {"id": "174", "words": ["is", "there", "a", "comedy", "crime", "drama"], "labels": ["O", "O", "O", "B-genre", "I-genre", "I-genre"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, rating, title, average ratings, trailer, plot, review, genre, actor, year, character, song and O.\nSentence: is there a comedy crime drama", "prompt_labels": "is(O) there(O) a(O) comedy(B-genre) crime(I-genre) drama(I-genre)"}}
{"id": "1633", "dataset": "mit-movie", "split": "test", "label_list": ["character", "average ratings", "review", "director", "song", "year", "actor", "genre", "rating", "trailer", "plot", "title"], "instance": {"id": "1633", "words": ["list", "a", "psychological", "drama", "movie", "that", "centers", "on", "lost", "experiences"], "labels": ["O", "O", "B-genre", "I-genre", "O", "O", "O", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, average ratings, review, director, song, year, actor, genre, rating, trailer, plot, title and O.\nSentence: list a psychological drama movie that centers on lost experiences", "prompt_labels": "list(O) a(O) psychological(B-genre) drama(I-genre) movie(O) that(O) centers(O) on(O) lost(B-plot) experiences(I-plot)"}}
{"id": "235", "dataset": "mit-movie", "split": "test", "label_list": ["character", "director", "title", "plot", "review", "rating", "average ratings", "actor", "trailer", "genre", "song", "year"], "instance": {"id": "235", "words": ["what", "was", "the", "name", "of", "clive", "owens", "character", "in", "sin", "city"], "labels": ["O", "O", "O", "O", "O", "B-actor", "I-actor", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, director, title, plot, review, rating, average ratings, actor, trailer, genre, song, year and O.\nSentence: what was the name of clive owens character in sin city", "prompt_labels": "what(O) was(O) the(O) name(O) of(O) clive(B-actor) owens(I-actor) character(O) in(O) sin(B-title) city(I-title)"}}
{"id": "1907", "dataset": "mit-movie", "split": "test", "label_list": ["director", "character", "plot", "title", "average ratings", "trailer", "year", "actor", "genre", "rating", "review", "song"], "instance": {"id": "1907", "words": ["what", "are", "some", "good", "kids", "movies", "starring", "adrian", "pasdar"], "labels": ["O", "O", "O", "B-average ratings", "B-genre", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, character, plot, title, average ratings, trailer, year, actor, genre, rating, review, song and O.\nSentence: what are some good kids movies starring adrian pasdar", "prompt_labels": "what(O) are(O) some(O) good(B-average ratings) kids(B-genre) movies(O) starring(O) adrian(B-actor) pasdar(I-actor)"}}
{"id": "825", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "review", "genre", "trailer", "plot", "title", "character", "director", "song", "rating", "year", "average ratings"], "instance": {"id": "825", "words": ["what", "was", "the", "last", "movie", "released", "in", "2011"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, review, genre, trailer, plot, title, character, director, song, rating, year, average ratings and O.\nSentence: what was the last movie released in 2011", "prompt_labels": "what(O) was(O) the(O) last(O) movie(O) released(O) in(O) 2011(B-year)"}}
{"id": "542", "dataset": "mit-movie", "split": "test", "label_list": ["director", "title", "actor", "review", "character", "trailer", "rating", "average ratings", "genre", "song", "year", "plot"], "instance": {"id": "542", "words": ["find", "a", "review", "for", "the", "last", "harry", "potter", "movie"], "labels": ["O", "O", "O", "O", "O", "O", "B-character", "I-character", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, title, actor, review, character, trailer, rating, average ratings, genre, song, year, plot and O.\nSentence: find a review for the last harry potter movie", "prompt_labels": "find(O) a(O) review(O) for(O) the(O) last(O) harry(B-character) potter(I-character) movie(O)"}}
{"id": "611", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "year", "plot", "song", "actor", "character", "genre", "trailer", "review", "title", "director", "average ratings"], "instance": {"id": "611", "words": ["is", "there", "a", "drama", "starring", "angelina", "jolie", "and", "antonio", "banderas"], "labels": ["O", "O", "O", "B-genre", "O", "B-actor", "I-actor", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, year, plot, song, actor, character, genre, trailer, review, title, director, average ratings and O.\nSentence: is there a drama starring angelina jolie and antonio banderas", "prompt_labels": "is(O) there(O) a(O) drama(B-genre) starring(O) angelina(B-actor) jolie(I-actor) and(O) antonio(B-actor) banderas(I-actor)"}}
{"id": "1086", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "character", "review", "year", "trailer", "rating", "actor", "title", "plot", "genre", "director", "song"], "instance": {"id": "1086", "words": ["are", "there", "any", "crime", "movies", "with", "jennifer", "love", "hewitt", "in", "them"], "labels": ["O", "O", "O", "B-genre", "O", "O", "B-actor", "I-actor", "I-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, character, review, year, trailer, rating, actor, title, plot, genre, director, song and O.\nSentence: are there any crime movies with jennifer love hewitt in them", "prompt_labels": "are(O) there(O) any(O) crime(B-genre) movies(O) with(O) jennifer(B-actor) love(I-actor) hewitt(I-actor) in(O) them(O)"}}
{"id": "482", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "review", "plot", "genre", "director", "average ratings", "trailer", "song", "rating", "character", "title", "year"], "instance": {"id": "482", "words": ["show", "me", "a", "non", "peter", "sellers", "film", "where", "the", "lead", "actor", "plays", "just", "one", "role"], "labels": ["O", "O", "O", "O", "B-actor", "I-actor", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, review, plot, genre, director, average ratings, trailer, song, rating, character, title, year and O.\nSentence: show me a non peter sellers film where the lead actor plays just one role", "prompt_labels": "show(O) me(O) a(O) non(O) peter(B-actor) sellers(I-actor) film(O) where(O) the(O) lead(O) actor(O) plays(O) just(O) one(O) role(O)"}}
{"id": "739", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "plot", "year", "rating", "title", "genre", "director", "character", "trailer", "actor", "review", "song"], "instance": {"id": "739", "words": ["are", "there", "any", "g", "rated", "movies", "with", "paris", "in", "the", "title"], "labels": ["O", "O", "O", "B-rating", "I-rating", "O", "O", "B-plot", "I-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, plot, year, rating, title, genre, director, character, trailer, actor, review, song and O.\nSentence: are there any g rated movies with paris in the title", "prompt_labels": "are(O) there(O) any(O) g(B-rating) rated(I-rating) movies(O) with(O) paris(B-plot) in(I-plot) the(I-plot) title(I-plot)"}}
{"id": "749", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "director", "review", "plot", "character", "title", "song", "average ratings", "trailer", "genre", "actor", "year"], "instance": {"id": "749", "words": ["what", "movie", "is", "r2d2", "in"], "labels": ["O", "O", "O", "B-character", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, review, plot, character, title, song, average ratings, trailer, genre, actor, year and O.\nSentence: what movie is r2d2 in", "prompt_labels": "what(O) movie(O) is(O) r2d2(B-character) in(O)"}}
{"id": "535", "dataset": "mit-movie", "split": "test", "label_list": ["song", "rating", "year", "director", "plot", "genre", "title", "actor", "average ratings", "trailer", "review", "character"], "instance": {"id": "535", "words": ["show", "me", "the", "second", "star", "wars", "film"], "labels": ["O", "O", "O", "O", "B-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, rating, year, director, plot, genre, title, actor, average ratings, trailer, review, character and O.\nSentence: show me the second star wars film", "prompt_labels": "show(O) me(O) the(O) second(O) star(B-title) wars(I-title) film(O)"}}
{"id": "2166", "dataset": "mit-movie", "split": "test", "label_list": ["song", "review", "title", "character", "trailer", "director", "rating", "genre", "average ratings", "plot", "actor", "year"], "instance": {"id": "2166", "words": ["what", "pollution", "mockumentary", "in", "the", "last", "ten", "years", "was", "directed", "by", "stephan", "elliott", "and", "rated", "pg", "13"], "labels": ["O", "B-plot", "B-genre", "O", "O", "B-year", "I-year", "I-year", "O", "O", "O", "B-director", "I-director", "O", "O", "B-rating", "I-rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, review, title, character, trailer, director, rating, genre, average ratings, plot, actor, year and O.\nSentence: what pollution mockumentary in the last ten years was directed by stephan elliott and rated pg 13", "prompt_labels": "what(O) pollution(B-plot) mockumentary(B-genre) in(O) the(O) last(B-year) ten(I-year) years(I-year) was(O) directed(O) by(O) stephan(B-director) elliott(I-director) and(O) rated(O) pg(B-rating) 13(I-rating)"}}
{"id": "2361", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "plot", "song", "year", "title", "trailer", "director", "review", "actor", "rating", "average ratings", "character"], "instance": {"id": "2361", "words": ["list", "animated", "pg", "13", "rated", "movie", "starring", "sean", "connery"], "labels": ["O", "B-genre", "B-rating", "I-rating", "O", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, plot, song, year, title, trailer, director, review, actor, rating, average ratings, character and O.\nSentence: list animated pg 13 rated movie starring sean connery", "prompt_labels": "list(O) animated(B-genre) pg(B-rating) 13(I-rating) rated(O) movie(O) starring(O) sean(B-actor) connery(I-actor)"}}
{"id": "777", "dataset": "mit-movie", "split": "test", "label_list": ["review", "song", "actor", "average ratings", "rating", "character", "title", "year", "trailer", "genre", "director", "plot"], "instance": {"id": "777", "words": ["show", "me", "the", "film", "based", "on", "a", "stephen", "king", "book", "about", "the", "flu", "killing", "most", "of", "humanity"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-genre", "I-genre", "O", "O", "O", "B-plot", "I-plot", "I-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, actor, average ratings, rating, character, title, year, trailer, genre, director, plot and O.\nSentence: show me the film based on a stephen king book about the flu killing most of humanity", "prompt_labels": "show(O) me(O) the(O) film(O) based(O) on(O) a(O) stephen(B-genre) king(I-genre) book(O) about(O) the(O) flu(B-plot) killing(I-plot) most(I-plot) of(I-plot) humanity(I-plot)"}}
{"id": "1594", "dataset": "mit-movie", "split": "test", "label_list": ["title", "year", "actor", "rating", "character", "average ratings", "review", "genre", "director", "trailer", "plot", "song"], "instance": {"id": "1594", "words": ["list", "a", "nc", "17", "rated", "movie", "with", "actress", "kathie", "lee", "gifford", "from", "the", "year", "1940"], "labels": ["O", "O", "B-rating", "I-rating", "O", "O", "O", "O", "B-actor", "I-actor", "I-actor", "O", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, year, actor, rating, character, average ratings, review, genre, director, trailer, plot, song and O.\nSentence: list a nc 17 rated movie with actress kathie lee gifford from the year 1940", "prompt_labels": "list(O) a(O) nc(B-rating) 17(I-rating) rated(O) movie(O) with(O) actress(O) kathie(B-actor) lee(I-actor) gifford(I-actor) from(O) the(O) year(O) 1940(B-year)"}}
{"id": "955", "dataset": "mit-movie", "split": "test", "label_list": ["song", "average ratings", "genre", "plot", "year", "review", "rating", "trailer", "title", "director", "character", "actor"], "instance": {"id": "955", "words": ["is", "the", "hunger", "games", "a", "violent", "movie"], "labels": ["O", "B-title", "I-title", "I-title", "O", "B-plot", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, average ratings, genre, plot, year, review, rating, trailer, title, director, character, actor and O.\nSentence: is the hunger games a violent movie", "prompt_labels": "is(O) the(B-title) hunger(I-title) games(I-title) a(O) violent(B-plot) movie(O)"}}
{"id": "2288", "dataset": "mit-movie", "split": "test", "label_list": ["character", "trailer", "title", "song", "director", "average ratings", "review", "genre", "year", "plot", "actor", "rating"], "instance": {"id": "2288", "words": ["which", "r", "rated", "family", "movie", "starring", "jerry", "obach", "was", "released", "within", "the", "past", "eight", "years"], "labels": ["O", "B-rating", "O", "B-genre", "O", "O", "B-actor", "I-actor", "O", "O", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, trailer, title, song, director, average ratings, review, genre, year, plot, actor, rating and O.\nSentence: which r rated family movie starring jerry obach was released within the past eight years", "prompt_labels": "which(O) r(B-rating) rated(O) family(B-genre) movie(O) starring(O) jerry(B-actor) obach(I-actor) was(O) released(O) within(O) the(O) past(B-year) eight(I-year) years(I-year)"}}
{"id": "1469", "dataset": "mit-movie", "split": "test", "label_list": ["title", "trailer", "director", "character", "plot", "review", "song", "genre", "year", "average ratings", "actor", "rating"], "instance": {"id": "1469", "words": ["is", "there", "a", "animation", "pg", "13", "movie", "with", "trisha", "romance"], "labels": ["O", "O", "O", "B-genre", "B-rating", "I-rating", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, trailer, director, character, plot, review, song, genre, year, average ratings, actor, rating and O.\nSentence: is there a animation pg 13 movie with trisha romance", "prompt_labels": "is(O) there(O) a(O) animation(B-genre) pg(B-rating) 13(I-rating) movie(O) with(O) trisha(B-actor) romance(I-actor)"}}
{"id": "343", "dataset": "mit-movie", "split": "test", "label_list": ["song", "actor", "average ratings", "plot", "trailer", "genre", "character", "review", "title", "director", "rating", "year"], "instance": {"id": "343", "words": ["best", "crome", "movie", "from", "almodovar"], "labels": ["B-review", "O", "O", "O", "B-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, actor, average ratings, plot, trailer, genre, character, review, title, director, rating, year and O.\nSentence: best crome movie from almodovar", "prompt_labels": "best(B-review) crome(O) movie(O) from(O) almodovar(B-director)"}}
{"id": "259", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "character", "rating", "song", "plot", "title", "year", "actor", "genre", "trailer", "review", "director"], "instance": {"id": "259", "words": ["what", "is", "the", "plot", "of", "the", "wild", "bunch"], "labels": ["O", "O", "O", "O", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, character, rating, song, plot, title, year, actor, genre, trailer, review, director and O.\nSentence: what is the plot of the wild bunch", "prompt_labels": "what(O) is(O) the(O) plot(O) of(O) the(B-title) wild(I-title) bunch(I-title)"}}
{"id": "2175", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "trailer", "review", "character", "genre", "title", "song", "actor", "year", "average ratings", "rating", "director"], "instance": {"id": "2175", "words": ["what", "rating", "is", "the", "film", "looney", "tunes", "robin", "hood", "daffy"], "labels": ["O", "O", "O", "O", "O", "B-title", "I-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, trailer, review, character, genre, title, song, actor, year, average ratings, rating, director and O.\nSentence: what rating is the film looney tunes robin hood daffy", "prompt_labels": "what(O) rating(O) is(O) the(O) film(O) looney(B-title) tunes(I-title) robin(I-title) hood(I-title) daffy(I-title)"}}
{"id": "510", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "review", "genre", "rating", "actor", "title", "plot", "director", "character", "average ratings", "song", "year"], "instance": {"id": "510", "words": ["did", "leonardo", "dicaprio", "star", "in", "any", "movies", "based", "on", "a", "play", "by", "shakespeare"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "O", "B-plot", "I-plot", "I-plot", "I-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, review, genre, rating, actor, title, plot, director, character, average ratings, song, year and O.\nSentence: did leonardo dicaprio star in any movies based on a play by shakespeare", "prompt_labels": "did(O) leonardo(B-actor) dicaprio(I-actor) star(O) in(O) any(O) movies(O) based(B-plot) on(I-plot) a(I-plot) play(I-plot) by(I-plot) shakespeare(I-plot)"}}
{"id": "2092", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "plot", "actor", "year", "trailer", "rating", "review", "director", "title", "character", "genre", "song"], "instance": {"id": "2092", "words": ["what", "is", "the", "name", "of", "a", "movie", "for", "children", "directed", "by", "robert", "zemeckis"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-genre", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, plot, actor, year, trailer, rating, review, director, title, character, genre, song and O.\nSentence: what is the name of a movie for children directed by robert zemeckis", "prompt_labels": "what(O) is(O) the(O) name(O) of(O) a(O) movie(O) for(O) children(B-genre) directed(O) by(O) robert(B-director) zemeckis(I-director)"}}
{"id": "1772", "dataset": "mit-movie", "split": "test", "label_list": ["character", "plot", "trailer", "average ratings", "song", "year", "title", "director", "review", "genre", "actor", "rating"], "instance": {"id": "1772", "words": ["tell", "me", "the", "name", "of", "a", "susan", "seidelman", "tale", "movie", "that", "came", "out", "in", "the", "past", "two", "decades", "and", "was", "about", "a", "warthog"], "labels": ["O", "O", "O", "O", "O", "O", "B-director", "I-director", "B-genre", "O", "O", "O", "O", "O", "O", "B-year", "I-year", "I-year", "O", "O", "O", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, plot, trailer, average ratings, song, year, title, director, review, genre, actor, rating and O.\nSentence: tell me the name of a susan seidelman tale movie that came out in the past two decades and was about a warthog", "prompt_labels": "tell(O) me(O) the(O) name(O) of(O) a(O) susan(B-director) seidelman(I-director) tale(B-genre) movie(O) that(O) came(O) out(O) in(O) the(O) past(B-year) two(I-year) decades(I-year) and(O) was(O) about(O) a(O) warthog(B-plot)"}}
{"id": "1283", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "review", "average ratings", "director", "year", "rating", "trailer", "title", "genre", "song", "actor", "character"], "instance": {"id": "1283", "words": ["has", "pascal", "franchot", "directed", "any", "really", "popular", "animation", "in", "the", "last", "two", "years"], "labels": ["O", "B-director", "I-director", "O", "O", "B-average ratings", "I-average ratings", "B-genre", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, review, average ratings, director, year, rating, trailer, title, genre, song, actor, character and O.\nSentence: has pascal franchot directed any really popular animation in the last two years", "prompt_labels": "has(O) pascal(B-director) franchot(I-director) directed(O) any(O) really(B-average ratings) popular(I-average ratings) animation(B-genre) in(O) the(O) last(B-year) two(I-year) years(I-year)"}}
{"id": "207", "dataset": "mit-movie", "split": "test", "label_list": ["trailer", "plot", "director", "genre", "review", "title", "year", "song", "actor", "average ratings", "rating", "character"], "instance": {"id": "207", "words": ["what", "are", "some", "good", "horror", "movies", "from", "1974"], "labels": ["O", "O", "O", "B-review", "B-genre", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, plot, director, genre, review, title, year, song, actor, average ratings, rating, character and O.\nSentence: what are some good horror movies from 1974", "prompt_labels": "what(O) are(O) some(O) good(B-review) horror(B-genre) movies(O) from(O) 1974(B-year)"}}
{"id": "1753", "dataset": "mit-movie", "split": "test", "label_list": ["song", "title", "genre", "year", "rating", "character", "actor", "average ratings", "trailer", "director", "review", "plot"], "instance": {"id": "1753", "words": ["please", "suggest", "some", "animation", "movies", "from", "the", "past", "nine", "decades"], "labels": ["O", "O", "O", "B-genre", "O", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, title, genre, year, rating, character, actor, average ratings, trailer, director, review, plot and O.\nSentence: please suggest some animation movies from the past nine decades", "prompt_labels": "please(O) suggest(O) some(O) animation(B-genre) movies(O) from(O) the(O) past(B-year) nine(I-year) decades(I-year)"}}
{"id": "1659", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "year", "rating", "trailer", "character", "average ratings", "plot", "actor", "review", "song", "title", "director"], "instance": {"id": "1659", "words": ["list", "all", "horror", "movies", "directed", "by", "amos", "kollek", "in", "the", "past", "five", "decades"], "labels": ["O", "O", "B-genre", "O", "O", "O", "B-director", "I-director", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, year, rating, trailer, character, average ratings, plot, actor, review, song, title, director and O.\nSentence: list all horror movies directed by amos kollek in the past five decades", "prompt_labels": "list(O) all(O) horror(B-genre) movies(O) directed(O) by(O) amos(B-director) kollek(I-director) in(O) the(O) past(B-year) five(I-year) decades(I-year)"}}
{"id": "1707", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "title", "director", "year", "character", "review", "song", "genre", "average ratings", "trailer", "plot", "actor"], "instance": {"id": "1707", "words": ["list", "the", "science", "fiction", "films", "directed", "by", "deborah", "kaplan", "from", "the", "last", "six", "decades", "that", "was", "rated", "very", "good", "by", "viewers"], "labels": ["O", "O", "B-genre", "I-genre", "O", "O", "O", "B-director", "I-director", "O", "O", "B-year", "I-year", "I-year", "O", "O", "O", "B-average ratings", "I-average ratings", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, title, director, year, character, review, song, genre, average ratings, trailer, plot, actor and O.\nSentence: list the science fiction films directed by deborah kaplan from the last six decades that was rated very good by viewers", "prompt_labels": "list(O) the(O) science(B-genre) fiction(I-genre) films(O) directed(O) by(O) deborah(B-director) kaplan(I-director) from(O) the(O) last(B-year) six(I-year) decades(I-year) that(O) was(O) rated(O) very(B-average ratings) good(I-average ratings) by(O) viewers(O)"}}
{"id": "1604", "dataset": "mit-movie", "split": "test", "label_list": ["character", "review", "rating", "director", "plot", "title", "actor", "trailer", "average ratings", "song", "genre", "year"], "instance": {"id": "1604", "words": ["list", "a", "biographical", "movie", "that", "has", "excellent", "ratings", "starring", "john", "travolta"], "labels": ["O", "O", "B-genre", "O", "O", "B-average ratings", "I-average ratings", "I-average ratings", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, review, rating, director, plot, title, actor, trailer, average ratings, song, genre, year and O.\nSentence: list a biographical movie that has excellent ratings starring john travolta", "prompt_labels": "list(O) a(O) biographical(B-genre) movie(O) that(O) has(B-average ratings) excellent(I-average ratings) ratings(I-average ratings) starring(O) john(B-actor) travolta(I-actor)"}}
{"id": "2256", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "actor", "title", "trailer", "song", "plot", "average ratings", "review", "character", "year", "director", "genre"], "instance": {"id": "2256", "words": ["whats", "the", "science", "fiction", "movie", "in", "1990", "that", "was", "rated", "r", "averaged", "really", "good", "ratings", "directed", "by", "brett", "ratner"], "labels": ["O", "O", "B-genre", "I-genre", "O", "O", "B-year", "O", "O", "O", "B-rating", "O", "B-average ratings", "I-average ratings", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, actor, title, trailer, song, plot, average ratings, review, character, year, director, genre and O.\nSentence: whats the science fiction movie in 1990 that was rated r averaged really good ratings directed by brett ratner", "prompt_labels": "whats(O) the(O) science(B-genre) fiction(I-genre) movie(O) in(O) 1990(B-year) that(O) was(O) rated(O) r(B-rating) averaged(O) really(B-average ratings) good(I-average ratings) ratings(O) directed(O) by(O) brett(B-director) ratner(I-director)"}}
{"id": "1777", "dataset": "mit-movie", "split": "test", "label_list": ["character", "rating", "genre", "actor", "review", "title", "director", "trailer", "average ratings", "year", "plot", "song"], "instance": {"id": "1777", "words": ["the", "movie", "blue", "steel"], "labels": ["O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, rating, genre, actor, review, title, director, trailer, average ratings, year, plot, song and O.\nSentence: the movie blue steel", "prompt_labels": "the(O) movie(O) blue(B-title) steel(I-title)"}}
{"id": "342", "dataset": "mit-movie", "split": "test", "label_list": ["review", "genre", "title", "trailer", "actor", "plot", "year", "song", "average ratings", "character", "director", "rating"], "instance": {"id": "342", "words": ["in", "what", "spiderman", "film", "did", "peter", "parker", "turns", "evil"], "labels": ["O", "O", "B-title", "O", "O", "B-actor", "I-actor", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, genre, title, trailer, actor, plot, year, song, average ratings, character, director, rating and O.\nSentence: in what spiderman film did peter parker turns evil", "prompt_labels": "in(O) what(O) spiderman(B-title) film(O) did(O) peter(B-actor) parker(I-actor) turns(B-plot) evil(I-plot)"}}
{"id": "1517", "dataset": "mit-movie", "split": "test", "label_list": ["character", "director", "song", "year", "actor", "trailer", "genre", "review", "rating", "title", "plot", "average ratings"], "instance": {"id": "1517", "words": ["is", "there", "a", "movie", "for", "children", "starring", "liam", "neeson"], "labels": ["O", "O", "O", "O", "O", "B-genre", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, director, song, year, actor, trailer, genre, review, rating, title, plot, average ratings and O.\nSentence: is there a movie for children starring liam neeson", "prompt_labels": "is(O) there(O) a(O) movie(O) for(O) children(B-genre) starring(O) liam(B-actor) neeson(I-actor)"}}
{"id": "1313", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "review", "average ratings", "year", "actor", "character", "title", "trailer", "director", "plot", "song", "rating"], "instance": {"id": "1313", "words": ["i", "am", "looking", "for", "a", "thriller", "that", "was", "directed", "by", "alex", "pires", "sometime", "in", "the", "past", "five", "years"], "labels": ["O", "O", "O", "O", "O", "B-genre", "O", "O", "O", "O", "B-director", "I-director", "O", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, review, average ratings, year, actor, character, title, trailer, director, plot, song, rating and O.\nSentence: i am looking for a thriller that was directed by alex pires sometime in the past five years", "prompt_labels": "i(O) am(O) looking(O) for(O) a(O) thriller(B-genre) that(O) was(O) directed(O) by(O) alex(B-director) pires(I-director) sometime(O) in(O) the(O) past(B-year) five(I-year) years(I-year)"}}
{"id": "2161", "dataset": "mit-movie", "split": "test", "label_list": ["rating", "director", "actor", "review", "plot", "title", "average ratings", "song", "genre", "trailer", "year", "character"], "instance": {"id": "2161", "words": ["what", "must", "see", "r", "rated", "horror", "movie", "was", "directed", "by", "john", "hillcoat"], "labels": ["O", "B-average ratings", "I-average ratings", "B-rating", "O", "B-genre", "O", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, director, actor, review, plot, title, average ratings, song, genre, trailer, year, character and O.\nSentence: what must see r rated horror movie was directed by john hillcoat", "prompt_labels": "what(O) must(B-average ratings) see(I-average ratings) r(B-rating) rated(O) horror(B-genre) movie(O) was(O) directed(O) by(O) john(B-director) hillcoat(I-director)"}}
{"id": "1092", "dataset": "mit-movie", "split": "test", "label_list": ["genre", "director", "title", "actor", "trailer", "review", "year", "song", "character", "rating", "plot", "average ratings"], "instance": {"id": "1092", "words": ["are", "there", "any", "movies", "on", "affairs", "in", "the", "past", "year"], "labels": ["O", "O", "O", "O", "O", "B-plot", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, director, title, actor, trailer, review, year, song, character, rating, plot, average ratings and O.\nSentence: are there any movies on affairs in the past year", "prompt_labels": "are(O) there(O) any(O) movies(O) on(O) affairs(B-plot) in(O) the(O) past(B-year) year(I-year)"}}
{"id": "1624", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "review", "genre", "rating", "director", "actor", "trailer", "character", "average ratings", "year", "song", "title"], "instance": {"id": "1624", "words": ["list", "a", "movie", "wall", "e"], "labels": ["O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, review, genre, rating, director, actor, trailer, character, average ratings, year, song, title and O.\nSentence: list a movie wall e", "prompt_labels": "list(O) a(O) movie(O) wall(B-title) e(I-title)"}}
{"id": "2300", "dataset": "mit-movie", "split": "test", "label_list": ["average ratings", "year", "director", "actor", "genre", "song", "rating", "plot", "character", "title", "review", "trailer"], "instance": {"id": "2300", "words": ["who", "directed", "all", "star", "superman"], "labels": ["O", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, year, director, actor, genre, song, rating, plot, character, title, review, trailer and O.\nSentence: who directed all star superman", "prompt_labels": "who(O) directed(O) all(B-title) star(I-title) superman(I-title)"}}
{"id": "1154", "dataset": "mit-movie", "split": "test", "label_list": ["character", "year", "review", "average ratings", "song", "plot", "genre", "rating", "director", "title", "trailer", "actor"], "instance": {"id": "1154", "words": ["did", "nick", "stagliano", "direct", "any", "pg", "13", "mystery", "movies", "in", "the", "1980", "s", "rated", "around", "eight", "stars"], "labels": ["O", "B-director", "I-director", "O", "O", "B-rating", "I-rating", "B-genre", "O", "O", "O", "B-year", "I-year", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, year, review, average ratings, song, plot, genre, rating, director, title, trailer, actor and O.\nSentence: did nick stagliano direct any pg 13 mystery movies in the 1980 s rated around eight stars", "prompt_labels": "did(O) nick(B-director) stagliano(I-director) direct(O) any(O) pg(B-rating) 13(I-rating) mystery(B-genre) movies(O) in(O) the(O) 1980(B-year) s(I-year) rated(O) around(O) eight(B-average ratings) stars(I-average ratings)"}}
{"id": "1561", "dataset": "mit-movie", "split": "test", "label_list": ["title", "year", "genre", "rating", "average ratings", "review", "actor", "director", "plot", "song", "trailer", "character"], "instance": {"id": "1561", "words": ["is", "there", "any", "good", "adventure", "films", "that", "were", "made", "this", "year"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, year, genre, rating, average ratings, review, actor, director, plot, song, trailer, character and O.\nSentence: is there any good adventure films that were made this year", "prompt_labels": "is(O) there(O) any(O) good(O) adventure(B-genre) films(O) that(O) were(O) made(O) this(O) year(O)"}}
{"id": "2075", "dataset": "mit-movie", "split": "test", "label_list": ["title", "song", "review", "plot", "character", "trailer", "average ratings", "actor", "year", "genre", "director", "rating"], "instance": {"id": "2075", "words": ["what", "is", "the", "last", "war", "movie", "that", "received", "two", "thumbs", "up", "about", "a", "general"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O", "O", "B-average ratings", "I-average ratings", "I-average ratings", "O", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, song, review, plot, character, trailer, average ratings, actor, year, genre, director, rating and O.\nSentence: what is the last war movie that received two thumbs up about a general", "prompt_labels": "what(O) is(O) the(O) last(O) war(B-genre) movie(O) that(O) received(O) two(B-average ratings) thumbs(I-average ratings) up(I-average ratings) about(O) a(O) general(B-plot)"}}
{"id": "1782", "dataset": "mit-movie", "split": "test", "label_list": ["director", "year", "genre", "plot", "character", "title", "average ratings", "actor", "song", "review", "trailer", "rating"], "instance": {"id": "1782", "words": ["this", "mockumentary", "of", "the", "last", "decade", "received", "four", "stars"], "labels": ["O", "B-genre", "O", "O", "B-year", "I-year", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, year, genre, plot, character, title, average ratings, actor, song, review, trailer, rating and O.\nSentence: this mockumentary of the last decade received four stars", "prompt_labels": "this(O) mockumentary(B-genre) of(O) the(O) last(B-year) decade(I-year) received(O) four(B-average ratings) stars(I-average ratings)"}}
{"id": "1473", "dataset": "mit-movie", "split": "test", "label_list": ["year", "director", "song", "plot", "review", "title", "character", "average ratings", "trailer", "actor", "rating", "genre"], "instance": {"id": "1473", "words": ["is", "there", "a", "crime", "movie", "withing", "the", "last", "eight", "years", "that", "had", "an", "average", "eight", "star", "rating"], "labels": ["O", "O", "O", "B-genre", "O", "O", "O", "B-year", "I-year", "I-year", "O", "O", "O", "O", "B-average ratings", "I-average ratings", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, song, plot, review, title, character, average ratings, trailer, actor, rating, genre and O.\nSentence: is there a crime movie withing the last eight years that had an average eight star rating", "prompt_labels": "is(O) there(O) a(O) crime(B-genre) movie(O) withing(O) the(O) last(B-year) eight(I-year) years(I-year) that(O) had(O) an(O) average(O) eight(B-average ratings) star(I-average ratings) rating(O)"}}
{"id": "662", "dataset": "mit-movie", "split": "test", "label_list": ["director", "trailer", "review", "plot", "actor", "genre", "song", "year", "character", "average ratings", "title", "rating"], "instance": {"id": "662", "words": ["what", "year", "was", "drive", "released"], "labels": ["O", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, trailer, review, plot, actor, genre, song, year, character, average ratings, title, rating and O.\nSentence: what year was drive released", "prompt_labels": "what(O) year(O) was(O) drive(B-year) released(I-year)"}}
{"id": "1486", "dataset": "mit-movie", "split": "test", "label_list": ["actor", "rating", "character", "average ratings", "title", "director", "plot", "review", "genre", "song", "year", "trailer"], "instance": {"id": "1486", "words": ["is", "there", "a", "good", "comedy", "that", "is", "rated", "pg", "13", "about", "sexual", "desire"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O", "O", "B-rating", "I-rating", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, rating, character, average ratings, title, director, plot, review, genre, song, year, trailer and O.\nSentence: is there a good comedy that is rated pg 13 about sexual desire", "prompt_labels": "is(O) there(O) a(O) good(O) comedy(B-genre) that(O) is(O) rated(O) pg(B-rating) 13(I-rating) about(O) sexual(B-plot) desire(I-plot)"}}
{"id": "1437", "dataset": "mit-movie", "split": "test", "label_list": ["plot", "director", "song", "trailer", "actor", "character", "title", "rating", "review", "year", "genre", "average ratings"], "instance": {"id": "1437", "words": ["is", "heath", "ledger", "in", "and", "rated", "g", "film", "noir", "movies", "that", "got", "two", "thumbs", "up"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "B-rating", "B-genre", "I-genre", "O", "O", "O", "B-average ratings", "I-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, director, song, trailer, actor, character, title, rating, review, year, genre, average ratings and O.\nSentence: is heath ledger in and rated g film noir movies that got two thumbs up", "prompt_labels": "is(O) heath(B-actor) ledger(I-actor) in(O) and(O) rated(O) g(B-rating) film(B-genre) noir(I-genre) movies(O) that(O) got(O) two(B-average ratings) thumbs(I-average ratings) up(I-average ratings)"}}
{"id": "2185", "dataset": "mit-movie", "split": "test", "label_list": ["review", "plot", "title", "song", "year", "director", "average ratings", "actor", "rating", "character", "genre", "trailer"], "instance": {"id": "2185", "words": ["what", "science", "fiction", "films", "did", "j", "randolph", "harrison", "direct", "from", "1970"], "labels": ["O", "B-genre", "I-genre", "O", "O", "B-director", "I-director", "I-director", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, title, song, year, director, average ratings, actor, rating, character, genre, trailer and O.\nSentence: what science fiction films did j randolph harrison direct from 1970", "prompt_labels": "what(O) science(B-genre) fiction(I-genre) films(O) did(O) j(B-director) randolph(I-director) harrison(I-director) direct(O) from(O) 1970(B-year)"}}
{"id": "874", "dataset": "mit-movie", "split": "test", "label_list": ["song", "average ratings", "director", "title", "character", "genre", "year", "review", "rating", "plot", "trailer", "actor"], "instance": {"id": "874", "words": ["i", "want", "a", "1970s", "sailing", "flick"], "labels": ["O", "O", "O", "B-year", "B-plot", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, average ratings, director, title, character, genre, year, review, rating, plot, trailer, actor and O.\nSentence: i want a 1970s sailing flick", "prompt_labels": "i(O) want(O) a(O) 1970s(B-year) sailing(B-plot) flick(O)"}}
{"id": "1323", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Rating", "Amenity", "Price", "Hours", "Restaurant Name", "Cuisine", "Location"], "instance": {"id": "1323", "words": ["where", "can", "i", "get", "caribbean", "food", "not", "far", "from", "here", "in", "a", "romantic", "setting"], "labels": ["O", "O", "O", "O", "B-Cuisine", "O", "B-Location", "I-Location", "I-Location", "I-Location", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Rating, Amenity, Price, Hours, Restaurant Name, Cuisine, Location and O.\nSentence: where can i get caribbean food not far from here in a romantic setting", "prompt_labels": "where(O) can(O) i(O) get(O) caribbean(B-Cuisine) food(O) not(B-Location) far(I-Location) from(I-Location) here(I-Location) in(O) a(O) romantic(B-Amenity) setting(I-Amenity)"}}
{"id": "1031", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Hours", "Amenity", "Rating", "Dish", "Price", "Location", "Cuisine"], "instance": {"id": "1031", "words": ["please", "help", "me", "locate", "a", "restaurant", "that", "allows", "smoking"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Amenity, Rating, Dish, Price, Location, Cuisine and O.\nSentence: please help me locate a restaurant that allows smoking", "prompt_labels": "please(O) help(O) me(O) locate(O) a(O) restaurant(O) that(O) allows(B-Amenity) smoking(I-Amenity)"}}
{"id": "1426", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Dish", "Location", "Hours", "Rating", "Cuisine", "Amenity", "Price"], "instance": {"id": "1426", "words": ["where", "is", "the", "nearest", "4", "star", "restaurant"], "labels": ["O", "O", "O", "B-Location", "B-Rating", "I-Rating", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Location, Hours, Rating, Cuisine, Amenity, Price and O.\nSentence: where is the nearest 4 star restaurant", "prompt_labels": "where(O) is(O) the(O) nearest(B-Location) 4(B-Rating) star(I-Rating) restaurant(O)"}}
{"id": "1206", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Cuisine", "Rating", "Dish", "Location", "Restaurant Name", "Amenity", "Hours"], "instance": {"id": "1206", "words": ["what", "time", "does", "mcdonalds", "in", "main", "street", "open"], "labels": ["O", "O", "O", "B-Restaurant Name", "O", "B-Location", "I-Location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Rating, Dish, Location, Restaurant Name, Amenity, Hours and O.\nSentence: what time does mcdonalds in main street open", "prompt_labels": "what(O) time(O) does(O) mcdonalds(B-Restaurant Name) in(O) main(B-Location) street(I-Location) open(O)"}}
{"id": "1145", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Cuisine", "Price", "Amenity", "Hours", "Rating", "Restaurant Name", "Location"], "instance": {"id": "1145", "words": ["what", "is", "the", "phone", "number", "of", "bills", "pizza"], "labels": ["O", "O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Price, Amenity, Hours, Rating, Restaurant Name, Location and O.\nSentence: what is the phone number of bills pizza", "prompt_labels": "what(O) is(O) the(O) phone(O) number(O) of(O) bills(B-Restaurant Name) pizza(I-Restaurant Name)"}}
{"id": "0", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Dish", "Price", "Rating", "Location", "Amenity", "Cuisine", "Hours"], "instance": {"id": "0", "words": ["a", "four", "star", "restaurant", "with", "a", "bar"], "labels": ["O", "B-Rating", "I-Rating", "O", "B-Location", "I-Location", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Price, Rating, Location, Amenity, Cuisine, Hours and O.\nSentence: a four star restaurant with a bar", "prompt_labels": "a(O) four(B-Rating) star(I-Rating) restaurant(O) with(B-Location) a(I-Location) bar(B-Amenity)"}}
{"id": "491", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Amenity", "Dish", "Hours", "Price", "Cuisine", "Location", "Rating"], "instance": {"id": "491", "words": ["hi", "please", "find", "me", "a", "sushi", "restaurant", "that", "has", "good", "reviews", "and", "that", "isnt", "too", "expensive"], "labels": ["O", "O", "O", "O", "O", "B-Cuisine", "O", "O", "O", "B-Rating", "I-Rating", "O", "O", "B-Price", "I-Price", "I-Price"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Dish, Hours, Price, Cuisine, Location, Rating and O.\nSentence: hi please find me a sushi restaurant that has good reviews and that isnt too expensive", "prompt_labels": "hi(O) please(O) find(O) me(O) a(O) sushi(B-Cuisine) restaurant(O) that(O) has(O) good(B-Rating) reviews(I-Rating) and(O) that(O) isnt(B-Price) too(I-Price) expensive(I-Price)"}}
{"id": "1434", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Price", "Amenity", "Restaurant Name", "Dish", "Hours", "Cuisine", "Location"], "instance": {"id": "1434", "words": ["where", "is", "the", "nearest", "ice", "cream", "shop"], "labels": ["O", "O", "O", "B-Location", "B-Cuisine", "I-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Amenity, Restaurant Name, Dish, Hours, Cuisine, Location and O.\nSentence: where is the nearest ice cream shop", "prompt_labels": "where(O) is(O) the(O) nearest(B-Location) ice(B-Cuisine) cream(I-Cuisine) shop(O)"}}
{"id": "29", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Hours", "Rating", "Cuisine", "Location", "Price", "Dish", "Restaurant Name"], "instance": {"id": "29", "words": ["are", "there", "any", "child", "friendly", "restaurants", "within", "ten", "miles"], "labels": ["O", "O", "O", "B-Amenity", "I-Amenity", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Rating, Cuisine, Location, Price, Dish, Restaurant Name and O.\nSentence: are there any child friendly restaurants within ten miles", "prompt_labels": "are(O) there(O) any(O) child(B-Amenity) friendly(I-Amenity) restaurants(O) within(B-Location) ten(I-Location) miles(I-Location)"}}
{"id": "203", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Price", "Amenity", "Hours", "Rating", "Restaurant Name", "Cuisine", "Dish"], "instance": {"id": "203", "words": ["can", "you", "search", "for", "the", "most", "expensive", "restaurant"], "labels": ["O", "O", "O", "O", "O", "B-Price", "I-Price", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Amenity, Hours, Rating, Restaurant Name, Cuisine, Dish and O.\nSentence: can you search for the most expensive restaurant", "prompt_labels": "can(O) you(O) search(O) for(O) the(O) most(B-Price) expensive(I-Price) restaurant(O)"}}
{"id": "813", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Cuisine", "Hours", "Amenity", "Restaurant Name", "Rating", "Price", "Location"], "instance": {"id": "813", "words": ["is", "there", "a", "diner", "with", "a", "patio", "in", "boxford"], "labels": ["O", "O", "O", "B-Cuisine", "O", "O", "B-Amenity", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Hours, Amenity, Restaurant Name, Rating, Price, Location and O.\nSentence: is there a diner with a patio in boxford", "prompt_labels": "is(O) there(O) a(O) diner(B-Cuisine) with(O) a(O) patio(B-Amenity) in(O) boxford(B-Location)"}}
{"id": "1409", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Location", "Rating", "Hours", "Restaurant Name", "Dish", "Amenity", "Price"], "instance": {"id": "1409", "words": ["where", "is", "the", "closest", "apple", "bees"], "labels": ["O", "O", "O", "B-Location", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Rating, Hours, Restaurant Name, Dish, Amenity, Price and O.\nSentence: where is the closest apple bees", "prompt_labels": "where(O) is(O) the(O) closest(B-Location) apple(B-Restaurant Name) bees(I-Restaurant Name)"}}
{"id": "581", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Amenity", "Location", "Price", "Cuisine", "Hours", "Dish", "Rating"], "instance": {"id": "581", "words": ["i", "need", "a", "place", "for", "kids", "to", "eat", "at", "12", "pm"], "labels": ["O", "O", "O", "O", "O", "B-Amenity", "O", "O", "O", "B-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Location, Price, Cuisine, Hours, Dish, Rating and O.\nSentence: i need a place for kids to eat at 12 pm", "prompt_labels": "i(O) need(O) a(O) place(O) for(O) kids(B-Amenity) to(O) eat(O) at(O) 12(B-Hours) pm(I-Hours)"}}
{"id": "1123", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Amenity", "Dish", "Location", "Restaurant Name", "Cuisine", "Hours", "Price"], "instance": {"id": "1123", "words": ["what", "is", "the", "best", "steak", "house", "in", "kansas", "city", "mo"], "labels": ["O", "O", "O", "B-Rating", "B-Cuisine", "I-Cuisine", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Dish, Location, Restaurant Name, Cuisine, Hours, Price and O.\nSentence: what is the best steak house in kansas city mo", "prompt_labels": "what(O) is(O) the(O) best(B-Rating) steak(B-Cuisine) house(I-Cuisine) in(O) kansas(B-Location) city(I-Location) mo(I-Location)"}}
{"id": "671", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Rating", "Restaurant Name", "Amenity", "Cuisine", "Hours", "Dish", "Location"], "instance": {"id": "671", "words": ["i", "want", "to", "go", "to", "a", "nicely", "priced", "place", "within", "10", "minutes", "that", "has", "a", "good", "tomato", "sauce"], "labels": ["O", "O", "O", "O", "O", "O", "B-Price", "O", "O", "B-Location", "I-Location", "I-Location", "O", "O", "O", "B-Rating", "B-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Restaurant Name, Amenity, Cuisine, Hours, Dish, Location and O.\nSentence: i want to go to a nicely priced place within 10 minutes that has a good tomato sauce", "prompt_labels": "i(O) want(O) to(O) go(O) to(O) a(O) nicely(B-Price) priced(O) place(O) within(B-Location) 10(I-Location) minutes(I-Location) that(O) has(O) a(O) good(B-Rating) tomato(B-Dish) sauce(I-Dish)"}}
{"id": "504", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Cuisine", "Amenity", "Dish", "Price", "Location", "Hours", "Restaurant Name"], "instance": {"id": "504", "words": ["how", "do", "you", "get", "to", "als", "diner", "from", "here"], "labels": ["O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Amenity, Dish, Price, Location, Hours, Restaurant Name and O.\nSentence: how do you get to als diner from here", "prompt_labels": "how(O) do(O) you(O) get(O) to(O) als(B-Restaurant Name) diner(I-Restaurant Name) from(B-Location) here(I-Location)"}}
{"id": "1074", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Hours", "Amenity", "Restaurant Name", "Rating", "Cuisine", "Dish", "Location"], "instance": {"id": "1074", "words": ["take", "me", "to", "a", "nice", "restaurant", "with", "generous", "portions"], "labels": ["O", "O", "O", "O", "B-Rating", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Amenity, Restaurant Name, Rating, Cuisine, Dish, Location and O.\nSentence: take me to a nice restaurant with generous portions", "prompt_labels": "take(O) me(O) to(O) a(O) nice(B-Rating) restaurant(O) with(O) generous(B-Amenity) portions(I-Amenity)"}}
{"id": "1471", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Rating", "Hours", "Location", "Cuisine", "Amenity", "Price", "Dish"], "instance": {"id": "1471", "words": ["wheres", "the", "closest", "pizza", "place"], "labels": ["O", "O", "B-Location", "B-Dish", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Hours, Location, Cuisine, Amenity, Price, Dish and O.\nSentence: wheres the closest pizza place", "prompt_labels": "wheres(O) the(O) closest(B-Location) pizza(B-Dish) place(O)"}}
{"id": "482", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Rating", "Cuisine", "Location", "Hours", "Amenity", "Dish", "Restaurant Name"], "instance": {"id": "482", "words": ["help", "me", "find", "a", "place", "my", "kids", "would", "like", "to", "eat"], "labels": ["O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Cuisine, Location, Hours, Amenity, Dish, Restaurant Name and O.\nSentence: help me find a place my kids would like to eat", "prompt_labels": "help(O) me(O) find(O) a(O) place(O) my(O) kids(B-Amenity) would(I-Amenity) like(I-Amenity) to(I-Amenity) eat(I-Amenity)"}}
{"id": "512", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Rating", "Hours", "Amenity", "Restaurant Name", "Cuisine", "Location", "Price"], "instance": {"id": "512", "words": ["how", "far", "away", "is", "the", "nearest", "steak", "house"], "labels": ["O", "O", "O", "O", "O", "B-Location", "B-Cuisine", "I-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Rating, Hours, Amenity, Restaurant Name, Cuisine, Location, Price and O.\nSentence: how far away is the nearest steak house", "prompt_labels": "how(O) far(O) away(O) is(O) the(O) nearest(B-Location) steak(B-Cuisine) house(I-Cuisine)"}}
{"id": "255", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Location", "Hours", "Rating", "Amenity", "Price", "Restaurant Name", "Cuisine"], "instance": {"id": "255", "words": ["do", "you", "know", "if", "reggianos", "serve", "breakfast"], "labels": ["O", "O", "O", "O", "B-Restaurant Name", "O", "B-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Hours, Rating, Amenity, Price, Restaurant Name, Cuisine and O.\nSentence: do you know if reggianos serve breakfast", "prompt_labels": "do(O) you(O) know(O) if(O) reggianos(B-Restaurant Name) serve(O) breakfast(B-Hours)"}}
{"id": "760", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Cuisine", "Restaurant Name", "Amenity", "Location", "Dish", "Rating", "Hours"], "instance": {"id": "760", "words": ["is", "a", "dairy", "queen", "nearby"], "labels": ["O", "O", "B-Restaurant Name", "I-Restaurant Name", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Restaurant Name, Amenity, Location, Dish, Rating, Hours and O.\nSentence: is a dairy queen nearby", "prompt_labels": "is(O) a(O) dairy(B-Restaurant Name) queen(I-Restaurant Name) nearby(B-Location)"}}
{"id": "604", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Restaurant Name", "Location", "Price", "Cuisine", "Rating", "Hours", "Amenity"], "instance": {"id": "604", "words": ["i", "need", "something", "hot", "to", "eat", "on", "the", "way", "to", "work"], "labels": ["O", "O", "O", "B-Cuisine", "O", "O", "O", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Restaurant Name, Location, Price, Cuisine, Rating, Hours, Amenity and O.\nSentence: i need something hot to eat on the way to work", "prompt_labels": "i(O) need(O) something(O) hot(B-Cuisine) to(O) eat(O) on(O) the(O) way(B-Location) to(I-Location) work(I-Location)"}}
{"id": "1269", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Amenity", "Cuisine", "Rating", "Dish", "Hours", "Location", "Price"], "instance": {"id": "1269", "words": ["where", "can", "i", "find", "a", "coffee", "shop", "within", "10", "miles", "that", "is", "not", "part", "of", "a", "chain"], "labels": ["O", "O", "O", "O", "O", "B-Cuisine", "O", "B-Location", "I-Location", "I-Location", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Cuisine, Rating, Dish, Hours, Location, Price and O.\nSentence: where can i find a coffee shop within 10 miles that is not part of a chain", "prompt_labels": "where(O) can(O) i(O) find(O) a(O) coffee(B-Cuisine) shop(O) within(B-Location) 10(I-Location) miles(I-Location) that(O) is(O) not(B-Amenity) part(I-Amenity) of(I-Amenity) a(I-Amenity) chain(I-Amenity)"}}
{"id": "539", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Price", "Rating", "Location", "Dish", "Amenity", "Restaurant Name", "Hours"], "instance": {"id": "539", "words": ["huges", "bar", "and", "brill", "thanks"], "labels": ["B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Rating, Location, Dish, Amenity, Restaurant Name, Hours and O.\nSentence: huges bar and brill thanks", "prompt_labels": "huges(B-Restaurant Name) bar(I-Restaurant Name) and(I-Restaurant Name) brill(I-Restaurant Name) thanks(O)"}}
{"id": "311", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Price", "Rating", "Dish", "Amenity", "Cuisine", "Location", "Hours"], "instance": {"id": "311", "words": ["does", "the", "chinese", "buffet", "on", "6", "th", "avenue", "have", "a", "smoking", "section"], "labels": ["O", "O", "B-Cuisine", "I-Cuisine", "O", "B-Location", "I-Location", "I-Location", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Rating, Dish, Amenity, Cuisine, Location, Hours and O.\nSentence: does the chinese buffet on 6 th avenue have a smoking section", "prompt_labels": "does(O) the(O) chinese(B-Cuisine) buffet(I-Cuisine) on(O) 6(B-Location) th(I-Location) avenue(I-Location) have(O) a(O) smoking(B-Amenity) section(I-Amenity)"}}
{"id": "168", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Price", "Location", "Rating", "Amenity", "Dish", "Cuisine", "Restaurant Name"], "instance": {"id": "168", "words": ["can", "you", "find", "me", "hotel", "dining", "with", "comfort", "food"], "labels": ["O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "O", "B-Cuisine", "I-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Location, Rating, Amenity, Dish, Cuisine, Restaurant Name and O.\nSentence: can you find me hotel dining with comfort food", "prompt_labels": "can(O) you(O) find(O) me(O) hotel(B-Restaurant Name) dining(I-Restaurant Name) with(O) comfort(B-Cuisine) food(I-Cuisine)"}}
{"id": "1348", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Amenity", "Cuisine", "Dish", "Rating", "Hours", "Restaurant Name", "Price"], "instance": {"id": "1348", "words": ["where", "can", "i", "get", "the", "top", "rated", "hamburger", "in", "baltimore"], "labels": ["O", "O", "O", "O", "O", "B-Rating", "I-Rating", "B-Dish", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Cuisine, Dish, Rating, Hours, Restaurant Name, Price and O.\nSentence: where can i get the top rated hamburger in baltimore", "prompt_labels": "where(O) can(O) i(O) get(O) the(O) top(B-Rating) rated(I-Rating) hamburger(B-Dish) in(O) baltimore(B-Location)"}}
{"id": "795", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Restaurant Name", "Rating", "Cuisine", "Amenity", "Dish", "Hours", "Location"], "instance": {"id": "795", "words": ["is", "the", "tylers", "restaurant", "in", "baltimore", "a", "little", "pricey"], "labels": ["O", "O", "B-Restaurant Name", "I-Restaurant Name", "O", "B-Location", "O", "B-Price", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Restaurant Name, Rating, Cuisine, Amenity, Dish, Hours, Location and O.\nSentence: is the tylers restaurant in baltimore a little pricey", "prompt_labels": "is(O) the(O) tylers(B-Restaurant Name) restaurant(I-Restaurant Name) in(O) baltimore(B-Location) a(O) little(B-Price) pricey(O)"}}
{"id": "175", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Hours", "Cuisine", "Amenity", "Dish", "Location", "Restaurant Name", "Rating"], "instance": {"id": "175", "words": ["can", "you", "find", "the", "waterfront", "restaurant", "albertos", "deli", "of", "course", "thats", "open", "until", "11", "pm"], "labels": ["O", "O", "O", "O", "B-Location", "O", "B-Restaurant Name", "I-Restaurant Name", "O", "O", "O", "B-Hours", "I-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Cuisine, Amenity, Dish, Location, Restaurant Name, Rating and O.\nSentence: can you find the waterfront restaurant albertos deli of course thats open until 11 pm", "prompt_labels": "can(O) you(O) find(O) the(O) waterfront(B-Location) restaurant(O) albertos(B-Restaurant Name) deli(I-Restaurant Name) of(O) course(O) thats(O) open(B-Hours) until(I-Hours) 11(I-Hours) pm(I-Hours)"}}
{"id": "989", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Location", "Cuisine", "Restaurant Name", "Rating", "Dish", "Amenity", "Price"], "instance": {"id": "989", "words": ["looking", "for", "urban", "gourmet", "on", "bay", "road", "with", "great", "wine", "lists"], "labels": ["O", "O", "B-Cuisine", "I-Cuisine", "O", "B-Location", "I-Location", "O", "B-Rating", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Cuisine, Restaurant Name, Rating, Dish, Amenity, Price and O.\nSentence: looking for urban gourmet on bay road with great wine lists", "prompt_labels": "looking(O) for(O) urban(B-Cuisine) gourmet(I-Cuisine) on(O) bay(B-Location) road(I-Location) with(O) great(B-Rating) wine(O) lists(O)"}}
{"id": "1401", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Cuisine", "Rating", "Location", "Amenity", "Dish", "Price", "Restaurant Name"], "instance": {"id": "1401", "words": ["where", "is", "the", "apple", "store", "in", "the", "area"], "labels": ["O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Rating, Location, Amenity, Dish, Price, Restaurant Name and O.\nSentence: where is the apple store in the area", "prompt_labels": "where(O) is(O) the(O) apple(B-Restaurant Name) store(I-Restaurant Name) in(B-Location) the(I-Location) area(I-Location)"}}
{"id": "362", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Rating", "Dish", "Amenity", "Restaurant Name", "Cuisine", "Hours", "Location"], "instance": {"id": "362", "words": ["find", "me", "a", "chinese", "take", "out", "restaurant"], "labels": ["O", "O", "O", "B-Cuisine", "B-Amenity", "I-Amenity", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Dish, Amenity, Restaurant Name, Cuisine, Hours, Location and O.\nSentence: find me a chinese take out restaurant", "prompt_labels": "find(O) me(O) a(O) chinese(B-Cuisine) take(B-Amenity) out(I-Amenity) restaurant(O)"}}
{"id": "1415", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Rating", "Location", "Price", "Hours", "Dish", "Amenity", "Cuisine"], "instance": {"id": "1415", "words": ["where", "is", "the", "closest", "non", "smoking", "restaurant"], "labels": ["O", "O", "O", "B-Location", "B-Amenity", "I-Amenity", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Location, Price, Hours, Dish, Amenity, Cuisine and O.\nSentence: where is the closest non smoking restaurant", "prompt_labels": "where(O) is(O) the(O) closest(B-Location) non(B-Amenity) smoking(I-Amenity) restaurant(O)"}}
{"id": "237", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Price", "Location", "Amenity", "Dish", "Restaurant Name", "Cuisine", "Hours"], "instance": {"id": "237", "words": ["do", "any", "of", "the", "family", "restaurants", "down", "town", "serve", "strawberry", "milk"], "labels": ["O", "O", "O", "O", "B-Amenity", "O", "B-Location", "I-Location", "O", "B-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Location, Amenity, Dish, Restaurant Name, Cuisine, Hours and O.\nSentence: do any of the family restaurants down town serve strawberry milk", "prompt_labels": "do(O) any(O) of(O) the(O) family(B-Amenity) restaurants(O) down(B-Location) town(I-Location) serve(O) strawberry(B-Dish) milk(I-Dish)"}}
{"id": "76", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Cuisine", "Hours", "Rating", "Restaurant Name", "Amenity", "Price", "Location"], "instance": {"id": "76", "words": ["are", "there", "any", "restaurants", "on", "kilmarnock", "street", "that", "feature", "large", "portions", "and", "a", "brewpub"], "labels": ["O", "O", "O", "O", "O", "B-Location", "I-Location", "O", "O", "B-Amenity", "I-Amenity", "O", "O", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Hours, Rating, Restaurant Name, Amenity, Price, Location and O.\nSentence: are there any restaurants on kilmarnock street that feature large portions and a brewpub", "prompt_labels": "are(O) there(O) any(O) restaurants(O) on(O) kilmarnock(B-Location) street(I-Location) that(O) feature(O) large(B-Amenity) portions(I-Amenity) and(O) a(O) brewpub(B-Amenity)"}}
{"id": "670", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Dish", "Restaurant Name", "Rating", "Hours", "Cuisine", "Location", "Price"], "instance": {"id": "670", "words": ["i", "want", "to", "go", "dancing", "at", "a", "nearby", "place", "and", "i", "want", "scallops", "while", "im", "at", "it"], "labels": ["O", "O", "O", "O", "B-Amenity", "O", "O", "B-Location", "O", "O", "O", "O", "B-Dish", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Restaurant Name, Rating, Hours, Cuisine, Location, Price and O.\nSentence: i want to go dancing at a nearby place and i want scallops while im at it", "prompt_labels": "i(O) want(O) to(O) go(O) dancing(B-Amenity) at(O) a(O) nearby(B-Location) place(O) and(O) i(O) want(O) scallops(B-Dish) while(O) im(O) at(O) it(O)"}}
{"id": "1499", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Cuisine", "Amenity", "Dish", "Restaurant Name", "Hours", "Price", "Location"], "instance": {"id": "1499", "words": ["which", "restaurants", "near", "here", "are", "open", "till", "midnight"], "labels": ["O", "O", "B-Location", "I-Location", "O", "B-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Amenity, Dish, Restaurant Name, Hours, Price, Location and O.\nSentence: which restaurants near here are open till midnight", "prompt_labels": "which(O) restaurants(O) near(B-Location) here(I-Location) are(O) open(B-Hours) till(I-Hours) midnight(I-Hours)"}}
{"id": "797", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Price", "Location", "Rating", "Cuisine", "Hours", "Dish", "Amenity"], "instance": {"id": "797", "words": ["is", "there", "a", "bakery", "near", "here"], "labels": ["O", "O", "O", "B-Cuisine", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Location, Rating, Cuisine, Hours, Dish, Amenity and O.\nSentence: is there a bakery near here", "prompt_labels": "is(O) there(O) a(O) bakery(B-Cuisine) near(B-Location) here(I-Location)"}}
{"id": "879", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Dish", "Restaurant Name", "Location", "Cuisine", "Price", "Hours", "Rating"], "instance": {"id": "879", "words": ["is", "there", "a", "thai", "restaurant", "with", "a", "great", "wine", "list"], "labels": ["O", "O", "O", "B-Cuisine", "O", "O", "O", "B-Rating", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Restaurant Name, Location, Cuisine, Price, Hours, Rating and O.\nSentence: is there a thai restaurant with a great wine list", "prompt_labels": "is(O) there(O) a(O) thai(B-Cuisine) restaurant(O) with(O) a(O) great(B-Rating) wine(B-Amenity) list(I-Amenity)"}}
{"id": "1022", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Location", "Dish", "Rating", "Cuisine", "Restaurant Name", "Amenity", "Price"], "instance": {"id": "1022", "words": ["please", "find", "me", "a", "cheap", "chinese", "restaurant", "nearby"], "labels": ["O", "O", "O", "O", "B-Price", "B-Cuisine", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Dish, Rating, Cuisine, Restaurant Name, Amenity, Price and O.\nSentence: please find me a cheap chinese restaurant nearby", "prompt_labels": "please(O) find(O) me(O) a(O) cheap(B-Price) chinese(B-Cuisine) restaurant(O) nearby(B-Location)"}}
{"id": "421", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Rating", "Hours", "Dish", "Amenity", "Location", "Restaurant Name", "Price"], "instance": {"id": "421", "words": ["find", "me", "an", "italian", "restaurant", "which", "has", "received", "high", "ratings", "for", "its", "service"], "labels": ["O", "O", "O", "B-Cuisine", "O", "O", "O", "O", "B-Rating", "I-Rating", "I-Rating", "I-Rating", "I-Rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Hours, Dish, Amenity, Location, Restaurant Name, Price and O.\nSentence: find me an italian restaurant which has received high ratings for its service", "prompt_labels": "find(O) me(O) an(O) italian(B-Cuisine) restaurant(O) which(O) has(O) received(O) high(B-Rating) ratings(I-Rating) for(I-Rating) its(I-Rating) service(I-Rating)"}}
{"id": "35", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Rating", "Location", "Amenity", "Dish", "Price", "Cuisine", "Hours"], "instance": {"id": "35", "words": ["are", "there", "any", "exciting", "joints", "along", "the", "way", "thats", "reasonably", "priced"], "labels": ["O", "O", "O", "B-Amenity", "O", "B-Location", "I-Location", "I-Location", "O", "B-Price", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Location, Amenity, Dish, Price, Cuisine, Hours and O.\nSentence: are there any exciting joints along the way thats reasonably priced", "prompt_labels": "are(O) there(O) any(O) exciting(B-Amenity) joints(O) along(B-Location) the(I-Location) way(I-Location) thats(O) reasonably(B-Price) priced(O)"}}
{"id": "427", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Restaurant Name", "Hours", "Cuisine", "Rating", "Amenity", "Location", "Dish"], "instance": {"id": "427", "words": ["find", "me", "restaurant", "that", "isnt", "cheap", "with", "chocolate", "cake", "on", "the", "dessert", "menu", "and", "byob"], "labels": ["O", "O", "O", "O", "B-Price", "I-Price", "O", "B-Dish", "I-Dish", "O", "O", "O", "O", "O", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Restaurant Name, Hours, Cuisine, Rating, Amenity, Location, Dish and O.\nSentence: find me restaurant that isnt cheap with chocolate cake on the dessert menu and byob", "prompt_labels": "find(O) me(O) restaurant(O) that(O) isnt(B-Price) cheap(I-Price) with(O) chocolate(B-Dish) cake(I-Dish) on(O) the(O) dessert(O) menu(O) and(O) byob(B-Amenity)"}}
{"id": "1382", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Location", "Hours", "Amenity", "Rating", "Dish", "Price", "Restaurant Name"], "instance": {"id": "1382", "words": ["where", "is", "a", "restaurant", "that", "opens", "24", "hours"], "labels": ["O", "O", "O", "O", "O", "B-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Hours, Amenity, Rating, Dish, Price, Restaurant Name and O.\nSentence: where is a restaurant that opens 24 hours", "prompt_labels": "where(O) is(O) a(O) restaurant(O) that(O) opens(B-Hours) 24(I-Hours) hours(I-Hours)"}}
{"id": "145", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Rating", "Cuisine", "Price", "Hours", "Restaurant Name", "Dish", "Location"], "instance": {"id": "145", "words": ["can", "you", "find", "a", "steak", "house", "that", "serves", "wine"], "labels": ["O", "O", "O", "O", "B-Cuisine", "I-Cuisine", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Cuisine, Price, Hours, Restaurant Name, Dish, Location and O.\nSentence: can you find a steak house that serves wine", "prompt_labels": "can(O) you(O) find(O) a(O) steak(B-Cuisine) house(I-Cuisine) that(O) serves(O) wine(B-Dish)"}}
{"id": "622", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Cuisine", "Rating", "Amenity", "Hours", "Location", "Restaurant Name", "Dish"], "instance": {"id": "622", "words": ["i", "want", "a", "greek", "sandwich", "with", "goat", "cheese"], "labels": ["O", "O", "O", "B-Cuisine", "B-Dish", "I-Dish", "I-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Rating, Amenity, Hours, Location, Restaurant Name, Dish and O.\nSentence: i want a greek sandwich with goat cheese", "prompt_labels": "i(O) want(O) a(O) greek(B-Cuisine) sandwich(B-Dish) with(I-Dish) goat(I-Dish) cheese(I-Dish)"}}
{"id": "1301", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Price", "Restaurant Name", "Amenity", "Dish", "Rating", "Hours", "Cuisine"], "instance": {"id": "1301", "words": ["where", "can", "i", "find", "the", "cheapest", "take", "out", "chinese", "food"], "labels": ["O", "O", "O", "O", "O", "B-Price", "B-Amenity", "I-Amenity", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Restaurant Name, Amenity, Dish, Rating, Hours, Cuisine and O.\nSentence: where can i find the cheapest take out chinese food", "prompt_labels": "where(O) can(O) i(O) find(O) the(O) cheapest(B-Price) take(B-Amenity) out(I-Amenity) chinese(B-Cuisine) food(O)"}}
{"id": "1003", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Cuisine", "Restaurant Name", "Hours", "Price", "Dish", "Amenity", "Location"], "instance": {"id": "1003", "words": ["my", "date", "and", "i", "would", "like", "some", "espresso", "thats", "within", "10", "min"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Dish", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Restaurant Name, Hours, Price, Dish, Amenity, Location and O.\nSentence: my date and i would like some espresso thats within 10 min", "prompt_labels": "my(O) date(O) and(O) i(O) would(O) like(O) some(O) espresso(B-Dish) thats(O) within(B-Location) 10(I-Location) min(I-Location)"}}
{"id": "1420", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Price", "Amenity", "Rating", "Restaurant Name", "Location", "Hours", "Dish"], "instance": {"id": "1420", "words": ["where", "is", "the", "closest", "restaurant", "that", "serves", "chinese", "food"], "labels": ["O", "O", "O", "B-Location", "O", "O", "O", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Amenity, Rating, Restaurant Name, Location, Hours, Dish and O.\nSentence: where is the closest restaurant that serves chinese food", "prompt_labels": "where(O) is(O) the(O) closest(B-Location) restaurant(O) that(O) serves(O) chinese(B-Cuisine) food(O)"}}
{"id": "1176", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Rating", "Price", "Cuisine", "Hours", "Dish", "Location", "Restaurant Name"], "instance": {"id": "1176", "words": ["what", "restaurant", "are", "cheap", "and", "only", "vegan"], "labels": ["O", "O", "O", "B-Price", "O", "O", "B-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Price, Cuisine, Hours, Dish, Location, Restaurant Name and O.\nSentence: what restaurant are cheap and only vegan", "prompt_labels": "what(O) restaurant(O) are(O) cheap(B-Price) and(O) only(O) vegan(B-Cuisine)"}}
{"id": "423", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Rating", "Cuisine", "Price", "Amenity", "Hours", "Restaurant Name", "Location"], "instance": {"id": "423", "words": ["find", "me", "chicken", "places", "that", "accept", "discover", "card"], "labels": ["O", "O", "B-Dish", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Rating, Cuisine, Price, Amenity, Hours, Restaurant Name, Location and O.\nSentence: find me chicken places that accept discover card", "prompt_labels": "find(O) me(O) chicken(B-Dish) places(O) that(O) accept(B-Amenity) discover(I-Amenity) card(I-Amenity)"}}
{"id": "584", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Price", "Dish", "Restaurant Name", "Rating", "Hours", "Cuisine", "Location"], "instance": {"id": "584", "words": ["i", "need", "a", "quick", "bite", "to", "eat"], "labels": ["O", "O", "O", "B-Amenity", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Dish, Restaurant Name, Rating, Hours, Cuisine, Location and O.\nSentence: i need a quick bite to eat", "prompt_labels": "i(O) need(O) a(O) quick(B-Amenity) bite(O) to(O) eat(O)"}}
{"id": "711", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Price", "Location", "Restaurant Name", "Dish", "Hours", "Amenity", "Cuisine"], "instance": {"id": "711", "words": ["id", "like", "to", "find", "a", "diner", "that", "has", "grilled", "cheese", "and", "soup", "close", "to", "here", "could", "you", "suggest", "one"], "labels": ["O", "O", "O", "O", "O", "B-Cuisine", "O", "O", "B-Dish", "I-Dish", "I-Dish", "I-Dish", "B-Location", "I-Location", "I-Location", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Location, Restaurant Name, Dish, Hours, Amenity, Cuisine and O.\nSentence: id like to find a diner that has grilled cheese and soup close to here could you suggest one", "prompt_labels": "id(O) like(O) to(O) find(O) a(O) diner(B-Cuisine) that(O) has(O) grilled(B-Dish) cheese(I-Dish) and(I-Dish) soup(I-Dish) close(B-Location) to(I-Location) here(I-Location) could(O) you(O) suggest(O) one(O)"}}
{"id": "1355", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Amenity", "Hours", "Location", "Cuisine", "Restaurant Name", "Dish", "Rating"], "instance": {"id": "1355", "words": ["where", "can", "i", "take", "a", "date", "nearby", "for", "enchiladas"], "labels": ["O", "O", "O", "O", "O", "B-Amenity", "B-Location", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Hours, Location, Cuisine, Restaurant Name, Dish, Rating and O.\nSentence: where can i take a date nearby for enchiladas", "prompt_labels": "where(O) can(O) i(O) take(O) a(O) date(B-Amenity) nearby(B-Location) for(O) enchiladas(B-Dish)"}}
{"id": "1489", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Dish", "Hours", "Location", "Cuisine", "Price", "Rating", "Amenity"], "instance": {"id": "1489", "words": ["which", "restaurant", "can", "i", "get", "to", "within", "5", "minutes", "which", "serves", "healthy", "portions"], "labels": ["O", "O", "O", "O", "O", "O", "B-Location", "I-Location", "I-Location", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Hours, Location, Cuisine, Price, Rating, Amenity and O.\nSentence: which restaurant can i get to within 5 minutes which serves healthy portions", "prompt_labels": "which(O) restaurant(O) can(O) i(O) get(O) to(O) within(B-Location) 5(I-Location) minutes(I-Location) which(O) serves(O) healthy(B-Amenity) portions(I-Amenity)"}}
{"id": "667", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Rating", "Cuisine", "Amenity", "Hours", "Dish", "Price", "Location"], "instance": {"id": "667", "words": ["i", "want", "to", "get", "some", "chinese", "food"], "labels": ["O", "O", "O", "O", "O", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Cuisine, Amenity, Hours, Dish, Price, Location and O.\nSentence: i want to get some chinese food", "prompt_labels": "i(O) want(O) to(O) get(O) some(O) chinese(B-Cuisine) food(O)"}}
{"id": "186", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Amenity", "Location", "Dish", "Price", "Cuisine", "Restaurant Name", "Hours"], "instance": {"id": "186", "words": ["can", "you", "help", "me", "find", "a", "tong", "villa", "that", "serves", "small", "portions"], "labels": ["O", "O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Location, Dish, Price, Cuisine, Restaurant Name, Hours and O.\nSentence: can you help me find a tong villa that serves small portions", "prompt_labels": "can(O) you(O) help(O) me(O) find(O) a(O) tong(B-Restaurant Name) villa(I-Restaurant Name) that(O) serves(O) small(B-Amenity) portions(I-Amenity)"}}
{"id": "1101", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Hours", "Amenity", "Location", "Price", "Restaurant Name", "Dish", "Rating"], "instance": {"id": "1101", "words": ["what", "are", "the", "reviews", "on", "this", "italian", "restaurant"], "labels": ["O", "O", "O", "O", "O", "O", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Amenity, Location, Price, Restaurant Name, Dish, Rating and O.\nSentence: what are the reviews on this italian restaurant", "prompt_labels": "what(O) are(O) the(O) reviews(O) on(O) this(O) italian(B-Cuisine) restaurant(O)"}}
{"id": "995", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Hours", "Dish", "Location", "Amenity", "Cuisine", "Restaurant Name", "Price"], "instance": {"id": "995", "words": ["make", "a", "reservation", "tonight", "for", "four", "at", "billies", "steakhouse"], "labels": ["O", "O", "B-Amenity", "B-Hours", "B-Amenity", "I-Amenity", "O", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Dish, Location, Amenity, Cuisine, Restaurant Name, Price and O.\nSentence: make a reservation tonight for four at billies steakhouse", "prompt_labels": "make(O) a(O) reservation(B-Amenity) tonight(B-Hours) for(B-Amenity) four(I-Amenity) at(O) billies(B-Restaurant Name) steakhouse(I-Restaurant Name)"}}
{"id": "1375", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Hours", "Rating", "Cuisine", "Price", "Amenity", "Restaurant Name", "Dish"], "instance": {"id": "1375", "words": ["where", "is", "a", "good", "place", "to", "get", "seafood"], "labels": ["O", "O", "O", "B-Rating", "O", "O", "O", "B-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Hours, Rating, Cuisine, Price, Amenity, Restaurant Name, Dish and O.\nSentence: where is a good place to get seafood", "prompt_labels": "where(O) is(O) a(O) good(B-Rating) place(O) to(O) get(O) seafood(B-Cuisine)"}}
{"id": "1106", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Dish", "Hours", "Amenity", "Restaurant Name", "Rating", "Location", "Price"], "instance": {"id": "1106", "words": ["what", "do", "you", "know", "about", "restaurants", "that", "have", "impressive", "wine", "lists"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Dish, Hours, Amenity, Restaurant Name, Rating, Location, Price and O.\nSentence: what do you know about restaurants that have impressive wine lists", "prompt_labels": "what(O) do(O) you(O) know(O) about(O) restaurants(O) that(O) have(O) impressive(B-Amenity) wine(I-Amenity) lists(I-Amenity)"}}
{"id": "141", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Cuisine", "Location", "Amenity", "Rating", "Price", "Restaurant Name", "Hours"], "instance": {"id": "141", "words": ["can", "you", "find", "a", "restaurant", "that", "serves", "duck", "that", "not", "cheap", "near", "here"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Dish", "O", "B-Price", "I-Price", "B-Location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Location, Amenity, Rating, Price, Restaurant Name, Hours and O.\nSentence: can you find a restaurant that serves duck that not cheap near here", "prompt_labels": "can(O) you(O) find(O) a(O) restaurant(O) that(O) serves(O) duck(B-Dish) that(O) not(B-Price) cheap(I-Price) near(B-Location) here(O)"}}
{"id": "1028", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Amenity", "Price", "Location", "Dish", "Restaurant Name", "Rating", "Hours"], "instance": {"id": "1028", "words": ["please", "get", "me", "the", "street", "address", "of", "the", "dennys", "in", "this", "area"], "labels": ["O", "O", "O", "O", "B-Location", "I-Location", "O", "O", "B-Restaurant Name", "O", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Amenity, Price, Location, Dish, Restaurant Name, Rating, Hours and O.\nSentence: please get me the street address of the dennys in this area", "prompt_labels": "please(O) get(O) me(O) the(O) street(B-Location) address(I-Location) of(O) the(O) dennys(B-Restaurant Name) in(O) this(O) area(B-Location)"}}
{"id": "715", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Hours", "Cuisine", "Rating", "Amenity", "Price", "Dish", "Location"], "instance": {"id": "715", "words": ["id", "there", "a", "mina", "bakery", "in", "chinatown"], "labels": ["O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Cuisine, Rating, Amenity, Price, Dish, Location and O.\nSentence: id there a mina bakery in chinatown", "prompt_labels": "id(O) there(O) a(O) mina(B-Restaurant Name) bakery(I-Restaurant Name) in(B-Location) chinatown(I-Location)"}}
{"id": "557", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Restaurant Name", "Cuisine", "Price", "Dish", "Amenity", "Rating", "Hours"], "instance": {"id": "557", "words": ["i", "am", "looking", "for", "the", "best", "large", "buffet", "within", "15", "miles"], "labels": ["O", "O", "O", "O", "O", "B-Rating", "B-Amenity", "I-Amenity", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Restaurant Name, Cuisine, Price, Dish, Amenity, Rating, Hours and O.\nSentence: i am looking for the best large buffet within 15 miles", "prompt_labels": "i(O) am(O) looking(O) for(O) the(O) best(B-Rating) large(B-Amenity) buffet(I-Amenity) within(B-Location) 15(I-Location) miles(I-Location)"}}
{"id": "718", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Hours", "Cuisine", "Dish", "Price", "Restaurant Name", "Location", "Rating"], "instance": {"id": "718", "words": ["im", "feeling", "a", "little", "down", "so", "id", "like", "go", "somewhere", "thats", "really", "bright", "and", "fun", "for", "breakfast"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity", "O", "B-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Cuisine, Dish, Price, Restaurant Name, Location, Rating and O.\nSentence: im feeling a little down so id like go somewhere thats really bright and fun for breakfast", "prompt_labels": "im(O) feeling(O) a(O) little(O) down(O) so(O) id(O) like(O) go(O) somewhere(O) thats(O) really(O) bright(B-Amenity) and(I-Amenity) fun(I-Amenity) for(O) breakfast(B-Hours)"}}
{"id": "728", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Price", "Rating", "Amenity", "Location", "Dish", "Hours", "Cuisine"], "instance": {"id": "728", "words": ["im", "in", "the", "mood", "for", "mexican", "food"], "labels": ["O", "O", "O", "O", "O", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Rating, Amenity, Location, Dish, Hours, Cuisine and O.\nSentence: im in the mood for mexican food", "prompt_labels": "im(O) in(O) the(O) mood(O) for(O) mexican(B-Cuisine) food(O)"}}
{"id": "868", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Price", "Cuisine", "Amenity", "Restaurant Name", "Location", "Hours", "Rating"], "instance": {"id": "868", "words": ["is", "there", "a", "sbarro", "in", "the", "galleria", "mall"], "labels": ["O", "O", "O", "B-Restaurant Name", "O", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Cuisine, Amenity, Restaurant Name, Location, Hours, Rating and O.\nSentence: is there a sbarro in the galleria mall", "prompt_labels": "is(O) there(O) a(O) sbarro(B-Restaurant Name) in(O) the(O) galleria(B-Location) mall(I-Location)"}}
{"id": "1048", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Restaurant Name", "Location", "Hours", "Cuisine", "Price", "Dish", "Amenity"], "instance": {"id": "1048", "words": ["restuarnt", "that", "cathay", "owns"], "labels": ["O", "O", "B-Amenity", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Location, Hours, Cuisine, Price, Dish, Amenity and O.\nSentence: restuarnt that cathay owns", "prompt_labels": "restuarnt(O) that(O) cathay(B-Amenity) owns(O)"}}
{"id": "867", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Hours", "Amenity", "Location", "Dish", "Rating", "Price", "Cuisine"], "instance": {"id": "867", "words": ["is", "there", "a", "restaurant", "with", "a", "bar", "scene", "nearby", "that", "serves", "small", "portioned", "meals", "and", "snacks"], "labels": ["O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "B-Location", "O", "O", "B-Cuisine", "I-Cuisine", "I-Cuisine", "I-Cuisine", "I-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Amenity, Location, Dish, Rating, Price, Cuisine and O.\nSentence: is there a restaurant with a bar scene nearby that serves small portioned meals and snacks", "prompt_labels": "is(O) there(O) a(O) restaurant(O) with(O) a(O) bar(B-Amenity) scene(I-Amenity) nearby(B-Location) that(O) serves(O) small(B-Cuisine) portioned(I-Cuisine) meals(I-Cuisine) and(I-Cuisine) snacks(I-Cuisine)"}}
{"id": "1144", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Hours", "Amenity", "Cuisine", "Rating", "Restaurant Name", "Price", "Location"], "instance": {"id": "1144", "words": ["what", "is", "the", "phone", "number", "of", "a", "nearby", "restaurant", "that", "serves", "garlic", "shrimp"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Location", "O", "O", "O", "B-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Hours, Amenity, Cuisine, Rating, Restaurant Name, Price, Location and O.\nSentence: what is the phone number of a nearby restaurant that serves garlic shrimp", "prompt_labels": "what(O) is(O) the(O) phone(O) number(O) of(O) a(O) nearby(B-Location) restaurant(O) that(O) serves(O) garlic(B-Dish) shrimp(I-Dish)"}}
{"id": "412", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Location", "Rating", "Hours", "Cuisine", "Amenity", "Price", "Restaurant Name"], "instance": {"id": "412", "words": ["find", "me", "a", "tgi", "fridays", "near", "me"], "labels": ["O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Rating, Hours, Cuisine, Amenity, Price, Restaurant Name and O.\nSentence: find me a tgi fridays near me", "prompt_labels": "find(O) me(O) a(O) tgi(B-Restaurant Name) fridays(I-Restaurant Name) near(B-Location) me(I-Location)"}}
{"id": "45", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Amenity", "Hours", "Rating", "Price", "Restaurant Name", "Location", "Cuisine"], "instance": {"id": "45", "words": ["are", "there", "any", "greek", "restaurants", "in", "the", "area"], "labels": ["O", "O", "O", "B-Cuisine", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Hours, Rating, Price, Restaurant Name, Location, Cuisine and O.\nSentence: are there any greek restaurants in the area", "prompt_labels": "are(O) there(O) any(O) greek(B-Cuisine) restaurants(O) in(B-Location) the(I-Location) area(I-Location)"}}
{"id": "425", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Hours", "Restaurant Name", "Price", "Amenity", "Location", "Cuisine", "Rating"], "instance": {"id": "425", "words": ["find", "me", "fast", "food", "that", "serves", "salad"], "labels": ["O", "O", "B-Cuisine", "I-Cuisine", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Hours, Restaurant Name, Price, Amenity, Location, Cuisine, Rating and O.\nSentence: find me fast food that serves salad", "prompt_labels": "find(O) me(O) fast(B-Cuisine) food(I-Cuisine) that(O) serves(O) salad(B-Dish)"}}
{"id": "1343", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Dish", "Price", "Amenity", "Location", "Restaurant Name", "Cuisine", "Rating"], "instance": {"id": "1343", "words": ["where", "can", "i", "get", "some", "sushi"], "labels": ["O", "O", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Dish, Price, Amenity, Location, Restaurant Name, Cuisine, Rating and O.\nSentence: where can i get some sushi", "prompt_labels": "where(O) can(O) i(O) get(O) some(O) sushi(B-Dish)"}}
{"id": "1187", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Price", "Hours", "Rating", "Restaurant Name", "Cuisine", "Amenity", "Location"], "instance": {"id": "1187", "words": ["what", "restaurants", "are", "close", "to", "the", "museum"], "labels": ["O", "O", "O", "O", "O", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Hours, Rating, Restaurant Name, Cuisine, Amenity, Location and O.\nSentence: what restaurants are close to the museum", "prompt_labels": "what(O) restaurants(O) are(O) close(O) to(O) the(O) museum(B-Location)"}}
{"id": "224", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Dish", "Price", "Cuisine", "Hours", "Rating", "Location", "Restaurant Name"], "instance": {"id": "224", "words": ["create", "directions", "to", "closest", "chinese", "restaurtant"], "labels": ["O", "O", "O", "B-Location", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Price, Cuisine, Hours, Rating, Location, Restaurant Name and O.\nSentence: create directions to closest chinese restaurtant", "prompt_labels": "create(O) directions(O) to(O) closest(B-Location) chinese(B-Cuisine) restaurtant(O)"}}
{"id": "515", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Location", "Amenity", "Cuisine", "Restaurant Name", "Dish", "Price", "Hours"], "instance": {"id": "515", "words": ["how", "far", "is", "the", "arbys"], "labels": ["O", "O", "O", "O", "B-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Location, Amenity, Cuisine, Restaurant Name, Dish, Price, Hours and O.\nSentence: how far is the arbys", "prompt_labels": "how(O) far(O) is(O) the(O) arbys(B-Restaurant Name)"}}
{"id": "1110", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Location", "Dish", "Cuisine", "Price", "Rating", "Hours", "Restaurant Name"], "instance": {"id": "1110", "words": ["what", "is", "a", "family", "friendly", "restaurant", "in", "dorche", "that", "serves", "vietnamese", "food"], "labels": ["O", "O", "O", "B-Amenity", "I-Amenity", "O", "O", "B-Location", "O", "O", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Dish, Cuisine, Price, Rating, Hours, Restaurant Name and O.\nSentence: what is a family friendly restaurant in dorche that serves vietnamese food", "prompt_labels": "what(O) is(O) a(O) family(B-Amenity) friendly(I-Amenity) restaurant(O) in(O) dorche(B-Location) that(O) serves(O) vietnamese(B-Cuisine) food(O)"}}
{"id": "706", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Amenity", "Cuisine", "Rating", "Hours", "Dish", "Location", "Restaurant Name"], "instance": {"id": "706", "words": ["id", "like", "to", "eat", "in", "a", "reasonably", "priced", "restaurant", "that", "is", "not", "part", "of", "chain", "and", "that", "serves", "american", "food"], "labels": ["O", "O", "O", "O", "O", "O", "B-Price", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity", "I-Amenity", "O", "O", "O", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Cuisine, Rating, Hours, Dish, Location, Restaurant Name and O.\nSentence: id like to eat in a reasonably priced restaurant that is not part of chain and that serves american food", "prompt_labels": "id(O) like(O) to(O) eat(O) in(O) a(O) reasonably(B-Price) priced(O) restaurant(O) that(O) is(O) not(B-Amenity) part(I-Amenity) of(I-Amenity) chain(I-Amenity) and(O) that(O) serves(O) american(B-Cuisine) food(O)"}}
{"id": "881", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Hours", "Cuisine", "Dish", "Location", "Restaurant Name", "Price", "Rating"], "instance": {"id": "881", "words": ["is", "there", "a", "very", "high", "end", "pastry", "place", "close", "to", "me"], "labels": ["O", "O", "O", "B-Price", "I-Price", "I-Price", "B-Cuisine", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Cuisine, Dish, Location, Restaurant Name, Price, Rating and O.\nSentence: is there a very high end pastry place close to me", "prompt_labels": "is(O) there(O) a(O) very(B-Price) high(I-Price) end(I-Price) pastry(B-Cuisine) place(O) close(B-Location) to(I-Location) me(I-Location)"}}
{"id": "878", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Rating", "Price", "Hours", "Dish", "Amenity", "Location", "Restaurant Name"], "instance": {"id": "878", "words": ["is", "there", "a", "taqueria", "el", "rancho", "grandes", "around", "here", "for", "taking", "a", "date"], "labels": ["O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "B-Location", "I-Location", "O", "B-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Price, Hours, Dish, Amenity, Location, Restaurant Name and O.\nSentence: is there a taqueria el rancho grandes around here for taking a date", "prompt_labels": "is(O) there(O) a(O) taqueria(B-Restaurant Name) el(I-Restaurant Name) rancho(I-Restaurant Name) grandes(I-Restaurant Name) around(B-Location) here(I-Location) for(O) taking(B-Amenity) a(I-Amenity) date(I-Amenity)"}}
{"id": "1002", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Dish", "Location", "Price", "Rating", "Amenity", "Hours", "Restaurant Name"], "instance": {"id": "1002", "words": ["moderately", "priced", "seafood", "restaurant"], "labels": ["B-Price", "O", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Dish, Location, Price, Rating, Amenity, Hours, Restaurant Name and O.\nSentence: moderately priced seafood restaurant", "prompt_labels": "moderately(B-Price) priced(O) seafood(B-Cuisine) restaurant(O)"}}
{"id": "295", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Rating", "Dish", "Price", "Location", "Restaurant Name", "Hours", "Amenity"], "instance": {"id": "295", "words": ["does", "olive", "garden", "serve", "wine"], "labels": ["O", "B-Restaurant Name", "I-Restaurant Name", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Dish, Price, Location, Restaurant Name, Hours, Amenity and O.\nSentence: does olive garden serve wine", "prompt_labels": "does(O) olive(B-Restaurant Name) garden(I-Restaurant Name) serve(O) wine(B-Dish)"}}
{"id": "122", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Rating", "Amenity", "Dish", "Price", "Hours", "Cuisine", "Restaurant Name"], "instance": {"id": "122", "words": ["can", "i", "get", "a", "chefands", "table", "on", "north", "bedford", "street", "very", "late", "at", "night"], "labels": ["O", "O", "O", "O", "B-Amenity", "I-Amenity", "O", "B-Location", "I-Location", "I-Location", "B-Hours", "I-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Amenity, Dish, Price, Hours, Cuisine, Restaurant Name and O.\nSentence: can i get a chefands table on north bedford street very late at night", "prompt_labels": "can(O) i(O) get(O) a(O) chefands(B-Amenity) table(I-Amenity) on(O) north(B-Location) bedford(I-Location) street(I-Location) very(B-Hours) late(I-Hours) at(I-Hours) night(I-Hours)"}}
{"id": "780", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Rating", "Price", "Location", "Amenity", "Restaurant Name", "Hours", "Cuisine"], "instance": {"id": "780", "words": ["is", "the", "a", "chau", "restaurant", "within", "a", "mile", "from", "here", "a", "local", "favorite"], "labels": ["O", "O", "O", "B-Restaurant Name", "O", "B-Location", "I-Location", "I-Location", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Rating, Price, Location, Amenity, Restaurant Name, Hours, Cuisine and O.\nSentence: is the a chau restaurant within a mile from here a local favorite", "prompt_labels": "is(O) the(O) a(O) chau(B-Restaurant Name) restaurant(O) within(B-Location) a(I-Location) mile(I-Location) from(O) here(O) a(O) local(O) favorite(O)"}}
{"id": "1025", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Cuisine", "Dish", "Location", "Rating", "Hours", "Price", "Restaurant Name"], "instance": {"id": "1025", "words": ["please", "find", "me", "the", "nearest", "thai", "place"], "labels": ["O", "O", "O", "O", "B-Location", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Cuisine, Dish, Location, Rating, Hours, Price, Restaurant Name and O.\nSentence: please find me the nearest thai place", "prompt_labels": "please(O) find(O) me(O) the(O) nearest(B-Location) thai(B-Cuisine) place(O)"}}
{"id": "131", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Location", "Rating", "Price", "Restaurant Name", "Hours", "Amenity", "Dish"], "instance": {"id": "131", "words": ["can", "i", "wear", "shorts"], "labels": ["O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Rating, Price, Restaurant Name, Hours, Amenity, Dish and O.\nSentence: can i wear shorts", "prompt_labels": "can(O) i(O) wear(B-Amenity) shorts(I-Amenity)"}}
{"id": "1174", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Location", "Price", "Dish", "Hours", "Amenity", "Restaurant Name", "Cuisine"], "instance": {"id": "1174", "words": ["what", "places", "serve", "dinner", "late"], "labels": ["O", "O", "O", "B-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Location, Price, Dish, Hours, Amenity, Restaurant Name, Cuisine and O.\nSentence: what places serve dinner late", "prompt_labels": "what(O) places(O) serve(O) dinner(B-Hours) late(I-Hours)"}}
{"id": "1389", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Restaurant Name", "Location", "Rating", "Cuisine", "Amenity", "Dish", "Price"], "instance": {"id": "1389", "words": ["where", "is", "good", "ethnic", "food"], "labels": ["O", "O", "O", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Location, Rating, Cuisine, Amenity, Dish, Price and O.\nSentence: where is good ethnic food", "prompt_labels": "where(O) is(O) good(O) ethnic(B-Cuisine) food(O)"}}
{"id": "789", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Dish", "Restaurant Name", "Price", "Hours", "Cuisine", "Rating", "Location"], "instance": {"id": "789", "words": ["is", "the", "restaurant", "pushcart", "open", "until", "11", "am"], "labels": ["O", "O", "O", "B-Restaurant Name", "B-Hours", "O", "B-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Restaurant Name, Price, Hours, Cuisine, Rating, Location and O.\nSentence: is the restaurant pushcart open until 11 am", "prompt_labels": "is(O) the(O) restaurant(O) pushcart(B-Restaurant Name) open(B-Hours) until(O) 11(B-Hours) am(I-Hours)"}}
{"id": "1020", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Cuisine", "Dish", "Amenity", "Hours", "Price", "Location", "Restaurant Name"], "instance": {"id": "1020", "words": ["please", "find", "a", "taco", "place", "near", "my", "house"], "labels": ["O", "O", "O", "B-Cuisine", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Dish, Amenity, Hours, Price, Location, Restaurant Name and O.\nSentence: please find a taco place near my house", "prompt_labels": "please(O) find(O) a(O) taco(B-Cuisine) place(O) near(B-Location) my(I-Location) house(I-Location)"}}
{"id": "1014", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Price", "Hours", "Restaurant Name", "Amenity", "Dish", "Location", "Rating"], "instance": {"id": "1014", "words": ["nyc", "5", "star", "pizza", "parlors"], "labels": ["B-Location", "B-Rating", "I-Rating", "B-Dish", "B-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Hours, Restaurant Name, Amenity, Dish, Location, Rating and O.\nSentence: nyc 5 star pizza parlors", "prompt_labels": "nyc(B-Location) 5(B-Rating) star(I-Rating) pizza(B-Dish) parlors(B-Restaurant Name)"}}
{"id": "1517", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Amenity", "Location", "Hours", "Price", "Cuisine", "Restaurant Name", "Dish"], "instance": {"id": "1517", "words": ["yes", "please", "get", "me", "mcdonalds", "phone", "number", "in", "patchogue", "new", "york"], "labels": ["O", "O", "O", "O", "B-Restaurant Name", "O", "O", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Location, Hours, Price, Cuisine, Restaurant Name, Dish and O.\nSentence: yes please get me mcdonalds phone number in patchogue new york", "prompt_labels": "yes(O) please(O) get(O) me(O) mcdonalds(B-Restaurant Name) phone(O) number(O) in(O) patchogue(B-Location) new(I-Location) york(I-Location)"}}
{"id": "1229", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Hours", "Amenity", "Restaurant Name", "Cuisine", "Price", "Dish", "Rating"], "instance": {"id": "1229", "words": ["whats", "the", "best", "barbeque", "restaurant", "in", "memphis"], "labels": ["O", "O", "B-Rating", "B-Cuisine", "O", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Hours, Amenity, Restaurant Name, Cuisine, Price, Dish, Rating and O.\nSentence: whats the best barbeque restaurant in memphis", "prompt_labels": "whats(O) the(O) best(B-Rating) barbeque(B-Cuisine) restaurant(O) in(O) memphis(B-Location)"}}
{"id": "653", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Price", "Hours", "Amenity", "Restaurant Name", "Dish", "Rating", "Location"], "instance": {"id": "653", "words": ["i", "want", "to", "find", "a", "german", "restaurant", "on", "the", "lower", "east", "side", "that", "serves", "brunch", "on", "saturday", "and", "or", "sunday"], "labels": ["O", "O", "O", "O", "O", "B-Cuisine", "O", "O", "O", "B-Location", "I-Location", "I-Location", "O", "O", "B-Hours", "O", "B-Hours", "I-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Hours, Amenity, Restaurant Name, Dish, Rating, Location and O.\nSentence: i want to find a german restaurant on the lower east side that serves brunch on saturday and or sunday", "prompt_labels": "i(O) want(O) to(O) find(O) a(O) german(B-Cuisine) restaurant(O) on(O) the(O) lower(B-Location) east(I-Location) side(I-Location) that(O) serves(O) brunch(B-Hours) on(O) saturday(B-Hours) and(I-Hours) or(I-Hours) sunday(I-Hours)"}}
{"id": "1449", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Hours", "Rating", "Price", "Location", "Cuisine", "Restaurant Name", "Dish"], "instance": {"id": "1449", "words": ["where", "is", "the", "nearest", "tapas", "restaurant"], "labels": ["O", "O", "O", "B-Location", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Rating, Price, Location, Cuisine, Restaurant Name, Dish and O.\nSentence: where is the nearest tapas restaurant", "prompt_labels": "where(O) is(O) the(O) nearest(B-Location) tapas(B-Cuisine) restaurant(O)"}}
{"id": "259", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Restaurant Name", "Price", "Hours", "Dish", "Cuisine", "Amenity", "Location"], "instance": {"id": "259", "words": ["do", "you", "know", "if", "there", "are", "any", "fine", "dining", "cajun", "restaurant", "with", "a", "fireplace", "that", "is", "within", "one", "mile"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "B-Cuisine", "O", "O", "O", "B-Amenity", "O", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Price, Hours, Dish, Cuisine, Amenity, Location and O.\nSentence: do you know if there are any fine dining cajun restaurant with a fireplace that is within one mile", "prompt_labels": "do(O) you(O) know(O) if(O) there(O) are(O) any(O) fine(B-Amenity) dining(I-Amenity) cajun(B-Cuisine) restaurant(O) with(O) a(O) fireplace(B-Amenity) that(O) is(O) within(B-Location) one(I-Location) mile(I-Location)"}}
{"id": "418", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Amenity", "Location", "Hours", "Cuisine", "Dish", "Restaurant Name", "Price"], "instance": {"id": "418", "words": ["find", "me", "an", "ethiopian", "restaurant", "within", "5", "miles", "of", "here"], "labels": ["O", "O", "O", "B-Cuisine", "O", "B-Location", "I-Location", "I-Location", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Location, Hours, Cuisine, Dish, Restaurant Name, Price and O.\nSentence: find me an ethiopian restaurant within 5 miles of here", "prompt_labels": "find(O) me(O) an(O) ethiopian(B-Cuisine) restaurant(O) within(B-Location) 5(I-Location) miles(I-Location) of(O) here(O)"}}
{"id": "1009", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Cuisine", "Location", "Amenity", "Rating", "Hours", "Dish", "Restaurant Name"], "instance": {"id": "1009", "words": ["navigate", "me", "to", "a", "thai", "restaurant", "thats", "4", "stars", "or", "higher"], "labels": ["O", "O", "O", "O", "B-Cuisine", "O", "O", "B-Rating", "I-Rating", "I-Rating", "I-Rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Location, Amenity, Rating, Hours, Dish, Restaurant Name and O.\nSentence: navigate me to a thai restaurant thats 4 stars or higher", "prompt_labels": "navigate(O) me(O) to(O) a(O) thai(B-Cuisine) restaurant(O) thats(O) 4(B-Rating) stars(I-Rating) or(I-Rating) higher(I-Rating)"}}
{"id": "276", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Dish", "Location", "Price", "Hours", "Rating", "Cuisine", "Amenity"], "instance": {"id": "276", "words": ["does", "bellinis", "have", "any", "outdoor", "parking"], "labels": ["O", "B-Restaurant Name", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Location, Price, Hours, Rating, Cuisine, Amenity and O.\nSentence: does bellinis have any outdoor parking", "prompt_labels": "does(O) bellinis(B-Restaurant Name) have(O) any(O) outdoor(B-Amenity) parking(I-Amenity)"}}
{"id": "333", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Location", "Restaurant Name", "Amenity", "Price", "Hours", "Dish", "Cuisine"], "instance": {"id": "333", "words": ["find", "a", "carry", "out", "chinese", "restaurant"], "labels": ["O", "O", "B-Amenity", "I-Amenity", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Location, Restaurant Name, Amenity, Price, Hours, Dish, Cuisine and O.\nSentence: find a carry out chinese restaurant", "prompt_labels": "find(O) a(O) carry(B-Amenity) out(I-Amenity) chinese(B-Cuisine) restaurant(O)"}}
{"id": "659", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Location", "Price", "Dish", "Amenity", "Rating", "Restaurant Name", "Cuisine"], "instance": {"id": "659", "words": ["i", "want", "to", "find", "a", "place", "to", "eat", "that", "is", "very", "clean", "and", "has", "good", "service"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Price, Dish, Amenity, Rating, Restaurant Name, Cuisine and O.\nSentence: i want to find a place to eat that is very clean and has good service", "prompt_labels": "i(O) want(O) to(O) find(O) a(O) place(O) to(O) eat(O) that(O) is(O) very(B-Amenity) clean(I-Amenity) and(O) has(O) good(B-Amenity) service(I-Amenity)"}}
{"id": "1483", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Dish", "Rating", "Hours", "Price", "Cuisine", "Restaurant Name", "Amenity"], "instance": {"id": "1483", "words": ["which", "five", "star", "italian", "restaurants", "in", "manattan", "have", "the", "best", "reviews"], "labels": ["O", "B-Rating", "I-Rating", "B-Cuisine", "O", "O", "B-Location", "O", "O", "B-Rating", "I-Rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Dish, Rating, Hours, Price, Cuisine, Restaurant Name, Amenity and O.\nSentence: which five star italian restaurants in manattan have the best reviews", "prompt_labels": "which(O) five(B-Rating) star(I-Rating) italian(B-Cuisine) restaurants(O) in(O) manattan(B-Location) have(O) the(O) best(B-Rating) reviews(I-Rating)"}}
{"id": "782", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Cuisine", "Dish", "Location", "Hours", "Price", "Restaurant Name", "Amenity"], "instance": {"id": "782", "words": ["is", "the", "chateau", "restaurant", "affordable"], "labels": ["O", "O", "B-Restaurant Name", "I-Restaurant Name", "B-Price"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Dish, Location, Hours, Price, Restaurant Name, Amenity and O.\nSentence: is the chateau restaurant affordable", "prompt_labels": "is(O) the(O) chateau(B-Restaurant Name) restaurant(I-Restaurant Name) affordable(B-Price)"}}
{"id": "1156", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Rating", "Cuisine", "Hours", "Location", "Restaurant Name", "Dish", "Price"], "instance": {"id": "1156", "words": ["what", "kind", "of", "chicken", "dishes", "does", "theos", "serve"], "labels": ["O", "O", "O", "O", "O", "O", "B-Restaurant Name", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Cuisine, Hours, Location, Restaurant Name, Dish, Price and O.\nSentence: what kind of chicken dishes does theos serve", "prompt_labels": "what(O) kind(O) of(O) chicken(O) dishes(O) does(O) theos(B-Restaurant Name) serve(O)"}}
{"id": "1352", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Hours", "Location", "Rating", "Price", "Cuisine", "Amenity", "Dish"], "instance": {"id": "1352", "words": ["where", "can", "i", "go", "to", "get", "a", "sandwich", "around", "here"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Dish", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Location, Rating, Price, Cuisine, Amenity, Dish and O.\nSentence: where can i go to get a sandwich around here", "prompt_labels": "where(O) can(O) i(O) go(O) to(O) get(O) a(O) sandwich(B-Dish) around(B-Location) here(I-Location)"}}
{"id": "873", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Rating", "Dish", "Amenity", "Cuisine", "Location", "Price", "Hours"], "instance": {"id": "873", "words": ["is", "there", "a", "sports", "bar", "within", "a", "mile", "of", "the", "minneapolis", "convention", "center"], "labels": ["O", "O", "O", "B-Cuisine", "I-Cuisine", "B-Location", "I-Location", "I-Location", "I-Location", "I-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Dish, Amenity, Cuisine, Location, Price, Hours and O.\nSentence: is there a sports bar within a mile of the minneapolis convention center", "prompt_labels": "is(O) there(O) a(O) sports(B-Cuisine) bar(I-Cuisine) within(B-Location) a(I-Location) mile(I-Location) of(I-Location) the(I-Location) minneapolis(I-Location) convention(I-Location) center(I-Location)"}}
{"id": "1414", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Cuisine", "Hours", "Amenity", "Price", "Location", "Rating", "Dish"], "instance": {"id": "1414", "words": ["where", "is", "the", "closest", "jimmie", "johns"], "labels": ["O", "O", "O", "B-Location", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Hours, Amenity, Price, Location, Rating, Dish and O.\nSentence: where is the closest jimmie johns", "prompt_labels": "where(O) is(O) the(O) closest(B-Location) jimmie(B-Restaurant Name) johns(I-Restaurant Name)"}}
{"id": "934", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Rating", "Restaurant Name", "Dish", "Price", "Location", "Amenity", "Cuisine"], "instance": {"id": "934", "words": ["jeez", "the", "traffic", "is", "horrible", "today", "it", "looks", "like", "we", "need", "something", "quick", "off", "to", "donalds"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Rating, Restaurant Name, Dish, Price, Location, Amenity, Cuisine and O.\nSentence: jeez the traffic is horrible today it looks like we need something quick off to donalds", "prompt_labels": "jeez(O) the(O) traffic(O) is(O) horrible(O) today(O) it(O) looks(O) like(O) we(O) need(O) something(O) quick(O) off(O) to(O) donalds(B-Restaurant Name)"}}
{"id": "416", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Dish", "Rating", "Price", "Cuisine", "Hours", "Amenity", "Location"], "instance": {"id": "416", "words": ["find", "me", "all", "the", "local", "italian", "joints"], "labels": ["O", "O", "O", "O", "B-Location", "B-Cuisine", "I-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Rating, Price, Cuisine, Hours, Amenity, Location and O.\nSentence: find me all the local italian joints", "prompt_labels": "find(O) me(O) all(O) the(O) local(B-Location) italian(B-Cuisine) joints(I-Cuisine)"}}
{"id": "1146", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Location", "Hours", "Amenity", "Rating", "Cuisine", "Dish", "Restaurant Name"], "instance": {"id": "1146", "words": ["what", "is", "the", "phone", "number", "of", "the", "mcdonalds", "on", "the", "east", "side", "of", "town"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Restaurant Name", "O", "O", "B-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Hours, Amenity, Rating, Cuisine, Dish, Restaurant Name and O.\nSentence: what is the phone number of the mcdonalds on the east side of town", "prompt_labels": "what(O) is(O) the(O) phone(O) number(O) of(O) the(O) mcdonalds(B-Restaurant Name) on(O) the(O) east(B-Location) side(I-Location) of(I-Location) town(I-Location)"}}
{"id": "1203", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Restaurant Name", "Dish", "Cuisine", "Amenity", "Price", "Hours", "Location"], "instance": {"id": "1203", "words": ["what", "time", "does", "kareems", "restaurant", "open"], "labels": ["O", "O", "O", "B-Restaurant Name", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Dish, Cuisine, Amenity, Price, Hours, Location and O.\nSentence: what time does kareems restaurant open", "prompt_labels": "what(O) time(O) does(O) kareems(B-Restaurant Name) restaurant(O) open(O)"}}
{"id": "1361", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Cuisine", "Price", "Location", "Amenity", "Rating", "Hours", "Dish"], "instance": {"id": "1361", "words": ["where", "can", "i", "take", "my", "kids", "to", "get", "an", "omelet"], "labels": ["O", "O", "O", "O", "O", "B-Amenity", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Price, Location, Amenity, Rating, Hours, Dish and O.\nSentence: where can i take my kids to get an omelet", "prompt_labels": "where(O) can(O) i(O) take(O) my(O) kids(B-Amenity) to(O) get(O) an(O) omelet(B-Dish)"}}
{"id": "441", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Cuisine", "Location", "Amenity", "Rating", "Dish", "Price", "Restaurant Name"], "instance": {"id": "441", "words": ["find", "nearby", "restaurants", "with", "coupons"], "labels": ["O", "B-Location", "O", "O", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Location, Amenity, Rating, Dish, Price, Restaurant Name and O.\nSentence: find nearby restaurants with coupons", "prompt_labels": "find(O) nearby(B-Location) restaurants(O) with(O) coupons(B-Amenity)"}}
{"id": "1232", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Dish", "Price", "Hours", "Location", "Cuisine", "Restaurant Name", "Amenity"], "instance": {"id": "1232", "words": ["whats", "the", "best", "place", "around", "here", "to", "get", "cheese"], "labels": ["O", "O", "B-Rating", "O", "O", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Price, Hours, Location, Cuisine, Restaurant Name, Amenity and O.\nSentence: whats the best place around here to get cheese", "prompt_labels": "whats(O) the(O) best(B-Rating) place(O) around(O) here(O) to(O) get(O) cheese(B-Dish)"}}
{"id": "60", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Amenity", "Hours", "Location", "Cuisine", "Restaurant Name", "Dish", "Rating"], "instance": {"id": "60", "words": ["are", "there", "any", "nationally", "known", "chefs", "with", "restaurants", "in", "this", "city"], "labels": ["O", "O", "O", "B-Rating", "I-Rating", "O", "O", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Hours, Location, Cuisine, Restaurant Name, Dish, Rating and O.\nSentence: are there any nationally known chefs with restaurants in this city", "prompt_labels": "are(O) there(O) any(O) nationally(B-Rating) known(I-Rating) chefs(O) with(O) restaurants(O) in(B-Location) this(I-Location) city(I-Location)"}}
{"id": "1220", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Location", "Price", "Hours", "Dish", "Cuisine", "Amenity", "Rating"], "instance": {"id": "1220", "words": ["what", "vegetarian", "options", "does", "zaxbys", "offer"], "labels": ["O", "O", "O", "O", "B-Restaurant Name", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Price, Hours, Dish, Cuisine, Amenity, Rating and O.\nSentence: what vegetarian options does zaxbys offer", "prompt_labels": "what(O) vegetarian(O) options(O) does(O) zaxbys(B-Restaurant Name) offer(O)"}}
{"id": "893", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Price", "Dish", "Hours", "Restaurant Name", "Amenity", "Rating", "Location"], "instance": {"id": "893", "words": ["is", "there", "an", "indian", "restaurant", "within", "5", "miles", "that", "has", "a", "dinner", "buffet"], "labels": ["O", "O", "O", "B-Cuisine", "O", "B-Location", "I-Location", "I-Location", "O", "O", "O", "B-Hours", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Dish, Hours, Restaurant Name, Amenity, Rating, Location and O.\nSentence: is there an indian restaurant within 5 miles that has a dinner buffet", "prompt_labels": "is(O) there(O) an(O) indian(B-Cuisine) restaurant(O) within(B-Location) 5(I-Location) miles(I-Location) that(O) has(O) a(O) dinner(B-Hours) buffet(B-Amenity)"}}
{"id": "974", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Location", "Hours", "Dish", "Restaurant Name", "Amenity", "Price", "Cuisine"], "instance": {"id": "974", "words": ["looking", "for", "crowd", "pleasing", "fatz", "along", "the", "way"], "labels": ["O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Location, Hours, Dish, Restaurant Name, Amenity, Price, Cuisine and O.\nSentence: looking for crowd pleasing fatz along the way", "prompt_labels": "looking(O) for(O) crowd(B-Restaurant Name) pleasing(I-Restaurant Name) fatz(I-Restaurant Name) along(B-Location) the(I-Location) way(I-Location)"}}
{"id": "375", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Location", "Restaurant Name", "Amenity", "Dish", "Cuisine", "Price", "Hours"], "instance": {"id": "375", "words": ["find", "me", "a", "good", "pub", "that", "has", "a", "dance", "floor"], "labels": ["O", "O", "O", "B-Rating", "B-Cuisine", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Location, Restaurant Name, Amenity, Dish, Cuisine, Price, Hours and O.\nSentence: find me a good pub that has a dance floor", "prompt_labels": "find(O) me(O) a(O) good(B-Rating) pub(B-Cuisine) that(O) has(O) a(O) dance(B-Amenity) floor(I-Amenity)"}}
{"id": "1215", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Amenity", "Price", "Dish", "Location", "Rating", "Hours", "Cuisine"], "instance": {"id": "1215", "words": ["what", "time", "does", "the", "pho", "place", "in", "reno", "close"], "labels": ["O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "O", "B-Location", "B-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Price, Dish, Location, Rating, Hours, Cuisine and O.\nSentence: what time does the pho place in reno close", "prompt_labels": "what(O) time(O) does(O) the(O) pho(B-Restaurant Name) place(I-Restaurant Name) in(O) reno(B-Location) close(B-Hours)"}}
{"id": "320", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Restaurant Name", "Hours", "Location", "Cuisine", "Price", "Dish", "Rating"], "instance": {"id": "320", "words": ["does", "the", "nearest", "asian", "restaurant", "have", "a", "kids", "menu"], "labels": ["O", "O", "B-Location", "B-Cuisine", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Restaurant Name, Hours, Location, Cuisine, Price, Dish, Rating and O.\nSentence: does the nearest asian restaurant have a kids menu", "prompt_labels": "does(O) the(O) nearest(B-Location) asian(B-Cuisine) restaurant(O) have(O) a(O) kids(B-Amenity) menu(I-Amenity)"}}
{"id": "884", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Rating", "Restaurant Name", "Dish", "Price", "Hours", "Cuisine", "Location"], "instance": {"id": "884", "words": ["is", "there", "a", "white", "castle", "on", "berkeley", "avenue"], "labels": ["O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Restaurant Name, Dish, Price, Hours, Cuisine, Location and O.\nSentence: is there a white castle on berkeley avenue", "prompt_labels": "is(O) there(O) a(O) white(B-Restaurant Name) castle(I-Restaurant Name) on(O) berkeley(B-Location) avenue(I-Location)"}}
{"id": "542", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Rating", "Dish", "Cuisine", "Amenity", "Location", "Hours", "Price"], "instance": {"id": "542", "words": ["i", "am", "hungry", "please", "find", "nearby", "restaurants"], "labels": ["O", "O", "O", "O", "O", "B-Location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Dish, Cuisine, Amenity, Location, Hours, Price and O.\nSentence: i am hungry please find nearby restaurants", "prompt_labels": "i(O) am(O) hungry(O) please(O) find(O) nearby(B-Location) restaurants(O)"}}
{"id": "1119", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Amenity", "Cuisine", "Location", "Hours", "Price", "Rating", "Dish"], "instance": {"id": "1119", "words": ["what", "is", "the", "best", "ice", "cream", "parlor"], "labels": ["O", "O", "O", "B-Rating", "B-Cuisine", "I-Cuisine", "I-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Cuisine, Location, Hours, Price, Rating, Dish and O.\nSentence: what is the best ice cream parlor", "prompt_labels": "what(O) is(O) the(O) best(B-Rating) ice(B-Cuisine) cream(I-Cuisine) parlor(I-Cuisine)"}}
{"id": "657", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Rating", "Restaurant Name", "Location", "Dish", "Price", "Cuisine", "Hours"], "instance": {"id": "657", "words": ["i", "want", "to", "find", "a", "place", "that", "serves", "beef", "patties"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Restaurant Name, Location, Dish, Price, Cuisine, Hours and O.\nSentence: i want to find a place that serves beef patties", "prompt_labels": "i(O) want(O) to(O) find(O) a(O) place(O) that(O) serves(O) beef(B-Dish) patties(I-Dish)"}}
{"id": "201", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Price", "Cuisine", "Amenity", "Restaurant Name", "Location", "Rating", "Dish"], "instance": {"id": "201", "words": ["can", "you", "please", "direct", "me", "to", "the", "nearest", "chinese", "restaurant"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Location", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Cuisine, Amenity, Restaurant Name, Location, Rating, Dish and O.\nSentence: can you please direct me to the nearest chinese restaurant", "prompt_labels": "can(O) you(O) please(O) direct(O) me(O) to(O) the(O) nearest(B-Location) chinese(B-Cuisine) restaurant(O)"}}
{"id": "713", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Rating", "Price", "Dish", "Cuisine", "Restaurant Name", "Location", "Hours"], "instance": {"id": "713", "words": ["id", "like", "to", "know", "the", "closest", "starbucks", "open", "past", "9", "pm"], "labels": ["O", "O", "O", "O", "O", "B-Location", "B-Restaurant Name", "O", "B-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Price, Dish, Cuisine, Restaurant Name, Location, Hours and O.\nSentence: id like to know the closest starbucks open past 9 pm", "prompt_labels": "id(O) like(O) to(O) know(O) the(O) closest(B-Location) starbucks(B-Restaurant Name) open(O) past(B-Hours) 9(I-Hours) pm(I-Hours)"}}
{"id": "1261", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Location", "Price", "Amenity", "Restaurant Name", "Cuisine", "Rating", "Hours"], "instance": {"id": "1261", "words": ["where", "can", "i", "eat", "by", "the", "water", "close", "by"], "labels": ["O", "O", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Price, Amenity, Restaurant Name, Cuisine, Rating, Hours and O.\nSentence: where can i eat by the water close by", "prompt_labels": "where(O) can(O) i(O) eat(O) by(B-Amenity) the(I-Amenity) water(I-Amenity) close(B-Location) by(I-Location)"}}
{"id": "1444", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Amenity", "Restaurant Name", "Hours", "Price", "Rating", "Location", "Dish"], "instance": {"id": "1444", "words": ["where", "is", "the", "nearest", "restaurant", "that", "serves", "steak"], "labels": ["O", "O", "O", "B-Location", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Amenity, Restaurant Name, Hours, Price, Rating, Location, Dish and O.\nSentence: where is the nearest restaurant that serves steak", "prompt_labels": "where(O) is(O) the(O) nearest(B-Location) restaurant(O) that(O) serves(O) steak(B-Dish)"}}
{"id": "161", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Dish", "Amenity", "Location", "Rating", "Hours", "Cuisine", "Price"], "instance": {"id": "161", "words": ["can", "you", "find", "me", "a", "pizzeria", "that", "delivers", "after", "midnight"], "labels": ["O", "O", "O", "O", "O", "B-Cuisine", "O", "B-Amenity", "B-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Amenity, Location, Rating, Hours, Cuisine, Price and O.\nSentence: can you find me a pizzeria that delivers after midnight", "prompt_labels": "can(O) you(O) find(O) me(O) a(O) pizzeria(B-Cuisine) that(O) delivers(B-Amenity) after(B-Hours) midnight(I-Hours)"}}
{"id": "384", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Cuisine", "Dish", "Price", "Amenity", "Location", "Hours", "Restaurant Name"], "instance": {"id": "384", "words": ["find", "me", "a", "place", "that", "sells", "burgers", "closest", "to", "me"], "labels": ["O", "O", "O", "O", "O", "O", "B-Dish", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Dish, Price, Amenity, Location, Hours, Restaurant Name and O.\nSentence: find me a place that sells burgers closest to me", "prompt_labels": "find(O) me(O) a(O) place(O) that(O) sells(O) burgers(B-Dish) closest(B-Location) to(I-Location) me(I-Location)"}}
{"id": "1457", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Hours", "Price", "Cuisine", "Dish", "Restaurant Name", "Location", "Rating"], "instance": {"id": "1457", "words": ["where", "is", "there", "a", "fatz", "with", "interesting", "people", "around"], "labels": ["O", "O", "O", "O", "B-Restaurant Name", "O", "B-Amenity", "I-Amenity", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Price, Cuisine, Dish, Restaurant Name, Location, Rating and O.\nSentence: where is there a fatz with interesting people around", "prompt_labels": "where(O) is(O) there(O) a(O) fatz(B-Restaurant Name) with(O) interesting(B-Amenity) people(I-Amenity) around(O)"}}
{"id": "452", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Price", "Location", "Rating", "Cuisine", "Hours", "Amenity", "Restaurant Name"], "instance": {"id": "452", "words": ["find", "the", "closest", "sea", "food", "restaurant"], "labels": ["O", "O", "B-Location", "B-Cuisine", "I-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Location, Rating, Cuisine, Hours, Amenity, Restaurant Name and O.\nSentence: find the closest sea food restaurant", "prompt_labels": "find(O) the(O) closest(B-Location) sea(B-Cuisine) food(I-Cuisine) restaurant(O)"}}
{"id": "47", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Cuisine", "Dish", "Price", "Location", "Restaurant Name", "Rating", "Amenity"], "instance": {"id": "47", "words": ["are", "there", "any", "hamburger", "restaurants", "close", "by"], "labels": ["O", "O", "O", "B-Cuisine", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Dish, Price, Location, Restaurant Name, Rating, Amenity and O.\nSentence: are there any hamburger restaurants close by", "prompt_labels": "are(O) there(O) any(O) hamburger(B-Cuisine) restaurants(O) close(B-Location) by(I-Location)"}}
{"id": "124", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Price", "Hours", "Dish", "Location", "Rating", "Cuisine", "Restaurant Name"], "instance": {"id": "124", "words": ["can", "i", "get", "gluten", "free", "pizza", "within", "10", "miles", "of", "here"], "labels": ["O", "O", "O", "B-Dish", "I-Dish", "I-Dish", "B-Location", "I-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Hours, Dish, Location, Rating, Cuisine, Restaurant Name and O.\nSentence: can i get gluten free pizza within 10 miles of here", "prompt_labels": "can(O) i(O) get(O) gluten(B-Dish) free(I-Dish) pizza(I-Dish) within(B-Location) 10(I-Location) miles(I-Location) of(I-Location) here(I-Location)"}}
{"id": "899", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Location", "Cuisine", "Amenity", "Price", "Restaurant Name", "Hours", "Dish"], "instance": {"id": "899", "words": ["is", "there", "an", "s", "and", "i", "to", "go", "nearby", "that", "has", "small", "portions"], "labels": ["O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "B-Location", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Location, Cuisine, Amenity, Price, Restaurant Name, Hours, Dish and O.\nSentence: is there an s and i to go nearby that has small portions", "prompt_labels": "is(O) there(O) an(O) s(B-Restaurant Name) and(I-Restaurant Name) i(I-Restaurant Name) to(I-Restaurant Name) go(I-Restaurant Name) nearby(B-Location) that(O) has(O) small(O) portions(O)"}}
{"id": "1491", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Restaurant Name", "Amenity", "Dish", "Rating", "Price", "Cuisine", "Location"], "instance": {"id": "1491", "words": ["which", "restaurant", "has", "a", "smoking", "section"], "labels": ["O", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Amenity, Dish, Rating, Price, Cuisine, Location and O.\nSentence: which restaurant has a smoking section", "prompt_labels": "which(O) restaurant(O) has(O) a(O) smoking(B-Amenity) section(I-Amenity)"}}
{"id": "379", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Dish", "Location", "Amenity", "Restaurant Name", "Hours", "Rating", "Cuisine"], "instance": {"id": "379", "words": ["find", "me", "a", "local", "outdoors", "shop", "that", "sells", "fire", "wood"], "labels": ["O", "O", "O", "B-Location", "B-Amenity", "I-Amenity", "O", "B-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Location, Amenity, Restaurant Name, Hours, Rating, Cuisine and O.\nSentence: find me a local outdoors shop that sells fire wood", "prompt_labels": "find(O) me(O) a(O) local(B-Location) outdoors(B-Amenity) shop(I-Amenity) that(O) sells(B-Amenity) fire(I-Amenity) wood(I-Amenity)"}}
{"id": "484", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Price", "Restaurant Name", "Location", "Cuisine", "Rating", "Hours", "Amenity"], "instance": {"id": "484", "words": ["help", "me", "find", "jack", "in", "the", "box", "in", "beverly", "hills"], "labels": ["O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Restaurant Name, Location, Cuisine, Rating, Hours, Amenity and O.\nSentence: help me find jack in the box in beverly hills", "prompt_labels": "help(O) me(O) find(O) jack(B-Restaurant Name) in(I-Restaurant Name) the(I-Restaurant Name) box(I-Restaurant Name) in(O) beverly(B-Location) hills(I-Location)"}}
{"id": "629", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Amenity", "Restaurant Name", "Rating", "Cuisine", "Location", "Hours", "Dish"], "instance": {"id": "629", "words": ["i", "want", "a", "restaurant", "where", "i", "can", "order", "some", "carry", "out", "potato", "soup"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "B-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Restaurant Name, Rating, Cuisine, Location, Hours, Dish and O.\nSentence: i want a restaurant where i can order some carry out potato soup", "prompt_labels": "i(O) want(O) a(O) restaurant(O) where(O) i(O) can(O) order(O) some(O) carry(B-Amenity) out(I-Amenity) potato(B-Dish) soup(I-Dish)"}}
{"id": "1124", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Cuisine", "Dish", "Price", "Hours", "Restaurant Name", "Rating", "Amenity"], "instance": {"id": "1124", "words": ["what", "is", "the", "closest", "bar", "near", "heidis", "restaurant"], "labels": ["O", "O", "O", "B-Location", "O", "O", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Dish, Price, Hours, Restaurant Name, Rating, Amenity and O.\nSentence: what is the closest bar near heidis restaurant", "prompt_labels": "what(O) is(O) the(O) closest(B-Location) bar(O) near(O) heidis(B-Restaurant Name) restaurant(I-Restaurant Name)"}}
{"id": "282", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Restaurant Name", "Amenity", "Rating", "Cuisine", "Location", "Dish", "Hours"], "instance": {"id": "282", "words": ["does", "firstwatch", "breakfast", "restuarant", "have", "outdoor", "seating"], "labels": ["O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Restaurant Name, Amenity, Rating, Cuisine, Location, Dish, Hours and O.\nSentence: does firstwatch breakfast restuarant have outdoor seating", "prompt_labels": "does(O) firstwatch(B-Restaurant Name) breakfast(I-Restaurant Name) restuarant(I-Restaurant Name) have(O) outdoor(B-Amenity) seating(I-Amenity)"}}
{"id": "640", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Price", "Dish", "Restaurant Name", "Hours", "Rating", "Amenity", "Location"], "instance": {"id": "640", "words": ["i", "want", "something", "to", "eat", "close", "by"], "labels": ["O", "O", "O", "O", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Dish, Restaurant Name, Hours, Rating, Amenity, Location and O.\nSentence: i want something to eat close by", "prompt_labels": "i(O) want(O) something(O) to(O) eat(O) close(B-Location) by(I-Location)"}}
{"id": "386", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Amenity", "Hours", "Restaurant Name", "Price", "Cuisine", "Rating", "Dish"], "instance": {"id": "386", "words": ["find", "me", "a", "place", "to", "eat", "near", "by"], "labels": ["O", "O", "O", "O", "O", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Hours, Restaurant Name, Price, Cuisine, Rating, Dish and O.\nSentence: find me a place to eat near by", "prompt_labels": "find(O) me(O) a(O) place(O) to(O) eat(O) near(B-Location) by(I-Location)"}}
{"id": "49", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Cuisine", "Rating", "Price", "Amenity", "Restaurant Name", "Location", "Dish"], "instance": {"id": "49", "words": ["are", "there", "any", "ice", "cream", "shops", "in", "my", "neighborhood", "that", "are", "open", "right", "now"], "labels": ["O", "O", "O", "B-Cuisine", "I-Cuisine", "O", "B-Location", "I-Location", "I-Location", "O", "O", "B-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Rating, Price, Amenity, Restaurant Name, Location, Dish and O.\nSentence: are there any ice cream shops in my neighborhood that are open right now", "prompt_labels": "are(O) there(O) any(O) ice(B-Cuisine) cream(I-Cuisine) shops(O) in(B-Location) my(I-Location) neighborhood(I-Location) that(O) are(O) open(B-Hours) right(I-Hours) now(I-Hours)"}}
{"id": "171", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Hours", "Dish", "Price", "Location", "Rating", "Amenity", "Cuisine"], "instance": {"id": "171", "words": ["can", "you", "find", "out", "if", "the", "best", "little", "restaurant", "has", "dancing", "and", "a", "good", "looking", "atmosphere"], "labels": ["O", "O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "O", "B-Amenity", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Dish, Price, Location, Rating, Amenity, Cuisine and O.\nSentence: can you find out if the best little restaurant has dancing and a good looking atmosphere", "prompt_labels": "can(O) you(O) find(O) out(O) if(O) the(O) best(B-Restaurant Name) little(I-Restaurant Name) restaurant(I-Restaurant Name) has(O) dancing(B-Amenity) and(O) a(O) good(B-Amenity) looking(I-Amenity) atmosphere(I-Amenity)"}}
{"id": "679", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Cuisine", "Restaurant Name", "Amenity", "Dish", "Rating", "Location", "Hours"], "instance": {"id": "679", "words": ["i", "want", "to", "reserve", "the", "back", "room", "at", "bensons", "grill", "for", "my", "daughters", "birthday", "party", "next", "friday", "night", "is", "it", "available"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "O", "O", "O", "O", "O", "O", "B-Hours", "I-Hours", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Restaurant Name, Amenity, Dish, Rating, Location, Hours and O.\nSentence: i want to reserve the back room at bensons grill for my daughters birthday party next friday night is it available", "prompt_labels": "i(O) want(O) to(O) reserve(O) the(O) back(O) room(O) at(O) bensons(B-Restaurant Name) grill(I-Restaurant Name) for(O) my(O) daughters(O) birthday(O) party(O) next(O) friday(B-Hours) night(I-Hours) is(O) it(O) available(O)"}}
{"id": "1439", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Amenity", "Rating", "Price", "Cuisine", "Location", "Hours", "Dish"], "instance": {"id": "1439", "words": ["where", "is", "the", "nearest", "place", "that", "serves", "nachos"], "labels": ["O", "O", "O", "B-Location", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Rating, Price, Cuisine, Location, Hours, Dish and O.\nSentence: where is the nearest place that serves nachos", "prompt_labels": "where(O) is(O) the(O) nearest(B-Location) place(O) that(O) serves(O) nachos(B-Dish)"}}
{"id": "637", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Restaurant Name", "Dish", "Location", "Amenity", "Cuisine", "Hours", "Price"], "instance": {"id": "637", "words": ["i", "want", "some", "chips", "and", "salsa"], "labels": ["O", "O", "O", "B-Dish", "I-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Dish, Location, Amenity, Cuisine, Hours, Price and O.\nSentence: i want some chips and salsa", "prompt_labels": "i(O) want(O) some(O) chips(B-Dish) and(I-Dish) salsa(I-Dish)"}}
{"id": "547", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Hours", "Dish", "Rating", "Restaurant Name", "Location", "Price", "Amenity"], "instance": {"id": "547", "words": ["i", "am", "looking", "for", "a", "local", "pizza", "restaurant", "that", "delivers"], "labels": ["O", "O", "O", "O", "O", "B-Location", "B-Dish", "O", "O", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Dish, Rating, Restaurant Name, Location, Price, Amenity and O.\nSentence: i am looking for a local pizza restaurant that delivers", "prompt_labels": "i(O) am(O) looking(O) for(O) a(O) local(B-Location) pizza(B-Dish) restaurant(O) that(O) delivers(B-Amenity)"}}
{"id": "1219", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Hours", "Dish", "Cuisine", "Location", "Restaurant Name", "Price", "Rating"], "instance": {"id": "1219", "words": ["what", "type", "of", "cusine", "does", "gates", "have"], "labels": ["O", "O", "O", "O", "O", "B-Restaurant Name", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Dish, Cuisine, Location, Restaurant Name, Price, Rating and O.\nSentence: what type of cusine does gates have", "prompt_labels": "what(O) type(O) of(O) cusine(O) does(O) gates(B-Restaurant Name) have(O)"}}
{"id": "1304", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Rating", "Hours", "Location", "Price", "Amenity", "Restaurant Name", "Dish"], "instance": {"id": "1304", "words": ["where", "can", "i", "find", "tofu", "in", "city", "hall", "plaza", "with", "parking"], "labels": ["O", "O", "O", "O", "B-Dish", "O", "B-Location", "I-Location", "I-Location", "O", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Hours, Location, Price, Amenity, Restaurant Name, Dish and O.\nSentence: where can i find tofu in city hall plaza with parking", "prompt_labels": "where(O) can(O) i(O) find(O) tofu(B-Dish) in(O) city(B-Location) hall(I-Location) plaza(I-Location) with(O) parking(B-Amenity)"}}
{"id": "592", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Rating", "Price", "Dish", "Cuisine", "Restaurant Name", "Hours", "Amenity"], "instance": {"id": "592", "words": ["i", "need", "an", "italian", "restaurant", "with", "a", "kids", "menu"], "labels": ["O", "O", "O", "B-Cuisine", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Price, Dish, Cuisine, Restaurant Name, Hours, Amenity and O.\nSentence: i need an italian restaurant with a kids menu", "prompt_labels": "i(O) need(O) an(O) italian(B-Cuisine) restaurant(O) with(O) a(O) kids(B-Amenity) menu(I-Amenity)"}}
{"id": "18", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Cuisine", "Location", "Hours", "Dish", "Rating", "Price", "Restaurant Name"], "instance": {"id": "18", "words": ["are", "reservations", "available", "for", "four", "people", "for", "8", "pm", "tonight", "at", "112", "eatery"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Cuisine, Location, Hours, Dish, Rating, Price, Restaurant Name and O.\nSentence: are reservations available for four people for 8 pm tonight at 112 eatery", "prompt_labels": "are(O) reservations(O) available(O) for(O) four(O) people(O) for(O) 8(O) pm(O) tonight(O) at(O) 112(B-Restaurant Name) eatery(I-Restaurant Name)"}}
{"id": "36", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Amenity", "Dish", "Rating", "Location", "Restaurant Name", "Price", "Cuisine"], "instance": {"id": "36", "words": ["are", "there", "any", "fancy", "cambodian", "places", "on", "seaver", "street"], "labels": ["O", "O", "O", "B-Amenity", "B-Cuisine", "O", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Dish, Rating, Location, Restaurant Name, Price, Cuisine and O.\nSentence: are there any fancy cambodian places on seaver street", "prompt_labels": "are(O) there(O) any(O) fancy(B-Amenity) cambodian(B-Cuisine) places(O) on(O) seaver(B-Location) street(I-Location)"}}
{"id": "1185", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Location", "Amenity", "Price", "Rating", "Hours", "Cuisine", "Dish"], "instance": {"id": "1185", "words": ["what", "restaurant", "serves", "the", "largest", "portions", "of", "meat", "within", "10", "mile"], "labels": ["O", "O", "O", "O", "B-Amenity", "I-Amenity", "O", "B-Dish", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Amenity, Price, Rating, Hours, Cuisine, Dish and O.\nSentence: what restaurant serves the largest portions of meat within 10 mile", "prompt_labels": "what(O) restaurant(O) serves(O) the(O) largest(B-Amenity) portions(I-Amenity) of(O) meat(B-Dish) within(B-Location) 10(I-Location) mile(I-Location)"}}
{"id": "1346", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Amenity", "Location", "Cuisine", "Hours", "Price", "Restaurant Name", "Dish"], "instance": {"id": "1346", "words": ["where", "can", "i", "get", "starbucks", "around", "me"], "labels": ["O", "O", "O", "O", "B-Restaurant Name", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Location, Cuisine, Hours, Price, Restaurant Name, Dish and O.\nSentence: where can i get starbucks around me", "prompt_labels": "where(O) can(O) i(O) get(O) starbucks(B-Restaurant Name) around(B-Location) me(I-Location)"}}
{"id": "1406", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Restaurant Name", "Hours", "Rating", "Cuisine", "Amenity", "Location", "Dish"], "instance": {"id": "1406", "words": ["where", "is", "the", "best", "reviewed", "place", "to", "get", "a", "burger", "and", "fries", "near", "beacon", "hill"], "labels": ["O", "O", "O", "B-Rating", "I-Rating", "O", "O", "O", "O", "B-Dish", "I-Dish", "I-Dish", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Restaurant Name, Hours, Rating, Cuisine, Amenity, Location, Dish and O.\nSentence: where is the best reviewed place to get a burger and fries near beacon hill", "prompt_labels": "where(O) is(O) the(O) best(B-Rating) reviewed(I-Rating) place(O) to(O) get(O) a(O) burger(B-Dish) and(I-Dish) fries(I-Dish) near(B-Location) beacon(I-Location) hill(I-Location)"}}
{"id": "1506", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Hours", "Cuisine", "Location", "Dish", "Amenity", "Rating", "Restaurant Name"], "instance": {"id": "1506", "words": ["who", "has", "the", "best", "lobster", "in", "town"], "labels": ["O", "O", "O", "B-Rating", "B-Dish", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Cuisine, Location, Dish, Amenity, Rating, Restaurant Name and O.\nSentence: who has the best lobster in town", "prompt_labels": "who(O) has(O) the(O) best(B-Rating) lobster(B-Dish) in(B-Location) town(I-Location)"}}
{"id": "117", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Price", "Rating", "Restaurant Name", "Hours", "Cuisine", "Location", "Amenity"], "instance": {"id": "117", "words": ["can", "i", "dine", "at", "the", "barat", "a", "nossa", "casa"], "labels": ["O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Rating, Restaurant Name, Hours, Cuisine, Location, Amenity and O.\nSentence: can i dine at the barat a nossa casa", "prompt_labels": "can(O) i(O) dine(O) at(O) the(O) barat(B-Restaurant Name) a(I-Restaurant Name) nossa(I-Restaurant Name) casa(I-Restaurant Name)"}}
{"id": "446", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Price", "Location", "Hours", "Cuisine", "Amenity", "Restaurant Name", "Dish"], "instance": {"id": "446", "words": ["find", "restaurants", "within", "5", "miles", "with", "entrees", "under", "15"], "labels": ["O", "O", "B-Location", "I-Location", "I-Location", "O", "B-Price", "I-Price", "I-Price"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Location, Hours, Cuisine, Amenity, Restaurant Name, Dish and O.\nSentence: find restaurants within 5 miles with entrees under 15", "prompt_labels": "find(O) restaurants(O) within(B-Location) 5(I-Location) miles(I-Location) with(O) entrees(B-Price) under(I-Price) 15(I-Price)"}}
{"id": "1168", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Restaurant Name", "Hours", "Cuisine", "Dish", "Location", "Amenity", "Rating"], "instance": {"id": "1168", "words": ["what", "pizza", "place", "has", "the", "best", "toppings", "and", "is", "no", "farther", "than", "4", "miles"], "labels": ["O", "B-Dish", "O", "O", "O", "B-Rating", "I-Rating", "O", "O", "B-Location", "I-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Restaurant Name, Hours, Cuisine, Dish, Location, Amenity, Rating and O.\nSentence: what pizza place has the best toppings and is no farther than 4 miles", "prompt_labels": "what(O) pizza(B-Dish) place(O) has(O) the(O) best(B-Rating) toppings(I-Rating) and(O) is(O) no(B-Location) farther(I-Location) than(I-Location) 4(I-Location) miles(I-Location)"}}
{"id": "639", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Amenity", "Cuisine", "Location", "Price", "Hours", "Restaurant Name", "Dish"], "instance": {"id": "639", "words": ["i", "want", "something", "full", "of", "grease"], "labels": ["O", "O", "O", "O", "O", "B-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Cuisine, Location, Price, Hours, Restaurant Name, Dish and O.\nSentence: i want something full of grease", "prompt_labels": "i(O) want(O) something(O) full(O) of(O) grease(B-Cuisine)"}}
{"id": "8", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Hours", "Restaurant Name", "Dish", "Location", "Cuisine", "Rating", "Amenity"], "instance": {"id": "8", "words": ["any", "mexican", "places", "have", "a", "tameles", "special", "today"], "labels": ["O", "B-Cuisine", "O", "O", "O", "B-Dish", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Restaurant Name, Dish, Location, Cuisine, Rating, Amenity and O.\nSentence: any mexican places have a tameles special today", "prompt_labels": "any(O) mexican(B-Cuisine) places(O) have(O) a(O) tameles(B-Dish) special(B-Amenity) today(I-Amenity)"}}
{"id": "334", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Cuisine", "Price", "Amenity", "Hours", "Rating", "Location", "Restaurant Name"], "instance": {"id": "334", "words": ["find", "a", "cheap", "brewpub", "that", "serves", "beef"], "labels": ["O", "O", "B-Price", "B-Cuisine", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Price, Amenity, Hours, Rating, Location, Restaurant Name and O.\nSentence: find a cheap brewpub that serves beef", "prompt_labels": "find(O) a(O) cheap(B-Price) brewpub(B-Cuisine) that(O) serves(O) beef(B-Dish)"}}
{"id": "1353", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Price", "Cuisine", "Rating", "Hours", "Location", "Restaurant Name", "Dish"], "instance": {"id": "1353", "words": ["where", "can", "i", "have", "a", "glass", "of", "wine", "in", "jamaica", "plain", "that", "also", "does", "not", "allow", "smoking"], "labels": ["O", "O", "O", "O", "O", "B-Rating", "O", "B-Cuisine", "O", "B-Location", "I-Location", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Cuisine, Rating, Hours, Location, Restaurant Name, Dish and O.\nSentence: where can i have a glass of wine in jamaica plain that also does not allow smoking", "prompt_labels": "where(O) can(O) i(O) have(O) a(O) glass(B-Rating) of(O) wine(B-Cuisine) in(O) jamaica(B-Location) plain(I-Location) that(O) also(O) does(B-Amenity) not(I-Amenity) allow(I-Amenity) smoking(I-Amenity)"}}
{"id": "221", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Amenity", "Location", "Restaurant Name", "Cuisine", "Rating", "Dish", "Hours"], "instance": {"id": "221", "words": ["could", "you", "locate", "an", "italian", "restaurant", "around", "four", "miles", "away", "with", "pesto", "pasta"], "labels": ["O", "O", "O", "O", "B-Cuisine", "O", "O", "B-Location", "I-Location", "I-Location", "O", "B-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Location, Restaurant Name, Cuisine, Rating, Dish, Hours and O.\nSentence: could you locate an italian restaurant around four miles away with pesto pasta", "prompt_labels": "could(O) you(O) locate(O) an(O) italian(B-Cuisine) restaurant(O) around(O) four(B-Location) miles(I-Location) away(I-Location) with(O) pesto(B-Dish) pasta(I-Dish)"}}
{"id": "459", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Hours", "Price", "Amenity", "Location", "Cuisine", "Rating", "Dish"], "instance": {"id": "459", "words": ["find", "us", "locations", "of", "china", "wok"], "labels": ["O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Price, Amenity, Location, Cuisine, Rating, Dish and O.\nSentence: find us locations of china wok", "prompt_labels": "find(O) us(O) locations(O) of(O) china(B-Restaurant Name) wok(I-Restaurant Name)"}}
{"id": "947", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Price", "Hours", "Cuisine", "Rating", "Dish", "Restaurant Name", "Amenity"], "instance": {"id": "947", "words": ["local", "restaurant", "joints", "with", "smoking", "areas"], "labels": ["B-Location", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Hours, Cuisine, Rating, Dish, Restaurant Name, Amenity and O.\nSentence: local restaurant joints with smoking areas", "prompt_labels": "local(B-Location) restaurant(O) joints(O) with(O) smoking(B-Amenity) areas(I-Amenity)"}}
{"id": "1091", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Rating", "Price", "Restaurant Name", "Amenity", "Dish", "Hours", "Location"], "instance": {"id": "1091", "words": ["what", "are", "some", "of", "the", "best", "restaurants", "in", "this", "city"], "labels": ["O", "O", "O", "O", "O", "B-Rating", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Price, Restaurant Name, Amenity, Dish, Hours, Location and O.\nSentence: what are some of the best restaurants in this city", "prompt_labels": "what(O) are(O) some(O) of(O) the(O) best(B-Rating) restaurants(O) in(B-Location) this(I-Location) city(I-Location)"}}
{"id": "138", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Price", "Cuisine", "Dish", "Rating", "Restaurant Name", "Location", "Hours"], "instance": {"id": "138", "words": ["can", "you", "find", "a", "pizza", "place", "with", "a", "buffet", "within", "15", "miles"], "labels": ["O", "O", "O", "O", "B-Cuisine", "O", "O", "O", "B-Amenity", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Cuisine, Dish, Rating, Restaurant Name, Location, Hours and O.\nSentence: can you find a pizza place with a buffet within 15 miles", "prompt_labels": "can(O) you(O) find(O) a(O) pizza(B-Cuisine) place(O) with(O) a(O) buffet(B-Amenity) within(B-Location) 15(I-Location) miles(I-Location)"}}
{"id": "518", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Cuisine", "Restaurant Name", "Price", "Location", "Dish", "Hours", "Amenity"], "instance": {"id": "518", "words": ["how", "far", "is", "the", "nearest", "olive", "garden"], "labels": ["O", "O", "O", "O", "B-Location", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Restaurant Name, Price, Location, Dish, Hours, Amenity and O.\nSentence: how far is the nearest olive garden", "prompt_labels": "how(O) far(O) is(O) the(O) nearest(B-Location) olive(B-Restaurant Name) garden(I-Restaurant Name)"}}
{"id": "511", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Rating", "Hours", "Restaurant Name", "Dish", "Location", "Cuisine", "Amenity"], "instance": {"id": "511", "words": ["how", "far", "away", "is", "the", "nearest", "applebees"], "labels": ["O", "O", "O", "O", "O", "B-Location", "B-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Hours, Restaurant Name, Dish, Location, Cuisine, Amenity and O.\nSentence: how far away is the nearest applebees", "prompt_labels": "how(O) far(O) away(O) is(O) the(O) nearest(B-Location) applebees(B-Restaurant Name)"}}
{"id": "721", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Dish", "Price", "Cuisine", "Restaurant Name", "Location", "Rating", "Hours"], "instance": {"id": "721", "words": ["im", "hungry", "find", "me", "a", "restaurant", "with", "large", "portions"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Price, Cuisine, Restaurant Name, Location, Rating, Hours and O.\nSentence: im hungry find me a restaurant with large portions", "prompt_labels": "im(O) hungry(O) find(O) me(O) a(O) restaurant(O) with(O) large(B-Amenity) portions(I-Amenity)"}}
{"id": "32", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Price", "Rating", "Restaurant Name", "Cuisine", "Amenity", "Dish", "Hours"], "instance": {"id": "32", "words": ["are", "there", "any", "dining", "specials", "at", "le", "bec", "fin"], "labels": ["O", "O", "O", "O", "B-Amenity", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Rating, Restaurant Name, Cuisine, Amenity, Dish, Hours and O.\nSentence: are there any dining specials at le bec fin", "prompt_labels": "are(O) there(O) any(O) dining(O) specials(B-Amenity) at(O) le(B-Restaurant Name) bec(I-Restaurant Name) fin(I-Restaurant Name)"}}
{"id": "1279", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Hours", "Rating", "Dish", "Price", "Restaurant Name", "Amenity", "Location"], "instance": {"id": "1279", "words": ["where", "can", "i", "find", "a", "place", "for", "people", "watching", "nearby"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Rating, Dish, Price, Restaurant Name, Amenity, Location and O.\nSentence: where can i find a place for people watching nearby", "prompt_labels": "where(O) can(O) i(O) find(O) a(O) place(O) for(O) people(B-Amenity) watching(I-Amenity) nearby(B-Location)"}}
{"id": "958", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Restaurant Name", "Rating", "Price", "Location", "Amenity", "Hours", "Cuisine"], "instance": {"id": "958", "words": ["look", "up", "the", "reviews", "for", "this", "new", "asain", "restaurant"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Restaurant Name, Rating, Price, Location, Amenity, Hours, Cuisine and O.\nSentence: look up the reviews for this new asain restaurant", "prompt_labels": "look(O) up(O) the(O) reviews(O) for(O) this(O) new(O) asain(B-Cuisine) restaurant(O)"}}
{"id": "748", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Location", "Cuisine", "Price", "Hours", "Restaurant Name", "Rating", "Dish"], "instance": {"id": "748", "words": ["im", "low", "on", "cash", "where", "is", "the", "nearest", "atm"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Cuisine, Price, Hours, Restaurant Name, Rating, Dish and O.\nSentence: im low on cash where is the nearest atm", "prompt_labels": "im(O) low(O) on(O) cash(O) where(O) is(O) the(O) nearest(B-Location) atm(O)"}}
{"id": "970", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Price", "Location", "Amenity", "Hours", "Cuisine", "Rating", "Dish"], "instance": {"id": "970", "words": ["looking", "for", "an", "expensive", "seafood", "place", "make", "a", "reservation", "for", "6", "people", "at", "6", "00"], "labels": ["O", "O", "O", "B-Price", "B-Cuisine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Location, Amenity, Hours, Cuisine, Rating, Dish and O.\nSentence: looking for an expensive seafood place make a reservation for 6 people at 6 00", "prompt_labels": "looking(O) for(O) an(O) expensive(B-Price) seafood(B-Cuisine) place(O) make(O) a(O) reservation(O) for(O) 6(O) people(O) at(O) 6(O) 00(O)"}}
{"id": "1326", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Rating", "Location", "Dish", "Hours", "Restaurant Name", "Price", "Amenity"], "instance": {"id": "1326", "words": ["where", "can", "i", "get", "doughnuts", "right", "now"], "labels": ["O", "O", "O", "O", "B-Dish", "O", "B-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Location, Dish, Hours, Restaurant Name, Price, Amenity and O.\nSentence: where can i get doughnuts right now", "prompt_labels": "where(O) can(O) i(O) get(O) doughnuts(B-Dish) right(O) now(B-Hours)"}}
{"id": "805", "dataset": "mit-restaurant", "split": "test", "label_list": ["Rating", "Location", "Price", "Cuisine", "Amenity", "Hours", "Dish", "Restaurant Name"], "instance": {"id": "805", "words": ["is", "there", "a", "cheap", "vegetarian", "restaurant", "nearby"], "labels": ["O", "O", "O", "B-Price", "B-Cuisine", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Location, Price, Cuisine, Amenity, Hours, Dish, Restaurant Name and O.\nSentence: is there a cheap vegetarian restaurant nearby", "prompt_labels": "is(O) there(O) a(O) cheap(B-Price) vegetarian(B-Cuisine) restaurant(O) nearby(B-Location)"}}
{"id": "814", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Price", "Dish", "Amenity", "Cuisine", "Hours", "Rating", "Location"], "instance": {"id": "814", "words": ["is", "there", "a", "donut", "and", "donuts", "restaurant", "within", "5", "miles", "with", "a", "beer", "list"], "labels": ["O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "B-Location", "I-Location", "I-Location", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Dish, Amenity, Cuisine, Hours, Rating, Location and O.\nSentence: is there a donut and donuts restaurant within 5 miles with a beer list", "prompt_labels": "is(O) there(O) a(O) donut(B-Restaurant Name) and(I-Restaurant Name) donuts(I-Restaurant Name) restaurant(I-Restaurant Name) within(B-Location) 5(I-Location) miles(I-Location) with(O) a(O) beer(B-Amenity) list(I-Amenity)"}}
{"id": "750", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Price", "Amenity", "Cuisine", "Hours", "Dish", "Rating", "Restaurant Name"], "instance": {"id": "750", "words": ["im", "on", "a", "really", "tight", "budget", "but", "im", "hungry", "help", "me", "out"], "labels": ["O", "O", "O", "O", "B-Price", "I-Price", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Amenity, Cuisine, Hours, Dish, Rating, Restaurant Name and O.\nSentence: im on a really tight budget but im hungry help me out", "prompt_labels": "im(O) on(O) a(O) really(O) tight(B-Price) budget(I-Price) but(O) im(O) hungry(O) help(O) me(O) out(O)"}}
{"id": "784", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Hours", "Rating", "Amenity", "Price", "Dish", "Location", "Restaurant Name"], "instance": {"id": "784", "words": ["is", "the", "mcdonalds", "near", "my", "house", "open", "after", "midnight"], "labels": ["O", "O", "B-Restaurant Name", "B-Location", "I-Location", "I-Location", "B-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Rating, Amenity, Price, Dish, Location, Restaurant Name and O.\nSentence: is the mcdonalds near my house open after midnight", "prompt_labels": "is(O) the(O) mcdonalds(B-Restaurant Name) near(B-Location) my(I-Location) house(I-Location) open(B-Hours) after(I-Hours) midnight(I-Hours)"}}
{"id": "176", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Amenity", "Location", "Rating", "Hours", "Cuisine", "Price", "Dish"], "instance": {"id": "176", "words": ["can", "you", "find", "us", "an", "ice", "cream", "shop", "near", "haight", "street"], "labels": ["O", "O", "O", "O", "O", "B-Cuisine", "I-Cuisine", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Location, Rating, Hours, Cuisine, Price, Dish and O.\nSentence: can you find us an ice cream shop near haight street", "prompt_labels": "can(O) you(O) find(O) us(O) an(O) ice(B-Cuisine) cream(I-Cuisine) shop(O) near(B-Location) haight(I-Location) street(I-Location)"}}
{"id": "815", "dataset": "mit-restaurant", "split": "test", "label_list": ["Restaurant Name", "Location", "Amenity", "Hours", "Cuisine", "Price", "Rating", "Dish"], "instance": {"id": "815", "words": ["is", "there", "a", "dress", "code", "and", "yuris", "dine", "in", "restaurant"], "labels": ["O", "O", "O", "B-Amenity", "I-Amenity", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Amenity, Hours, Cuisine, Price, Rating, Dish and O.\nSentence: is there a dress code and yuris dine in restaurant", "prompt_labels": "is(O) there(O) a(O) dress(B-Amenity) code(I-Amenity) and(O) yuris(B-Restaurant Name) dine(I-Restaurant Name) in(I-Restaurant Name) restaurant(I-Restaurant Name)"}}
{"id": "1490", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Hours", "Price", "Dish", "Cuisine", "Rating", "Restaurant Name", "Amenity"], "instance": {"id": "1490", "words": ["which", "restaurant", "has", "a", "better", "rating", "on", "yelp", "sakura", "or", "fuji", "ya"], "labels": ["O", "O", "O", "O", "B-Rating", "I-Rating", "O", "O", "B-Restaurant Name", "O", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Hours, Price, Dish, Cuisine, Rating, Restaurant Name, Amenity and O.\nSentence: which restaurant has a better rating on yelp sakura or fuji ya", "prompt_labels": "which(O) restaurant(O) has(O) a(O) better(B-Rating) rating(I-Rating) on(O) yelp(O) sakura(B-Restaurant Name) or(O) fuji(B-Restaurant Name) ya(I-Restaurant Name)"}}
{"id": "850", "dataset": "mit-restaurant", "split": "test", "label_list": ["Location", "Rating", "Amenity", "Restaurant Name", "Price", "Hours", "Cuisine", "Dish"], "instance": {"id": "850", "words": ["is", "there", "a", "ponderosa", "near", "here"], "labels": ["O", "O", "O", "B-Restaurant Name", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Amenity, Restaurant Name, Price, Hours, Cuisine, Dish and O.\nSentence: is there a ponderosa near here", "prompt_labels": "is(O) there(O) a(O) ponderosa(B-Restaurant Name) near(B-Location) here(I-Location)"}}
{"id": "991", "dataset": "mit-restaurant", "split": "test", "label_list": ["Dish", "Location", "Restaurant Name", "Hours", "Rating", "Amenity", "Cuisine", "Price"], "instance": {"id": "991", "words": ["make", "a", "5", "00", "p", "m", "reservation", "for", "black", "angus"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Restaurant Name, Hours, Rating, Amenity, Cuisine, Price and O.\nSentence: make a 5 00 p m reservation for black angus", "prompt_labels": "make(O) a(O) 5(O) 00(O) p(O) m(O) reservation(O) for(O) black(B-Restaurant Name) angus(I-Restaurant Name)"}}
{"id": "212", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Hours", "Restaurant Name", "Price", "Rating", "Cuisine", "Location", "Dish"], "instance": {"id": "212", "words": ["car", "look", "for", "upscale", "restaurants", "downtown", "and", "make", "reservations", "at", "the", "best", "reviewed"], "labels": ["O", "O", "O", "B-Price", "O", "B-Location", "O", "O", "O", "O", "O", "B-Rating", "I-Rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Restaurant Name, Price, Rating, Cuisine, Location, Dish and O.\nSentence: car look for upscale restaurants downtown and make reservations at the best reviewed", "prompt_labels": "car(O) look(O) for(O) upscale(B-Price) restaurants(O) downtown(B-Location) and(O) make(O) reservations(O) at(O) the(O) best(B-Rating) reviewed(I-Rating)"}}
{"id": "1486", "dataset": "mit-restaurant", "split": "test", "label_list": ["Hours", "Amenity", "Location", "Rating", "Price", "Dish", "Cuisine", "Restaurant Name"], "instance": {"id": "1486", "words": ["which", "parks", "are", "kid", "friendly"], "labels": ["O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Location, Rating, Price, Dish, Cuisine, Restaurant Name and O.\nSentence: which parks are kid friendly", "prompt_labels": "which(O) parks(O) are(O) kid(B-Amenity) friendly(I-Amenity)"}}
{"id": "450", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Location", "Amenity", "Rating", "Dish", "Hours", "Cuisine", "Restaurant Name"], "instance": {"id": "450", "words": ["find", "the", "closest", "dunkin", "donuts"], "labels": ["O", "O", "B-Location", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Amenity, Rating, Dish, Hours, Cuisine, Restaurant Name and O.\nSentence: find the closest dunkin donuts", "prompt_labels": "find(O) the(O) closest(B-Location) dunkin(B-Restaurant Name) donuts(I-Restaurant Name)"}}
{"id": "1274", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Restaurant Name", "Price", "Rating", "Dish", "Amenity", "Hours", "Location"], "instance": {"id": "1274", "words": ["where", "can", "i", "find", "a", "good", "pork", "chop"], "labels": ["O", "O", "O", "O", "O", "B-Rating", "B-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Price, Rating, Dish, Amenity, Hours, Location and O.\nSentence: where can i find a good pork chop", "prompt_labels": "where(O) can(O) i(O) find(O) a(O) good(B-Rating) pork(B-Dish) chop(I-Dish)"}}
{"id": "772", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Price", "Location", "Dish", "Restaurant Name", "Rating", "Hours", "Cuisine"], "instance": {"id": "772", "words": ["is", "outback", "having", "any", "specials", "today"], "labels": ["O", "B-Restaurant Name", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Location, Dish, Restaurant Name, Rating, Hours, Cuisine and O.\nSentence: is outback having any specials today", "prompt_labels": "is(O) outback(B-Restaurant Name) having(O) any(O) specials(O) today(O)"}}
{"id": "94", "dataset": "mit-restaurant", "split": "test", "label_list": ["Cuisine", "Price", "Rating", "Dish", "Location", "Hours", "Restaurant Name", "Amenity"], "instance": {"id": "94", "words": ["are", "there", "any", "turkish", "restaurants", "in", "florida"], "labels": ["O", "O", "O", "B-Cuisine", "O", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Rating, Dish, Location, Hours, Restaurant Name, Amenity and O.\nSentence: are there any turkish restaurants in florida", "prompt_labels": "are(O) there(O) any(O) turkish(B-Cuisine) restaurants(O) in(O) florida(B-Location)"}}
{"id": "895", "dataset": "mit-restaurant", "split": "test", "label_list": ["Price", "Hours", "Cuisine", "Rating", "Amenity", "Location", "Dish", "Restaurant Name"], "instance": {"id": "895", "words": ["is", "there", "an", "italian", "place", "nearby"], "labels": ["O", "O", "O", "B-Cuisine", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Cuisine, Rating, Amenity, Location, Dish, Restaurant Name and O.\nSentence: is there an italian place nearby", "prompt_labels": "is(O) there(O) an(O) italian(B-Cuisine) place(O) nearby(B-Location)"}}
{"id": "982", "dataset": "mit-restaurant", "split": "test", "label_list": ["Amenity", "Cuisine", "Price", "Hours", "Location", "Dish", "Rating", "Restaurant Name"], "instance": {"id": "982", "words": ["looking", "for", "sebastians", "restaurant", "offering", "generous", "portions"], "labels": ["O", "O", "B-Restaurant Name", "I-Restaurant Name", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Cuisine, Price, Hours, Location, Dish, Rating, Restaurant Name and O.\nSentence: looking for sebastians restaurant offering generous portions", "prompt_labels": "looking(O) for(O) sebastians(B-Restaurant Name) restaurant(I-Restaurant Name) offering(O) generous(B-Amenity) portions(I-Amenity)"}}
