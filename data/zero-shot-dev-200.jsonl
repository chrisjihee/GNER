{"id": "265", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "programming language", "product", "country", "task", "organization", "field", "university", "metric", "conference", "location", "researcher", "algorithm"], "instance": {"id": "265", "words": ["They", "ended", "up", "awarding", "eleven", "PR2s", "to", "different", "institutions", ",", "including", "University", "of", "Freiburg", ",", "Bosch", ",", "Georgia", "Tech", ",", "KU", "Leuven", ",", "MIT", ",", "Stanford", ",", "Technical", "University", "of", "Munich", ",", "UC", "Berkeley", ",", "U", "Penn", ",", "USC", ",", "and", "University", "of", "Tokyo", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "B-university", "O", "B-university", "I-university", "O", "B-university", "I-university", "O", "B-university", "O", "B-university", "O", "B-university", "I-university", "I-university", "I-university", "O", "B-university", "I-university", "O", "B-university", "I-university", "O", "B-university", "O", "O", "B-university", "I-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, programming language, product, country, task, organization, field, university, metric, conference, location, researcher, algorithm and O.\nSentence: They ended up awarding eleven PR2s to different institutions , including University of Freiburg , Bosch , Georgia Tech , KU Leuven , MIT , Stanford , Technical University of Munich , UC Berkeley , U Penn , USC , and University of Tokyo .", "prompt_labels": "They(O) ended(O) up(O) awarding(O) eleven(O) PR2s(O) to(O) different(O) institutions(O) ,(O) including(O) University(B-university) of(I-university) Freiburg(I-university) ,(O) Bosch(B-university) ,(O) Georgia(B-university) Tech(I-university) ,(O) KU(B-university) Leuven(I-university) ,(O) MIT(B-university) ,(O) Stanford(B-university) ,(O) Technical(B-university) University(I-university) of(I-university) Munich(I-university) ,(O) UC(B-university) Berkeley(I-university) ,(O) U(B-university) Penn(I-university) ,(O) USC(B-university) ,(O) and(O) University(B-university) of(I-university) Tokyo(I-university) .(O)"}}
{"id": "76", "dataset": "crossner_ai", "split": "dev", "label_list": ["product", "country", "researcher", "metric", "programming language", "field", "person", "university", "conference", "location", "algorithm", "task", "organization"], "instance": {"id": "76", "words": ["Sensitivity", "is", "not", "the", "same", "as", "the", "precision", "or", "positive", "predictive", "value", "(", "ratio", "of", "TRUE", "positives", "to", "combined", "TRUE", "and", "FALSE", "positives", ")", ",", "which", "is", "as", "much", "a", "statement", "about", "the", "proportion", "of", "actual", "positives", "in", "the", "population", "being", "tested", "as", "it", "is", "about", "the", "test", "."], "labels": ["B-metric", "O", "O", "O", "O", "O", "O", "B-metric", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "B-metric", "I-metric", "O", "O", "B-metric", "I-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, country, researcher, metric, programming language, field, person, university, conference, location, algorithm, task, organization and O.\nSentence: Sensitivity is not the same as the precision or positive predictive value ( ratio of TRUE positives to combined TRUE and FALSE positives ) , which is as much a statement about the proportion of actual positives in the population being tested as it is about the test .", "prompt_labels": "Sensitivity(B-metric) is(O) not(O) the(O) same(O) as(O) the(O) precision(B-metric) or(O) positive(B-metric) predictive(I-metric) value(I-metric) ((O) ratio(O) of(O) TRUE(B-metric) positives(I-metric) to(O) combined(O) TRUE(B-metric) and(I-metric) FALSE(I-metric) positives(I-metric) )(O) ,(O) which(O) is(O) as(O) much(O) a(O) statement(O) about(O) the(O) proportion(O) of(O) actual(O) positives(O) in(O) the(O) population(O) being(O) tested(O) as(O) it(O) is(O) about(O) the(O) test(O) .(O)"}}
{"id": "268", "dataset": "crossner_ai", "split": "dev", "label_list": ["product", "person", "university", "organization", "location", "researcher", "metric", "algorithm", "field", "task", "conference", "programming language", "country"], "instance": {"id": "268", "words": ["It", "has", "been", "applied", "successfully", "to", "various", "problems", ",", "including", "robot", "control", ",", "elevator", "scheduling", ",", "telecommunications", ",", ",", "checkers", "and", "Go", "(", "AlphaGo", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "O", "O", "B-task", "O", "B-task", "O", "B-product", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, person, university, organization, location, researcher, metric, algorithm, field, task, conference, programming language, country and O.\nSentence: It has been applied successfully to various problems , including robot control , elevator scheduling , telecommunications , , checkers and Go ( AlphaGo ) .", "prompt_labels": "It(O) has(O) been(O) applied(O) successfully(O) to(O) various(O) problems(O) ,(O) including(O) robot(B-task) control(I-task) ,(O) elevator(B-task) scheduling(I-task) ,(O) telecommunications(B-task) ,(O) ,(O) checkers(B-task) and(O) Go(B-task) ((O) AlphaGo(B-product) )(O) .(O)"}}
{"id": "27", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "programming language", "field", "country", "researcher", "location", "person", "task", "organization", "university", "metric", "conference", "product"], "instance": {"id": "27", "words": ["The", "iPhone", "4S", ",", "iPad", "3", ",", "iPad", "Mini", "1G", ",", "iPad", "Air", ",", "iPad", "Pro", "1G", ",", "iPod", "Touch", "5G", "and", "later", ",", "all", "come", "with", "a", "more", "advanced", "voice", "assistant", "called", "Siri", "."], "labels": ["O", "B-product", "I-product", "O", "B-product", "I-product", "O", "B-product", "I-product", "I-product", "O", "B-product", "I-product", "O", "B-product", "I-product", "I-product", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, programming language, field, country, researcher, location, person, task, organization, university, metric, conference, product and O.\nSentence: The iPhone 4S , iPad 3 , iPad Mini 1G , iPad Air , iPad Pro 1G , iPod Touch 5G and later , all come with a more advanced voice assistant called Siri .", "prompt_labels": "The(O) iPhone(B-product) 4S(I-product) ,(O) iPad(B-product) 3(I-product) ,(O) iPad(B-product) Mini(I-product) 1G(I-product) ,(O) iPad(B-product) Air(I-product) ,(O) iPad(B-product) Pro(I-product) 1G(I-product) ,(O) iPod(B-product) Touch(I-product) 5G(I-product) and(O) later(O) ,(O) all(O) come(O) with(O) a(O) more(O) advanced(O) voice(O) assistant(O) called(O) Siri(B-product) .(O)"}}
{"id": "21", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "task", "product", "country", "organization", "researcher", "programming language", "conference", "field", "university", "person", "algorithm", "metric"], "instance": {"id": "21", "words": ["Webber", "became", "a", "Fellow", "of", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "in", "1991", ","], "labels": ["B-researcher", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, task, product, country, organization, researcher, programming language, conference, field, university, person, algorithm, metric and O.\nSentence: Webber became a Fellow of the Association for the Advancement of Artificial Intelligence in 1991 ,", "prompt_labels": "Webber(B-researcher) became(O) a(O) Fellow(O) of(O) the(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) in(O) 1991(O) ,(O)"}}
{"id": "231", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "location", "algorithm", "country", "programming language", "person", "conference", "field", "organization", "product", "university", "task", "metric"], "instance": {"id": "231", "words": ["The", "most", "commonly", "used", "robot", "configurations", "are", "articulated", "robots", ",", "SCARA", "robots", ",", "delta", "robots", "and", "cartesian", "coordinate", "robots", ",", "(", "gantry", "robots", "or", "x-y-z", "robots", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "B-product", "I-product", "O", "B-product", "I-product", "O", "B-product", "I-product", "I-product", "O", "O", "B-product", "I-product", "O", "B-product", "I-product", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, location, algorithm, country, programming language, person, conference, field, organization, product, university, task, metric and O.\nSentence: The most commonly used robot configurations are articulated robots , SCARA robots , delta robots and cartesian coordinate robots , ( gantry robots or x-y-z robots ) .", "prompt_labels": "The(O) most(O) commonly(O) used(O) robot(O) configurations(O) are(O) articulated(B-product) robots(I-product) ,(O) SCARA(B-product) robots(I-product) ,(O) delta(B-product) robots(I-product) and(O) cartesian(B-product) coordinate(I-product) robots(I-product) ,(O) ((O) gantry(B-product) robots(I-product) or(O) x-y-z(B-product) robots(I-product) )(O) .(O)"}}
{"id": "332", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "organization", "metric", "conference", "product", "researcher", "programming language", "person", "task", "location", "country", "university", "algorithm"], "instance": {"id": "332", "words": ["Essentially", ",", "this", "combines", "maximum", "likelihood", "estimation", "with", "a", "regularization", "procedure", "that", "favors", "simpler", "models", "over", "more", "complex", "models", "."], "labels": ["O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, organization, metric, conference, product, researcher, programming language, person, task, location, country, university, algorithm and O.\nSentence: Essentially , this combines maximum likelihood estimation with a regularization procedure that favors simpler models over more complex models .", "prompt_labels": "Essentially(O) ,(O) this(O) combines(O) maximum(B-algorithm) likelihood(I-algorithm) estimation(I-algorithm) with(O) a(O) regularization(B-algorithm) procedure(I-algorithm) that(O) favors(O) simpler(O) models(O) over(O) more(O) complex(O) models(O) .(O)"}}
{"id": "16", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "researcher", "programming language", "product", "conference", "field", "metric", "task", "location", "algorithm", "organization", "person", "country"], "instance": {"id": "16", "words": ["An", "implementation", "of", "several", "whitening", "procedures", "in", "R", ",", "including", "ZCA-whitening", "and", "PCA", "whitening", "but", "also", "CCA", "whitening", ",", "is", "available", "in", "the", "whitening", "R", "package", "published", "on", "CRAN", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-programming language", "O", "O", "B-algorithm", "O", "B-algorithm", "I-algorithm", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "B-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, researcher, programming language, product, conference, field, metric, task, location, algorithm, organization, person, country and O.\nSentence: An implementation of several whitening procedures in R , including ZCA-whitening and PCA whitening but also CCA whitening , is available in the whitening R package published on CRAN .", "prompt_labels": "An(O) implementation(O) of(O) several(O) whitening(O) procedures(O) in(O) R(B-programming language) ,(O) including(O) ZCA-whitening(B-algorithm) and(O) PCA(B-algorithm) whitening(I-algorithm) but(O) also(O) CCA(B-algorithm) whitening(I-algorithm) ,(O) is(O) available(O) in(O) the(O) whitening(B-product) R(I-product) package(I-product) published(O) on(O) CRAN(B-product) .(O)"}}
{"id": "65", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "country", "metric", "location", "researcher", "product", "organization", "task", "person", "programming language", "algorithm", "university", "conference"], "instance": {"id": "65", "words": ["The", "extension", "of", "this", "concept", "to", "non-binary", "classifications", "yields", "the", "confusion", "matrix", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "B-metric", "I-metric", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, country, metric, location, researcher, product, organization, task, person, programming language, algorithm, university, conference and O.\nSentence: The extension of this concept to non-binary classifications yields the confusion matrix .", "prompt_labels": "The(O) extension(O) of(O) this(O) concept(O) to(O) non-binary(B-task) classifications(I-task) yields(O) the(O) confusion(B-metric) matrix(I-metric) .(O)"}}
{"id": "315", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "metric", "algorithm", "organization", "university", "conference", "person", "product", "programming language", "field", "country", "location", "task"], "instance": {"id": "315", "words": ["He", "has", "been", "elected", "a", "Fellow", "of", "the", "Association", "for", "Computing", "Machinery", "(", "ACM", ")", ",", "the", "Institute", "of", "Electrical", "and", "Electronics", "Engineers", "(", "IEEE", ")", ",", "the", "International", "Association", "for", "Pattern", "Recognition", "(", "IAPR", ")", ",", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "(", "AAAI", ")", ",", "American", "Association", "for", "Advancement", "of", "Science", "(", "AAAS", ")", ",", "and", "the", "Society", "for", "Optics", "and", "Photonics", "Technology", "(", "SPIE", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, metric, algorithm, organization, university, conference, person, product, programming language, field, country, location, task and O.\nSentence: He has been elected a Fellow of the Association for Computing Machinery ( ACM ) , the Institute of Electrical and Electronics Engineers ( IEEE ) , the International Association for Pattern Recognition ( IAPR ) , the Association for the Advancement of Artificial Intelligence ( AAAI ) , American Association for Advancement of Science ( AAAS ) , and the Society for Optics and Photonics Technology ( SPIE ) .", "prompt_labels": "He(O) has(O) been(O) elected(O) a(O) Fellow(O) of(O) the(O) Association(B-conference) for(I-conference) Computing(I-conference) Machinery(I-conference) ((O) ACM(B-conference) )(O) ,(O) the(O) Institute(B-organization) of(I-organization) Electrical(I-organization) and(I-organization) Electronics(I-organization) Engineers(I-organization) ((O) IEEE(B-organization) )(O) ,(O) the(O) International(B-conference) Association(I-conference) for(I-conference) Pattern(I-conference) Recognition(I-conference) ((O) IAPR(B-conference) )(O) ,(O) the(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) ((O) AAAI(B-conference) )(O) ,(O) American(B-conference) Association(I-conference) for(I-conference) Advancement(I-conference) of(I-conference) Science(I-conference) ((O) AAAS(B-conference) )(O) ,(O) and(O) the(O) Society(B-conference) for(I-conference) Optics(I-conference) and(I-conference) Photonics(I-conference) Technology(I-conference) ((O) SPIE(B-conference) )(O) .(O)"}}
{"id": "92", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "task", "field", "person", "metric", "university", "algorithm", "organization", "product", "conference", "researcher", "country", "programming language"], "instance": {"id": "92", "words": ["Recent", "research", "has", "increasingly", "focused", "on", "unsupervised", "learning", "and", "semi-supervised", "learning", "algorithms", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, task, field, person, metric, university, algorithm, organization, product, conference, researcher, country, programming language and O.\nSentence: Recent research has increasingly focused on unsupervised learning and semi-supervised learning algorithms .", "prompt_labels": "Recent(O) research(O) has(O) increasingly(O) focused(O) on(O) unsupervised(B-field) learning(I-field) and(O) semi-supervised(B-field) learning(I-field) algorithms(O) .(O)"}}
{"id": "175", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "programming language", "organization", "algorithm", "field", "university", "product", "task", "person", "researcher", "location", "country", "conference"], "instance": {"id": "175", "words": ["Rotten", "Tomatoes", "is", "a", "top", "1000", "site", ",", "placing", "around", "#", "400", "globally", "and", "top", "150", "for", "the", "US", "only", ",", "according", "to", "website", "ranker", "Alexa", "."], "labels": ["B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "B-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, programming language, organization, algorithm, field, university, product, task, person, researcher, location, country, conference and O.\nSentence: Rotten Tomatoes is a top 1000 site , placing around # 400 globally and top 150 for the US only , according to website ranker Alexa .", "prompt_labels": "Rotten(B-organization) Tomatoes(I-organization) is(O) a(O) top(O) 1000(O) site(O) ,(O) placing(O) around(O) #(O) 400(O) globally(O) and(O) top(O) 150(O) for(O) the(O) US(B-country) only(O) ,(O) according(O) to(O) website(O) ranker(O) Alexa(B-product) .(O)"}}
{"id": "78", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "field", "organization", "location", "metric", "country", "product", "person", "programming language", "algorithm", "conference", "researcher", "task"], "instance": {"id": "78", "words": ["Text", "analysis", "involves", "information", "retrieval", ",", "lexical", "analysis", "to", "study", "word", "frequency", "distributions", ",", "pattern", "recognition", ",", "tagging", "/", "annotation", ",", "information", "extraction", ",", "data", "mining", "techniques", "including", "link", "and", "association", "analysis", ",", "visualization", ",", "and", "predictive", "analytics", "."], "labels": ["B-field", "I-field", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-task", "I-task", "I-task", "O", "B-task", "I-task", "O", "B-field", "I-field", "O", "O", "B-task", "I-task", "I-task", "I-task", "O", "B-task", "O", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, field, organization, location, metric, country, product, person, programming language, algorithm, conference, researcher, task and O.\nSentence: Text analysis involves information retrieval , lexical analysis to study word frequency distributions , pattern recognition , tagging / annotation , information extraction , data mining techniques including link and association analysis , visualization , and predictive analytics .", "prompt_labels": "Text(B-field) analysis(I-field) involves(O) information(B-task) retrieval(I-task) ,(O) lexical(B-task) analysis(I-task) to(O) study(O) word(O) frequency(O) distributions(O) ,(O) pattern(B-field) recognition(I-field) ,(O) tagging(B-task) /(I-task) annotation(I-task) ,(O) information(B-task) extraction(I-task) ,(O) data(B-field) mining(I-field) techniques(O) including(O) link(B-task) and(I-task) association(I-task) analysis(I-task) ,(O) visualization(B-task) ,(O) and(O) predictive(B-task) analytics(I-task) .(O)"}}
{"id": "160", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "organization", "product", "task", "field", "algorithm", "location", "person", "metric", "conference", "university", "programming language", "country"], "instance": {"id": "160", "words": ["Recent", "text", "recognition", "is", "based", "on", "Recurrent", "neural", "network", "(", "Long", "short-term", "memory", ")", "and", "does", "not", "require", "a", "language", "model", "."], "labels": ["O", "B-task", "I-task", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, organization, product, task, field, algorithm, location, person, metric, conference, university, programming language, country and O.\nSentence: Recent text recognition is based on Recurrent neural network ( Long short-term memory ) and does not require a language model .", "prompt_labels": "Recent(O) text(B-task) recognition(I-task) is(O) based(O) on(O) Recurrent(B-algorithm) neural(I-algorithm) network(I-algorithm) ((O) Long(B-algorithm) short-term(I-algorithm) memory(I-algorithm) )(O) and(O) does(O) not(O) require(O) a(O) language(B-algorithm) model(I-algorithm) .(O)"}}
{"id": "4", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "field", "metric", "organization", "product", "country", "programming language", "conference", "location", "university", "person", "task", "algorithm"], "instance": {"id": "4", "words": ["Segmenting", "the", "text", "into", "topics", "or", "discourse", "turns", "might", "be", "useful", "in", "some", "natural", "processing", "tasks", ":", "it", "can", "improve", "information", "retrieval", "or", "speech", "recognition", "significantly", "(", "by", "indexing", "/", "recognizing", "documents", "more", "precisely", "or", "by", "giving", "the", "specific", "part", "of", "a", "document", "corresponding", "to", "the", "query", "as", "a", "result", ")", "."], "labels": ["B-task", "I-task", "I-task", "I-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, field, metric, organization, product, country, programming language, conference, location, university, person, task, algorithm and O.\nSentence: Segmenting the text into topics or discourse turns might be useful in some natural processing tasks : it can improve information retrieval or speech recognition significantly ( by indexing / recognizing documents more precisely or by giving the specific part of a document corresponding to the query as a result ) .", "prompt_labels": "Segmenting(B-task) the(I-task) text(I-task) into(I-task) topics(I-task) or(O) discourse(O) turns(O) might(O) be(O) useful(O) in(O) some(O) natural(O) processing(O) tasks(O) :(O) it(O) can(O) improve(O) information(B-task) retrieval(I-task) or(O) speech(B-task) recognition(I-task) significantly(O) ((O) by(O) indexing(O) /(O) recognizing(O) documents(O) more(O) precisely(O) or(O) by(O) giving(O) the(O) specific(O) part(O) of(O) a(O) document(O) corresponding(O) to(O) the(O) query(O) as(O) a(O) result(O) )(O) .(O)"}}
{"id": "152", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "researcher", "task", "conference", "location", "university", "product", "algorithm", "programming language", "person", "metric", "country", "organization"], "instance": {"id": "152", "words": ["An", "example", "of", "non-linear", "normalization", "is", "when", "the", "normalization", "follows", "a", "sigmoid", "function", ",", "in", "that", "case", ",", "the", "normalized", "image", "is", "computed", "according", "to", "the", "formula"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, researcher, task, conference, location, university, product, algorithm, programming language, person, metric, country, organization and O.\nSentence: An example of non-linear normalization is when the normalization follows a sigmoid function , in that case , the normalized image is computed according to the formula", "prompt_labels": "An(O) example(O) of(O) non-linear(O) normalization(O) is(O) when(O) the(O) normalization(O) follows(O) a(O) sigmoid(B-algorithm) function(I-algorithm) ,(O) in(O) that(O) case(O) ,(O) the(O) normalized(O) image(O) is(O) computed(O) according(O) to(O) the(O) formula(O)"}}
{"id": "107", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "organization", "programming language", "conference", "location", "task", "metric", "algorithm", "product", "country", "researcher", "person", "university"], "instance": {"id": "107", "words": ["The", "software", "is", "implemented", "in", "C", "+", "+", "and", "it", "is", "wrapped", "for", "Python", "."], "labels": ["O", "O", "O", "O", "O", "B-programming language", "I-programming language", "I-programming language", "O", "O", "O", "O", "O", "B-programming language", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, organization, programming language, conference, location, task, metric, algorithm, product, country, researcher, person, university and O.\nSentence: The software is implemented in C + + and it is wrapped for Python .", "prompt_labels": "The(O) software(O) is(O) implemented(O) in(O) C(B-programming language) +(I-programming language) +(I-programming language) and(O) it(O) is(O) wrapped(O) for(O) Python(B-programming language) .(O)"}}
{"id": "69", "dataset": "crossner_ai", "split": "dev", "label_list": ["organization", "metric", "researcher", "conference", "programming language", "algorithm", "field", "location", "product", "task", "person", "country", "university"], "instance": {"id": "69", "words": ["The", "condensation", "algorithm", "has", "also", "been", "used", "for", "facial", "recognition", "system", "in", "a", "video", "sequence", "."], "labels": ["O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, metric, researcher, conference, programming language, algorithm, field, location, product, task, person, country, university and O.\nSentence: The condensation algorithm has also been used for facial recognition system in a video sequence .", "prompt_labels": "The(O) condensation(B-algorithm) algorithm(I-algorithm) has(O) also(O) been(O) used(O) for(O) facial(B-product) recognition(I-product) system(I-product) in(O) a(O) video(O) sequence(O) .(O)"}}
{"id": "335", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "university", "location", "country", "person", "organization", "task", "product", "programming language", "conference", "algorithm", "researcher", "metric"], "instance": {"id": "335", "words": ["Prolonged", "use", "of", "speech", "recognition", "software", "in", "conjunction", "with", "word", "processor", "s", "has", "shown", "benefits", "to", "short-term-memory", "restrengthening", "in", "brain", "AVM", "patients", "who", "have", "been", "treated", "with", "resection", "."], "labels": ["O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, university, location, country, person, organization, task, product, programming language, conference, algorithm, researcher, metric and O.\nSentence: Prolonged use of speech recognition software in conjunction with word processor s has shown benefits to short-term-memory restrengthening in brain AVM patients who have been treated with resection .", "prompt_labels": "Prolonged(O) use(O) of(O) speech(B-product) recognition(I-product) software(I-product) in(O) conjunction(O) with(O) word(B-product) processor(I-product) s(O) has(O) shown(O) benefits(O) to(O) short-term-memory(O) restrengthening(O) in(O) brain(O) AVM(O) patients(O) who(O) have(O) been(O) treated(O) with(O) resection(O) .(O)"}}
{"id": "161", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "programming language", "researcher", "location", "conference", "person", "organization", "metric", "algorithm", "country", "product", "task", "university"], "instance": {"id": "161", "words": ["Popular", "loss", "functions", "include", "the", "hinge", "loss", "(", "for", "linear", "SVMs", ")", "and", "the", "log", "loss", "(", "for", "logistic", "regression", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "B-metric", "I-metric", "O", "O", "B-algorithm", "I-algorithm", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, programming language, researcher, location, conference, person, organization, metric, algorithm, country, product, task, university and O.\nSentence: Popular loss functions include the hinge loss ( for linear SVMs ) and the log loss ( for logistic regression ) .", "prompt_labels": "Popular(O) loss(O) functions(O) include(O) the(O) hinge(B-metric) loss(I-metric) ((O) for(O) linear(B-algorithm) SVMs(I-algorithm) )(O) and(O) the(O) log(B-metric) loss(I-metric) ((O) for(O) logistic(B-algorithm) regression(I-algorithm) )(O) .(O)"}}
{"id": "70", "dataset": "crossner_ai", "split": "dev", "label_list": ["country", "product", "location", "person", "conference", "field", "researcher", "task", "programming language", "algorithm", "metric", "organization", "university"], "instance": {"id": "70", "words": ["Information", "Dissemination", "is", "also", "part", "of", "ELRA", "'s", "missions", "which", "is", "carried", "through", "both", "the", "organisation", "of", "the", "conference", "LREC", "and", "the", "Language", "Resources", "and", "Evaluation", "Journal", "edited", "by", "Springer", "."], "labels": ["B-task", "I-task", "O", "O", "O", "O", "B-conference", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "B-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, product, location, person, conference, field, researcher, task, programming language, algorithm, metric, organization, university and O.\nSentence: Information Dissemination is also part of ELRA 's missions which is carried through both the organisation of the conference LREC and the Language Resources and Evaluation Journal edited by Springer .", "prompt_labels": "Information(B-task) Dissemination(I-task) is(O) also(O) part(O) of(O) ELRA(B-conference) 's(O) missions(O) which(O) is(O) carried(O) through(O) both(O) the(O) organisation(O) of(O) the(O) conference(O) LREC(B-conference) and(O) the(O) Language(B-conference) Resources(I-conference) and(I-conference) Evaluation(I-conference) Journal(I-conference) edited(O) by(O) Springer(B-conference) .(O)"}}
{"id": "63", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "researcher", "location", "organization", "field", "metric", "programming language", "country", "university", "task", "person", "product", "conference"], "instance": {"id": "63", "words": ["In", "many", "applications", "the", "units", "of", "these", "networks", "apply", "a", "sigmoid", "function", "as", "an", "activation", "function", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, researcher, location, organization, field, metric, programming language, country, university, task, person, product, conference and O.\nSentence: In many applications the units of these networks apply a sigmoid function as an activation function .", "prompt_labels": "In(O) many(O) applications(O) the(O) units(O) of(O) these(O) networks(O) apply(O) a(O) sigmoid(B-algorithm) function(I-algorithm) as(O) an(O) activation(O) function(O) .(O)"}}
{"id": "131", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "field", "university", "product", "country", "person", "algorithm", "organization", "location", "conference", "metric", "programming language", "task"], "instance": {"id": "131", "words": ["Where", "BLEU", "simply", "calculates", "n-gram", "precision", "adding", "equal", "weight", "to", "each", "one", ",", "NIST", "also", "calculates", "how", "informative", "a", "particular", "n-gram", "is", "."], "labels": ["O", "B-metric", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, field, university, product, country, person, algorithm, organization, location, conference, metric, programming language, task and O.\nSentence: Where BLEU simply calculates n-gram precision adding equal weight to each one , NIST also calculates how informative a particular n-gram is .", "prompt_labels": "Where(O) BLEU(B-metric) simply(O) calculates(O) n-gram(B-metric) precision(I-metric) adding(O) equal(O) weight(O) to(O) each(O) one(O) ,(O) NIST(B-metric) also(O) calculates(O) how(O) informative(O) a(O) particular(O) n-gram(O) is(O) .(O)"}}
{"id": "280", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "country", "university", "metric", "researcher", "organization", "person", "product", "conference", "task", "location", "field", "programming language"], "instance": {"id": "280", "words": ["Then", "it", "is", "also", "possible", "to", "use", "these", "probabilities", "and", "evaluate", "the", "mean", "squared", "error", "(", "or", "some", "other", "similar", "measure", ")", "between", "the", "probabilities", "and", "the", "actual", "values", ",", "then", "combine", "this", "with", "the", "confusion", "matrix", "to", "create", "very", "efficient", "fitness", "functions", "for", "logistic", "regression", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, country, university, metric, researcher, organization, person, product, conference, task, location, field, programming language and O.\nSentence: Then it is also possible to use these probabilities and evaluate the mean squared error ( or some other similar measure ) between the probabilities and the actual values , then combine this with the confusion matrix to create very efficient fitness functions for logistic regression .", "prompt_labels": "Then(O) it(O) is(O) also(O) possible(O) to(O) use(O) these(O) probabilities(O) and(O) evaluate(O) the(O) mean(B-metric) squared(I-metric) error(I-metric) ((O) or(O) some(O) other(O) similar(O) measure(O) )(O) between(O) the(O) probabilities(O) and(O) the(O) actual(O) values(O) ,(O) then(O) combine(O) this(O) with(O) the(O) confusion(B-metric) matrix(I-metric) to(O) create(O) very(O) efficient(O) fitness(O) functions(O) for(O) logistic(O) regression(O) .(O)"}}
{"id": "169", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "task", "organization", "product", "country", "location", "metric", "researcher", "person", "conference", "university", "algorithm", "field"], "instance": {"id": "169", "words": ["It", "joins", "a", "collection", "of", "historically", "important", "robots", "that", "includes", "an", "early", "Unimate", "and", "the", "Odetics", "Odex", "1", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "O", "B-product", "I-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, task, organization, product, country, location, metric, researcher, person, conference, university, algorithm, field and O.\nSentence: It joins a collection of historically important robots that includes an early Unimate and the Odetics Odex 1 .", "prompt_labels": "It(O) joins(O) a(O) collection(O) of(O) historically(O) important(O) robots(O) that(O) includes(O) an(O) early(O) Unimate(B-product) and(O) the(O) Odetics(B-product) Odex(I-product) 1(I-product) .(O)"}}
{"id": "10", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "product", "conference", "location", "researcher", "country", "field", "programming language", "university", "organization", "person", "algorithm", "metric"], "instance": {"id": "10", "words": ["In", "1969", ",", "Scheinman", "invented", "the", "Stanford", "arm", ","], "labels": ["O", "O", "O", "B-researcher", "O", "O", "B-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, product, conference, location, researcher, country, field, programming language, university, organization, person, algorithm, metric and O.\nSentence: In 1969 , Scheinman invented the Stanford arm ,", "prompt_labels": "In(O) 1969(O) ,(O) Scheinman(B-researcher) invented(O) the(O) Stanford(B-product) arm(I-product) ,(O)"}}
{"id": "262", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "researcher", "product", "university", "programming language", "conference", "country", "field", "algorithm", "organization", "person", "location", "task"], "instance": {"id": "262", "words": ["In", "1960", ",", "Devol", "personally", "sold", "the", "first", "Unimate", "robot", ",", "which", "was", "shipped", "in", "1961", "to", "General", "Motors", "."], "labels": ["O", "O", "O", "B-researcher", "O", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, researcher, product, university, programming language, conference, country, field, algorithm, organization, person, location, task and O.\nSentence: In 1960 , Devol personally sold the first Unimate robot , which was shipped in 1961 to General Motors .", "prompt_labels": "In(O) 1960(O) ,(O) Devol(B-researcher) personally(O) sold(O) the(O) first(O) Unimate(B-product) robot(O) ,(O) which(O) was(O) shipped(O) in(O) 1961(O) to(O) General(B-organization) Motors(I-organization) .(O)"}}
{"id": "281", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "country", "organization", "task", "metric", "conference", "researcher", "programming language", "product", "person", "field", "location", "university"], "instance": {"id": "281", "words": ["VoiceOver", "was", "for", "the", "first", "time", "featured", "in", "2005", "in", "Mac", "OS", "X", "Tiger", "(", "10.4", ")", "."], "labels": ["B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "I-product", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, country, organization, task, metric, conference, researcher, programming language, product, person, field, location, university and O.\nSentence: VoiceOver was for the first time featured in 2005 in Mac OS X Tiger ( 10.4 ) .", "prompt_labels": "VoiceOver(B-product) was(O) for(O) the(O) first(O) time(O) featured(O) in(O) 2005(O) in(O) Mac(B-product) OS(I-product) X(I-product) Tiger(I-product) ((O) 10.4(O) )(O) .(O)"}}
{"id": "322", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "person", "university", "product", "organization", "task", "algorithm", "researcher", "location", "country", "metric", "programming language", "field"], "instance": {"id": "322", "words": ["Robots", "can", "be", "autonomous", "or", "semi-autonomous", "and", "range", "from", "humanoids", "such", "as", "Honda", "'", "s", "Advanced", "Step", "in", "Innovative", "Mobility", "(", "ASIMO", ")", "and", "TOSY", "'", "s", "TOSY", "Ping", "Pong", "Playing", "Robot", "(", "TOPIO", ")", "to", "industrial", "robot", "s", ",", "medical", "operating", "robot", "s", ",", "patient", "assist", "robots", ",", "dog", "therapy", "robots", ",", "collectively", "programmed", "swarm", "robots", ",", "UAV", "drones", "such", "as", "General", "Atomics", "MQ-1", "Predator", ",", "and", "even", "microscopic", "nano", "robots", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "B-product", "I-product", "I-product", "I-product", "I-product", "O", "B-product", "O", "O", "B-organization", "O", "O", "B-product", "I-product", "I-product", "I-product", "I-product", "O", "B-product", "O", "O", "B-product", "I-product", "O", "O", "B-product", "I-product", "I-product", "O", "O", "B-product", "I-product", "I-product", "O", "B-product", "I-product", "I-product", "O", "O", "O", "B-product", "I-product", "O", "B-product", "I-product", "O", "O", "B-product", "I-product", "I-product", "I-product", "O", "O", "O", "B-product", "I-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, person, university, product, organization, task, algorithm, researcher, location, country, metric, programming language, field and O.\nSentence: Robots can be autonomous or semi-autonomous and range from humanoids such as Honda ' s Advanced Step in Innovative Mobility ( ASIMO ) and TOSY ' s TOSY Ping Pong Playing Robot ( TOPIO ) to industrial robot s , medical operating robot s , patient assist robots , dog therapy robots , collectively programmed swarm robots , UAV drones such as General Atomics MQ-1 Predator , and even microscopic nano robots .", "prompt_labels": "Robots(O) can(O) be(O) autonomous(O) or(O) semi-autonomous(O) and(O) range(O) from(O) humanoids(O) such(O) as(O) Honda(B-organization) '(O) s(O) Advanced(B-product) Step(I-product) in(I-product) Innovative(I-product) Mobility(I-product) ((O) ASIMO(B-product) )(O) and(O) TOSY(B-organization) '(O) s(O) TOSY(B-product) Ping(I-product) Pong(I-product) Playing(I-product) Robot(I-product) ((O) TOPIO(B-product) )(O) to(O) industrial(B-product) robot(I-product) s(O) ,(O) medical(B-product) operating(I-product) robot(I-product) s(O) ,(O) patient(B-product) assist(I-product) robots(I-product) ,(O) dog(B-product) therapy(I-product) robots(I-product) ,(O) collectively(O) programmed(O) swarm(B-product) robots(I-product) ,(O) UAV(B-product) drones(I-product) such(O) as(O) General(B-product) Atomics(I-product) MQ-1(I-product) Predator(I-product) ,(O) and(O) even(O) microscopic(B-product) nano(I-product) robots(I-product) .(O)"}}
{"id": "327", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "task", "algorithm", "researcher", "conference", "organization", "field", "programming language", "country", "metric", "person", "university", "product"], "instance": {"id": "327", "words": ["The", "creation", "and", "implementation", "of", "chatbots", "is", "still", "a", "developing", "area", ",", "heavily", "related", "to", "artificial", "intelligence", "and", "machine", "learning", ",", "so", "the", "provided", "solutions", ",", "while", "possessing", "obvious", "advantages", ",", "have", "some", "important", "limitations", "in", "terms", "of", "functionalities", "and", "use", "cases", "."], "labels": ["O", "O", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, task, algorithm, researcher, conference, organization, field, programming language, country, metric, person, university, product and O.\nSentence: The creation and implementation of chatbots is still a developing area , heavily related to artificial intelligence and machine learning , so the provided solutions , while possessing obvious advantages , have some important limitations in terms of functionalities and use cases .", "prompt_labels": "The(O) creation(O) and(O) implementation(O) of(O) chatbots(B-product) is(O) still(O) a(O) developing(O) area(O) ,(O) heavily(O) related(O) to(O) artificial(B-field) intelligence(I-field) and(O) machine(B-field) learning(I-field) ,(O) so(O) the(O) provided(O) solutions(O) ,(O) while(O) possessing(O) obvious(O) advantages(O) ,(O) have(O) some(O) important(O) limitations(O) in(O) terms(O) of(O) functionalities(O) and(O) use(O) cases(O) .(O)"}}
{"id": "182", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "field", "conference", "algorithm", "person", "location", "organization", "metric", "researcher", "product", "task", "programming language", "country"], "instance": {"id": "182", "words": ["A", "fairly", "simple", "non-linear", "function", ",", "the", "sigmoid", "function", "such", "as", "the", "logistic", "function", "also", "has", "an", "easily", "calculated", "derivative", ",", "which", "can", "be", "important", "when", "calculating", "the", "weight", "updates", "in", "the", "network", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, field, conference, algorithm, person, location, organization, metric, researcher, product, task, programming language, country and O.\nSentence: A fairly simple non-linear function , the sigmoid function such as the logistic function also has an easily calculated derivative , which can be important when calculating the weight updates in the network .", "prompt_labels": "A(O) fairly(O) simple(O) non-linear(O) function(O) ,(O) the(O) sigmoid(B-algorithm) function(I-algorithm) such(O) as(O) the(O) logistic(B-algorithm) function(I-algorithm) also(O) has(O) an(O) easily(O) calculated(O) derivative(O) ,(O) which(O) can(O) be(O) important(O) when(O) calculating(O) the(O) weight(O) updates(O) in(O) the(O) network(O) .(O)"}}
{"id": "18", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "country", "university", "field", "person", "location", "product", "algorithm", "researcher", "task", "programming language", "organization", "conference"], "instance": {"id": "18", "words": ["The", "company", "was", "founded", "by", "Kiichiro", "Toyoda", "in", "1937", ",", "as", "a", "spinoff", "from", "Sakichi", "Toyoda", "company", "Toyota", "Industries", "to", "create", "automobiles", "."], "labels": ["O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-organization", "I-organization", "O", "O", "B-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, country, university, field, person, location, product, algorithm, researcher, task, programming language, organization, conference and O.\nSentence: The company was founded by Kiichiro Toyoda in 1937 , as a spinoff from Sakichi Toyoda company Toyota Industries to create automobiles .", "prompt_labels": "The(O) company(O) was(O) founded(O) by(O) Kiichiro(B-person) Toyoda(I-person) in(O) 1937(O) ,(O) as(O) a(O) spinoff(O) from(O) Sakichi(B-person) Toyoda(I-person) company(O) Toyota(B-organization) Industries(I-organization) to(O) create(O) automobiles(B-product) .(O)"}}
{"id": "219", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "task", "researcher", "metric", "product", "location", "field", "conference", "country", "algorithm", "university", "organization", "programming language"], "instance": {"id": "219", "words": ["Following", "that", "was", "Paramount", "'s", "first", "feature", ",", "Sangaree", "with", "Fernando", "Lamas", "and", "Arlene", "Dahl", "."], "labels": ["O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, task, researcher, metric, product, location, field, conference, country, algorithm, university, organization, programming language and O.\nSentence: Following that was Paramount 's first feature , Sangaree with Fernando Lamas and Arlene Dahl .", "prompt_labels": "Following(O) that(O) was(O) Paramount(B-organization) 's(O) first(O) feature(O) ,(O) Sangaree(O) with(O) Fernando(B-person) Lamas(I-person) and(O) Arlene(B-person) Dahl(I-person) .(O)"}}
{"id": "146", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "conference", "field", "country", "programming language", "university", "metric", "product", "organization", "algorithm", "researcher", "location", "person"], "instance": {"id": "146", "words": ["The", "F-score", "is", "often", "used", "in", "the", "field", "of", "information", "retrieval", "for", "measuring", "search", ",", "document", "classification", ",", "and", "query", "classification", "performance.", "and", "so", "F_beta", "is", "seen", "in", "wide", "application", "."], "labels": ["O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "B-task", "O", "B-task", "I-task", "O", "O", "B-task", "I-task", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, conference, field, country, programming language, university, metric, product, organization, algorithm, researcher, location, person and O.\nSentence: The F-score is often used in the field of information retrieval for measuring search , document classification , and query classification performance. and so F_beta is seen in wide application .", "prompt_labels": "The(O) F-score(B-metric) is(O) often(O) used(O) in(O) the(O) field(O) of(O) information(B-task) retrieval(I-task) for(O) measuring(O) search(B-task) ,(O) document(B-task) classification(I-task) ,(O) and(O) query(B-task) classification(I-task) performance.(O) and(O) so(O) F_beta(B-metric) is(O) seen(O) in(O) wide(O) application(O) .(O)"}}
{"id": "105", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "algorithm", "person", "field", "programming language", "researcher", "organization", "task", "university", "product", "location", "country", "conference"], "instance": {"id": "105", "words": ["For", "many", "years", "starting", "from", "1986", ",", "Miller", "directed", "the", "development", "of", "WordNet", ",", "a", "large", "computer-readable", "electronic", "reference", "usable", "in", "applications", "such", "as", "search", "engines", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-researcher", "O", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, algorithm, person, field, programming language, researcher, organization, task, university, product, location, country, conference and O.\nSentence: For many years starting from 1986 , Miller directed the development of WordNet , a large computer-readable electronic reference usable in applications such as search engines .", "prompt_labels": "For(O) many(O) years(O) starting(O) from(O) 1986(O) ,(O) Miller(B-researcher) directed(O) the(O) development(O) of(O) WordNet(B-product) ,(O) a(O) large(O) computer-readable(O) electronic(O) reference(O) usable(O) in(O) applications(O) such(O) as(O) search(B-product) engines(I-product) .(O)"}}
{"id": "90", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "location", "organization", "country", "university", "researcher", "programming language", "metric", "product", "task", "algorithm", "person", "field"], "instance": {"id": "90", "words": ["In", "2000", "Manuel", "Toharia", ",", "a", "speaker", "at", "previous", "Campus", "Parties", ",", "and", "director", "of", "Príncipe", "Felipe", "'s", "Museum", "of", "Sciences", "in", "Valencia", "'s", "City", "of", "arts", "and", "Sciences", "suggested", "that", "Ragageles", "expand", "and", "make", "the", "event", "more", "international", "by", "moving", "it", "to", "the", "famous", "museum", "."], "labels": ["O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "B-conference", "I-conference", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "I-location", "I-location", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, location, organization, country, university, researcher, programming language, metric, product, task, algorithm, person, field and O.\nSentence: In 2000 Manuel Toharia , a speaker at previous Campus Parties , and director of Príncipe Felipe 's Museum of Sciences in Valencia 's City of arts and Sciences suggested that Ragageles expand and make the event more international by moving it to the famous museum .", "prompt_labels": "In(O) 2000(O) Manuel(B-person) Toharia(I-person) ,(O) a(O) speaker(O) at(O) previous(O) Campus(B-conference) Parties(I-conference) ,(O) and(O) director(O) of(O) Príncipe(B-organization) Felipe(I-organization) 's(I-organization) Museum(I-organization) of(I-organization) Sciences(I-organization) in(O) Valencia(B-location) 's(I-location) City(I-location) of(I-location) arts(I-location) and(I-location) Sciences(I-location) suggested(O) that(O) Ragageles(B-person) expand(O) and(O) make(O) the(O) event(O) more(O) international(O) by(O) moving(O) it(O) to(O) the(O) famous(O) museum(O) .(O)"}}
{"id": "103", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "university", "country", "field", "organization", "programming language", "conference", "researcher", "product", "person", "location", "metric", "algorithm"], "instance": {"id": "103", "words": ["Together", "with", "Geoffrey", "Hinton", "and", "Yann", "LeCun", ",", "Bengio", "won", "the", "2018", "Turing", "Award", "."], "labels": ["O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, university, country, field, organization, programming language, conference, researcher, product, person, location, metric, algorithm and O.\nSentence: Together with Geoffrey Hinton and Yann LeCun , Bengio won the 2018 Turing Award .", "prompt_labels": "Together(O) with(O) Geoffrey(B-researcher) Hinton(I-researcher) and(O) Yann(B-researcher) LeCun(I-researcher) ,(O) Bengio(B-researcher) won(O) the(O) 2018(O) Turing(O) Award(O) .(O)"}}
{"id": "0", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "field", "person", "researcher", "programming language", "product", "country", "algorithm", "organization", "task", "location", "university", "conference"], "instance": {"id": "0", "words": ["Here", ",", "accuracy", "is", "measured", "by", "error", "rate", ",", "which", "is", "defined", "as", ":"], "labels": ["O", "O", "B-metric", "O", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, field, person, researcher, programming language, product, country, algorithm, organization, task, location, university, conference and O.\nSentence: Here , accuracy is measured by error rate , which is defined as :", "prompt_labels": "Here(O) ,(O) accuracy(B-metric) is(O) measured(O) by(O) error(B-metric) rate(I-metric) ,(O) which(O) is(O) defined(O) as(O) :(O)"}}
{"id": "159", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "programming language", "field", "task", "conference", "product", "country", "researcher", "organization", "person", "location", "university", "metric"], "instance": {"id": "159", "words": ["The", "information", "is", "a", "blend", "of", "sitemaps", "and", "RSS", "and", "is", "created", "using", "the", "Information", "Model", "(", "IM", ")", "and", "Biomedical", "Resource", "Ontology", "(", "BRO", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, programming language, field, task, conference, product, country, researcher, organization, person, location, university, metric and O.\nSentence: The information is a blend of sitemaps and RSS and is created using the Information Model ( IM ) and Biomedical Resource Ontology ( BRO ) .", "prompt_labels": "The(O) information(O) is(O) a(O) blend(O) of(O) sitemaps(O) and(O) RSS(O) and(O) is(O) created(O) using(O) the(O) Information(B-algorithm) Model(I-algorithm) ((O) IM(B-algorithm) )(O) and(O) Biomedical(B-algorithm) Resource(I-algorithm) Ontology(I-algorithm) ((O) BRO(B-algorithm) )(O) .(O)"}}
{"id": "9", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "metric", "researcher", "programming language", "field", "country", "product", "algorithm", "location", "person", "conference", "university", "organization"], "instance": {"id": "9", "words": ["Machine", "learning", "techniques", ",", "either", "Supervised", "learning", "or", "Unsupervised", "learning", ",", "have", "been", "used", "to", "induce", "such", "rules", "automatically", "."], "labels": ["B-field", "I-field", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, metric, researcher, programming language, field, country, product, algorithm, location, person, conference, university, organization and O.\nSentence: Machine learning techniques , either Supervised learning or Unsupervised learning , have been used to induce such rules automatically .", "prompt_labels": "Machine(B-field) learning(I-field) techniques(O) ,(O) either(O) Supervised(B-field) learning(I-field) or(O) Unsupervised(B-field) learning(I-field) ,(O) have(O) been(O) used(O) to(O) induce(O) such(O) rules(O) automatically(O) .(O)"}}
{"id": "345", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "organization", "metric", "field", "location", "university", "researcher", "task", "programming language", "algorithm", "product", "person", "country"], "instance": {"id": "345", "words": ["He", "graduated", "from", "Moscow", "State", "University", "and", "in", "November", "1978", ",", "he", "left", "for", "the", "United", "States", "thanks", "to", "the", "personal", "intervention", "of", "Senator", "Edward", "M.", "Kennedy", ".."], "labels": ["O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, organization, metric, field, location, university, researcher, task, programming language, algorithm, product, person, country and O.\nSentence: He graduated from Moscow State University and in November 1978 , he left for the United States thanks to the personal intervention of Senator Edward M. Kennedy ..", "prompt_labels": "He(O) graduated(O) from(O) Moscow(B-university) State(I-university) University(I-university) and(O) in(O) November(O) 1978(O) ,(O) he(O) left(O) for(O) the(O) United(B-country) States(I-country) thanks(O) to(O) the(O) personal(O) intervention(O) of(O) Senator(B-person) Edward(I-person) M.(I-person) Kennedy(I-person) ..(O)"}}
{"id": "167", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "product", "field", "university", "metric", "task", "country", "algorithm", "location", "organization", "programming language", "person", "researcher"], "instance": {"id": "167", "words": ["During", "his", "time", "at", "Duke", ",", "he", "worked", "on", "an", "automated", "crossword", "solver", "PROVERB", ",", "which", "won", "an", "Outstanding", "Paper", "Award", "in", "1999", "from", "AAAI", "and", "competed", "in", "the", "American", "Crossword", "Puzzle", "Tournament", "."], "labels": ["O", "O", "O", "O", "B-university", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, product, field, university, metric, task, country, algorithm, location, organization, programming language, person, researcher and O.\nSentence: During his time at Duke , he worked on an automated crossword solver PROVERB , which won an Outstanding Paper Award in 1999 from AAAI and competed in the American Crossword Puzzle Tournament .", "prompt_labels": "During(O) his(O) time(O) at(O) Duke(B-university) ,(O) he(O) worked(O) on(O) an(O) automated(O) crossword(O) solver(O) PROVERB(B-product) ,(O) which(O) won(O) an(O) Outstanding(O) Paper(O) Award(O) in(O) 1999(O) from(O) AAAI(B-conference) and(O) competed(O) in(O) the(O) American(O) Crossword(O) Puzzle(O) Tournament(O) .(O)"}}
{"id": "147", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "field", "country", "product", "university", "algorithm", "researcher", "task", "organization", "location", "conference", "person", "metric"], "instance": {"id": "147", "words": ["This", "is", "done", "by", "modeling", "the", "received", "signal", "then", "using", "a", "statistical", "estimation", "method", "such", "as", "maximum", "likelihood", "(", "ML", ")", ",", "majority", "voting", "(", "MV", ")", "or", "maximum", "a", "posteriori", "(", "MAP", ")", "to", "make", "a", "decision", "about", "which", "target", "in", "the", "library", "best", "fits", "the", "model", "built", "using", "the", "received", "signal", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "B-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, field, country, product, university, algorithm, researcher, task, organization, location, conference, person, metric and O.\nSentence: This is done by modeling the received signal then using a statistical estimation method such as maximum likelihood ( ML ) , majority voting ( MV ) or maximum a posteriori ( MAP ) to make a decision about which target in the library best fits the model built using the received signal .", "prompt_labels": "This(O) is(O) done(O) by(O) modeling(O) the(O) received(O) signal(O) then(O) using(O) a(O) statistical(O) estimation(O) method(O) such(O) as(O) maximum(B-algorithm) likelihood(I-algorithm) ((O) ML(B-algorithm) )(O) ,(O) majority(B-algorithm) voting(I-algorithm) ((O) MV(B-algorithm) )(O) or(O) maximum(B-algorithm) a(I-algorithm) posteriori(I-algorithm) ((O) MAP(B-algorithm) )(O) to(O) make(O) a(O) decision(O) about(O) which(O) target(O) in(O) the(O) library(O) best(O) fits(O) the(O) model(O) built(O) using(O) the(O) received(O) signal(O) .(O)"}}
{"id": "348", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "researcher", "metric", "task", "programming language", "conference", "field", "organization", "country", "location", "product", "university", "algorithm"], "instance": {"id": "348", "words": ["Natural", "language", "processing", "(", "NLP", ")", "is", "a", "subfield", "of", "linguistics", ",", "computer", "science", ",", "information", "engineering", ",", "and", "artificial", "intelligence", "concerned", "with", "the", "interactions", "between", "computers", "and", "human", "(", "natural", ")", "languages", ",", "in", "particular", "how", "to", "program", "computers", "to", "process", "and", "analyze", "large", "amounts", "of", "natural", "language", "data", "."], "labels": ["B-field", "I-field", "I-field", "O", "B-field", "O", "O", "O", "O", "O", "B-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, researcher, metric, task, programming language, conference, field, organization, country, location, product, university, algorithm and O.\nSentence: Natural language processing ( NLP ) is a subfield of linguistics , computer science , information engineering , and artificial intelligence concerned with the interactions between computers and human ( natural ) languages , in particular how to program computers to process and analyze large amounts of natural language data .", "prompt_labels": "Natural(B-field) language(I-field) processing(I-field) ((O) NLP(B-field) )(O) is(O) a(O) subfield(O) of(O) linguistics(B-field) ,(O) computer(B-field) science(I-field) ,(O) information(B-field) engineering(I-field) ,(O) and(O) artificial(B-field) intelligence(I-field) concerned(O) with(O) the(O) interactions(O) between(O) computers(O) and(O) human(O) ((O) natural(O) )(O) languages(O) ,(O) in(O) particular(O) how(O) to(O) program(O) computers(O) to(O) process(O) and(O) analyze(O) large(O) amounts(O) of(O) natural(O) language(O) data(O) .(O)"}}
{"id": "241", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "country", "programming language", "university", "product", "location", "organization", "task", "algorithm", "field", "conference", "person", "metric"], "instance": {"id": "241", "words": ["Hayes", "has", "served", "as", "secretary", "of", "AISB", ",", "chairman", "and", "trustee", "of", "IJCAI", ",", "associate", "editor", "of", "Artificial", "Intelligence", ",", "a", "governor", "of", "the", "Cognitive", "Science", "Society", "and", "president", "of", "American", "Association", "for", "Artificial", "Intelligence", "."], "labels": ["B-researcher", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "B-conference", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, country, programming language, university, product, location, organization, task, algorithm, field, conference, person, metric and O.\nSentence: Hayes has served as secretary of AISB , chairman and trustee of IJCAI , associate editor of Artificial Intelligence , a governor of the Cognitive Science Society and president of American Association for Artificial Intelligence .", "prompt_labels": "Hayes(B-researcher) has(O) served(O) as(O) secretary(O) of(O) AISB(B-organization) ,(O) chairman(O) and(O) trustee(O) of(O) IJCAI(B-conference) ,(O) associate(O) editor(O) of(O) Artificial(B-field) Intelligence(I-field) ,(O) a(O) governor(O) of(O) the(O) Cognitive(B-organization) Science(I-organization) Society(I-organization) and(O) president(O) of(O) American(B-organization) Association(I-organization) for(I-organization) Artificial(I-organization) Intelligence(I-organization) .(O)"}}
{"id": "202", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "metric", "field", "task", "algorithm", "researcher", "person", "programming language", "organization", "university", "country", "product", "location"], "instance": {"id": "202", "words": ["Sycara", "served", "as", "the", "program", "chair", "of", "the", "Second", "International", "Semantic", "Web", "Conference", "(", "ISWC", "2003", ")", ";", "general", "chair", ",", "of", "the", "Second", "International", "Conference", "on", "Autonomous", "Agents", "(", "Agents", "98", ")", ";", "chair", "of", "the", "Steering", "Committee", "of", "the", "Agents", "Conference", "(", "1999-2001", ")", ";", "scholarship", "chair", "of", "AAAI", "(", "1993-1999", ")", ";"], "labels": ["B-researcher", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "I-conference", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "I-conference", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "B-conference", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, metric, field, task, algorithm, researcher, person, programming language, organization, university, country, product, location and O.\nSentence: Sycara served as the program chair of the Second International Semantic Web Conference ( ISWC 2003 ) ; general chair , of the Second International Conference on Autonomous Agents ( Agents 98 ) ; chair of the Steering Committee of the Agents Conference ( 1999-2001 ) ; scholarship chair of AAAI ( 1993-1999 ) ;", "prompt_labels": "Sycara(B-researcher) served(O) as(O) the(O) program(O) chair(O) of(O) the(O) Second(B-conference) International(I-conference) Semantic(I-conference) Web(I-conference) Conference(I-conference) ((O) ISWC(B-conference) 2003(I-conference) )(O) ;(O) general(O) chair(O) ,(O) of(O) the(O) Second(B-conference) International(I-conference) Conference(I-conference) on(I-conference) Autonomous(I-conference) Agents(I-conference) ((O) Agents(B-conference) 98(I-conference) )(O) ;(O) chair(O) of(O) the(O) Steering(B-organization) Committee(I-organization) of(I-organization) the(I-organization) Agents(I-organization) Conference(I-organization) ((O) 1999-2001(O) )(O) ;(O) scholarship(O) chair(O) of(O) AAAI(B-conference) ((O) 1993-1999(O) )(O) ;(O)"}}
{"id": "288", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "country", "researcher", "programming language", "conference", "university", "task", "product", "field", "person", "metric", "organization", "location"], "instance": {"id": "288", "words": ["The", "values", "of", "sensitivity", "and", "specificity", "are", "agnostic", "to", "the", "percent", "of", "positive", "cases", "in", "the", "population", "of", "interest", "(", "as", "opposed", "to", ",", "for", "example", ",", "precision", ")", "."], "labels": ["O", "O", "O", "B-metric", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, country, researcher, programming language, conference, university, task, product, field, person, metric, organization, location and O.\nSentence: The values of sensitivity and specificity are agnostic to the percent of positive cases in the population of interest ( as opposed to , for example , precision ) .", "prompt_labels": "The(O) values(O) of(O) sensitivity(B-metric) and(O) specificity(B-metric) are(O) agnostic(O) to(O) the(O) percent(O) of(O) positive(O) cases(O) in(O) the(O) population(O) of(O) interest(O) ((O) as(O) opposed(O) to(O) ,(O) for(O) example(O) ,(O) precision(B-metric) )(O) .(O)"}}
{"id": "349", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "product", "programming language", "location", "person", "field", "conference", "algorithm", "organization", "university", "researcher", "country", "metric"], "instance": {"id": "349", "words": ["Other", "active", "youth-led", "climate", "groups", "include", "Extinction", "Rebellion", ",", "the", "Sunrise", "Movement", ",", "SustainUS", ",", "the", ",", "among", "others", "working", "at", "both", "the", "transnational", "and", "local", "levels", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, product, programming language, location, person, field, conference, algorithm, organization, university, researcher, country, metric and O.\nSentence: Other active youth-led climate groups include Extinction Rebellion , the Sunrise Movement , SustainUS , the , among others working at both the transnational and local levels .", "prompt_labels": "Other(O) active(O) youth-led(O) climate(O) groups(O) include(O) Extinction(B-organization) Rebellion(I-organization) ,(O) the(O) Sunrise(B-organization) Movement(I-organization) ,(O) SustainUS(B-organization) ,(O) the(O) ,(O) among(O) others(O) working(O) at(O) both(O) the(O) transnational(O) and(O) local(O) levels(O) .(O)"}}
{"id": "252", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "product", "university", "country", "person", "metric", "programming language", "organization", "conference", "location", "field", "algorithm", "researcher"], "instance": {"id": "252", "words": ["Improved", "maximum", "likelihood", "method", "(", "IMLM", ")", "is", "a", "combination", "of", "two", "MLM", "(", "maximum", "likelihood", ")", "estimators", "."], "labels": ["O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "O", "O", "O", "O", "B-algorithm", "O", "B-metric", "I-metric", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, product, university, country, person, metric, programming language, organization, conference, location, field, algorithm, researcher and O.\nSentence: Improved maximum likelihood method ( IMLM ) is a combination of two MLM ( maximum likelihood ) estimators .", "prompt_labels": "Improved(O) maximum(B-algorithm) likelihood(I-algorithm) method(I-algorithm) ((O) IMLM(B-algorithm) )(O) is(O) a(O) combination(O) of(O) two(O) MLM(B-algorithm) ((O) maximum(B-metric) likelihood(I-metric) )(O) estimators(O) .(O)"}}
{"id": "317", "dataset": "crossner_ai", "split": "dev", "label_list": ["organization", "university", "metric", "field", "conference", "algorithm", "researcher", "person", "task", "product", "programming language", "country", "location"], "instance": {"id": "317", "words": ["Indy", "is", "written", "in", "Java", "and", "therefore", "runs", "on", "most", "modern", "operating", "system", "s", "."], "labels": ["B-product", "O", "O", "O", "B-programming language", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, university, metric, field, conference, algorithm, researcher, person, task, product, programming language, country, location and O.\nSentence: Indy is written in Java and therefore runs on most modern operating system s .", "prompt_labels": "Indy(B-product) is(O) written(O) in(O) Java(B-programming language) and(O) therefore(O) runs(O) on(O) most(O) modern(O) operating(O) system(O) s(O) .(O)"}}
{"id": "150", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "conference", "researcher", "country", "organization", "metric", "product", "task", "person", "university", "programming language", "field", "algorithm"], "instance": {"id": "150", "words": ["This", "makes", "it", "practical", "for", "analyzing", "large", "data", "sets", "(", "hundreds", "or", "thousands", "of", "taxa", ")", "and", "for", "bootstrapping", ",", "for", "which", "purposes", "other", "means", "of", "analysis", "(", "e.g.", "maximum", "parsimony", ",", "maximum", "likelihood", ")", "may", "be", "computation", "ally", "prohibitive", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, conference, researcher, country, organization, metric, product, task, person, university, programming language, field, algorithm and O.\nSentence: This makes it practical for analyzing large data sets ( hundreds or thousands of taxa ) and for bootstrapping , for which purposes other means of analysis ( e.g. maximum parsimony , maximum likelihood ) may be computation ally prohibitive .", "prompt_labels": "This(O) makes(O) it(O) practical(O) for(O) analyzing(O) large(O) data(O) sets(O) ((O) hundreds(O) or(O) thousands(O) of(O) taxa(O) )(O) and(O) for(O) bootstrapping(B-algorithm) ,(O) for(O) which(O) purposes(O) other(O) means(O) of(O) analysis(O) ((O) e.g.(O) maximum(B-algorithm) parsimony(I-algorithm) ,(O) maximum(B-algorithm) likelihood(I-algorithm) )(O) may(O) be(O) computation(O) ally(O) prohibitive(O) .(O)"}}
{"id": "6", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "university", "product", "organization", "person", "field", "researcher", "country", "programming language", "task", "algorithm", "location", "metric"], "instance": {"id": "6", "words": ["It", "considers", "both", "the", "precision", "p", "and", "the", "recall", "r", "of", "the", "test", "to", "compute", "the", "score", ":", "p", "is", "the", "number", "of", "correct", "positive", "results", "divided", "by", "the", "number", "of", "all", "positive", "results", "returned", "by", "the", "classifier", ",", "and", "r", "is", "the", "number", "of", "correct", "positive", "results", "divided", "by", "the", "number", "of", "all", "relevant", "samples", "(", "all", "samples", "that", "should", "have", "been", "identified", "as", "positive", ")", "."], "labels": ["O", "O", "O", "O", "B-metric", "B-metric", "O", "O", "B-metric", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, university, product, organization, person, field, researcher, country, programming language, task, algorithm, location, metric and O.\nSentence: It considers both the precision p and the recall r of the test to compute the score : p is the number of correct positive results divided by the number of all positive results returned by the classifier , and r is the number of correct positive results divided by the number of all relevant samples ( all samples that should have been identified as positive ) .", "prompt_labels": "It(O) considers(O) both(O) the(O) precision(B-metric) p(B-metric) and(O) the(O) recall(B-metric) r(B-metric) of(O) the(O) test(O) to(O) compute(O) the(O) score(O) :(O) p(B-metric) is(O) the(O) number(O) of(O) correct(O) positive(O) results(O) divided(O) by(O) the(O) number(O) of(O) all(O) positive(O) results(O) returned(O) by(O) the(O) classifier(O) ,(O) and(O) r(B-metric) is(O) the(O) number(O) of(O) correct(O) positive(O) results(O) divided(O) by(O) the(O) number(O) of(O) all(O) relevant(O) samples(O) ((O) all(O) samples(O) that(O) should(O) have(O) been(O) identified(O) as(O) positive(O) )(O) .(O)"}}
{"id": "91", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "field", "country", "university", "conference", "organization", "metric", "algorithm", "product", "person", "programming language", "location", "researcher"], "instance": {"id": "91", "words": ["Within", "20", "minutes", ",", "a", "facial", "recognition", "system", "identifies", "personal", "information", "including", "family", "name", ",", "ID", "number", "and", "address", "which", "are", "displayed", "in", "the", "street", "on", "an", "advertising", "screen", "."], "labels": ["O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, field, country, university, conference, organization, metric, algorithm, product, person, programming language, location, researcher and O.\nSentence: Within 20 minutes , a facial recognition system identifies personal information including family name , ID number and address which are displayed in the street on an advertising screen .", "prompt_labels": "Within(O) 20(O) minutes(O) ,(O) a(O) facial(B-product) recognition(I-product) system(I-product) identifies(O) personal(O) information(O) including(O) family(O) name(O) ,(O) ID(O) number(O) and(O) address(O) which(O) are(O) displayed(O) in(O) the(O) street(O) on(O) an(O) advertising(O) screen(O) .(O)"}}
{"id": "320", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "product", "algorithm", "conference", "organization", "researcher", "programming language", "location", "metric", "country", "field", "university", "task"], "instance": {"id": "320", "words": ["The", "basic", "concepts", "involved", "in", "spectral", "estimation", "include", "autocorrelation", ",", "multi-D", "Fourier", "transform", ",", "mean", "square", "error", "and", "entropy", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, product, algorithm, conference, organization, researcher, programming language, location, metric, country, field, university, task and O.\nSentence: The basic concepts involved in spectral estimation include autocorrelation , multi-D Fourier transform , mean square error and entropy .", "prompt_labels": "The(O) basic(O) concepts(O) involved(O) in(O) spectral(O) estimation(O) include(O) autocorrelation(B-algorithm) ,(O) multi-D(B-algorithm) Fourier(I-algorithm) transform(I-algorithm) ,(O) mean(B-metric) square(I-metric) error(I-metric) and(O) entropy(B-metric) .(O)"}}
{"id": "104", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "programming language", "organization", "field", "researcher", "product", "metric", "task", "country", "location", "university", "person", "algorithm"], "instance": {"id": "104", "words": ["Additional", "series", "were", "filmed", "at", "the", "UK", "venue", "for", "specific", "sectors", "of", "the", "global", "market", ",", "including", "two", "series", "of", "Robot", "Wars", "Extreme", "Warriors", "with", "United", "States", "competitors", "for", "the", "TNN", "network", "(", "hosted", "by", "Mick", "Foley", "with", "Rebecca", "Grant", "serving", "as", "pit", "reporter", ")", ",", "two", "of", "Dutch", "Robot", "Wars", "for", "distribution", "in", "the", "Netherlands", "and", "a", "single", "series", "for", "Germany", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, programming language, organization, field, researcher, product, metric, task, country, location, university, person, algorithm and O.\nSentence: Additional series were filmed at the UK venue for specific sectors of the global market , including two series of Robot Wars Extreme Warriors with United States competitors for the TNN network ( hosted by Mick Foley with Rebecca Grant serving as pit reporter ) , two of Dutch Robot Wars for distribution in the Netherlands and a single series for Germany .", "prompt_labels": "Additional(O) series(O) were(O) filmed(O) at(O) the(O) UK(B-country) venue(O) for(O) specific(O) sectors(O) of(O) the(O) global(O) market(O) ,(O) including(O) two(O) series(O) of(O) Robot(O) Wars(O) Extreme(O) Warriors(O) with(O) United(B-country) States(I-country) competitors(O) for(O) the(O) TNN(B-organization) network(I-organization) ((O) hosted(O) by(O) Mick(B-person) Foley(I-person) with(O) Rebecca(B-person) Grant(I-person) serving(O) as(O) pit(O) reporter(O) )(O) ,(O) two(O) of(O) Dutch(O) Robot(O) Wars(O) for(O) distribution(O) in(O) the(O) Netherlands(B-country) and(O) a(O) single(O) series(O) for(O) Germany(B-country) .(O)"}}
{"id": "185", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "product", "algorithm", "university", "researcher", "country", "person", "metric", "conference", "location", "programming language", "task", "organization"], "instance": {"id": "185", "words": ["Aspects", "of", "ontology", "editors", "include", ":", "visual", "navigation", "possibilities", "within", "the", "knowledge", "model", ",", "inference", "engine", "s", "and", "extraction", ";", "support", "for", "modules", ";", "the", "import", "and", "export", "of", "foreign", "knowledge", "representation", "languages", "for", "ontology", "matching", ";", "and", "the", "support", "of", "meta-ontologies", "such", "as", "OWL-S", ",", "Dublin", "Core", ",", "etc", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "O", "B-task", "O", "B-task", "I-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "B-task", "O", "O", "B-product", "O", "B-product", "I-product", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, product, algorithm, university, researcher, country, person, metric, conference, location, programming language, task, organization and O.\nSentence: Aspects of ontology editors include : visual navigation possibilities within the knowledge model , inference engine s and extraction ; support for modules ; the import and export of foreign knowledge representation languages for ontology matching ; and the support of meta-ontologies such as OWL-S , Dublin Core , etc .", "prompt_labels": "Aspects(O) of(O) ontology(O) editors(O) include(O) :(O) visual(B-task) navigation(I-task) possibilities(O) within(O) the(O) knowledge(B-task) model(I-task) ,(O) inference(B-task) engine(I-task) s(O) and(O) extraction(B-task) ;(O) support(B-task) for(I-task) modules(I-task) ;(O) the(O) import(O) and(O) export(O) of(O) foreign(O) knowledge(B-task) representation(I-task) languages(O) for(O) ontology(B-task) matching(I-task) ;(O) and(O) the(O) support(O) of(O) meta-ontologies(B-task) such(O) as(O) OWL-S(B-product) ,(O) Dublin(B-product) Core(I-product) ,(O) etc(O) .(O)"}}
{"id": "12", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "organization", "algorithm", "person", "researcher", "metric", "country", "field", "university", "product", "programming language", "location", "task"], "instance": {"id": "12", "words": ["In", "machine", "learning", ",", "support-vector", "machines", "(", "SVMs", ",", "also", "support-vector", "networks", ")", "are", "supervised", "learning", "models", "with", "learning", "algorithm", "s", "that", "analyze", "data", "used", "for", "classification", "and", "regression", "analysis", "."], "labels": ["O", "B-field", "I-field", "O", "B-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, organization, algorithm, person, researcher, metric, country, field, university, product, programming language, location, task and O.\nSentence: In machine learning , support-vector machines ( SVMs , also support-vector networks ) are supervised learning models with learning algorithm s that analyze data used for classification and regression analysis .", "prompt_labels": "In(O) machine(B-field) learning(I-field) ,(O) support-vector(B-algorithm) machines(I-algorithm) ((O) SVMs(B-algorithm) ,(O) also(O) support-vector(B-algorithm) networks(I-algorithm) )(O) are(O) supervised(B-field) learning(I-field) models(O) with(O) learning(O) algorithm(O) s(O) that(O) analyze(O) data(O) used(O) for(O) classification(B-task) and(O) regression(B-task) analysis(I-task) .(O)"}}
{"id": "72", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "person", "task", "algorithm", "metric", "researcher", "field", "organization", "product", "university", "programming language", "country", "conference"], "instance": {"id": "72", "words": ["Due", "to", "its", "generality", ",", "the", "field", "is", "studied", "in", "many", "other", "disciplines", ",", "such", "as", "game", "theory", ",", "control", "theory", ",", "operations", "research", ",", "information", "theory", ",", "simulation-based", "optimization", ",", "multi-agent", "systems", ",", "swarm", "intelligence", ",", "statistics", "and", "genetic", "algorithm", "s", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-product", "I-product", "O", "B-field", "I-field", "O", "B-field", "O", "B-algorithm", "I-algorithm", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, task, algorithm, metric, researcher, field, organization, product, university, programming language, country, conference and O.\nSentence: Due to its generality , the field is studied in many other disciplines , such as game theory , control theory , operations research , information theory , simulation-based optimization , multi-agent systems , swarm intelligence , statistics and genetic algorithm s .", "prompt_labels": "Due(O) to(O) its(O) generality(O) ,(O) the(O) field(O) is(O) studied(O) in(O) many(O) other(O) disciplines(O) ,(O) such(O) as(O) game(B-field) theory(I-field) ,(O) control(B-field) theory(I-field) ,(O) operations(B-field) research(I-field) ,(O) information(B-field) theory(I-field) ,(O) simulation-based(B-field) optimization(I-field) ,(O) multi-agent(B-product) systems(I-product) ,(O) swarm(B-field) intelligence(I-field) ,(O) statistics(B-field) and(O) genetic(B-algorithm) algorithm(I-algorithm) s(O) .(O)"}}
{"id": "81", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "person", "researcher", "country", "metric", "task", "product", "organization", "programming language", "university", "location", "field", "conference"], "instance": {"id": "81", "words": ["As", "a", "performance", "metric", ",", "the", "uncertainty", "coefficient", "has", "the", "advantage", "over", "simple", "accuracy", "in", "that", "it", "is", "not", "affected", "by", "the", "relative", "sizes", "of", "the", "different", "classes", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, person, researcher, country, metric, task, product, organization, programming language, university, location, field, conference and O.\nSentence: As a performance metric , the uncertainty coefficient has the advantage over simple accuracy in that it is not affected by the relative sizes of the different classes .", "prompt_labels": "As(O) a(O) performance(O) metric(O) ,(O) the(O) uncertainty(B-metric) coefficient(I-metric) has(O) the(O) advantage(O) over(O) simple(O) accuracy(B-metric) in(O) that(O) it(O) is(O) not(O) affected(O) by(O) the(O) relative(O) sizes(O) of(O) the(O) different(O) classes(O) .(O)"}}
{"id": "25", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "person", "metric", "conference", "location", "field", "university", "programming language", "task", "country", "organization", "researcher", "product"], "instance": {"id": "25", "words": ["Expo", "II", "was", "announced", "as", "being", "the", "locale", "for", "the", "world", "premiere", "of", "several", "films", "never", "before", "seen", "in", "3D", ",", "including", "The", "Diamond", "Wizard", "and", "the", "Universal", "short", ",", "Hawaiian", "Nights", "with", "Mamie", "Van", "Doren", "and", "Pinky", "Lee", "."], "labels": ["B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, person, metric, conference, location, field, university, programming language, task, country, organization, researcher, product and O.\nSentence: Expo II was announced as being the locale for the world premiere of several films never before seen in 3D , including The Diamond Wizard and the Universal short , Hawaiian Nights with Mamie Van Doren and Pinky Lee .", "prompt_labels": "Expo(B-location) II(I-location) was(O) announced(O) as(O) being(O) the(O) locale(O) for(O) the(O) world(O) premiere(O) of(O) several(O) films(O) never(O) before(O) seen(O) in(O) 3D(O) ,(O) including(O) The(O) Diamond(O) Wizard(O) and(O) the(O) Universal(O) short(O) ,(O) Hawaiian(O) Nights(O) with(O) Mamie(B-person) Van(I-person) Doren(I-person) and(O) Pinky(B-person) Lee(I-person) .(O)"}}
{"id": "177", "dataset": "crossner_ai", "split": "dev", "label_list": ["country", "task", "person", "researcher", "organization", "product", "metric", "location", "programming language", "algorithm", "university", "conference", "field"], "instance": {"id": "177", "words": ["The", "SSD", "is", "also", "known", "as", "mean", "squared", "error", "."], "labels": ["O", "B-metric", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, task, person, researcher, organization, product, metric, location, programming language, algorithm, university, conference, field and O.\nSentence: The SSD is also known as mean squared error .", "prompt_labels": "The(O) SSD(B-metric) is(O) also(O) known(O) as(O) mean(B-metric) squared(I-metric) error(I-metric) .(O)"}}
{"id": "199", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "task", "conference", "location", "university", "metric", "person", "field", "country", "organization", "algorithm", "researcher", "product"], "instance": {"id": "199", "words": ["The", "term", "Semantic", "Web", "was", "coined", "by", "Tim", "Berners-Lee", ",", "the", "inventor", "of", "the", "World", "Wide", "Web", "and", "director", "of", "the", "World", "Wide", "Web", "Consortium", "(", "W3C", ")", ",", "which", "oversees", "the", "development", "of", "proposed", "Semantic", "Web", "standards", "."], "labels": ["O", "O", "B-product", "I-product", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, task, conference, location, university, metric, person, field, country, organization, algorithm, researcher, product and O.\nSentence: The term Semantic Web was coined by Tim Berners-Lee , the inventor of the World Wide Web and director of the World Wide Web Consortium ( W3C ) , which oversees the development of proposed Semantic Web standards .", "prompt_labels": "The(O) term(O) Semantic(B-product) Web(I-product) was(O) coined(O) by(O) Tim(B-researcher) Berners-Lee(I-researcher) ,(O) the(O) inventor(O) of(O) the(O) World(B-product) Wide(I-product) Web(I-product) and(O) director(O) of(O) the(O) World(B-organization) Wide(I-organization) Web(I-organization) Consortium(I-organization) ((O) W3C(B-organization) )(O) ,(O) which(O) oversees(O) the(O) development(O) of(O) proposed(O) Semantic(B-product) Web(I-product) standards(I-product) .(O)"}}
{"id": "208", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "country", "location", "field", "product", "university", "person", "researcher", "task", "conference", "metric", "organization", "programming language"], "instance": {"id": "208", "words": ["The", "unknown", "parameters", "in", "each", "vector", "βsubk", "/", "sub", "are", "typically", "jointly", "estimated", "by", "maximum", "a", "posteriori", "(", "MAP", ")", "estimation", ",", "which", "is", "an", "extension", "of", "maximum", "likelihood", "using", "regularization", "of", "the", "weights", "to", "prevent", "pathological", "solutions", "(", "usually", "a", "squared", "regularizing", "function", ",", "which", "is", "equivalent", "to", "placing", "a", "zero-mean", "Gaussian", "prior", "distribution", "on", "the", "weights", ",", "but", "other", "distributions", "are", "also", "possible", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, country, location, field, product, university, person, researcher, task, conference, metric, organization, programming language and O.\nSentence: The unknown parameters in each vector βsubk / sub are typically jointly estimated by maximum a posteriori ( MAP ) estimation , which is an extension of maximum likelihood using regularization of the weights to prevent pathological solutions ( usually a squared regularizing function , which is equivalent to placing a zero-mean Gaussian prior distribution on the weights , but other distributions are also possible ) .", "prompt_labels": "The(O) unknown(O) parameters(O) in(O) each(O) vector(O) βsubk(O) /(O) sub(O) are(O) typically(O) jointly(O) estimated(O) by(O) maximum(B-algorithm) a(I-algorithm) posteriori(I-algorithm) ((O) MAP(B-algorithm) )(O) estimation(O) ,(O) which(O) is(O) an(O) extension(O) of(O) maximum(B-algorithm) likelihood(I-algorithm) using(O) regularization(O) of(O) the(O) weights(O) to(O) prevent(O) pathological(O) solutions(O) ((O) usually(O) a(O) squared(B-algorithm) regularizing(I-algorithm) function(I-algorithm) ,(O) which(O) is(O) equivalent(O) to(O) placing(O) a(O) zero-mean(O) Gaussian(O) prior(O) distribution(O) on(O) the(O) weights(O) ,(O) but(O) other(O) distributions(O) are(O) also(O) possible(O) )(O) .(O)"}}
{"id": "155", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "university", "metric", "researcher", "algorithm", "country", "programming language", "task", "conference", "product", "organization", "field", "person"], "instance": {"id": "155", "words": ["In", "August", "2016", ",", "a", "research", "programme", "with", "University", "College", "Hospital", "was", "announced", "with", "the", "aim", "of", "developing", "an", "algorithm", "that", "can", "automatically", "differentiate", "between", "healthy", "and", "cancerous", "tissues", "in", "head", "and", "neck", "areas", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, university, metric, researcher, algorithm, country, programming language, task, conference, product, organization, field, person and O.\nSentence: In August 2016 , a research programme with University College Hospital was announced with the aim of developing an algorithm that can automatically differentiate between healthy and cancerous tissues in head and neck areas .", "prompt_labels": "In(O) August(O) 2016(O) ,(O) a(O) research(O) programme(O) with(O) University(B-organization) College(I-organization) Hospital(I-organization) was(O) announced(O) with(O) the(O) aim(O) of(O) developing(O) an(O) algorithm(O) that(O) can(O) automatically(O) differentiate(O) between(O) healthy(O) and(O) cancerous(O) tissues(O) in(O) head(O) and(O) neck(O) areas(O) .(O)"}}
{"id": "247", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "researcher", "programming language", "conference", "product", "person", "algorithm", "task", "location", "country", "organization", "metric", "university"], "instance": {"id": "247", "words": ["With", "David", "E.", "Rumelhart", "and", "Ronald", "J.", "Williams", ",", "Hinton", "was", "co-author", "of", "a", "highly", "cited", "paper", "published", "in", "1986", "that", "popularized", "the", "backpropagation", "algorithm", "for", "training", "multi-layer", "neural", "networks", ",", "The", "dramatic", "image-recognition", "milestone", "of", "the", "AlexNet", "designed", "by", "his", "student", "Alex", "Krizhevsky", "{", "{", "cite", "web"], "labels": ["O", "B-researcher", "I-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "I-researcher", "O", "B-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "B-task", "O", "O", "O", "B-algorithm", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, researcher, programming language, conference, product, person, algorithm, task, location, country, organization, metric, university and O.\nSentence: With David E. Rumelhart and Ronald J. Williams , Hinton was co-author of a highly cited paper published in 1986 that popularized the backpropagation algorithm for training multi-layer neural networks , The dramatic image-recognition milestone of the AlexNet designed by his student Alex Krizhevsky { { cite web", "prompt_labels": "With(O) David(B-researcher) E.(I-researcher) Rumelhart(I-researcher) and(O) Ronald(B-researcher) J.(I-researcher) Williams(I-researcher) ,(O) Hinton(B-researcher) was(O) co-author(O) of(O) a(O) highly(O) cited(O) paper(O) published(O) in(O) 1986(O) that(O) popularized(O) the(O) backpropagation(B-algorithm) algorithm(I-algorithm) for(O) training(O) multi-layer(B-algorithm) neural(I-algorithm) networks(I-algorithm) ,(O) The(O) dramatic(O) image-recognition(B-task) milestone(O) of(O) the(O) AlexNet(B-algorithm) designed(O) by(O) his(O) student(O) Alex(B-researcher) Krizhevsky(I-researcher) {(O) {(O) cite(O) web(O)"}}
{"id": "112", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "metric", "field", "algorithm", "location", "organization", "programming language", "conference", "task", "product", "researcher", "country", "university"], "instance": {"id": "112", "words": ["The", "Baum-Welch", "algorithm", "uses", "the", "well", "known", "EM", "algorithm", "to", "find", "the", "maximum", "likelihood", "estimate", "of", "the", "parameters", "of", "a", "hidden", "Markov", "model", "given", "a", "set", "of", "observed", "feature", "vectors", "."], "labels": ["O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, metric, field, algorithm, location, organization, programming language, conference, task, product, researcher, country, university and O.\nSentence: The Baum-Welch algorithm uses the well known EM algorithm to find the maximum likelihood estimate of the parameters of a hidden Markov model given a set of observed feature vectors .", "prompt_labels": "The(O) Baum-Welch(B-algorithm) algorithm(I-algorithm) uses(O) the(O) well(O) known(O) EM(B-algorithm) algorithm(I-algorithm) to(O) find(O) the(O) maximum(B-metric) likelihood(I-metric) estimate(I-metric) of(O) the(O) parameters(O) of(O) a(O) hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) given(O) a(O) set(O) of(O) observed(O) feature(O) vectors(O) .(O)"}}
{"id": "55", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "location", "researcher", "algorithm", "field", "person", "country", "product", "task", "university", "conference", "metric", "organization"], "instance": {"id": "55", "words": ["Lafferty", "served", "many", "prestigious", "positions", ",", "including", ":", "1", ")", "program", "co-chair", "and", "general", "co-chair", "of", "the", "Neural", "Information", "Processing", "Systems", "(", "Conference", "on", "Neural", "Information", "Processing", "Systems", ")", "Foundation", "conferences", ";", "2", ")", "co-director", "of", "CMU", "'s", "new", "Ph.D.", "Machine", "Learning", "Ph.D.", "Program", ";", "3", ")", "associate", "editor", "of", "the", "Journal", "of", "Machine", "Learning", "Research"], "labels": ["B-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, location, researcher, algorithm, field, person, country, product, task, university, conference, metric, organization and O.\nSentence: Lafferty served many prestigious positions , including : 1 ) program co-chair and general co-chair of the Neural Information Processing Systems ( Conference on Neural Information Processing Systems ) Foundation conferences ; 2 ) co-director of CMU 's new Ph.D. Machine Learning Ph.D. Program ; 3 ) associate editor of the Journal of Machine Learning Research", "prompt_labels": "Lafferty(B-researcher) served(O) many(O) prestigious(O) positions(O) ,(O) including(O) :(O) 1(O) )(O) program(O) co-chair(O) and(O) general(O) co-chair(O) of(O) the(O) Neural(B-conference) Information(I-conference) Processing(I-conference) Systems(I-conference) ((O) Conference(B-conference) on(I-conference) Neural(I-conference) Information(I-conference) Processing(I-conference) Systems(I-conference) )(O) Foundation(O) conferences(O) ;(O) 2(O) )(O) co-director(O) of(O) CMU(B-university) 's(O) new(O) Ph.D.(O) Machine(B-field) Learning(I-field) Ph.D.(O) Program(O) ;(O) 3(O) )(O) associate(O) editor(O) of(O) the(O) Journal(B-conference) of(I-conference) Machine(I-conference) Learning(I-conference) Research(I-conference)"}}
{"id": "24", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "metric", "programming language", "conference", "researcher", "country", "algorithm", "location", "field", "product", "person", "university", "organization"], "instance": {"id": "24", "words": ["He", "received", "a", "PhD", "in", "Radio", "Physics", "and", "Electronics", "from", "the", "Rajabazar", "Science", "College", "campus", "of", "University", "of", "Calcutta", "in", "1979", "as", "a", "student", "of", "Indian", "Statistical", "Institute", ",", "and", "another", "PhD", "in", "Electrical", "Engineering", "along", "with", "Diploma", "of", "the", "Imperial", "College", "from", "Imperial", "College", ",", "University", "of", "London", ",", "in", "1982", "."], "labels": ["O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "O", "O", "B-university", "I-university", "I-university", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, metric, programming language, conference, researcher, country, algorithm, location, field, product, person, university, organization and O.\nSentence: He received a PhD in Radio Physics and Electronics from the Rajabazar Science College campus of University of Calcutta in 1979 as a student of Indian Statistical Institute , and another PhD in Electrical Engineering along with Diploma of the Imperial College from Imperial College , University of London , in 1982 .", "prompt_labels": "He(O) received(O) a(O) PhD(O) in(O) Radio(B-field) Physics(I-field) and(O) Electronics(B-field) from(O) the(O) Rajabazar(B-university) Science(I-university) College(I-university) campus(O) of(O) University(B-university) of(I-university) Calcutta(I-university) in(O) 1979(O) as(O) a(O) student(O) of(O) Indian(B-university) Statistical(I-university) Institute(I-university) ,(O) and(O) another(O) PhD(O) in(O) Electrical(B-field) Engineering(I-field) along(O) with(O) Diploma(O) of(O) the(O) Imperial(O) College(O) from(O) Imperial(B-university) College(I-university) ,(O) University(B-university) of(I-university) London(I-university) ,(O) in(O) 1982(O) .(O)"}}
{"id": "285", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "programming language", "metric", "person", "location", "country", "algorithm", "product", "conference", "researcher", "organization", "university", "task"], "instance": {"id": "285", "words": ["Here", ",", "math", "\\", "sigma", "/", "math", "is", "an", "element-wise", "activation", "function", "such", "as", "a", "sigmoid", "function", "or", "a", "rectified", "linear", "unit", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, programming language, metric, person, location, country, algorithm, product, conference, researcher, organization, university, task and O.\nSentence: Here , math \\ sigma / math is an element-wise activation function such as a sigmoid function or a rectified linear unit .", "prompt_labels": "Here(O) ,(O) math(O) \\(O) sigma(O) /(O) math(O) is(O) an(O) element-wise(B-algorithm) activation(I-algorithm) function(I-algorithm) such(O) as(O) a(O) sigmoid(B-algorithm) function(I-algorithm) or(O) a(O) rectified(B-algorithm) linear(I-algorithm) unit(I-algorithm) .(O)"}}
{"id": "68", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "product", "person", "metric", "conference", "location", "algorithm", "task", "organization", "researcher", "field", "country", "university"], "instance": {"id": "68", "words": ["She", "has", "also", "served", "as", "Area", "Chair", "of", "several", "machine", "learning", "and", "vision", "conferences", "including", "Conference", "on", "Neural", "Information", "Processing", "Systems", ",", "International", "Conference", "on", "Learning", "Representations", ",", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", ",", "International", "Conference", "on", "Computer", "Vision", ",", "and", "European", "Conference", "on", "Computer", "Vision", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, product, person, metric, conference, location, algorithm, task, organization, researcher, field, country, university and O.\nSentence: She has also served as Area Chair of several machine learning and vision conferences including Conference on Neural Information Processing Systems , International Conference on Learning Representations , Conference on Computer Vision and Pattern Recognition , International Conference on Computer Vision , and European Conference on Computer Vision .", "prompt_labels": "She(O) has(O) also(O) served(O) as(O) Area(O) Chair(O) of(O) several(O) machine(B-field) learning(I-field) and(O) vision(B-field) conferences(O) including(O) Conference(B-conference) on(I-conference) Neural(I-conference) Information(I-conference) Processing(I-conference) Systems(I-conference) ,(O) International(B-conference) Conference(I-conference) on(I-conference) Learning(I-conference) Representations(I-conference) ,(O) Conference(B-conference) on(I-conference) Computer(I-conference) Vision(I-conference) and(I-conference) Pattern(I-conference) Recognition(I-conference) ,(O) International(B-conference) Conference(I-conference) on(I-conference) Computer(I-conference) Vision(I-conference) ,(O) and(O) European(B-conference) Conference(I-conference) on(I-conference) Computer(I-conference) Vision(I-conference) .(O)"}}
{"id": "115", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "person", "programming language", "metric", "location", "product", "researcher", "organization", "field", "country", "algorithm", "conference", "task"], "instance": {"id": "115", "words": ["In", "1978", ",", "the", "PUMA", "(", "Programmable", "Universal", "Machine", "for", "Assembly", ")", "robot", "was", "developed", "by", "Unimation", "from", "Vicarm", "(", "Victor", "Scheinman", ")", "and", "with", "support", "from", "General", "Motors", "."], "labels": ["O", "O", "O", "O", "B-product", "O", "B-product", "I-product", "I-product", "I-product", "I-product", "O", "O", "O", "O", "O", "B-organization", "O", "B-organization", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, person, programming language, metric, location, product, researcher, organization, field, country, algorithm, conference, task and O.\nSentence: In 1978 , the PUMA ( Programmable Universal Machine for Assembly ) robot was developed by Unimation from Vicarm ( Victor Scheinman ) and with support from General Motors .", "prompt_labels": "In(O) 1978(O) ,(O) the(O) PUMA(B-product) ((O) Programmable(B-product) Universal(I-product) Machine(I-product) for(I-product) Assembly(I-product) )(O) robot(O) was(O) developed(O) by(O) Unimation(B-organization) from(O) Vicarm(B-organization) ((O) Victor(B-researcher) Scheinman(I-researcher) )(O) and(O) with(O) support(O) from(O) General(B-organization) Motors(I-organization) .(O)"}}
{"id": "98", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "university", "person", "conference", "country", "programming language", "algorithm", "location", "product", "task", "organization", "metric", "field"], "instance": {"id": "98", "words": ["Common", "criteria", "are", "the", "Mean", "Squared", "Error", "criterion", "implemented", "in", "MSECriterion", "and", "the", "cross-entropy", "criterion", "implemented", "in", "NLLCriterion", "."], "labels": ["O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "B-metric", "O", "O", "B-metric", "I-metric", "O", "O", "B-metric", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, university, person, conference, country, programming language, algorithm, location, product, task, organization, metric, field and O.\nSentence: Common criteria are the Mean Squared Error criterion implemented in MSECriterion and the cross-entropy criterion implemented in NLLCriterion .", "prompt_labels": "Common(O) criteria(O) are(O) the(O) Mean(B-metric) Squared(I-metric) Error(I-metric) criterion(O) implemented(O) in(O) MSECriterion(B-metric) and(O) the(O) cross-entropy(B-metric) criterion(I-metric) implemented(O) in(O) NLLCriterion(B-metric) .(O)"}}
{"id": "118", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "organization", "conference", "country", "researcher", "person", "field", "task", "programming language", "metric", "university", "product", "algorithm"], "instance": {"id": "118", "words": ["He", "also", "contributed", "much", "through", "the", "establishment", "of", "ELRA", "and", "the", "LREC", "conference", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "O", "O", "B-conference", "I-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, conference, country, researcher, person, field, task, programming language, metric, university, product, algorithm and O.\nSentence: He also contributed much through the establishment of ELRA and the LREC conference .", "prompt_labels": "He(O) also(O) contributed(O) much(O) through(O) the(O) establishment(O) of(O) ELRA(B-conference) and(O) the(O) LREC(B-conference) conference(I-conference) .(O)"}}
{"id": "295", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "person", "conference", "metric", "product", "university", "programming language", "country", "researcher", "location", "algorithm", "field", "organization"], "instance": {"id": "295", "words": ["An", "empirical", "whitening", "transform", "is", "obtained", "by", "estimating", "the", "covariance", "(", "e.g.", "by", "maximum", "likelihood", ")", "and", "subsequently", "constructing", "a", "corresponding", "estimated", "whitening", "matrix", "(", "e.g.", "by", "Cholesky", "decomposition", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, person, conference, metric, product, university, programming language, country, researcher, location, algorithm, field, organization and O.\nSentence: An empirical whitening transform is obtained by estimating the covariance ( e.g. by maximum likelihood ) and subsequently constructing a corresponding estimated whitening matrix ( e.g. by Cholesky decomposition ) .", "prompt_labels": "An(O) empirical(O) whitening(O) transform(O) is(O) obtained(O) by(O) estimating(O) the(O) covariance(O) ((O) e.g.(O) by(O) maximum(B-algorithm) likelihood(I-algorithm) )(O) and(O) subsequently(O) constructing(O) a(O) corresponding(O) estimated(O) whitening(O) matrix(O) ((O) e.g.(O) by(O) Cholesky(B-algorithm) decomposition(I-algorithm) )(O) .(O)"}}
{"id": "290", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "country", "researcher", "university", "location", "product", "metric", "field", "algorithm", "programming language", "organization", "conference", "task"], "instance": {"id": "290", "words": ["The", "Document", "Understanding", "Conferences", ",", "conducted", "annually", "by", "NIST", ",", "have", "developed", "sophisticated", "evaluation", "criteria", "for", "techniques", "accepting", "the", "multi-document", "summarization", "challenge", "."], "labels": ["O", "B-conference", "I-conference", "I-conference", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, researcher, university, location, product, metric, field, algorithm, programming language, organization, conference, task and O.\nSentence: The Document Understanding Conferences , conducted annually by NIST , have developed sophisticated evaluation criteria for techniques accepting the multi-document summarization challenge .", "prompt_labels": "The(O) Document(B-conference) Understanding(I-conference) Conferences(I-conference) ,(O) conducted(O) annually(O) by(O) NIST(B-organization) ,(O) have(O) developed(O) sophisticated(O) evaluation(O) criteria(O) for(O) techniques(O) accepting(O) the(O) multi-document(B-task) summarization(I-task) challenge(O) .(O)"}}
{"id": "226", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "university", "product", "researcher", "location", "person", "organization", "country", "field", "algorithm", "task", "conference", "metric"], "instance": {"id": "226", "words": ["In", "2007", ",", "at", "the", "International", "Conference", "on", "Computer", "Vision", ",", "Terzopoulos", "was", "awarded", "the", "inaugural", "IEEE", "PAMI", "Computer", "Vision", "Distinguished", "Researcher", "Award", "for", "pioneering", "and", "sustained", "research", "on", "deformable", "models", "and", "their", "applications", "."], "labels": ["O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, university, product, researcher, location, person, organization, country, field, algorithm, task, conference, metric and O.\nSentence: In 2007 , at the International Conference on Computer Vision , Terzopoulos was awarded the inaugural IEEE PAMI Computer Vision Distinguished Researcher Award for pioneering and sustained research on deformable models and their applications .", "prompt_labels": "In(O) 2007(O) ,(O) at(O) the(O) International(B-conference) Conference(I-conference) on(I-conference) Computer(I-conference) Vision(I-conference) ,(O) Terzopoulos(B-researcher) was(O) awarded(O) the(O) inaugural(O) IEEE(O) PAMI(O) Computer(O) Vision(O) Distinguished(O) Researcher(O) Award(O) for(O) pioneering(O) and(O) sustained(O) research(O) on(O) deformable(O) models(O) and(O) their(O) applications(O) .(O)"}}
{"id": "23", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "conference", "algorithm", "task", "country", "metric", "field", "location", "programming language", "person", "university", "product", "organization"], "instance": {"id": "23", "words": ["However", ",", "in", "the", "last", "years", ",", "one", "can", "observe", "appearing", "of", "different", "e-services", "and", "related", "initiatives", "in", "developing", "countries", "such", "as", "Project", "Nemmadi", ",", "MCA21", "Mission", "Mode", "Project", "or", "Digital", "India", "even", "more", ",", "in", "India", ";", "Electronic", "Government", "Directorate", "in", "Pakistan", ";", "etc", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-organization", "I-organization", "I-organization", "O", "B-country", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, conference, algorithm, task, country, metric, field, location, programming language, person, university, product, organization and O.\nSentence: However , in the last years , one can observe appearing of different e-services and related initiatives in developing countries such as Project Nemmadi , MCA21 Mission Mode Project or Digital India even more , in India ; Electronic Government Directorate in Pakistan ; etc .", "prompt_labels": "However(O) ,(O) in(O) the(O) last(O) years(O) ,(O) one(O) can(O) observe(O) appearing(O) of(O) different(O) e-services(O) and(O) related(O) initiatives(O) in(O) developing(O) countries(O) such(O) as(O) Project(O) Nemmadi(O) ,(O) MCA21(O) Mission(O) Mode(O) Project(O) or(O) Digital(O) India(O) even(O) more(O) ,(O) in(O) India(B-country) ;(O) Electronic(B-organization) Government(I-organization) Directorate(I-organization) in(O) Pakistan(B-country) ;(O) etc(O) .(O)"}}
{"id": "129", "dataset": "crossner_ai", "split": "dev", "label_list": ["country", "university", "conference", "person", "programming language", "product", "algorithm", "field", "organization", "location", "researcher", "metric", "task"], "instance": {"id": "129", "words": ["He", "was", "elected", "to", "the", "American", "Academy", "of", "Arts", "and", "Sciences", "and", "the", "National", "Academy", "of", "Sciences", "and", "has", "received", "a", "series", "of", "awards", ":"], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, conference, person, programming language, product, algorithm, field, organization, location, researcher, metric, task and O.\nSentence: He was elected to the American Academy of Arts and Sciences and the National Academy of Sciences and has received a series of awards :", "prompt_labels": "He(O) was(O) elected(O) to(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) and(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) and(O) has(O) received(O) a(O) series(O) of(O) awards(O) :(O)"}}
{"id": "326", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "field", "product", "task", "organization", "person", "metric", "conference", "programming language", "university", "location", "country", "algorithm"], "instance": {"id": "326", "words": ["In", "1969", "Victor", "Scheinman", "at", "Stanford", "University", "invented", "the", "Stanford", "arm", ",", "an", "all-electric", ",", "6-axis", "articulated", "robot", "designed", "to", "permit", "an", "arm", "solution", "."], "labels": ["O", "O", "B-researcher", "I-researcher", "O", "B-university", "I-university", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, field, product, task, organization, person, metric, conference, programming language, university, location, country, algorithm and O.\nSentence: In 1969 Victor Scheinman at Stanford University invented the Stanford arm , an all-electric , 6-axis articulated robot designed to permit an arm solution .", "prompt_labels": "In(O) 1969(O) Victor(B-researcher) Scheinman(I-researcher) at(O) Stanford(B-university) University(I-university) invented(O) the(O) Stanford(B-product) arm(I-product) ,(O) an(O) all-electric(O) ,(O) 6-axis(B-product) articulated(I-product) robot(I-product) designed(O) to(O) permit(O) an(O) arm(O) solution(O) .(O)"}}
{"id": "264", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "country", "university", "organization", "programming language", "product", "metric", "field", "location", "task", "researcher", "algorithm", "person"], "instance": {"id": "264", "words": ["In", "July", "2016", ",", "a", "collaboration", "between", "DeepMind", "and", "Moorfields", "Eye", "Hospital", "was", "announced", "to", "develop", "AI", "applications", "for", "healthcare", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, country, university, organization, programming language, product, metric, field, location, task, researcher, algorithm, person and O.\nSentence: In July 2016 , a collaboration between DeepMind and Moorfields Eye Hospital was announced to develop AI applications for healthcare .", "prompt_labels": "In(O) July(O) 2016(O) ,(O) a(O) collaboration(O) between(O) DeepMind(B-organization) and(O) Moorfields(B-organization) Eye(I-organization) Hospital(I-organization) was(O) announced(O) to(O) develop(O) AI(O) applications(O) for(O) healthcare(O) .(O)"}}
{"id": "289", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "conference", "researcher", "programming language", "location", "country", "metric", "field", "product", "person", "algorithm", "university", "organization"], "instance": {"id": "289", "words": ["But", "perceptron", "models", "were", "made", "very", "unpopular", "by", "the", "book", "Perceptrons", "by", "Marvin", "Minsky", "and", "Seymour", "Papert", ",", "published", "in", "1969", "."], "labels": ["O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, conference, researcher, programming language, location, country, metric, field, product, person, algorithm, university, organization and O.\nSentence: But perceptron models were made very unpopular by the book Perceptrons by Marvin Minsky and Seymour Papert , published in 1969 .", "prompt_labels": "But(O) perceptron(B-algorithm) models(I-algorithm) were(O) made(O) very(O) unpopular(O) by(O) the(O) book(O) Perceptrons(O) by(O) Marvin(B-researcher) Minsky(I-researcher) and(O) Seymour(B-researcher) Papert(I-researcher) ,(O) published(O) in(O) 1969(O) .(O)"}}
{"id": "178", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "programming language", "organization", "product", "algorithm", "field", "person", "country", "university", "researcher", "task", "conference", "metric"], "instance": {"id": "178", "words": ["Decision", "tree", "learning", ",", "neural", "networks", ",", "or", "a", "naive", "Bayes", "classifier", "could", "be", "used", "in", "combination", "with", "measures", "of", "model", "quality", "such", "as", "balanced", "accuracy"], "labels": ["B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, programming language, organization, product, algorithm, field, person, country, university, researcher, task, conference, metric and O.\nSentence: Decision tree learning , neural networks , or a naive Bayes classifier could be used in combination with measures of model quality such as balanced accuracy", "prompt_labels": "Decision(B-algorithm) tree(I-algorithm) learning(I-algorithm) ,(O) neural(B-algorithm) networks(I-algorithm) ,(O) or(O) a(O) naive(B-algorithm) Bayes(I-algorithm) classifier(I-algorithm) could(O) be(O) used(O) in(O) combination(O) with(O) measures(O) of(O) model(O) quality(O) such(O) as(O) balanced(B-metric) accuracy(I-metric)"}}
{"id": "234", "dataset": "crossner_ai", "split": "dev", "label_list": ["country", "product", "programming language", "task", "person", "metric", "algorithm", "organization", "field", "researcher", "conference", "university", "location"], "instance": {"id": "234", "words": ["The", "Butler", "'s", "in", "Love", ",", "a", "short", "film", "directed", "by", "David", "Arquette", "and", "starring", "Elizabeth", "Berkley", "and", "Thomas", "Jane", "was", "released", "on", "June", "23", ",", "2008", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, product, programming language, task, person, metric, algorithm, organization, field, researcher, conference, university, location and O.\nSentence: The Butler 's in Love , a short film directed by David Arquette and starring Elizabeth Berkley and Thomas Jane was released on June 23 , 2008 .", "prompt_labels": "The(O) Butler(O) 's(O) in(O) Love(O) ,(O) a(O) short(O) film(O) directed(O) by(O) David(B-person) Arquette(I-person) and(O) starring(O) Elizabeth(B-person) Berkley(I-person) and(O) Thomas(B-person) Jane(I-person) was(O) released(O) on(O) June(O) 23(O) ,(O) 2008(O) .(O)"}}
{"id": "134", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "field", "researcher", "person", "university", "organization", "metric", "conference", "task", "location", "product", "country", "algorithm"], "instance": {"id": "134", "words": ["The", "following", "MATLAB", "code", "demonstrates", "a", "concrete", "solution", "for", "solving", "the", "non-linear", "system", "of", "equations", "presented", "in", "the", "previous", "section", ":", "See", "also"], "labels": ["O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, field, researcher, person, university, organization, metric, conference, task, location, product, country, algorithm and O.\nSentence: The following MATLAB code demonstrates a concrete solution for solving the non-linear system of equations presented in the previous section : See also", "prompt_labels": "The(O) following(O) MATLAB(B-product) code(O) demonstrates(O) a(O) concrete(O) solution(O) for(O) solving(O) the(O) non-linear(O) system(O) of(O) equations(O) presented(O) in(O) the(O) previous(O) section(O) :(O) See(O) also(O)"}}
{"id": "119", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "organization", "metric", "researcher", "country", "product", "university", "field", "conference", "algorithm", "task", "location", "person"], "instance": {"id": "119", "words": ["A", "popular", "application", "for", "serial", "robots", "in", "today", "'s", "industry", "is", "the", "pick-and-place", "assembly", "robot", ",", "called", "a", "SCARA", "robot", ",", "which", "has", "four", "degrees", "of", "freedom", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-programming language", "O", "O", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, organization, metric, researcher, country, product, university, field, conference, algorithm, task, location, person and O.\nSentence: A popular application for serial robots in today 's industry is the pick-and-place assembly robot , called a SCARA robot , which has four degrees of freedom .", "prompt_labels": "A(O) popular(O) application(O) for(O) serial(O) robots(O) in(O) today(O) 's(O) industry(O) is(O) the(O) pick-and-place(O) assembly(B-programming language) robot(O) ,(O) called(O) a(O) SCARA(B-product) robot(I-product) ,(O) which(O) has(O) four(O) degrees(O) of(O) freedom(O) .(O)"}}
{"id": "307", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "metric", "location", "product", "conference", "organization", "programming language", "university", "algorithm", "task", "field", "researcher", "country"], "instance": {"id": "307", "words": ["Collaborative", "filtering", "encompasses", "techniques", "for", "matching", "people", "with", "similar", "interests", "and", "making", "recommender", "system", "on", "this", "basis", "."], "labels": ["B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, metric, location, product, conference, organization, programming language, university, algorithm, task, field, researcher, country and O.\nSentence: Collaborative filtering encompasses techniques for matching people with similar interests and making recommender system on this basis .", "prompt_labels": "Collaborative(B-algorithm) filtering(I-algorithm) encompasses(O) techniques(O) for(O) matching(O) people(O) with(O) similar(O) interests(O) and(O) making(O) recommender(B-product) system(I-product) on(O) this(O) basis(O) .(O)"}}
{"id": "141", "dataset": "crossner_ai", "split": "dev", "label_list": ["country", "field", "organization", "task", "algorithm", "metric", "person", "product", "programming language", "conference", "researcher", "university", "location"], "instance": {"id": "141", "words": ["OMR", "is", "generally", "distinguished", "from", "optical", "character", "recognition", "(", "OCR", ")", "by", "the", "fact", "that", "a", "complicated", "pattern", "recognition", "engine", "is", "not", "required", "."], "labels": ["B-task", "O", "O", "O", "O", "B-task", "I-task", "I-task", "O", "B-task", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, field, organization, task, algorithm, metric, person, product, programming language, conference, researcher, university, location and O.\nSentence: OMR is generally distinguished from optical character recognition ( OCR ) by the fact that a complicated pattern recognition engine is not required .", "prompt_labels": "OMR(B-task) is(O) generally(O) distinguished(O) from(O) optical(B-task) character(I-task) recognition(I-task) ((O) OCR(B-task) )(O) by(O) the(O) fact(O) that(O) a(O) complicated(O) pattern(B-field) recognition(I-field) engine(O) is(O) not(O) required(O) .(O)"}}
{"id": "53", "dataset": "crossner_ai", "split": "dev", "label_list": ["product", "programming language", "country", "task", "organization", "conference", "location", "metric", "person", "university", "field", "algorithm", "researcher"], "instance": {"id": "53", "words": ["He", "is", "also", "the", "President", "of", "the", "Neural", "Information", "Processing", "Systems", "Foundation", ",", "a", "non-profit", "organization", "that", "oversees", "the", "annual", "Conference", "on", "Neural", "Information", "Processing", "Systems", "Conference", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, programming language, country, task, organization, conference, location, metric, person, university, field, algorithm, researcher and O.\nSentence: He is also the President of the Neural Information Processing Systems Foundation , a non-profit organization that oversees the annual Conference on Neural Information Processing Systems Conference .", "prompt_labels": "He(O) is(O) also(O) the(O) President(O) of(O) the(O) Neural(B-organization) Information(I-organization) Processing(I-organization) Systems(I-organization) Foundation(I-organization) ,(O) a(O) non-profit(O) organization(O) that(O) oversees(O) the(O) annual(O) Conference(B-conference) on(I-conference) Neural(I-conference) Information(I-conference) Processing(I-conference) Systems(I-conference) Conference(I-conference) .(O)"}}
{"id": "39", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "organization", "algorithm", "university", "metric", "conference", "field", "product", "person", "location", "programming language", "task", "country"], "instance": {"id": "39", "words": ["General", "sampling", "from", "the", "truncated", "normal", "can", "be", "achieved", "using", "approximations", "to", "the", "normal", "CDF", "and", "the", "probit", "function", ",", "and", "R", "has", "a", "function", "codertnorm", "(", ")", "/", "code", "for", "generating", "truncated-normal", "samples", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "B-programming language", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, organization, algorithm, university, metric, conference, field, product, person, location, programming language, task, country and O.\nSentence: General sampling from the truncated normal can be achieved using approximations to the normal CDF and the probit function , and R has a function codertnorm ( ) / code for generating truncated-normal samples .", "prompt_labels": "General(O) sampling(O) from(O) the(O) truncated(O) normal(O) can(O) be(O) achieved(O) using(O) approximations(O) to(O) the(O) normal(O) CDF(B-algorithm) and(O) the(O) probit(B-algorithm) function(I-algorithm) ,(O) and(O) R(B-programming language) has(O) a(O) function(O) codertnorm(O) ((O) )(O) /(O) code(O) for(O) generating(O) truncated-normal(O) samples(O) .(O)"}}
{"id": "8", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "algorithm", "location", "product", "organization", "task", "researcher", "conference", "university", "country", "metric", "person", "field"], "instance": {"id": "8", "words": ["Representing", "words", "considering", "their", "context", "through", "fixed", "size", "dense", "vectors", "(", "word", "embedding", "s", ")", "has", "become", "one", "the", "most", "fundamental", "blocks", "in", "several", "NLP", "systems.", "an", "unsupervised", "disambiguation", "system", "uses", "the", "similarity", "between", "word", "senses", "in", "a", "fixed", "context", "window", "to", "select", "the", "most", "suitable", "word", "sense", "using", "a", "pre-trained", "word", "embedding", "model", "and", "WordNet", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, algorithm, location, product, organization, task, researcher, conference, university, country, metric, person, field and O.\nSentence: Representing words considering their context through fixed size dense vectors ( word embedding s ) has become one the most fundamental blocks in several NLP systems. an unsupervised disambiguation system uses the similarity between word senses in a fixed context window to select the most suitable word sense using a pre-trained word embedding model and WordNet .", "prompt_labels": "Representing(O) words(O) considering(O) their(O) context(O) through(O) fixed(O) size(O) dense(O) vectors(O) ((O) word(O) embedding(O) s(O) )(O) has(O) become(O) one(O) the(O) most(O) fundamental(O) blocks(O) in(O) several(O) NLP(B-field) systems.(O) an(O) unsupervised(B-product) disambiguation(I-product) system(I-product) uses(O) the(O) similarity(O) between(O) word(O) senses(O) in(O) a(O) fixed(O) context(O) window(O) to(O) select(O) the(O) most(O) suitable(O) word(O) sense(O) using(O) a(O) pre-trained(O) word(O) embedding(O) model(O) and(O) WordNet(B-product) .(O)"}}
{"id": "256", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "conference", "algorithm", "researcher", "product", "field", "organization", "country", "location", "programming language", "task", "metric", "person"], "instance": {"id": "256", "words": ["Continuing", "the", "example", "using", "the", "maximum", "likelihood", "estimator", ",", "the", "probability", "density", "function", "(", "pdf", ")", "of", "the", "noise", "for", "one", "sample", "mathw", "n", "/", "math", "is"], "labels": ["O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, conference, algorithm, researcher, product, field, organization, country, location, programming language, task, metric, person and O.\nSentence: Continuing the example using the maximum likelihood estimator , the probability density function ( pdf ) of the noise for one sample mathw n / math is", "prompt_labels": "Continuing(O) the(O) example(O) using(O) the(O) maximum(B-metric) likelihood(I-metric) estimator(I-metric) ,(O) the(O) probability(B-algorithm) density(I-algorithm) function(I-algorithm) ((O) pdf(B-algorithm) )(O) of(O) the(O) noise(O) for(O) one(O) sample(O) mathw(O) n(O) /(O) math(O) is(O)"}}
{"id": "248", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "metric", "person", "programming language", "product", "algorithm", "university", "task", "location", "organization", "researcher", "conference", "country"], "instance": {"id": "248", "words": ["When", "the", "value", "being", "predicted", "is", "continuously", "distributed", ",", "the", "mean", "squared", "error", ",", "root", "mean", "squared", "error", "or", "median", "absolute", "deviation", "could", "be", "used", "to", "summarize", "the", "errors", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "I-metric", "I-metric", "I-metric", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, metric, person, programming language, product, algorithm, university, task, location, organization, researcher, conference, country and O.\nSentence: When the value being predicted is continuously distributed , the mean squared error , root mean squared error or median absolute deviation could be used to summarize the errors .", "prompt_labels": "When(O) the(O) value(O) being(O) predicted(O) is(O) continuously(O) distributed(O) ,(O) the(O) mean(B-metric) squared(I-metric) error(I-metric) ,(O) root(B-metric) mean(I-metric) squared(I-metric) error(I-metric) or(O) median(B-metric) absolute(I-metric) deviation(I-metric) could(O) be(O) used(O) to(O) summarize(O) the(O) errors(O) .(O)"}}
{"id": "128", "dataset": "crossner_ai", "split": "dev", "label_list": ["organization", "programming language", "university", "algorithm", "task", "country", "metric", "person", "location", "researcher", "product", "field", "conference"], "instance": {"id": "128", "words": ["This", "update", "rule", "is", "in", "fact", "the", "stochastic", "gradient", "descent", "update", "for", "linear", "regression", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "B-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, programming language, university, algorithm, task, country, metric, person, location, researcher, product, field, conference and O.\nSentence: This update rule is in fact the stochastic gradient descent update for linear regression .", "prompt_labels": "This(O) update(O) rule(O) is(O) in(O) fact(O) the(O) stochastic(B-algorithm) gradient(I-algorithm) descent(I-algorithm) update(O) for(O) linear(B-algorithm) regression(I-algorithm) .(O)"}}
{"id": "279", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "university", "programming language", "person", "product", "algorithm", "task", "location", "organization", "country", "conference", "field", "metric"], "instance": {"id": "279", "words": ["Typical", "discriminative", "models", "include", "logistic", "regression", "(", "LR", ")", ",", "support", "vector", "machine", "s", "(", "SVM", ")", ",", "conditional", "random", "fields", "(", "CRFs", ")", "(", "specified", "over", "an", "undirected", "graph", ")", ",", "decision", "trees", ",", "neural", "networks", ",", "and", "many", "others", "."], "labels": ["O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "B-algorithm", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "B-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, university, programming language, person, product, algorithm, task, location, organization, country, conference, field, metric and O.\nSentence: Typical discriminative models include logistic regression ( LR ) , support vector machine s ( SVM ) , conditional random fields ( CRFs ) ( specified over an undirected graph ) , decision trees , neural networks , and many others .", "prompt_labels": "Typical(O) discriminative(O) models(O) include(O) logistic(B-algorithm) regression(I-algorithm) ((O) LR(B-algorithm) )(O) ,(O) support(B-algorithm) vector(I-algorithm) machine(I-algorithm) s(O) ((O) SVM(B-algorithm) )(O) ,(O) conditional(B-algorithm) random(I-algorithm) fields(I-algorithm) ((O) CRFs(B-algorithm) )(O) ((O) specified(O) over(O) an(O) undirected(B-algorithm) graph(I-algorithm) )(O) ,(O) decision(B-algorithm) trees(I-algorithm) ,(O) neural(B-algorithm) networks(I-algorithm) ,(O) and(O) many(O) others(O) .(O)"}}
{"id": "7", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "metric", "task", "location", "programming language", "field", "conference", "researcher", "product", "organization", "university", "country", "algorithm"], "instance": {"id": "7", "words": ["Since", "the", "Google", "acquisition", ",", "the", "company", "has", "notched", "up", "a", "number", "of", "significant", "achievements", ",", "perhaps", "the", "most", "notable", "being", "the", "creation", "of", "AlphaGo", ",", "a", "program", "that", "defeated", "world", "champion", "Lee", "Sedol", "at", "the", "complex", "game", "of", "Go", "."], "labels": ["O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, metric, task, location, programming language, field, conference, researcher, product, organization, university, country, algorithm and O.\nSentence: Since the Google acquisition , the company has notched up a number of significant achievements , perhaps the most notable being the creation of AlphaGo , a program that defeated world champion Lee Sedol at the complex game of Go .", "prompt_labels": "Since(O) the(O) Google(B-organization) acquisition(O) ,(O) the(O) company(O) has(O) notched(O) up(O) a(O) number(O) of(O) significant(O) achievements(O) ,(O) perhaps(O) the(O) most(O) notable(O) being(O) the(O) creation(O) of(O) AlphaGo(B-product) ,(O) a(O) program(O) that(O) defeated(O) world(O) champion(O) Lee(B-person) Sedol(I-person) at(O) the(O) complex(O) game(O) of(O) Go(O) .(O)"}}
{"id": "206", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "product", "country", "field", "researcher", "metric", "organization", "algorithm", "university", "person", "location", "programming language", "task"], "instance": {"id": "206", "words": ["In", "2000", ",", "she", "was", "elected", "as", "a", "Fellow", "of", "the", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, product, country, field, researcher, metric, organization, algorithm, university, person, location, programming language, task and O.\nSentence: In 2000 , she was elected as a Fellow of the Association for the Advancement of Artificial Intelligence .", "prompt_labels": "In(O) 2000(O) ,(O) she(O) was(O) elected(O) as(O) a(O) Fellow(O) of(O) the(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) .(O)"}}
{"id": "79", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "field", "country", "researcher", "algorithm", "organization", "university", "location", "conference", "programming language", "product", "person", "task"], "instance": {"id": "79", "words": ["Several", "metrics", "use", "WordNet", ",", "a", "manually", "constructed", "lexical", "database", "of", "English", "words", "."], "labels": ["O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, field, country, researcher, algorithm, organization, university, location, conference, programming language, product, person, task and O.\nSentence: Several metrics use WordNet , a manually constructed lexical database of English words .", "prompt_labels": "Several(O) metrics(O) use(O) WordNet(B-product) ,(O) a(O) manually(O) constructed(O) lexical(O) database(O) of(O) English(O) words(O) .(O)"}}
{"id": "149", "dataset": "crossner_ai", "split": "dev", "label_list": ["country", "person", "metric", "conference", "product", "task", "algorithm", "programming language", "organization", "location", "researcher", "field", "university"], "instance": {"id": "149", "words": ["Since", "paraphrase", "recognition", "can", "be", "posed", "as", "a", "classification", "problem", ",", "most", "standard", "evaluations", "metrics", "such", "as", "accuracy", ",", "f1", "score", ",", "or", "an", "ROC", "curve", "do", "relatively", "well", "."], "labels": ["O", "B-task", "I-task", "O", "O", "O", "O", "O", "B-task", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "B-metric", "I-metric", "O", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, metric, conference, product, task, algorithm, programming language, organization, location, researcher, field, university and O.\nSentence: Since paraphrase recognition can be posed as a classification problem , most standard evaluations metrics such as accuracy , f1 score , or an ROC curve do relatively well .", "prompt_labels": "Since(O) paraphrase(B-task) recognition(I-task) can(O) be(O) posed(O) as(O) a(O) classification(B-task) problem(O) ,(O) most(O) standard(O) evaluations(O) metrics(O) such(O) as(O) accuracy(B-metric) ,(O) f1(B-metric) score(I-metric) ,(O) or(O) an(O) ROC(B-metric) curve(I-metric) do(O) relatively(O) well(O) .(O)"}}
{"id": "17", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "country", "task", "researcher", "organization", "product", "person", "programming language", "field", "metric", "conference", "algorithm", "location"], "instance": {"id": "17", "words": ["Today", ",", "the", "field", "has", "become", "even", "more", "daunting", "and", "complex", "with", "the", "addition", "of", "circuit", ",", "systems", "and", "signal", "analysis", "and", "design", "languages", "and", "software", ",", "from", "MATLAB", "and", "Simulink", "to", "NumPy", ",", "VHDL", ",", "PSpice", ",", "Verilog", "and", "even", "Assembly", "language", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "B-product", "O", "B-product", "O", "B-product", "O", "B-product", "O", "B-product", "O", "O", "B-programming language", "I-programming language", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, country, task, researcher, organization, product, person, programming language, field, metric, conference, algorithm, location and O.\nSentence: Today , the field has become even more daunting and complex with the addition of circuit , systems and signal analysis and design languages and software , from MATLAB and Simulink to NumPy , VHDL , PSpice , Verilog and even Assembly language .", "prompt_labels": "Today(O) ,(O) the(O) field(O) has(O) become(O) even(O) more(O) daunting(O) and(O) complex(O) with(O) the(O) addition(O) of(O) circuit(O) ,(O) systems(O) and(O) signal(O) analysis(O) and(O) design(O) languages(O) and(O) software(O) ,(O) from(O) MATLAB(B-product) and(O) Simulink(B-product) to(O) NumPy(B-product) ,(O) VHDL(B-product) ,(O) PSpice(B-product) ,(O) Verilog(B-product) and(O) even(O) Assembly(B-programming language) language(I-programming language) .(O)"}}
{"id": "282", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "organization", "university", "person", "product", "algorithm", "conference", "researcher", "task", "programming language", "country", "metric", "location"], "instance": {"id": "282", "words": ["In", "practice", ",", "machine", "learning", "algorithms", "cope", "with", "that", "either", "by", "employing", "a", "convex", "approximation", "to", "the", "0-1", "loss", "function", "(", "like", "hinge", "loss", "for", "Support", "vector", "machine", ")", ",", "which", "is", "easier", "to", "optimize", ",", "or", "by", "imposing", "assumptions", "on", "the", "distribution", "mathP", "(", "x", ",", "y", ")", "/", "math", "(", "and", "thus", "stop", "being", "agnostic", "learning", "algorithms", "to", "which", "the", "above", "result", "applies", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, organization, university, person, product, algorithm, conference, researcher, task, programming language, country, metric, location and O.\nSentence: In practice , machine learning algorithms cope with that either by employing a convex approximation to the 0-1 loss function ( like hinge loss for Support vector machine ) , which is easier to optimize , or by imposing assumptions on the distribution mathP ( x , y ) / math ( and thus stop being agnostic learning algorithms to which the above result applies ) .", "prompt_labels": "In(O) practice(O) ,(O) machine(O) learning(O) algorithms(O) cope(O) with(O) that(O) either(O) by(O) employing(O) a(O) convex(B-algorithm) approximation(I-algorithm) to(O) the(O) 0-1(O) loss(O) function(O) ((O) like(O) hinge(B-metric) loss(I-metric) for(O) Support(B-algorithm) vector(I-algorithm) machine(I-algorithm) )(O) ,(O) which(O) is(O) easier(O) to(O) optimize(O) ,(O) or(O) by(O) imposing(O) assumptions(O) on(O) the(O) distribution(O) mathP(O) ((O) x(O) ,(O) y(O) )(O) /(O) math(O) ((O) and(O) thus(O) stop(O) being(O) agnostic(O) learning(O) algorithms(O) to(O) which(O) the(O) above(O) result(O) applies(O) )(O) .(O)"}}
{"id": "29", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "field", "algorithm", "metric", "task", "product", "country", "university", "location", "conference", "researcher", "organization", "programming language"], "instance": {"id": "29", "words": ["The", "EM", "algorithm", "is", "used", "to", "find", "(", "local", ")", "maximum", "likelihood", "parameters", "of", "a", "statistical", "model", "in", "cases", "where", "the", "equations", "cannot", "be", "solved", "directly", "."], "labels": ["O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, field, algorithm, metric, task, product, country, university, location, conference, researcher, organization, programming language and O.\nSentence: The EM algorithm is used to find ( local ) maximum likelihood parameters of a statistical model in cases where the equations cannot be solved directly .", "prompt_labels": "The(O) EM(B-algorithm) algorithm(I-algorithm) is(O) used(O) to(O) find(O) ((O) local(O) )(O) maximum(B-metric) likelihood(I-metric) parameters(O) of(O) a(O) statistical(O) model(O) in(O) cases(O) where(O) the(O) equations(O) cannot(O) be(O) solved(O) directly(O) .(O)"}}
{"id": "133", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "programming language", "organization", "product", "university", "field", "conference", "country", "task", "researcher", "algorithm", "person", "metric"], "instance": {"id": "133", "words": ["Sycara", "is", "a", "Fellow", "of", "Institute", "of", "Electrical", "and", "Electronics", "Engineers", "(", "IEEE", ")", ",", "and", "a", "Fellow", "of", "American", "Association", "for", "Artificial", "Intelligence", "(", "AAAI", ")", "."], "labels": ["B-researcher", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, programming language, organization, product, university, field, conference, country, task, researcher, algorithm, person, metric and O.\nSentence: Sycara is a Fellow of Institute of Electrical and Electronics Engineers ( IEEE ) , and a Fellow of American Association for Artificial Intelligence ( AAAI ) .", "prompt_labels": "Sycara(B-researcher) is(O) a(O) Fellow(O) of(O) Institute(B-organization) of(I-organization) Electrical(I-organization) and(I-organization) Electronics(I-organization) Engineers(I-organization) ((O) IEEE(B-organization) )(O) ,(O) and(O) a(O) Fellow(O) of(O) American(B-conference) Association(I-conference) for(I-conference) Artificial(I-conference) Intelligence(I-conference) ((O) AAAI(B-conference) )(O) .(O)"}}
{"id": "116", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "location", "country", "metric", "conference", "programming language", "person", "field", "algorithm", "researcher", "product", "organization", "task"], "instance": {"id": "116", "words": ["LSTM", "was", "proposed", "in", "1997", "by", "Sepp", "Hochreiter", "and", "Jürgen", "Schmidhuber", "."], "labels": ["B-algorithm", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, location, country, metric, conference, programming language, person, field, algorithm, researcher, product, organization, task and O.\nSentence: LSTM was proposed in 1997 by Sepp Hochreiter and Jürgen Schmidhuber .", "prompt_labels": "LSTM(B-algorithm) was(O) proposed(O) in(O) 1997(O) by(O) Sepp(B-researcher) Hochreiter(I-researcher) and(O) Jürgen(B-researcher) Schmidhuber(I-researcher) .(O)"}}
{"id": "77", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "person", "university", "algorithm", "task", "country", "conference", "metric", "product", "organization", "researcher", "programming language", "location"], "instance": {"id": "77", "words": ["The", "screenplay", "by", "Hampton", "Fancher", "!", "--", "Not", "titled", "Android", "initially", "-", "See", "Sammon", ",", "pp.", "32", "and", "38", "for", "explanation", "--", "was", "optioned", "in", "1977", ".", "Sammon", ",", "pp.", "23-30", "Producer", "Michael", "Deeley", "became", "interested", "in", "Fancher", "'s", "draft", "and", "convinced", "director", "Ridley", "Scott", "to", "film", "it", "."], "labels": ["O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-product", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, person, university, algorithm, task, country, conference, metric, product, organization, researcher, programming language, location and O.\nSentence: The screenplay by Hampton Fancher ! -- Not titled Android initially - See Sammon , pp. 32 and 38 for explanation -- was optioned in 1977 . Sammon , pp. 23-30 Producer Michael Deeley became interested in Fancher 's draft and convinced director Ridley Scott to film it .", "prompt_labels": "The(O) screenplay(O) by(O) Hampton(B-person) Fancher(I-person) !(O) --(O) Not(O) titled(O) Android(B-product) initially(O) -(O) See(O) Sammon(B-person) ,(O) pp.(O) 32(O) and(O) 38(O) for(O) explanation(O) --(O) was(O) optioned(O) in(O) 1977(O) .(O) Sammon(B-person) ,(O) pp.(O) 23-30(O) Producer(O) Michael(B-person) Deeley(I-person) became(O) interested(O) in(O) Fancher(B-person) 's(O) draft(O) and(O) convinced(O) director(O) Ridley(B-person) Scott(I-person) to(O) film(O) it(O) .(O)"}}
{"id": "195", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "metric", "country", "researcher", "product", "algorithm", "location", "person", "field", "task", "organization", "conference", "programming language"], "instance": {"id": "195", "words": ["The", "field", "of", "spoken", "dialog", "systems", "is", "quite", "large", "and", "includes", "research", "(", "featured", "at", "scientific", "conferences", "such", "as", "SIGdial", "and", "Interspeech", ")", "and", "a", "large", "industrial", "sector", "(", "with", "its", "own", "meetings", "such", "as", "SpeechTek", "and", "AVIOS", ")", "."], "labels": ["O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "O", "B-conference", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "O", "B-conference", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, metric, country, researcher, product, algorithm, location, person, field, task, organization, conference, programming language and O.\nSentence: The field of spoken dialog systems is quite large and includes research ( featured at scientific conferences such as SIGdial and Interspeech ) and a large industrial sector ( with its own meetings such as SpeechTek and AVIOS ) .", "prompt_labels": "The(O) field(O) of(O) spoken(B-product) dialog(I-product) systems(I-product) is(O) quite(O) large(O) and(O) includes(O) research(O) ((O) featured(O) at(O) scientific(O) conferences(O) such(O) as(O) SIGdial(B-conference) and(O) Interspeech(B-conference) )(O) and(O) a(O) large(O) industrial(O) sector(O) ((O) with(O) its(O) own(O) meetings(O) such(O) as(O) SpeechTek(B-conference) and(O) AVIOS(B-conference) )(O) .(O)"}}
{"id": "111", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "product", "location", "country", "conference", "algorithm", "metric", "university", "task", "researcher", "organization", "person", "field"], "instance": {"id": "111", "words": ["At", "the", "2018", "Conference", "on", "Neural", "Information", "Processing", "Systems", "(", "NeurIPS", ")", "researchers", "from", "Google", "presented", "the", "work", "."], "labels": ["O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O", "O", "B-organization", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, product, location, country, conference, algorithm, metric, university, task, researcher, organization, person, field and O.\nSentence: At the 2018 Conference on Neural Information Processing Systems ( NeurIPS ) researchers from Google presented the work .", "prompt_labels": "At(O) the(O) 2018(B-conference) Conference(I-conference) on(I-conference) Neural(I-conference) Information(I-conference) Processing(I-conference) Systems(I-conference) ((O) NeurIPS(B-conference) )(O) researchers(O) from(O) Google(B-organization) presented(O) the(O) work(O) .(O)"}}
{"id": "243", "dataset": "crossner_ai", "split": "dev", "label_list": ["product", "field", "programming language", "conference", "university", "metric", "algorithm", "researcher", "country", "person", "organization", "task", "location"], "instance": {"id": "243", "words": ["A", "recommender", "system", "aims", "to", "predict", "the", "preference", "for", "an", "item", "of", "a", "target", "user", "."], "labels": ["O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, field, programming language, conference, university, metric, algorithm, researcher, country, person, organization, task, location and O.\nSentence: A recommender system aims to predict the preference for an item of a target user .", "prompt_labels": "A(O) recommender(B-product) system(I-product) aims(O) to(O) predict(O) the(O) preference(O) for(O) an(O) item(O) of(O) a(O) target(O) user(O) .(O)"}}
{"id": "82", "dataset": "crossner_ai", "split": "dev", "label_list": ["organization", "field", "task", "researcher", "location", "country", "product", "programming language", "metric", "algorithm", "conference", "person", "university"], "instance": {"id": "82", "words": ["Researchers", "have", "attempted", "a", "number", "of", "methods", "such", "as", "optical", "flow", ",", "Kalman", "filtering", ",", "Hidden", "Markov", "model", "s", ",", "etc", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, field, task, researcher, location, country, product, programming language, metric, algorithm, conference, person, university and O.\nSentence: Researchers have attempted a number of methods such as optical flow , Kalman filtering , Hidden Markov model s , etc .", "prompt_labels": "Researchers(O) have(O) attempted(O) a(O) number(O) of(O) methods(O) such(O) as(O) optical(B-algorithm) flow(I-algorithm) ,(O) Kalman(B-algorithm) filtering(I-algorithm) ,(O) Hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) s(O) ,(O) etc(O) .(O)"}}
{"id": "198", "dataset": "crossner_ai", "split": "dev", "label_list": ["product", "person", "conference", "task", "field", "programming language", "country", "organization", "location", "university", "researcher", "algorithm", "metric"], "instance": {"id": "198", "words": ["More", "exotic", "fitness", "functions", "that", "explore", "model", "granularity", "include", "the", "area", "under", "the", "ROC", "curve", "and", "rank", "measure", "."], "labels": ["O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, person, conference, task, field, programming language, country, organization, location, university, researcher, algorithm, metric and O.\nSentence: More exotic fitness functions that explore model granularity include the area under the ROC curve and rank measure .", "prompt_labels": "More(O) exotic(B-algorithm) fitness(I-algorithm) functions(I-algorithm) that(O) explore(O) model(O) granularity(O) include(O) the(O) area(O) under(O) the(O) ROC(B-metric) curve(I-metric) and(O) rank(O) measure(O) .(O)"}}
{"id": "170", "dataset": "crossner_ai", "split": "dev", "label_list": ["product", "university", "person", "field", "organization", "country", "programming language", "algorithm", "task", "metric", "researcher", "location", "conference"], "instance": {"id": "170", "words": ["A", "guest", "editor", "for", "that", "issue", "will", "be", "David", "'s", "former", "colleague", "at", "NIST", ",", "Judah", "Levine", "who", "is", "the", "most", "recent", "recipient", "of", "the", "I.", "I.", "Rabi", "Award", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "O", "O", "O", "O", "B-organization", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, university, person, field, organization, country, programming language, algorithm, task, metric, researcher, location, conference and O.\nSentence: A guest editor for that issue will be David 's former colleague at NIST , Judah Levine who is the most recent recipient of the I. I. Rabi Award .", "prompt_labels": "A(O) guest(O) editor(O) for(O) that(O) issue(O) will(O) be(O) David(B-researcher) 's(O) former(O) colleague(O) at(O) NIST(B-organization) ,(O) Judah(B-researcher) Levine(I-researcher) who(O) is(O) the(O) most(O) recent(O) recipient(O) of(O) the(O) I.(O) I.(O) Rabi(O) Award(O) .(O)"}}
{"id": "14", "dataset": "crossner_ai", "split": "dev", "label_list": ["product", "programming language", "person", "organization", "researcher", "algorithm", "field", "conference", "task", "country", "metric", "location", "university"], "instance": {"id": "14", "words": ["It", "includes", "an", "upper", "ontology", ",", "created", "by", "the", "IEEE", "working", "group", "P1600.1", "(", "originally", "by", "Ian", "Niles", "and", "Adam", "Pease", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, programming language, person, organization, researcher, algorithm, field, conference, task, country, metric, location, university and O.\nSentence: It includes an upper ontology , created by the IEEE working group P1600.1 ( originally by Ian Niles and Adam Pease ) .", "prompt_labels": "It(O) includes(O) an(O) upper(O) ontology(O) ,(O) created(O) by(O) the(O) IEEE(B-organization) working(O) group(O) P1600.1(O) ((O) originally(O) by(O) Ian(B-researcher) Niles(I-researcher) and(O) Adam(B-researcher) Pease(I-researcher) )(O) .(O)"}}
{"id": "253", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "researcher", "person", "conference", "metric", "university", "product", "location", "programming language", "country", "task", "organization", "algorithm"], "instance": {"id": "253", "words": ["These", "methods", "may", "also", "analyze", "a", "program", "'s", "output", "and", "its", "usefulness", "and", "therefore", "may", "involve", "the", "analysis", "of", "its", "confusion", "matrix", "(", "or", "table", "of", "confusion", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, researcher, person, conference, metric, university, product, location, programming language, country, task, organization, algorithm and O.\nSentence: These methods may also analyze a program 's output and its usefulness and therefore may involve the analysis of its confusion matrix ( or table of confusion ) .", "prompt_labels": "These(O) methods(O) may(O) also(O) analyze(O) a(O) program(O) 's(O) output(O) and(O) its(O) usefulness(O) and(O) therefore(O) may(O) involve(O) the(O) analysis(O) of(O) its(O) confusion(B-metric) matrix(I-metric) ((O) or(O) table(B-metric) of(I-metric) confusion(I-metric) )(O) .(O)"}}
{"id": "96", "dataset": "crossner_ai", "split": "dev", "label_list": ["organization", "location", "university", "programming language", "country", "algorithm", "person", "researcher", "product", "task", "metric", "field", "conference"], "instance": {"id": "96", "words": ["Evolutionary", "programming", "was", "introduced", "by", "Lawrence", "J.", "Fogel", "in", "the", "US", ",", "while", "John", "Henry", "Holland", "called", "his", "method", "a", "genetic", "algorithm", "."], "labels": ["B-algorithm", "I-algorithm", "O", "O", "O", "B-researcher", "I-researcher", "I-researcher", "O", "O", "B-country", "O", "O", "B-researcher", "I-researcher", "I-researcher", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, university, programming language, country, algorithm, person, researcher, product, task, metric, field, conference and O.\nSentence: Evolutionary programming was introduced by Lawrence J. Fogel in the US , while John Henry Holland called his method a genetic algorithm .", "prompt_labels": "Evolutionary(B-algorithm) programming(I-algorithm) was(O) introduced(O) by(O) Lawrence(B-researcher) J.(I-researcher) Fogel(I-researcher) in(O) the(O) US(B-country) ,(O) while(O) John(B-researcher) Henry(I-researcher) Holland(I-researcher) called(O) his(O) method(O) a(O) genetic(B-algorithm) algorithm(I-algorithm) .(O)"}}
{"id": "176", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "product", "task", "researcher", "country", "algorithm", "conference", "field", "programming language", "university", "metric", "person", "organization"], "instance": {"id": "176", "words": ["Generally", "speaking", "all", "learning", "displays", "incremental", "change", "over", "time", ",", "but", "describes", "an", "Sigmoid", "function", "which", "has", "different", "appearances", "depending", "on", "the", "time", "scale", "of", "observation", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, product, task, researcher, country, algorithm, conference, field, programming language, university, metric, person, organization and O.\nSentence: Generally speaking all learning displays incremental change over time , but describes an Sigmoid function which has different appearances depending on the time scale of observation .", "prompt_labels": "Generally(O) speaking(O) all(O) learning(O) displays(O) incremental(O) change(O) over(O) time(O) ,(O) but(O) describes(O) an(O) Sigmoid(B-algorithm) function(I-algorithm) which(O) has(O) different(O) appearances(O) depending(O) on(O) the(O) time(O) scale(O) of(O) observation(O) .(O)"}}
{"id": "57", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "location", "person", "organization", "algorithm", "task", "country", "conference", "field", "university", "researcher", "product", "metric"], "instance": {"id": "57", "words": ["Apertium", "is", "a", "shallow-transfer", "machine", "translation", "system", ",", "which", "uses", "finite", "state", "transducer", "s", "for", "all", "of", "its", "lexical", "transformations", ",", "and", "hidden", "Markov", "model", "s", "for", "part-of-speech", "tagging", "or", "word", "category", "disambiguation", "."], "labels": ["B-product", "O", "O", "B-product", "I-product", "I-product", "I-product", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, location, person, organization, algorithm, task, country, conference, field, university, researcher, product, metric and O.\nSentence: Apertium is a shallow-transfer machine translation system , which uses finite state transducer s for all of its lexical transformations , and hidden Markov model s for part-of-speech tagging or word category disambiguation .", "prompt_labels": "Apertium(B-product) is(O) a(O) shallow-transfer(B-product) machine(I-product) translation(I-product) system(I-product) ,(O) which(O) uses(O) finite(B-algorithm) state(I-algorithm) transducer(I-algorithm) s(O) for(O) all(O) of(O) its(O) lexical(O) transformations(O) ,(O) and(O) hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) s(O) for(O) part-of-speech(B-task) tagging(I-task) or(O) word(B-task) category(I-task) disambiguation(I-task) .(O)"}}
{"id": "40", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "field", "metric", "person", "product", "country", "algorithm", "researcher", "task", "organization", "location", "university", "conference"], "instance": {"id": "40", "words": ["He", "has", "also", "received", "honorary", "doctorates", "from", "the", "universities", "of", "Newcastle", ",", "Surrey", ",", "Tel", "Aviv", "University", ",", ",", "Simon", "Fraser", "University", "and", "the", "University", "of", "Tromsø", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "B-university", "O", "B-university", "I-university", "I-university", "O", "O", "B-university", "I-university", "I-university", "O", "O", "B-university", "I-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, field, metric, person, product, country, algorithm, researcher, task, organization, location, university, conference and O.\nSentence: He has also received honorary doctorates from the universities of Newcastle , Surrey , Tel Aviv University , , Simon Fraser University and the University of Tromsø .", "prompt_labels": "He(O) has(O) also(O) received(O) honorary(O) doctorates(O) from(O) the(O) universities(B-university) of(I-university) Newcastle(I-university) ,(O) Surrey(B-university) ,(O) Tel(B-university) Aviv(I-university) University(I-university) ,(O) ,(O) Simon(B-university) Fraser(I-university) University(I-university) and(O) the(O) University(B-university) of(I-university) Tromsø(I-university) .(O)"}}
{"id": "44", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "university", "location", "organization", "algorithm", "metric", "researcher", "country", "programming language", "product", "conference", "task", "field"], "instance": {"id": "44", "words": ["Two", "professors", ",", "Hal", "Abelson", "and", "Gerald", "Jay", "Sussman", ",", "chose", "to", "remain", "neutral", "-", "their", "group", "was", "referred", "to", "variously", "as", "Switzerland", "and", "Project", "MAC", "for", "the", "next", "30", "years", "."], "labels": ["O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, university, location, organization, algorithm, metric, researcher, country, programming language, product, conference, task, field and O.\nSentence: Two professors , Hal Abelson and Gerald Jay Sussman , chose to remain neutral - their group was referred to variously as Switzerland and Project MAC for the next 30 years .", "prompt_labels": "Two(O) professors(O) ,(O) Hal(B-researcher) Abelson(I-researcher) and(O) Gerald(B-researcher) Jay(I-researcher) Sussman(I-researcher) ,(O) chose(O) to(O) remain(O) neutral(O) -(O) their(O) group(O) was(O) referred(O) to(O) variously(O) as(O) Switzerland(B-country) and(O) Project(O) MAC(O) for(O) the(O) next(O) 30(O) years(O) .(O)"}}
{"id": "46", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "country", "field", "person", "programming language", "conference", "location", "product", "researcher", "metric", "university", "task", "organization"], "instance": {"id": "46", "words": ["Subsequent", "works", "focused", "on", "addressing", "these", "problems", ",", "but", "it", "was", "not", "until", "the", "advent", "of", "the", "modern", "computer", "and", "the", "popularisation", "of", "Maximum", "Likelihood", "(", "MLE", ")", "parameterisation", "techniques", "that", "research", "really", "took", "off", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, country, field, person, programming language, conference, location, product, researcher, metric, university, task, organization and O.\nSentence: Subsequent works focused on addressing these problems , but it was not until the advent of the modern computer and the popularisation of Maximum Likelihood ( MLE ) parameterisation techniques that research really took off .", "prompt_labels": "Subsequent(O) works(O) focused(O) on(O) addressing(O) these(O) problems(O) ,(O) but(O) it(O) was(O) not(O) until(O) the(O) advent(O) of(O) the(O) modern(O) computer(O) and(O) the(O) popularisation(O) of(O) Maximum(B-metric) Likelihood(I-metric) ((O) MLE(B-metric) )(O) parameterisation(O) techniques(O) that(O) research(O) really(O) took(O) off(O) .(O)"}}
{"id": "38", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "algorithm", "location", "field", "product", "programming language", "task", "organization", "metric", "person", "country", "researcher", "university"], "instance": {"id": "38", "words": ["Image", "segmentation", "using", "k-means", "clustering", "algorithms", "has", "long", "been", "used", "for", "pattern", "recognition", ",", "object", "detection", ",", "and", "medical", "imaging", "."], "labels": ["B-task", "I-task", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-task", "I-task", "O", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, algorithm, location, field, product, programming language, task, organization, metric, person, country, researcher, university and O.\nSentence: Image segmentation using k-means clustering algorithms has long been used for pattern recognition , object detection , and medical imaging .", "prompt_labels": "Image(B-task) segmentation(I-task) using(O) k-means(B-algorithm) clustering(I-algorithm) algorithms(I-algorithm) has(O) long(O) been(O) used(O) for(O) pattern(B-field) recognition(I-field) ,(O) object(B-task) detection(I-task) ,(O) and(O) medical(B-field) imaging(I-field) .(O)"}}
{"id": "347", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "person", "conference", "researcher", "organization", "product", "location", "algorithm", "field", "task", "metric", "university", "country"], "instance": {"id": "347", "words": ["Other", "ways", "anomalous", "propagation", "is", "recorded", "is", "by", "troposcatter", "s", "causing", "irregularities", "in", "the", "troposphere", ",", "scattering", "due", "to", "meteor", "s", ",", "refraction", "in", "the", "ionized", "regions", "and", "layers", "of", "the", "ionosphere", ",", "and", "reflection", "from", "the", "ionosphere", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, person, conference, researcher, organization, product, location, algorithm, field, task, metric, university, country and O.\nSentence: Other ways anomalous propagation is recorded is by troposcatter s causing irregularities in the troposphere , scattering due to meteor s , refraction in the ionized regions and layers of the ionosphere , and reflection from the ionosphere .", "prompt_labels": "Other(O) ways(O) anomalous(O) propagation(O) is(O) recorded(O) is(O) by(O) troposcatter(O) s(O) causing(O) irregularities(O) in(O) the(O) troposphere(O) ,(O) scattering(O) due(O) to(O) meteor(O) s(O) ,(O) refraction(O) in(O) the(O) ionized(O) regions(O) and(O) layers(O) of(O) the(O) ionosphere(O) ,(O) and(O) reflection(O) from(O) the(O) ionosphere(O) .(O)"}}
{"id": "144", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "researcher", "organization", "person", "algorithm", "location", "programming language", "country", "task", "field", "conference", "university", "product"], "instance": {"id": "144", "words": ["Two", "examples", "of", "popular", "parallel", "robots", "are", "the", "Stewart", "platform", "and", "the", "Delta", "robot", "."], "labels": ["O", "O", "O", "O", "B-product", "I-product", "O", "O", "B-product", "I-product", "O", "O", "B-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, researcher, organization, person, algorithm, location, programming language, country, task, field, conference, university, product and O.\nSentence: Two examples of popular parallel robots are the Stewart platform and the Delta robot .", "prompt_labels": "Two(O) examples(O) of(O) popular(O) parallel(B-product) robots(I-product) are(O) the(O) Stewart(B-product) platform(I-product) and(O) the(O) Delta(B-product) robot(I-product) .(O)"}}
{"id": "86", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "task", "field", "conference", "country", "product", "algorithm", "programming language", "organization", "researcher", "metric", "person", "university"], "instance": {"id": "86", "words": ["A", "collaborative", "robot", "or", "cobot", "is", "a", "robot", "that", "can", "safely", "and", "effectively", "interact", "with", "human", "workers", "while", "performing", "simple", "industrial", "tasks", "."], "labels": ["O", "B-product", "I-product", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, task, field, conference, country, product, algorithm, programming language, organization, researcher, metric, person, university and O.\nSentence: A collaborative robot or cobot is a robot that can safely and effectively interact with human workers while performing simple industrial tasks .", "prompt_labels": "A(O) collaborative(B-product) robot(I-product) or(O) cobot(B-product) is(O) a(O) robot(O) that(O) can(O) safely(O) and(O) effectively(O) interact(O) with(O) human(O) workers(O) while(O) performing(O) simple(O) industrial(O) tasks(O) .(O)"}}
{"id": "189", "dataset": "crossner_ai", "split": "dev", "label_list": ["organization", "task", "researcher", "person", "product", "algorithm", "conference", "field", "university", "programming language", "metric", "country", "location"], "instance": {"id": "189", "words": ["It", "involves", "the", "fields", "of", "computer", "vision", "or", "machine", "vision", ",", "and", "medical", "imaging", ",", "and", "makes", "heavy", "use", "of", "pattern", "recognition", ",", "digital", "geometry", ",", "and", "signal", "processing", "."], "labels": ["O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, task, researcher, person, product, algorithm, conference, field, university, programming language, metric, country, location and O.\nSentence: It involves the fields of computer vision or machine vision , and medical imaging , and makes heavy use of pattern recognition , digital geometry , and signal processing .", "prompt_labels": "It(O) involves(O) the(O) fields(O) of(O) computer(B-field) vision(I-field) or(O) machine(B-field) vision(I-field) ,(O) and(O) medical(B-field) imaging(I-field) ,(O) and(O) makes(O) heavy(O) use(O) of(O) pattern(B-field) recognition(I-field) ,(O) digital(B-field) geometry(I-field) ,(O) and(O) signal(B-field) processing(I-field) .(O)"}}
{"id": "110", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "location", "field", "person", "task", "programming language", "metric", "organization", "university", "product", "country", "researcher", "algorithm"], "instance": {"id": "110", "words": ["He", "subsequently", "extended", "an", "invitation", "for", "Wydner", "to", "attend", "the", "annual", "meeting", "of", "the", "American", "Translators", "Association", "that", "following", "October", "where", "the", "Weidner", "Machine", "Translation", "System", "hailed", "a", "hoped-for", "breakthrough", "in", "machine", "translation", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-researcher", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "I-product", "O", "O", "O", "O", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, location, field, person, task, programming language, metric, organization, university, product, country, researcher, algorithm and O.\nSentence: He subsequently extended an invitation for Wydner to attend the annual meeting of the American Translators Association that following October where the Weidner Machine Translation System hailed a hoped-for breakthrough in machine translation .", "prompt_labels": "He(O) subsequently(O) extended(O) an(O) invitation(O) for(O) Wydner(B-researcher) to(O) attend(O) the(O) annual(O) meeting(O) of(O) the(O) American(B-organization) Translators(I-organization) Association(I-organization) that(O) following(O) October(O) where(O) the(O) Weidner(B-product) Machine(I-product) Translation(I-product) System(I-product) hailed(O) a(O) hoped-for(O) breakthrough(O) in(O) machine(B-task) translation(I-task) .(O)"}}
{"id": "340", "dataset": "crossner_ai", "split": "dev", "label_list": ["organization", "person", "researcher", "location", "country", "university", "algorithm", "conference", "metric", "product", "programming language", "field", "task"], "instance": {"id": "340", "words": ["There", "are", "bindings", "in", "Python", ",", "Java", "and", "MATLAB", "/", "OCTAVE", "."], "labels": ["O", "O", "O", "O", "B-programming language", "O", "B-programming language", "O", "B-product", "O", "B-programming language", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, researcher, location, country, university, algorithm, conference, metric, product, programming language, field, task and O.\nSentence: There are bindings in Python , Java and MATLAB / OCTAVE .", "prompt_labels": "There(O) are(O) bindings(O) in(O) Python(B-programming language) ,(O) Java(B-programming language) and(O) MATLAB(B-product) /(O) OCTAVE(B-programming language) .(O)"}}
{"id": "66", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "programming language", "person", "conference", "location", "field", "university", "organization", "metric", "researcher", "task", "product", "country"], "instance": {"id": "66", "words": ["An", "updated", "measurement", "noise", "variance", "estimate", "can", "be", "obtained", "from", "the", "maximum", "likelihood", "calculation"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, programming language, person, conference, location, field, university, organization, metric, researcher, task, product, country and O.\nSentence: An updated measurement noise variance estimate can be obtained from the maximum likelihood calculation", "prompt_labels": "An(O) updated(O) measurement(O) noise(O) variance(O) estimate(O) can(O) be(O) obtained(O) from(O) the(O) maximum(B-algorithm) likelihood(I-algorithm) calculation(O)"}}
{"id": "84", "dataset": "crossner_ai", "split": "dev", "label_list": ["country", "field", "metric", "university", "programming language", "location", "conference", "task", "researcher", "algorithm", "organization", "product", "person"], "instance": {"id": "84", "words": ["Like", "other", "similar", "languages", "such", "as", "APL", "and", "MATLAB", ",", "R", "supports", "matrix", "arithmetic", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-programming language", "O", "B-product", "O", "B-programming language", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, field, metric, university, programming language, location, conference, task, researcher, algorithm, organization, product, person and O.\nSentence: Like other similar languages such as APL and MATLAB , R supports matrix arithmetic .", "prompt_labels": "Like(O) other(O) similar(O) languages(O) such(O) as(O) APL(B-programming language) and(O) MATLAB(B-product) ,(O) R(B-programming language) supports(O) matrix(O) arithmetic(O) .(O)"}}
{"id": "240", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "person", "product", "location", "metric", "algorithm", "organization", "university", "field", "task", "researcher", "programming language", "country"], "instance": {"id": "240", "words": ["It", "is", "designed", "to", "be", "used", "through", "a", "number", "of", "computer", "languages", ",", "include", "Python", ",", "Ruby", ",", "and", "Scheme", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-programming language", "O", "B-programming language", "O", "O", "B-programming language", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, person, product, location, metric, algorithm, organization, university, field, task, researcher, programming language, country and O.\nSentence: It is designed to be used through a number of computer languages , include Python , Ruby , and Scheme .", "prompt_labels": "It(O) is(O) designed(O) to(O) be(O) used(O) through(O) a(O) number(O) of(O) computer(O) languages(O) ,(O) include(O) Python(B-programming language) ,(O) Ruby(B-programming language) ,(O) and(O) Scheme(B-programming language) .(O)"}}
{"id": "343", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "person", "task", "location", "researcher", "organization", "metric", "product", "field", "programming language", "university", "algorithm", "country"], "instance": {"id": "343", "words": ["A", "parallel", "manipulator", "is", "a", "mechanical", "system", "that", "uses", "several", "serial", "manipulator", "s", "to", "support", "a", "single", "platform", ",", "or", "end-effector", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, person, task, location, researcher, organization, metric, product, field, programming language, university, algorithm, country and O.\nSentence: A parallel manipulator is a mechanical system that uses several serial manipulator s to support a single platform , or end-effector .", "prompt_labels": "A(O) parallel(O) manipulator(O) is(O) a(O) mechanical(O) system(O) that(O) uses(O) several(O) serial(B-product) manipulator(I-product) s(O) to(O) support(O) a(O) single(O) platform(O) ,(O) or(O) end-effector(O) .(O)"}}
{"id": "196", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "product", "field", "location", "conference", "programming language", "organization", "country", "person", "metric", "university", "algorithm", "task"], "instance": {"id": "196", "words": ["Challenges", "in", "natural", "language", "processing", "frequently", "involve", "speech", "recognition", ",", "natural", "language", "understanding", ",", "and", "natural", "language", "generation", "."], "labels": ["O", "O", "B-field", "I-field", "I-field", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "I-task", "O", "O", "B-task", "I-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, product, field, location, conference, programming language, organization, country, person, metric, university, algorithm, task and O.\nSentence: Challenges in natural language processing frequently involve speech recognition , natural language understanding , and natural language generation .", "prompt_labels": "Challenges(O) in(O) natural(B-field) language(I-field) processing(I-field) frequently(O) involve(O) speech(B-task) recognition(I-task) ,(O) natural(B-task) language(I-task) understanding(I-task) ,(O) and(O) natural(B-task) language(I-task) generation(I-task) .(O)"}}
{"id": "237", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "programming language", "product", "conference", "country", "researcher", "field", "organization", "university", "metric", "task", "algorithm", "location"], "instance": {"id": "237", "words": ["LEPOR", "is", "designed", "with", "the", "factors", "of", "enhanced", "length", "penalty", ",", "precision", ",", "n-gram", "word", "order", "penalty", ",", "and", "recall", "."], "labels": ["B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, programming language, product, conference, country, researcher, field, organization, university, metric, task, algorithm, location and O.\nSentence: LEPOR is designed with the factors of enhanced length penalty , precision , n-gram word order penalty , and recall .", "prompt_labels": "LEPOR(B-metric) is(O) designed(O) with(O) the(O) factors(O) of(O) enhanced(O) length(O) penalty(O) ,(O) precision(B-metric) ,(O) n-gram(O) word(O) order(O) penalty(O) ,(O) and(O) recall(B-metric) .(O)"}}
{"id": "245", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "country", "location", "programming language", "product", "researcher", "task", "field", "person", "organization", "conference", "metric", "algorithm"], "instance": {"id": "245", "words": ["Applications", "of", "DSP", "include", "audio", "signal", "processing", ",", "audio", "compression", ",", "digital", "image", "processing", ",", "video", "compression", ",", "speech", "processing", ",", "speech", "recognition", ",", "digital", "communication", "s", ",", "digital", "synthesizer", "s", ",", "radar", ",", "sonar", ",", "financial", "signal", "processing", ",", "seismology", "and", "biomedicine", "."], "labels": ["O", "O", "B-field", "O", "B-task", "I-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "B-task", "B-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "O", "B-task", "I-task", "O", "O", "B-field", "O", "B-field", "O", "B-field", "I-field", "I-field", "O", "B-field", "O", "B-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, country, location, programming language, product, researcher, task, field, person, organization, conference, metric, algorithm and O.\nSentence: Applications of DSP include audio signal processing , audio compression , digital image processing , video compression , speech processing , speech recognition , digital communication s , digital synthesizer s , radar , sonar , financial signal processing , seismology and biomedicine .", "prompt_labels": "Applications(O) of(O) DSP(B-field) include(O) audio(B-task) signal(I-task) processing(I-task) ,(O) audio(B-task) compression(I-task) ,(O) digital(B-task) image(B-task) processing(B-task) ,(O) video(B-task) compression(I-task) ,(O) speech(B-task) processing(I-task) ,(O) speech(B-task) recognition(I-task) ,(O) digital(B-task) communication(I-task) s(O) ,(O) digital(B-task) synthesizer(I-task) s(O) ,(O) radar(B-field) ,(O) sonar(B-field) ,(O) financial(B-field) signal(I-field) processing(I-field) ,(O) seismology(B-field) and(O) biomedicine(B-field) .(O)"}}
{"id": "260", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "task", "researcher", "location", "programming language", "person", "country", "organization", "university", "field", "algorithm", "product", "conference"], "instance": {"id": "260", "words": ["Long", "short-term", "memory", "(", "LSTM", ")", "networks", "were", "invented", "by", "Sepp", "Hochreiter", "and", "Jürgen", "Schmidhuber", "in", "1997", "and", "set", "accuracy", "records", "in", "multiple", "applications", "domains", "."], "labels": ["B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, task, researcher, location, programming language, person, country, organization, university, field, algorithm, product, conference and O.\nSentence: Long short-term memory ( LSTM ) networks were invented by Sepp Hochreiter and Jürgen Schmidhuber in 1997 and set accuracy records in multiple applications domains .", "prompt_labels": "Long(B-algorithm) short-term(I-algorithm) memory(I-algorithm) ((O) LSTM(B-algorithm) )(O) networks(O) were(O) invented(O) by(O) Sepp(B-researcher) Hochreiter(I-researcher) and(O) Jürgen(B-researcher) Schmidhuber(I-researcher) in(O) 1997(O) and(O) set(O) accuracy(O) records(O) in(O) multiple(O) applications(O) domains(O) .(O)"}}
{"id": "22", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "metric", "location", "country", "researcher", "conference", "field", "organization", "programming language", "university", "product", "task", "algorithm"], "instance": {"id": "22", "words": ["With", "this", "company", "he", "was", "developing", "data-mining", "and", "database", "technology", ",", "more", "specific", "high-level", "ontologies", "for", "intelligence", "and", "automated", "natural", "language", "understanding", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-field", "O", "B-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, metric, location, country, researcher, conference, field, organization, programming language, university, product, task, algorithm and O.\nSentence: With this company he was developing data-mining and database technology , more specific high-level ontologies for intelligence and automated natural language understanding .", "prompt_labels": "With(O) this(O) company(O) he(O) was(O) developing(O) data-mining(B-field) and(O) database(B-field) technology(O) ,(O) more(O) specific(O) high-level(O) ontologies(O) for(O) intelligence(O) and(O) automated(B-task) natural(I-task) language(B-task) understanding(I-task) .(O)"}}
{"id": "229", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "country", "task", "location", "conference", "field", "product", "researcher", "metric", "programming language", "university", "organization", "person"], "instance": {"id": "229", "words": ["The", "Rancho", "Arm", "was", "developed", "as", "a", "robotic", "arm", "to", "help", "handicapped", "patients", "at", "the", "Rancho", "Los", "Amigos", "National", "Rehabilitation", "Center", "in", "Downey", ",", "California", ";", "this", "computer-controlled", "arm", "was", "bought", "by", "Stanford", "University", "in", "1963", "."], "labels": ["O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "I-location", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, country, task, location, conference, field, product, researcher, metric, programming language, university, organization, person and O.\nSentence: The Rancho Arm was developed as a robotic arm to help handicapped patients at the Rancho Los Amigos National Rehabilitation Center in Downey , California ; this computer-controlled arm was bought by Stanford University in 1963 .", "prompt_labels": "The(O) Rancho(B-product) Arm(I-product) was(O) developed(O) as(O) a(O) robotic(O) arm(O) to(O) help(O) handicapped(O) patients(O) at(O) the(O) Rancho(B-location) Los(I-location) Amigos(I-location) National(I-location) Rehabilitation(I-location) Center(I-location) in(O) Downey(B-location) ,(O) California(B-location) ;(O) this(O) computer-controlled(O) arm(O) was(O) bought(O) by(O) Stanford(B-university) University(I-university) in(O) 1963(O) .(O)"}}
{"id": "140", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "organization", "conference", "location", "algorithm", "programming language", "university", "metric", "country", "researcher", "product", "task", "field"], "instance": {"id": "140", "words": ["In", "this", "process", ",", "which", "is", "known", "as", "cross-validation", ",", "the", "MSE", "is", "often", "called", "the", "mean", "squared", "prediction", "error", ",", "and", "is", "computed", "as"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "O", "O", "B-metric", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, conference, location, algorithm, programming language, university, metric, country, researcher, product, task, field and O.\nSentence: In this process , which is known as cross-validation , the MSE is often called the mean squared prediction error , and is computed as", "prompt_labels": "In(O) this(O) process(O) ,(O) which(O) is(O) known(O) as(O) cross-validation(B-algorithm) ,(O) the(O) MSE(B-metric) is(O) often(O) called(O) the(O) mean(B-metric) squared(I-metric) prediction(I-metric) error(I-metric) ,(O) and(O) is(O) computed(O) as(O)"}}
{"id": "85", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "researcher", "person", "location", "university", "product", "field", "algorithm", "metric", "conference", "organization", "task", "country"], "instance": {"id": "85", "words": ["On", "7", "June", "2014", ",", "in", "a", "Turing", "test", "competition", "at", "the", "Royal", "Society", ",", "organised", "by", "Kevin", "Warwick", "of", "the", "University", "of", "Reading", "to", "mark", "the", "60th", "anniversary", "of", "Turing", "'s", "death", ",", "Goostman", "won", "after", "33", "%", "of", "the", "judges", "were", "convinced", "that", "the", "bot", "was", "human", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, researcher, person, location, university, product, field, algorithm, metric, conference, organization, task, country and O.\nSentence: On 7 June 2014 , in a Turing test competition at the Royal Society , organised by Kevin Warwick of the University of Reading to mark the 60th anniversary of Turing 's death , Goostman won after 33 % of the judges were convinced that the bot was human .", "prompt_labels": "On(O) 7(O) June(O) 2014(O) ,(O) in(O) a(O) Turing(O) test(O) competition(O) at(O) the(O) Royal(B-organization) Society(I-organization) ,(O) organised(O) by(O) Kevin(B-researcher) Warwick(I-researcher) of(O) the(O) University(B-university) of(I-university) Reading(I-university) to(O) mark(O) the(O) 60th(O) anniversary(O) of(O) Turing(O) 's(O) death(O) ,(O) Goostman(B-researcher) won(O) after(O) 33(O) %(O) of(O) the(O) judges(O) were(O) convinced(O) that(O) the(O) bot(O) was(O) human(O) .(O)"}}
{"id": "222", "dataset": "crossner_ai", "split": "dev", "label_list": ["product", "country", "person", "conference", "organization", "metric", "programming language", "algorithm", "university", "location", "researcher", "field", "task"], "instance": {"id": "222", "words": ["Hayes", "is", "a", "charter", "Fellow", "of", "AAAI", "and", "of", "the", "Cognitive", "Science", "Society"], "labels": ["B-researcher", "O", "O", "O", "O", "O", "B-conference", "O", "O", "O", "B-organization", "I-organization", "I-organization"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, country, person, conference, organization, metric, programming language, algorithm, university, location, researcher, field, task and O.\nSentence: Hayes is a charter Fellow of AAAI and of the Cognitive Science Society", "prompt_labels": "Hayes(B-researcher) is(O) a(O) charter(O) Fellow(O) of(O) AAAI(B-conference) and(O) of(O) the(O) Cognitive(B-organization) Science(I-organization) Society(I-organization)"}}
{"id": "124", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "researcher", "conference", "person", "country", "field", "location", "programming language", "metric", "algorithm", "organization", "university", "product"], "instance": {"id": "124", "words": ["These", "regions", "could", "signal", "the", "presence", "of", "objects", "or", "parts", "of", "objects", "in", "the", "image", "domain", "with", "application", "to", "object", "recognition", "and", "/", "or", "object", "video", "tracking", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "B-task", "I-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, researcher, conference, person, country, field, location, programming language, metric, algorithm, organization, university, product and O.\nSentence: These regions could signal the presence of objects or parts of objects in the image domain with application to object recognition and / or object video tracking .", "prompt_labels": "These(O) regions(O) could(O) signal(O) the(O) presence(O) of(O) objects(O) or(O) parts(O) of(O) objects(O) in(O) the(O) image(O) domain(O) with(O) application(O) to(O) object(B-task) recognition(I-task) and(O) /(O) or(O) object(B-task) video(I-task) tracking(I-task) .(O)"}}
{"id": "311", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "programming language", "conference", "task", "product", "person", "university", "country", "location", "organization", "field", "algorithm", "metric"], "instance": {"id": "311", "words": ["During", "the", "VEX", "Robotics", "World", "Championship", ",", "a", "Parade", "of", "Nations", "is", "held", "in", "Freedom", "Hall", "that", "includes", "hundreds", "of", "students", "from", "more", "than", "30", "countries", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, programming language, conference, task, product, person, university, country, location, organization, field, algorithm, metric and O.\nSentence: During the VEX Robotics World Championship , a Parade of Nations is held in Freedom Hall that includes hundreds of students from more than 30 countries .", "prompt_labels": "During(O) the(O) VEX(O) Robotics(O) World(O) Championship(O) ,(O) a(O) Parade(O) of(O) Nations(O) is(O) held(O) in(O) Freedom(B-location) Hall(I-location) that(O) includes(O) hundreds(O) of(O) students(O) from(O) more(O) than(O) 30(O) countries(O) .(O)"}}
{"id": "130", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "product", "algorithm", "programming language", "metric", "organization", "university", "person", "task", "conference", "country", "location", "field"], "instance": {"id": "130", "words": ["The", "most", "recent", "school", "of", "thought", "on", "Honda", "'s", "strategy", "was", "put", "forward", "by", "Gary", "Hamel", "and", "C.", "K.", "Prahalad", "in", "1989", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, product, algorithm, programming language, metric, organization, university, person, task, conference, country, location, field and O.\nSentence: The most recent school of thought on Honda 's strategy was put forward by Gary Hamel and C. K. Prahalad in 1989 .", "prompt_labels": "The(O) most(O) recent(O) school(O) of(O) thought(O) on(O) Honda(B-organization) 's(O) strategy(O) was(O) put(O) forward(O) by(O) Gary(B-person) Hamel(I-person) and(O) C.(B-person) K.(I-person) Prahalad(I-person) in(O) 1989(O) .(O)"}}
{"id": "203", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "conference", "algorithm", "organization", "university", "location", "person", "country", "task", "researcher", "programming language", "product", "field"], "instance": {"id": "203", "words": ["In", "2016", ",", "she", "was", "selected", "as", "the", "ACL", "(", "Association", "for", "Computational", "Linguistics", ")", "Lifetime", "Achievement", "Award", "winner", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "O", "B-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, conference, algorithm, organization, university, location, person, country, task, researcher, programming language, product, field and O.\nSentence: In 2016 , she was selected as the ACL ( Association for Computational Linguistics ) Lifetime Achievement Award winner .", "prompt_labels": "In(O) 2016(O) ,(O) she(O) was(O) selected(O) as(O) the(O) ACL(B-conference) ((O) Association(B-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) )(O) Lifetime(O) Achievement(O) Award(O) winner(O) .(O)"}}
{"id": "71", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "metric", "university", "task", "product", "country", "programming language", "algorithm", "researcher", "conference", "organization", "person", "location"], "instance": {"id": "71", "words": ["In", "linear", "time-invariant", "(", "LTI", ")", "system", "theory", ",", "control", "theory", ",", "and", "in", "digital", "signal", "processing", "or", "signal", "processing", ",", "the", "relationship", "between", "the", "input", "signal", ",", "math", "\\", "displaystyle", "x", "(", "t", ")", "/", "math", ",", "to", "output", "signal", ",", "math", "\\", "displaystyle", "y", "(", "t", ")", "/", "math", ",", "of", "an", "LTI", "system", "is", "governed", "by", "a", "convolution", "operation", ":"], "labels": ["O", "B-field", "I-field", "I-field", "I-field", "I-field", "I-field", "I-field", "O", "B-field", "I-field", "O", "O", "O", "B-field", "I-field", "I-field", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "B-algorithm", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, metric, university, task, product, country, programming language, algorithm, researcher, conference, organization, person, location and O.\nSentence: In linear time-invariant ( LTI ) system theory , control theory , and in digital signal processing or signal processing , the relationship between the input signal , math \\ displaystyle x ( t ) / math , to output signal , math \\ displaystyle y ( t ) / math , of an LTI system is governed by a convolution operation :", "prompt_labels": "In(O) linear(B-field) time-invariant(I-field) ((I-field) LTI(I-field) )(I-field) system(I-field) theory(I-field) ,(O) control(B-field) theory(I-field) ,(O) and(O) in(O) digital(B-field) signal(I-field) processing(I-field) or(O) signal(B-field) processing(I-field) ,(O) the(O) relationship(O) between(O) the(O) input(O) signal(O) ,(O) math(O) \\(O) displaystyle(O) x(O) ((O) t(O) )(O) /(O) math(O) ,(O) to(O) output(O) signal(O) ,(O) math(O) \\(O) displaystyle(O) y(O) ((O) t(O) )(O) /(O) math(O) ,(O) of(O) an(O) LTI(B-field) system(I-field) is(O) governed(O) by(O) a(O) convolution(B-algorithm) operation(O) :(O)"}}
{"id": "277", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "university", "location", "field", "algorithm", "programming language", "conference", "country", "person", "organization", "task", "product", "metric"], "instance": {"id": "277", "words": ["If", "a", "convex", "loss", "is", "utilized", "(", "as", "in", "AdaBoost", ",", "LogitBoost", ",", "and", "all", "members", "of", "the", "AnyBoost", "family", "of", "algorithms", ")", "then", "an", "example", "with", "higher", "margin", "will", "receive", "less", "(", "or", "equal", ")", "weight", "than", "an", "example", "with", "lower", "margin", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "O", "B-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, university, location, field, algorithm, programming language, conference, country, person, organization, task, product, metric and O.\nSentence: If a convex loss is utilized ( as in AdaBoost , LogitBoost , and all members of the AnyBoost family of algorithms ) then an example with higher margin will receive less ( or equal ) weight than an example with lower margin .", "prompt_labels": "If(O) a(O) convex(O) loss(O) is(O) utilized(O) ((O) as(O) in(O) AdaBoost(B-algorithm) ,(O) LogitBoost(B-algorithm) ,(O) and(O) all(O) members(O) of(O) the(O) AnyBoost(O) family(O) of(O) algorithms(O) )(O) then(O) an(O) example(O) with(O) higher(O) margin(O) will(O) receive(O) less(O) ((O) or(O) equal(O) )(O) weight(O) than(O) an(O) example(O) with(O) lower(O) margin(O) .(O)"}}
{"id": "163", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "field", "metric", "university", "person", "organization", "conference", "country", "product", "location", "algorithm", "task", "programming language"], "instance": {"id": "163", "words": ["His", "work", "inspired", "subsequent", "generations", "of", "robotics", "researchers", "such", "as", "Rodney", "Brooks", ",", "Hans", "Moravec", "and", "Mark", "Tilden", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, field, metric, university, person, organization, conference, country, product, location, algorithm, task, programming language and O.\nSentence: His work inspired subsequent generations of robotics researchers such as Rodney Brooks , Hans Moravec and Mark Tilden .", "prompt_labels": "His(O) work(O) inspired(O) subsequent(O) generations(O) of(O) robotics(O) researchers(O) such(O) as(O) Rodney(B-researcher) Brooks(I-researcher) ,(O) Hans(B-researcher) Moravec(I-researcher) and(O) Mark(B-researcher) Tilden(I-researcher) .(O)"}}
{"id": "250", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "product", "programming language", "metric", "university", "conference", "task", "algorithm", "researcher", "field", "location", "country", "organization"], "instance": {"id": "250", "words": ["If", "named", "entities", "cannot", "be", "recognized", "by", "the", "machine", "translator", ",", "they", "may", "be", "erroneously", "translated", "as", "common", "nouns", ",", "which", "would", "most", "likely", "not", "affect", "the", "Bilingual", "evaluation", "understudy", "rating", "of", "the", "translation", "but", "would", "change", "the", "text", "'s", "human", "readability", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, product, programming language, metric, university, conference, task, algorithm, researcher, field, location, country, organization and O.\nSentence: If named entities cannot be recognized by the machine translator , they may be erroneously translated as common nouns , which would most likely not affect the Bilingual evaluation understudy rating of the translation but would change the text 's human readability .", "prompt_labels": "If(O) named(O) entities(O) cannot(O) be(O) recognized(O) by(O) the(O) machine(B-product) translator(I-product) ,(O) they(O) may(O) be(O) erroneously(O) translated(O) as(O) common(O) nouns(O) ,(O) which(O) would(O) most(O) likely(O) not(O) affect(O) the(O) Bilingual(B-metric) evaluation(I-metric) understudy(I-metric) rating(O) of(O) the(O) translation(O) but(O) would(O) change(O) the(O) text(O) 's(O) human(O) readability(O) .(O)"}}
{"id": "308", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "programming language", "product", "organization", "conference", "person", "researcher", "university", "country", "metric", "location", "algorithm", "task"], "instance": {"id": "308", "words": ["A", "number", "of", "WordNet-based", "word", "similarity", "algorithms", "are", "implemented", "in", "a", "Perl", "package", "called", "WordNet", "::", "Similarity", "."], "labels": ["O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "B-programming language", "O", "O", "B-product", "I-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, programming language, product, organization, conference, person, researcher, university, country, metric, location, algorithm, task and O.\nSentence: A number of WordNet-based word similarity algorithms are implemented in a Perl package called WordNet :: Similarity .", "prompt_labels": "A(O) number(O) of(O) WordNet-based(B-algorithm) word(I-algorithm) similarity(I-algorithm) algorithms(I-algorithm) are(O) implemented(O) in(O) a(O) Perl(B-programming language) package(O) called(O) WordNet(B-product) ::(I-product) Similarity(I-product) .(O)"}}
{"id": "249", "dataset": "crossner_ai", "split": "dev", "label_list": ["university", "conference", "algorithm", "task", "country", "researcher", "person", "field", "location", "programming language", "organization", "product", "metric"], "instance": {"id": "249", "words": ["Conceptual", "clustering", "developed", "mainly", "during", "the", "1980s", ",", "as", "a", "machine", "learning", "paradigm", "for", "unsupervised", "learning", "."], "labels": ["B-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, conference, algorithm, task, country, researcher, person, field, location, programming language, organization, product, metric and O.\nSentence: Conceptual clustering developed mainly during the 1980s , as a machine learning paradigm for unsupervised learning .", "prompt_labels": "Conceptual(B-algorithm) clustering(I-algorithm) developed(O) mainly(O) during(O) the(O) 1980s(O) ,(O) as(O) a(O) machine(B-field) learning(I-field) paradigm(O) for(O) unsupervised(B-field) learning(I-field) .(O)"}}
{"id": "30", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "conference", "algorithm", "person", "field", "location", "product", "task", "university", "programming language", "country", "metric", "organization"], "instance": {"id": "30", "words": ["This", "research", "was", "fundamental", "to", "the", "development", "of", "modern", "techniques", "of", "speech", "synthesis", ",", "reading", "machines", "for", "the", "blind", ",", "the", "study", "of", "speech", "perception", "and", "speech", "recognition", ",", "and", "the", "development", "of", "the", "motor", "theory", "of", "speech", "perception", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "I-task", "I-task", "I-task", "O", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "I-task", "I-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, conference, algorithm, person, field, location, product, task, university, programming language, country, metric, organization and O.\nSentence: This research was fundamental to the development of modern techniques of speech synthesis , reading machines for the blind , the study of speech perception and speech recognition , and the development of the motor theory of speech perception .", "prompt_labels": "This(O) research(O) was(O) fundamental(O) to(O) the(O) development(O) of(O) modern(O) techniques(O) of(O) speech(B-task) synthesis(I-task) ,(O) reading(B-task) machines(I-task) for(I-task) the(I-task) blind(I-task) ,(O) the(O) study(O) of(O) speech(B-task) perception(I-task) and(O) speech(B-task) recognition(I-task) ,(O) and(O) the(O) development(O) of(O) the(O) motor(B-task) theory(I-task) of(I-task) speech(I-task) perception(I-task) .(O)"}}
{"id": "121", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "field", "task", "country", "algorithm", "programming language", "researcher", "product", "conference", "metric", "person", "university", "organization"], "instance": {"id": "121", "words": ["As", "a", "platform", ",", "LinguaStream", "provides", "an", "extensive", "Java", "API", "."], "labels": ["O", "O", "O", "O", "B-product", "O", "O", "O", "B-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, field, task, country, algorithm, programming language, researcher, product, conference, metric, person, university, organization and O.\nSentence: As a platform , LinguaStream provides an extensive Java API .", "prompt_labels": "As(O) a(O) platform(O) ,(O) LinguaStream(B-product) provides(O) an(O) extensive(O) Java(B-product) API(I-product) .(O)"}}
{"id": "93", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "product", "person", "field", "researcher", "metric", "university", "task", "algorithm", "conference", "organization", "country", "programming language"], "instance": {"id": "93", "words": ["Computation", "of", "this", "example", "using", "Python", "code", ":"], "labels": ["O", "O", "O", "O", "O", "B-programming language", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, product, person, field, researcher, metric, university, task, algorithm, conference, organization, country, programming language and O.\nSentence: Computation of this example using Python code :", "prompt_labels": "Computation(O) of(O) this(O) example(O) using(O) Python(B-programming language) code(O) :(O)"}}
{"id": "132", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "country", "organization", "product", "person", "university", "metric", "programming language", "researcher", "task", "algorithm", "field", "conference"], "instance": {"id": "132", "words": ["He", "was", "honored", "with", "the", "2019", "Lifetime", "Achievement", "Award", "from", "the", "Association", "for", "Computational", "Linguistics", "(", "ACL", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, organization, product, person, university, metric, programming language, researcher, task, algorithm, field, conference and O.\nSentence: He was honored with the 2019 Lifetime Achievement Award from the Association for Computational Linguistics ( ACL ) .", "prompt_labels": "He(O) was(O) honored(O) with(O) the(O) 2019(O) Lifetime(O) Achievement(O) Award(O) from(O) the(O) Association(B-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) ((O) ACL(B-conference) )(O) .(O)"}}
{"id": "342", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "programming language", "metric", "person", "field", "country", "organization", "university", "researcher", "task", "product", "location", "algorithm"], "instance": {"id": "342", "words": ["John", "McCarthy", "is", "one", "of", "the", "founding", "fathers", "of", "artificial", "intelligence", ",", "together", "with", "Alan", "Turing", ",", "Marvin", "Minsky", ",", "Allen", "Newell", ",", "and", "Herbert", "A.", "Simon", "."], "labels": ["B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "B-researcher", "I-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, programming language, metric, person, field, country, organization, university, researcher, task, product, location, algorithm and O.\nSentence: John McCarthy is one of the founding fathers of artificial intelligence , together with Alan Turing , Marvin Minsky , Allen Newell , and Herbert A. Simon .", "prompt_labels": "John(B-researcher) McCarthy(I-researcher) is(O) one(O) of(O) the(O) founding(O) fathers(O) of(O) artificial(B-field) intelligence(I-field) ,(O) together(O) with(O) Alan(B-researcher) Turing(I-researcher) ,(O) Marvin(B-researcher) Minsky(I-researcher) ,(O) Allen(B-researcher) Newell(I-researcher) ,(O) and(O) Herbert(B-researcher) A.(I-researcher) Simon(I-researcher) .(O)"}}
{"id": "301", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "algorithm", "university", "conference", "researcher", "organization", "product", "metric", "country", "programming language", "person", "field", "location"], "instance": {"id": "301", "words": ["In", "VLDB", "'", "8", ":", "Proceedings", "of", "the", "34th", "International", "Conference", "on", "Very", "Large", "Data", "Bases", ",", "pages", "422--433.", "showed", "that", "the", "given", "values", "for", "mathC", "/", "math", "and", "mathK", "/", "math", "generally", "imply", "relatively", "low", "accuracy", "of", "iteratively", "computed", "SimRank", "scores", "."], "labels": ["O", "B-conference", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "O", "O", "O", "B-metric", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, algorithm, university, conference, researcher, organization, product, metric, country, programming language, person, field, location and O.\nSentence: In VLDB ' 8 : Proceedings of the 34th International Conference on Very Large Data Bases , pages 422--433. showed that the given values for mathC / math and mathK / math generally imply relatively low accuracy of iteratively computed SimRank scores .", "prompt_labels": "In(O) VLDB(B-conference) '(O) 8(O) :(O) Proceedings(B-conference) of(I-conference) the(I-conference) 34th(I-conference) International(I-conference) Conference(I-conference) on(I-conference) Very(I-conference) Large(I-conference) Data(I-conference) Bases(I-conference) ,(O) pages(O) 422--433.(O) showed(O) that(O) the(O) given(O) values(O) for(O) mathC(O) /(O) math(O) and(O) mathK(O) /(O) math(O) generally(O) imply(O) relatively(O) low(O) accuracy(B-metric) of(O) iteratively(O) computed(O) SimRank(B-metric) scores(O) .(O)"}}
{"id": "3", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "organization", "programming language", "country", "researcher", "algorithm", "task", "person", "conference", "location", "product", "metric", "university"], "instance": {"id": "3", "words": ["The", "first", "picture", "to", "be", "scanned", ",", "stored", ",", "and", "recreated", "in", "digital", "pixels", "was", "displayed", "on", "the", "Standards", "Eastern", "Automatic", "Computer", "(", "SEAC", ")", "at", "NIST", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "I-product", "O", "B-product", "O", "O", "B-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, organization, programming language, country, researcher, algorithm, task, person, conference, location, product, metric, university and O.\nSentence: The first picture to be scanned , stored , and recreated in digital pixels was displayed on the Standards Eastern Automatic Computer ( SEAC ) at NIST .", "prompt_labels": "The(O) first(O) picture(O) to(O) be(O) scanned(O) ,(O) stored(O) ,(O) and(O) recreated(O) in(O) digital(O) pixels(O) was(O) displayed(O) on(O) the(O) Standards(B-product) Eastern(I-product) Automatic(I-product) Computer(I-product) ((O) SEAC(B-product) )(O) at(O) NIST(B-organization) .(O)"}}
{"id": "312", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "product", "field", "organization", "country", "algorithm", "metric", "researcher", "university", "task", "location", "conference", "programming language"], "instance": {"id": "312", "words": ["Other", "measures", "of", "accuracy", "include", "Single", "Word", "Error", "Rate", "(", "SWER", ")", "and", "Command", "Success", "Rate", "(", "CSR", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "I-metric", "O", "B-metric", "O", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, product, field, organization, country, algorithm, metric, researcher, university, task, location, conference, programming language and O.\nSentence: Other measures of accuracy include Single Word Error Rate ( SWER ) and Command Success Rate ( CSR ) .", "prompt_labels": "Other(O) measures(O) of(O) accuracy(O) include(O) Single(B-metric) Word(I-metric) Error(I-metric) Rate(I-metric) ((O) SWER(B-metric) )(O) and(O) Command(B-metric) Success(I-metric) Rate(I-metric) ((O) CSR(B-metric) )(O) .(O)"}}
{"id": "97", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "product", "programming language", "university", "organization", "conference", "country", "person", "algorithm", "task", "researcher", "location", "field"], "instance": {"id": "97", "words": ["The", "back-of-the-envelope", "calculations", "by", "Doug", ",", "Alan", ",", "and", "their", "colleagues", "(", "including", "Marvin", "Minsky", ",", "Allen", "Newell", ",", "Edward", "Feigenbaum", ",", "and", "John", "McCarthy", ")", "indicated", "that", "that", "effort", "would", "require", "between", "1000", "and", "3000", "person-years", "of", "effort", ",", "far", "beyond", "the", "standard", "academic", "project", "model", "."], "labels": ["O", "O", "O", "O", "B-researcher", "O", "B-researcher", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, product, programming language, university, organization, conference, country, person, algorithm, task, researcher, location, field and O.\nSentence: The back-of-the-envelope calculations by Doug , Alan , and their colleagues ( including Marvin Minsky , Allen Newell , Edward Feigenbaum , and John McCarthy ) indicated that that effort would require between 1000 and 3000 person-years of effort , far beyond the standard academic project model .", "prompt_labels": "The(O) back-of-the-envelope(O) calculations(O) by(O) Doug(B-researcher) ,(O) Alan(B-researcher) ,(O) and(O) their(O) colleagues(O) ((O) including(O) Marvin(B-researcher) Minsky(I-researcher) ,(O) Allen(B-researcher) Newell(I-researcher) ,(O) Edward(B-researcher) Feigenbaum(I-researcher) ,(O) and(O) John(B-researcher) McCarthy(I-researcher) )(O) indicated(O) that(O) that(O) effort(O) would(O) require(O) between(O) 1000(O) and(O) 3000(O) person-years(O) of(O) effort(O) ,(O) far(O) beyond(O) the(O) standard(O) academic(O) project(O) model(O) .(O)"}}
{"id": "246", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "university", "researcher", "conference", "task", "person", "location", "algorithm", "organization", "product", "metric", "country", "programming language"], "instance": {"id": "246", "words": ["(", "February", "20", ",", "1912", "-", "August", "11", ",", "2011", ")", "was", "an", "American", "inventor", ",", "best", "known", "for", "creating", "Unimate", ",", "the", "first", "industrial", "robot", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, university, researcher, conference, task, person, location, algorithm, organization, product, metric, country, programming language and O.\nSentence: ( February 20 , 1912 - August 11 , 2011 ) was an American inventor , best known for creating Unimate , the first industrial robot .", "prompt_labels": "((O) February(O) 20(O) ,(O) 1912(O) -(O) August(O) 11(O) ,(O) 2011(O) )(O) was(O) an(O) American(O) inventor(O) ,(O) best(O) known(O) for(O) creating(O) Unimate(B-product) ,(O) the(O) first(O) industrial(O) robot(O) .(O)"}}
{"id": "173", "dataset": "crossner_ai", "split": "dev", "label_list": ["country", "conference", "location", "organization", "field", "researcher", "product", "algorithm", "metric", "university", "person", "task", "programming language"], "instance": {"id": "173", "words": ["For", "example", ",", "the", "best", "system", "entering", "MUC-7", "scored", "93.39", "%", "of", "F-measure", "while", "human", "annotators", "scored", "97.6", "%", "and", "96.95", "%", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-conference", "O", "O", "O", "O", "B-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, conference, location, organization, field, researcher, product, algorithm, metric, university, person, task, programming language and O.\nSentence: For example , the best system entering MUC-7 scored 93.39 % of F-measure while human annotators scored 97.6 % and 96.95 % .", "prompt_labels": "For(O) example(O) ,(O) the(O) best(O) system(O) entering(O) MUC-7(B-conference) scored(O) 93.39(O) %(O) of(O) F-measure(B-metric) while(O) human(O) annotators(O) scored(O) 97.6(O) %(O) and(O) 96.95(O) %(O) .(O)"}}
{"id": "334", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "product", "field", "task", "country", "metric", "programming language", "algorithm", "location", "organization", "researcher", "university", "person"], "instance": {"id": "334", "words": ["In", "English", ",", "WordNet", "is", "an", "example", "of", "a", "semantic", "network", "."], "labels": ["O", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, product, field, task, country, metric, programming language, algorithm, location, organization, researcher, university, person and O.\nSentence: In English , WordNet is an example of a semantic network .", "prompt_labels": "In(O) English(O) ,(O) WordNet(B-product) is(O) an(O) example(O) of(O) a(O) semantic(O) network(O) .(O)"}}
{"id": "333", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "metric", "university", "task", "field", "organization", "algorithm", "conference", "researcher", "programming language", "location", "product", "country"], "instance": {"id": "333", "words": ["The", "true-positive", "rate", "is", "also", "known", "as", "sensitivity", ",", "recall", "or", "probability", "of", "detection", "math", "to", "the", "discrimination", "threshold", ")", "of", "the", "detection", "probability", "in", "the", "y-axis", "versus", "the", "cumulative", "distribution", "function", "of", "the", "false-alarm", "probability", "on", "the", "x-axis", "."], "labels": ["O", "B-metric", "I-metric", "O", "O", "O", "O", "B-metric", "O", "B-metric", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, metric, university, task, field, organization, algorithm, conference, researcher, programming language, location, product, country and O.\nSentence: The true-positive rate is also known as sensitivity , recall or probability of detection math to the discrimination threshold ) of the detection probability in the y-axis versus the cumulative distribution function of the false-alarm probability on the x-axis .", "prompt_labels": "The(O) true-positive(B-metric) rate(I-metric) is(O) also(O) known(O) as(O) sensitivity(B-metric) ,(O) recall(B-metric) or(O) probability(B-metric) of(I-metric) detection(I-metric) math(O) to(O) the(O) discrimination(O) threshold(O) )(O) of(O) the(O) detection(O) probability(O) in(O) the(O) y-axis(O) versus(O) the(O) cumulative(B-algorithm) distribution(I-algorithm) function(I-algorithm) of(O) the(O) false-alarm(B-metric) probability(I-metric) on(O) the(O) x-axis(O) .(O)"}}
{"id": "184", "dataset": "crossner_ai", "split": "dev", "label_list": ["organization", "person", "metric", "programming language", "field", "conference", "university", "location", "task", "country", "researcher", "algorithm", "product"], "instance": {"id": "184", "words": ["Some", "specialized", "software", "can", "narrate", "RSS", "."], "labels": ["O", "O", "O", "O", "O", "B-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, metric, programming language, field, conference, university, location, task, country, researcher, algorithm, product and O.\nSentence: Some specialized software can narrate RSS .", "prompt_labels": "Some(O) specialized(O) software(O) can(O) narrate(O) RSS(B-product) .(O)"}}
{"id": "286", "dataset": "crossner_ai", "split": "dev", "label_list": ["metric", "programming language", "task", "product", "organization", "field", "university", "location", "algorithm", "person", "country", "researcher", "conference"], "instance": {"id": "286", "words": ["Traditional", "phonetic-based", "(", "i.e.", ",", "all", "Hidden", "Markov", "model", "-based", "model", ")", "approaches", "required", "separate", "components", "and", "training", "for", "the", "pronunciation", ",", "acoustic", "and", "language", "model", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, programming language, task, product, organization, field, university, location, algorithm, person, country, researcher, conference and O.\nSentence: Traditional phonetic-based ( i.e. , all Hidden Markov model -based model ) approaches required separate components and training for the pronunciation , acoustic and language model .", "prompt_labels": "Traditional(O) phonetic-based(O) ((O) i.e.(O) ,(O) all(O) Hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) -based(O) model(O) )(O) approaches(O) required(O) separate(O) components(O) and(O) training(O) for(O) the(O) pronunciation(O) ,(O) acoustic(O) and(O) language(O) model(O) .(O)"}}
{"id": "108", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "conference", "field", "algorithm", "task", "product", "university", "person", "researcher", "country", "metric", "programming language", "organization"], "instance": {"id": "108", "words": ["In", "1857", ",", "at", "the", "request", "of", "the", "Tokugawa", "Shogunate", ",", "a", "group", "of", "Dutch", "engineers", "began", "work", "on", "the", "Nagasaki", "Yotetsusho", ",", "a", "modern", ",", "Western-style", "foundry", "and", "shipyard", "near", "the", "Dutch", "settlement", "of", "Dejima", ",", "at", "Nagasaki", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, conference, field, algorithm, task, product, university, person, researcher, country, metric, programming language, organization and O.\nSentence: In 1857 , at the request of the Tokugawa Shogunate , a group of Dutch engineers began work on the Nagasaki Yotetsusho , a modern , Western-style foundry and shipyard near the Dutch settlement of Dejima , at Nagasaki .", "prompt_labels": "In(O) 1857(O) ,(O) at(O) the(O) request(O) of(O) the(O) Tokugawa(B-country) Shogunate(I-country) ,(O) a(O) group(O) of(O) Dutch(O) engineers(O) began(O) work(O) on(O) the(O) Nagasaki(O) Yotetsusho(O) ,(O) a(O) modern(O) ,(O) Western-style(O) foundry(O) and(O) shipyard(O) near(O) the(O) Dutch(O) settlement(O) of(O) Dejima(O) ,(O) at(O) Nagasaki(O) .(O)"}}
{"id": "221", "dataset": "crossner_ai", "split": "dev", "label_list": ["country", "university", "location", "organization", "task", "algorithm", "conference", "metric", "programming language", "researcher", "person", "product", "field"], "instance": {"id": "221", "words": ["At", "the", "IEEE", "Conference", "on", "Computer", "Vision", "and", "Pattern", "Recognition", "in", "2006", ",", "Qiang", "Zhu", ",", "Shai", "Avidan", ",", "Mei-Chen", "Yeh", ",", "and", "Kwang-Ting", "Cheng", "presented", "an", "algorithm", "to", "significantly", "speed", "up", "human", "detection", "using", "HOG", "descriptor", "methods", "."], "labels": ["O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, location, organization, task, algorithm, conference, metric, programming language, researcher, person, product, field and O.\nSentence: At the IEEE Conference on Computer Vision and Pattern Recognition in 2006 , Qiang Zhu , Shai Avidan , Mei-Chen Yeh , and Kwang-Ting Cheng presented an algorithm to significantly speed up human detection using HOG descriptor methods .", "prompt_labels": "At(O) the(O) IEEE(B-conference) Conference(I-conference) on(I-conference) Computer(I-conference) Vision(I-conference) and(I-conference) Pattern(I-conference) Recognition(I-conference) in(O) 2006(O) ,(O) Qiang(B-researcher) Zhu(I-researcher) ,(O) Shai(B-researcher) Avidan(I-researcher) ,(O) Mei-Chen(B-researcher) Yeh(I-researcher) ,(O) and(O) Kwang-Ting(B-researcher) Cheng(I-researcher) presented(O) an(O) algorithm(O) to(O) significantly(O) speed(O) up(O) human(B-task) detection(I-task) using(O) HOG(B-algorithm) descriptor(I-algorithm) methods(I-algorithm) .(O)"}}
{"id": "217", "dataset": "crossner_ai", "split": "dev", "label_list": ["country", "person", "researcher", "field", "product", "task", "location", "algorithm", "conference", "university", "metric", "organization", "programming language"], "instance": {"id": "217", "words": ["A", "frustrating", "outcome", "of", "the", "same", "study", "by", "Stanford", "(", "and", "other", "attempts", "to", "improve", "named", "recognition", "translation", ")", "is", "that", "many", "times", ",", "a", "decrease", "in", "the", "Bilingual", "evaluation", "understudy", "scores", "for", "translation", "will", "result", "from", "the", "inclusion", "of", "methods", "for", "named", "entity", "translation", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-university", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, researcher, field, product, task, location, algorithm, conference, university, metric, organization, programming language and O.\nSentence: A frustrating outcome of the same study by Stanford ( and other attempts to improve named recognition translation ) is that many times , a decrease in the Bilingual evaluation understudy scores for translation will result from the inclusion of methods for named entity translation .", "prompt_labels": "A(O) frustrating(O) outcome(O) of(O) the(O) same(O) study(O) by(O) Stanford(B-university) ((O) and(O) other(O) attempts(O) to(O) improve(O) named(B-task) recognition(I-task) translation(I-task) )(O) is(O) that(O) many(O) times(O) ,(O) a(O) decrease(O) in(O) the(O) Bilingual(B-metric) evaluation(I-metric) understudy(I-metric) scores(O) for(O) translation(O) will(O) result(O) from(O) the(O) inclusion(O) of(O) methods(O) for(O) named(B-task) entity(I-task) translation(I-task) .(O)"}}
{"id": "172", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "person", "organization", "university", "metric", "algorithm", "programming language", "field", "country", "product", "conference", "location", "researcher"], "instance": {"id": "172", "words": ["The", "Apple", "iOS", "operating", "system", "used", "on", "the", "iPhone", ",", "iPad", "and", "iPod", "Touch", "uses", "VoiceOver", "speech", "synthesis", "accessibility", "."], "labels": ["O", "B-product", "I-product", "I-product", "I-product", "O", "O", "O", "B-product", "O", "B-product", "O", "B-product", "I-product", "O", "B-product", "I-product", "I-product", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, person, organization, university, metric, algorithm, programming language, field, country, product, conference, location, researcher and O.\nSentence: The Apple iOS operating system used on the iPhone , iPad and iPod Touch uses VoiceOver speech synthesis accessibility .", "prompt_labels": "The(O) Apple(B-product) iOS(I-product) operating(I-product) system(I-product) used(O) on(O) the(O) iPhone(B-product) ,(O) iPad(B-product) and(O) iPod(B-product) Touch(I-product) uses(O) VoiceOver(B-product) speech(I-product) synthesis(I-product) accessibility(O) .(O)"}}
{"id": "329", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "person", "algorithm", "university", "researcher", "product", "country", "field", "location", "task", "organization", "metric", "programming language"], "instance": {"id": "329", "words": ["The", "formal", "RoboCup", "competition", "was", "preceded", "by", "the", "(", "often", "unacknowledged", ")", "first", "International", "Micro", "Robot", "World", "Cup", "Soccer", "Tournament", "(", "MIROSOT", ")", "held", "by", "KAIST", "in", "Taejon", ",", "Korea", ",", "in", "November", "1996", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "O", "B-location", "O", "B-country", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, person, algorithm, university, researcher, product, country, field, location, task, organization, metric, programming language and O.\nSentence: The formal RoboCup competition was preceded by the ( often unacknowledged ) first International Micro Robot World Cup Soccer Tournament ( MIROSOT ) held by KAIST in Taejon , Korea , in November 1996 .", "prompt_labels": "The(O) formal(O) RoboCup(O) competition(O) was(O) preceded(O) by(O) the(O) ((O) often(O) unacknowledged(O) )(O) first(O) International(O) Micro(O) Robot(O) World(O) Cup(O) Soccer(O) Tournament(O) ((O) MIROSOT(O) )(O) held(O) by(O) KAIST(B-university) in(O) Taejon(B-location) ,(O) Korea(B-country) ,(O) in(O) November(O) 1996(O) .(O)"}}
{"id": "101", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "algorithm", "organization", "person", "task", "researcher", "field", "programming language", "product", "university", "metric", "location", "country"], "instance": {"id": "101", "words": ["Techniques", "such", "as", "dynamic", "Markov", "Networks", ",", "Convolutional", "neural", "network", "and", "Long", "short-term", "memory", "are", "often", "employed", "to", "exploit", "the", "inter-frame", "correlations", "."], "labels": ["O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, algorithm, organization, person, task, researcher, field, programming language, product, university, metric, location, country and O.\nSentence: Techniques such as dynamic Markov Networks , Convolutional neural network and Long short-term memory are often employed to exploit the inter-frame correlations .", "prompt_labels": "Techniques(O) such(O) as(O) dynamic(B-algorithm) Markov(I-algorithm) Networks(I-algorithm) ,(O) Convolutional(B-algorithm) neural(I-algorithm) network(I-algorithm) and(O) Long(B-algorithm) short-term(I-algorithm) memory(I-algorithm) are(O) often(O) employed(O) to(O) exploit(O) the(O) inter-frame(O) correlations(O) .(O)"}}
{"id": "306", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "conference", "university", "location", "field", "product", "person", "organization", "algorithm", "researcher", "task", "country", "metric"], "instance": {"id": "306", "words": ["Data", "mining", "is", "a", "related", "field", "of", "study", ",", "focusing", "on", "exploratory", "data", "analysis", "through", "unsupervised", "learning", "."], "labels": ["B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "I-task", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, conference, university, location, field, product, person, organization, algorithm, researcher, task, country, metric and O.\nSentence: Data mining is a related field of study , focusing on exploratory data analysis through unsupervised learning .", "prompt_labels": "Data(B-field) mining(I-field) is(O) a(O) related(O) field(O) of(O) study(O) ,(O) focusing(O) on(O) exploratory(B-task) data(I-task) analysis(I-task) through(O) unsupervised(B-field) learning(I-field) .(O)"}}
{"id": "299", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "person", "metric", "product", "task", "organization", "field", "country", "conference", "programming language", "algorithm", "university", "location"], "instance": {"id": "299", "words": ["Collaborative", "filtering", "(", "CF", ")", "is", "a", "technique", "used", "by", "recommender", "system", "s", "."], "labels": ["B-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, person, metric, product, task, organization, field, country, conference, programming language, algorithm, university, location and O.\nSentence: Collaborative filtering ( CF ) is a technique used by recommender system s .", "prompt_labels": "Collaborative(B-algorithm) filtering(I-algorithm) ((O) CF(B-algorithm) )(O) is(O) a(O) technique(O) used(O) by(O) recommender(B-product) system(I-product) s(O) .(O)"}}
{"id": "257", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "task", "country", "field", "programming language", "conference", "product", "metric", "organization", "researcher", "university", "algorithm", "person"], "instance": {"id": "257", "words": ["Sub-domains", "of", "computer", "vision", "include", "scene", "reconstruction", ",", "event", "detection", ",", "video", "tracking", ",", "object", "recognition", ",", "3D", "pose", "estimation", ",", "learning", ",", "indexing", ",", "motion", "estimation", ",", "visual", "servoing", ",", "3D", "scene", "modeling", ",", "and", "image", "restoration", "."], "labels": ["O", "O", "B-field", "I-field", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "I-task", "O", "B-task", "O", "B-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "O", "B-task", "I-task", "I-task", "O", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, task, country, field, programming language, conference, product, metric, organization, researcher, university, algorithm, person and O.\nSentence: Sub-domains of computer vision include scene reconstruction , event detection , video tracking , object recognition , 3D pose estimation , learning , indexing , motion estimation , visual servoing , 3D scene modeling , and image restoration .", "prompt_labels": "Sub-domains(O) of(O) computer(B-field) vision(I-field) include(O) scene(B-task) reconstruction(I-task) ,(O) event(B-task) detection(I-task) ,(O) video(B-task) tracking(I-task) ,(O) object(B-task) recognition(I-task) ,(O) 3D(B-task) pose(I-task) estimation(I-task) ,(O) learning(B-task) ,(O) indexing(B-task) ,(O) motion(B-task) estimation(I-task) ,(O) visual(B-task) servoing(I-task) ,(O) 3D(B-task) scene(I-task) modeling(I-task) ,(O) and(O) image(B-task) restoration(I-task) .(O)"}}
{"id": "215", "dataset": "crossner_ai", "split": "dev", "label_list": ["location", "algorithm", "person", "organization", "conference", "country", "university", "researcher", "task", "programming language", "product", "field", "metric"], "instance": {"id": "215", "words": ["Getoor", "has", "multiple", "best", "paper", "awards", ",", "an", "NSF", "Career", "Award", ",", "and", "is", "an", "Association", "for", "the", "Advancement", "of", "Artificial", "Intelligence", "(", "AAAI", ")", "Fellow", "."], "labels": ["B-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "B-conference", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, algorithm, person, organization, conference, country, university, researcher, task, programming language, product, field, metric and O.\nSentence: Getoor has multiple best paper awards , an NSF Career Award , and is an Association for the Advancement of Artificial Intelligence ( AAAI ) Fellow .", "prompt_labels": "Getoor(B-researcher) has(O) multiple(O) best(O) paper(O) awards(O) ,(O) an(O) NSF(O) Career(O) Award(O) ,(O) and(O) is(O) an(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) ((O) AAAI(B-conference) )(O) Fellow(O) .(O)"}}
{"id": "194", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "metric", "conference", "location", "field", "university", "task", "researcher", "programming language", "organization", "country", "product", "algorithm"], "instance": {"id": "194", "words": ["gnuplot", "can", "be", "used", "from", "various", "programming", "languages", "to", "graph", "data", ",", "including", "Perl", "(", "via", "PDL", "and", "CPAN", "packages", ")", ",", "Python", "(", "via", ")", "."], "labels": ["B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-programming language", "O", "O", "B-product", "O", "B-product", "O", "O", "O", "B-programming language", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, metric, conference, location, field, university, task, researcher, programming language, organization, country, product, algorithm and O.\nSentence: gnuplot can be used from various programming languages to graph data , including Perl ( via PDL and CPAN packages ) , Python ( via ) .", "prompt_labels": "gnuplot(B-product) can(O) be(O) used(O) from(O) various(O) programming(O) languages(O) to(O) graph(O) data(O) ,(O) including(O) Perl(B-programming language) ((O) via(O) PDL(B-product) and(O) CPAN(B-product) packages(O) )(O) ,(O) Python(B-programming language) ((O) via(O) )(O) .(O)"}}
{"id": "273", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "person", "university", "metric", "location", "task", "organization", "programming language", "algorithm", "country", "product", "conference", "field"], "instance": {"id": "273", "words": ["The", "SMC", "is", "very", "similar", "to", "the", "more", "popular", "Jaccard", "index", "."], "labels": ["O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, person, university, metric, location, task, organization, programming language, algorithm, country, product, conference, field and O.\nSentence: The SMC is very similar to the more popular Jaccard index .", "prompt_labels": "The(O) SMC(B-organization) is(O) very(O) similar(O) to(O) the(O) more(O) popular(O) Jaccard(B-metric) index(I-metric) .(O)"}}
{"id": "181", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "conference", "task", "field", "metric", "person", "university", "algorithm", "product", "organization", "researcher", "location", "country"], "instance": {"id": "181", "words": ["In", "information", "theory", "and", "computer", "science", ",", "a", "code", "is", "usually", "considered", "as", "an", "algorithm", "that", "uniquely", "represents", "symbols", "from", "some", "source", "alphabet", ",", "by", "encoded", "strings", ",", "which", "may", "be", "in", "some", "other", "target", "alphabet", "."], "labels": ["O", "B-field", "I-field", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, conference, task, field, metric, person, university, algorithm, product, organization, researcher, location, country and O.\nSentence: In information theory and computer science , a code is usually considered as an algorithm that uniquely represents symbols from some source alphabet , by encoded strings , which may be in some other target alphabet .", "prompt_labels": "In(O) information(B-field) theory(I-field) and(O) computer(B-field) science(I-field) ,(O) a(O) code(O) is(O) usually(O) considered(O) as(O) an(O) algorithm(O) that(O) uniquely(O) represents(O) symbols(O) from(O) some(O) source(O) alphabet(O) ,(O) by(O) encoded(O) strings(O) ,(O) which(O) may(O) be(O) in(O) some(O) other(O) target(O) alphabet(O) .(O)"}}
{"id": "305", "dataset": "crossner_ai", "split": "dev", "label_list": ["researcher", "product", "field", "metric", "task", "conference", "organization", "algorithm", "university", "programming language", "location", "country", "person"], "instance": {"id": "305", "words": ["Popular", "examples", "of", "fitness", "functions", "based", "on", "the", "probabilities", "include", "maximum", "likelihood", "estimation", "and", "hinge", "loss", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "I-metric", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, product, field, metric, task, conference, organization, algorithm, university, programming language, location, country, person and O.\nSentence: Popular examples of fitness functions based on the probabilities include maximum likelihood estimation and hinge loss .", "prompt_labels": "Popular(O) examples(O) of(O) fitness(O) functions(O) based(O) on(O) the(O) probabilities(O) include(O) maximum(B-metric) likelihood(I-metric) estimation(I-metric) and(O) hinge(B-metric) loss(I-metric) .(O)"}}
{"id": "43", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "university", "algorithm", "person", "researcher", "metric", "product", "programming language", "task", "country", "field", "location", "organization"], "instance": {"id": "43", "words": ["The", "ACL", "has", "a", "European", "(", "European", "Chapter", "of", "the", "Association", "for", "Computational", "Linguistics", ")"], "labels": ["O", "B-conference", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, university, algorithm, person, researcher, metric, product, programming language, task, country, field, location, organization and O.\nSentence: The ACL has a European ( European Chapter of the Association for Computational Linguistics )", "prompt_labels": "The(O) ACL(B-conference) has(O) a(O) European(O) ((O) European(B-conference) Chapter(I-conference) of(I-conference) the(I-conference) Association(I-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) )(O)"}}
{"id": "232", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "university", "researcher", "algorithm", "metric", "field", "location", "person", "product", "task", "country", "programming language", "organization"], "instance": {"id": "232", "words": ["Alternatively", ",", "it", "can", "be", "used", "directly", "with", "the", "Perl", "Module", "TM", "(", "which", "also", "supports", "LTM", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-programming language", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, university, researcher, algorithm, metric, field, location, person, product, task, country, programming language, organization and O.\nSentence: Alternatively , it can be used directly with the Perl Module TM ( which also supports LTM ) .", "prompt_labels": "Alternatively(O) ,(O) it(O) can(O) be(O) used(O) directly(O) with(O) the(O) Perl(B-programming language) Module(O) TM(O) ((O) which(O) also(O) supports(O) LTM(O) )(O) .(O)"}}
{"id": "201", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "task", "field", "metric", "organization", "person", "product", "university", "programming language", "conference", "location", "country", "researcher"], "instance": {"id": "201", "words": ["Early", "interlingual", "MT", "systems", "were", "also", "built", "at", "Stanford", "in", "the", "1970s", "by", "Roger", "Schank", "and", "Yorick", "Wilks", ";", "the", "former", "became", "the", "basis", "of", "a", "commercial", "system", "for", "the", "transfer", "of", "funds", ",", "and", "the", "latter", "'s", "code", "is", "preserved", "at", "The", "Computer", "Museum", "at", "Boston", "as", "the", "first", "interlingual", "machine", "translation", "system", "."], "labels": ["O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "B-university", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "O", "O", "B-product", "I-product", "I-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, task, field, metric, organization, person, product, university, programming language, conference, location, country, researcher and O.\nSentence: Early interlingual MT systems were also built at Stanford in the 1970s by Roger Schank and Yorick Wilks ; the former became the basis of a commercial system for the transfer of funds , and the latter 's code is preserved at The Computer Museum at Boston as the first interlingual machine translation system .", "prompt_labels": "Early(O) interlingual(B-product) MT(I-product) systems(I-product) were(O) also(O) built(O) at(O) Stanford(B-university) in(O) the(O) 1970s(O) by(O) Roger(B-researcher) Schank(I-researcher) and(O) Yorick(B-researcher) Wilks(I-researcher) ;(O) the(O) former(O) became(O) the(O) basis(O) of(O) a(O) commercial(O) system(O) for(O) the(O) transfer(O) of(O) funds(O) ,(O) and(O) the(O) latter(O) 's(O) code(O) is(O) preserved(O) at(O) The(B-location) Computer(I-location) Museum(I-location) at(O) Boston(B-location) as(O) the(O) first(O) interlingual(B-product) machine(I-product) translation(I-product) system(I-product) .(O)"}}
{"id": "244", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "researcher", "person", "conference", "university", "task", "organization", "location", "country", "algorithm", "field", "product", "metric"], "instance": {"id": "244", "words": ["Convolution", "has", "applications", "that", "include", "probability", ",", "statistics", ",", "computer", "vision", ",", "natural", "language", "processing", ",", "image", "processing", "and", "signal", "processing", ",", "engineering", ",", "and", "differential", "equations", "."], "labels": ["B-algorithm", "O", "O", "O", "O", "B-field", "O", "B-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-field", "O", "O", "B-field", "I-field", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, researcher, person, conference, university, task, organization, location, country, algorithm, field, product, metric and O.\nSentence: Convolution has applications that include probability , statistics , computer vision , natural language processing , image processing and signal processing , engineering , and differential equations .", "prompt_labels": "Convolution(B-algorithm) has(O) applications(O) that(O) include(O) probability(B-field) ,(O) statistics(B-field) ,(O) computer(B-field) vision(I-field) ,(O) natural(B-field) language(I-field) processing(I-field) ,(O) image(B-field) processing(I-field) and(O) signal(B-field) processing(I-field) ,(O) engineering(B-field) ,(O) and(O) differential(B-field) equations(I-field) .(O)"}}
{"id": "318", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "field", "metric", "algorithm", "product", "researcher", "person", "organization", "programming language", "country", "university", "task", "location"], "instance": {"id": "318", "words": ["NMF", "is", "an", "instance", "of", "nonnegative", "quadratic", "programming", "(", "NQP", ")", ",", "just", "like", "the", "support", "vector", "machine", "(", "SVM", ")", "."], "labels": ["B-algorithm", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, field, metric, algorithm, product, researcher, person, organization, programming language, country, university, task, location and O.\nSentence: NMF is an instance of nonnegative quadratic programming ( NQP ) , just like the support vector machine ( SVM ) .", "prompt_labels": "NMF(B-algorithm) is(O) an(O) instance(O) of(O) nonnegative(B-algorithm) quadratic(I-algorithm) programming(I-algorithm) ((O) NQP(B-algorithm) )(O) ,(O) just(O) like(O) the(O) support(B-algorithm) vector(I-algorithm) machine(I-algorithm) ((O) SVM(B-algorithm) )(O) .(O)"}}
{"id": "45", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "person", "metric", "university", "field", "organization", "location", "programming language", "product", "country", "conference", "researcher", "algorithm"], "instance": {"id": "45", "words": ["Following", "his", "PhD", ",", "Ghahramani", "moved", "to", "the", "University", "of", "Toronto", "in", "1995", "as", "an", "ITRC", "Postdoctoral", "Fellow", "in", "the", "Artificial", "Intelligence", "Lab", ",", "working", "with", "Geoffrey", "Hinton", "."], "labels": ["O", "O", "O", "O", "B-researcher", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, person, metric, university, field, organization, location, programming language, product, country, conference, researcher, algorithm and O.\nSentence: Following his PhD , Ghahramani moved to the University of Toronto in 1995 as an ITRC Postdoctoral Fellow in the Artificial Intelligence Lab , working with Geoffrey Hinton .", "prompt_labels": "Following(O) his(O) PhD(O) ,(O) Ghahramani(B-researcher) moved(O) to(O) the(O) University(B-university) of(I-university) Toronto(I-university) in(O) 1995(O) as(O) an(O) ITRC(B-organization) Postdoctoral(O) Fellow(O) in(O) the(O) Artificial(B-organization) Intelligence(I-organization) Lab(I-organization) ,(O) working(O) with(O) Geoffrey(B-researcher) Hinton(I-researcher) .(O)"}}
{"id": "31", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "person", "university", "metric", "country", "organization", "location", "researcher", "algorithm", "field", "task", "product", "programming language"], "instance": {"id": "31", "words": ["The", "Arduino", "integrated", "development", "environment", "(", "IDE", ")", "is", "a", "cross-platform", "application", "(", "for", "Windows", ",", "macOS", ",", "and", "Linux", ")", "that", "is", "written", "in", "the", "programming", "language", "Java", "."], "labels": ["O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "O", "B-product", "O", "O", "B-product", "O", "O", "O", "O", "O", "O", "O", "O", "B-programming language", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, person, university, metric, country, organization, location, researcher, algorithm, field, task, product, programming language and O.\nSentence: The Arduino integrated development environment ( IDE ) is a cross-platform application ( for Windows , macOS , and Linux ) that is written in the programming language Java .", "prompt_labels": "The(O) Arduino(B-product) integrated(O) development(O) environment(O) ((O) IDE(O) )(O) is(O) a(O) cross-platform(O) application(O) ((O) for(O) Windows(B-product) ,(O) macOS(B-product) ,(O) and(O) Linux(B-product) )(O) that(O) is(O) written(O) in(O) the(O) programming(O) language(O) Java(B-programming language) .(O)"}}
{"id": "180", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "algorithm", "conference", "organization", "programming language", "university", "person", "researcher", "metric", "task", "product", "country", "location"], "instance": {"id": "180", "words": ["Along", "with", "Geoffrey", "Hinton", "and", "Yann", "LeCun", ",", "Bengio", "is", "considered", "by", "Cade", "Metz", "as", "one", "of", "the", "three", "people", "most", "responsible", "for", "the", "advancement", "of", "deep", "learning", "during", "the", "1990s", "and", "2000s", "."], "labels": ["O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, algorithm, conference, organization, programming language, university, person, researcher, metric, task, product, country, location and O.\nSentence: Along with Geoffrey Hinton and Yann LeCun , Bengio is considered by Cade Metz as one of the three people most responsible for the advancement of deep learning during the 1990s and 2000s .", "prompt_labels": "Along(O) with(O) Geoffrey(B-researcher) Hinton(I-researcher) and(O) Yann(B-researcher) LeCun(I-researcher) ,(O) Bengio(B-researcher) is(O) considered(O) by(O) Cade(B-researcher) Metz(I-researcher) as(O) one(O) of(O) the(O) three(O) people(O) most(O) responsible(O) for(O) the(O) advancement(O) of(O) deep(B-field) learning(I-field) during(O) the(O) 1990s(O) and(O) 2000s(O) .(O)"}}
{"id": "296", "dataset": "crossner_ai", "split": "dev", "label_list": ["conference", "country", "metric", "researcher", "programming language", "field", "task", "location", "algorithm", "university", "person", "product", "organization"], "instance": {"id": "296", "words": ["IAI", "is", "the", "world", "'s", "largest", "manufacturer", "of", "cartesian", "coordinate", "robot", "s", "and", "is", "an", "established", "leader", "in", "low", "cost", ",", "high", "performance", "SCARA", "robot", "s", "."], "labels": ["B-organization", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, country, metric, researcher, programming language, field, task, location, algorithm, university, person, product, organization and O.\nSentence: IAI is the world 's largest manufacturer of cartesian coordinate robot s and is an established leader in low cost , high performance SCARA robot s .", "prompt_labels": "IAI(B-organization) is(O) the(O) world(O) 's(O) largest(O) manufacturer(O) of(O) cartesian(B-product) coordinate(I-product) robot(I-product) s(O) and(O) is(O) an(O) established(O) leader(O) in(O) low(O) cost(O) ,(O) high(O) performance(O) SCARA(B-product) robot(I-product) s(O) .(O)"}}
{"id": "213", "dataset": "crossner_ai", "split": "dev", "label_list": ["organization", "country", "location", "algorithm", "field", "programming language", "person", "conference", "product", "task", "metric", "researcher", "university"], "instance": {"id": "213", "words": ["In", "his", "seminal", "paper", ",", "Harry", "Blum", "of", "the", "Air", "Force", "Cambridge", "Research", "Laboratories", "at", "Hanscom", "Air", "Force", "Base", ",", "in", "Bedford", ",", "Massachusetts", ",", "defined", "a", "medial", "axis", "for", "computing", "a", "skeleton", "of", "a", "shape", ",", "using", "an", "intuitive", "model", "of", "fire", "propagation", "on", "a", "grass", "field", ",", "where", "the", "field", "has", "the", "form", "of", "the", "given", "shape", "."], "labels": ["O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, location, algorithm, field, programming language, person, conference, product, task, metric, researcher, university and O.\nSentence: In his seminal paper , Harry Blum of the Air Force Cambridge Research Laboratories at Hanscom Air Force Base , in Bedford , Massachusetts , defined a medial axis for computing a skeleton of a shape , using an intuitive model of fire propagation on a grass field , where the field has the form of the given shape .", "prompt_labels": "In(O) his(O) seminal(O) paper(O) ,(O) Harry(B-researcher) Blum(I-researcher) of(O) the(O) Air(B-organization) Force(I-organization) Cambridge(I-organization) Research(I-organization) Laboratories(I-organization) at(O) Hanscom(B-location) Air(I-location) Force(I-location) Base(I-location) ,(O) in(O) Bedford(B-location) ,(O) Massachusetts(B-location) ,(O) defined(O) a(O) medial(O) axis(O) for(O) computing(O) a(O) skeleton(O) of(O) a(O) shape(O) ,(O) using(O) an(O) intuitive(O) model(O) of(O) fire(O) propagation(O) on(O) a(O) grass(O) field(O) ,(O) where(O) the(O) field(O) has(O) the(O) form(O) of(O) the(O) given(O) shape(O) .(O)"}}
{"id": "5", "dataset": "crossner_ai", "split": "dev", "label_list": ["organization", "field", "metric", "product", "programming language", "person", "conference", "algorithm", "task", "country", "researcher", "university", "location"], "instance": {"id": "5", "words": ["At", "Indiana", "University", "in", "1999", "he", "organized", "such", "a", "symposium", ",", "and", "in", "April", "2000", ",", "he", "organized", "a", "larger", "symposium", "entitled", "Spiritual", "Robots", "at", "Stanford", "University", ",", "in", "which", "he", "moderated", "a", "panel", "consisting", "of", "Ray", "Kurzweil", ",", "Hans", "Moravec", ",", "Kevin", "Kelly", ",", "Ralph", "Merkle", ",", "Bill", "Joy", ",", "Frank", "Drake", ",", "John", "Henry", "Holland", "and", "John", "Koza", "."], "labels": ["O", "B-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "O", "B-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "I-researcher", "O", "B-researcher", "I-researcher", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, field, metric, product, programming language, person, conference, algorithm, task, country, researcher, university, location and O.\nSentence: At Indiana University in 1999 he organized such a symposium , and in April 2000 , he organized a larger symposium entitled Spiritual Robots at Stanford University , in which he moderated a panel consisting of Ray Kurzweil , Hans Moravec , Kevin Kelly , Ralph Merkle , Bill Joy , Frank Drake , John Henry Holland and John Koza .", "prompt_labels": "At(O) Indiana(B-university) University(I-university) in(O) 1999(O) he(O) organized(O) such(O) a(O) symposium(O) ,(O) and(O) in(O) April(O) 2000(O) ,(O) he(O) organized(O) a(O) larger(O) symposium(O) entitled(O) Spiritual(B-conference) Robots(I-conference) at(O) Stanford(B-university) University(I-university) ,(O) in(O) which(O) he(O) moderated(O) a(O) panel(O) consisting(O) of(O) Ray(B-researcher) Kurzweil(I-researcher) ,(O) Hans(B-researcher) Moravec(I-researcher) ,(O) Kevin(B-researcher) Kelly(I-researcher) ,(O) Ralph(B-researcher) Merkle(I-researcher) ,(O) Bill(B-researcher) Joy(I-researcher) ,(O) Frank(B-researcher) Drake(I-researcher) ,(O) John(B-researcher) Henry(I-researcher) Holland(I-researcher) and(O) John(B-researcher) Koza(I-researcher) .(O)"}}
{"id": "276", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "field", "country", "product", "conference", "university", "person", "programming language", "task", "location", "organization", "metric", "researcher"], "instance": {"id": "276", "words": ["Bandwidth", "in", "hertz", "is", "a", "central", "concept", "in", "many", "fields", ",", "including", "electronics", ",", "information", "theory", ",", "digital", "communication", "s", ",", "radio", "communication", "s", ",", "signal", "processing", ",", "and", "spectroscopy", "and", "is", "one", "of", "the", "determinants", "of", "the", "capacity", "of", "a", "given", "communication", "channel", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "O", "B-field", "I-field", "O", "O", "B-field", "I-field", "O", "O", "B-field", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, field, country, product, conference, university, person, programming language, task, location, organization, metric, researcher and O.\nSentence: Bandwidth in hertz is a central concept in many fields , including electronics , information theory , digital communication s , radio communication s , signal processing , and spectroscopy and is one of the determinants of the capacity of a given communication channel .", "prompt_labels": "Bandwidth(O) in(O) hertz(O) is(O) a(O) central(O) concept(O) in(O) many(O) fields(O) ,(O) including(O) electronics(B-field) ,(O) information(B-field) theory(I-field) ,(O) digital(B-field) communication(I-field) s(O) ,(O) radio(B-field) communication(I-field) s(O) ,(O) signal(B-field) processing(I-field) ,(O) and(O) spectroscopy(B-field) and(O) is(O) one(O) of(O) the(O) determinants(O) of(O) the(O) capacity(O) of(O) a(O) given(O) communication(O) channel(O) .(O)"}}
{"id": "143", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "task", "country", "conference", "programming language", "university", "location", "product", "organization", "field", "metric", "researcher", "person"], "instance": {"id": "143", "words": ["Classification", "can", "be", "thought", "of", "as", "two", "separate", "problems", "-", "binary", "classification", "and", "multiclass", "classification", "."], "labels": ["B-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "B-task", "I-task", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, task, country, conference, programming language, university, location, product, organization, field, metric, researcher, person and O.\nSentence: Classification can be thought of as two separate problems - binary classification and multiclass classification .", "prompt_labels": "Classification(B-task) can(O) be(O) thought(O) of(O) as(O) two(O) separate(O) problems(O) -(O) binary(B-task) classification(I-task) and(O) multiclass(B-task) classification(I-task) .(O)"}}
{"id": "210", "dataset": "crossner_ai", "split": "dev", "label_list": ["organization", "field", "algorithm", "metric", "task", "university", "researcher", "product", "country", "person", "location", "programming language", "conference"], "instance": {"id": "210", "words": ["An", "illustration", "of", "their", "capabilities", "is", "given", "by", "the", "ImageNet", "Large", "Scale", "Visual", "Recognition", "Challenge", ";", "this", "is", "a", "benchmark", "in", "object", "classification", "and", "detection", ",", "with", "millions", "of", "images", "and", "hundreds", "of", "object", "classes", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-conference", "I-conference", "I-conference", "I-conference", "I-conference", "I-conference", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "I-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, field, algorithm, metric, task, university, researcher, product, country, person, location, programming language, conference and O.\nSentence: An illustration of their capabilities is given by the ImageNet Large Scale Visual Recognition Challenge ; this is a benchmark in object classification and detection , with millions of images and hundreds of object classes .", "prompt_labels": "An(O) illustration(O) of(O) their(O) capabilities(O) is(O) given(O) by(O) the(O) ImageNet(B-conference) Large(I-conference) Scale(I-conference) Visual(I-conference) Recognition(I-conference) Challenge(I-conference) ;(O) this(O) is(O) a(O) benchmark(O) in(O) object(B-task) classification(I-task) and(I-task) detection(I-task) ,(O) with(O) millions(O) of(O) images(O) and(O) hundreds(O) of(O) object(O) classes(O) .(O)"}}
{"id": "300", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "field", "university", "product", "country", "organization", "conference", "metric", "location", "algorithm", "researcher", "programming language", "task"], "instance": {"id": "300", "words": ["The", "FALSE", "positive", "rate", "is", "the", "proportion", "of", "all", "negatives", "that", "still", "yield", "positive", "test", "outcomes", ",", "i.e.", ",", "the", "conditional", "probability", "of", "a", "positive", "test", "result", "given", "an", "event", "that", "was", "not", "present", "."], "labels": ["O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, field, university, product, country, organization, conference, metric, location, algorithm, researcher, programming language, task and O.\nSentence: The FALSE positive rate is the proportion of all negatives that still yield positive test outcomes , i.e. , the conditional probability of a positive test result given an event that was not present .", "prompt_labels": "The(O) FALSE(B-metric) positive(I-metric) rate(I-metric) is(O) the(O) proportion(O) of(O) all(O) negatives(O) that(O) still(O) yield(O) positive(O) test(O) outcomes(O) ,(O) i.e.(O) ,(O) the(O) conditional(O) probability(O) of(O) a(O) positive(O) test(O) result(O) given(O) an(O) event(O) that(O) was(O) not(O) present(O) .(O)"}}
{"id": "328", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "researcher", "country", "location", "task", "algorithm", "metric", "university", "product", "programming language", "organization", "conference", "person"], "instance": {"id": "328", "words": ["In", "terms", "of", "freely", "available", "resources", ",", "Carnegie", "Mellon", "University", "'", "s", "Sphinx", "toolkit", "is", "one", "place", "to", "start", "to", "both", "learn", "about", "speech", "recognition", "and", "to", "start", "experimenting", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "B-product", "I-product", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-task", "I-task", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, researcher, country, location, task, algorithm, metric, university, product, programming language, organization, conference, person and O.\nSentence: In terms of freely available resources , Carnegie Mellon University ' s Sphinx toolkit is one place to start to both learn about speech recognition and to start experimenting .", "prompt_labels": "In(O) terms(O) of(O) freely(O) available(O) resources(O) ,(O) Carnegie(B-university) Mellon(I-university) University(I-university) '(O) s(O) Sphinx(B-product) toolkit(I-product) is(O) one(O) place(O) to(O) start(O) to(O) both(O) learn(O) about(O) speech(B-task) recognition(I-task) and(O) to(O) start(O) experimenting(O) .(O)"}}
{"id": "228", "dataset": "crossner_ai", "split": "dev", "label_list": ["person", "organization", "programming language", "task", "metric", "university", "algorithm", "country", "researcher", "location", "conference", "field", "product"], "instance": {"id": "228", "words": ["(", "2005", ")", "we", "can", "differ", "three", "different", "perspectives", "of", "text", "mining", ",", "namely", "text", "mining", "as", "information", "extraction", ",", "text", "mining", "as", "text", "data", "mining", ",", "and", "text", "mining", "as", "Data", "mining", "(", "Knowledge", "Discovery", "in", "Databases", ")", "process.Hotho", ",", "A.", ",", "Nürnberger", ",", "A.", "and", "Paaß", ",", "G.", "(", "2005", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-field", "I-field", "O", "O", "B-field", "I-field", "O", "B-task", "I-task", "O", "B-field", "I-field", "O", "O", "B-field", "I-field", "O", "O", "B-field", "I-field", "O", "B-field", "I-field", "O", "B-task", "I-task", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, programming language, task, metric, university, algorithm, country, researcher, location, conference, field, product and O.\nSentence: ( 2005 ) we can differ three different perspectives of text mining , namely text mining as information extraction , text mining as text data mining , and text mining as Data mining ( Knowledge Discovery in Databases ) process.Hotho , A. , Nürnberger , A. and Paaß , G. ( 2005 ) .", "prompt_labels": "((O) 2005(O) )(O) we(O) can(O) differ(O) three(O) different(O) perspectives(O) of(O) text(B-field) mining(I-field) ,(O) namely(O) text(B-field) mining(I-field) as(O) information(B-task) extraction(I-task) ,(O) text(B-field) mining(I-field) as(O) text(O) data(B-field) mining(I-field) ,(O) and(O) text(B-field) mining(I-field) as(O) Data(B-field) mining(I-field) ((O) Knowledge(B-task) Discovery(I-task) in(O) Databases(O) )(O) process.Hotho(O) ,(O) A.(O) ,(O) Nürnberger(O) ,(O) A.(O) and(O) Paaß(O) ,(O) G.(O) ((O) 2005(O) )(O) .(O)"}}
{"id": "122", "dataset": "crossner_ai", "split": "dev", "label_list": ["field", "country", "conference", "organization", "programming language", "university", "location", "algorithm", "researcher", "task", "metric", "product", "person"], "instance": {"id": "122", "words": ["The", "robot", "kit", "is", "Android-based", ",", "and", "it", "is", "programmed", "using", "Java", ",", "the", "Blocks", "programming", "interface", ",", "or", "other", "Android", "programming", "systems", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-programming language", "O", "O", "O", "O", "O", "O", "O", "O", "B-product", "I-product", "I-product", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, country, conference, organization, programming language, university, location, algorithm, researcher, task, metric, product, person and O.\nSentence: The robot kit is Android-based , and it is programmed using Java , the Blocks programming interface , or other Android programming systems .", "prompt_labels": "The(O) robot(O) kit(O) is(O) Android-based(O) ,(O) and(O) it(O) is(O) programmed(O) using(O) Java(B-programming language) ,(O) the(O) Blocks(O) programming(O) interface(O) ,(O) or(O) other(O) Android(B-product) programming(I-product) systems(I-product) .(O)"}}
{"id": "106", "dataset": "crossner_ai", "split": "dev", "label_list": ["programming language", "organization", "person", "country", "task", "location", "product", "university", "algorithm", "conference", "field", "metric", "researcher"], "instance": {"id": "106", "words": ["Since", "2009", ",", "the", "recurrent", "neural", "network", "s", "and", "deep", "feedforward", "neural", "networks", "developed", "in", "the", "research", "group", "of", "Jürgen", "Schmidhuber", "at", "the", "Swiss", "AI", "Lab", "IDSIA", "have", "won", "several", "international", "handwriting", "competitions", ".."], "labels": ["O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "I-algorithm", "O", "O", "O", "O", "O", "O", "B-researcher", "I-researcher", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, organization, person, country, task, location, product, university, algorithm, conference, field, metric, researcher and O.\nSentence: Since 2009 , the recurrent neural network s and deep feedforward neural networks developed in the research group of Jürgen Schmidhuber at the Swiss AI Lab IDSIA have won several international handwriting competitions ..", "prompt_labels": "Since(O) 2009(O) ,(O) the(O) recurrent(B-algorithm) neural(I-algorithm) network(I-algorithm) s(O) and(O) deep(B-algorithm) feedforward(I-algorithm) neural(I-algorithm) networks(I-algorithm) developed(O) in(O) the(O) research(O) group(O) of(O) Jürgen(B-researcher) Schmidhuber(I-researcher) at(O) the(O) Swiss(B-organization) AI(I-organization) Lab(I-organization) IDSIA(I-organization) have(O) won(O) several(O) international(O) handwriting(O) competitions(O) ..(O)"}}
{"id": "331", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "conference", "location", "country", "field", "metric", "product", "university", "person", "researcher", "programming language", "task", "organization"], "instance": {"id": "331", "words": ["In", "particular", ",", "RLS", "is", "designed", "to", "minimize", "the", "mean", "squared", "error", "between", "the", "predicted", "values", "and", "the", "TRUE", "labels", ",", "subject", "to", "regularization", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, conference, location, country, field, metric, product, university, person, researcher, programming language, task, organization and O.\nSentence: In particular , RLS is designed to minimize the mean squared error between the predicted values and the TRUE labels , subject to regularization .", "prompt_labels": "In(O) particular(O) ,(O) RLS(O) is(O) designed(O) to(O) minimize(O) the(O) mean(B-metric) squared(I-metric) error(I-metric) between(O) the(O) predicted(O) values(O) and(O) the(O) TRUE(O) labels(O) ,(O) subject(O) to(O) regularization(O) .(O)"}}
{"id": "95", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "conference", "researcher", "person", "metric", "task", "organization", "field", "programming language", "university", "location", "country", "product"], "instance": {"id": "95", "words": ["In", "preliminary", "experimental", "results", "with", "noisy", "datasets", ",", "BrownBoost", "outperformed", "AdaBoost", "'", "s", "generalization", "error", ";", "however", ",", "LogitBoost", "performed", "as", "well", "as", "BrownBoost", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "O", "B-algorithm", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "O", "O", "O", "O", "B-algorithm", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, conference, researcher, person, metric, task, organization, field, programming language, university, location, country, product and O.\nSentence: In preliminary experimental results with noisy datasets , BrownBoost outperformed AdaBoost ' s generalization error ; however , LogitBoost performed as well as BrownBoost .", "prompt_labels": "In(O) preliminary(O) experimental(O) results(O) with(O) noisy(O) datasets(O) ,(O) BrownBoost(B-algorithm) outperformed(O) AdaBoost(B-algorithm) '(O) s(O) generalization(O) error(O) ;(O) however(O) ,(O) LogitBoost(B-algorithm) performed(O) as(O) well(O) as(O) BrownBoost(B-algorithm) .(O)"}}
{"id": "15", "dataset": "crossner_ai", "split": "dev", "label_list": ["algorithm", "country", "product", "university", "location", "programming language", "researcher", "task", "conference", "metric", "field", "person", "organization"], "instance": {"id": "15", "words": ["In", "Cryo", "Electron", "Tomography", ",", "where", "the", "limited", "number", "of", "projections", "are", "acquired", "due", "to", "the", "hardware", "limitations", "and", "to", "avoid", "the", "biological", "specimen", "damage", ",", "it", "can", "be", "used", "along", "with", "compressive", "sensing", "techniques", "or", "regularization", "functions", "(", "e.g.", "Huber", "loss", ")", "to", "improve", "the", "reconstruction", "for", "better", "interpretation", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-algorithm", "I-algorithm", "I-algorithm", "O", "B-algorithm", "I-algorithm", "O", "O", "B-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, country, product, university, location, programming language, researcher, task, conference, metric, field, person, organization and O.\nSentence: In Cryo Electron Tomography , where the limited number of projections are acquired due to the hardware limitations and to avoid the biological specimen damage , it can be used along with compressive sensing techniques or regularization functions ( e.g. Huber loss ) to improve the reconstruction for better interpretation .", "prompt_labels": "In(O) Cryo(O) Electron(O) Tomography(O) ,(O) where(O) the(O) limited(O) number(O) of(O) projections(O) are(O) acquired(O) due(O) to(O) the(O) hardware(O) limitations(O) and(O) to(O) avoid(O) the(O) biological(O) specimen(O) damage(O) ,(O) it(O) can(O) be(O) used(O) along(O) with(O) compressive(B-algorithm) sensing(I-algorithm) techniques(I-algorithm) or(O) regularization(B-algorithm) functions(I-algorithm) ((O) e.g.(O) Huber(B-metric) loss(I-metric) )(O) to(O) improve(O) the(O) reconstruction(O) for(O) better(O) interpretation(O) .(O)"}}
{"id": "154", "dataset": "crossner_ai", "split": "dev", "label_list": ["task", "programming language", "university", "conference", "product", "researcher", "location", "organization", "field", "metric", "person", "country", "algorithm"], "instance": {"id": "154", "words": ["The", "commonly", "used", "metrics", "are", "the", "mean", "squared", "error", "and", "root", "mean", "squared", "error", ",", "the", "latter", "having", "been", "used", "in", "the", "Netflix", "Prize", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-metric", "I-metric", "I-metric", "O", "B-metric", "I-metric", "I-metric", "I-metric", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, programming language, university, conference, product, researcher, location, organization, field, metric, person, country, algorithm and O.\nSentence: The commonly used metrics are the mean squared error and root mean squared error , the latter having been used in the Netflix Prize .", "prompt_labels": "The(O) commonly(O) used(O) metrics(O) are(O) the(O) mean(B-metric) squared(I-metric) error(I-metric) and(O) root(B-metric) mean(I-metric) squared(I-metric) error(I-metric) ,(O) the(O) latter(O) having(O) been(O) used(O) in(O) the(O) Netflix(O) Prize(O) .(O)"}}
{"id": "324", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "book", "writer", "person", "magazine", "poem", "literary genre", "country", "award", "organization", "event"], "instance": {"id": "324", "words": ["Wilfred", "Owen", "was", "killed", "in", "battle", ";", "but", "his", "poems", "created", "at", "the", "front", "did", "achieve", "popular", "attention", "after", "the", "war", "'s", "end", ",", ".e.g.", ",", "Dulce", "Et", "Decorum", "Est", ",", "Insensibility", ",", "Anthem", "for", "Doomed", "Youth", ",", "Futility", "and", "Strange", "Meeting", "."], "labels": ["B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "O", "B-poem", "O", "B-poem", "I-poem", "I-poem", "I-poem", "O", "B-poem", "O", "B-poem", "I-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, book, writer, person, magazine, poem, literary genre, country, award, organization, event and O.\nSentence: Wilfred Owen was killed in battle ; but his poems created at the front did achieve popular attention after the war 's end , .e.g. , Dulce Et Decorum Est , Insensibility , Anthem for Doomed Youth , Futility and Strange Meeting .", "prompt_labels": "Wilfred(B-writer) Owen(I-writer) was(O) killed(O) in(O) battle(O) ;(O) but(O) his(O) poems(O) created(O) at(O) the(O) front(O) did(O) achieve(O) popular(O) attention(O) after(O) the(O) war(O) 's(O) end(O) ,(O) .e.g.(O) ,(O) Dulce(B-poem) Et(I-poem) Decorum(I-poem) Est(I-poem) ,(O) Insensibility(B-poem) ,(O) Anthem(B-poem) for(I-poem) Doomed(I-poem) Youth(I-poem) ,(O) Futility(B-poem) and(O) Strange(B-poem) Meeting(I-poem) .(O)"}}
{"id": "11", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "event", "poem", "location", "award", "book", "literary genre", "writer", "organization", "person", "magazine"], "instance": {"id": "11", "words": ["The", "Lancer", "/", "Ace", "editions", "(", "1966-1977", ")", ",", "under", "the", "direction", "of", "L.", "Sprague", "de", "Camp", "and", "Lin", "Carter", ",", "were", "the", "first", "comprehensive", "paperbacks", ",", "compiling", "the", "material", "from", "the", "Gnome", "Press", "series", "together", "in", "chronological", "order", "with", "all", "the", "remaining", "original", "Howard", "material", ",", "including", "that", "left", "unpublished", "in", "his", "lifetime", "and", "fragments", "and", "outlines", "."], "labels": ["B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, poem, location, award, book, literary genre, writer, organization, person, magazine and O.\nSentence: The Lancer / Ace editions ( 1966-1977 ) , under the direction of L. Sprague de Camp and Lin Carter , were the first comprehensive paperbacks , compiling the material from the Gnome Press series together in chronological order with all the remaining original Howard material , including that left unpublished in his lifetime and fragments and outlines .", "prompt_labels": "The(B-book) Lancer(I-book) /(I-book) Ace(I-book) editions(I-book) ((O) 1966-1977(O) )(O) ,(O) under(O) the(O) direction(O) of(O) L.(B-writer) Sprague(I-writer) de(I-writer) Camp(I-writer) and(O) Lin(B-writer) Carter(I-writer) ,(O) were(O) the(O) first(O) comprehensive(O) paperbacks(O) ,(O) compiling(O) the(O) material(O) from(O) the(O) Gnome(O) Press(O) series(O) together(O) in(O) chronological(O) order(O) with(O) all(O) the(O) remaining(O) original(O) Howard(B-writer) material(O) ,(O) including(O) that(O) left(O) unpublished(O) in(O) his(O) lifetime(O) and(O) fragments(O) and(O) outlines(O) .(O)"}}
{"id": "150", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "location", "award", "book", "country", "writer", "literary genre", "person", "poem", "organization", "magazine"], "instance": {"id": "150", "words": ["Among", "books", "on", "the", "list", "considered", "to", "be", "the", "Great", "American", "Novel", "were", "Moby-Dick", ",", "Adventures", "of", "Huckleberry", "Finn", ",", "The", "Great", "Gatsby", ",", "The", "Grapes", "of", "Wrath", ",", "The", "Catcher", "in", "the", "Rye", ",", "Invisible", "Man", ",", "and", "To", "Kill", "a", "Mockingbird", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "O", "B-book", "O", "B-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "O", "O", "B-book", "I-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, award, book, country, writer, literary genre, person, poem, organization, magazine and O.\nSentence: Among books on the list considered to be the Great American Novel were Moby-Dick , Adventures of Huckleberry Finn , The Great Gatsby , The Grapes of Wrath , The Catcher in the Rye , Invisible Man , and To Kill a Mockingbird .", "prompt_labels": "Among(O) books(O) on(O) the(O) list(O) considered(O) to(O) be(O) the(O) Great(B-literary genre) American(I-literary genre) Novel(I-literary genre) were(O) Moby-Dick(B-book) ,(O) Adventures(B-book) of(I-book) Huckleberry(I-book) Finn(I-book) ,(O) The(B-book) Great(I-book) Gatsby(I-book) ,(O) The(B-book) Grapes(I-book) of(I-book) Wrath(I-book) ,(O) The(B-book) Catcher(I-book) in(I-book) the(I-book) Rye(I-book) ,(O) Invisible(B-book) Man(I-book) ,(O) and(O) To(B-book) Kill(I-book) a(I-book) Mockingbird(I-book) .(O)"}}
{"id": "376", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "literary genre", "country", "magazine", "organization", "writer", "award", "book", "location", "poem", "person"], "instance": {"id": "376", "words": ["She", "wrote", "two", "additional", "novels", ",", "Northanger", "Abbey", "and", "Persuasion", ",", "both", "published", "posthumously", "in", "1818", ",", "and", "began", "another", ",", "eventually", "titled", "Sanditon", ",", "but", "died", "before", "its", "completion", "."], "labels": ["O", "O", "O", "O", "B-literary genre", "O", "B-book", "I-book", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, literary genre, country, magazine, organization, writer, award, book, location, poem, person and O.\nSentence: She wrote two additional novels , Northanger Abbey and Persuasion , both published posthumously in 1818 , and began another , eventually titled Sanditon , but died before its completion .", "prompt_labels": "She(O) wrote(O) two(O) additional(O) novels(B-literary genre) ,(O) Northanger(B-book) Abbey(I-book) and(O) Persuasion(B-book) ,(O) both(O) published(O) posthumously(O) in(O) 1818(O) ,(O) and(O) began(O) another(O) ,(O) eventually(O) titled(O) Sanditon(B-book) ,(O) but(O) died(O) before(O) its(O) completion(O) .(O)"}}
{"id": "304", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "person", "book", "literary genre", "poem", "writer", "location", "magazine", "event", "award", "country"], "instance": {"id": "304", "words": ["At", "the", "entrance", "to", "the", "Gerasimov", "Institute", "of", "Cinematography", "in", "Moscow", ",", "there", "is", "a", "monument", "that", "includes", "statues", "of", "Tarkovsky", ",", "Gennady", "Shpalikov", "and", "Vasily", "Shukshin", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, book, literary genre, poem, writer, location, magazine, event, award, country and O.\nSentence: At the entrance to the Gerasimov Institute of Cinematography in Moscow , there is a monument that includes statues of Tarkovsky , Gennady Shpalikov and Vasily Shukshin .", "prompt_labels": "At(O) the(O) entrance(O) to(O) the(O) Gerasimov(B-organization) Institute(I-organization) of(I-organization) Cinematography(I-organization) in(O) Moscow(B-location) ,(O) there(O) is(O) a(O) monument(O) that(O) includes(O) statues(O) of(O) Tarkovsky(B-writer) ,(O) Gennady(B-writer) Shpalikov(I-writer) and(O) Vasily(B-writer) Shukshin(I-writer) .(O)"}}
{"id": "190", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "book", "location", "poem", "writer", "country", "award", "magazine", "literary genre", "person", "organization"], "instance": {"id": "190", "words": ["Another", "poem", "that", "is", "ambiguous", "in", "this", "respect", "is", "The", "Virgin", "Carrying", "a", "Lantern", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "I-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, book, location, poem, writer, country, award, magazine, literary genre, person, organization and O.\nSentence: Another poem that is ambiguous in this respect is The Virgin Carrying a Lantern .", "prompt_labels": "Another(O) poem(O) that(O) is(O) ambiguous(O) in(O) this(O) respect(O) is(O) The(B-poem) Virgin(I-poem) Carrying(I-poem) a(I-poem) Lantern(I-poem) .(O)"}}
{"id": "297", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "award", "writer", "literary genre", "event", "country", "book", "location", "poem", "magazine", "organization"], "instance": {"id": "297", "words": ["The", "late", "1980s", "and", "the", "1990s", "saw", "a", "boom", "in", "popular-fiction", "versions", "of", "alternate", "history", ",", "fueled", "by", "the", "emergence", "of", "the", "prolific", "alternate", "history", "author", "Harry", "Turtledove", ",", "as", "well", "as", "the", "development", "of", "the", "steampunk", "genre", "and", "two", "series", "of", "anthologies", "-", "the", "What", "Might", "Have", "Been", "series", "edited", "by", "Gregory", "Benford", "and", "the", "Alternate", "series", "edited", "by", "Mike", "Resnick", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, writer, literary genre, event, country, book, location, poem, magazine, organization and O.\nSentence: The late 1980s and the 1990s saw a boom in popular-fiction versions of alternate history , fueled by the emergence of the prolific alternate history author Harry Turtledove , as well as the development of the steampunk genre and two series of anthologies - the What Might Have Been series edited by Gregory Benford and the Alternate series edited by Mike Resnick .", "prompt_labels": "The(O) late(O) 1980s(O) and(O) the(O) 1990s(O) saw(O) a(O) boom(O) in(O) popular-fiction(B-literary genre) versions(O) of(O) alternate(B-literary genre) history(I-literary genre) ,(O) fueled(O) by(O) the(O) emergence(O) of(O) the(O) prolific(O) alternate(O) history(O) author(O) Harry(B-writer) Turtledove(I-writer) ,(O) as(O) well(O) as(O) the(O) development(O) of(O) the(O) steampunk(B-literary genre) genre(I-literary genre) and(O) two(O) series(O) of(O) anthologies(O) -(O) the(O) What(B-book) Might(I-book) Have(I-book) Been(I-book) series(O) edited(O) by(O) Gregory(B-writer) Benford(I-writer) and(O) the(O) Alternate(B-book) series(I-book) edited(O) by(O) Mike(B-writer) Resnick(I-writer) .(O)"}}
{"id": "397", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "book", "location", "literary genre", "magazine", "country", "event", "organization", "writer", "award", "poem"], "instance": {"id": "397", "words": ["Iain", "Moncreiffe", "and", "Don", "Pottinger", "jokingly", "mentioned", "in", "their", "1956", "book", "Blood", "Royal", "the", "sentence", ":", "Without", "Little", "Father", "need", "for", "Big", "Brother", ",", "referring", "to", "the", "Russian", "Revolution", "and", "the", "Soviet", "Union", "."], "labels": ["B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, book, location, literary genre, magazine, country, event, organization, writer, award, poem and O.\nSentence: Iain Moncreiffe and Don Pottinger jokingly mentioned in their 1956 book Blood Royal the sentence : Without Little Father need for Big Brother , referring to the Russian Revolution and the Soviet Union .", "prompt_labels": "Iain(B-writer) Moncreiffe(I-writer) and(O) Don(B-writer) Pottinger(I-writer) jokingly(O) mentioned(O) in(O) their(O) 1956(O) book(O) Blood(B-book) Royal(I-book) the(O) sentence(O) :(O) Without(O) Little(B-person) Father(I-person) need(O) for(O) Big(B-person) Brother(I-person) ,(O) referring(O) to(O) the(O) Russian(B-event) Revolution(I-event) and(O) the(O) Soviet(B-country) Union(I-country) .(O)"}}
{"id": "53", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "country", "poem", "person", "award", "book", "literary genre", "writer", "event", "location", "magazine"], "instance": {"id": "53", "words": ["Rolling", "Stone", "magazine", "ranked", "him", "number", "13", "in", "its", "list", "of", "100", "Greatest", "Artists", "."], "labels": ["B-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, poem, person, award, book, literary genre, writer, event, location, magazine and O.\nSentence: Rolling Stone magazine ranked him number 13 in its list of 100 Greatest Artists .", "prompt_labels": "Rolling(B-magazine) Stone(I-magazine) magazine(O) ranked(O) him(O) number(O) 13(O) in(O) its(O) list(O) of(O) 100(O) Greatest(O) Artists(O) .(O)"}}
{"id": "91", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "poem", "magazine", "book", "event", "country", "organization", "award", "person", "location", "literary genre"], "instance": {"id": "91", "words": ["The", "concept", "of", "a", "cross-time", "version", "of", "a", "world", "war", ",", "involving", "rival", "paratime", "empires", ",", "was", "developed", "in", "Fritz", "Leiber", "'", "s", "Change", "War", "series", ",", "starting", "with", "the", "Hugo", "Award", "winning", "The", "Big", "Time", "(", "1958", ")", ";", "followed", "by", "Richard", "C.", "Meredith", "'", "s", "Timeliner", "trilogy", "in", "the", "1970s", ",", "Michael", "McCollum", "'", "s", "A", "Greater", "Infinity", "(", "1982", ")", "and", "John", "Barnes", "'", "Timeline", "Wars", "trilogy", "in", "the", "1990s", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "B-award", "I-award", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-book", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, poem, magazine, book, event, country, organization, award, person, location, literary genre and O.\nSentence: The concept of a cross-time version of a world war , involving rival paratime empires , was developed in Fritz Leiber ' s Change War series , starting with the Hugo Award winning The Big Time ( 1958 ) ; followed by Richard C. Meredith ' s Timeliner trilogy in the 1970s , Michael McCollum ' s A Greater Infinity ( 1982 ) and John Barnes ' Timeline Wars trilogy in the 1990s .", "prompt_labels": "The(O) concept(O) of(O) a(O) cross-time(O) version(O) of(O) a(O) world(O) war(O) ,(O) involving(O) rival(O) paratime(O) empires(O) ,(O) was(O) developed(O) in(O) Fritz(B-writer) Leiber(I-writer) '(O) s(O) Change(B-book) War(I-book) series(I-book) ,(O) starting(O) with(O) the(O) Hugo(B-award) Award(I-award) winning(O) The(B-book) Big(I-book) Time(I-book) ((O) 1958(O) )(O) ;(O) followed(O) by(O) Richard(B-writer) C.(I-writer) Meredith(I-writer) '(O) s(O) Timeliner(B-book) trilogy(O) in(O) the(O) 1970s(O) ,(O) Michael(B-writer) McCollum(I-writer) '(O) s(O) A(B-book) Greater(I-book) Infinity(I-book) ((O) 1982(O) )(O) and(O) John(B-writer) Barnes(I-writer) '(O) Timeline(B-book) Wars(I-book) trilogy(I-book) in(O) the(O) 1990s(O) .(O)"}}
{"id": "5", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "award", "writer", "literary genre", "magazine", "organization", "location", "event", "poem", "country", "person"], "instance": {"id": "5", "words": ["The", "poor", "conditions", "of", "the", "hospital", "in", "Lambaréné", "were", "also", "famously", "criticized", "by", "Nigerian", "professor", "and", "novelist", "Chinua", "Achebe", "in", "his", "essay", "on", "Joseph", "Conrad", "'", "s", "novel", "Heart", "of", "Darkness", ":", "In", "a", "comment", "which", "has", "often", "been", "quoted", "Schweitzer", "says", ":", "'", "The", "African", "is", "indeed", "my", "brother", "but", "my", "junior", "brother", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "B-literary genre", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, award, writer, literary genre, magazine, organization, location, event, poem, country, person and O.\nSentence: The poor conditions of the hospital in Lambaréné were also famously criticized by Nigerian professor and novelist Chinua Achebe in his essay on Joseph Conrad ' s novel Heart of Darkness : In a comment which has often been quoted Schweitzer says : ' The African is indeed my brother but my junior brother .", "prompt_labels": "The(O) poor(O) conditions(O) of(O) the(O) hospital(O) in(O) Lambaréné(B-location) were(O) also(O) famously(O) criticized(O) by(O) Nigerian(O) professor(O) and(O) novelist(O) Chinua(B-writer) Achebe(I-writer) in(O) his(O) essay(O) on(O) Joseph(B-person) Conrad(I-person) '(O) s(O) novel(B-literary genre) Heart(B-book) of(I-book) Darkness(I-book) :(O) In(O) a(O) comment(O) which(O) has(O) often(O) been(O) quoted(O) Schweitzer(B-writer) says(O) :(O) '(O) The(O) African(O) is(O) indeed(O) my(O) brother(O) but(O) my(O) junior(O) brother(O) .(O)"}}
{"id": "68", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "poem", "person", "magazine", "writer", "book", "country", "organization", "literary genre", "event", "location"], "instance": {"id": "68", "words": ["The", "scholar", "and", "translator", "David", "Hawkes", "divides", "the", "verses", "of", "what", "seem", "to", "be", "of", "the", "earlier", "(", "pre-Han", "era", ")", ",", "into", "two", "types", ",", "each", "type", "being", "characterized", "by", "one", "of", "two", "characteristic", "metrical", "forms", "(", "with", "the", "exception", "of", "the", "mixed", "poetry", "and", "prose", "narratives", "of", "the", "Bu", "Ju", "and", "of", "The", "Fisherman", ")", "."], "labels": ["O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "B-literary genre", "O", "O", "O", "B-poem", "I-poem", "O", "O", "B-poem", "I-poem", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, poem, person, magazine, writer, book, country, organization, literary genre, event, location and O.\nSentence: The scholar and translator David Hawkes divides the verses of what seem to be of the earlier ( pre-Han era ) , into two types , each type being characterized by one of two characteristic metrical forms ( with the exception of the mixed poetry and prose narratives of the Bu Ju and of The Fisherman ) .", "prompt_labels": "The(O) scholar(O) and(O) translator(O) David(B-writer) Hawkes(I-writer) divides(O) the(O) verses(B-literary genre) of(O) what(O) seem(O) to(O) be(O) of(O) the(O) earlier(O) ((O) pre-Han(O) era(O) )(O) ,(O) into(O) two(O) types(O) ,(O) each(O) type(O) being(O) characterized(O) by(O) one(O) of(O) two(O) characteristic(O) metrical(O) forms(O) ((O) with(O) the(O) exception(O) of(O) the(O) mixed(O) poetry(B-literary genre) and(O) prose(B-literary genre) narratives(O) of(O) the(O) Bu(B-poem) Ju(I-poem) and(O) of(O) The(B-poem) Fisherman(I-poem) )(O) .(O)"}}
{"id": "226", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "organization", "event", "writer", "poem", "country", "book", "magazine", "person", "literary genre", "award"], "instance": {"id": "226", "words": ["Dexter", "received", "several", "Crime", "Writers", "'", "Association", "awards", ":", "two", "Silver", "Daggers", "for", "Service", "of", "All", "the", "Dead", "in", "1979", "and", "The", "Dead", "of", "Jericho", "in", "1981", ";", "two", "Gold", "Dagger", "s", "for", "The", "Wench", "is", "Dead", "in", "1989", "and", "The", "Way", "Through", "the", "Woods", "in", "1992", ";", "and", "a", "Cartier", "Diamond", "Dagger", "for", "lifetime", "achievement", "in", "1997", "."], "labels": ["B-writer", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, event, writer, poem, country, book, magazine, person, literary genre, award and O.\nSentence: Dexter received several Crime Writers ' Association awards : two Silver Daggers for Service of All the Dead in 1979 and The Dead of Jericho in 1981 ; two Gold Dagger s for The Wench is Dead in 1989 and The Way Through the Woods in 1992 ; and a Cartier Diamond Dagger for lifetime achievement in 1997 .", "prompt_labels": "Dexter(B-writer) received(O) several(O) Crime(B-award) Writers(I-award) '(I-award) Association(I-award) awards(I-award) :(O) two(O) Silver(B-award) Daggers(I-award) for(O) Service(B-book) of(I-book) All(I-book) the(I-book) Dead(I-book) in(O) 1979(O) and(O) The(B-book) Dead(I-book) of(I-book) Jericho(I-book) in(O) 1981(O) ;(O) two(O) Gold(B-award) Dagger(I-award) s(O) for(O) The(B-book) Wench(I-book) is(I-book) Dead(I-book) in(O) 1989(O) and(O) The(B-book) Way(I-book) Through(I-book) the(I-book) Woods(I-book) in(O) 1992(O) ;(O) and(O) a(O) Cartier(B-award) Diamond(I-award) Dagger(I-award) for(I-award) lifetime(I-award) achievement(I-award) in(O) 1997(O) .(O)"}}
{"id": "145", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "book", "writer", "person", "award", "event", "literary genre", "organization", "country", "magazine", "poem"], "instance": {"id": "145", "words": ["Spengler", "spent", "his", "final", "years", "in", "Munich", ",", "listening", "to", "Ludwig", "van", "Beethoven", ",", "reading", "Molière", "and", "Shakespeare", ",", "buying", "several", "thousand", "books", ",", "and", "collecting", "ancient", "Turkey", ",", "Persia", "n", "and", "India", "n", "weapons", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "B-writer", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "B-country", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, book, writer, person, award, event, literary genre, organization, country, magazine, poem and O.\nSentence: Spengler spent his final years in Munich , listening to Ludwig van Beethoven , reading Molière and Shakespeare , buying several thousand books , and collecting ancient Turkey , Persia n and India n weapons .", "prompt_labels": "Spengler(B-person) spent(O) his(O) final(O) years(O) in(O) Munich(B-location) ,(O) listening(O) to(O) Ludwig(B-person) van(I-person) Beethoven(I-person) ,(O) reading(O) Molière(B-writer) and(O) Shakespeare(B-writer) ,(O) buying(O) several(O) thousand(O) books(O) ,(O) and(O) collecting(O) ancient(O) Turkey(B-country) ,(O) Persia(B-country) n(O) and(O) India(B-country) n(O) weapons(O) .(O)"}}
{"id": "310", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "literary genre", "location", "writer", "book", "event", "award", "organization", "person", "poem", "magazine"], "instance": {"id": "310", "words": ["Lord", "of", "the", "Flies", "is", "a", "1954", "novel", "by", "Nobel", "Prize", "-winning", "British", "author", "William", "Golding", "."], "labels": ["B-book", "I-book", "I-book", "I-book", "O", "O", "O", "B-literary genre", "O", "B-award", "I-award", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, literary genre, location, writer, book, event, award, organization, person, poem, magazine and O.\nSentence: Lord of the Flies is a 1954 novel by Nobel Prize -winning British author William Golding .", "prompt_labels": "Lord(B-book) of(I-book) the(I-book) Flies(I-book) is(O) a(O) 1954(O) novel(B-literary genre) by(O) Nobel(B-award) Prize(I-award) -winning(O) British(O) author(O) William(B-writer) Golding(I-writer) .(O)"}}
{"id": "390", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "literary genre", "book", "magazine", "event", "writer", "country", "organization", "poem", "location", "award"], "instance": {"id": "390", "words": ["However", ",", "the", "resulting", "film", ",", "Conan", "the", "Barbarian", "(", "1982", ")", ",", "was", "a", "combination", "of", "director", "John", "Milius", "'", "ideas", "and", "plots", "from", "Conan", "stories", "(", "written", "also", "by", "Howard", "'s", "successors", ",", "notably", "Lin", "Carter", "and", "L.", "Sprague", "de", "Camp", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "I-writer", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, literary genre, book, magazine, event, writer, country, organization, poem, location, award and O.\nSentence: However , the resulting film , Conan the Barbarian ( 1982 ) , was a combination of director John Milius ' ideas and plots from Conan stories ( written also by Howard 's successors , notably Lin Carter and L. Sprague de Camp ) .", "prompt_labels": "However(O) ,(O) the(O) resulting(O) film(O) ,(O) Conan(O) the(O) Barbarian(O) ((O) 1982(O) )(O) ,(O) was(O) a(O) combination(O) of(O) director(O) John(B-person) Milius(I-person) '(O) ideas(O) and(O) plots(O) from(O) Conan(B-person) stories(O) ((O) written(O) also(O) by(O) Howard(B-writer) 's(O) successors(O) ,(O) notably(O) Lin(B-writer) Carter(I-writer) and(O) L.(B-writer) Sprague(I-writer) de(I-writer) Camp(I-writer) )(O) .(O)"}}
{"id": "353", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "magazine", "writer", "country", "poem", "award", "literary genre", "person", "event", "book", "location"], "instance": {"id": "353", "words": ["Formally", ",", "the", "use", "of", "sound", "to", "create", "atmosphere", ",", "and", "of", "symbols", "(", "images", "that", "take", "on", "an", "expanded", "function", "within", "the", "poem", ")", ",", "betray", "a", "move", "towards", "considering", "the", "poem", "as", "a", "self-referential", "object", ",", "an", "idea", "further", "developed", "by", "the", "Symbolists", "Paul", "Verlaine", "and", "Stéphane", "Mallarmé", ",", "who", "acknowledge", "Baudelaire", "as", "a", "pioneer", "in", "this", "regard", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, magazine, writer, country, poem, award, literary genre, person, event, book, location and O.\nSentence: Formally , the use of sound to create atmosphere , and of symbols ( images that take on an expanded function within the poem ) , betray a move towards considering the poem as a self-referential object , an idea further developed by the Symbolists Paul Verlaine and Stéphane Mallarmé , who acknowledge Baudelaire as a pioneer in this regard .", "prompt_labels": "Formally(O) ,(O) the(O) use(O) of(O) sound(O) to(O) create(O) atmosphere(O) ,(O) and(O) of(O) symbols(O) ((O) images(O) that(O) take(O) on(O) an(O) expanded(O) function(O) within(O) the(O) poem(B-literary genre) )(O) ,(O) betray(O) a(O) move(O) towards(O) considering(O) the(O) poem(B-literary genre) as(O) a(O) self-referential(O) object(O) ,(O) an(O) idea(O) further(O) developed(O) by(O) the(O) Symbolists(O) Paul(B-writer) Verlaine(I-writer) and(O) Stéphane(B-writer) Mallarmé(I-writer) ,(O) who(O) acknowledge(O) Baudelaire(B-writer) as(O) a(O) pioneer(O) in(O) this(O) regard(O) .(O)"}}
{"id": "260", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "literary genre", "poem", "book", "event", "writer", "location", "country", "person", "magazine", "award"], "instance": {"id": "260", "words": ["A", "kind", "of", "poetic", "manifesto", "in", "the", "vein", "of", "Horace", "'", "s", "Ars", "Poetica", ",", "the", "essay", "was", "met", "with", "enthusiastic", "attention", "and", "won", "Pope", "a", "wider", "circle", "of", "prominent", "friends", ",", "most", "notably", "Joseph", "Addison", "and", "Richard", "Steele", ",", "who", "had", "recently", "started", "collaborating", "on", "the", "influential", "The", "Spectator", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "B-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, literary genre, poem, book, event, writer, location, country, person, magazine, award and O.\nSentence: A kind of poetic manifesto in the vein of Horace ' s Ars Poetica , the essay was met with enthusiastic attention and won Pope a wider circle of prominent friends , most notably Joseph Addison and Richard Steele , who had recently started collaborating on the influential The Spectator .", "prompt_labels": "A(O) kind(O) of(O) poetic(O) manifesto(O) in(O) the(O) vein(O) of(O) Horace(B-writer) '(O) s(O) Ars(B-poem) Poetica(I-poem) ,(O) the(O) essay(O) was(O) met(O) with(O) enthusiastic(O) attention(O) and(O) won(O) Pope(B-person) a(O) wider(O) circle(O) of(O) prominent(O) friends(O) ,(O) most(O) notably(O) Joseph(B-writer) Addison(I-writer) and(O) Richard(B-writer) Steele(I-writer) ,(O) who(O) had(O) recently(O) started(O) collaborating(O) on(O) the(O) influential(O) The(B-magazine) Spectator(I-magazine) .(O)"}}
{"id": "135", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "location", "award", "book", "writer", "magazine", "event", "literary genre", "organization", "poem", "country"], "instance": {"id": "135", "words": ["In", "1959", ",", "Capp", "recorded", "and", "released", "an", "album", "for", "Folkways", "Records", "(", "now", "owned", "by", "the", "Smithsonian", ")", "on", "which", "he", "identified", "and", "described", "The", "Mechanics", "of", "the", "Comic", "Strip", "."], "labels": ["O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, award, book, writer, magazine, event, literary genre, organization, poem, country and O.\nSentence: In 1959 , Capp recorded and released an album for Folkways Records ( now owned by the Smithsonian ) on which he identified and described The Mechanics of the Comic Strip .", "prompt_labels": "In(O) 1959(O) ,(O) Capp(B-writer) recorded(O) and(O) released(O) an(O) album(O) for(O) Folkways(B-organization) Records(I-organization) ((O) now(O) owned(O) by(O) the(O) Smithsonian(B-organization) )(O) on(O) which(O) he(O) identified(O) and(O) described(O) The(O) Mechanics(O) of(O) the(O) Comic(O) Strip(O) .(O)"}}
{"id": "363", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "person", "book", "organization", "location", "magazine", "poem", "award", "event", "country", "literary genre"], "instance": {"id": "363", "words": ["In", "Rebuilding", "Russia", ",", "an", "essay", "first", "published", "in", "1990", "in", "Komsomolskaya", "Pravda", "Solzhenitsyn", "urged", "the", "Soviet", "Union", "to", "grant", "independence", "to", "all", "the", "non-Slav", "republics", ",", "which", "he", "claimed", "were", "sapping", "the", "Russian", "nation", "and", "he", "called", "for", "the", "creation", "of", "a", "new", "Slavic", "state", "bringing", "together", "Russia", ",", "Ukraine", ",", "Belarus", ",", "and", "parts", "of", "Kazakhstan", "that", "he", "considered", "to", "be", "Russified", "."], "labels": ["O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, person, book, organization, location, magazine, poem, award, event, country, literary genre and O.\nSentence: In Rebuilding Russia , an essay first published in 1990 in Komsomolskaya Pravda Solzhenitsyn urged the Soviet Union to grant independence to all the non-Slav republics , which he claimed were sapping the Russian nation and he called for the creation of a new Slavic state bringing together Russia , Ukraine , Belarus , and parts of Kazakhstan that he considered to be Russified .", "prompt_labels": "In(O) Rebuilding(B-book) Russia(I-book) ,(O) an(O) essay(O) first(O) published(O) in(O) 1990(O) in(O) Komsomolskaya(B-organization) Pravda(I-organization) Solzhenitsyn(B-writer) urged(O) the(O) Soviet(O) Union(O) to(O) grant(O) independence(O) to(O) all(O) the(O) non-Slav(O) republics(O) ,(O) which(O) he(O) claimed(O) were(O) sapping(O) the(O) Russian(O) nation(O) and(O) he(O) called(O) for(O) the(O) creation(O) of(O) a(O) new(O) Slavic(O) state(O) bringing(O) together(O) Russia(B-country) ,(O) Ukraine(B-country) ,(O) Belarus(B-country) ,(O) and(O) parts(O) of(O) Kazakhstan(B-country) that(O) he(O) considered(O) to(O) be(O) Russified(O) .(O)"}}
{"id": "198", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "location", "person", "writer", "literary genre", "award", "event", "poem", "book", "country", "organization"], "instance": {"id": "198", "words": ["When", "Martin", "Gardner", "retired", "from", "writing", "his", "Mathematical", "Games", "column", "for", "Scientific", "American", "magazine", ",", "Hofstadter", "succeeded", "him", "in", "1981-1983", "with", "a", "column", "titled", "Metamagical", "Themas", "(", "an", "anagram", "of", "Mathematical", "Games", ")", "."], "labels": ["O", "B-writer", "I-writer", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "B-magazine", "I-magazine", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, person, writer, literary genre, award, event, poem, book, country, organization and O.\nSentence: When Martin Gardner retired from writing his Mathematical Games column for Scientific American magazine , Hofstadter succeeded him in 1981-1983 with a column titled Metamagical Themas ( an anagram of Mathematical Games ) .", "prompt_labels": "When(O) Martin(B-writer) Gardner(I-writer) retired(O) from(O) writing(O) his(O) Mathematical(B-book) Games(I-book) column(O) for(O) Scientific(B-magazine) American(I-magazine) magazine(O) ,(O) Hofstadter(B-writer) succeeded(O) him(O) in(O) 1981-1983(O) with(O) a(O) column(O) titled(O) Metamagical(B-book) Themas(I-book) ((O) an(O) anagram(O) of(O) Mathematical(B-book) Games(I-book) )(O) .(O)"}}
{"id": "311", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "book", "person", "location", "literary genre", "event", "poem", "writer", "award", "magazine", "country"], "instance": {"id": "311", "words": ["He", "was", "a", "court", "poet", "of", "Udaya", "Varma", "(", "1446-1475", ")", "and", "the", "author", "of", "Krishnagatha", ",", "a", "poem", "which", "is", "considered", "a", "landmark", "in", "the", "development", "of", "Malayalam", "literature", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "B-poem", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, book, person, location, literary genre, event, poem, writer, award, magazine, country and O.\nSentence: He was a court poet of Udaya Varma ( 1446-1475 ) and the author of Krishnagatha , a poem which is considered a landmark in the development of Malayalam literature .", "prompt_labels": "He(O) was(O) a(O) court(O) poet(O) of(O) Udaya(B-person) Varma(I-person) ((O) 1446-1475(O) )(O) and(O) the(O) author(O) of(O) Krishnagatha(B-poem) ,(O) a(O) poem(B-literary genre) which(O) is(O) considered(O) a(O) landmark(O) in(O) the(O) development(O) of(O) Malayalam(B-literary genre) literature(I-literary genre) .(O)"}}
{"id": "227", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "book", "person", "literary genre", "poem", "country", "event", "organization", "award", "location", "writer"], "instance": {"id": "227", "words": ["According", "to", "a", "June", "2019", "article", "in", "The", "New", "York", "Times", "Magazine", ",", "virtually", "all", "of", "Holly", "'s", "masters", "were", "lost", "in", "the", "2008", "Universal", "fire", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, book, person, literary genre, poem, country, event, organization, award, location, writer and O.\nSentence: According to a June 2019 article in The New York Times Magazine , virtually all of Holly 's masters were lost in the 2008 Universal fire .", "prompt_labels": "According(O) to(O) a(O) June(O) 2019(O) article(O) in(O) The(B-magazine) New(I-magazine) York(I-magazine) Times(I-magazine) Magazine(I-magazine) ,(O) virtually(O) all(O) of(O) Holly(B-person) 's(O) masters(O) were(O) lost(O) in(O) the(O) 2008(B-event) Universal(I-event) fire(I-event) .(O)"}}
{"id": "41", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "award", "book", "event", "location", "writer", "organization", "literary genre", "country", "person", "poem"], "instance": {"id": "41", "words": ["He", "studied", "Platonism", "in", "Athens", ",", "travelled", "to", "Roman", "Italy", ",", "Asia", "Minor", ",", "and", "Egypt", ",", "and", "was", "an", "initiate", "in", "several", "cults", "or", "mysteries", "."], "labels": ["O", "O", "O", "O", "B-location", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, award, book, event, location, writer, organization, literary genre, country, person, poem and O.\nSentence: He studied Platonism in Athens , travelled to Roman Italy , Asia Minor , and Egypt , and was an initiate in several cults or mysteries .", "prompt_labels": "He(O) studied(O) Platonism(O) in(O) Athens(B-location) ,(O) travelled(O) to(O) Roman(B-location) Italy(I-location) ,(O) Asia(B-location) Minor(I-location) ,(O) and(O) Egypt(B-country) ,(O) and(O) was(O) an(O) initiate(O) in(O) several(O) cults(O) or(O) mysteries(O) .(O)"}}
{"id": "109", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "literary genre", "book", "award", "organization", "event", "magazine", "poem", "location", "writer", "country"], "instance": {"id": "109", "words": ["His", "third", "book", ",", "and", "second", "novel", ",", "A", "Fine", "Balance", "(", "1995", ")", ",", "won", "the", "second", "annual", "Giller", "Prize", "in", "1995", ",", "and", "the", "Los", "Angeles", "Times", "Book", "Prize", "for", "Fiction", "in", "1996", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-literary genre", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, literary genre, book, award, organization, event, magazine, poem, location, writer, country and O.\nSentence: His third book , and second novel , A Fine Balance ( 1995 ) , won the second annual Giller Prize in 1995 , and the Los Angeles Times Book Prize for Fiction in 1996 .", "prompt_labels": "His(O) third(O) book(O) ,(O) and(O) second(O) novel(B-literary genre) ,(O) A(B-book) Fine(I-book) Balance(I-book) ((O) 1995(O) )(O) ,(O) won(O) the(O) second(O) annual(O) Giller(B-award) Prize(I-award) in(O) 1995(O) ,(O) and(O) the(O) Los(B-award) Angeles(I-award) Times(I-award) Book(I-award) Prize(I-award) for(I-award) Fiction(I-award) in(O) 1996(O) .(O)"}}
{"id": "210", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "organization", "country", "literary genre", "writer", "award", "event", "book", "location", "poem", "person"], "instance": {"id": "210", "words": ["He", "won", "three", "Tony", "Award", "s", "and", "three", "Academy", "Awards", ",", "among", "other", "honors", "."], "labels": ["O", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, organization, country, literary genre, writer, award, event, book, location, poem, person and O.\nSentence: He won three Tony Award s and three Academy Awards , among other honors .", "prompt_labels": "He(O) won(O) three(O) Tony(B-award) Award(I-award) s(O) and(O) three(O) Academy(B-award) Awards(I-award) ,(O) among(O) other(O) honors(O) .(O)"}}
{"id": "375", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "poem", "location", "book", "event", "literary genre", "magazine", "organization", "person", "country", "writer"], "instance": {"id": "375", "words": ["He", "even", "writes", "a", "book", "about", "mystery", "fiction", "in", "which", "he", "deals", "sternly", "with", "Edgar", "Allan", "Poe", "and", "Wilkie", "Collins", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, poem, location, book, event, literary genre, magazine, organization, person, country, writer and O.\nSentence: He even writes a book about mystery fiction in which he deals sternly with Edgar Allan Poe and Wilkie Collins .", "prompt_labels": "He(O) even(O) writes(O) a(O) book(O) about(O) mystery(B-literary genre) fiction(I-literary genre) in(O) which(O) he(O) deals(O) sternly(O) with(O) Edgar(B-writer) Allan(I-writer) Poe(I-writer) and(O) Wilkie(B-writer) Collins(I-writer) .(O)"}}
{"id": "107", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "location", "award", "event", "book", "literary genre", "writer", "magazine", "poem", "country", "person"], "instance": {"id": "107", "words": ["The", "work", "was", "such", "a", "popular", "success", "that", "the", "poet", "wrote", "a", "sequel", ",", "Remedia", "Amoris", "(", "Remedies", "for", "Love", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "O", "B-poem", "I-poem", "I-poem", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, award, event, book, literary genre, writer, magazine, poem, country, person and O.\nSentence: The work was such a popular success that the poet wrote a sequel , Remedia Amoris ( Remedies for Love ) .", "prompt_labels": "The(O) work(O) was(O) such(O) a(O) popular(O) success(O) that(O) the(O) poet(O) wrote(O) a(O) sequel(O) ,(O) Remedia(B-poem) Amoris(I-poem) ((O) Remedies(B-poem) for(I-poem) Love(I-poem) )(O) .(O)"}}
{"id": "200", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "book", "organization", "writer", "person", "event", "literary genre", "location", "award", "poem", "magazine"], "instance": {"id": "200", "words": ["In", "1662", "he", "published", "The", "Day", "of", "Doom", "or", "a", "Poetical", "Description", "of", "the", "Great", "and", "Last", "Judgment", ",", "a", "doggerel", "epitome", "of", "Calvinistic", "theology", ",", "according", "to", "the", "anthology", ",", "Colonial", "Prose", "and", "Poetry", "(", "1903", ")", ",", "that", "attained", "immediately", "a", "phenomenal", "popularity", "."], "labels": ["O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, book, organization, writer, person, event, literary genre, location, award, poem, magazine and O.\nSentence: In 1662 he published The Day of Doom or a Poetical Description of the Great and Last Judgment , a doggerel epitome of Calvinistic theology , according to the anthology , Colonial Prose and Poetry ( 1903 ) , that attained immediately a phenomenal popularity .", "prompt_labels": "In(O) 1662(O) he(O) published(O) The(B-poem) Day(I-poem) of(I-poem) Doom(I-poem) or(I-poem) a(I-poem) Poetical(I-poem) Description(I-poem) of(I-poem) the(I-poem) Great(I-poem) and(I-poem) Last(I-poem) Judgment(I-poem) ,(O) a(O) doggerel(O) epitome(O) of(O) Calvinistic(O) theology(O) ,(O) according(O) to(O) the(O) anthology(O) ,(O) Colonial(B-book) Prose(I-book) and(I-book) Poetry(I-book) ((O) 1903(O) )(O) ,(O) that(O) attained(O) immediately(O) a(O) phenomenal(O) popularity(O) .(O)"}}
{"id": "133", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "event", "magazine", "literary genre", "location", "award", "writer", "book", "poem", "person", "country"], "instance": {"id": "133", "words": ["Again", ",", "in", "the", "English", "Renaissance", "fantasy", "Armor", "of", "Light", "by", "Melissa", "Scott", "and", "Lisa", "A.", "Barnett", ",", "the", "magic", "used", "in", "the", "book", ",", "by", "Dr.", "John", "Dee", "and", "others", ",", "actually", "was", "practiced", "in", "the", "Renaissance", ";", "positing", "a", "secret", "history", "of", "effective", "magic", "makes", "this", "an", "alternate", "history", "with", "a", "POD", ",", "Sir", "Philip", "Sidney", "'", "s", "surviving", "the", "Battle", "of", "Zutphen", "in", "1586", ",", "and", "shortly", "thereafter", "saving", "the", "life", "of", "Christopher", "Marlowe", "."], "labels": ["O", "O", "O", "O", "B-event", "I-event", "I-event", "B-book", "I-book", "I-book", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, magazine, literary genre, location, award, writer, book, poem, person, country and O.\nSentence: Again , in the English Renaissance fantasy Armor of Light by Melissa Scott and Lisa A. Barnett , the magic used in the book , by Dr. John Dee and others , actually was practiced in the Renaissance ; positing a secret history of effective magic makes this an alternate history with a POD , Sir Philip Sidney ' s surviving the Battle of Zutphen in 1586 , and shortly thereafter saving the life of Christopher Marlowe .", "prompt_labels": "Again(O) ,(O) in(O) the(O) English(B-event) Renaissance(I-event) fantasy(I-event) Armor(B-book) of(I-book) Light(I-book) by(O) Melissa(B-writer) Scott(I-writer) and(O) Lisa(B-writer) A.(I-writer) Barnett(I-writer) ,(O) the(O) magic(O) used(O) in(O) the(O) book(O) ,(O) by(O) Dr.(B-person) John(I-person) Dee(I-person) and(O) others(O) ,(O) actually(O) was(O) practiced(O) in(O) the(O) Renaissance(B-event) ;(O) positing(O) a(O) secret(O) history(O) of(O) effective(O) magic(O) makes(O) this(O) an(O) alternate(B-literary genre) history(I-literary genre) with(O) a(O) POD(O) ,(O) Sir(B-person) Philip(I-person) Sidney(I-person) '(O) s(O) surviving(O) the(O) Battle(B-event) of(I-event) Zutphen(I-event) in(O) 1586(O) ,(O) and(O) shortly(O) thereafter(O) saving(O) the(O) life(O) of(O) Christopher(B-person) Marlowe(I-person) .(O)"}}
{"id": "118", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "poem", "location", "person", "book", "award", "literary genre", "organization", "magazine", "writer", "event"], "instance": {"id": "118", "words": ["Liszt", "included", "Weißheimer", "'s", "symphony", "on", "Friedrich", "Schiller", "'", "s", "Ritter", "Toggenburg", "on", "the", "program", "for", "the", "court", "concerts", "that", "he", "conducted", "on", "13", "March", "1860", "."], "labels": ["B-writer", "O", "B-writer", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, poem, location, person, book, award, literary genre, organization, magazine, writer, event and O.\nSentence: Liszt included Weißheimer 's symphony on Friedrich Schiller ' s Ritter Toggenburg on the program for the court concerts that he conducted on 13 March 1860 .", "prompt_labels": "Liszt(B-writer) included(O) Weißheimer(B-writer) 's(O) symphony(O) on(O) Friedrich(B-writer) Schiller(I-writer) '(O) s(O) Ritter(B-poem) Toggenburg(I-poem) on(O) the(O) program(O) for(O) the(O) court(O) concerts(O) that(O) he(O) conducted(O) on(O) 13(O) March(O) 1860(O) .(O)"}}
{"id": "218", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "organization", "country", "book", "award", "person", "poem", "magazine", "event", "location", "writer"], "instance": {"id": "218", "words": ["Auden", "'s", "next", "large-scale", "work", "was", "The", "Orators", ":", "An", "English", "Study", "(", "1932", ";", "revised", "editions", ",", "1934", ",", "1966", ")", ",", "in", "verse", "and", "prose", ",", "largely", "about", "hero-worship", "in", "personal", "and", "political", "life", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, organization, country, book, award, person, poem, magazine, event, location, writer and O.\nSentence: Auden 's next large-scale work was The Orators : An English Study ( 1932 ; revised editions , 1934 , 1966 ) , in verse and prose , largely about hero-worship in personal and political life .", "prompt_labels": "Auden(B-writer) 's(O) next(O) large-scale(O) work(O) was(O) The(B-poem) Orators(I-poem) :(I-poem) An(I-poem) English(I-poem) Study(I-poem) ((O) 1932(O) ;(O) revised(O) editions(O) ,(O) 1934(O) ,(O) 1966(O) )(O) ,(O) in(O) verse(B-literary genre) and(O) prose(B-literary genre) ,(O) largely(O) about(O) hero-worship(O) in(O) personal(O) and(O) political(O) life(O) .(O)"}}
{"id": "382", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "event", "writer", "location", "literary genre", "poem", "organization", "person", "book", "award", "country"], "instance": {"id": "382", "words": ["In", "a", "2006", "interview", "with", "Tatler", "magazine", ",", "Rowling", "noted", "that", ",", "like", "Graham", "Greene", ",", "my", "faith", "is", "sometimes", "about", "if", "my", "faith", "will", "return", "."], "labels": ["O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O", "B-writer", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, event, writer, location, literary genre, poem, organization, person, book, award, country and O.\nSentence: In a 2006 interview with Tatler magazine , Rowling noted that , like Graham Greene , my faith is sometimes about if my faith will return .", "prompt_labels": "In(O) a(O) 2006(O) interview(O) with(O) Tatler(B-magazine) magazine(I-magazine) ,(O) Rowling(B-writer) noted(O) that(O) ,(O) like(O) Graham(B-writer) Greene(I-writer) ,(O) my(O) faith(O) is(O) sometimes(O) about(O) if(O) my(O) faith(O) will(O) return(O) .(O)"}}
{"id": "151", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "award", "literary genre", "location", "organization", "book", "event", "country", "poem", "person", "magazine"], "instance": {"id": "151", "words": ["In", "his", "book", "Trinity", "of", "Passion", ",", "author", "Alan", "M.", "Wald", "conjectures", "that", "Miller", "was", "a", "member", "of", "a", "writer", "'s", "unit", "of", "the", "Communist", "Party", "around", "1946", ",", "using", "the", "pseudonym", "Matt", "Wayne", ",", "and", "editing", "a", "drama", "column", "in", "the", "magazine", "The", "New", "Masses", "."], "labels": ["O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, award, literary genre, location, organization, book, event, country, poem, person, magazine and O.\nSentence: In his book Trinity of Passion , author Alan M. Wald conjectures that Miller was a member of a writer 's unit of the Communist Party around 1946 , using the pseudonym Matt Wayne , and editing a drama column in the magazine The New Masses .", "prompt_labels": "In(O) his(O) book(O) Trinity(B-book) of(I-book) Passion(I-book) ,(O) author(O) Alan(B-writer) M.(I-writer) Wald(I-writer) conjectures(O) that(O) Miller(B-writer) was(O) a(O) member(O) of(O) a(O) writer(O) 's(O) unit(O) of(O) the(O) Communist(B-organization) Party(I-organization) around(O) 1946(O) ,(O) using(O) the(O) pseudonym(O) Matt(B-writer) Wayne(I-writer) ,(O) and(O) editing(O) a(O) drama(O) column(O) in(O) the(O) magazine(O) The(B-magazine) New(I-magazine) Masses(I-magazine) .(O)"}}
{"id": "331", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "country", "writer", "magazine", "organization", "award", "location", "book", "event", "literary genre", "poem"], "instance": {"id": "331", "words": ["Georg", "Trakl", "(", "3", "February", "1887", "-", "3", "November", "1914", ")", "was", "an", "Austria-Hungary", "poet", "and", "brother", "of", "the", "pianist", "Grete", "Trakl", "."], "labels": ["B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, writer, magazine, organization, award, location, book, event, literary genre, poem and O.\nSentence: Georg Trakl ( 3 February 1887 - 3 November 1914 ) was an Austria-Hungary poet and brother of the pianist Grete Trakl .", "prompt_labels": "Georg(B-writer) Trakl(I-writer) ((O) 3(O) February(O) 1887(O) -(O) 3(O) November(O) 1914(O) )(O) was(O) an(O) Austria-Hungary(B-country) poet(O) and(O) brother(O) of(O) the(O) pianist(O) Grete(B-person) Trakl(I-person) .(O)"}}
{"id": "256", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "magazine", "literary genre", "person", "book", "location", "writer", "award", "country", "organization", "poem"], "instance": {"id": "256", "words": ["In", "1946", ",", "she", "was", "awarded", "a", "Guggenheim", "Fellowship", "for", "Creative", "Work", "in", "the", "Field", "of", "Motion", "Pictures", ",", "and", "won", "the", "Grand", "Prix", "Internationale", "for", "16mm", "experimental", "film", "at", "the", "Cannes", "Film", "Festival", "for", "Meshes", "of", "the", "Afternoon", "(", "1943", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, magazine, literary genre, person, book, location, writer, award, country, organization, poem and O.\nSentence: In 1946 , she was awarded a Guggenheim Fellowship for Creative Work in the Field of Motion Pictures , and won the Grand Prix Internationale for 16mm experimental film at the Cannes Film Festival for Meshes of the Afternoon ( 1943 ) .", "prompt_labels": "In(O) 1946(O) ,(O) she(O) was(O) awarded(O) a(O) Guggenheim(B-award) Fellowship(I-award) for(I-award) Creative(I-award) Work(I-award) in(I-award) the(I-award) Field(I-award) of(I-award) Motion(I-award) Pictures(I-award) ,(O) and(O) won(O) the(O) Grand(B-award) Prix(I-award) Internationale(O) for(O) 16mm(O) experimental(O) film(O) at(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) for(O) Meshes(O) of(O) the(O) Afternoon(O) ((O) 1943(O) )(O) .(O)"}}
{"id": "39", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "award", "poem", "country", "event", "location", "organization", "writer", "magazine", "literary genre", "book"], "instance": {"id": "39", "words": ["In", "an", "autobiographical", "essay", "published", "in", "the", "1950s", ",", "Pasternak", "described", "the", "execution", "of", "Tabidze", "and", "the", "suicides", "of", "Marina", "Tsvetaeva", "and", "Paolo", "Iashvili", "as", "the", "greatest", "heartbreaks", "of", "his", "life", "."], "labels": ["O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, poem, country, event, location, organization, writer, magazine, literary genre, book and O.\nSentence: In an autobiographical essay published in the 1950s , Pasternak described the execution of Tabidze and the suicides of Marina Tsvetaeva and Paolo Iashvili as the greatest heartbreaks of his life .", "prompt_labels": "In(O) an(O) autobiographical(B-literary genre) essay(I-literary genre) published(O) in(O) the(O) 1950s(O) ,(O) Pasternak(B-writer) described(O) the(O) execution(O) of(O) Tabidze(B-writer) and(O) the(O) suicides(O) of(O) Marina(B-writer) Tsvetaeva(I-writer) and(O) Paolo(B-writer) Iashvili(I-writer) as(O) the(O) greatest(O) heartbreaks(O) of(O) his(O) life(O) .(O)"}}
{"id": "334", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "poem", "book", "organization", "literary genre", "award", "person", "location", "event", "magazine", "writer"], "instance": {"id": "334", "words": ["When", "Farnsworth", "Wright", "started", "a", "new", "pulp", "in", "1930", "called", "Oriental", "Stories", ",", "Howard", "was", "overjoyed", "-", "here", "was", "a", "venue", "where", "he", "could", "run", "riot", "through", "favorite", "themes", "of", "history", "and", "battle", "and", "exotic", "mysticism", "."], "labels": ["O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "B-literary genre", "O", "B-literary genre", "I-literary genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, poem, book, organization, literary genre, award, person, location, event, magazine, writer and O.\nSentence: When Farnsworth Wright started a new pulp in 1930 called Oriental Stories , Howard was overjoyed - here was a venue where he could run riot through favorite themes of history and battle and exotic mysticism .", "prompt_labels": "When(O) Farnsworth(B-writer) Wright(I-writer) started(O) a(O) new(O) pulp(O) in(O) 1930(O) called(O) Oriental(B-magazine) Stories(I-magazine) ,(O) Howard(B-writer) was(O) overjoyed(O) -(O) here(O) was(O) a(O) venue(O) where(O) he(O) could(O) run(O) riot(O) through(O) favorite(O) themes(O) of(O) history(B-literary genre) and(O) battle(B-literary genre) and(O) exotic(B-literary genre) mysticism(I-literary genre) .(O)"}}
{"id": "379", "dataset": "crossner_literature", "split": "dev", "label_list": ["poem", "literary genre", "event", "location", "country", "magazine", "writer", "organization", "person", "award", "book"], "instance": {"id": "379", "words": ["When", "a", "little", "older", ",", "she", "moved", "on", "to", "reading", "the", "surreal", "verse", "of", "Edward", "Lear", "and", "Lewis", "Carroll", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, literary genre, event, location, country, magazine, writer, organization, person, award, book and O.\nSentence: When a little older , she moved on to reading the surreal verse of Edward Lear and Lewis Carroll .", "prompt_labels": "When(O) a(O) little(O) older(O) ,(O) she(O) moved(O) on(O) to(O) reading(O) the(O) surreal(B-literary genre) verse(I-literary genre) of(O) Edward(B-writer) Lear(I-writer) and(O) Lewis(B-writer) Carroll(I-writer) .(O)"}}
{"id": "294", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "poem", "event", "writer", "award", "person", "organization", "country", "book", "literary genre", "magazine"], "instance": {"id": "294", "words": ["Master", "Humphrey", "'s", "Clock", "was", "shut", "down", ",", "though", "Dickens", "was", "still", "keen", "on", "the", "idea", "of", "the", "weekly", "magazine", ",", "a", "form", "he", "liked", ",", "a", "liking", "that", "had", "begun", "with", "his", "childhood", "reading", "of", "the", "eighteenth-century", "magazines", "Tatler", "and", "The", "Spectator", "."], "labels": ["B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "O", "B-magazine", "I-magazine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, poem, event, writer, award, person, organization, country, book, literary genre, magazine and O.\nSentence: Master Humphrey 's Clock was shut down , though Dickens was still keen on the idea of the weekly magazine , a form he liked , a liking that had begun with his childhood reading of the eighteenth-century magazines Tatler and The Spectator .", "prompt_labels": "Master(B-book) Humphrey(I-book) 's(I-book) Clock(I-book) was(O) shut(O) down(O) ,(O) though(O) Dickens(B-writer) was(O) still(O) keen(O) on(O) the(O) idea(O) of(O) the(O) weekly(O) magazine(O) ,(O) a(O) form(O) he(O) liked(O) ,(O) a(O) liking(O) that(O) had(O) begun(O) with(O) his(O) childhood(O) reading(O) of(O) the(O) eighteenth-century(O) magazines(O) Tatler(B-magazine) and(O) The(B-magazine) Spectator(I-magazine) .(O)"}}
{"id": "325", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "country", "poem", "person", "award", "event", "organization", "book", "location", "writer", "literary genre"], "instance": {"id": "325", "words": ["Her", "first", "appearance", "in", "a", "full-length", "novel", "was", "in", "The", "Murder", "at", "the", "Vicarage", "in", "1930", "and", "her", "last", "appearance", "was", "in", "Sleeping", "Murder", "in", "1976", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, country, poem, person, award, event, organization, book, location, writer, literary genre and O.\nSentence: Her first appearance in a full-length novel was in The Murder at the Vicarage in 1930 and her last appearance was in Sleeping Murder in 1976 .", "prompt_labels": "Her(O) first(O) appearance(O) in(O) a(O) full-length(O) novel(B-literary genre) was(O) in(O) The(B-book) Murder(I-book) at(I-book) the(I-book) Vicarage(I-book) in(O) 1930(O) and(O) her(O) last(O) appearance(O) was(O) in(O) Sleeping(B-book) Murder(I-book) in(O) 1976(O) .(O)"}}
{"id": "131", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "location", "literary genre", "person", "award", "organization", "country", "writer", "poem", "book", "event"], "instance": {"id": "131", "words": ["Colin", "Macmillan", "Turnbull", "(", "November", "23", ",", "1924", "-", "July", "28", ",", "1994", ")", "was", "a", "British-American", "anthropologist", "who", "came", "to", "public", "attention", "with", "the", "popular", "books", "The", "Forest", "People", "(", "on", "the", "Mbuti", "Pygmies", "of", "Zaire", ")", "and", "The", "Mountain", "People", "(", "on", "the", "Ik", "people", "of", "Uganda", ")", ",", "and", "one", "of", "the", "first", "anthropologists", "to", "work", "in", "the", "field", "of", "ethnomusicology", "."], "labels": ["B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, literary genre, person, award, organization, country, writer, poem, book, event and O.\nSentence: Colin Macmillan Turnbull ( November 23 , 1924 - July 28 , 1994 ) was a British-American anthropologist who came to public attention with the popular books The Forest People ( on the Mbuti Pygmies of Zaire ) and The Mountain People ( on the Ik people of Uganda ) , and one of the first anthropologists to work in the field of ethnomusicology .", "prompt_labels": "Colin(B-writer) Macmillan(I-writer) Turnbull(I-writer) ((O) November(O) 23(O) ,(O) 1924(O) -(O) July(O) 28(O) ,(O) 1994(O) )(O) was(O) a(O) British-American(O) anthropologist(O) who(O) came(O) to(O) public(O) attention(O) with(O) the(O) popular(O) books(O) The(B-book) Forest(I-book) People(I-book) ((O) on(O) the(O) Mbuti(B-book) Pygmies(I-book) of(I-book) Zaire(I-book) )(O) and(O) The(B-book) Mountain(I-book) People(I-book) ((O) on(O) the(O) Ik(B-book) people(I-book) of(I-book) Uganda(I-book) )(O) ,(O) and(O) one(O) of(O) the(O) first(O) anthropologists(O) to(O) work(O) in(O) the(O) field(O) of(O) ethnomusicology(O) .(O)"}}
{"id": "143", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "writer", "organization", "literary genre", "location", "person", "award", "poem", "country", "book", "event"], "instance": {"id": "143", "words": ["For", "example", ",", "Susanna", "Moodie", "and", "Catharine", "Parr", "Traill", ",", "English", "sisters", "who", "adopted", "the", "country", "as", "their", "own", ",", "moved", "to", "Upper", "Canada", "in", "1832", "."], "labels": ["O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, writer, organization, literary genre, location, person, award, poem, country, book, event and O.\nSentence: For example , Susanna Moodie and Catharine Parr Traill , English sisters who adopted the country as their own , moved to Upper Canada in 1832 .", "prompt_labels": "For(O) example(O) ,(O) Susanna(B-writer) Moodie(I-writer) and(O) Catharine(B-writer) Parr(I-writer) Traill(I-writer) ,(O) English(O) sisters(O) who(O) adopted(O) the(O) country(O) as(O) their(O) own(O) ,(O) moved(O) to(O) Upper(B-country) Canada(I-country) in(O) 1832(O) .(O)"}}
{"id": "296", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "award", "location", "literary genre", "event", "poem", "writer", "person", "country", "organization", "book"], "instance": {"id": "296", "words": ["In", "1947", ",", "Capp", "earned", "a", "Newsweek", "cover", "story", "."], "labels": ["O", "O", "O", "B-writer", "O", "O", "B-magazine", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, award, location, literary genre, event, poem, writer, person, country, organization, book and O.\nSentence: In 1947 , Capp earned a Newsweek cover story .", "prompt_labels": "In(O) 1947(O) ,(O) Capp(B-writer) earned(O) a(O) Newsweek(B-magazine) cover(O) story(O) .(O)"}}
{"id": "277", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "magazine", "organization", "book", "literary genre", "award", "event", "country", "poem", "location", "person"], "instance": {"id": "277", "words": ["In", "Switzerland", ",", "Johann", "David", "Wyss", "published", "The", "Swiss", "Family", "Robinson", "in", "1812", ",", "with", "the", "aim", "of", "teaching", "children", "about", "family", "values", ",", "good", "husbandry", ",", "the", "uses", "of", "the", "natural", "world", "and", "self-reliance", "."], "labels": ["O", "B-country", "O", "B-writer", "I-writer", "I-writer", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, magazine, organization, book, literary genre, award, event, country, poem, location, person and O.\nSentence: In Switzerland , Johann David Wyss published The Swiss Family Robinson in 1812 , with the aim of teaching children about family values , good husbandry , the uses of the natural world and self-reliance .", "prompt_labels": "In(O) Switzerland(B-country) ,(O) Johann(B-writer) David(I-writer) Wyss(I-writer) published(O) The(B-book) Swiss(I-book) Family(I-book) Robinson(I-book) in(O) 1812(O) ,(O) with(O) the(O) aim(O) of(O) teaching(O) children(O) about(O) family(O) values(O) ,(O) good(O) husbandry(O) ,(O) the(O) uses(O) of(O) the(O) natural(O) world(O) and(O) self-reliance(O) .(O)"}}
{"id": "242", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "poem", "writer", "magazine", "country", "book", "person", "award", "location", "organization", "event"], "instance": {"id": "242", "words": ["He", "adopted", "a", "pseudonym", "from", "Edward", "Lear", "'", "s", "poem", "The", "Scroobious", "Pip", "and", "in", "2002", "whilst", "working", "in", "a", "HMV", "store", "he", "made", "the", "decision", "to", "start", "saving", "his", "writing", "."], "labels": ["O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-literary genre", "B-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, poem, writer, magazine, country, book, person, award, location, organization, event and O.\nSentence: He adopted a pseudonym from Edward Lear ' s poem The Scroobious Pip and in 2002 whilst working in a HMV store he made the decision to start saving his writing .", "prompt_labels": "He(O) adopted(O) a(O) pseudonym(O) from(O) Edward(B-writer) Lear(I-writer) '(O) s(O) poem(B-literary genre) The(B-poem) Scroobious(I-poem) Pip(I-poem) and(O) in(O) 2002(O) whilst(O) working(O) in(O) a(O) HMV(B-organization) store(O) he(O) made(O) the(O) decision(O) to(O) start(O) saving(O) his(O) writing(O) .(O)"}}
{"id": "388", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "poem", "literary genre", "location", "organization", "country", "person", "award", "event", "writer", "book"], "instance": {"id": "388", "words": ["The", "Macedonian", "phalanx", "of", "Aelian", "had", "many", "points", "of", "resemblance", "to", "the", "solid", "masses", "of", "pikemen", "and", "the", "squadrons", "of", "cavalry", "of", "the", "Spain", "and", "Netherlands", "systems", ",", "and", "the", "translations", "made", "in", "the", "16th", "century", "formed", "the", "groundwork", "of", "numerous", "books", "on", "drill", "and", "tactics", "."], "labels": ["O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, poem, literary genre, location, organization, country, person, award, event, writer, book and O.\nSentence: The Macedonian phalanx of Aelian had many points of resemblance to the solid masses of pikemen and the squadrons of cavalry of the Spain and Netherlands systems , and the translations made in the 16th century formed the groundwork of numerous books on drill and tactics .", "prompt_labels": "The(O) Macedonian(O) phalanx(O) of(O) Aelian(B-writer) had(O) many(O) points(O) of(O) resemblance(O) to(O) the(O) solid(O) masses(O) of(O) pikemen(O) and(O) the(O) squadrons(O) of(O) cavalry(O) of(O) the(O) Spain(B-country) and(O) Netherlands(B-country) systems(O) ,(O) and(O) the(O) translations(O) made(O) in(O) the(O) 16th(O) century(O) formed(O) the(O) groundwork(O) of(O) numerous(O) books(O) on(O) drill(O) and(O) tactics(O) .(O)"}}
{"id": "63", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "book", "magazine", "event", "organization", "location", "country", "literary genre", "writer", "person", "poem"], "instance": {"id": "63", "words": ["In", "1930", "he", "was", "nominated", "for", "the", "Nobel", "Prize", "in", "Literature", "by", "Swedish", "author", "Anders", "Österling", ",", "but", "was", "passed", "over", "in", "favor", "of", "Sinclair", "Lewis", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, book, magazine, event, organization, location, country, literary genre, writer, person, poem and O.\nSentence: In 1930 he was nominated for the Nobel Prize in Literature by Swedish author Anders Österling , but was passed over in favor of Sinclair Lewis .", "prompt_labels": "In(O) 1930(O) he(O) was(O) nominated(O) for(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) by(O) Swedish(O) author(O) Anders(B-writer) Österling(I-writer) ,(O) but(O) was(O) passed(O) over(O) in(O) favor(O) of(O) Sinclair(B-writer) Lewis(I-writer) .(O)"}}
{"id": "290", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "award", "literary genre", "magazine", "book", "person", "writer", "poem", "country", "organization", "event"], "instance": {"id": "290", "words": ["For", "his", "performance", "in", "the", "latter", ",", "Phoenix", "garnered", "enormous", "praise", "and", "won", "a", "Volpi", "Cup", "for", "Best", "Actor", "at", "the", "Venice", "Film", "Festival", ",", "along", "with", "Best", "Actor", "from", "the", "National", "Society", "of", "Film", "Critics", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "B-award", "I-award", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, literary genre, magazine, book, person, writer, poem, country, organization, event and O.\nSentence: For his performance in the latter , Phoenix garnered enormous praise and won a Volpi Cup for Best Actor at the Venice Film Festival , along with Best Actor from the National Society of Film Critics .", "prompt_labels": "For(O) his(O) performance(O) in(O) the(O) latter(O) ,(O) Phoenix(B-person) garnered(O) enormous(O) praise(O) and(O) won(O) a(O) Volpi(B-award) Cup(I-award) for(I-award) Best(I-award) Actor(I-award) at(O) the(O) Venice(B-event) Film(I-event) Festival(I-event) ,(O) along(O) with(O) Best(B-award) Actor(I-award) from(O) the(O) National(B-organization) Society(I-organization) of(I-organization) Film(I-organization) Critics(I-organization) .(O)"}}
{"id": "42", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "writer", "award", "event", "literary genre", "location", "poem", "person", "magazine", "country", "book"], "instance": {"id": "42", "words": ["She", "was", "appointed", "Burmese", "ambassador", "to", "India", "and", "Nepal", "in", "1960", ",", "and", "Aung", "San", "Suu", "Kyi", "followed", "her", "there", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "I-writer", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, writer, award, event, literary genre, location, poem, person, magazine, country, book and O.\nSentence: She was appointed Burmese ambassador to India and Nepal in 1960 , and Aung San Suu Kyi followed her there .", "prompt_labels": "She(O) was(O) appointed(O) Burmese(O) ambassador(O) to(O) India(B-country) and(O) Nepal(B-country) in(O) 1960(O) ,(O) and(O) Aung(B-writer) San(I-writer) Suu(I-writer) Kyi(I-writer) followed(O) her(O) there(O) .(O)"}}
{"id": "244", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "person", "book", "organization", "location", "literary genre", "event", "writer", "poem", "award", "country"], "instance": {"id": "244", "words": ["Beginning", "with", "The", "Probability", "Broach", "in", "1980", ",", "L.", "Neil", "Smith", "wrote", "several", "novels", "that", "postulated", "the", "disintegration", "of", "the", "US", "Federal", "Government", "after", "Albert", "Gallatin", "joins", "the", "Whiskey", "Rebellion", "in", "1794", "and", "eventually", "leads", "to", "the", "creation", "of", "a", "libertarian", "utopia", "."], "labels": ["O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-person", "I-person", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, person, book, organization, location, literary genre, event, writer, poem, award, country and O.\nSentence: Beginning with The Probability Broach in 1980 , L. Neil Smith wrote several novels that postulated the disintegration of the US Federal Government after Albert Gallatin joins the Whiskey Rebellion in 1794 and eventually leads to the creation of a libertarian utopia .", "prompt_labels": "Beginning(O) with(O) The(B-book) Probability(I-book) Broach(I-book) in(O) 1980(O) ,(O) L.(B-writer) Neil(I-writer) Smith(I-writer) wrote(O) several(O) novels(B-literary genre) that(O) postulated(O) the(O) disintegration(O) of(O) the(O) US(B-organization) Federal(I-organization) Government(I-organization) after(O) Albert(B-person) Gallatin(I-person) joins(O) the(O) Whiskey(B-event) Rebellion(I-event) in(O) 1794(O) and(O) eventually(O) leads(O) to(O) the(O) creation(O) of(O) a(O) libertarian(O) utopia(O) .(O)"}}
{"id": "178", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "event", "location", "country", "organization", "magazine", "literary genre", "person", "writer", "book", "poem"], "instance": {"id": "178", "words": ["Guillaume", "Apollinaire", ",", "André", "Salmon", "and", "Max", "Jacob", "sought", "him", "out", "in", "his", "truncated", "apartment", "."], "labels": ["B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, location, country, organization, magazine, literary genre, person, writer, book, poem and O.\nSentence: Guillaume Apollinaire , André Salmon and Max Jacob sought him out in his truncated apartment .", "prompt_labels": "Guillaume(B-writer) Apollinaire(I-writer) ,(O) André(B-writer) Salmon(I-writer) and(O) Max(B-writer) Jacob(I-writer) sought(O) him(O) out(O) in(O) his(O) truncated(O) apartment(O) .(O)"}}
{"id": "285", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "literary genre", "award", "location", "magazine", "person", "event", "country", "book", "poem", "writer"], "instance": {"id": "285", "words": ["Śrīharṣa", "composed", "the", "poem", "(", "kāvya", ")", "Naishadha", "Charita", "(", "IAST", ":", "Naiṣadhacarita", ")", "in", "1174", ",", "during", "the", "reign", "of", "the", "Vijayachandra", "'s", "son", "Jayachandra", "."], "labels": ["B-writer", "O", "O", "B-literary genre", "O", "B-literary genre", "O", "B-poem", "I-poem", "O", "O", "O", "B-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "B-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, literary genre, award, location, magazine, person, event, country, book, poem, writer and O.\nSentence: Śrīharṣa composed the poem ( kāvya ) Naishadha Charita ( IAST : Naiṣadhacarita ) in 1174 , during the reign of the Vijayachandra 's son Jayachandra .", "prompt_labels": "Śrīharṣa(B-writer) composed(O) the(O) poem(B-literary genre) ((O) kāvya(B-literary genre) )(O) Naishadha(B-poem) Charita(I-poem) ((O) IAST(O) :(O) Naiṣadhacarita(B-poem) )(O) in(O) 1174(O) ,(O) during(O) the(O) reign(O) of(O) the(O) Vijayachandra(B-person) 's(O) son(O) Jayachandra(B-person) .(O)"}}
{"id": "239", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "location", "person", "award", "writer", "book", "literary genre", "magazine", "poem", "country", "event"], "instance": {"id": "239", "words": ["Wilson", "appeared", "in", "another", "Wes", "Anderson", "film", ",", "The", "Darjeeling", "Limited", ",", "which", "screened", "at", "the", "45th", "annual", "New", "York", "Film", "Festival", ",", "the", "Venice", "Film", "Festival", "and", "opened", "September", "30", ",", "2007", ",", "co-starring", "Jason", "Schwartzman", "and", "Adrien", "Brody", "."], "labels": ["B-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, person, award, writer, book, literary genre, magazine, poem, country, event and O.\nSentence: Wilson appeared in another Wes Anderson film , The Darjeeling Limited , which screened at the 45th annual New York Film Festival , the Venice Film Festival and opened September 30 , 2007 , co-starring Jason Schwartzman and Adrien Brody .", "prompt_labels": "Wilson(B-person) appeared(O) in(O) another(O) Wes(B-person) Anderson(I-person) film(O) ,(O) The(O) Darjeeling(O) Limited(O) ,(O) which(O) screened(O) at(O) the(O) 45th(O) annual(O) New(B-event) York(I-event) Film(I-event) Festival(I-event) ,(O) the(O) Venice(B-event) Film(I-event) Festival(I-event) and(O) opened(O) September(O) 30(O) ,(O) 2007(O) ,(O) co-starring(O) Jason(B-person) Schwartzman(I-person) and(O) Adrien(B-person) Brody(I-person) .(O)"}}
{"id": "287", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "location", "organization", "book", "event", "award", "literary genre", "poem", "magazine", "country", "person"], "instance": {"id": "287", "words": ["Qualification", "for", "Euro", "1988", "meant", "winning", "UEFA", "Euro", "1988", "qualifying", "Group", "7", "containing", "Bulgaria", ",", "Luxembourg", "and", "Scotland", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "B-country", "O", "B-country", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, location, organization, book, event, award, literary genre, poem, magazine, country, person and O.\nSentence: Qualification for Euro 1988 meant winning UEFA Euro 1988 qualifying Group 7 containing Bulgaria , Luxembourg and Scotland .", "prompt_labels": "Qualification(O) for(O) Euro(O) 1988(O) meant(O) winning(O) UEFA(B-event) Euro(I-event) 1988(I-event) qualifying(I-event) Group(I-event) 7(I-event) containing(O) Bulgaria(B-country) ,(O) Luxembourg(B-country) and(O) Scotland(B-country) .(O)"}}
{"id": "284", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "literary genre", "poem", "person", "award", "location", "organization", "magazine", "country", "book", "event"], "instance": {"id": "284", "words": ["Some", "of", "his", "biggest", "influences", "were", "Ralph", "Waldo", "Emerson", "'", "s", "essays", ",", "Walt", "Whitman", ",", "H.", "L.", "Mencken", "'", "s", "The", "American", "Mercury", ",", "Samuel", "Johnson", "'", "s", "The", "History", "of", "Rasselas", ",", "Prince", "of", "Abissinia", ",", "Alexandre", "Dumas", ",", "Edgar", "Allan", "Poe", ",", "Walter", "Scott", ",", "and", "Henry", "David", "Thoreau", "'", "s", "Walden", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-literary genre", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, literary genre, poem, person, award, location, organization, magazine, country, book, event and O.\nSentence: Some of his biggest influences were Ralph Waldo Emerson ' s essays , Walt Whitman , H. L. Mencken ' s The American Mercury , Samuel Johnson ' s The History of Rasselas , Prince of Abissinia , Alexandre Dumas , Edgar Allan Poe , Walter Scott , and Henry David Thoreau ' s Walden .", "prompt_labels": "Some(O) of(O) his(O) biggest(O) influences(O) were(O) Ralph(B-writer) Waldo(I-writer) Emerson(I-writer) '(O) s(O) essays(B-literary genre) ,(O) Walt(B-writer) Whitman(I-writer) ,(O) H.(B-writer) L.(I-writer) Mencken(I-writer) '(O) s(O) The(B-magazine) American(I-magazine) Mercury(I-magazine) ,(O) Samuel(B-writer) Johnson(I-writer) '(O) s(O) The(B-book) History(I-book) of(I-book) Rasselas(I-book) ,(I-book) Prince(I-book) of(I-book) Abissinia(I-book) ,(O) Alexandre(B-writer) Dumas(I-writer) ,(O) Edgar(B-writer) Allan(I-writer) Poe(I-writer) ,(O) Walter(B-writer) Scott(I-writer) ,(O) and(O) Henry(B-writer) David(I-writer) Thoreau(I-writer) '(O) s(O) Walden(B-book) .(O)"}}
{"id": "381", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "location", "magazine", "writer", "literary genre", "book", "event", "country", "person", "award", "poem"], "instance": {"id": "381", "words": ["Diaries", "had", "been", "written", "by", "men", "in", "Chinese", "for", "some", "time", ",", "but", "in", "the", "early", "tenth", "century", "Ki", "no", "Tsurayuki", "chose", "to", "write", "his", "Tosa", "Nikki", "from", "the", "standpoint", "of", "a", "woman", ",", "in", "kana", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "B-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, magazine, writer, literary genre, book, event, country, person, award, poem and O.\nSentence: Diaries had been written by men in Chinese for some time , but in the early tenth century Ki no Tsurayuki chose to write his Tosa Nikki from the standpoint of a woman , in kana .", "prompt_labels": "Diaries(O) had(O) been(O) written(O) by(O) men(O) in(O) Chinese(O) for(O) some(O) time(O) ,(O) but(O) in(O) the(O) early(O) tenth(O) century(O) Ki(B-writer) no(I-writer) Tsurayuki(I-writer) chose(O) to(O) write(O) his(O) Tosa(B-poem) Nikki(I-poem) from(O) the(O) standpoint(O) of(O) a(O) woman(O) ,(O) in(O) kana(O) .(O)"}}
{"id": "97", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "event", "writer", "person", "country", "award", "book", "location", "organization", "magazine", "poem"], "instance": {"id": "97", "words": ["She", "wrote", "several", "popular", "comedies", ",", "of", "which", "Das", "Testament", "is", "the", "best", ",", "and", "translated", "The", "Spectator", "(", "9", "volumes", ",", "1739-1743", ")", ",", "Alexander", "Pope", "'", "s", "Rape", "of", "the", "Lock", "(", "1744", ")", "and", "other", "English", "and", "French", "works", "."], "labels": ["O", "O", "O", "O", "B-literary genre", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, event, writer, person, country, award, book, location, organization, magazine, poem and O.\nSentence: She wrote several popular comedies , of which Das Testament is the best , and translated The Spectator ( 9 volumes , 1739-1743 ) , Alexander Pope ' s Rape of the Lock ( 1744 ) and other English and French works .", "prompt_labels": "She(O) wrote(O) several(O) popular(O) comedies(B-literary genre) ,(O) of(O) which(O) Das(B-book) Testament(I-book) is(O) the(O) best(O) ,(O) and(O) translated(O) The(B-magazine) Spectator(I-magazine) ((O) 9(O) volumes(O) ,(O) 1739-1743(O) )(O) ,(O) Alexander(B-writer) Pope(I-writer) '(O) s(O) Rape(B-book) of(I-book) the(I-book) Lock(I-book) ((O) 1744(O) )(O) and(O) other(O) English(O) and(O) French(O) works(O) .(O)"}}
{"id": "235", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "event", "writer", "country", "person", "magazine", "organization", "location", "award", "literary genre", "poem"], "instance": {"id": "235", "words": ["The", "opera", "incorporates", "material", "from", "several", "of", "Blake", "'s", "other", "poems", ";", "the", "Introduction", ",", "A", "Cradle", "Song", "and", "The", "Divine", "Image", "from", "Songs", "of", "Innocence", "(", "1789", ")", ",", "and", "The", "Tyger", "from", "Songs", "of", "Innocence", "and", "of", "Experience", "(", "1794", ")", "."], "labels": ["O", "B-literary genre", "O", "O", "O", "O", "O", "B-writer", "O", "O", "B-literary genre", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "O", "B-poem", "I-poem", "I-poem", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-poem", "I-poem", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, event, writer, country, person, magazine, organization, location, award, literary genre, poem and O.\nSentence: The opera incorporates material from several of Blake 's other poems ; the Introduction , A Cradle Song and The Divine Image from Songs of Innocence ( 1789 ) , and The Tyger from Songs of Innocence and of Experience ( 1794 ) .", "prompt_labels": "The(O) opera(B-literary genre) incorporates(O) material(O) from(O) several(O) of(O) Blake(B-writer) 's(O) other(O) poems(B-literary genre) ;(O) the(O) Introduction(O) ,(O) A(B-poem) Cradle(I-poem) Song(I-poem) and(O) The(B-poem) Divine(I-poem) Image(I-poem) from(O) Songs(B-book) of(I-book) Innocence(I-book) ((O) 1789(O) )(O) ,(O) and(O) The(B-poem) Tyger(I-poem) from(O) Songs(B-book) of(I-book) Innocence(I-book) and(I-book) of(I-book) Experience(I-book) ((O) 1794(O) )(O) .(O)"}}
{"id": "159", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "poem", "organization", "literary genre", "book", "person", "location", "event", "magazine", "writer", "country"], "instance": {"id": "159", "words": ["In", "2000", ",", "Anderson", "starred", "in", "the", "film", "The", "House", "of", "Mirth", "with", "Eric", "Stoltz", "-", "Terence", "Davies", "'", "adaptation", "of", "the", "Edith", "Wharton", "novel", "of", "the", "The", "House", "of", "Mirth", "-", "for", "which", "she", "won", "critical", "acclaim", "and", "awards", "such", "as", "the", "British", "Independent", "Film", "Award", "for", "Best", "Actress", ",", "Village", "Voice", "Film", "Poll", "Best", "Lead", "Performance", ",", "and", "a", "nomination", "for", "the", "National", "Society", "of", "Film", "Critics", "Award", "for", "Best", "Actress", "."], "labels": ["O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "B-writer", "I-writer", "B-literary genre", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, poem, organization, literary genre, book, person, location, event, magazine, writer, country and O.\nSentence: In 2000 , Anderson starred in the film The House of Mirth with Eric Stoltz - Terence Davies ' adaptation of the Edith Wharton novel of the The House of Mirth - for which she won critical acclaim and awards such as the British Independent Film Award for Best Actress , Village Voice Film Poll Best Lead Performance , and a nomination for the National Society of Film Critics Award for Best Actress .", "prompt_labels": "In(O) 2000(O) ,(O) Anderson(B-writer) starred(O) in(O) the(O) film(O) The(O) House(O) of(O) Mirth(O) with(O) Eric(B-person) Stoltz(I-person) -(O) Terence(B-person) Davies(I-person) '(O) adaptation(O) of(O) the(O) Edith(B-writer) Wharton(I-writer) novel(B-literary genre) of(O) the(O) The(B-book) House(I-book) of(I-book) Mirth(I-book) -(O) for(O) which(O) she(O) won(O) critical(O) acclaim(O) and(O) awards(O) such(O) as(O) the(O) British(B-award) Independent(I-award) Film(I-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) ,(O) Village(B-award) Voice(I-award) Film(I-award) Poll(I-award) Best(I-award) Lead(I-award) Performance(I-award) ,(O) and(O) a(O) nomination(O) for(O) the(O) National(B-award) Society(I-award) of(I-award) Film(I-award) Critics(I-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) .(O)"}}
{"id": "356", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "person", "location", "organization", "writer", "literary genre", "award", "country", "poem", "book", "magazine"], "instance": {"id": "356", "words": ["Only", "The", "Graveyard", "Book", "by", "Neil", "Gaiman", "(", "2009", ")", "has", "won", "both", "the", "Carnegie", "Medal", "and", "the", "equivalent", "American", "award", ",", "the", "Newbery", "Medal", "."], "labels": ["O", "B-book", "I-book", "I-book", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, location, organization, writer, literary genre, award, country, poem, book, magazine and O.\nSentence: Only The Graveyard Book by Neil Gaiman ( 2009 ) has won both the Carnegie Medal and the equivalent American award , the Newbery Medal .", "prompt_labels": "Only(O) The(B-book) Graveyard(I-book) Book(I-book) by(O) Neil(B-writer) Gaiman(I-writer) ((O) 2009(O) )(O) has(O) won(O) both(O) the(O) Carnegie(B-award) Medal(I-award) and(O) the(O) equivalent(O) American(O) award(O) ,(O) the(O) Newbery(B-award) Medal(I-award) .(O)"}}
{"id": "0", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "award", "person", "organization", "poem", "magazine", "book", "literary genre", "location", "event", "writer"], "instance": {"id": "0", "words": ["In", "1982", ",", "she", "wrote", "the", "novel", "The", "Color", "Purple", ",", "for", "which", "she", "won", "the", "National", "Book", "Award", "for", "hardcover", "fiction", ",", "and", "the", "Pulitzer", "Prize", "for", "Fiction", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-literary genre", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, person, organization, poem, magazine, book, literary genre, location, event, writer and O.\nSentence: In 1982 , she wrote the novel The Color Purple , for which she won the National Book Award for hardcover fiction , and the Pulitzer Prize for Fiction .", "prompt_labels": "In(O) 1982(O) ,(O) she(O) wrote(O) the(O) novel(B-literary genre) The(B-book) Color(I-book) Purple(I-book) ,(O) for(O) which(O) she(O) won(O) the(O) National(B-award) Book(I-award) Award(I-award) for(I-award) hardcover(I-award) fiction(I-award) ,(O) and(O) the(O) Pulitzer(B-award) Prize(I-award) for(I-award) Fiction(I-award) .(O)"}}
{"id": "232", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "literary genre", "poem", "location", "event", "magazine", "organization", "country", "book", "award", "writer"], "instance": {"id": "232", "words": ["Also", "during", "this", "time", ",", "he", "formed", "friendships", "with", "some", "of", "the", "prominent", "young", "Bengali", "poets", "of", "the", "time", "including", "Shakti", "Chattopadhyay", "and", "Sunil", "Gangopadhyay", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, literary genre, poem, location, event, magazine, organization, country, book, award, writer and O.\nSentence: Also during this time , he formed friendships with some of the prominent young Bengali poets of the time including Shakti Chattopadhyay and Sunil Gangopadhyay .", "prompt_labels": "Also(O) during(O) this(O) time(O) ,(O) he(O) formed(O) friendships(O) with(O) some(O) of(O) the(O) prominent(O) young(O) Bengali(O) poets(O) of(O) the(O) time(O) including(O) Shakti(B-writer) Chattopadhyay(I-writer) and(O) Sunil(B-writer) Gangopadhyay(I-writer) .(O)"}}
{"id": "371", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "person", "writer", "organization", "location", "magazine", "literary genre", "poem", "book", "event", "award"], "instance": {"id": "371", "words": ["He", "won", "the", "Nebula", "Award", "three", "times", "(", "out", "of", "14", "nominations", ")", "and", "the", "Hugo", "Award", "six", "times", "(", "also", "out", "of", "14", "nominations", ")", ",", "including", "two", "Hugos", "for", "novels", ":", "the", "serialized", "novel", "...", "And", "Call", "Me", "Conrad", "(", "1965", ")", ",", "subsequently", "published", "under", "the", "title", "This", "Immortal", "(", "1966", ")", "and", "then", "the", "novel", "Lord", "of", "Light", "(", "1967", ")", "."], "labels": ["O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "O", "B-literary genre", "O", "O", "O", "B-literary genre", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "B-literary genre", "B-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, writer, organization, location, magazine, literary genre, poem, book, event, award and O.\nSentence: He won the Nebula Award three times ( out of 14 nominations ) and the Hugo Award six times ( also out of 14 nominations ) , including two Hugos for novels : the serialized novel ... And Call Me Conrad ( 1965 ) , subsequently published under the title This Immortal ( 1966 ) and then the novel Lord of Light ( 1967 ) .", "prompt_labels": "He(O) won(O) the(O) Nebula(B-award) Award(I-award) three(O) times(O) ((O) out(O) of(O) 14(O) nominations(O) )(O) and(O) the(O) Hugo(B-award) Award(I-award) six(O) times(O) ((O) also(O) out(O) of(O) 14(O) nominations(O) )(O) ,(O) including(O) two(O) Hugos(B-award) for(O) novels(B-literary genre) :(O) the(O) serialized(O) novel(B-literary genre) ...(O) And(B-book) Call(I-book) Me(I-book) Conrad(I-book) ((O) 1965(O) )(O) ,(O) subsequently(O) published(O) under(O) the(O) title(O) This(B-book) Immortal(I-book) ((O) 1966(O) )(O) and(O) then(O) the(O) novel(B-literary genre) Lord(B-book) of(I-book) Light(I-book) ((O) 1967(O) )(O) .(O)"}}
{"id": "40", "dataset": "crossner_literature", "split": "dev", "label_list": ["poem", "writer", "organization", "location", "magazine", "book", "event", "country", "award", "literary genre", "person"], "instance": {"id": "40", "words": ["Milestone", "won", "his", "second", "Academy", "Awards", "for", "All", "Quiet", "on", "the", "Western", "Front", ",", "a", "harrowing", "screen", "adaptation", "of", "the", "antiwar", "novel", "by", "Erich", "Maria", "Remarque", "."], "labels": ["B-writer", "O", "O", "O", "B-award", "I-award", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, writer, organization, location, magazine, book, event, country, award, literary genre, person and O.\nSentence: Milestone won his second Academy Awards for All Quiet on the Western Front , a harrowing screen adaptation of the antiwar novel by Erich Maria Remarque .", "prompt_labels": "Milestone(B-writer) won(O) his(O) second(O) Academy(B-award) Awards(I-award) for(O) All(B-book) Quiet(I-book) on(I-book) the(I-book) Western(I-book) Front(I-book) ,(O) a(O) harrowing(O) screen(O) adaptation(O) of(O) the(O) antiwar(O) novel(B-literary genre) by(O) Erich(B-writer) Maria(I-writer) Remarque(I-writer) .(O)"}}
{"id": "199", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "location", "book", "organization", "literary genre", "writer", "person", "poem", "magazine", "event", "award"], "instance": {"id": "199", "words": ["Norman", "Spinrad", "wrote", "The", "Iron", "Dream", "in", "1972", ",", "which", "is", "intended", "to", "be", "a", "science", "fiction", "novel", "written", "by", "Adolf", "Hitler", "after", "fleeing", "from", "Europe", "to", "North", "America", "in", "the", "1920s", "."], "labels": ["B-writer", "I-writer", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "O", "O", "B-person", "I-person", "O", "O", "O", "B-location", "O", "B-location", "I-location", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, book, organization, literary genre, writer, person, poem, magazine, event, award and O.\nSentence: Norman Spinrad wrote The Iron Dream in 1972 , which is intended to be a science fiction novel written by Adolf Hitler after fleeing from Europe to North America in the 1920s .", "prompt_labels": "Norman(B-writer) Spinrad(I-writer) wrote(O) The(B-book) Iron(I-book) Dream(I-book) in(O) 1972(O) ,(O) which(O) is(O) intended(O) to(O) be(O) a(O) science(B-literary genre) fiction(I-literary genre) novel(I-literary genre) written(O) by(O) Adolf(B-person) Hitler(I-person) after(O) fleeing(O) from(O) Europe(B-location) to(O) North(B-location) America(I-location) in(O) the(O) 1920s(O) .(O)"}}
{"id": "12", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "person", "location", "poem", "award", "event", "writer", "book", "magazine", "organization", "country"], "instance": {"id": "12", "words": ["Later", "in", "his", "life", ",", "Ginsberg", "formed", "a", "bridge", "between", "the", "beat", "movement", "of", "the", "1950s", "and", "the", "hippie", "s", "of", "the", "1960s", ",", "befriending", ",", "among", "others", ",", "Timothy", "Leary", ",", "Ken", "Kesey", ",", "Hunter", "S.", "Thompson", ",", "and", "Bob", "Dylan", "."], "labels": ["O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, person, location, poem, award, event, writer, book, magazine, organization, country and O.\nSentence: Later in his life , Ginsberg formed a bridge between the beat movement of the 1950s and the hippie s of the 1960s , befriending , among others , Timothy Leary , Ken Kesey , Hunter S. Thompson , and Bob Dylan .", "prompt_labels": "Later(O) in(O) his(O) life(O) ,(O) Ginsberg(B-writer) formed(O) a(O) bridge(O) between(O) the(O) beat(O) movement(O) of(O) the(O) 1950s(O) and(O) the(O) hippie(O) s(O) of(O) the(O) 1960s(O) ,(O) befriending(O) ,(O) among(O) others(O) ,(O) Timothy(B-writer) Leary(I-writer) ,(O) Ken(B-writer) Kesey(I-writer) ,(O) Hunter(B-writer) S.(I-writer) Thompson(I-writer) ,(O) and(O) Bob(B-writer) Dylan(I-writer) .(O)"}}
{"id": "3", "dataset": "crossner_literature", "split": "dev", "label_list": ["poem", "location", "writer", "event", "book", "organization", "person", "literary genre", "magazine", "award", "country"], "instance": {"id": "3", "words": ["Atwood", "has", "strong", "views", "on", "environmental", "issues", ",", "and", "she", "and", "Graeme", "Gibson", "were", "the", "joint", "honorary", "presidents", "of", "the", "Rare", "Bird", "Club", "within", "BirdLife", "International", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, location, writer, event, book, organization, person, literary genre, magazine, award, country and O.\nSentence: Atwood has strong views on environmental issues , and she and Graeme Gibson were the joint honorary presidents of the Rare Bird Club within BirdLife International .", "prompt_labels": "Atwood(B-writer) has(O) strong(O) views(O) on(O) environmental(O) issues(O) ,(O) and(O) she(O) and(O) Graeme(B-writer) Gibson(I-writer) were(O) the(O) joint(O) honorary(O) presidents(O) of(O) the(O) Rare(B-organization) Bird(I-organization) Club(I-organization) within(O) BirdLife(B-organization) International(I-organization) .(O)"}}
{"id": "99", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "organization", "book", "writer", "person", "literary genre", "event", "location", "award", "country", "poem"], "instance": {"id": "99", "words": ["Davy", "was", "a", "baronet", ",", "President", "of", "the", "Royal", "Society", "(", "PRS", ")", ",", "Royal", "Irish", "Academy", ",", "and", "Geological", "Society", "of", "London", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, organization, book, writer, person, literary genre, event, location, award, country, poem and O.\nSentence: Davy was a baronet , President of the Royal Society ( PRS ) , Royal Irish Academy , and Geological Society of London .", "prompt_labels": "Davy(B-person) was(O) a(O) baronet(O) ,(O) President(O) of(O) the(O) Royal(O) Society(O) ((O) PRS(O) )(O) ,(O) Royal(B-organization) Irish(I-organization) Academy(I-organization) ,(O) and(O) Geological(B-organization) Society(I-organization) of(I-organization) London(I-organization) .(O)"}}
{"id": "204", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "poem", "book", "writer", "magazine", "literary genre", "organization", "event", "award", "person", "location"], "instance": {"id": "204", "words": ["Klinger", ",", "The", "Daily", "Mail", "review", "of", "1", "June", "1897", "proclaimed", "it", "a", "classic", "of", "Gothic", "horror", ",", "In", "seeking", "a", "parallel", "to", "this", "weird", ",", "powerful", ",", "and", "horrorful", "story", "our", "mind", "reverts", "to", "such", "tales", "as", "The", "Mysteries", "of", "Udolpho", ",", "Frankenstein", ",", "The", "Fall", "of", "the", "House", "of", "Usher", "...", "but", "Dracula", "is", "even", "more", "appalling", "in", "its", "gloomy", "fascination", "than", "any", "one", "of", "these", "."], "labels": ["B-person", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "B-book", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, poem, book, writer, magazine, literary genre, organization, event, award, person, location and O.\nSentence: Klinger , The Daily Mail review of 1 June 1897 proclaimed it a classic of Gothic horror , In seeking a parallel to this weird , powerful , and horrorful story our mind reverts to such tales as The Mysteries of Udolpho , Frankenstein , The Fall of the House of Usher ... but Dracula is even more appalling in its gloomy fascination than any one of these .", "prompt_labels": "Klinger(B-person) ,(O) The(O) Daily(B-organization) Mail(I-organization) review(O) of(O) 1(O) June(O) 1897(O) proclaimed(O) it(O) a(O) classic(O) of(O) Gothic(B-literary genre) horror(I-literary genre) ,(O) In(O) seeking(O) a(O) parallel(O) to(O) this(O) weird(O) ,(O) powerful(O) ,(O) and(O) horrorful(O) story(O) our(O) mind(O) reverts(O) to(O) such(O) tales(O) as(O) The(B-book) Mysteries(I-book) of(I-book) Udolpho(I-book) ,(O) Frankenstein(B-book) ,(O) The(B-book) Fall(I-book) of(I-book) the(I-book) House(I-book) of(I-book) Usher(I-book) ...(O) but(O) Dracula(B-book) is(O) even(O) more(O) appalling(O) in(O) its(O) gloomy(O) fascination(O) than(O) any(O) one(O) of(O) these(O) .(O)"}}
{"id": "249", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "person", "magazine", "literary genre", "award", "book", "location", "poem", "country", "event", "organization"], "instance": {"id": "249", "words": ["Philip", "K.", "Dick", "'", "s", "novel", ",", "The", "Man", "in", "the", "High", "Castle", "(", "1962", ")", ",", "is", "an", "alternate", "history", "in", "which", "Nazi", "Germany", "and", "Imperial", "Japan", "won", "World", "War", "II", "."], "labels": ["B-writer", "I-writer", "I-writer", "O", "O", "B-literary genre", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "B-country", "I-country", "O", "B-country", "I-country", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, person, magazine, literary genre, award, book, location, poem, country, event, organization and O.\nSentence: Philip K. Dick ' s novel , The Man in the High Castle ( 1962 ) , is an alternate history in which Nazi Germany and Imperial Japan won World War II .", "prompt_labels": "Philip(B-writer) K.(I-writer) Dick(I-writer) '(O) s(O) novel(B-literary genre) ,(O) The(B-book) Man(I-book) in(I-book) the(I-book) High(I-book) Castle(I-book) ((O) 1962(O) )(O) ,(O) is(O) an(O) alternate(B-literary genre) history(I-literary genre) in(O) which(O) Nazi(B-country) Germany(I-country) and(O) Imperial(B-country) Japan(I-country) won(O) World(B-event) War(I-event) II(I-event) .(O)"}}
{"id": "255", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "writer", "person", "organization", "location", "award", "event", "literary genre", "magazine", "poem", "book"], "instance": {"id": "255", "words": ["William", "F.", "Buckley", ",", "Jr", "and", "other", "contributors", "for", "the", "National", "Review", "magazine", "."], "labels": ["B-writer", "I-writer", "I-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, writer, person, organization, location, award, event, literary genre, magazine, poem, book and O.\nSentence: William F. Buckley , Jr and other contributors for the National Review magazine .", "prompt_labels": "William(B-writer) F.(I-writer) Buckley(I-writer) ,(I-writer) Jr(I-writer) and(O) other(O) contributors(O) for(O) the(O) National(B-magazine) Review(I-magazine) magazine(O) .(O)"}}
{"id": "106", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "organization", "book", "location", "poem", "person", "award", "magazine", "writer", "literary genre", "event"], "instance": {"id": "106", "words": ["91", "A", "subsequent", "journey", "through", "the", "British", "East", "Africa", "colonies", "and", "the", "Belgian", "Congo", "formed", "the", "basis", "of", "two", "books", ";", "the", "travelogue", "Remote", "People", "(", "1931", ")", "and", "the", "comic", "novel", "Black", "Mischief", "(", "1932", ")", ".", "Sykes", ",", "p", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "B-book", "I-book", "O", "O", "O", "O", "B-writer", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, book, location, poem, person, award, magazine, writer, literary genre, event and O.\nSentence: 91 A subsequent journey through the British East Africa colonies and the Belgian Congo formed the basis of two books ; the travelogue Remote People ( 1931 ) and the comic novel Black Mischief ( 1932 ) . Sykes , p .", "prompt_labels": "91(O) A(O) subsequent(O) journey(O) through(O) the(O) British(B-location) East(I-location) Africa(I-location) colonies(I-location) and(O) the(O) Belgian(B-country) Congo(I-country) formed(O) the(O) basis(O) of(O) two(O) books(O) ;(O) the(O) travelogue(O) Remote(B-book) People(I-book) ((O) 1931(O) )(O) and(O) the(O) comic(B-literary genre) novel(I-literary genre) Black(B-book) Mischief(I-book) ((O) 1932(O) )(O) .(O) Sykes(B-writer) ,(O) p(O) .(O)"}}
{"id": "47", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "person", "location", "organization", "magazine", "award", "literary genre", "country", "writer", "poem", "event"], "instance": {"id": "47", "words": ["In", "March", "2020", ",", "a", "third", "season", "of", "Cosmos", "named", "Cosmos", ":", "Possible", "Worlds", ",", "for", "which", "Druyan", "was", "executive", "producer", ",", "writer", ",", "and", "director", "premiered", "on", "National", "Geographic", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, person, location, organization, magazine, award, literary genre, country, writer, poem, event and O.\nSentence: In March 2020 , a third season of Cosmos named Cosmos : Possible Worlds , for which Druyan was executive producer , writer , and director premiered on National Geographic .", "prompt_labels": "In(O) March(O) 2020(O) ,(O) a(O) third(O) season(O) of(O) Cosmos(O) named(O) Cosmos(O) :(O) Possible(O) Worlds(O) ,(O) for(O) which(O) Druyan(B-writer) was(O) executive(O) producer(O) ,(O) writer(O) ,(O) and(O) director(O) premiered(O) on(O) National(B-magazine) Geographic(I-magazine) .(O)"}}
{"id": "313", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "magazine", "organization", "literary genre", "poem", "award", "event", "location", "book", "person", "writer"], "instance": {"id": "313", "words": ["Gates", "read", "the", "January", "1975", "issue", "of", "Popular", "Electronics", "which", "demonstrated", "the", "Altair", "8800", ",", "and", "he", "contacted", "Micro", "Instrumentation", "and", "Telemetry", "Systems", "(", "MITS", ")", "to", "inform", "them", "that", "he", "and", "others", "were", "working", "on", "a", "BASIC", "interpreter", "for", "the", "platform", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, magazine, organization, literary genre, poem, award, event, location, book, person, writer and O.\nSentence: Gates read the January 1975 issue of Popular Electronics which demonstrated the Altair 8800 , and he contacted Micro Instrumentation and Telemetry Systems ( MITS ) to inform them that he and others were working on a BASIC interpreter for the platform .", "prompt_labels": "Gates(B-person) read(O) the(O) January(O) 1975(O) issue(O) of(O) Popular(B-magazine) Electronics(I-magazine) which(O) demonstrated(O) the(O) Altair(O) 8800(O) ,(O) and(O) he(O) contacted(O) Micro(B-organization) Instrumentation(I-organization) and(I-organization) Telemetry(I-organization) Systems(I-organization) ((O) MITS(B-organization) )(O) to(O) inform(O) them(O) that(O) he(O) and(O) others(O) were(O) working(O) on(O) a(O) BASIC(O) interpreter(O) for(O) the(O) platform(O) .(O)"}}
{"id": "29", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "poem", "country", "literary genre", "person", "writer", "magazine", "location", "book", "organization", "award"], "instance": {"id": "29", "words": ["Other", "figures", "in", "literature", "who", "were", "strongly", "influenced", "by", "Schopenhauer", "were", "Thomas", "Mann", ",", "Afanasy", "Fet", ",", "Joris-Karl", "Huysmans", "and", "George", "Santayana", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, poem, country, literary genre, person, writer, magazine, location, book, organization, award and O.\nSentence: Other figures in literature who were strongly influenced by Schopenhauer were Thomas Mann , Afanasy Fet , Joris-Karl Huysmans and George Santayana .", "prompt_labels": "Other(O) figures(O) in(O) literature(O) who(O) were(O) strongly(O) influenced(O) by(O) Schopenhauer(B-writer) were(O) Thomas(B-writer) Mann(I-writer) ,(O) Afanasy(B-writer) Fet(I-writer) ,(O) Joris-Karl(B-writer) Huysmans(I-writer) and(O) George(B-writer) Santayana(I-writer) .(O)"}}
{"id": "383", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "event", "book", "magazine", "poem", "literary genre", "country", "writer", "award", "location", "person"], "instance": {"id": "383", "words": ["In", "2003", ",", "Van", "Sant", "'s", "film", "about", "the", "Columbine", "High", "School", "massacre", ",", "Elephant", ",", "won", "the", "Palme", "d", "'Or", "at", "the", "Cannes", "Film", "Festival", "."], "labels": ["O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, book, magazine, poem, literary genre, country, writer, award, location, person and O.\nSentence: In 2003 , Van Sant 's film about the Columbine High School massacre , Elephant , won the Palme d 'Or at the Cannes Film Festival .", "prompt_labels": "In(O) 2003(O) ,(O) Van(B-person) Sant(I-person) 's(O) film(O) about(O) the(O) Columbine(B-event) High(I-event) School(I-event) massacre(I-event) ,(O) Elephant(O) ,(O) won(O) the(O) Palme(B-award) d(I-award) 'Or(I-award) at(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) .(O)"}}
{"id": "352", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "magazine", "award", "book", "person", "location", "writer", "literary genre", "poem", "country", "event"], "instance": {"id": "352", "words": ["Hannon", "contributed", "to", "a", "musical", "version", "of", "Swallows", "and", "Amazons", ",", "writing", "the", "music", "while", "Helen", "Edmundson", "wrote", "the", "book", "and", "lyrics", ",", "which", "premiered", "in", "December", "2010", "at", "the", "Bristol", "Old", "Vic", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, magazine, award, book, person, location, writer, literary genre, poem, country, event and O.\nSentence: Hannon contributed to a musical version of Swallows and Amazons , writing the music while Helen Edmundson wrote the book and lyrics , which premiered in December 2010 at the Bristol Old Vic .", "prompt_labels": "Hannon(B-person) contributed(O) to(O) a(O) musical(O) version(O) of(O) Swallows(B-book) and(I-book) Amazons(I-book) ,(O) writing(O) the(O) music(O) while(O) Helen(B-writer) Edmundson(I-writer) wrote(O) the(O) book(O) and(O) lyrics(O) ,(O) which(O) premiered(O) in(O) December(O) 2010(O) at(O) the(O) Bristol(B-location) Old(I-location) Vic(I-location) .(O)"}}
{"id": "139", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "writer", "literary genre", "country", "award", "book", "location", "event", "person", "poem", "organization"], "instance": {"id": "139", "words": ["Poet", "and", "lead", "singer", "of", "King", "Missile", ",", "John", "S.", "Hall", "has", "also", "long", "been", "a", "vocal", "opponent", ",", "taking", "issue", "with", "such", "factors", "as", "its", "inherently", "competitive", "nature", "In", "his", "2005", "interview", "in", "Words", "In", "Your", "Face", ":", "A", "Guided", "Tour", "Through", "Twenty", "Years", "of", "the", "New", "York", "City", "Poetry", "Slam", ",", "he", "recalls", "seeing", "his", "first", "slam", ",", "at", "the", "Nuyorican", "Poets", "Café", ":", "...", "I", "hated", "it", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, writer, literary genre, country, award, book, location, event, person, poem, organization and O.\nSentence: Poet and lead singer of King Missile , John S. Hall has also long been a vocal opponent , taking issue with such factors as its inherently competitive nature In his 2005 interview in Words In Your Face : A Guided Tour Through Twenty Years of the New York City Poetry Slam , he recalls seeing his first slam , at the Nuyorican Poets Café : ... I hated it .", "prompt_labels": "Poet(O) and(O) lead(O) singer(O) of(O) King(O) Missile(O) ,(O) John(B-writer) S.(I-writer) Hall(I-writer) has(O) also(O) long(O) been(O) a(O) vocal(O) opponent(O) ,(O) taking(O) issue(O) with(O) such(O) factors(O) as(O) its(O) inherently(O) competitive(O) nature(O) In(O) his(O) 2005(O) interview(O) in(O) Words(B-book) In(I-book) Your(I-book) Face(I-book) :(I-book) A(I-book) Guided(I-book) Tour(I-book) Through(I-book) Twenty(I-book) Years(I-book) of(I-book) the(I-book) New(I-book) York(I-book) City(I-book) Poetry(I-book) Slam(I-book) ,(O) he(O) recalls(O) seeing(O) his(O) first(O) slam(O) ,(O) at(O) the(O) Nuyorican(B-location) Poets(I-location) Café(I-location) :(O) ...(O) I(O) hated(O) it(O) .(O)"}}
{"id": "289", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "location", "poem", "organization", "person", "writer", "book", "magazine", "award", "country", "event"], "instance": {"id": "289", "words": ["In", "January", "2008", "Fry", "broke", "his", "arm", "while", "filming", "Last", "Chance", "to", "See", "in", "Brazil", "."], "labels": ["O", "O", "O", "B-person", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, location, poem, organization, person, writer, book, magazine, award, country, event and O.\nSentence: In January 2008 Fry broke his arm while filming Last Chance to See in Brazil .", "prompt_labels": "In(O) January(O) 2008(O) Fry(B-person) broke(O) his(O) arm(O) while(O) filming(O) Last(B-book) Chance(I-book) to(I-book) See(I-book) in(O) Brazil(B-country) .(O)"}}
{"id": "165", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "poem", "literary genre", "event", "writer", "book", "award", "magazine", "organization", "location", "country"], "instance": {"id": "165", "words": ["In", "addition", "to", "receiving", "a", "star", "on", "the", "Hollywood", "Walk", "of", "Fame", ",", "media", "appearances", "included", "write-ups", "in", "CCM", "Magazine", ",", "and", "a", "performance", "on", "The", "View", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, poem, literary genre, event, writer, book, award, magazine, organization, location, country and O.\nSentence: In addition to receiving a star on the Hollywood Walk of Fame , media appearances included write-ups in CCM Magazine , and a performance on The View .", "prompt_labels": "In(O) addition(O) to(O) receiving(O) a(O) star(O) on(O) the(O) Hollywood(B-location) Walk(I-location) of(I-location) Fame(I-location) ,(O) media(O) appearances(O) included(O) write-ups(O) in(O) CCM(B-magazine) Magazine(I-magazine) ,(O) and(O) a(O) performance(O) on(O) The(O) View(O) .(O)"}}
{"id": "24", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "event", "country", "literary genre", "person", "writer", "book", "location", "award", "poem", "magazine"], "instance": {"id": "24", "words": ["He", "reprised", "the", "role", "in", "Evil", "Under", "the", "Sun", "(", "1982", ")", "and", "Appointment", "with", "Death", "(", "1988", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, country, literary genre, person, writer, book, location, award, poem, magazine and O.\nSentence: He reprised the role in Evil Under the Sun ( 1982 ) and Appointment with Death ( 1988 ) .", "prompt_labels": "He(O) reprised(O) the(O) role(O) in(O) Evil(B-book) Under(I-book) the(I-book) Sun(I-book) ((O) 1982(O) )(O) and(O) Appointment(B-book) with(I-book) Death(I-book) ((O) 1988(O) )(O) .(O)"}}
{"id": "321", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "magazine", "literary genre", "person", "organization", "event", "location", "writer", "country", "poem", "award"], "instance": {"id": "321", "words": ["Richards", "has", "received", "numerous", "awards", "including", "two", "Gemini", "Awards", "for", "scriptwriting", "for", "Small", "Gifts", "and", "For", "Those", "Who", "Hunt", "the", "Wounded", "Down", ",", "the", "Alden", "Nowlan", "Award", "for", "Excellence", "in", "the", "Arts", ",", "and", "the", "Canadian", "Authors", "Association", "Award", "for", "his", "novel", "Evening", "Snow", "Will", "Bring", "Such", "Peace", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "B-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "B-literary genre", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, magazine, literary genre, person, organization, event, location, writer, country, poem, award and O.\nSentence: Richards has received numerous awards including two Gemini Awards for scriptwriting for Small Gifts and For Those Who Hunt the Wounded Down , the Alden Nowlan Award for Excellence in the Arts , and the Canadian Authors Association Award for his novel Evening Snow Will Bring Such Peace .", "prompt_labels": "Richards(B-writer) has(O) received(O) numerous(O) awards(O) including(O) two(O) Gemini(B-award) Awards(I-award) for(O) scriptwriting(O) for(O) Small(B-book) Gifts(I-book) and(O) For(B-book) Those(I-book) Who(I-book) Hunt(I-book) the(I-book) Wounded(I-book) Down(I-book) ,(O) the(O) Alden(B-award) Nowlan(I-award) Award(I-award) for(I-award) Excellence(I-award) in(I-award) the(I-award) Arts(I-award) ,(O) and(O) the(O) Canadian(B-award) Authors(I-award) Association(I-award) Award(I-award) for(O) his(O) novel(B-literary genre) Evening(B-book) Snow(I-book) Will(I-book) Bring(I-book) Such(I-book) Peace(I-book) .(O)"}}
{"id": "225", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "writer", "event", "location", "country", "magazine", "poem", "book", "award", "literary genre", "organization"], "instance": {"id": "225", "words": ["Pat", "Barker", "'", "s", "historical", "novel", "Regeneration", "(", "1991", ")", "describes", "the", "meeting", "and", "relationship", "between", "Sassoon", "and", "Owen", ","], "labels": ["B-writer", "I-writer", "O", "O", "B-literary genre", "I-literary genre", "B-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "B-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, event, location, country, magazine, poem, book, award, literary genre, organization and O.\nSentence: Pat Barker ' s historical novel Regeneration ( 1991 ) describes the meeting and relationship between Sassoon and Owen ,", "prompt_labels": "Pat(B-writer) Barker(I-writer) '(O) s(O) historical(B-literary genre) novel(I-literary genre) Regeneration(B-magazine) ((O) 1991(O) )(O) describes(O) the(O) meeting(O) and(O) relationship(O) between(O) Sassoon(B-writer) and(O) Owen(B-writer) ,(O)"}}
{"id": "149", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "country", "person", "literary genre", "writer", "organization", "location", "poem", "magazine", "book", "award"], "instance": {"id": "149", "words": ["Out", "of", "public", "office", "for", "the", "first", "time", "since", "the", "1960s", ",", "Bush", "became", "chairman", "on", "the", "Executive", "Committee", "of", "the", "First", "International", "Bank", "in", "Houston.", "continued", "his", "membership", "in", "the", "Council", "on", "Foreign", "Relations", ",", "and", "joined", "the", "Trilateral", "Commission", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, person, literary genre, writer, organization, location, poem, magazine, book, award and O.\nSentence: Out of public office for the first time since the 1960s , Bush became chairman on the Executive Committee of the First International Bank in Houston. continued his membership in the Council on Foreign Relations , and joined the Trilateral Commission .", "prompt_labels": "Out(O) of(O) public(O) office(O) for(O) the(O) first(O) time(O) since(O) the(O) 1960s(O) ,(O) Bush(B-person) became(O) chairman(O) on(O) the(O) Executive(O) Committee(O) of(O) the(O) First(B-organization) International(I-organization) Bank(I-organization) in(O) Houston.(B-location) continued(O) his(O) membership(O) in(O) the(O) Council(B-organization) on(I-organization) Foreign(I-organization) Relations(I-organization) ,(O) and(O) joined(O) the(O) Trilateral(B-organization) Commission(I-organization) .(O)"}}
{"id": "251", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "person", "poem", "award", "writer", "event", "literary genre", "organization", "book", "location", "magazine"], "instance": {"id": "251", "words": ["137", "was", "a", "German", "-", "Netherlands", "canon", "regular", "of", "the", "late", "medieval", "period", "and", "the", "author", "of", "The", "Imitation", "of", "Christ", ",", "one", "of", "the", "most", "popular", "and", "best", "known", "Christian", "devotional", "books", "."], "labels": ["B-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, poem, award, writer, event, literary genre, organization, book, location, magazine and O.\nSentence: 137 was a German - Netherlands canon regular of the late medieval period and the author of The Imitation of Christ , one of the most popular and best known Christian devotional books .", "prompt_labels": "137(B-book) was(O) a(O) German(O) -(O) Netherlands(O) canon(O) regular(O) of(O) the(O) late(O) medieval(O) period(O) and(O) the(O) author(O) of(O) The(B-book) Imitation(I-book) of(I-book) Christ(I-book) ,(O) one(O) of(O) the(O) most(O) popular(O) and(O) best(O) known(O) Christian(O) devotional(O) books(O) .(O)"}}
{"id": "346", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "person", "literary genre", "organization", "location", "country", "magazine", "poem", "book", "writer", "award"], "instance": {"id": "346", "words": ["Bergman", "became", "one", "of", "the", "few", "actresses", "ever", "to", "receive", "three", "Oscars", "when", "she", "won", "her", "third", "(", "and", "first", "in", "the", "category", "of", "Academy", "Award", "for", "Best", "Supporting", "Actress", ")", "for", "her", "performance", "in", "Murder", "on", "the", "Orient", "Express", "(", "1974", ")", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, literary genre, organization, location, country, magazine, poem, book, writer, award and O.\nSentence: Bergman became one of the few actresses ever to receive three Oscars when she won her third ( and first in the category of Academy Award for Best Supporting Actress ) for her performance in Murder on the Orient Express ( 1974 ) .", "prompt_labels": "Bergman(B-writer) became(O) one(O) of(O) the(O) few(O) actresses(O) ever(O) to(O) receive(O) three(O) Oscars(B-award) when(O) she(O) won(O) her(O) third(O) ((O) and(O) first(O) in(O) the(O) category(O) of(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actress(I-award) )(O) for(O) her(O) performance(O) in(O) Murder(O) on(O) the(O) Orient(O) Express(O) ((O) 1974(O) )(O) .(O)"}}
{"id": "215", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "literary genre", "magazine", "organization", "award", "writer", "book", "event", "person", "location", "poem"], "instance": {"id": "215", "words": ["Hasanaginica", "was", "the", "only", "authentic", "ballad", "included", "into", "La", "Guzla", ",", "an", "1827", "literary", "hoax", "of", "Prosper", "Mérimée", "."], "labels": ["B-book", "O", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "O", "O", "B-event", "I-event", "I-event", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, literary genre, magazine, organization, award, writer, book, event, person, location, poem and O.\nSentence: Hasanaginica was the only authentic ballad included into La Guzla , an 1827 literary hoax of Prosper Mérimée .", "prompt_labels": "Hasanaginica(B-book) was(O) the(O) only(O) authentic(O) ballad(O) included(O) into(O) La(B-poem) Guzla(I-poem) ,(O) an(O) 1827(B-event) literary(I-event) hoax(I-event) of(O) Prosper(B-writer) Mérimée(I-writer) .(O)"}}
{"id": "168", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "event", "book", "award", "person", "location", "organization", "magazine", "writer", "country", "poem"], "instance": {"id": "168", "words": ["In", "an", "April", "2015", "interview", "with", "Billboard", ",", "Lavigne", "announced", "a", "new", "single", "titled", "Fly", ",", "which", "was", "released", "on", "April", "26", "in", "association", "with", "the", "2015", "Special", "Olympics", "World", "Summer", "Games", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, event, book, award, person, location, organization, magazine, writer, country, poem and O.\nSentence: In an April 2015 interview with Billboard , Lavigne announced a new single titled Fly , which was released on April 26 in association with the 2015 Special Olympics World Summer Games .", "prompt_labels": "In(O) an(O) April(O) 2015(O) interview(O) with(O) Billboard(O) ,(O) Lavigne(B-person) announced(O) a(O) new(O) single(O) titled(O) Fly(O) ,(O) which(O) was(O) released(O) on(O) April(O) 26(O) in(O) association(O) with(O) the(O) 2015(B-event) Special(I-event) Olympics(I-event) World(I-event) Summer(I-event) Games(I-event) .(O)"}}
{"id": "386", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "poem", "country", "writer", "event", "organization", "location", "award", "person", "literary genre", "magazine"], "instance": {"id": "386", "words": ["Thornton", "had", "his", "first", "break", "when", "he", "co-wrote", "and", "starred", "in", "the", "1992", "thriller", "One", "FALSE", "Move", ",", "and", "received", "international", "attention", "after", "writing", ",", "directing", ",", "and", "starring", "in", "the", "independent", "drama", "film", "Sling", "Blade", "(", "1996", ")", ",", "for", "which", "he", "won", "an", "Academy", "Award", "for", "Best", "Adapted", "Screenplay", "and", "was", "nominated", "for", "an", "Academy", "Award", "for", "Best", "Actor", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, poem, country, writer, event, organization, location, award, person, literary genre, magazine and O.\nSentence: Thornton had his first break when he co-wrote and starred in the 1992 thriller One FALSE Move , and received international attention after writing , directing , and starring in the independent drama film Sling Blade ( 1996 ) , for which he won an Academy Award for Best Adapted Screenplay and was nominated for an Academy Award for Best Actor .", "prompt_labels": "Thornton(B-writer) had(O) his(O) first(O) break(O) when(O) he(O) co-wrote(O) and(O) starred(O) in(O) the(O) 1992(O) thriller(O) One(O) FALSE(O) Move(O) ,(O) and(O) received(O) international(O) attention(O) after(O) writing(O) ,(O) directing(O) ,(O) and(O) starring(O) in(O) the(O) independent(O) drama(O) film(O) Sling(O) Blade(O) ((O) 1996(O) )(O) ,(O) for(O) which(O) he(O) won(O) an(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) and(O) was(O) nominated(O) for(O) an(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) .(O)"}}
{"id": "181", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "book", "award", "poem", "organization", "person", "literary genre", "writer", "magazine", "event", "location"], "instance": {"id": "181", "words": ["Like", "his", "contemporaries", "Algernon", "Blackwood", "and", "Arthur", "Machen", ",", "Rohmer", "claimed", "membership", "to", "one", "of", "the", "factions", "of", "the", "qabbalistic", "Hermetic", "Order", "of", "the", "Golden", "Dawn", "."], "labels": ["O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, book, award, poem, organization, person, literary genre, writer, magazine, event, location and O.\nSentence: Like his contemporaries Algernon Blackwood and Arthur Machen , Rohmer claimed membership to one of the factions of the qabbalistic Hermetic Order of the Golden Dawn .", "prompt_labels": "Like(O) his(O) contemporaries(O) Algernon(B-writer) Blackwood(I-writer) and(O) Arthur(B-writer) Machen(I-writer) ,(O) Rohmer(B-writer) claimed(O) membership(O) to(O) one(O) of(O) the(O) factions(O) of(O) the(O) qabbalistic(O) Hermetic(B-organization) Order(I-organization) of(I-organization) the(I-organization) Golden(I-organization) Dawn(I-organization) .(O)"}}
{"id": "350", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "book", "literary genre", "magazine", "organization", "poem", "writer", "person", "event", "location", "country"], "instance": {"id": "350", "words": ["He", "is", "a", "council", "member", "of", "the", "International", "Forum", "for", "Democratic", "Studies", "founded", "by", "the", "National", "Endowment", "for", "Democracy", "and", "was", "a", "member", "of", "the", "Political", "Science", "Department", "of", "the", "RAND", "Corporation", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, book, literary genre, magazine, organization, poem, writer, person, event, location, country and O.\nSentence: He is a council member of the International Forum for Democratic Studies founded by the National Endowment for Democracy and was a member of the Political Science Department of the RAND Corporation .", "prompt_labels": "He(O) is(O) a(O) council(O) member(O) of(O) the(O) International(O) Forum(O) for(O) Democratic(O) Studies(O) founded(O) by(O) the(O) National(B-organization) Endowment(I-organization) for(I-organization) Democracy(I-organization) and(O) was(O) a(O) member(O) of(O) the(O) Political(O) Science(O) Department(O) of(O) the(O) RAND(B-organization) Corporation(I-organization) .(O)"}}
{"id": "302", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "poem", "organization", "country", "person", "event", "location", "book", "literary genre", "award", "magazine"], "instance": {"id": "302", "words": ["In", "July", "2017", ",", "the", "book", "was", "listed", "as", "the", "most", "influential", "science", "book", "of", "all", "time", "in", "a", "poll", "to", "celebrate", "the", "30th", "anniversary", "of", "the", "Royal", "Society", "science", "book", "prize", ",", "ahead", "of", "Charles", "Darwin", "'", "s", "On", "the", "Origin", "of", "Species", "and", "Isaac", "Newton", "s", "Philosophiæ", "Naturalis", "Principia", "Mathematica", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "B-writer", "I-writer", "O", "B-book", "I-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, poem, organization, country, person, event, location, book, literary genre, award, magazine and O.\nSentence: In July 2017 , the book was listed as the most influential science book of all time in a poll to celebrate the 30th anniversary of the Royal Society science book prize , ahead of Charles Darwin ' s On the Origin of Species and Isaac Newton s Philosophiæ Naturalis Principia Mathematica .", "prompt_labels": "In(O) July(O) 2017(O) ,(O) the(O) book(O) was(O) listed(O) as(O) the(O) most(O) influential(O) science(O) book(O) of(O) all(O) time(O) in(O) a(O) poll(O) to(O) celebrate(O) the(O) 30th(O) anniversary(O) of(O) the(O) Royal(B-organization) Society(I-organization) science(O) book(O) prize(O) ,(O) ahead(O) of(O) Charles(B-writer) Darwin(I-writer) '(O) s(O) On(B-book) the(I-book) Origin(I-book) of(I-book) Species(I-book) and(O) Isaac(B-writer) Newton(I-writer) s(O) Philosophiæ(B-book) Naturalis(I-book) Principia(I-book) Mathematica(I-book) .(O)"}}
{"id": "17", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "event", "organization", "person", "literary genre", "country", "location", "award", "book", "magazine", "poem"], "instance": {"id": "17", "words": ["Silas", "Marner", "(", "1861", ")", "and", "Romola", "(", "1863", ")", "soon", "followed", ",", "and", "later", "Felix", "Holt", ",", "the", "Radical", "(", "1866", ")", "and", "her", "most", "acclaimed", "novel", ",", "Middlemarch", "(", "1871-1872", ")", "."], "labels": ["B-book", "I-book", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "B-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, event, organization, person, literary genre, country, location, award, book, magazine, poem and O.\nSentence: Silas Marner ( 1861 ) and Romola ( 1863 ) soon followed , and later Felix Holt , the Radical ( 1866 ) and her most acclaimed novel , Middlemarch ( 1871-1872 ) .", "prompt_labels": "Silas(B-book) Marner(I-book) ((O) 1861(O) )(O) and(O) Romola(B-book) ((O) 1863(O) )(O) soon(O) followed(O) ,(O) and(O) later(O) Felix(B-book) Holt(I-book) ,(I-book) the(I-book) Radical(I-book) ((O) 1866(O) )(O) and(O) her(O) most(O) acclaimed(O) novel(B-literary genre) ,(O) Middlemarch(B-book) ((O) 1871-1872(O) )(O) .(O)"}}
{"id": "307", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "book", "country", "location", "person", "organization", "magazine", "event", "writer", "poem", "award"], "instance": {"id": "307", "words": ["Also", ",", "In", "the", "short", "story", "La", "Santa", ",", "by", "Nobel", "Prize", "winner", "Gabriel", "García", "Márquez", "a", "character", "is", "named", "after", "Zavattini", "."], "labels": ["O", "O", "O", "O", "B-literary genre", "I-literary genre", "B-book", "I-book", "O", "O", "B-award", "I-award", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "B-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, book, country, location, person, organization, magazine, event, writer, poem, award and O.\nSentence: Also , In the short story La Santa , by Nobel Prize winner Gabriel García Márquez a character is named after Zavattini .", "prompt_labels": "Also(O) ,(O) In(O) the(O) short(B-literary genre) story(I-literary genre) La(B-book) Santa(I-book) ,(O) by(O) Nobel(B-award) Prize(I-award) winner(O) Gabriel(B-writer) García(I-writer) Márquez(I-writer) a(O) character(O) is(O) named(O) after(O) Zavattini(B-person) .(O)"}}
{"id": "9", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "person", "book", "award", "literary genre", "organization", "writer", "poem", "magazine", "event", "country"], "instance": {"id": "9", "words": ["In", "2012", ",", "the", "Nobel", "Records", "were", "opened", "after", "50", "years", "and", "it", "was", "revealed", "that", "Anouilh", "was", "among", "a", "shortlist", "of", "authors", "considered", "for", "the", "1962", "Nobel", "Prize", "in", "Literature", ",", "along", "with", "John", "Steinbeck", "(", "winner", ")", ",", "Robert", "Graves", ",", "Lawrence", "Durrell", "and", "Karen", "Blixen", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, book, award, literary genre, organization, writer, poem, magazine, event, country and O.\nSentence: In 2012 , the Nobel Records were opened after 50 years and it was revealed that Anouilh was among a shortlist of authors considered for the 1962 Nobel Prize in Literature , along with John Steinbeck ( winner ) , Robert Graves , Lawrence Durrell and Karen Blixen .", "prompt_labels": "In(O) 2012(O) ,(O) the(O) Nobel(O) Records(O) were(O) opened(O) after(O) 50(O) years(O) and(O) it(O) was(O) revealed(O) that(O) Anouilh(B-writer) was(O) among(O) a(O) shortlist(O) of(O) authors(O) considered(O) for(O) the(O) 1962(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) ,(O) along(O) with(O) John(B-writer) Steinbeck(I-writer) ((O) winner(O) )(O) ,(O) Robert(B-writer) Graves(I-writer) ,(O) Lawrence(B-writer) Durrell(I-writer) and(O) Karen(B-writer) Blixen(I-writer) .(O)"}}
{"id": "314", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "magazine", "location", "person", "event", "writer", "organization", "award", "poem", "book", "country"], "instance": {"id": "314", "words": ["Good", "Omens", ":", "The", "Nice", "and", "Accurate", "Prophecies", "of", "Agnes", "Nutter", ",", "Witch", "(", "1990", ")", "is", "a", "World", "Fantasy", "Award", "-nominated", "novel", "written", "as", "a", "collaboration", "between", "the", "English", "authors", "Terry", "Pratchett", "and", "Neil", "Gaiman", "."], "labels": ["B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, magazine, location, person, event, writer, organization, award, poem, book, country and O.\nSentence: Good Omens : The Nice and Accurate Prophecies of Agnes Nutter , Witch ( 1990 ) is a World Fantasy Award -nominated novel written as a collaboration between the English authors Terry Pratchett and Neil Gaiman .", "prompt_labels": "Good(B-book) Omens(I-book) :(I-book) The(I-book) Nice(I-book) and(I-book) Accurate(I-book) Prophecies(I-book) of(I-book) Agnes(I-book) Nutter(I-book) ,(I-book) Witch(I-book) ((O) 1990(O) )(O) is(O) a(O) World(B-award) Fantasy(I-award) Award(I-award) -nominated(O) novel(B-literary genre) written(O) as(O) a(O) collaboration(O) between(O) the(O) English(O) authors(O) Terry(B-writer) Pratchett(I-writer) and(O) Neil(B-writer) Gaiman(I-writer) .(O)"}}
{"id": "240", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "book", "award", "event", "poem", "location", "literary genre", "person", "country", "magazine", "organization"], "instance": {"id": "240", "words": ["Critics", "who", "frequently", "admire", "De", "Palma", "'s", "work", "include", "Pauline", "Kael", "and", "Roger", "Ebert", ",", "among", "others", "."], "labels": ["O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, award, event, poem, location, literary genre, person, country, magazine, organization and O.\nSentence: Critics who frequently admire De Palma 's work include Pauline Kael and Roger Ebert , among others .", "prompt_labels": "Critics(O) who(O) frequently(O) admire(O) De(B-writer) Palma(I-writer) 's(O) work(O) include(O) Pauline(B-writer) Kael(I-writer) and(O) Roger(B-writer) Ebert(I-writer) ,(O) among(O) others(O) .(O)"}}
{"id": "83", "dataset": "crossner_literature", "split": "dev", "label_list": ["poem", "literary genre", "country", "person", "writer", "magazine", "book", "event", "location", "award", "organization"], "instance": {"id": "83", "words": [")", "Huxley", "received", "screen", "credit", "for", "Pride", "and", "Prejudice", "(", "1940", ")", "and", "was", "paid", "for", "his", "work", "on", "a", "number", "of", "other", "films", ",", "including", "Jane", "Eyre", "(", "1944", ")", "."], "labels": ["O", "B-writer", "O", "B-award", "I-award", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, literary genre, country, person, writer, magazine, book, event, location, award, organization and O.\nSentence: ) Huxley received screen credit for Pride and Prejudice ( 1940 ) and was paid for his work on a number of other films , including Jane Eyre ( 1944 ) .", "prompt_labels": ")(O) Huxley(B-writer) received(O) screen(B-award) credit(I-award) for(O) Pride(B-book) and(I-book) Prejudice(I-book) ((O) 1940(O) )(O) and(O) was(O) paid(O) for(O) his(O) work(O) on(O) a(O) number(O) of(O) other(O) films(O) ,(O) including(O) Jane(O) Eyre(O) ((O) 1944(O) )(O) .(O)"}}
{"id": "273", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "poem", "writer", "book", "organization", "location", "country", "person", "magazine", "literary genre", "event"], "instance": {"id": "273", "words": ["First", "appearing", "in", "print", "in", "1887", "'s", "A", "Study", "in", "Scarlet", ",", "the", "character", "'s", "popularity", "became", "widespread", "with", "the", "first", "series", "of", "short", "stories", "in", "The", "Strand", "Magazine", ",", "beginning", "with", "A", "Scandal", "in", "Bohemia", "in", "1891", ";", "additional", "tales", "appeared", "from", "then", "until", "1927", ",", "eventually", "totalling", "four", "novels", "and", "56", "short", "stories", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-magazine", "I-magazine", "I-magazine", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "B-literary genre", "I-literary genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, poem, writer, book, organization, location, country, person, magazine, literary genre, event and O.\nSentence: First appearing in print in 1887 's A Study in Scarlet , the character 's popularity became widespread with the first series of short stories in The Strand Magazine , beginning with A Scandal in Bohemia in 1891 ; additional tales appeared from then until 1927 , eventually totalling four novels and 56 short stories .", "prompt_labels": "First(O) appearing(O) in(O) print(O) in(O) 1887(O) 's(O) A(B-book) Study(I-book) in(I-book) Scarlet(I-book) ,(O) the(O) character(O) 's(O) popularity(O) became(O) widespread(O) with(O) the(O) first(O) series(O) of(O) short(B-literary genre) stories(I-literary genre) in(O) The(B-magazine) Strand(I-magazine) Magazine(I-magazine) ,(O) beginning(O) with(O) A(B-book) Scandal(I-book) in(I-book) Bohemia(I-book) in(O) 1891(O) ;(O) additional(O) tales(O) appeared(O) from(O) then(O) until(O) 1927(O) ,(O) eventually(O) totalling(O) four(O) novels(B-literary genre) and(O) 56(O) short(B-literary genre) stories(I-literary genre) .(O)"}}
{"id": "280", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "book", "organization", "award", "magazine", "literary genre", "location", "person", "event", "country", "poem"], "instance": {"id": "280", "words": ["British", "policy", "in", "South", "Africa", "was", "to", "encourage", "federation", "between", "the", "British-run", "Cape", "Colony", "and", "Colony", "of", "Natal", ",", "and", "the", "Boer", "republics", ",", "the", "South", "African", "Republic", "(", "annexed", "by", "Britain", "in", "1877", ")", "and", "the", "Orange", "Free", "State", "."], "labels": ["O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "I-location", "O", "O", "O", "B-country", "I-country", "O", "O", "B-country", "I-country", "I-country", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "B-country", "I-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, organization, award, magazine, literary genre, location, person, event, country, poem and O.\nSentence: British policy in South Africa was to encourage federation between the British-run Cape Colony and Colony of Natal , and the Boer republics , the South African Republic ( annexed by Britain in 1877 ) and the Orange Free State .", "prompt_labels": "British(O) policy(O) in(O) South(B-country) Africa(I-country) was(O) to(O) encourage(O) federation(O) between(O) the(O) British-run(O) Cape(B-location) Colony(I-location) and(O) Colony(B-location) of(I-location) Natal(I-location) ,(O) and(O) the(O) Boer(B-country) republics(I-country) ,(O) the(O) South(B-country) African(I-country) Republic(I-country) ((O) annexed(O) by(O) Britain(B-country) in(O) 1877(O) )(O) and(O) the(O) Orange(B-country) Free(I-country) State(I-country) .(O)"}}
{"id": "28", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "writer", "poem", "country", "location", "event", "magazine", "book", "literary genre", "award", "organization"], "instance": {"id": "28", "words": ["Skíðaríma", ",", "Bjarkarímur", ",", "and", "Lokrur", "are", "other", "examples", "of", "early", "rímur", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, poem, country, location, event, magazine, book, literary genre, award, organization and O.\nSentence: Skíðaríma , Bjarkarímur , and Lokrur are other examples of early rímur .", "prompt_labels": "Skíðaríma(O) ,(O) Bjarkarímur(O) ,(O) and(O) Lokrur(O) are(O) other(O) examples(O) of(O) early(O) rímur(O) .(O)"}}
{"id": "222", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "writer", "literary genre", "organization", "poem", "award", "magazine", "event", "person", "location", "book"], "instance": {"id": "222", "words": ["Dennis", "MacAlistair", "Ritchie", "(", "September", "9", ",", "1941", "-", "October", "12", ",", "2011", ")", "Ritchie", "and", "Thompson", "were", "awarded", "the", "Turing", "Award", "from", "the", "Association", "for", "Computing", "Machinery", "in", "1983", ",", "the", "IEEE", "Richard", "W.", "Hamming", "Medal", "from", "the", "Institute", "of", "Electrical", "and", "Electronics", "Engineers", "in", "1990", "and", "the", "National", "Medal", "of", "Technology", "from", "President", "Bill", "Clinton", "in", "1999", "."], "labels": ["B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "B-person", "O", "O", "O", "B-award", "I-award", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "B-person", "I-person", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, writer, literary genre, organization, poem, award, magazine, event, person, location, book and O.\nSentence: Dennis MacAlistair Ritchie ( September 9 , 1941 - October 12 , 2011 ) Ritchie and Thompson were awarded the Turing Award from the Association for Computing Machinery in 1983 , the IEEE Richard W. Hamming Medal from the Institute of Electrical and Electronics Engineers in 1990 and the National Medal of Technology from President Bill Clinton in 1999 .", "prompt_labels": "Dennis(B-person) MacAlistair(I-person) Ritchie(I-person) ((O) September(O) 9(O) ,(O) 1941(O) -(O) October(O) 12(O) ,(O) 2011(O) )(O) Ritchie(B-person) and(O) Thompson(B-person) were(O) awarded(O) the(O) Turing(B-award) Award(I-award) from(O) the(O) Association(B-organization) for(I-organization) Computing(I-organization) Machinery(I-organization) in(O) 1983(O) ,(O) the(O) IEEE(B-award) Richard(I-award) W.(I-award) Hamming(I-award) Medal(I-award) from(O) the(O) Institute(B-organization) of(I-organization) Electrical(I-organization) and(I-organization) Electronics(I-organization) Engineers(I-organization) in(O) 1990(O) and(O) the(O) National(B-award) Medal(I-award) of(I-award) Technology(I-award) from(O) President(O) Bill(B-person) Clinton(I-person) in(O) 1999(O) .(O)"}}
{"id": "167", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "location", "writer", "country", "poem", "book", "organization", "magazine", "literary genre", "event", "person"], "instance": {"id": "167", "words": ["In", "the", "same", "year", ",", "he", "produced", "the", "first", "French", "language", "editions", "of", "Joseph", "Conrad", "'", "s", "Heart", "of", "Darkness", "and", "Lord", "Jim", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "O", "B-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, writer, country, poem, book, organization, magazine, literary genre, event, person and O.\nSentence: In the same year , he produced the first French language editions of Joseph Conrad ' s Heart of Darkness and Lord Jim .", "prompt_labels": "In(O) the(O) same(O) year(O) ,(O) he(O) produced(O) the(O) first(O) French(O) language(O) editions(O) of(O) Joseph(B-writer) Conrad(I-writer) '(O) s(O) Heart(B-book) of(I-book) Darkness(I-book) and(O) Lord(B-book) Jim(I-book) .(O)"}}
{"id": "136", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "award", "book", "event", "location", "writer", "poem", "literary genre", "organization", "country", "person"], "instance": {"id": "136", "words": ["Investigative", "journalist", "Michael", "Specter", ",", "in", "an", "article", "in", "The", "New", "Yorker", "on", "25", "August", "2014", "entitled", "Seeds", "of", "Doubt", ","], "labels": ["O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, award, book, event, location, writer, poem, literary genre, organization, country, person and O.\nSentence: Investigative journalist Michael Specter , in an article in The New Yorker on 25 August 2014 entitled Seeds of Doubt ,", "prompt_labels": "Investigative(O) journalist(O) Michael(B-writer) Specter(I-writer) ,(O) in(O) an(O) article(O) in(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) on(O) 25(O) August(O) 2014(O) entitled(O) Seeds(O) of(O) Doubt(O) ,(O)"}}
{"id": "357", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "location", "poem", "country", "organization", "award", "magazine", "literary genre", "event", "person", "book"], "instance": {"id": "357", "words": ["This", "was", "the", "case", ",", "for", "instance", ",", "in", "the", "The", "Weapon", "Shops", "of", "Isher", "series", ",", "the", "The", "Mixed", "Men", "series", ",", "and", "in", "single", "stories", "such", "as", "Heir", "Apparent", "(", "1945", ")", ",", "whose", "protagonist", "was", "described", "as", "a", "benevolent", "dictator", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, location, poem, country, organization, award, magazine, literary genre, event, person, book and O.\nSentence: This was the case , for instance , in the The Weapon Shops of Isher series , the The Mixed Men series , and in single stories such as Heir Apparent ( 1945 ) , whose protagonist was described as a benevolent dictator .", "prompt_labels": "This(O) was(O) the(O) case(O) ,(O) for(O) instance(O) ,(O) in(O) the(O) The(B-book) Weapon(I-book) Shops(I-book) of(I-book) Isher(I-book) series(O) ,(O) the(O) The(B-book) Mixed(I-book) Men(I-book) series(O) ,(O) and(O) in(O) single(O) stories(O) such(O) as(O) Heir(B-book) Apparent(I-book) ((O) 1945(O) )(O) ,(O) whose(O) protagonist(O) was(O) described(O) as(O) a(O) benevolent(O) dictator(O) .(O)"}}
{"id": "303", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "magazine", "literary genre", "writer", "book", "organization", "country", "poem", "location", "person", "event"], "instance": {"id": "303", "words": ["Among", "Montfort", "'s", "computer-generated", "books", "is", "#", "!", "(", "pronounced", "shebang", ")", ",", "in", "which", "he", "chooses", "the", "programming", "languages", "Python", ",", "Ruby", ",", "and", "Perl", "(", "the", "last", "of", "which", "has", "a", "Black", "Perl", "as", "a", "poetic", "medium", ")", "to", "create", "impressions", "of", "an", "ideal", "-", "machines", "based", "on", "the", "rules", "of", "language", "."], "labels": ["B-writer", "I-writer", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, magazine, literary genre, writer, book, organization, country, poem, location, person, event and O.\nSentence: Among Montfort 's computer-generated books is # ! ( pronounced shebang ) , in which he chooses the programming languages Python , Ruby , and Perl ( the last of which has a Black Perl as a poetic medium ) to create impressions of an ideal - machines based on the rules of language .", "prompt_labels": "Among(B-writer) Montfort(I-writer) 's(O) computer-generated(O) books(O) is(O) #(B-book) !(I-book) ((O) pronounced(O) shebang(O) )(O) ,(O) in(O) which(O) he(O) chooses(O) the(O) programming(O) languages(O) Python(O) ,(O) Ruby(O) ,(O) and(O) Perl(O) ((O) the(O) last(O) of(O) which(O) has(O) a(O) Black(B-poem) Perl(I-poem) as(O) a(O) poetic(O) medium(O) )(O) to(O) create(O) impressions(O) of(O) an(O) ideal(O) -(O) machines(O) based(O) on(O) the(O) rules(O) of(O) language(O) .(O)"}}
{"id": "337", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "literary genre", "magazine", "event", "organization", "location", "country", "poem", "person", "award", "writer"], "instance": {"id": "337", "words": ["S.", "M.", "Stirling", "wrote", "the", "Island", "in", "the", "Sea", "of", "Time", "trilogy", ",", "in", "which", "Nantucket", "Island", "and", "all", "its", "modern", "inhabitants", "are", "transported", "to", "Bronze", "Age", "times", "to", "become", "the", "world", "'s", "first", "superpower", "."], "labels": ["B-writer", "I-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, literary genre, magazine, event, organization, location, country, poem, person, award, writer and O.\nSentence: S. M. Stirling wrote the Island in the Sea of Time trilogy , in which Nantucket Island and all its modern inhabitants are transported to Bronze Age times to become the world 's first superpower .", "prompt_labels": "S.(B-writer) M.(I-writer) Stirling(I-writer) wrote(O) the(O) Island(B-book) in(I-book) the(I-book) Sea(I-book) of(I-book) Time(I-book) trilogy(O) ,(O) in(O) which(O) Nantucket(B-location) Island(I-location) and(O) all(O) its(O) modern(O) inhabitants(O) are(O) transported(O) to(O) Bronze(B-event) Age(I-event) times(O) to(O) become(O) the(O) world(O) 's(O) first(O) superpower(O) .(O)"}}
{"id": "369", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "location", "event", "award", "person", "organization", "writer", "poem", "literary genre", "country", "magazine"], "instance": {"id": "369", "words": ["It", "was", "during", "the", "1960s", "that", "Warhol", "began", "to", "make", "paintings", "of", "iconic", "American", "objects", "such", "as", "dollar", "bills", ",", "mushroom", "cloud", "s", ",", "electric", "chair", "s", ",", "Campbell", "'s", "Soup", "Cans", ",", "Coca-Cola", "bottles", ",", "celebrities", "such", "as", "Marilyn", "Monroe", ",", "Elvis", "Presley", ",", "Marlon", "Brando", ",", "Troy", "Donahue", ",", "Muhammad", "Ali", ",", "and", "Elizabeth", "Taylor", ",", "as", "well", "as", "newspaper", "headlines", "or", "photographs", "of", "police", "dogs", "attacking", "African-American", "protesters", "during", "the", "Birmingham", "campaign", "in", "the", "civil", "rights", "movement", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, location, event, award, person, organization, writer, poem, literary genre, country, magazine and O.\nSentence: It was during the 1960s that Warhol began to make paintings of iconic American objects such as dollar bills , mushroom cloud s , electric chair s , Campbell 's Soup Cans , Coca-Cola bottles , celebrities such as Marilyn Monroe , Elvis Presley , Marlon Brando , Troy Donahue , Muhammad Ali , and Elizabeth Taylor , as well as newspaper headlines or photographs of police dogs attacking African-American protesters during the Birmingham campaign in the civil rights movement .", "prompt_labels": "It(O) was(O) during(O) the(O) 1960s(O) that(O) Warhol(B-writer) began(O) to(O) make(O) paintings(O) of(O) iconic(O) American(O) objects(O) such(O) as(O) dollar(O) bills(O) ,(O) mushroom(O) cloud(O) s(O) ,(O) electric(O) chair(O) s(O) ,(O) Campbell(O) 's(O) Soup(O) Cans(O) ,(O) Coca-Cola(O) bottles(O) ,(O) celebrities(O) such(O) as(O) Marilyn(B-person) Monroe(I-person) ,(O) Elvis(B-person) Presley(I-person) ,(O) Marlon(B-person) Brando(I-person) ,(O) Troy(B-person) Donahue(I-person) ,(O) Muhammad(B-person) Ali(I-person) ,(O) and(O) Elizabeth(B-person) Taylor(I-person) ,(O) as(O) well(O) as(O) newspaper(O) headlines(O) or(O) photographs(O) of(O) police(O) dogs(O) attacking(O) African-American(O) protesters(O) during(O) the(O) Birmingham(B-event) campaign(I-event) in(O) the(O) civil(B-event) rights(I-event) movement(I-event) .(O)"}}
{"id": "264", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "book", "country", "event", "poem", "magazine", "location", "writer", "award", "person", "organization"], "instance": {"id": "264", "words": ["Ireland", "'s", "1990", "FIFA", "World", "Cup", "Group", "F", "opponents", "in", "Italy", "'", "90", "were", "England", ",", "Egypt", "and", "the", "Netherlands", "."], "labels": ["B-country", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "B-country", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, book, country, event, poem, magazine, location, writer, award, person, organization and O.\nSentence: Ireland 's 1990 FIFA World Cup Group F opponents in Italy ' 90 were England , Egypt and the Netherlands .", "prompt_labels": "Ireland(B-country) 's(O) 1990(B-event) FIFA(I-event) World(I-event) Cup(I-event) Group(I-event) F(I-event) opponents(O) in(O) Italy(B-country) '(O) 90(O) were(O) England(B-country) ,(O) Egypt(B-country) and(O) the(O) Netherlands(B-country) .(O)"}}
{"id": "291", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "writer", "literary genre", "country", "award", "poem", "book", "magazine", "organization", "person", "location"], "instance": {"id": "291", "words": ["The", "operetta", "Candide", "was", "originally", "conceived", "by", "playwright", "Lillian", "Hellman", ",", "as", "a", "play", "with", "incidental", "music", "."], "labels": ["O", "O", "B-book", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, writer, literary genre, country, award, poem, book, magazine, organization, person, location and O.\nSentence: The operetta Candide was originally conceived by playwright Lillian Hellman , as a play with incidental music .", "prompt_labels": "The(O) operetta(O) Candide(B-book) was(O) originally(O) conceived(O) by(O) playwright(O) Lillian(B-writer) Hellman(I-writer) ,(O) as(O) a(O) play(O) with(O) incidental(O) music(O) .(O)"}}
{"id": "73", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "organization", "magazine", "location", "poem", "writer", "country", "event", "literary genre", "book", "person"], "instance": {"id": "73", "words": ["Orwell", "wrote", "the", "book", "between", "November", "1943", "and", "February", "1944", ",", "when", "the", "UK", "was", "in", "its", "Allies", "of", "World", "War", "II", "with", "the", "Soviet", "Union", "against", "Nazi", "Germany", ",", "and", "the", "British", "people", "and", "intelligentsia", "held", "Stalin", "in", "high", "esteem", ",", "a", "phenomenon", "Orwell", "hated", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-country", "I-country", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, magazine, location, poem, writer, country, event, literary genre, book, person and O.\nSentence: Orwell wrote the book between November 1943 and February 1944 , when the UK was in its Allies of World War II with the Soviet Union against Nazi Germany , and the British people and intelligentsia held Stalin in high esteem , a phenomenon Orwell hated .", "prompt_labels": "Orwell(B-writer) wrote(O) the(O) book(O) between(O) November(O) 1943(O) and(O) February(O) 1944(O) ,(O) when(O) the(O) UK(B-country) was(O) in(O) its(O) Allies(B-organization) of(I-organization) World(I-organization) War(I-organization) II(I-organization) with(O) the(O) Soviet(B-country) Union(I-country) against(O) Nazi(B-country) Germany(I-country) ,(O) and(O) the(O) British(O) people(O) and(O) intelligentsia(O) held(O) Stalin(B-person) in(O) high(O) esteem(O) ,(O) a(O) phenomenon(O) Orwell(B-writer) hated(O) .(O)"}}
{"id": "182", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "magazine", "writer", "country", "event", "location", "organization", "poem", "person", "book", "award"], "instance": {"id": "182", "words": ["Asimov", "notes", "in", "his", "introduction", "to", "the", "short", "story", "collection", "The", "Complete", "Robot", "(", "1982", ")", "that", "he", "was", "largely", "inspired", "by", "the", "almost", "relentless", "tendency", "of", "robots", "up", "to", "that", "time", "to", "fall", "consistently", "into", "a", "Frankenstein", "plot", "in", "which", "they", "destroyed", "their", "creators", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, magazine, writer, country, event, location, organization, poem, person, book, award and O.\nSentence: Asimov notes in his introduction to the short story collection The Complete Robot ( 1982 ) that he was largely inspired by the almost relentless tendency of robots up to that time to fall consistently into a Frankenstein plot in which they destroyed their creators .", "prompt_labels": "Asimov(B-writer) notes(O) in(O) his(O) introduction(O) to(O) the(O) short(B-literary genre) story(I-literary genre) collection(O) The(B-book) Complete(I-book) Robot(I-book) ((O) 1982(O) )(O) that(O) he(O) was(O) largely(O) inspired(O) by(O) the(O) almost(O) relentless(O) tendency(O) of(O) robots(O) up(O) to(O) that(O) time(O) to(O) fall(O) consistently(O) into(O) a(O) Frankenstein(B-book) plot(O) in(O) which(O) they(O) destroyed(O) their(O) creators(O) .(O)"}}
{"id": "364", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "book", "writer", "person", "award", "location", "country", "magazine", "organization", "literary genre", "poem"], "instance": {"id": "364", "words": ["He", "knew", "patristic", "literature", ",", "as", "well", "as", "Pliny", "the", "Elder", ",", "Virgil", ",", "Lucretius", ",", "Ovid", ",", "Horace", "and", "other", "classical", "writers", "."], "labels": ["O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "O", "B-writer", "O", "B-writer", "O", "B-writer", "O", "O", "B-literary genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, book, writer, person, award, location, country, magazine, organization, literary genre, poem and O.\nSentence: He knew patristic literature , as well as Pliny the Elder , Virgil , Lucretius , Ovid , Horace and other classical writers .", "prompt_labels": "He(O) knew(O) patristic(B-literary genre) literature(I-literary genre) ,(O) as(O) well(O) as(O) Pliny(B-writer) the(I-writer) Elder(I-writer) ,(O) Virgil(B-writer) ,(O) Lucretius(B-writer) ,(O) Ovid(B-writer) ,(O) Horace(B-writer) and(O) other(O) classical(B-literary genre) writers(O) .(O)"}}
{"id": "193", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "poem", "event", "country", "book", "writer", "organization", "literary genre", "location", "award", "magazine"], "instance": {"id": "193", "words": ["In", "view", "of", "the", "success", "of", "her", "novels", ",", "particularly", "Jane", "Eyre", ",", "Brontë", "was", "persuaded", "by", "her", "publisher", "to", "make", "occasional", "visits", "to", "London", ",", "where", "she", "revealed", "her", "TRUE", "identity", "and", "began", "to", "move", "in", "more", "exalted", "social", "circles", ",", "becoming", "friends", "with", "Harriet", "Martineau", "and", "Elizabeth", "Gaskell", ",", "and", "acquainted", "with", "William", "Makepeace", "Thackeray", "and", "G.H.", "Lewes", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "B-book", "I-book", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, poem, event, country, book, writer, organization, literary genre, location, award, magazine and O.\nSentence: In view of the success of her novels , particularly Jane Eyre , Brontë was persuaded by her publisher to make occasional visits to London , where she revealed her TRUE identity and began to move in more exalted social circles , becoming friends with Harriet Martineau and Elizabeth Gaskell , and acquainted with William Makepeace Thackeray and G.H. Lewes .", "prompt_labels": "In(O) view(O) of(O) the(O) success(O) of(O) her(O) novels(B-literary genre) ,(O) particularly(O) Jane(B-book) Eyre(I-book) ,(O) Brontë(B-writer) was(O) persuaded(O) by(O) her(O) publisher(O) to(O) make(O) occasional(O) visits(O) to(O) London(B-location) ,(O) where(O) she(O) revealed(O) her(O) TRUE(O) identity(O) and(O) began(O) to(O) move(O) in(O) more(O) exalted(O) social(O) circles(O) ,(O) becoming(O) friends(O) with(O) Harriet(B-writer) Martineau(I-writer) and(O) Elizabeth(B-writer) Gaskell(I-writer) ,(O) and(O) acquainted(O) with(O) William(B-writer) Makepeace(I-writer) Thackeray(I-writer) and(O) G.H.(B-writer) Lewes(I-writer) .(O)"}}
{"id": "261", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "location", "organization", "book", "person", "literary genre", "poem", "award", "writer", "event", "country"], "instance": {"id": "261", "words": ["The", "novel", "was", "called", "The", "Monogram", "Murders", ",", "and", "was", "set", "in", "the", "late", "1920s", ",", "placing", "it", "chronologically", "between", "The", "Mystery", "of", "the", "Blue", "Train", "and", "Peril", "at", "End", "House", "."], "labels": ["O", "B-literary genre", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, organization, book, person, literary genre, poem, award, writer, event, country and O.\nSentence: The novel was called The Monogram Murders , and was set in the late 1920s , placing it chronologically between The Mystery of the Blue Train and Peril at End House .", "prompt_labels": "The(O) novel(B-literary genre) was(O) called(O) The(B-book) Monogram(I-book) Murders(I-book) ,(O) and(O) was(O) set(O) in(O) the(O) late(O) 1920s(O) ,(O) placing(O) it(O) chronologically(O) between(O) The(B-book) Mystery(I-book) of(I-book) the(I-book) Blue(I-book) Train(I-book) and(O) Peril(B-book) at(I-book) End(I-book) House(I-book) .(O)"}}
{"id": "338", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "country", "literary genre", "magazine", "writer", "award", "location", "event", "book", "poem", "organization"], "instance": {"id": "338", "words": ["He", "was", "awarded", "the", "Pulitzer", "Prize", "for", "poetry", "in", "1954", "for", "his", "book", "The", "Waking", ",", "and", "he", "won", "the", "annual", "National", "Book", "Award", "for", "Poetry", "twice", ",", "in", "1959", "for", "Words", "for", "the", "Wind"], "labels": ["O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-poem", "I-poem", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "I-poem"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, literary genre, magazine, writer, award, location, event, book, poem, organization and O.\nSentence: He was awarded the Pulitzer Prize for poetry in 1954 for his book The Waking , and he won the annual National Book Award for Poetry twice , in 1959 for Words for the Wind", "prompt_labels": "He(O) was(O) awarded(O) the(O) Pulitzer(B-award) Prize(I-award) for(I-award) poetry(I-award) in(O) 1954(O) for(O) his(O) book(O) The(B-poem) Waking(I-poem) ,(O) and(O) he(O) won(O) the(O) annual(O) National(B-award) Book(I-award) Award(I-award) for(I-award) Poetry(I-award) twice(O) ,(O) in(O) 1959(O) for(O) Words(B-poem) for(I-poem) the(I-poem) Wind(I-poem)"}}
{"id": "320", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "country", "writer", "award", "book", "magazine", "organization", "location", "literary genre", "person", "poem"], "instance": {"id": "320", "words": ["As", "a", "distinguished", "writer", "sympathizing", "with", "the", "cause", "of", "communism", ",", "he", "was", "invited", "to", "speak", "at", "Maxim", "Gorky", "'", "s", "funeral", "and", "to", "tour", "the", "Soviet", "Union", "as", "a", "guest", "of", "the", "Soviet", "Union", "of", "Writers", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, writer, award, book, magazine, organization, location, literary genre, person, poem and O.\nSentence: As a distinguished writer sympathizing with the cause of communism , he was invited to speak at Maxim Gorky ' s funeral and to tour the Soviet Union as a guest of the Soviet Union of Writers .", "prompt_labels": "As(O) a(O) distinguished(O) writer(O) sympathizing(O) with(O) the(O) cause(O) of(O) communism(O) ,(O) he(O) was(O) invited(O) to(O) speak(O) at(O) Maxim(B-event) Gorky(I-event) '(I-event) s(I-event) funeral(I-event) and(O) to(O) tour(O) the(O) Soviet(B-country) Union(I-country) as(O) a(O) guest(O) of(O) the(O) Soviet(B-event) Union(I-event) of(I-event) Writers(I-event) .(O)"}}
{"id": "203", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "event", "writer", "poem", "magazine", "award", "country", "literary genre", "location", "person", "organization"], "instance": {"id": "203", "words": ["The", "provinces", "ceded", "to", "Augustus", "for", "that", "ten-year", "period", "comprised", "much", "of", "the", "conquered", "Roman", "world", ",", "including", "all", "of", "Hispania", "and", "Gaul", ",", "Syria", ",", "Cilicia", ",", "Cyprus", ",", "and", "Egypt", "."], "labels": ["O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, event, writer, poem, magazine, award, country, literary genre, location, person, organization and O.\nSentence: The provinces ceded to Augustus for that ten-year period comprised much of the conquered Roman world , including all of Hispania and Gaul , Syria , Cilicia , Cyprus , and Egypt .", "prompt_labels": "The(O) provinces(O) ceded(O) to(O) Augustus(B-person) for(O) that(O) ten-year(O) period(O) comprised(O) much(O) of(O) the(O) conquered(O) Roman(O) world(O) ,(O) including(O) all(O) of(O) Hispania(B-location) and(O) Gaul(B-location) ,(O) Syria(B-location) ,(O) Cilicia(B-location) ,(O) Cyprus(B-location) ,(O) and(O) Egypt(B-location) .(O)"}}
{"id": "121", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "writer", "location", "country", "organization", "award", "book", "magazine", "literary genre", "poem", "event"], "instance": {"id": "121", "words": ["Another", "historical", "novel", "by", "Graves", ",", "Count", "Belisarius", "(", "1938", ")", ",", "recounts", "the", "career", "of", "the", "Byzantine", "Empire", "general", "Belisarius", "."], "labels": ["O", "O", "B-literary genre", "O", "B-writer", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "B-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, location, country, organization, award, book, magazine, literary genre, poem, event and O.\nSentence: Another historical novel by Graves , Count Belisarius ( 1938 ) , recounts the career of the Byzantine Empire general Belisarius .", "prompt_labels": "Another(O) historical(O) novel(B-literary genre) by(O) Graves(B-writer) ,(O) Count(B-book) Belisarius(I-book) ((O) 1938(O) )(O) ,(O) recounts(O) the(O) career(O) of(O) the(O) Byzantine(B-country) Empire(I-country) general(O) Belisarius(B-person) .(O)"}}
{"id": "257", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "event", "person", "writer", "poem", "organization", "magazine", "location", "book", "award", "literary genre"], "instance": {"id": "257", "words": ["Bal", "Thackeray", "in", "India", "Today", ",", "Shiv", "Sena", "leader", "of", "Bombay", ",", "15", "June", "1984", "."], "labels": ["B-person", "I-person", "O", "B-magazine", "I-magazine", "O", "B-organization", "I-organization", "O", "O", "B-location", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, person, writer, poem, organization, magazine, location, book, award, literary genre and O.\nSentence: Bal Thackeray in India Today , Shiv Sena leader of Bombay , 15 June 1984 .", "prompt_labels": "Bal(B-person) Thackeray(I-person) in(O) India(B-magazine) Today(I-magazine) ,(O) Shiv(B-organization) Sena(I-organization) leader(O) of(O) Bombay(B-location) ,(O) 15(O) June(O) 1984(O) .(O)"}}
{"id": "175", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "country", "award", "event", "organization", "literary genre", "writer", "location", "person", "poem", "book"], "instance": {"id": "175", "words": ["On", "June", "17", ",", "1915", ",", "shortly", "after", "submitting", "his", "patent", "application", "for", "the", "doll", "'s", "design", ",", "Johnny", "Gruelle", "applied", "for", "a", "registered", "trademark", "for", "the", "Raggedy", "Ann", "name", ",", "which", "he", "created", "by", "combining", "words", "from", "two", "of", "James", "Whitcomb", "Riley", "poems", ",", "The", "Raggedy", "Man", "and", "Little", "Orphant", "Annie", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "B-literary genre", "O", "B-poem", "I-poem", "I-poem", "O", "B-poem", "I-poem", "I-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, country, award, event, organization, literary genre, writer, location, person, poem, book and O.\nSentence: On June 17 , 1915 , shortly after submitting his patent application for the doll 's design , Johnny Gruelle applied for a registered trademark for the Raggedy Ann name , which he created by combining words from two of James Whitcomb Riley poems , The Raggedy Man and Little Orphant Annie .", "prompt_labels": "On(O) June(O) 17(O) ,(O) 1915(O) ,(O) shortly(O) after(O) submitting(O) his(O) patent(O) application(O) for(O) the(O) doll(O) 's(O) design(O) ,(O) Johnny(B-person) Gruelle(I-person) applied(O) for(O) a(O) registered(O) trademark(O) for(O) the(O) Raggedy(O) Ann(O) name(O) ,(O) which(O) he(O) created(O) by(O) combining(O) words(O) from(O) two(O) of(O) James(B-writer) Whitcomb(I-writer) Riley(I-writer) poems(B-literary genre) ,(O) The(B-poem) Raggedy(I-poem) Man(I-poem) and(O) Little(B-poem) Orphant(I-poem) Annie(I-poem) .(O)"}}
{"id": "92", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "award", "literary genre", "country", "location", "organization", "writer", "magazine", "poem", "person", "book"], "instance": {"id": "92", "words": ["Before", "writing", "Dracula", ",", "Stoker", "met", "Ármin", "Vámbéry", ",", "a", "Hungarian-Jewish", "writer", "and", "traveller", "(", "born", "in", "Szent-György", ",", "Kingdom", "of", "Hungary", "now", "Svätý", "Jur", ",", "Slovakia", ")", "."], "labels": ["O", "O", "B-book", "O", "B-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-country", "I-country", "I-country", "O", "B-location", "I-location", "O", "B-country", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, literary genre, country, location, organization, writer, magazine, poem, person, book and O.\nSentence: Before writing Dracula , Stoker met Ármin Vámbéry , a Hungarian-Jewish writer and traveller ( born in Szent-György , Kingdom of Hungary now Svätý Jur , Slovakia ) .", "prompt_labels": "Before(O) writing(O) Dracula(B-book) ,(O) Stoker(B-writer) met(O) Ármin(B-writer) Vámbéry(I-writer) ,(O) a(O) Hungarian-Jewish(O) writer(O) and(O) traveller(O) ((O) born(O) in(O) Szent-György(B-location) ,(O) Kingdom(B-country) of(I-country) Hungary(I-country) now(O) Svätý(B-location) Jur(I-location) ,(O) Slovakia(B-country) )(O) .(O)"}}
{"id": "132", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "writer", "poem", "location", "organization", "literary genre", "book", "award", "country", "magazine", "event"], "instance": {"id": "132", "words": ["Some", "of", "these", "friends", "include", ":", "David", "Amram", ",", "Bob", "Kaufman", ";", "Diane", "di", "Prima", ";", "Jim", "Cohn", ";", "poets", "associated", "with", "the", "Black", "Mountain", "College", "such", "as", "Charles", "Olson", ",", "Robert", "Creeley", ",", "and", "Denise", "Levertov", ";", "poets", "associated", "with", "the", "New", "York", "School", "such", "as", "Frank", "O", "'Hara", "and", "Kenneth", "Koch", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, poem, location, organization, literary genre, book, award, country, magazine, event and O.\nSentence: Some of these friends include : David Amram , Bob Kaufman ; Diane di Prima ; Jim Cohn ; poets associated with the Black Mountain College such as Charles Olson , Robert Creeley , and Denise Levertov ; poets associated with the New York School such as Frank O 'Hara and Kenneth Koch .", "prompt_labels": "Some(O) of(O) these(O) friends(O) include(O) :(O) David(B-writer) Amram(I-writer) ,(O) Bob(B-writer) Kaufman(I-writer) ;(O) Diane(B-writer) di(I-writer) Prima(I-writer) ;(O) Jim(B-writer) Cohn(I-writer) ;(O) poets(O) associated(O) with(O) the(O) Black(B-organization) Mountain(I-organization) College(I-organization) such(O) as(O) Charles(B-writer) Olson(I-writer) ,(O) Robert(B-writer) Creeley(I-writer) ,(O) and(O) Denise(B-writer) Levertov(I-writer) ;(O) poets(O) associated(O) with(O) the(O) New(B-organization) York(I-organization) School(I-organization) such(O) as(O) Frank(B-writer) O(I-writer) 'Hara(I-writer) and(O) Kenneth(B-writer) Koch(I-writer) .(O)"}}
{"id": "59", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "writer", "poem", "event", "magazine", "person", "book", "award", "location", "literary genre", "organization"], "instance": {"id": "59", "words": ["He", "received", "an", "Academy", "Awards", "nomination", "for", "Academy", "Award", "for", "Best", "Supporting", "Actor", "for", "1987", "'s", "Broadcast", "News", "and", "was", "widely", "praised", "for", "his", "performance", "in", "the", "2011", "film", "Drive", "."], "labels": ["O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, writer, poem, event, magazine, person, book, award, location, literary genre, organization and O.\nSentence: He received an Academy Awards nomination for Academy Award for Best Supporting Actor for 1987 's Broadcast News and was widely praised for his performance in the 2011 film Drive .", "prompt_labels": "He(O) received(O) an(O) Academy(B-award) Awards(I-award) nomination(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actor(I-award) for(O) 1987(O) 's(O) Broadcast(O) News(O) and(O) was(O) widely(O) praised(O) for(O) his(O) performance(O) in(O) the(O) 2011(O) film(O) Drive(O) .(O)"}}
{"id": "396", "dataset": "crossner_literature", "split": "dev", "label_list": ["poem", "writer", "location", "organization", "event", "award", "literary genre", "magazine", "country", "book", "person"], "instance": {"id": "396", "words": ["From", "1949", "to", "1953", ",", "Clement", "'s", "first", "three", "novels", "were", "two-", ",", "three-", ",", "and", "four-part", "Astounding", "serials", "under", "Campbell", ":", "Needle", "(", "Doubleday", ",", "1950", ")", ",", "Iceworld", "(", "Gnome", "Press", ",", "1953", ")", ",", "and", "Mission", "of", "Gravity", "(", "1954", ")", ",", "his", "best-known", "novel", ",", "published", "by", "Doubleday", "'s", "Science", "Fiction", "Book", "Club", "(", "established", "1953", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "B-book", "O", "O", "B-writer", "O", "B-book", "O", "B-organization", "O", "O", "O", "O", "B-book", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "B-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, writer, location, organization, event, award, literary genre, magazine, country, book, person and O.\nSentence: From 1949 to 1953 , Clement 's first three novels were two- , three- , and four-part Astounding serials under Campbell : Needle ( Doubleday , 1950 ) , Iceworld ( Gnome Press , 1953 ) , and Mission of Gravity ( 1954 ) , his best-known novel , published by Doubleday 's Science Fiction Book Club ( established 1953 ) .", "prompt_labels": "From(O) 1949(O) to(O) 1953(O) ,(O) Clement(B-writer) 's(O) first(O) three(O) novels(B-literary genre) were(O) two-(O) ,(O) three-(O) ,(O) and(O) four-part(O) Astounding(B-book) serials(O) under(O) Campbell(B-writer) :(O) Needle(B-book) ((O) Doubleday(B-organization) ,(O) 1950(O) )(O) ,(O) Iceworld(B-book) ((O) Gnome(B-organization) Press(I-organization) ,(O) 1953(O) )(O) ,(O) and(O) Mission(B-book) of(I-book) Gravity(I-book) ((O) 1954(O) )(O) ,(O) his(O) best-known(O) novel(B-literary genre) ,(O) published(O) by(O) Doubleday(B-organization) 's(O) Science(B-organization) Fiction(I-organization) Book(I-organization) Club(I-organization) ((O) established(O) 1953(O) )(O) .(O)"}}
{"id": "27", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "literary genre", "location", "magazine", "event", "organization", "country", "writer", "book", "poem", "person"], "instance": {"id": "27", "words": ["It", "was", "a", "finalist", "for", "the", "National", "Book", "Award", "for", "Fiction", "!", "--", "National", "Book", "Awards", "were", "called", "American", "for", "several", "years", "from", "1980", ",", "but", "must", "not", "be", "confused", "with", "American", "Book", "Awards", "--", "in", "1979", "(", "which", "ultimately", "went", "to", "Tim", "O", "'Brien", "for", "Going", "After", "Cacciato", ")", "in", "the", "film", "as", "an", "official", "in", "one", "of", "Garp", "'s", "high", "school", "wrestling", "matches", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, literary genre, location, magazine, event, organization, country, writer, book, poem, person and O.\nSentence: It was a finalist for the National Book Award for Fiction ! -- National Book Awards were called American for several years from 1980 , but must not be confused with American Book Awards -- in 1979 ( which ultimately went to Tim O 'Brien for Going After Cacciato ) in the film as an official in one of Garp 's high school wrestling matches .", "prompt_labels": "It(O) was(O) a(O) finalist(O) for(O) the(O) National(B-award) Book(I-award) Award(I-award) for(I-award) Fiction(I-award) !(O) --(O) National(B-award) Book(I-award) Awards(I-award) were(O) called(O) American(O) for(O) several(O) years(O) from(O) 1980(O) ,(O) but(O) must(O) not(O) be(O) confused(O) with(O) American(B-award) Book(I-award) Awards(I-award) --(O) in(O) 1979(O) ((O) which(O) ultimately(O) went(O) to(O) Tim(B-writer) O(I-writer) 'Brien(I-writer) for(O) Going(B-book) After(I-book) Cacciato(I-book) )(O) in(O) the(O) film(O) as(O) an(O) official(O) in(O) one(O) of(O) Garp(B-person) 's(O) high(O) school(O) wrestling(O) matches(O) .(O)"}}
{"id": "78", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "event", "person", "organization", "award", "magazine", "location", "book", "writer", "poem", "literary genre"], "instance": {"id": "78", "words": ["She", "especially", "enjoyed", "the", "St.", "Nicholas", "Magazine", "(", "which", "carried", "her", "first", "published", "stories", ")", ",", "the", "works", "of", "Beatrix", "Potter", ",", "and", "the", "novels", "of", "Gene", "Stratton-Porter", ",", "and", "in", "her", "teen", "years", ",", "Herman", "Melville", ",", "Joseph", "Conrad", "and", "Robert", "Louis", "Stevenson", "."], "labels": ["O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "B-literary genre", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, person, organization, award, magazine, location, book, writer, poem, literary genre and O.\nSentence: She especially enjoyed the St. Nicholas Magazine ( which carried her first published stories ) , the works of Beatrix Potter , and the novels of Gene Stratton-Porter , and in her teen years , Herman Melville , Joseph Conrad and Robert Louis Stevenson .", "prompt_labels": "She(O) especially(O) enjoyed(O) the(O) St.(B-magazine) Nicholas(I-magazine) Magazine(I-magazine) ((O) which(O) carried(O) her(O) first(O) published(O) stories(O) )(O) ,(O) the(O) works(O) of(O) Beatrix(B-writer) Potter(I-writer) ,(O) and(O) the(O) novels(B-literary genre) of(O) Gene(B-writer) Stratton-Porter(I-writer) ,(O) and(O) in(O) her(O) teen(O) years(O) ,(O) Herman(B-writer) Melville(I-writer) ,(O) Joseph(B-writer) Conrad(I-writer) and(O) Robert(B-writer) Louis(I-writer) Stevenson(I-writer) .(O)"}}
{"id": "268", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "event", "book", "country", "magazine", "writer", "award", "poem", "person", "location", "literary genre"], "instance": {"id": "268", "words": ["His", "books", "include", "those", "of", "Timeline", "191", "(", "a.k.a.", "Southern", "Victory", ",", "also", "known", "as", "TL-191", ")", ",", "in", "which", ",", "while", "the", "Confederate", "States", "of", "America", "won", "the", "American", "Civil", "War", ",", "the", "Union", "and", "German", "Empire", "defeat", "the", "Entente", "Powers", "in", "the", "two", "Great", "War", "s", "of", "the", "1910s", "and", "1940s", "(", "with", "a", "Nazi-esque", "Confederate", "government", "attempting", "to", "exterminate", "its", "Black", "population", ")", ",", "and", "the", "Worldwar", "series", ",", "in", "which", "aliens", "invaded", "Earth", "during", "World", "War", "II", "."], "labels": ["O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "I-country", "I-country", "O", "O", "B-event", "I-event", "I-event", "O", "B-organization", "I-organization", "O", "B-country", "I-country", "O", "O", "B-organization", "I-organization", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, book, country, magazine, writer, award, poem, person, location, literary genre and O.\nSentence: His books include those of Timeline 191 ( a.k.a. Southern Victory , also known as TL-191 ) , in which , while the Confederate States of America won the American Civil War , the Union and German Empire defeat the Entente Powers in the two Great War s of the 1910s and 1940s ( with a Nazi-esque Confederate government attempting to exterminate its Black population ) , and the Worldwar series , in which aliens invaded Earth during World War II .", "prompt_labels": "His(O) books(O) include(O) those(O) of(O) Timeline(B-book) 191(I-book) ((O) a.k.a.(O) Southern(B-book) Victory(I-book) ,(O) also(O) known(O) as(O) TL-191(B-book) )(O) ,(O) in(O) which(O) ,(O) while(O) the(O) Confederate(B-country) States(I-country) of(I-country) America(I-country) won(O) the(O) American(B-event) Civil(I-event) War(I-event) ,(O) the(B-organization) Union(I-organization) and(O) German(B-country) Empire(I-country) defeat(O) the(O) Entente(B-organization) Powers(I-organization) in(O) the(O) two(O) Great(B-event) War(I-event) s(O) of(O) the(O) 1910s(O) and(O) 1940s(O) ((O) with(O) a(O) Nazi-esque(B-organization) Confederate(I-organization) government(I-organization) attempting(O) to(O) exterminate(O) its(O) Black(O) population(O) )(O) ,(O) and(O) the(O) Worldwar(O) series(O) ,(O) in(O) which(O) aliens(O) invaded(O) Earth(O) during(O) World(B-event) War(I-event) II(I-event) .(O)"}}
{"id": "392", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "poem", "writer", "literary genre", "book", "event", "award", "organization", "country", "person", "magazine"], "instance": {"id": "392", "words": ["Baron", "Cohen", "'s", "other", "work", "includes", "voicing", "King", "Julien", "XIII", "in", "the", "Madagascar", "film", "series", "(", "2005-2012", ")", "and", "appearing", "in", "films", "such", "as", "Talladega", "Nights", ":", "The", "Ballad", "of", "Ricky", "Bobby", "(", "2006", ")", ",", "Sweeney", "Todd", ":", "The", "Demon", "Barber", "of", "Fleet", "Street", "(", "2007", ")", ",", "Hugo", "(", "2011", ")", ",", "and", "Les", "Misérables", "(", "2012", ")", "."], "labels": ["B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, poem, writer, literary genre, book, event, award, organization, country, person, magazine and O.\nSentence: Baron Cohen 's other work includes voicing King Julien XIII in the Madagascar film series ( 2005-2012 ) and appearing in films such as Talladega Nights : The Ballad of Ricky Bobby ( 2006 ) , Sweeney Todd : The Demon Barber of Fleet Street ( 2007 ) , Hugo ( 2011 ) , and Les Misérables ( 2012 ) .", "prompt_labels": "Baron(B-person) Cohen(I-person) 's(O) other(O) work(O) includes(O) voicing(O) King(O) Julien(O) XIII(O) in(O) the(O) Madagascar(B-country) film(O) series(O) ((O) 2005-2012(O) )(O) and(O) appearing(O) in(O) films(O) such(O) as(O) Talladega(O) Nights(O) :(O) The(O) Ballad(O) of(O) Ricky(O) Bobby(O) ((O) 2006(O) )(O) ,(O) Sweeney(O) Todd(O) :(O) The(O) Demon(O) Barber(O) of(O) Fleet(O) Street(O) ((O) 2007(O) )(O) ,(O) Hugo(B-person) ((O) 2011(O) )(O) ,(O) and(O) Les(B-book) Misérables(I-book) ((O) 2012(O) )(O) .(O)"}}
{"id": "164", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "book", "writer", "magazine", "poem", "literary genre", "event", "award", "location", "organization", "person"], "instance": {"id": "164", "words": ["In", "2005", ",", "Arkham", "House", "was", "awarded", "the", "World", "Fantasy", "Award", "for", "Small", "Press", "Achievements", "-", "the", "trophy", "at", "that", "time", "was", "a", "bust", "of", "H.", "P.", "Lovecraft", "."], "labels": ["O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, book, writer, magazine, poem, literary genre, event, award, location, organization, person and O.\nSentence: In 2005 , Arkham House was awarded the World Fantasy Award for Small Press Achievements - the trophy at that time was a bust of H. P. Lovecraft .", "prompt_labels": "In(O) 2005(O) ,(O) Arkham(B-organization) House(I-organization) was(O) awarded(O) the(O) World(B-award) Fantasy(I-award) Award(I-award) for(I-award) Small(I-award) Press(I-award) Achievements(I-award) -(O) the(O) trophy(O) at(O) that(O) time(O) was(O) a(O) bust(O) of(O) H.(B-writer) P.(I-writer) Lovecraft(I-writer) .(O)"}}
{"id": "111", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "country", "magazine", "location", "book", "organization", "literary genre", "person", "award", "poem", "event"], "instance": {"id": "111", "words": ["The", "three", "films", "garnered", "prestigious", "international", "awards", ",", "including", "the", "Golden", "Lion", "for", "Best", "Film", "at", "the", "Venice", "Film", "Festival", "and", "the", "Silver", "Bear", "for", "Best", "Director", "at", "the", "44th", "Berlin", "International", "Film", "Festival", ",", "in", "addition", "to", "three", "Academy", "Awards", "nominations", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, country, magazine, location, book, organization, literary genre, person, award, poem, event and O.\nSentence: The three films garnered prestigious international awards , including the Golden Lion for Best Film at the Venice Film Festival and the Silver Bear for Best Director at the 44th Berlin International Film Festival , in addition to three Academy Awards nominations .", "prompt_labels": "The(O) three(O) films(O) garnered(O) prestigious(O) international(O) awards(O) ,(O) including(O) the(O) Golden(B-award) Lion(I-award) for(O) Best(O) Film(O) at(O) the(O) Venice(B-event) Film(I-event) Festival(I-event) and(O) the(O) Silver(B-award) Bear(I-award) for(I-award) Best(I-award) Director(I-award) at(O) the(O) 44th(B-event) Berlin(I-event) International(I-event) Film(I-event) Festival(I-event) ,(O) in(O) addition(O) to(O) three(O) Academy(B-award) Awards(I-award) nominations(O) .(O)"}}
{"id": "387", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "book", "organization", "event", "person", "poem", "writer", "award", "literary genre", "country", "location"], "instance": {"id": "387", "words": ["The", "dramatist", ",", "author", "and", "philosopher", "Voltaire", "created", ",", "with", "the", "support", "of", "the", "Académie", "française", ",", "a", "twelve-volume", "annotated", "set", "of", "Corneille", "'s", "dramatic", "works", ",", "the", "Commentaires", "sur", "Corneille", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, book, organization, event, person, poem, writer, award, literary genre, country, location and O.\nSentence: The dramatist , author and philosopher Voltaire created , with the support of the Académie française , a twelve-volume annotated set of Corneille 's dramatic works , the Commentaires sur Corneille .", "prompt_labels": "The(O) dramatist(O) ,(O) author(O) and(O) philosopher(O) Voltaire(B-writer) created(O) ,(O) with(O) the(O) support(O) of(O) the(O) Académie(B-organization) française(I-organization) ,(O) a(O) twelve-volume(O) annotated(O) set(O) of(O) Corneille(B-writer) 's(O) dramatic(O) works(O) ,(O) the(O) Commentaires(B-book) sur(I-book) Corneille(I-book) .(O)"}}
{"id": "192", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "location", "organization", "book", "magazine", "writer", "poem", "award", "event", "literary genre", "person"], "instance": {"id": "192", "words": ["On", "the", "day", "of", "the", "preview", ",", "however", ",", "he", "was", "expelled", "from", "the", "Surrealist", "Group", "by", "André", "Breton", ",", "who", "ordered", "the", "poet", "Paul", "Éluard", "to", "take", "down", "his", "pictures", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, organization, book, magazine, writer, poem, award, event, literary genre, person and O.\nSentence: On the day of the preview , however , he was expelled from the Surrealist Group by André Breton , who ordered the poet Paul Éluard to take down his pictures .", "prompt_labels": "On(O) the(O) day(O) of(O) the(O) preview(O) ,(O) however(O) ,(O) he(O) was(O) expelled(O) from(O) the(O) Surrealist(B-organization) Group(I-organization) by(O) André(B-writer) Breton(I-writer) ,(O) who(O) ordered(O) the(O) poet(O) Paul(B-writer) Éluard(I-writer) to(O) take(O) down(O) his(O) pictures(O) .(O)"}}
{"id": "336", "dataset": "crossner_literature", "split": "dev", "label_list": ["poem", "location", "organization", "event", "magazine", "book", "country", "person", "literary genre", "award", "writer"], "instance": {"id": "336", "words": ["In", "1909", ",", "Strindberg", "thought", "he", "might", "get", "the", "Nobel", "Prize", "in", "Literature", ",", "but", "instead", "lost", "to", "Selma", "Lagerlöf", ",", "the", "first", "woman", "and", "first", "Swede", "to", "win", "the", "award", "."], "labels": ["O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, location, organization, event, magazine, book, country, person, literary genre, award, writer and O.\nSentence: In 1909 , Strindberg thought he might get the Nobel Prize in Literature , but instead lost to Selma Lagerlöf , the first woman and first Swede to win the award .", "prompt_labels": "In(O) 1909(O) ,(O) Strindberg(B-writer) thought(O) he(O) might(O) get(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) ,(O) but(O) instead(O) lost(O) to(O) Selma(B-writer) Lagerlöf(I-writer) ,(O) the(O) first(O) woman(O) and(O) first(O) Swede(O) to(O) win(O) the(O) award(O) .(O)"}}
{"id": "8", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "writer", "award", "magazine", "organization", "literary genre", "location", "country", "book", "event", "poem"], "instance": {"id": "8", "words": ["In", "1829", "he", "was", "publicly", "crowned", "with", "laurel", "as", "the", "king", "of", "Nordic", "countries", "poetry", "and", "the", "Scandinavian", "King", "of", "Song", "(", "by", "Bishop", "Esaias", "Tegnér", ",", "who", "would", "be", "his", "Swedish", "parallel", ")", "in", "the", "cathedral", "of", "Lund", ",", "Sweden", ",", "based", "on", "a", "vast", "production", "of", "poetry", ",", "theatre", "plays", "and", "prose", ",", "inspired", "by", "Johann", "Wolfgang", "von", "Goethe", ",", "Gottlieb", "Fichte", ",", "and", "Friedrich", "von", "Schelling", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "B-literary genre", "I-literary genre", "O", "B-literary genre", "O", "O", "O", "B-writer", "I-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, award, magazine, organization, literary genre, location, country, book, event, poem and O.\nSentence: In 1829 he was publicly crowned with laurel as the king of Nordic countries poetry and the Scandinavian King of Song ( by Bishop Esaias Tegnér , who would be his Swedish parallel ) in the cathedral of Lund , Sweden , based on a vast production of poetry , theatre plays and prose , inspired by Johann Wolfgang von Goethe , Gottlieb Fichte , and Friedrich von Schelling .", "prompt_labels": "In(O) 1829(O) he(O) was(O) publicly(O) crowned(O) with(O) laurel(O) as(O) the(O) king(O) of(O) Nordic(B-literary genre) countries(I-literary genre) poetry(I-literary genre) and(O) the(O) Scandinavian(O) King(O) of(O) Song(O) ((O) by(O) Bishop(O) Esaias(B-writer) Tegnér(I-writer) ,(O) who(O) would(O) be(O) his(O) Swedish(O) parallel(O) )(O) in(O) the(O) cathedral(B-location) of(I-location) Lund(I-location) ,(O) Sweden(B-country) ,(O) based(O) on(O) a(O) vast(O) production(O) of(O) poetry(B-literary genre) ,(O) theatre(B-literary genre) plays(I-literary genre) and(O) prose(B-literary genre) ,(O) inspired(O) by(O) Johann(B-writer) Wolfgang(I-writer) von(I-writer) Goethe(I-writer) ,(O) Gottlieb(B-writer) Fichte(I-writer) ,(O) and(O) Friedrich(B-writer) von(I-writer) Schelling(I-writer) .(O)"}}
{"id": "312", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "location", "event", "writer", "literary genre", "award", "magazine", "organization", "poem", "book", "country"], "instance": {"id": "312", "words": ["England", ",", "England", "is", "a", "satirical", "postmodern", "novel", "by", "Julian", "Barnes", ",", "published", "and", "shortlisted", "for", "the", "Booker", "Prize", "in", "1998", "."], "labels": ["B-book", "I-book", "I-book", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, event, writer, literary genre, award, magazine, organization, poem, book, country and O.\nSentence: England , England is a satirical postmodern novel by Julian Barnes , published and shortlisted for the Booker Prize in 1998 .", "prompt_labels": "England(B-book) ,(I-book) England(I-book) is(O) a(O) satirical(B-literary genre) postmodern(I-literary genre) novel(I-literary genre) by(O) Julian(B-writer) Barnes(I-writer) ,(O) published(O) and(O) shortlisted(O) for(O) the(O) Booker(B-award) Prize(I-award) in(O) 1998(O) .(O)"}}
{"id": "398", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "person", "writer", "book", "literary genre", "country", "magazine", "poem", "location", "award", "event"], "instance": {"id": "398", "words": ["He", "taught", "French", "for", "a", "year", "at", "Eton", "College", ",", "where", "Eric", "Blair", "(", "who", "was", "to", "take", "the", "pen", "name", "George", "Orwell", ")", "and", "Steven", "Runciman", "were", "among", "his", "pupils", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, writer, book, literary genre, country, magazine, poem, location, award, event and O.\nSentence: He taught French for a year at Eton College , where Eric Blair ( who was to take the pen name George Orwell ) and Steven Runciman were among his pupils .", "prompt_labels": "He(O) taught(O) French(O) for(O) a(O) year(O) at(O) Eton(B-organization) College(I-organization) ,(O) where(O) Eric(B-writer) Blair(I-writer) ((O) who(O) was(O) to(O) take(O) the(O) pen(O) name(O) George(B-writer) Orwell(I-writer) )(O) and(O) Steven(B-writer) Runciman(I-writer) were(O) among(O) his(O) pupils(O) .(O)"}}
{"id": "243", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "literary genre", "location", "event", "person", "award", "writer", "country", "magazine", "organization", "poem"], "instance": {"id": "243", "words": ["Yet", "another", "source", "for", "Defoe", "'s", "novel", "may", "have", "been", "the", "Robert", "Knox", "account", "of", "his", "abduction", "by", "the", "King", "of", "Sri", "Lanka", "Rajasinha", "II", "of", "Kandy", "in", "1659", "in", "An", "Historical", "Relation", "of", "the", "Island", "Ceylon", ".Knox", ",", "Robert", "(", "1911", ")", "."], "labels": ["O", "O", "O", "O", "B-writer", "O", "B-literary genre", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "B-person", "I-person", "I-person", "I-person", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "B-person", "I-person", "I-person", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, literary genre, location, event, person, award, writer, country, magazine, organization, poem and O.\nSentence: Yet another source for Defoe 's novel may have been the Robert Knox account of his abduction by the King of Sri Lanka Rajasinha II of Kandy in 1659 in An Historical Relation of the Island Ceylon .Knox , Robert ( 1911 ) .", "prompt_labels": "Yet(O) another(O) source(O) for(O) Defoe(B-writer) 's(O) novel(B-literary genre) may(O) have(O) been(O) the(O) Robert(B-person) Knox(I-person) account(O) of(O) his(O) abduction(O) by(O) the(O) King(O) of(O) Sri(B-country) Lanka(I-country) Rajasinha(B-person) II(I-person) of(I-person) Kandy(I-person) in(O) 1659(O) in(O) An(B-book) Historical(I-book) Relation(I-book) of(I-book) the(I-book) Island(I-book) Ceylon(I-book) .Knox(B-person) ,(I-person) Robert(I-person) ((O) 1911(O) )(O) .(O)"}}
{"id": "211", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "country", "writer", "location", "magazine", "literary genre", "poem", "book", "award", "organization", "person"], "instance": {"id": "211", "words": ["Earlier", ",", "in", "1921", ",", "he", "was", "asked", "by", "the", "biochemist", "and", "president", "of", "the", "World", "Zionist", "Organization", ",", "Chaim", "Weizmann", ",", "to", "help", "raise", "funds", "for", "the", "planned", "university", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, writer, location, magazine, literary genre, poem, book, award, organization, person and O.\nSentence: Earlier , in 1921 , he was asked by the biochemist and president of the World Zionist Organization , Chaim Weizmann , to help raise funds for the planned university .", "prompt_labels": "Earlier(O) ,(O) in(O) 1921(O) ,(O) he(O) was(O) asked(O) by(O) the(O) biochemist(O) and(O) president(O) of(O) the(O) World(B-organization) Zionist(I-organization) Organization(I-organization) ,(O) Chaim(B-person) Weizmann(I-person) ,(O) to(O) help(O) raise(O) funds(O) for(O) the(O) planned(O) university(O) .(O)"}}
{"id": "202", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "event", "poem", "person", "location", "magazine", "award", "writer", "country", "book", "organization"], "instance": {"id": "202", "words": ["In", "2018", ",", "Martin", "called", "The", "Lord", "of", "the", "Rings", ",", "The", "Great", "Gatsby", ",", "Gone", "with", "the", "Wind", ",", "Great", "Expectations", ",", "Lonesome", "Dove", ",", "Catch-22", ",", "and", "Charlotte", "'s", "Web", "favorites", "all", ",", "towering", "masterpieces", ",", "books", "that", "changed", "my", "life", "."], "labels": ["O", "O", "O", "B-person", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "O", "B-book", "I-book", "O", "B-book", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, event, poem, person, location, magazine, award, writer, country, book, organization and O.\nSentence: In 2018 , Martin called The Lord of the Rings , The Great Gatsby , Gone with the Wind , Great Expectations , Lonesome Dove , Catch-22 , and Charlotte 's Web favorites all , towering masterpieces , books that changed my life .", "prompt_labels": "In(O) 2018(O) ,(O) Martin(B-person) called(O) The(B-book) Lord(I-book) of(I-book) the(I-book) Rings(I-book) ,(O) The(B-book) Great(I-book) Gatsby(I-book) ,(O) Gone(B-book) with(I-book) the(I-book) Wind(I-book) ,(O) Great(B-book) Expectations(I-book) ,(O) Lonesome(B-book) Dove(I-book) ,(O) Catch-22(B-book) ,(O) and(O) Charlotte(B-book) 's(I-book) Web(I-book) favorites(O) all(O) ,(O) towering(O) masterpieces(O) ,(O) books(O) that(O) changed(O) my(O) life(O) .(O)"}}
{"id": "6", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "magazine", "award", "writer", "organization", "event", "person", "poem", "country", "location", "literary genre"], "instance": {"id": "6", "words": ["His", "main", "work", "was", "the", "libretto", "for", "The", "Veiled", "Prophet", ",", "a", "Romantic", "Opera", "in", "3", "acts", "composed", "by", "Charles", "Villiers", "Stanford", ",", "adapted", "from", "the", "homonymous", "ballad", "in", "Thomas", "Moore", "oriental", "romance", "Lalla-Rookh", ",", "published", "1890", "."], "labels": ["O", "O", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "B-poem", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, magazine, award, writer, organization, event, person, poem, country, location, literary genre and O.\nSentence: His main work was the libretto for The Veiled Prophet , a Romantic Opera in 3 acts composed by Charles Villiers Stanford , adapted from the homonymous ballad in Thomas Moore oriental romance Lalla-Rookh , published 1890 .", "prompt_labels": "His(O) main(O) work(O) was(O) the(O) libretto(B-literary genre) for(O) The(O) Veiled(O) Prophet(O) ,(O) a(O) Romantic(B-literary genre) Opera(I-literary genre) in(O) 3(O) acts(O) composed(O) by(O) Charles(B-person) Villiers(I-person) Stanford(I-person) ,(O) adapted(O) from(O) the(O) homonymous(O) ballad(O) in(O) Thomas(B-writer) Moore(I-writer) oriental(O) romance(O) Lalla-Rookh(B-poem) ,(O) published(O) 1890(O) .(O)"}}
{"id": "275", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "person", "writer", "poem", "book", "literary genre", "organization", "country", "event", "location", "magazine"], "instance": {"id": "275", "words": ["The", "first", "film", "was", "premiered", "at", "the", "1962", "Cannes", "Film", "Festival", ",", "where", "Lloyd", "was", "fêted", "as", "a", "major", "rediscovery", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, writer, poem, book, literary genre, organization, country, event, location, magazine and O.\nSentence: The first film was premiered at the 1962 Cannes Film Festival , where Lloyd was fêted as a major rediscovery .", "prompt_labels": "The(O) first(O) film(O) was(O) premiered(O) at(O) the(O) 1962(O) Cannes(B-event) Film(I-event) Festival(I-event) ,(O) where(O) Lloyd(B-person) was(O) fêted(O) as(O) a(O) major(O) rediscovery(O) .(O)"}}
{"id": "214", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "literary genre", "location", "magazine", "writer", "poem", "event", "book", "person", "award", "organization"], "instance": {"id": "214", "words": ["His", "mother", "was", "murdered", "by", "the", "Nazi", "Germany", ",", "and", "two", "other", "sisters", ",", "Gittel", "and", "Devorah", ",", "died", "in", "Nazi", "concentration", "camps", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "B-person", "O", "B-person", "O", "O", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, literary genre, location, magazine, writer, poem, event, book, person, award, organization and O.\nSentence: His mother was murdered by the Nazi Germany , and two other sisters , Gittel and Devorah , died in Nazi concentration camps .", "prompt_labels": "His(O) mother(O) was(O) murdered(O) by(O) the(O) Nazi(B-country) Germany(I-country) ,(O) and(O) two(O) other(O) sisters(O) ,(O) Gittel(B-person) and(O) Devorah(B-person) ,(O) died(O) in(O) Nazi(B-location) concentration(I-location) camps(I-location) .(O)"}}
{"id": "220", "dataset": "crossner_literature", "split": "dev", "label_list": ["poem", "person", "event", "book", "award", "literary genre", "writer", "organization", "magazine", "country", "location"], "instance": {"id": "220", "words": ["Kubrick", "based", "his", "adapted", "screenplay", "on", "William", "Makepeace", "Thackeray", "'", "s", "The", "Luck", "of", "Barry", "Lyndon", "(", "republished", "as", "the", "novel", "Memoirs", "of", "Barry", "Lyndon", ",", "Esq", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-literary genre", "B-book", "I-book", "I-book", "I-book", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, person, event, book, award, literary genre, writer, organization, magazine, country, location and O.\nSentence: Kubrick based his adapted screenplay on William Makepeace Thackeray ' s The Luck of Barry Lyndon ( republished as the novel Memoirs of Barry Lyndon , Esq .", "prompt_labels": "Kubrick(B-writer) based(O) his(O) adapted(O) screenplay(O) on(O) William(B-writer) Makepeace(I-writer) Thackeray(I-writer) '(O) s(O) The(B-book) Luck(I-book) of(I-book) Barry(I-book) Lyndon(I-book) ((O) republished(O) as(O) the(O) novel(B-literary genre) Memoirs(B-book) of(I-book) Barry(I-book) Lyndon(I-book) ,(O) Esq(O) .(O)"}}
{"id": "236", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "event", "magazine", "location", "person", "writer", "poem", "award", "book", "literary genre", "country"], "instance": {"id": "236", "words": ["Havens", "was", "invited", "to", "perform", "at", "the", "2008", "Cannes", "Film", "Festival", "opening", "ceremony", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, magazine, location, person, writer, poem, award, book, literary genre, country and O.\nSentence: Havens was invited to perform at the 2008 Cannes Film Festival opening ceremony .", "prompt_labels": "Havens(B-person) was(O) invited(O) to(O) perform(O) at(O) the(O) 2008(O) Cannes(B-event) Film(I-event) Festival(I-event) opening(O) ceremony(O) .(O)"}}
{"id": "75", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "location", "person", "poem", "country", "event", "book", "writer", "magazine", "literary genre", "organization"], "instance": {"id": "75", "words": ["Amos", "prophesied", "during", "the", "reign", "of", "Jeroboam", "II", ",", "King", "of", "Israel", ",", "and", "of", "Uzziah", "of", "Kingdom", "of", "Judah", ",", "which", "places", "him", "in", "the", "first", "half", "of", "the", "8th", "century", "BC", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-country", "O", "O", "O", "B-person", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, person, poem, country, event, book, writer, magazine, literary genre, organization and O.\nSentence: Amos prophesied during the reign of Jeroboam II , King of Israel , and of Uzziah of Kingdom of Judah , which places him in the first half of the 8th century BC .", "prompt_labels": "Amos(B-writer) prophesied(O) during(O) the(O) reign(O) of(O) Jeroboam(B-person) II(I-person) ,(O) King(O) of(O) Israel(B-country) ,(O) and(O) of(O) Uzziah(B-person) of(O) Kingdom(B-country) of(I-country) Judah(I-country) ,(O) which(O) places(O) him(O) in(O) the(O) first(O) half(O) of(O) the(O) 8th(O) century(O) BC(O) .(O)"}}
{"id": "208", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "location", "poem", "event", "book", "magazine", "award", "literary genre", "organization", "writer", "country"], "instance": {"id": "208", "words": ["In", "1930", ",", "Milne", "adapted", "Kenneth", "Grahame", "'", "s", "novel", "The", "Wind", "in", "the", "Willows", "for", "the", "stage", "as", "Toad", "of", "Toad", "Hall", "."], "labels": ["O", "O", "O", "B-writer", "O", "B-writer", "I-writer", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, poem, event, book, magazine, award, literary genre, organization, writer, country and O.\nSentence: In 1930 , Milne adapted Kenneth Grahame ' s novel The Wind in the Willows for the stage as Toad of Toad Hall .", "prompt_labels": "In(O) 1930(O) ,(O) Milne(B-writer) adapted(O) Kenneth(B-writer) Grahame(I-writer) '(O) s(O) novel(O) The(B-book) Wind(I-book) in(I-book) the(I-book) Willows(I-book) for(O) the(O) stage(O) as(O) Toad(B-book) of(I-book) Toad(I-book) Hall(I-book) .(O)"}}
{"id": "117", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "poem", "location", "magazine", "organization", "event", "country", "person", "writer", "literary genre", "award"], "instance": {"id": "117", "words": ["Rand", "'s", "contemporary", "admirers", "included", "fellow", "novelists", ",", "such", "as", "Ira", "Levin", ",", "Kay", "Nolte", "Smith", "and", "L.", "Neil", "Smith", ";", "and", "later", "writers", "such", "as", "Erika", "Holzer", "and", "Terry", "Goodkind", "have", "been", "influenced", "by", "her", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, poem, location, magazine, organization, event, country, person, writer, literary genre, award and O.\nSentence: Rand 's contemporary admirers included fellow novelists , such as Ira Levin , Kay Nolte Smith and L. Neil Smith ; and later writers such as Erika Holzer and Terry Goodkind have been influenced by her .", "prompt_labels": "Rand(B-writer) 's(O) contemporary(O) admirers(O) included(O) fellow(O) novelists(O) ,(O) such(O) as(O) Ira(B-writer) Levin(I-writer) ,(O) Kay(B-writer) Nolte(I-writer) Smith(I-writer) and(O) L.(B-writer) Neil(I-writer) Smith(I-writer) ;(O) and(O) later(O) writers(O) such(O) as(O) Erika(B-writer) Holzer(I-writer) and(O) Terry(B-writer) Goodkind(I-writer) have(O) been(O) influenced(O) by(O) her(O) .(O)"}}
{"id": "166", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "organization", "writer", "person", "country", "event", "poem", "book", "magazine", "award", "literary genre"], "instance": {"id": "166", "words": ["Her", "last", "novel", ",", "Unless", "(", "2002", ")", ",", "was", "nominated", "for", "the", "2002", "Giller", "Prize", ",", "the", "Governor", "General", "of", "Canada", "Literary", "Award", ",", "the", "Booker", "Prize", "and", "the", "2003", "Orange", "Prize", "for", "Fiction", "."], "labels": ["O", "O", "B-literary genre", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, writer, person, country, event, poem, book, magazine, award, literary genre and O.\nSentence: Her last novel , Unless ( 2002 ) , was nominated for the 2002 Giller Prize , the Governor General of Canada Literary Award , the Booker Prize and the 2003 Orange Prize for Fiction .", "prompt_labels": "Her(O) last(O) novel(B-literary genre) ,(O) Unless(B-book) ((O) 2002(O) )(O) ,(O) was(O) nominated(O) for(O) the(O) 2002(O) Giller(B-award) Prize(I-award) ,(O) the(O) Governor(B-award) General(I-award) of(I-award) Canada(I-award) Literary(I-award) Award(I-award) ,(O) the(O) Booker(B-award) Prize(I-award) and(O) the(O) 2003(B-award) Orange(I-award) Prize(I-award) for(I-award) Fiction(I-award) .(O)"}}
{"id": "360", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "event", "award", "magazine", "person", "organization", "poem", "country", "literary genre", "book", "location"], "instance": {"id": "360", "words": ["Chekhov", "has", "also", "influenced", "the", "work", "of", "Japanese", "playwrights", "including", "Shimizu", "Kunio", ",", "Yōji", "Sakate", ",", "and", "Ai", "Nagai", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, event, award, magazine, person, organization, poem, country, literary genre, book, location and O.\nSentence: Chekhov has also influenced the work of Japanese playwrights including Shimizu Kunio , Yōji Sakate , and Ai Nagai .", "prompt_labels": "Chekhov(B-writer) has(O) also(O) influenced(O) the(O) work(O) of(O) Japanese(O) playwrights(O) including(O) Shimizu(B-writer) Kunio(I-writer) ,(O) Yōji(B-writer) Sakate(I-writer) ,(O) and(O) Ai(B-writer) Nagai(I-writer) .(O)"}}
{"id": "335", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "award", "person", "country", "magazine", "organization", "event", "poem", "location", "literary genre", "book"], "instance": {"id": "335", "words": ["Several", "of", "Du", "Maurier", "'s", "other", "novels", "have", "also", "been", "adapted", "for", "the", "screen", ",", "including", "Jamaica", "Inn", ",", "Frenchman", "'s", "Creek", ",", "Hungry", "Hill", ",", "and", "My", "Cousin", "Rachel", "."], "labels": ["O", "O", "B-writer", "I-writer", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "B-book", "I-book", "I-book", "O", "B-book", "I-book", "O", "O", "B-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, award, person, country, magazine, organization, event, poem, location, literary genre, book and O.\nSentence: Several of Du Maurier 's other novels have also been adapted for the screen , including Jamaica Inn , Frenchman 's Creek , Hungry Hill , and My Cousin Rachel .", "prompt_labels": "Several(O) of(O) Du(B-writer) Maurier(I-writer) 's(O) other(O) novels(B-literary genre) have(O) also(O) been(O) adapted(O) for(O) the(O) screen(O) ,(O) including(O) Jamaica(B-book) Inn(I-book) ,(O) Frenchman(B-book) 's(I-book) Creek(I-book) ,(O) Hungry(B-book) Hill(I-book) ,(O) and(O) My(B-book) Cousin(I-book) Rachel(I-book) .(O)"}}
{"id": "341", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "organization", "event", "book", "magazine", "person", "country", "award", "literary genre", "writer", "poem"], "instance": {"id": "341", "words": ["In", "the", "alternate", "history", "novel", "The", "Probability", "Broach", "(", "part", "of", "the", "North", "American", "Confederacy", "series", ")", "by", "L.", "Neil", "Smith", "in", "which", "the", "United", "States", "becomes", "a", "libertarian", "state", "after", "a", "successful", "Whiskey", "Rebellion", "and", "the", "overthrowing", "and", "execution", "of", "George", "Washington", "by", "firing", "squad", "for", "treason", "in", "1794", ",", "Spooner", "served", "as", "the", "14th", "President", "of", "the", "North", "American", "Confederacy", "from", "1860", "to", "1880", "."], "labels": ["O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "B-book", "I-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, event, book, magazine, person, country, award, literary genre, writer, poem and O.\nSentence: In the alternate history novel The Probability Broach ( part of the North American Confederacy series ) by L. Neil Smith in which the United States becomes a libertarian state after a successful Whiskey Rebellion and the overthrowing and execution of George Washington by firing squad for treason in 1794 , Spooner served as the 14th President of the North American Confederacy from 1860 to 1880 .", "prompt_labels": "In(O) the(O) alternate(B-literary genre) history(I-literary genre) novel(I-literary genre) The(B-book) Probability(I-book) Broach(I-book) ((O) part(O) of(O) the(O) North(B-book) American(I-book) Confederacy(I-book) series(O) )(O) by(O) L.(B-writer) Neil(I-writer) Smith(I-writer) in(O) which(O) the(O) United(B-country) States(I-country) becomes(O) a(O) libertarian(O) state(O) after(O) a(O) successful(O) Whiskey(B-event) Rebellion(I-event) and(O) the(O) overthrowing(O) and(O) execution(O) of(O) George(B-person) Washington(I-person) by(O) firing(O) squad(O) for(O) treason(O) in(O) 1794(O) ,(O) Spooner(B-person) served(O) as(O) the(O) 14th(O) President(O) of(O) the(O) North(B-country) American(I-country) Confederacy(I-country) from(O) 1860(O) to(O) 1880(O) .(O)"}}
{"id": "189", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "person", "writer", "event", "organization", "country", "book", "award", "poem", "magazine", "literary genre"], "instance": {"id": "189", "words": ["These", "avant-garde", "poets", "drew", "inspiration", "from", "earlier", "Greek", "authors", ",", "especially", "Sappho", "and", "Callimachus", ";", "Catullus", "himself", "used", "Sapphic", "meter", "in", "two", "poems", ",", "Catullus", "11", "and", "51", ",", "the", "second", "of", "which", "is", "almost", "a", "translation", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "B-writer", "O", "B-writer", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "B-literary genre", "O", "B-poem", "I-poem", "O", "B-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, writer, event, organization, country, book, award, poem, magazine, literary genre and O.\nSentence: These avant-garde poets drew inspiration from earlier Greek authors , especially Sappho and Callimachus ; Catullus himself used Sapphic meter in two poems , Catullus 11 and 51 , the second of which is almost a translation .", "prompt_labels": "These(O) avant-garde(O) poets(O) drew(O) inspiration(O) from(O) earlier(O) Greek(O) authors(O) ,(O) especially(O) Sappho(B-writer) and(O) Callimachus(B-writer) ;(O) Catullus(B-writer) himself(O) used(O) Sapphic(B-literary genre) meter(I-literary genre) in(O) two(O) poems(B-literary genre) ,(O) Catullus(B-poem) 11(I-poem) and(O) 51(B-poem) ,(O) the(O) second(O) of(O) which(O) is(O) almost(O) a(O) translation(O) .(O)"}}
{"id": "115", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "poem", "organization", "award", "person", "event", "literary genre", "book", "location", "magazine", "writer"], "instance": {"id": "115", "words": ["He", "particularly", "revered", "Johann", "Wolfgang", "von", "Goethe", ",", "Petrarch", ",", "Pedro", "Calderón", "de", "la", "Barca", "and", "William", "Shakespeare", "."], "labels": ["O", "O", "O", "B-writer", "I-writer", "I-writer", "I-writer", "O", "B-writer", "O", "B-writer", "I-writer", "I-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, poem, organization, award, person, event, literary genre, book, location, magazine, writer and O.\nSentence: He particularly revered Johann Wolfgang von Goethe , Petrarch , Pedro Calderón de la Barca and William Shakespeare .", "prompt_labels": "He(O) particularly(O) revered(O) Johann(B-writer) Wolfgang(I-writer) von(I-writer) Goethe(I-writer) ,(O) Petrarch(B-writer) ,(O) Pedro(B-writer) Calderón(I-writer) de(I-writer) la(I-writer) Barca(I-writer) and(O) William(B-writer) Shakespeare(I-writer) .(O)"}}
{"id": "20", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "award", "organization", "book", "person", "writer", "location", "poem", "literary genre", "event", "magazine"], "instance": {"id": "20", "words": ["The", "film", "is", "based", "on", "The", "Caine", "Mutiny", ",", "the", "1951", "Pulitzer", "Prize", "-winning", "novel", "written", "by", "Herman", "Wouk", "."], "labels": ["O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "B-award", "I-award", "O", "B-literary genre", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, organization, book, person, writer, location, poem, literary genre, event, magazine and O.\nSentence: The film is based on The Caine Mutiny , the 1951 Pulitzer Prize -winning novel written by Herman Wouk .", "prompt_labels": "The(O) film(O) is(O) based(O) on(O) The(B-book) Caine(I-book) Mutiny(I-book) ,(O) the(O) 1951(O) Pulitzer(B-award) Prize(I-award) -winning(O) novel(B-literary genre) written(O) by(O) Herman(B-writer) Wouk(I-writer) .(O)"}}
{"id": "278", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "person", "award", "poem", "event", "book", "writer", "magazine", "country", "literary genre", "location"], "instance": {"id": "278", "words": ["He", "is", "ironic", ",", "as", "in", "Invective", "Against", "Swans", "and", "Anatomy", "of", "Monotony", ",", "about", "the", "'", "spiritual", "'", "demand", "to", "transcend", "nature", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "I-poem", "O", "B-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, award, poem, event, book, writer, magazine, country, literary genre, location and O.\nSentence: He is ironic , as in Invective Against Swans and Anatomy of Monotony , about the ' spiritual ' demand to transcend nature .", "prompt_labels": "He(O) is(O) ironic(O) ,(O) as(O) in(O) Invective(B-poem) Against(I-poem) Swans(I-poem) and(O) Anatomy(B-poem) of(I-poem) Monotony(I-poem) ,(O) about(O) the(O) '(O) spiritual(O) '(O) demand(O) to(O) transcend(O) nature(O) .(O)"}}
{"id": "144", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "writer", "magazine", "literary genre", "location", "award", "event", "organization", "book", "person", "poem"], "instance": {"id": "144", "words": ["In", "a", "review", "in", "The", "Dial", ",", "T.", "S.", "Eliot", "said", "of", "Ulysses", ":", "I", "hold", "this", "book", "to", "be", "the", "most", "important", "expression", "which", "the", "present", "age", "has", "found", ";", "it", "is", "a", "book", "to", "which", "we", "are", "all", "indebted", ",", "and", "from", "which", "none", "of", "us", "can", "escape", "."], "labels": ["O", "O", "O", "O", "B-magazine", "I-magazine", "O", "B-writer", "I-writer", "I-writer", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, writer, magazine, literary genre, location, award, event, organization, book, person, poem and O.\nSentence: In a review in The Dial , T. S. Eliot said of Ulysses : I hold this book to be the most important expression which the present age has found ; it is a book to which we are all indebted , and from which none of us can escape .", "prompt_labels": "In(O) a(O) review(O) in(O) The(B-magazine) Dial(I-magazine) ,(O) T.(B-writer) S.(I-writer) Eliot(I-writer) said(O) of(O) Ulysses(B-book) :(O) I(O) hold(O) this(O) book(O) to(O) be(O) the(O) most(O) important(O) expression(O) which(O) the(O) present(O) age(O) has(O) found(O) ;(O) it(O) is(O) a(O) book(O) to(O) which(O) we(O) are(O) all(O) indebted(O) ,(O) and(O) from(O) which(O) none(O) of(O) us(O) can(O) escape(O) .(O)"}}
{"id": "142", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "organization", "poem", "literary genre", "country", "writer", "book", "location", "event", "magazine", "person"], "instance": {"id": "142", "words": ["Lerner", "and", "Loewe", "'s", "run", "of", "success", "continued", "with", "their", "next", "project", ",", "a", "film", "adaptation", "of", "stories", "from", "Colette", ",", "the", "Academy", "Awards", "-winning", "film", "musical", "Gigi", ",", "starring", "Leslie", "Caron", ",", "Louis", "Jourdan", "and", "Maurice", "Chevalier", "."], "labels": ["B-writer", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, poem, literary genre, country, writer, book, location, event, magazine, person and O.\nSentence: Lerner and Loewe 's run of success continued with their next project , a film adaptation of stories from Colette , the Academy Awards -winning film musical Gigi , starring Leslie Caron , Louis Jourdan and Maurice Chevalier .", "prompt_labels": "Lerner(B-writer) and(O) Loewe(B-writer) 's(O) run(O) of(O) success(O) continued(O) with(O) their(O) next(O) project(O) ,(O) a(O) film(O) adaptation(O) of(O) stories(O) from(O) Colette(B-writer) ,(O) the(O) Academy(B-award) Awards(I-award) -winning(O) film(O) musical(O) Gigi(O) ,(O) starring(O) Leslie(B-person) Caron(I-person) ,(O) Louis(B-person) Jourdan(I-person) and(O) Maurice(B-person) Chevalier(I-person) .(O)"}}
{"id": "274", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "event", "poem", "writer", "literary genre", "book", "magazine", "location", "organization", "person", "country"], "instance": {"id": "274", "words": ["His", "poem", "Not", "My", "Business", "is", "compulsory", "study", "in", "the", "AQA", "A", "syllabus", "for", "General", "Certificate", "of", "Secondary", "Education", "English", "Language", "."], "labels": ["O", "B-literary genre", "B-poem", "I-poem", "I-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, poem, writer, literary genre, book, magazine, location, organization, person, country and O.\nSentence: His poem Not My Business is compulsory study in the AQA A syllabus for General Certificate of Secondary Education English Language .", "prompt_labels": "His(O) poem(B-literary genre) Not(B-poem) My(I-poem) Business(I-poem) is(O) compulsory(O) study(O) in(O) the(O) AQA(O) A(O) syllabus(O) for(O) General(O) Certificate(O) of(O) Secondary(O) Education(O) English(O) Language(O) .(O)"}}
{"id": "327", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "location", "literary genre", "event", "book", "organization", "writer", "award", "person", "poem", "magazine"], "instance": {"id": "327", "words": ["On", "June", "25", ",", "2019", ",", "The", "New", "York", "Times", "Magazine", "listed", "Amy", "Grant", "among", "hundreds", "of", "artists", "whose", "material", "was", "reportedly", "destroyed", "in", "the", "2008", "Universal", "fire", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "I-magazine", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, literary genre, event, book, organization, writer, award, person, poem, magazine and O.\nSentence: On June 25 , 2019 , The New York Times Magazine listed Amy Grant among hundreds of artists whose material was reportedly destroyed in the 2008 Universal fire .", "prompt_labels": "On(O) June(O) 25(O) ,(O) 2019(O) ,(O) The(B-magazine) New(I-magazine) York(I-magazine) Times(I-magazine) Magazine(I-magazine) listed(O) Amy(B-person) Grant(I-person) among(O) hundreds(O) of(O) artists(O) whose(O) material(O) was(O) reportedly(O) destroyed(O) in(O) the(O) 2008(B-event) Universal(I-event) fire(I-event) .(O)"}}
{"id": "100", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "writer", "magazine", "book", "location", "organization", "award", "event", "poem", "literary genre", "country"], "instance": {"id": "100", "words": ["During", "his", "first", "years", "as", "bishop", ",", "Athanasius", "visited", "the", "churches", "of", "his", "territory", ",", "which", "at", "that", "time", "included", "all", "of", "Egypt", "and", "Libya", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, magazine, book, location, organization, award, event, poem, literary genre, country and O.\nSentence: During his first years as bishop , Athanasius visited the churches of his territory , which at that time included all of Egypt and Libya .", "prompt_labels": "During(O) his(O) first(O) years(O) as(O) bishop(O) ,(O) Athanasius(B-writer) visited(O) the(O) churches(O) of(O) his(O) territory(O) ,(O) which(O) at(O) that(O) time(O) included(O) all(O) of(O) Egypt(B-country) and(O) Libya(B-country) .(O)"}}
{"id": "348", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "person", "award", "organization", "location", "country", "writer", "book", "magazine", "event", "poem"], "instance": {"id": "348", "words": ["It", "is", "similar", "to", "the", "Historia", "Regum", "Britanniae", "by", "Geoffrey", "of", "Monmouth", "and", "was", "meant", "to", "rival", "the", "epic", "Alexandreis", "by", "Walter", "of", "Châtillon"], "labels": ["O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "B-poem", "O", "B-writer", "I-writer", "I-writer"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, person, award, organization, location, country, writer, book, magazine, event, poem and O.\nSentence: It is similar to the Historia Regum Britanniae by Geoffrey of Monmouth and was meant to rival the epic Alexandreis by Walter of Châtillon", "prompt_labels": "It(O) is(O) similar(O) to(O) the(O) Historia(B-book) Regum(I-book) Britanniae(I-book) by(O) Geoffrey(B-writer) of(I-writer) Monmouth(I-writer) and(O) was(O) meant(O) to(O) rival(O) the(O) epic(O) Alexandreis(B-poem) by(O) Walter(B-writer) of(I-writer) Châtillon(I-writer)"}}
{"id": "88", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "writer", "event", "award", "book", "literary genre", "country", "poem", "person", "location", "magazine"], "instance": {"id": "88", "words": ["The", "Man", "in", "the", "High", "Castle", "(", "1962", ")", "is", "set", "in", "an", "alternate", "history", "in", "which", "the", "United", "States", "is", "ruled", "by", "the", "victorious", "Axis", "powers", "."], "labels": ["B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, writer, event, award, book, literary genre, country, poem, person, location, magazine and O.\nSentence: The Man in the High Castle ( 1962 ) is set in an alternate history in which the United States is ruled by the victorious Axis powers .", "prompt_labels": "The(B-book) Man(I-book) in(I-book) the(I-book) High(I-book) Castle(I-book) ((O) 1962(O) )(O) is(O) set(O) in(O) an(O) alternate(B-literary genre) history(I-literary genre) in(O) which(O) the(O) United(B-country) States(I-country) is(O) ruled(O) by(O) the(O) victorious(O) Axis(O) powers(O) .(O)"}}
{"id": "308", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "event", "book", "location", "writer", "country", "poem", "organization", "award", "person", "magazine"], "instance": {"id": "308", "words": ["The", "Anglo-Irish", "writer", "Jonathan", "Swift", "(", "1667-1745", ")", ",", "in", "his", "Discourse", "on", "the", "Contests", "and", "Dissentions", "in", "Athens", "and", "Rome", ",", "criticized", "Augustus", "for", "installing", "tyranny", "over", "Rome", ",", "and", "likened", "what", "he", "believed", "United", "Kingdom", "'", "s", "virtuous", "constitutional", "monarchy", "to", "Rome", "'s", "moral", "Republic", "of", "the", "2nd", "century", "BC", "."], "labels": ["O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "B-person", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, event, book, location, writer, country, poem, organization, award, person, magazine and O.\nSentence: The Anglo-Irish writer Jonathan Swift ( 1667-1745 ) , in his Discourse on the Contests and Dissentions in Athens and Rome , criticized Augustus for installing tyranny over Rome , and likened what he believed United Kingdom ' s virtuous constitutional monarchy to Rome 's moral Republic of the 2nd century BC .", "prompt_labels": "The(O) Anglo-Irish(O) writer(O) Jonathan(B-writer) Swift(I-writer) ((O) 1667-1745(O) )(O) ,(O) in(O) his(O) Discourse(B-book) on(I-book) the(I-book) Contests(I-book) and(I-book) Dissentions(I-book) in(I-book) Athens(I-book) and(I-book) Rome(I-book) ,(O) criticized(O) Augustus(B-person) for(O) installing(O) tyranny(O) over(O) Rome(B-location) ,(O) and(O) likened(O) what(O) he(O) believed(O) United(B-country) Kingdom(I-country) '(O) s(O) virtuous(O) constitutional(O) monarchy(O) to(O) Rome(B-location) 's(O) moral(O) Republic(O) of(O) the(O) 2nd(O) century(O) BC(O) .(O)"}}
{"id": "76", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "poem", "country", "person", "award", "event", "organization", "book", "literary genre", "location", "magazine"], "instance": {"id": "76", "words": ["Jorge", "Luis", "Borges", "wrote", "a", "contemporary", "bestiary", "of", "sorts", ",", "the", "Book", "of", "Imaginary", "Beings", ",", "which", "collects", "imaginary", "beasts", "from", "bestiaries", "and", "fiction", "."], "labels": ["B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, poem, country, person, award, event, organization, book, literary genre, location, magazine and O.\nSentence: Jorge Luis Borges wrote a contemporary bestiary of sorts , the Book of Imaginary Beings , which collects imaginary beasts from bestiaries and fiction .", "prompt_labels": "Jorge(B-writer) Luis(I-writer) Borges(I-writer) wrote(O) a(O) contemporary(O) bestiary(O) of(O) sorts(O) ,(O) the(O) Book(B-book) of(I-book) Imaginary(I-book) Beings(I-book) ,(O) which(O) collects(O) imaginary(O) beasts(O) from(O) bestiaries(O) and(O) fiction(B-literary genre) .(O)"}}
{"id": "354", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "magazine", "location", "book", "person", "event", "country", "poem", "literary genre", "organization", "award"], "instance": {"id": "354", "words": ["As", "a", "result", "of", "his", "early", "work", "on", "the", "Sac", "Prairie", "Saga", ",", "Derleth", "was", "awarded", "the", "prestigious", "Guggenheim", "Fellowship", ";", "his", "sponsors", "were", "Helen", "C.", "White", ",", "Nobel", "Prize-winning", "novelist", "Sinclair", "Lewis", "and", "poet", "Edgar", "Lee", "Masters", "of", "Spoon", "River", "Anthology", "fame", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "B-book", "B-book", "O", "B-writer", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "B-award", "I-award", "O", "B-writer", "I-writer", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-book", "I-book", "I-book", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, magazine, location, book, person, event, country, poem, literary genre, organization, award and O.\nSentence: As a result of his early work on the Sac Prairie Saga , Derleth was awarded the prestigious Guggenheim Fellowship ; his sponsors were Helen C. White , Nobel Prize-winning novelist Sinclair Lewis and poet Edgar Lee Masters of Spoon River Anthology fame .", "prompt_labels": "As(O) a(O) result(O) of(O) his(O) early(O) work(O) on(O) the(O) Sac(B-book) Prairie(B-book) Saga(B-book) ,(O) Derleth(B-writer) was(O) awarded(O) the(O) prestigious(O) Guggenheim(B-award) Fellowship(I-award) ;(O) his(O) sponsors(O) were(O) Helen(B-person) C.(I-person) White(I-person) ,(O) Nobel(B-award) Prize-winning(I-award) novelist(O) Sinclair(B-writer) Lewis(I-writer) and(O) poet(O) Edgar(B-writer) Lee(I-writer) Masters(I-writer) of(O) Spoon(B-book) River(I-book) Anthology(I-book) fame(O) .(O)"}}
{"id": "224", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "event", "magazine", "organization", "poem", "country", "book", "location", "literary genre", "person", "writer"], "instance": {"id": "224", "words": ["In", "1900", ",", "Potter", "revised", "her", "tale", "about", "the", "four", "little", "rabbits", ",", "and", "fashioned", "a", "dummy", "book", "of", "it", "-", "it", "has", "been", "suggested", ",", "in", "imitation", "of", "Helen", "Bannerman", "'", "s", "1899", "bestseller", "The", "Story", "of", "Little", "Black", "Sambo", ".Stevenson", ",", "Laura", "C.", "A", "Vogue", "for", "Small", "Books", ":", "The", "Tale", "of", "Peter", "Rabbit", "and", "its", "Contemporary", "Competitors", "to", "reproduce", "her", "watercolours", "."], "labels": ["O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "B-writer", "I-writer", "I-writer", "I-writer", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, magazine, organization, poem, country, book, location, literary genre, person, writer and O.\nSentence: In 1900 , Potter revised her tale about the four little rabbits , and fashioned a dummy book of it - it has been suggested , in imitation of Helen Bannerman ' s 1899 bestseller The Story of Little Black Sambo .Stevenson , Laura C. A Vogue for Small Books : The Tale of Peter Rabbit and its Contemporary Competitors to reproduce her watercolours .", "prompt_labels": "In(O) 1900(O) ,(O) Potter(B-writer) revised(O) her(O) tale(O) about(O) the(O) four(O) little(O) rabbits(O) ,(O) and(O) fashioned(O) a(O) dummy(O) book(O) of(O) it(O) -(O) it(O) has(O) been(O) suggested(O) ,(O) in(O) imitation(O) of(O) Helen(B-writer) Bannerman(I-writer) '(O) s(O) 1899(O) bestseller(O) The(B-book) Story(I-book) of(I-book) Little(I-book) Black(I-book) Sambo(I-book) .Stevenson(B-writer) ,(I-writer) Laura(I-writer) C.(I-writer) A(B-book) Vogue(I-book) for(I-book) Small(I-book) Books(I-book) :(I-book) The(I-book) Tale(I-book) of(I-book) Peter(I-book) Rabbit(I-book) and(O) its(O) Contemporary(O) Competitors(O) to(O) reproduce(O) her(O) watercolours(O) .(O)"}}
{"id": "61", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "country", "magazine", "person", "literary genre", "award", "location", "poem", "book", "event", "writer"], "instance": {"id": "61", "words": ["Bernard", "Miles", "gave", "Milligan", "his", "first", "straight", "acting", "role", ",", "as", "Ben", "Gunn", ",", "in", "the", "Mermaid", "Theatre", "production", "of", "Treasure", "Island", "."], "labels": ["B-person", "I-person", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-location", "I-location", "O", "O", "B-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, magazine, person, literary genre, award, location, poem, book, event, writer and O.\nSentence: Bernard Miles gave Milligan his first straight acting role , as Ben Gunn , in the Mermaid Theatre production of Treasure Island .", "prompt_labels": "Bernard(B-person) Miles(I-person) gave(O) Milligan(B-person) his(O) first(O) straight(O) acting(O) role(O) ,(O) as(O) Ben(B-person) Gunn(I-person) ,(O) in(O) the(O) Mermaid(B-location) Theatre(I-location) production(O) of(O) Treasure(B-book) Island(I-book) .(O)"}}
{"id": "358", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "organization", "award", "poem", "location", "event", "book", "literary genre", "writer", "magazine", "person"], "instance": {"id": "358", "words": ["Gigi", "(", ")", "is", "a", "1944", "novella", "by", "France", "writer", "Colette", "."], "labels": ["B-book", "O", "O", "O", "O", "O", "B-literary genre", "O", "B-country", "O", "B-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, award, poem, location, event, book, literary genre, writer, magazine, person and O.\nSentence: Gigi ( ) is a 1944 novella by France writer Colette .", "prompt_labels": "Gigi(B-book) ((O) )(O) is(O) a(O) 1944(O) novella(B-literary genre) by(O) France(B-country) writer(O) Colette(B-writer) .(O)"}}
{"id": "102", "dataset": "crossner_literature", "split": "dev", "label_list": ["country", "event", "organization", "award", "magazine", "book", "writer", "poem", "person", "location", "literary genre"], "instance": {"id": "102", "words": ["He", "liked", "Pedro", "Calderón", "de", "la", "Barca", ",", "Lope", "de", "Vega", ",", "Miguel", "de", "Cervantes", ",", "and", "especially", "Baltasar", "Gracián", "."], "labels": ["O", "O", "B-writer", "I-writer", "I-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, organization, award, magazine, book, writer, poem, person, location, literary genre and O.\nSentence: He liked Pedro Calderón de la Barca , Lope de Vega , Miguel de Cervantes , and especially Baltasar Gracián .", "prompt_labels": "He(O) liked(O) Pedro(B-writer) Calderón(I-writer) de(I-writer) la(I-writer) Barca(I-writer) ,(O) Lope(B-writer) de(I-writer) Vega(I-writer) ,(O) Miguel(B-writer) de(I-writer) Cervantes(I-writer) ,(O) and(O) especially(O) Baltasar(B-writer) Gracián(I-writer) .(O)"}}
{"id": "173", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "country", "organization", "award", "poem", "event", "book", "literary genre", "writer", "location", "magazine"], "instance": {"id": "173", "words": ["A", "documentary", "film", "about", "Rivers", ",", "Joan", "Rivers", ":", "A", "Piece", "of", "Work", ",", "premiered", "at", "the", "San", "Francisco", "International", "Film", "Festival", "on", "May", "6", ",", "2010", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, organization, award, poem, event, book, literary genre, writer, location, magazine and O.\nSentence: A documentary film about Rivers , Joan Rivers : A Piece of Work , premiered at the San Francisco International Film Festival on May 6 , 2010 .", "prompt_labels": "A(O) documentary(O) film(O) about(O) Rivers(O) ,(O) Joan(O) Rivers(O) :(O) A(O) Piece(O) of(O) Work(O) ,(O) premiered(O) at(O) the(O) San(B-event) Francisco(I-event) International(I-event) Film(I-event) Festival(I-event) on(O) May(O) 6(O) ,(O) 2010(O) .(O)"}}
{"id": "384", "dataset": "crossner_literature", "split": "dev", "label_list": ["literary genre", "country", "magazine", "organization", "award", "event", "writer", "location", "book", "poem", "person"], "instance": {"id": "384", "words": ["The", "latest", "translations", "of", "Racine", "'s", "plays", "into", "English", "have", "been", "by", "Alan", "Hollinghurst", "(", "Berenice", ",", "Bajazet", ")", ",", "by", "RADA", "director", "Edward", "Kemp", "(", "Andromache", ")", ",", "Neil", "Bartlett", ",", "and", "poet", "Geoffrey", "Alan", "Argent", ",", "who", "earned", "a", "2011", "American", "Book", "Awards", "for", "the", "translating", "The", "Complete", "Plays", "of", "Jean", "Racine", "."], "labels": ["O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-book", "O", "B-book", "O", "O", "O", "B-organization", "O", "B-writer", "I-writer", "O", "B-person", "O", "O", "B-writer", "I-writer", "O", "O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "B-book", "I-book", "I-book", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, country, magazine, organization, award, event, writer, location, book, poem, person and O.\nSentence: The latest translations of Racine 's plays into English have been by Alan Hollinghurst ( Berenice , Bajazet ) , by RADA director Edward Kemp ( Andromache ) , Neil Bartlett , and poet Geoffrey Alan Argent , who earned a 2011 American Book Awards for the translating The Complete Plays of Jean Racine .", "prompt_labels": "The(O) latest(O) translations(O) of(O) Racine(B-writer) 's(O) plays(O) into(O) English(O) have(O) been(O) by(O) Alan(B-writer) Hollinghurst(I-writer) ((O) Berenice(B-book) ,(O) Bajazet(B-book) )(O) ,(O) by(O) RADA(B-organization) director(O) Edward(B-writer) Kemp(I-writer) ((O) Andromache(B-person) )(O) ,(O) Neil(B-writer) Bartlett(I-writer) ,(O) and(O) poet(O) Geoffrey(B-writer) Alan(I-writer) Argent(I-writer) ,(O) who(O) earned(O) a(O) 2011(O) American(B-award) Book(I-award) Awards(I-award) for(O) the(O) translating(O) The(B-book) Complete(I-book) Plays(I-book) of(O) Jean(B-writer) Racine(I-writer) .(O)"}}
{"id": "372", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "location", "event", "literary genre", "country", "poem", "book", "person", "writer", "award", "organization"], "instance": {"id": "372", "words": ["He", "wrote", "for", "many", "publications", ",", "including", "Rolling", "Stone", ",", "Esquire", ",", "The", "Boston", "Globe", ",", "Chicago", "Tribune", ",", "The", "New", "York", "Times", ",", "The", "San", "Francisco", "Examiner", ",", "Time", ",", "Vanity", "Fair", ",", "The", "San", "Juan", "Star", ",", "and", "Playboy", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "O", "B-magazine", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-magazine", "O", "B-magazine", "I-magazine", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-magazine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, event, literary genre, country, poem, book, person, writer, award, organization and O.\nSentence: He wrote for many publications , including Rolling Stone , Esquire , The Boston Globe , Chicago Tribune , The New York Times , The San Francisco Examiner , Time , Vanity Fair , The San Juan Star , and Playboy .", "prompt_labels": "He(O) wrote(O) for(O) many(O) publications(O) ,(O) including(O) Rolling(B-magazine) Stone(I-magazine) ,(O) Esquire(B-magazine) ,(O) The(B-organization) Boston(I-organization) Globe(I-organization) ,(O) Chicago(B-organization) Tribune(I-organization) ,(O) The(B-organization) New(I-organization) York(I-organization) Times(I-organization) ,(O) The(B-organization) San(I-organization) Francisco(I-organization) Examiner(I-organization) ,(O) Time(B-magazine) ,(O) Vanity(B-magazine) Fair(I-magazine) ,(O) The(B-organization) San(I-organization) Juan(I-organization) Star(I-organization) ,(O) and(O) Playboy(B-magazine) .(O)"}}
{"id": "134", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "book", "country", "poem", "person", "event", "award", "literary genre", "magazine", "location", "organization"], "instance": {"id": "134", "words": ["Disraeli", "'s", "early", "silver", "fork", "novels", "Vivian", "Grey", "(", "1826", ")", "and", "The", "Young", "Duke", "(", "1831", ")", "featured", "romanticised", "depictions", "of", "aristocratic", "life", "(", "despite", "his", "ignorance", "of", "it", ")", "with", "character", "sketches", "of", "well-known", "public", "figures", "lightly", "disguised", "."], "labels": ["B-writer", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "B-book", "I-book", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, country, poem, person, event, award, literary genre, magazine, location, organization and O.\nSentence: Disraeli 's early silver fork novels Vivian Grey ( 1826 ) and The Young Duke ( 1831 ) featured romanticised depictions of aristocratic life ( despite his ignorance of it ) with character sketches of well-known public figures lightly disguised .", "prompt_labels": "Disraeli(B-writer) 's(O) early(O) silver(B-literary genre) fork(I-literary genre) novels(I-literary genre) Vivian(B-book) Grey(I-book) ((O) 1826(O) )(O) and(O) The(B-book) Young(I-book) Duke(I-book) ((O) 1831(O) )(O) featured(O) romanticised(O) depictions(O) of(O) aristocratic(O) life(O) ((O) despite(O) his(O) ignorance(O) of(O) it(O) )(O) with(O) character(O) sketches(O) of(O) well-known(O) public(O) figures(O) lightly(O) disguised(O) .(O)"}}
{"id": "141", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "literary genre", "country", "magazine", "person", "award", "organization", "book", "poem", "event", "writer"], "instance": {"id": "141", "words": ["In", "1972", ",", "Ian", "Angus", "found", "the", "original", "typescript", "titled", "The", "Freedom", "of", "the", "Press", ",", "and", "Bernard", "Crick", "published", "it", ",", "together", "with", "his", "own", "introduction", ",", "in", "The", "Times", "Literary", "Supplement", "on", "15", "September", "1972", "as", "How", "the", "essay", "came", "to", "be", "written", "."], "labels": ["O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, literary genre, country, magazine, person, award, organization, book, poem, event, writer and O.\nSentence: In 1972 , Ian Angus found the original typescript titled The Freedom of the Press , and Bernard Crick published it , together with his own introduction , in The Times Literary Supplement on 15 September 1972 as How the essay came to be written .", "prompt_labels": "In(O) 1972(O) ,(O) Ian(B-person) Angus(I-person) found(O) the(O) original(O) typescript(O) titled(O) The(O) Freedom(O) of(O) the(O) Press(O) ,(O) and(O) Bernard(B-person) Crick(I-person) published(O) it(O) ,(O) together(O) with(O) his(O) own(O) introduction(O) ,(O) in(O) The(B-magazine) Times(I-magazine) Literary(I-magazine) Supplement(I-magazine) on(O) 15(O) September(O) 1972(O) as(O) How(O) the(O) essay(O) came(O) to(O) be(O) written(O) .(O)"}}
{"id": "238", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "country", "award", "book", "literary genre", "writer", "poem", "person", "magazine", "organization", "event"], "instance": {"id": "238", "words": ["The", "decade", "-", "and", "the", "Australian", "phase", "of", "Carey", "'s", "career", "-", "culminated", "with", "the", "publication", "of", "Oscar", "and", "Lucinda", "(", "1988", ")", ",", "which", "won", "the", "Booker", "Prize", "(", "as", "it", "was", "then", "known", ")", "and", "brought", "the", "author", "international", "recognition", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, award, book, literary genre, writer, poem, person, magazine, organization, event and O.\nSentence: The decade - and the Australian phase of Carey 's career - culminated with the publication of Oscar and Lucinda ( 1988 ) , which won the Booker Prize ( as it was then known ) and brought the author international recognition .", "prompt_labels": "The(O) decade(O) -(O) and(O) the(O) Australian(O) phase(O) of(O) Carey(B-writer) 's(O) career(O) -(O) culminated(O) with(O) the(O) publication(O) of(O) Oscar(B-book) and(I-book) Lucinda(I-book) ((O) 1988(O) )(O) ,(O) which(O) won(O) the(O) Booker(B-award) Prize(I-award) ((O) as(O) it(O) was(O) then(O) known(O) )(O) and(O) brought(O) the(O) author(O) international(O) recognition(O) .(O)"}}
{"id": "176", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "literary genre", "magazine", "book", "person", "event", "award", "location", "poem", "country", "organization"], "instance": {"id": "176", "words": ["Satirical", "poets", "outside", "England", "include", "Poland", "'", "s", "Ignacy", "Krasicki", ",", "Azerbaijan", "'", "s", "Mirza", "Alakbar", "Sabir", "and", "Portugal", "'", "s", "Manuel", "Maria", "Barbosa", "du", "Bocage", "."], "labels": ["B-literary genre", "O", "O", "B-country", "O", "B-country", "O", "O", "B-writer", "I-writer", "O", "B-country", "O", "O", "B-writer", "I-writer", "I-writer", "O", "B-country", "O", "O", "B-writer", "I-writer", "I-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, literary genre, magazine, book, person, event, award, location, poem, country, organization and O.\nSentence: Satirical poets outside England include Poland ' s Ignacy Krasicki , Azerbaijan ' s Mirza Alakbar Sabir and Portugal ' s Manuel Maria Barbosa du Bocage .", "prompt_labels": "Satirical(B-literary genre) poets(O) outside(O) England(B-country) include(O) Poland(B-country) '(O) s(O) Ignacy(B-writer) Krasicki(I-writer) ,(O) Azerbaijan(B-country) '(O) s(O) Mirza(B-writer) Alakbar(I-writer) Sabir(I-writer) and(O) Portugal(B-country) '(O) s(O) Manuel(B-writer) Maria(I-writer) Barbosa(I-writer) du(I-writer) Bocage(I-writer) .(O)"}}
{"id": "305", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "magazine", "country", "location", "book", "writer", "organization", "poem", "literary genre", "award", "event"], "instance": {"id": "305", "words": ["Le", "Guin", "influenced", "many", "other", "authors", ",", "including", "Booker", "Prize", "winner", "Salman", "Rushdie", ",", "David", "Mitchell", ",", "Neil", "Gaiman", ",", "and", "Iain", "Banks", "."], "labels": ["B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, magazine, country, location, book, writer, organization, poem, literary genre, award, event and O.\nSentence: Le Guin influenced many other authors , including Booker Prize winner Salman Rushdie , David Mitchell , Neil Gaiman , and Iain Banks .", "prompt_labels": "Le(B-writer) Guin(I-writer) influenced(O) many(O) other(O) authors(O) ,(O) including(O) Booker(B-award) Prize(I-award) winner(O) Salman(B-writer) Rushdie(I-writer) ,(O) David(B-writer) Mitchell(I-writer) ,(O) Neil(B-writer) Gaiman(I-writer) ,(O) and(O) Iain(B-writer) Banks(I-writer) .(O)"}}
{"id": "51", "dataset": "crossner_literature", "split": "dev", "label_list": ["poem", "magazine", "event", "literary genre", "person", "location", "country", "award", "writer", "organization", "book"], "instance": {"id": "51", "words": ["Moreover", ",", "he", "planned", "the", "publication", "of", "the", "compilation", "Nietzsche", "contra", "Wagner", "and", "of", "the", "poems", "that", "made", "up", "his", "collection", "Dionysian-Dithyrambs", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, magazine, event, literary genre, person, location, country, award, writer, organization, book and O.\nSentence: Moreover , he planned the publication of the compilation Nietzsche contra Wagner and of the poems that made up his collection Dionysian-Dithyrambs .", "prompt_labels": "Moreover(O) ,(O) he(O) planned(O) the(O) publication(O) of(O) the(O) compilation(O) Nietzsche(B-book) contra(I-book) Wagner(I-book) and(O) of(O) the(O) poems(O) that(O) made(O) up(O) his(O) collection(O) Dionysian-Dithyrambs(B-book) .(O)"}}
{"id": "127", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "award", "magazine", "literary genre", "poem", "location", "writer", "person", "organization", "country", "book"], "instance": {"id": "127", "words": ["Federico", "Fellini", ",", "Burke", "and", "Waller", ",", "12", "His", "films", "have", "ranked", "in", "polls", "such", "as", "Cahiers", "du", "cinéma", "and", "Sight", "&", "Sound", ",", "which", "lists", "his", "1963", "film", "8", "½", "as", "the", "10th-greatest", "film", "."], "labels": ["B-writer", "I-writer", "O", "B-writer", "O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "B-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, magazine, literary genre, poem, location, writer, person, organization, country, book and O.\nSentence: Federico Fellini , Burke and Waller , 12 His films have ranked in polls such as Cahiers du cinéma and Sight & Sound , which lists his 1963 film 8 ½ as the 10th-greatest film .", "prompt_labels": "Federico(B-writer) Fellini(I-writer) ,(O) Burke(B-writer) and(O) Waller(B-writer) ,(O) 12(O) His(O) films(O) have(O) ranked(O) in(O) polls(O) such(O) as(O) Cahiers(B-magazine) du(I-magazine) cinéma(I-magazine) and(O) Sight(B-magazine) &(I-magazine) Sound(I-magazine) ,(O) which(O) lists(O) his(O) 1963(O) film(O) 8(O) ½(O) as(O) the(O) 10th-greatest(O) film(O) .(O)"}}
{"id": "378", "dataset": "crossner_literature", "split": "dev", "label_list": ["writer", "book", "literary genre", "magazine", "poem", "location", "organization", "person", "event", "award", "country"], "instance": {"id": "378", "words": ["The", "Tale", "of", "Jemima", "Puddle-Duck", "and", "The", "Tale", "of", "Tom", "Kitten", "are", "representative", "of", "Hill", "Top", "Farm", "and", "her", "farming", "life", "and", "reflect", "her", "happiness", "with", "her", "country", "life.", "John", "Heelis", ",", "(", "1999", ")", "The", "Tale", "of", "Mrs", "William", "Heelis", "-", "Beatrix", "Potter", ";", "Lear", ",", "Ch", "."], "labels": ["B-book", "I-book", "I-book", "I-book", "I-book", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, literary genre, magazine, poem, location, organization, person, event, award, country and O.\nSentence: The Tale of Jemima Puddle-Duck and The Tale of Tom Kitten are representative of Hill Top Farm and her farming life and reflect her happiness with her country life. John Heelis , ( 1999 ) The Tale of Mrs William Heelis - Beatrix Potter ; Lear , Ch .", "prompt_labels": "The(B-book) Tale(I-book) of(I-book) Jemima(I-book) Puddle-Duck(I-book) and(O) The(B-book) Tale(I-book) of(I-book) Tom(I-book) Kitten(I-book) are(O) representative(O) of(O) Hill(B-location) Top(I-location) Farm(I-location) and(O) her(O) farming(O) life(O) and(O) reflect(O) her(O) happiness(O) with(O) her(O) country(O) life.(O) John(B-writer) Heelis(I-writer) ,(O) ((O) 1999(O) )(O) The(B-book) Tale(I-book) of(I-book) Mrs(I-book) William(I-book) Heelis(I-book) -(I-book) Beatrix(I-book) Potter(I-book) ;(O) Lear(O) ,(O) Ch(O) .(O)"}}
{"id": "180", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "writer", "event", "organization", "person", "poem", "literary genre", "award", "magazine", "book", "country"], "instance": {"id": "180", "words": ["Fredric", "Brown", "employed", "this", "subgenre", "to", "satirize", "the", "science", "fiction", "pulps", "and", "their", "adolescent", "readers", "-", "and", "fears", "of", "foreign", "invasion", "-", "in", "the", "classic", "What", "Mad", "Universe", "(", "1949", ")", "."], "labels": ["B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-literary genre", "I-literary genre", "I-literary genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, writer, event, organization, person, poem, literary genre, award, magazine, book, country and O.\nSentence: Fredric Brown employed this subgenre to satirize the science fiction pulps and their adolescent readers - and fears of foreign invasion - in the classic What Mad Universe ( 1949 ) .", "prompt_labels": "Fredric(B-writer) Brown(I-writer) employed(O) this(O) subgenre(O) to(O) satirize(O) the(O) science(B-literary genre) fiction(I-literary genre) pulps(I-literary genre) and(O) their(O) adolescent(O) readers(O) -(O) and(O) fears(O) of(O) foreign(O) invasion(O) -(O) in(O) the(O) classic(O) What(B-book) Mad(I-book) Universe(I-book) ((O) 1949(O) )(O) .(O)"}}
{"id": "79", "dataset": "crossner_literature", "split": "dev", "label_list": ["book", "person", "writer", "event", "location", "award", "poem", "country", "literary genre", "organization", "magazine"], "instance": {"id": "79", "words": ["He", "came", "to", "wide", "public", "attention", "with", "his", "first", "book", "Poems", "at", "the", "age", "of", "twenty-three", "in", "1930", ";", "it", "was", "followed", "in", "1932", "by", "The", "Orators", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-poem", "I-poem", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, person, writer, event, location, award, poem, country, literary genre, organization, magazine and O.\nSentence: He came to wide public attention with his first book Poems at the age of twenty-three in 1930 ; it was followed in 1932 by The Orators .", "prompt_labels": "He(O) came(O) to(O) wide(O) public(O) attention(O) with(O) his(O) first(O) book(O) Poems(B-poem) at(O) the(O) age(O) of(O) twenty-three(O) in(O) 1930(O) ;(O) it(O) was(O) followed(O) in(O) 1932(O) by(O) The(B-poem) Orators(I-poem) .(O)"}}
{"id": "4", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "location", "writer", "person", "poem", "book", "country", "award", "literary genre", "event", "organization"], "instance": {"id": "4", "words": ["In", "1998", ",", "Oldman", "was", "honored", "at", "the", "Camerimage", "Film", "Festival", ",", "where", "he", "was", "awarded", "the", "Krzysztof", "Kieślowski", "Award", "for", "Acting", ",", "the", "first", "recipient", "of", "the", "award", "."], "labels": ["O", "O", "O", "B-person", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, location, writer, person, poem, book, country, award, literary genre, event, organization and O.\nSentence: In 1998 , Oldman was honored at the Camerimage Film Festival , where he was awarded the Krzysztof Kieślowski Award for Acting , the first recipient of the award .", "prompt_labels": "In(O) 1998(O) ,(O) Oldman(B-person) was(O) honored(O) at(O) the(O) Camerimage(B-event) Film(I-event) Festival(I-event) ,(O) where(O) he(O) was(O) awarded(O) the(O) Krzysztof(B-award) Kieślowski(I-award) Award(I-award) for(I-award) Acting(I-award) ,(O) the(O) first(O) recipient(O) of(O) the(O) award(O) .(O)"}}
{"id": "393", "dataset": "crossner_literature", "split": "dev", "label_list": ["organization", "writer", "person", "location", "magazine", "book", "country", "literary genre", "event", "award", "poem"], "instance": {"id": "393", "words": ["In", "the", "early", "6th", "century", "BC", ",", "the", "Kingdom", "of", "Judah", "rebelled", "against", "the", "Neo-Babylonian", "Empire", "and", "was", "destroyed", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "I-country", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, writer, person, location, magazine, book, country, literary genre, event, award, poem and O.\nSentence: In the early 6th century BC , the Kingdom of Judah rebelled against the Neo-Babylonian Empire and was destroyed .", "prompt_labels": "In(O) the(O) early(O) 6th(O) century(O) BC(O) ,(O) the(O) Kingdom(B-country) of(I-country) Judah(I-country) rebelled(O) against(O) the(O) Neo-Babylonian(B-country) Empire(I-country) and(O) was(O) destroyed(O) .(O)"}}
{"id": "25", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "event", "poem", "book", "organization", "award", "country", "writer", "literary genre", "person", "magazine"], "instance": {"id": "25", "words": ["Russell", "was", "asked", "by", "The", "New", "Republic", ",", "a", "liberal", "American", "magazine", ",", "to", "elaborate", "his", "views", "on", "world", "peace", "."], "labels": ["B-writer", "O", "O", "O", "B-magazine", "I-magazine", "I-magazine", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, poem, book, organization, award, country, writer, literary genre, person, magazine and O.\nSentence: Russell was asked by The New Republic , a liberal American magazine , to elaborate his views on world peace .", "prompt_labels": "Russell(B-writer) was(O) asked(O) by(O) The(B-magazine) New(I-magazine) Republic(I-magazine) ,(O) a(O) liberal(O) American(O) magazine(O) ,(O) to(O) elaborate(O) his(O) views(O) on(O) world(O) peace(O) .(O)"}}
{"id": "171", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "book", "event", "person", "organization", "writer", "poem", "country", "literary genre", "magazine", "award"], "instance": {"id": "171", "words": ["But", "opponents", "of", "this", "proposition", "claim", "that", "Rabindranath", "Tagore", "mentioned", "only", "the", "border", "states", "of", "India", "to", "include", "complete", "India", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, book, event, person, organization, writer, poem, country, literary genre, magazine, award and O.\nSentence: But opponents of this proposition claim that Rabindranath Tagore mentioned only the border states of India to include complete India .", "prompt_labels": "But(O) opponents(O) of(O) this(O) proposition(O) claim(O) that(O) Rabindranath(B-writer) Tagore(I-writer) mentioned(O) only(O) the(O) border(O) states(O) of(O) India(B-country) to(O) include(O) complete(O) India(B-country) .(O)"}}
{"id": "70", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "location", "country", "magazine", "organization", "award", "literary genre", "writer", "poem", "book", "person"], "instance": {"id": "70", "words": ["Tarkovsky", "was", "the", "recipient", "of", "several", "awards", "at", "the", "Cannes", "Film", "Festival", "throughout", "his", "career", "(", "including", "the", "FIPRESCI", "prize", ",", "the", "Prize", "of", "the", "Ecumenical", "Jury", ",", "and", "the", "Grand", "Prix", "Spécial", "du", "Jury", ")", "and", "winner", "of", "the", "Golden", "Lion", "award", "at", "the", "Venice", "Film", "Festival", "for", "his", "debut", "film", "Ivan", "'s", "Childhood", "."], "labels": ["B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, country, magazine, organization, award, literary genre, writer, poem, book, person and O.\nSentence: Tarkovsky was the recipient of several awards at the Cannes Film Festival throughout his career ( including the FIPRESCI prize , the Prize of the Ecumenical Jury , and the Grand Prix Spécial du Jury ) and winner of the Golden Lion award at the Venice Film Festival for his debut film Ivan 's Childhood .", "prompt_labels": "Tarkovsky(B-writer) was(O) the(O) recipient(O) of(O) several(O) awards(O) at(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) throughout(O) his(O) career(O) ((O) including(O) the(O) FIPRESCI(B-award) prize(I-award) ,(O) the(O) Prize(B-award) of(I-award) the(I-award) Ecumenical(I-award) Jury(I-award) ,(O) and(O) the(O) Grand(B-award) Prix(I-award) Spécial(I-award) du(I-award) Jury(I-award) )(O) and(O) winner(O) of(O) the(O) Golden(B-award) Lion(I-award) award(I-award) at(O) the(O) Venice(B-event) Film(I-event) Festival(I-event) for(O) his(O) debut(O) film(O) Ivan(O) 's(O) Childhood(O) .(O)"}}
{"id": "19", "dataset": "crossner_literature", "split": "dev", "label_list": ["poem", "literary genre", "award", "organization", "magazine", "writer", "location", "book", "person", "event", "country"], "instance": {"id": "19", "words": ["When", "Davies", "retired", "from", "his", "position", "at", "the", "university", ",", "his", "seventh", "novel", ",", "a", "satire", "of", "academic", "life", ",", "The", "Rebel", "Angels", "(", "1981", ")", ",", "was", "published", ",", "followed", "by", "What", "'s", "Bred", "in", "the", "Bone", "(", "1985", ")", "which", "was", "short-listed", "for", "the", "Booker", "Prize", "for", "fiction", "in", "1986", "."], "labels": ["O", "B-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-literary genre", "O", "O", "B-literary genre", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, literary genre, award, organization, magazine, writer, location, book, person, event, country and O.\nSentence: When Davies retired from his position at the university , his seventh novel , a satire of academic life , The Rebel Angels ( 1981 ) , was published , followed by What 's Bred in the Bone ( 1985 ) which was short-listed for the Booker Prize for fiction in 1986 .", "prompt_labels": "When(O) Davies(B-writer) retired(O) from(O) his(O) position(O) at(O) the(O) university(O) ,(O) his(O) seventh(O) novel(B-literary genre) ,(O) a(O) satire(B-literary genre) of(O) academic(O) life(O) ,(O) The(B-book) Rebel(I-book) Angels(I-book) ((O) 1981(O) )(O) ,(O) was(O) published(O) ,(O) followed(O) by(O) What(B-book) 's(I-book) Bred(I-book) in(I-book) the(I-book) Bone(I-book) ((O) 1985(O) )(O) which(O) was(O) short-listed(O) for(O) the(O) Booker(B-award) Prize(I-award) for(I-award) fiction(I-award) in(O) 1986(O) .(O)"}}
{"id": "212", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "writer", "literary genre", "country", "book", "location", "event", "award", "poem", "magazine", "organization"], "instance": {"id": "212", "words": ["Albert", "Einstein", "was", "born", "in", "Ulm", ",", "in", "the", "Kingdom", "of", "Württemberg", "in", "the", "German", "Empire", ",", "on", "March", "14", ",", "1879", "."], "labels": ["B-person", "I-person", "O", "O", "O", "B-location", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, literary genre, country, book, location, event, award, poem, magazine, organization and O.\nSentence: Albert Einstein was born in Ulm , in the Kingdom of Württemberg in the German Empire , on March 14 , 1879 .", "prompt_labels": "Albert(B-person) Einstein(I-person) was(O) born(O) in(O) Ulm(B-location) ,(O) in(O) the(O) Kingdom(B-location) of(I-location) Württemberg(I-location) in(O) the(O) German(B-country) Empire(I-country) ,(O) on(O) March(O) 14(O) ,(O) 1879(O) .(O)"}}
{"id": "81", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "literary genre", "writer", "book", "country", "award", "organization", "event", "poem", "person", "magazine"], "instance": {"id": "81", "words": ["It", "was", "adapted", "by", "Talbot", "Jennings", ",", "Tess", "Slesinger", ",", "and", "Claudine", "West", "from", "the", "play", "by", "Owen", "Davis", "and", "Donald", "Davis", ",", "which", "was", "in", "itself", "based", "on", "the", "1931", "The", "Good", "Earth", "by", "Nobel", "Prize", "-winning", "author", "Pearl", "S.", "Buck", "."], "labels": ["O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "B-writer", "I-writer", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "I-book", "O", "B-award", "I-award", "O", "O", "B-writer", "I-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, literary genre, writer, book, country, award, organization, event, poem, person, magazine and O.\nSentence: It was adapted by Talbot Jennings , Tess Slesinger , and Claudine West from the play by Owen Davis and Donald Davis , which was in itself based on the 1931 The Good Earth by Nobel Prize -winning author Pearl S. Buck .", "prompt_labels": "It(O) was(O) adapted(O) by(O) Talbot(B-writer) Jennings(I-writer) ,(O) Tess(B-writer) Slesinger(I-writer) ,(O) and(O) Claudine(B-writer) West(I-writer) from(O) the(O) play(O) by(O) Owen(B-writer) Davis(I-writer) and(O) Donald(B-writer) Davis(I-writer) ,(O) which(O) was(O) in(O) itself(O) based(O) on(O) the(O) 1931(O) The(B-book) Good(I-book) Earth(I-book) by(O) Nobel(B-award) Prize(I-award) -winning(O) author(O) Pearl(B-writer) S.(I-writer) Buck(I-writer) .(O)"}}
{"id": "323", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "organization", "event", "award", "person", "writer", "country", "literary genre", "book", "magazine", "poem"], "instance": {"id": "323", "words": ["As", "novelist", "P.", "D.", "James", "observed", ",", "We", "can", "recognize", "the", "Prayer", "Book", "'s", "cadences", "in", "the", "works", "of", "Isaac", "Walton", "and", "John", "Bunyan", ",", "in", "the", "majestic", "phrases", "of", "John", "Milton", ",", "Sir", "Thomas", "Browne", "and", "Edward", "Gibbon", "."], "labels": ["O", "O", "B-writer", "I-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "I-writer", "O", "B-writer", "I-writer", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, event, award, person, writer, country, literary genre, book, magazine, poem and O.\nSentence: As novelist P. D. James observed , We can recognize the Prayer Book 's cadences in the works of Isaac Walton and John Bunyan , in the majestic phrases of John Milton , Sir Thomas Browne and Edward Gibbon .", "prompt_labels": "As(O) novelist(O) P.(B-writer) D.(I-writer) James(I-writer) observed(O) ,(O) We(O) can(O) recognize(O) the(O) Prayer(B-book) Book(I-book) 's(O) cadences(O) in(O) the(O) works(O) of(O) Isaac(B-writer) Walton(I-writer) and(O) John(B-writer) Bunyan(I-writer) ,(O) in(O) the(O) majestic(O) phrases(O) of(O) John(B-writer) Milton(I-writer) ,(O) Sir(B-writer) Thomas(I-writer) Browne(I-writer) and(O) Edward(B-writer) Gibbon(I-writer) .(O)"}}
{"id": "33", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "magazine", "award", "book", "literary genre", "poem", "country", "organization", "event", "writer", "location"], "instance": {"id": "33", "words": ["The", "poem", "is", "quoted", "by", "Sue", "Bridehead", "in", "Thomas", "Hardy", "'", "s", "1895", "novel", ",", "Jude", "the", "Obscure", "and", "also", "by", "Edward", "Ashburnham", "in", "Ford", "Madox", ".", "Ford", "'", "s", "The", "Good", "Soldier", "."], "labels": ["O", "B-literary genre", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "B-literary genre", "O", "B-book", "I-book", "I-book", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "B-writer", "O", "O", "B-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, magazine, award, book, literary genre, poem, country, organization, event, writer, location and O.\nSentence: The poem is quoted by Sue Bridehead in Thomas Hardy ' s 1895 novel , Jude the Obscure and also by Edward Ashburnham in Ford Madox . Ford ' s The Good Soldier .", "prompt_labels": "The(O) poem(B-literary genre) is(O) quoted(O) by(O) Sue(B-writer) Bridehead(I-writer) in(O) Thomas(B-writer) Hardy(I-writer) '(O) s(O) 1895(O) novel(B-literary genre) ,(O) Jude(B-book) the(I-book) Obscure(I-book) and(O) also(O) by(O) Edward(B-writer) Ashburnham(I-writer) in(O) Ford(B-writer) Madox(I-writer) .(O) Ford(B-writer) '(O) s(O) The(B-book) Good(I-book) Soldier(I-book) .(O)"}}
{"id": "120", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "award", "person", "location", "organization", "event", "literary genre", "writer", "country", "book", "poem"], "instance": {"id": "120", "words": ["The", "only", "completed", "screenplay", ",", "Heaven", ",", "was", "filmed", "by", "Tom", "Tykwer", "and", "premiered", "in", "2002", "at", "the", "Berlin", "International", "Film", "Festival", "."], "labels": ["O", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, award, person, location, organization, event, literary genre, writer, country, book, poem and O.\nSentence: The only completed screenplay , Heaven , was filmed by Tom Tykwer and premiered in 2002 at the Berlin International Film Festival .", "prompt_labels": "The(O) only(O) completed(O) screenplay(O) ,(O) Heaven(B-book) ,(O) was(O) filmed(O) by(O) Tom(B-person) Tykwer(I-person) and(O) premiered(O) in(O) 2002(O) at(O) the(O) Berlin(B-event) International(I-event) Film(I-event) Festival(I-event) .(O)"}}
{"id": "316", "dataset": "crossner_literature", "split": "dev", "label_list": ["award", "literary genre", "writer", "poem", "person", "magazine", "event", "country", "organization", "location", "book"], "instance": {"id": "316", "words": ["Hesse", "'s", "first", "great", "novel", ",", "Peter", "Camenzind", ",", "was", "received", "enthusiastically", "by", "young", "Germans", "desiring", "a", "different", "and", "more", "natural", "way", "of", "life", "in", "this", "time", "of", "great", "economic", "and", "technological", "progress", "in", "the", "country", "(", "see", "also", "Wandervogel", "movement", ")", ".Prinz", ",", "pp.", "139-42", "Demian", "had", "a", "strong", "and", "enduring", "influence", "on", "the", "generation", "returning", "home", "from", "the", "First", "World", "War", ".Zeller", ",", "p", "."], "labels": ["B-writer", "O", "O", "O", "B-literary genre", "O", "B-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "B-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, literary genre, writer, poem, person, magazine, event, country, organization, location, book and O.\nSentence: Hesse 's first great novel , Peter Camenzind , was received enthusiastically by young Germans desiring a different and more natural way of life in this time of great economic and technological progress in the country ( see also Wandervogel movement ) .Prinz , pp. 139-42 Demian had a strong and enduring influence on the generation returning home from the First World War .Zeller , p .", "prompt_labels": "Hesse(B-writer) 's(O) first(O) great(O) novel(B-literary genre) ,(O) Peter(B-book) Camenzind(I-book) ,(O) was(O) received(O) enthusiastically(O) by(O) young(O) Germans(O) desiring(O) a(O) different(O) and(O) more(O) natural(O) way(O) of(O) life(O) in(O) this(O) time(O) of(O) great(O) economic(O) and(O) technological(O) progress(O) in(O) the(O) country(O) ((O) see(O) also(O) Wandervogel(B-event) movement(I-event) )(O) .Prinz(O) ,(O) pp.(O) 139-42(O) Demian(B-book) had(O) a(O) strong(O) and(O) enduring(O) influence(O) on(O) the(O) generation(O) returning(O) home(O) from(O) the(O) First(B-event) World(I-event) War(I-event) .Zeller(O) ,(O) p(O) .(O)"}}
{"id": "38", "dataset": "crossner_literature", "split": "dev", "label_list": ["event", "location", "book", "writer", "country", "literary genre", "award", "poem", "organization", "magazine", "person"], "instance": {"id": "38", "words": ["Marsters", "moved", "to", "Chicago", ",", "where", "his", "first", "professional", "acting", "role", "was", "Ferdinand", "in", "The", "Tempest", "at", "the", "Goodman", "Theatre", "in", "1987", "."], "labels": ["B-person", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "O", "B-location", "I-location", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, book, writer, country, literary genre, award, poem, organization, magazine, person and O.\nSentence: Marsters moved to Chicago , where his first professional acting role was Ferdinand in The Tempest at the Goodman Theatre in 1987 .", "prompt_labels": "Marsters(B-person) moved(O) to(O) Chicago(B-location) ,(O) where(O) his(O) first(O) professional(O) acting(O) role(O) was(O) Ferdinand(O) in(O) The(B-book) Tempest(I-book) at(O) the(O) Goodman(B-location) Theatre(I-location) in(O) 1987(O) .(O)"}}
{"id": "299", "dataset": "crossner_literature", "split": "dev", "label_list": ["person", "award", "event", "location", "literary genre", "book", "organization", "magazine", "writer", "poem", "country"], "instance": {"id": "299", "words": ["She", "left", "the", "year", "after", "to", "teach", "her", "sisters", ",", "Emily", "Brontë", "and", "Anne", "Brontë", ",", "at", "home", ",", "returning", "in", "1835", "as", "a", "governess", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-writer", "I-writer", "O", "B-writer", "I-writer", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, event, location, literary genre, book, organization, magazine, writer, poem, country and O.\nSentence: She left the year after to teach her sisters , Emily Brontë and Anne Brontë , at home , returning in 1835 as a governess .", "prompt_labels": "She(O) left(O) the(O) year(O) after(O) to(O) teach(O) her(O) sisters(O) ,(O) Emily(B-writer) Brontë(I-writer) and(O) Anne(B-writer) Brontë(I-writer) ,(O) at(O) home(O) ,(O) returning(O) in(O) 1835(O) as(O) a(O) governess(O) .(O)"}}
{"id": "362", "dataset": "crossner_literature", "split": "dev", "label_list": ["magazine", "event", "organization", "person", "book", "award", "poem", "location", "country", "literary genre", "writer"], "instance": {"id": "362", "words": ["Kornbluth", "also", "wrote", "several", "novels", "under", "his", "own", "name", ",", "including", "The", "Syndic", "and", "Not", "This", "August", "."], "labels": ["B-writer", "O", "O", "O", "B-literary genre", "O", "O", "O", "O", "O", "O", "B-book", "I-book", "O", "B-book", "I-book", "I-book", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, event, organization, person, book, award, poem, location, country, literary genre, writer and O.\nSentence: Kornbluth also wrote several novels under his own name , including The Syndic and Not This August .", "prompt_labels": "Kornbluth(B-writer) also(O) wrote(O) several(O) novels(B-literary genre) under(O) his(O) own(O) name(O) ,(O) including(O) The(B-book) Syndic(I-book) and(O) Not(B-book) This(I-book) August(I-book) .(O)"}}
{"id": "219", "dataset": "crossner_literature", "split": "dev", "label_list": ["location", "event", "organization", "book", "magazine", "poem", "country", "award", "writer", "literary genre", "person"], "instance": {"id": "219", "words": ["Devil", "of", "a", "State", "is", "a", "follow-on", "to", "the", "trilogy", ",", "set", "in", "a", "fictionalised", "version", "of", "Brunei", "."], "labels": ["B-book", "I-book", "I-book", "I-book", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, organization, book, magazine, poem, country, award, writer, literary genre, person and O.\nSentence: Devil of a State is a follow-on to the trilogy , set in a fictionalised version of Brunei .", "prompt_labels": "Devil(B-book) of(I-book) a(I-book) State(I-book) is(O) a(O) follow-on(O) to(O) the(O) trilogy(O) ,(O) set(O) in(O) a(O) fictionalised(O) version(O) of(O) Brunei(B-country) .(O)"}}
{"id": "174", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "music genre", "musical artist", "organization", "location", "person", "country", "album", "musical instrument", "award", "song", "band"], "instance": {"id": "174", "words": ["He", "has", "appeared", "as", "a", "featured", "artist", "on", "many", "other", "songs", "and", "albums", ",", "having", "collaborated", "with", "artists", "such", "as", "Janet", "Jackson", ",", "Kool", "Moe", "Dee", ",", "The", "Dope", "Poet", "Society", ",", "Run-D.M.C.", ",", "Ice", "Cube", ",", "Boom", "Boom", "Satellites", ",", "Rage", "Against", "the", "Machine", ",", "Anthrax", ",", "John", "Mellencamp", "and", "many", "others", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, music genre, musical artist, organization, location, person, country, album, musical instrument, award, song, band and O.\nSentence: He has appeared as a featured artist on many other songs and albums , having collaborated with artists such as Janet Jackson , Kool Moe Dee , The Dope Poet Society , Run-D.M.C. , Ice Cube , Boom Boom Satellites , Rage Against the Machine , Anthrax , John Mellencamp and many others .", "prompt_labels": "He(O) has(O) appeared(O) as(O) a(O) featured(O) artist(O) on(O) many(O) other(O) songs(O) and(O) albums(O) ,(O) having(O) collaborated(O) with(O) artists(O) such(O) as(O) Janet(B-musical artist) Jackson(I-musical artist) ,(O) Kool(B-musical artist) Moe(I-musical artist) Dee(I-musical artist) ,(O) The(B-band) Dope(I-band) Poet(I-band) Society(I-band) ,(O) Run-D.M.C.(B-band) ,(O) Ice(B-musical artist) Cube(I-musical artist) ,(O) Boom(B-band) Boom(I-band) Satellites(I-band) ,(O) Rage(B-band) Against(I-band) the(I-band) Machine(I-band) ,(O) Anthrax(B-band) ,(O) John(B-musical artist) Mellencamp(I-musical artist) and(O) many(O) others(O) .(O)"}}
{"id": "226", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "organization", "musical instrument", "album", "band", "award", "country", "music genre", "song", "person", "event", "musical artist"], "instance": {"id": "226", "words": ["The", "album", "features", "covers", "of", "songs", "originally", "performed", "by", "The", "Who", ",", "Liz", "Phair", ",", "Creedence", "Clearwater", "Revival", ",", "Ryan", "Adams", ",", "I", "Blame", "Coco", ",", "and", "Led", "Zeppelin", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "I-band", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, musical instrument, album, band, award, country, music genre, song, person, event, musical artist and O.\nSentence: The album features covers of songs originally performed by The Who , Liz Phair , Creedence Clearwater Revival , Ryan Adams , I Blame Coco , and Led Zeppelin .", "prompt_labels": "The(O) album(O) features(O) covers(O) of(O) songs(O) originally(O) performed(O) by(O) The(B-band) Who(I-band) ,(O) Liz(B-musical artist) Phair(I-musical artist) ,(O) Creedence(B-band) Clearwater(I-band) Revival(I-band) ,(O) Ryan(B-musical artist) Adams(I-musical artist) ,(O) I(B-band) Blame(I-band) Coco(I-band) ,(O) and(O) Led(B-band) Zeppelin(I-band) .(O)"}}
{"id": "343", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "event", "musical artist", "country", "music genre", "person", "band", "award", "song", "album", "organization", "location"], "instance": {"id": "343", "words": ["Through", "extensive", "promoting", ",", "including", "appearances", "on", "1999", "MTV", "VMA", "(", "including", "a", "performance", "alongside", "Aerosmith", "and", "Run-DMC", ")", "and", "performing", "at", "Woodstock", "1999", ",", "Devil", "Without", "a", "Cause", "sold", "14", "million", "copies", ",", "the", "album", "'s", "success", "spurred", "by", "Kid", "Rock", "'s", "breakthrough", "hit", "single", "Bawitdaba", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "B-band", "O", "B-band", "O", "O", "O", "O", "B-event", "I-event", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "B-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, event, musical artist, country, music genre, person, band, award, song, album, organization, location and O.\nSentence: Through extensive promoting , including appearances on 1999 MTV VMA ( including a performance alongside Aerosmith and Run-DMC ) and performing at Woodstock 1999 , Devil Without a Cause sold 14 million copies , the album 's success spurred by Kid Rock 's breakthrough hit single Bawitdaba .", "prompt_labels": "Through(O) extensive(O) promoting(O) ,(O) including(O) appearances(O) on(O) 1999(O) MTV(B-award) VMA(I-award) ((O) including(O) a(O) performance(O) alongside(O) Aerosmith(B-band) and(O) Run-DMC(B-band) )(O) and(O) performing(O) at(O) Woodstock(B-event) 1999(I-event) ,(O) Devil(B-album) Without(I-album) a(I-album) Cause(I-album) sold(O) 14(O) million(O) copies(O) ,(O) the(O) album(O) 's(O) success(O) spurred(O) by(O) Kid(B-musical artist) Rock(I-musical artist) 's(O) breakthrough(O) hit(O) single(O) Bawitdaba(B-song) .(O)"}}
{"id": "101", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "event", "song", "album", "person", "musical artist", "country", "award", "music genre", "organization", "musical instrument", "band"], "instance": {"id": "101", "words": ["The", "film", "received", "several", "Golden", "Globe", "Awards", "and", "Academy", "Awards", "nominations", ",", "and", "earned", "Kidman", "a", "fourth", "Screen", "Actors", "Guild", "Award", "nomination", ",", "as", "part", "of", "the", "Screen", "Actors", "Guild", "Award", "for", "Outstanding", "Performance", "by", "a", "Cast", "in", "a", "Motion", "Picture", "."], "labels": ["O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, song, album, person, musical artist, country, award, music genre, organization, musical instrument, band and O.\nSentence: The film received several Golden Globe Awards and Academy Awards nominations , and earned Kidman a fourth Screen Actors Guild Award nomination , as part of the Screen Actors Guild Award for Outstanding Performance by a Cast in a Motion Picture .", "prompt_labels": "The(O) film(O) received(O) several(O) Golden(B-award) Globe(I-award) Awards(I-award) and(O) Academy(B-award) Awards(I-award) nominations(O) ,(O) and(O) earned(O) Kidman(O) a(O) fourth(O) Screen(B-award) Actors(I-award) Guild(I-award) Award(I-award) nomination(O) ,(O) as(O) part(O) of(O) the(O) Screen(B-award) Actors(I-award) Guild(I-award) Award(I-award) for(I-award) Outstanding(I-award) Performance(I-award) by(I-award) a(I-award) Cast(I-award) in(I-award) a(I-award) Motion(I-award) Picture(I-award) .(O)"}}
{"id": "115", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "organization", "event", "album", "person", "music genre", "song", "location", "country", "musical instrument", "musical artist", "award"], "instance": {"id": "115", "words": ["The", "Coliseum", "played", "host", "to", "Figure", "skating", "at", "the", "2010", "Winter", "Olympics", "and", "Short", "track", "speed", "skating", "at", "the", "2010", "Winter", "Olympics", "events", "for", "the", "XXI", "Olympic", "Winter", "Games", "in", "Vancouver", ",", "from", "February", "12", "to", "28", ",", "2010", "."], "labels": ["O", "B-location", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, organization, event, album, person, music genre, song, location, country, musical instrument, musical artist, award and O.\nSentence: The Coliseum played host to Figure skating at the 2010 Winter Olympics and Short track speed skating at the 2010 Winter Olympics events for the XXI Olympic Winter Games in Vancouver , from February 12 to 28 , 2010 .", "prompt_labels": "The(O) Coliseum(B-location) played(O) host(O) to(O) Figure(B-event) skating(I-event) at(I-event) the(I-event) 2010(I-event) Winter(I-event) Olympics(I-event) and(O) Short(B-event) track(I-event) speed(I-event) skating(I-event) at(I-event) the(I-event) 2010(I-event) Winter(I-event) Olympics(I-event) events(O) for(O) the(O) XXI(B-event) Olympic(I-event) Winter(I-event) Games(I-event) in(O) Vancouver(B-location) ,(O) from(O) February(O) 12(O) to(O) 28(O) ,(O) 2010(O) .(O)"}}
{"id": "133", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "award", "country", "event", "music genre", "musical artist", "person", "musical instrument", "location", "album", "song", "organization"], "instance": {"id": "133", "words": ["In", "2006", ",", "the", "Doha", "Asian", "Games", "Organising", "Committee", "has", "named", "Gary", "Valenciano", "the", "official", "performer", "of", "the", "2006", "Asian", "Games", "'", "theme", "song", ",", "Side", "By", "Side", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, award, country, event, music genre, musical artist, person, musical instrument, location, album, song, organization and O.\nSentence: In 2006 , the Doha Asian Games Organising Committee has named Gary Valenciano the official performer of the 2006 Asian Games ' theme song , Side By Side .", "prompt_labels": "In(O) 2006(O) ,(O) the(O) Doha(B-organization) Asian(I-organization) Games(I-organization) Organising(I-organization) Committee(I-organization) has(O) named(O) Gary(B-musical artist) Valenciano(I-musical artist) the(O) official(O) performer(O) of(O) the(O) 2006(B-event) Asian(I-event) Games(I-event) '(O) theme(O) song(O) ,(O) Side(B-song) By(I-song) Side(I-song) .(O)"}}
{"id": "61", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "band", "organization", "musical artist", "album", "music genre", "musical instrument", "event", "person", "country", "award", "location"], "instance": {"id": "61", "words": ["Other", "top-10", "entries", "from", "2015", "like", "Mark", "Ronson", "'", "s", "disco", "groove-infused", "Uptown", "Funk", ",", "Maroon", "5", "'", "s", "Sugar", ",", "the", "Weeknd", "'", "s", "Can", "'t", "Feel", "My", "Face", "and", "Jason", "Derulo", "'", "s", "Want", "to", "Want", "Me", "also", "ascended", "the", "charts", "and", "have", "a", "strong", "disco", "influence", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-song", "I-song", "B-song", "I-song", "O", "B-band", "I-band", "O", "O", "B-song", "O", "B-band", "I-band", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "B-musical artist", "I-musical artist", "O", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, band, organization, musical artist, album, music genre, musical instrument, event, person, country, award, location and O.\nSentence: Other top-10 entries from 2015 like Mark Ronson ' s disco groove-infused Uptown Funk , Maroon 5 ' s Sugar , the Weeknd ' s Can 't Feel My Face and Jason Derulo ' s Want to Want Me also ascended the charts and have a strong disco influence .", "prompt_labels": "Other(O) top-10(O) entries(O) from(O) 2015(O) like(O) Mark(B-musical artist) Ronson(I-musical artist) '(O) s(O) disco(B-song) groove-infused(I-song) Uptown(B-song) Funk(I-song) ,(O) Maroon(B-band) 5(I-band) '(O) s(O) Sugar(B-song) ,(O) the(B-band) Weeknd(I-band) '(O) s(O) Can(B-song) 't(I-song) Feel(I-song) My(I-song) Face(I-song) and(O) Jason(B-musical artist) Derulo(I-musical artist) '(O) s(O) Want(B-song) to(I-song) Want(I-song) Me(I-song) also(O) ascended(O) the(O) charts(O) and(O) have(O) a(O) strong(O) disco(B-music genre) influence(O) .(O)"}}
{"id": "92", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "music genre", "location", "musical artist", "country", "song", "band", "musical instrument", "album", "award", "event", "organization"], "instance": {"id": "92", "words": ["The", "5th", "Dimension", "is", "an", "United", "States", "popular", "music", "vocal", "group", ",", "whose", "repertoire", "includes", "Pop", "music", ",", "Rhythm", "and", "blues", ",", "Soul", "music", ",", "jazz", ",", "light", "opera", ",", "and", "Broadway", "-", "the", "melange", "was", "coined", "as", "Champagne", "Soul", "."], "labels": ["B-band", "I-band", "I-band", "O", "O", "B-country", "I-country", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, music genre, location, musical artist, country, song, band, musical instrument, album, award, event, organization and O.\nSentence: The 5th Dimension is an United States popular music vocal group , whose repertoire includes Pop music , Rhythm and blues , Soul music , jazz , light opera , and Broadway - the melange was coined as Champagne Soul .", "prompt_labels": "The(B-band) 5th(I-band) Dimension(I-band) is(O) an(O) United(B-country) States(I-country) popular(B-music genre) music(I-music genre) vocal(O) group(O) ,(O) whose(O) repertoire(O) includes(O) Pop(B-music genre) music(I-music genre) ,(O) Rhythm(B-music genre) and(I-music genre) blues(I-music genre) ,(O) Soul(B-music genre) music(I-music genre) ,(O) jazz(B-music genre) ,(O) light(B-music genre) opera(I-music genre) ,(O) and(O) Broadway(B-music genre) -(O) the(O) melange(O) was(O) coined(O) as(O) Champagne(B-music genre) Soul(I-music genre) .(O)"}}
{"id": "376", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "musical artist", "award", "album", "person", "band", "organization", "event", "location", "song", "country", "musical instrument"], "instance": {"id": "376", "words": ["Critics", "noted", "the", "album", "'s", "hybrid", "of", "various", "styles", "including", "Rock", "music", ",", "Hip", "hop", "music", ",", "Folk", "music", ",", "blues", ",", "psychedelia", ",", "and", "Country", "music", ","], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical artist, award, album, person, band, organization, event, location, song, country, musical instrument and O.\nSentence: Critics noted the album 's hybrid of various styles including Rock music , Hip hop music , Folk music , blues , psychedelia , and Country music ,", "prompt_labels": "Critics(O) noted(O) the(O) album(O) 's(O) hybrid(O) of(O) various(O) styles(O) including(O) Rock(B-music genre) music(I-music genre) ,(O) Hip(B-music genre) hop(I-music genre) music(I-music genre) ,(O) Folk(B-music genre) music(I-music genre) ,(O) blues(B-music genre) ,(O) psychedelia(B-music genre) ,(O) and(O) Country(B-music genre) music(I-music genre) ,(O)"}}
{"id": "18", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "person", "location", "band", "song", "event", "album", "organization", "music genre", "award", "musical artist", "musical instrument"], "instance": {"id": "18", "words": ["Andrews", "has", "won", "an", "Academy", "Award", ",", "a", "BAFTA", ",", "five", "Golden", "Globes", ",", "three", "Grammy", "Award", ",", "two", "Emmy", "Award", ",", "the", "AFI", "Life", "Achievement", "Award", ",", "the", "Screen", "Actors", "Guild", "Life", "Achievement", "Award", ",", "the", "Kennedy", "Center", "Honors", "Award", ",", "and", "the", "Disney", "Legends", "Award", "."], "labels": ["B-person", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, location, band, song, event, album, organization, music genre, award, musical artist, musical instrument and O.\nSentence: Andrews has won an Academy Award , a BAFTA , five Golden Globes , three Grammy Award , two Emmy Award , the AFI Life Achievement Award , the Screen Actors Guild Life Achievement Award , the Kennedy Center Honors Award , and the Disney Legends Award .", "prompt_labels": "Andrews(B-person) has(O) won(O) an(O) Academy(B-award) Award(I-award) ,(O) a(O) BAFTA(B-award) ,(O) five(O) Golden(B-award) Globes(I-award) ,(O) three(O) Grammy(B-award) Award(I-award) ,(O) two(O) Emmy(B-award) Award(I-award) ,(O) the(O) AFI(B-award) Life(I-award) Achievement(I-award) Award(I-award) ,(O) the(O) Screen(B-award) Actors(I-award) Guild(I-award) Life(I-award) Achievement(I-award) Award(I-award) ,(O) the(O) Kennedy(B-award) Center(I-award) Honors(I-award) Award(I-award) ,(O) and(O) the(O) Disney(B-award) Legends(I-award) Award(I-award) .(O)"}}
{"id": "326", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "event", "band", "location", "musical artist", "song", "musical instrument", "music genre", "person", "album", "award", "country"], "instance": {"id": "326", "words": ["He", "owned", "trophies", "for", "Male", "Vocalist", "of", "the", "Year", "from", "both", "the", "Country", "Music", "Association", "(", "CMA", ")", "and", "the", "Academy", "of", "Country", "Music", "(", "ACM", ")", ",", "and", "took", "the", "CMA", "'s", "top", "award", "as", "1968", "Entertainer", "of", "the", "Year", "."], "labels": ["O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, band, location, musical artist, song, musical instrument, music genre, person, album, award, country and O.\nSentence: He owned trophies for Male Vocalist of the Year from both the Country Music Association ( CMA ) and the Academy of Country Music ( ACM ) , and took the CMA 's top award as 1968 Entertainer of the Year .", "prompt_labels": "He(O) owned(O) trophies(O) for(O) Male(B-award) Vocalist(I-award) of(I-award) the(I-award) Year(I-award) from(O) both(O) the(O) Country(B-organization) Music(I-organization) Association(I-organization) ((O) CMA(B-organization) )(O) and(O) the(O) Academy(B-organization) of(I-organization) Country(I-organization) Music(I-organization) ((O) ACM(B-organization) )(O) ,(O) and(O) took(O) the(O) CMA(B-organization) 's(O) top(O) award(O) as(O) 1968(O) Entertainer(B-award) of(I-award) the(I-award) Year(I-award) .(O)"}}
{"id": "5", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "band", "person", "event", "music genre", "country", "location", "musical artist", "musical instrument", "organization", "song", "award"], "instance": {"id": "5", "words": ["His", "style", "incorporates", "elements", "of", "Rock", "music", ",", "blues", ",", "Soul", "music", ",", "R", "&", "B", ",", "funk", ",", "jazz", ",", "reggae", ",", "hard", "rock", ",", "Psychedelic", "rock", ",", "Pop", "music", ",", "Folk", "music", ",", "and", "ballads", "."], "labels": ["O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, band, person, event, music genre, country, location, musical artist, musical instrument, organization, song, award and O.\nSentence: His style incorporates elements of Rock music , blues , Soul music , R & B , funk , jazz , reggae , hard rock , Psychedelic rock , Pop music , Folk music , and ballads .", "prompt_labels": "His(O) style(O) incorporates(O) elements(O) of(O) Rock(B-music genre) music(I-music genre) ,(O) blues(B-music genre) ,(O) Soul(B-music genre) music(I-music genre) ,(O) R(B-music genre) &(I-music genre) B(I-music genre) ,(O) funk(B-music genre) ,(O) jazz(B-music genre) ,(O) reggae(B-music genre) ,(O) hard(B-music genre) rock(I-music genre) ,(O) Psychedelic(B-music genre) rock(I-music genre) ,(O) Pop(B-music genre) music(I-music genre) ,(O) Folk(B-music genre) music(I-music genre) ,(O) and(O) ballads(B-music genre) .(O)"}}
{"id": "218", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "album", "organization", "event", "music genre", "award", "location", "country", "song", "musical artist", "person", "musical instrument"], "instance": {"id": "218", "words": ["139", "with", "Curley", "Weaver", ",", "Tampa", "Red", ",", "Barbecue", "Bob", "and", "Kokomo", "Arnold", "as", "representatives", "of", "this", "style", "."], "labels": ["O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, album, organization, event, music genre, award, location, country, song, musical artist, person, musical instrument and O.\nSentence: 139 with Curley Weaver , Tampa Red , Barbecue Bob and Kokomo Arnold as representatives of this style .", "prompt_labels": "139(O) with(O) Curley(B-musical artist) Weaver(I-musical artist) ,(O) Tampa(B-musical artist) Red(I-musical artist) ,(O) Barbecue(B-musical artist) Bob(I-musical artist) and(O) Kokomo(B-musical artist) Arnold(I-musical artist) as(O) representatives(O) of(O) this(O) style(O) .(O)"}}
{"id": "268", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "musical instrument", "location", "album", "musical artist", "organization", "event", "song", "country", "person", "award", "band"], "instance": {"id": "268", "words": ["In", "April", "2005", ",", "members", "of", "the", "band", "visited", "Israel", "and", "the", "Palestinian", "territories", "with", "the", "UK", "charity", "War", "on", "Want", ";"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-location", "I-location", "O", "O", "B-country", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical instrument, location, album, musical artist, organization, event, song, country, person, award, band and O.\nSentence: In April 2005 , members of the band visited Israel and the Palestinian territories with the UK charity War on Want ;", "prompt_labels": "In(O) April(O) 2005(O) ,(O) members(O) of(O) the(O) band(O) visited(O) Israel(B-country) and(O) the(O) Palestinian(B-location) territories(I-location) with(O) the(O) UK(B-country) charity(O) War(B-organization) on(I-organization) Want(I-organization) ;(O)"}}
{"id": "123", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "event", "music genre", "location", "person", "musical artist", "award", "musical instrument", "organization", "band", "country", "album"], "instance": {"id": "123", "words": ["Styles", "like", "Rock", "music", ",", "Heavy", "metal", "music", ",", "Pop", "rock", ",", "Folk", "rock", ",", "Neo-Romantic", ",", "Pop", "and", "some", "experimental", "styles", "(", "e.g.", "Cantorock", ")", "were", "introduced", "."], "labels": ["O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, music genre, location, person, musical artist, award, musical instrument, organization, band, country, album and O.\nSentence: Styles like Rock music , Heavy metal music , Pop rock , Folk rock , Neo-Romantic , Pop and some experimental styles ( e.g. Cantorock ) were introduced .", "prompt_labels": "Styles(O) like(O) Rock(B-music genre) music(I-music genre) ,(O) Heavy(B-music genre) metal(I-music genre) music(I-music genre) ,(O) Pop(B-music genre) rock(I-music genre) ,(O) Folk(B-music genre) rock(I-music genre) ,(O) Neo-Romantic(B-music genre) ,(O) Pop(B-music genre) and(O) some(O) experimental(O) styles(O) ((O) e.g.(O) Cantorock(B-music genre) )(O) were(O) introduced(O) .(O)"}}
{"id": "202", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "song", "music genre", "event", "person", "band", "album", "location", "organization", "award", "musical instrument", "musical artist"], "instance": {"id": "202", "words": ["During", "the", "1948", "Summer", "Olympics", ",", "the", "venue", "hosted", "the", "Olympic", "boxing", ",", "Diving", "at", "the", "1948", "Summer", "Olympics", ",", "Swimming", "at", "the", "1948", "Summer", "Olympics", ",", "and", "Water", "polo", "at", "the", "1948", "Summer", "Olympics", "events.", "pp.", "43", ",", "49", "."], "labels": ["O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, song, music genre, event, person, band, album, location, organization, award, musical instrument, musical artist and O.\nSentence: During the 1948 Summer Olympics , the venue hosted the Olympic boxing , Diving at the 1948 Summer Olympics , Swimming at the 1948 Summer Olympics , and Water polo at the 1948 Summer Olympics events. pp. 43 , 49 .", "prompt_labels": "During(O) the(O) 1948(B-event) Summer(I-event) Olympics(I-event) ,(O) the(O) venue(O) hosted(O) the(O) Olympic(B-event) boxing(I-event) ,(O) Diving(B-event) at(I-event) the(I-event) 1948(I-event) Summer(I-event) Olympics(I-event) ,(O) Swimming(B-event) at(I-event) the(I-event) 1948(I-event) Summer(I-event) Olympics(I-event) ,(O) and(O) Water(B-event) polo(I-event) at(I-event) the(I-event) 1948(I-event) Summer(I-event) Olympics(I-event) events.(O) pp.(O) 43(O) ,(O) 49(O) .(O)"}}
{"id": "310", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "musical artist", "organization", "award", "country", "person", "location", "event", "music genre", "band", "musical instrument", "song"], "instance": {"id": "310", "words": ["Sangeet", "Natak", "Akademi", "is", "the", "national", "level", "academy", "for", "performing", "arts", "set", "up", "by", "the", "Government", "of", "India", "in", "1952", ",", "which", "bestows", "Sangeet", "Natak", "Akademi", "Award", "as", "the", "highest", "official", "Indian", "goernment", "'s", "recognition", "given", "to", "practicing", "artists", ",", "Sattriya", "Centre", ",", "Kathak", "Kendra", "(", "National", "Institute", "of", "Kathak", "Dance", ")", "at", "New", "Delhi", ",", "Centre", "for", "Kutiyattam", "at", "Thiruvananthapuram", ",", "Chhau", "Centre", "at", "Baripada", "in", "Jamshedpur", ",", "and", "the", "Northeast", "Centre", "."], "labels": ["B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-location", "I-location", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "O", "B-organization", "I-organization", "O", "B-location", "O", "B-location", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical artist, organization, award, country, person, location, event, music genre, band, musical instrument, song and O.\nSentence: Sangeet Natak Akademi is the national level academy for performing arts set up by the Government of India in 1952 , which bestows Sangeet Natak Akademi Award as the highest official Indian goernment 's recognition given to practicing artists , Sattriya Centre , Kathak Kendra ( National Institute of Kathak Dance ) at New Delhi , Centre for Kutiyattam at Thiruvananthapuram , Chhau Centre at Baripada in Jamshedpur , and the Northeast Centre .", "prompt_labels": "Sangeet(B-organization) Natak(I-organization) Akademi(I-organization) is(O) the(O) national(O) level(O) academy(O) for(O) performing(O) arts(O) set(O) up(O) by(O) the(O) Government(O) of(O) India(B-country) in(O) 1952(O) ,(O) which(O) bestows(O) Sangeet(B-award) Natak(I-award) Akademi(I-award) Award(I-award) as(O) the(O) highest(O) official(O) Indian(O) goernment(O) 's(O) recognition(O) given(O) to(O) practicing(O) artists(O) ,(O) Sattriya(B-organization) Centre(I-organization) ,(O) Kathak(B-organization) Kendra(I-organization) ((O) National(B-organization) Institute(I-organization) of(I-organization) Kathak(I-organization) Dance(I-organization) )(O) at(O) New(B-location) Delhi(I-location) ,(O) Centre(B-organization) for(I-organization) Kutiyattam(I-organization) at(O) Thiruvananthapuram(B-location) ,(O) Chhau(B-organization) Centre(I-organization) at(O) Baripada(B-location) in(O) Jamshedpur(B-location) ,(O) and(O) the(O) Northeast(B-organization) Centre(I-organization) .(O)"}}
{"id": "339", "dataset": "crossner_music", "split": "dev", "label_list": ["musical artist", "song", "award", "organization", "person", "location", "band", "album", "event", "country", "music genre", "musical instrument"], "instance": {"id": "339", "words": ["Lengthy", "sleeve", "notes", "are", "included", "with", "the", "albums", "Black", "Seeds", "of", "Vengeance", ",", "In", "Their", "Darkened", "Shrines", ",", "Annihilation", "of", "the", "Wicked", ",", "Those", "Whom", "the", "Gods", "Detest", ",", "At", "the", "Gate", "of", "Sethu", ",", "and", "What", "Should", "Not", "Be", "Unearthed", ",", "explaining", "the", "inspiration", "or", "source", "for", "the", "lyrics", "of", "each", "song", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, song, award, organization, person, location, band, album, event, country, music genre, musical instrument and O.\nSentence: Lengthy sleeve notes are included with the albums Black Seeds of Vengeance , In Their Darkened Shrines , Annihilation of the Wicked , Those Whom the Gods Detest , At the Gate of Sethu , and What Should Not Be Unearthed , explaining the inspiration or source for the lyrics of each song .", "prompt_labels": "Lengthy(O) sleeve(O) notes(O) are(O) included(O) with(O) the(O) albums(O) Black(B-album) Seeds(I-album) of(I-album) Vengeance(I-album) ,(O) In(B-album) Their(I-album) Darkened(I-album) Shrines(I-album) ,(O) Annihilation(B-album) of(I-album) the(I-album) Wicked(I-album) ,(O) Those(B-album) Whom(I-album) the(I-album) Gods(I-album) Detest(I-album) ,(O) At(B-album) the(I-album) Gate(I-album) of(I-album) Sethu(I-album) ,(O) and(O) What(B-album) Should(I-album) Not(I-album) Be(I-album) Unearthed(I-album) ,(O) explaining(O) the(O) inspiration(O) or(O) source(O) for(O) the(O) lyrics(O) of(O) each(O) song(O) .(O)"}}
{"id": "184", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "music genre", "musical instrument", "person", "country", "organization", "musical artist", "location", "award", "song", "album", "band"], "instance": {"id": "184", "words": ["The", "album", "Jazz", "from", "Hell", ",", "released", "in", "1986", ",", "earned", "Zappa", "his", "first", "Grammy", "Award", "in", "1988", "for", "Grammy", "Award", "for", "Best", "Rock", "Instrumental", "Performance", "."], "labels": ["O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, music genre, musical instrument, person, country, organization, musical artist, location, award, song, album, band and O.\nSentence: The album Jazz from Hell , released in 1986 , earned Zappa his first Grammy Award in 1988 for Grammy Award for Best Rock Instrumental Performance .", "prompt_labels": "The(O) album(O) Jazz(B-album) from(I-album) Hell(I-album) ,(O) released(O) in(O) 1986(O) ,(O) earned(O) Zappa(B-musical artist) his(O) first(O) Grammy(B-award) Award(I-award) in(O) 1988(O) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Rock(I-award) Instrumental(I-award) Performance(I-award) .(O)"}}
{"id": "108", "dataset": "crossner_music", "split": "dev", "label_list": ["musical artist", "event", "music genre", "band", "person", "album", "musical instrument", "location", "song", "country", "organization", "award"], "instance": {"id": "108", "words": ["The", "accordion", "has", "been", "used", "by", "tropipop", "musicians", "such", "as", "Carlos", "Vives", ",", "Andrés", "Cabas", ",", "Fonseca", "(", "singer", ")", "and", "Bacilos", ",", "as", "well", "as", "rock", "musicians", "such", "as", "Juanes", "and", "pop", "musicians", "as", "Shakira", "."], "labels": ["O", "B-musical instrument", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "O", "O", "O", "O", "B-band", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "B-musical artist", "O", "B-music genre", "O", "O", "B-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, event, music genre, band, person, album, musical instrument, location, song, country, organization, award and O.\nSentence: The accordion has been used by tropipop musicians such as Carlos Vives , Andrés Cabas , Fonseca ( singer ) and Bacilos , as well as rock musicians such as Juanes and pop musicians as Shakira .", "prompt_labels": "The(O) accordion(B-musical instrument) has(O) been(O) used(O) by(O) tropipop(B-music genre) musicians(O) such(O) as(O) Carlos(B-musical artist) Vives(I-musical artist) ,(O) Andrés(B-musical artist) Cabas(I-musical artist) ,(O) Fonseca(B-musical artist) ((O) singer(O) )(O) and(O) Bacilos(B-band) ,(O) as(O) well(O) as(O) rock(B-music genre) musicians(O) such(O) as(O) Juanes(B-musical artist) and(O) pop(B-music genre) musicians(O) as(O) Shakira(B-musical artist) .(O)"}}
{"id": "251", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "event", "album", "location", "band", "country", "song", "person", "organization", "award", "musical artist", "musical instrument"], "instance": {"id": "251", "words": ["Three", "albums", ",", "which", "again", "saw", "Moorcock", "contributing", "lyrics", "and", "vocals", ",", "were", "recorded", "for", "RCA", "/", "Active", ":", "Sonic", "Attack", ",", "the", "electronic", "Church", "of", "Hawkwind", "and", "Choose", "Your", "Masques", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "B-album", "I-album", "O", "O", "O", "B-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, event, album, location, band, country, song, person, organization, award, musical artist, musical instrument and O.\nSentence: Three albums , which again saw Moorcock contributing lyrics and vocals , were recorded for RCA / Active : Sonic Attack , the electronic Church of Hawkwind and Choose Your Masques .", "prompt_labels": "Three(O) albums(O) ,(O) which(O) again(O) saw(O) Moorcock(B-musical artist) contributing(O) lyrics(O) and(O) vocals(O) ,(O) were(O) recorded(O) for(O) RCA(B-album) /(I-album) Active(I-album) :(O) Sonic(B-album) Attack(I-album) ,(O) the(O) electronic(O) Church(B-album) of(I-album) Hawkwind(I-album) and(O) Choose(B-album) Your(I-album) Masques(I-album) .(O)"}}
{"id": "88", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "musical artist", "person", "award", "country", "event", "song", "music genre", "location", "organization", "musical instrument", "band"], "instance": {"id": "88", "words": ["Miles", "Davis", "on", "the", "roster", ",", "and", "his", "late", "1960s", "recordings", ",", "In", "a", "Silent", "Way", "and", "Bitches", "Brew", ",", "pioneered", "a", "Jazz", "fusion", "."], "labels": ["B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "O", "O", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical artist, person, award, country, event, song, music genre, location, organization, musical instrument, band and O.\nSentence: Miles Davis on the roster , and his late 1960s recordings , In a Silent Way and Bitches Brew , pioneered a Jazz fusion .", "prompt_labels": "Miles(B-musical artist) Davis(I-musical artist) on(O) the(O) roster(O) ,(O) and(O) his(O) late(O) 1960s(O) recordings(O) ,(O) In(B-album) a(I-album) Silent(I-album) Way(I-album) and(O) Bitches(B-album) Brew(I-album) ,(O) pioneered(O) a(O) Jazz(B-music genre) fusion(I-music genre) .(O)"}}
{"id": "191", "dataset": "crossner_music", "split": "dev", "label_list": ["musical artist", "location", "musical instrument", "country", "event", "album", "person", "organization", "song", "music genre", "band", "award"], "instance": {"id": "191", "words": ["It", "was", "designed", "by", "Kenzo", "Tange", "and", "built", "between", "1961", "and", "1964", "to", "house", "Swimming", "at", "the", "1964", "Summer", "Olympics", "and", "Diving", "at", "the", "1964", "Summer", "Olympics", "events", "in", "the", "1964", "Summer", "Olympics", "."], "labels": ["O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, musical instrument, country, event, album, person, organization, song, music genre, band, award and O.\nSentence: It was designed by Kenzo Tange and built between 1961 and 1964 to house Swimming at the 1964 Summer Olympics and Diving at the 1964 Summer Olympics events in the 1964 Summer Olympics .", "prompt_labels": "It(O) was(O) designed(O) by(O) Kenzo(B-person) Tange(I-person) and(O) built(O) between(O) 1961(O) and(O) 1964(O) to(O) house(O) Swimming(O) at(O) the(O) 1964(B-event) Summer(I-event) Olympics(I-event) and(O) Diving(O) at(O) the(O) 1964(B-event) Summer(I-event) Olympics(I-event) events(O) in(O) the(O) 1964(B-event) Summer(I-event) Olympics(I-event) .(O)"}}
{"id": "242", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "location", "person", "musical instrument", "event", "song", "country", "musical artist", "organization", "award", "band", "album"], "instance": {"id": "242", "words": ["Bedrock", "Records", "is", "an", "English", "record", "label", "for", "trance", ",", "house", "and", "techno", "started", "by", "Nick", "Muir", "and", "John", "Digweed", "."], "labels": ["B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, location, person, musical instrument, event, song, country, musical artist, organization, award, band, album and O.\nSentence: Bedrock Records is an English record label for trance , house and techno started by Nick Muir and John Digweed .", "prompt_labels": "Bedrock(B-organization) Records(I-organization) is(O) an(O) English(O) record(O) label(O) for(O) trance(B-music genre) ,(O) house(B-music genre) and(O) techno(B-music genre) started(O) by(O) Nick(B-musical artist) Muir(I-musical artist) and(O) John(B-musical artist) Digweed(I-musical artist) .(O)"}}
{"id": "294", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "song", "music genre", "organization", "musical artist", "musical instrument", "event", "award", "band", "country", "album", "location"], "instance": {"id": "294", "words": ["In", "April", "2018", ",", "Roach", "performed", "at", "the", "2018", "Commonwealth", "Games", "Closing", "Ceremony", "on", "the", "Gold", "Coast", "with", "Amy", "Shark", "."], "labels": ["O", "O", "O", "O", "B-musical artist", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "B-location", "I-location", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, song, music genre, organization, musical artist, musical instrument, event, award, band, country, album, location and O.\nSentence: In April 2018 , Roach performed at the 2018 Commonwealth Games Closing Ceremony on the Gold Coast with Amy Shark .", "prompt_labels": "In(O) April(O) 2018(O) ,(O) Roach(B-musical artist) performed(O) at(O) the(O) 2018(B-event) Commonwealth(I-event) Games(I-event) Closing(I-event) Ceremony(I-event) on(O) the(O) Gold(B-location) Coast(I-location) with(O) Amy(B-musical artist) Shark(I-musical artist) .(O)"}}
{"id": "98", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "person", "event", "award", "song", "location", "album", "musical instrument", "musical artist", "music genre", "organization", "band"], "instance": {"id": "98", "words": ["Lynn", "has", "received", "numerous", "awards", "and", "other", "accolades", "for", "her", "groundbreaking", "role", "in", "country", "music", ",", "including", "awards", "from", "both", "the", "Country", "Music", "Association", "and", "Academy", "of", "Country", "Music", "as", "a", "duet", "partner", "and", "an", "individual", "artist", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, event, award, song, location, album, musical instrument, musical artist, music genre, organization, band and O.\nSentence: Lynn has received numerous awards and other accolades for her groundbreaking role in country music , including awards from both the Country Music Association and Academy of Country Music as a duet partner and an individual artist .", "prompt_labels": "Lynn(B-musical artist) has(O) received(O) numerous(O) awards(O) and(O) other(O) accolades(O) for(O) her(O) groundbreaking(O) role(O) in(O) country(B-music genre) music(I-music genre) ,(O) including(O) awards(O) from(O) both(O) the(O) Country(B-organization) Music(I-organization) Association(I-organization) and(O) Academy(B-organization) of(I-organization) Country(I-organization) Music(I-organization) as(O) a(O) duet(O) partner(O) and(O) an(O) individual(O) artist(O) .(O)"}}
{"id": "249", "dataset": "crossner_music", "split": "dev", "label_list": ["musical artist", "event", "musical instrument", "organization", "band", "song", "country", "award", "person", "music genre", "album", "location"], "instance": {"id": "249", "words": ["The", "Family", "Values", "Tour", "was", "created", "by", "the", "American", "nu", "metal", "band", "Korn", "in", "1998", "to", "be", "an", "annual", "Rock", "music", "and", "Hip", "hop", "music", "tour", "."], "labels": ["O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-band", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, event, musical instrument, organization, band, song, country, award, person, music genre, album, location and O.\nSentence: The Family Values Tour was created by the American nu metal band Korn in 1998 to be an annual Rock music and Hip hop music tour .", "prompt_labels": "The(O) Family(B-event) Values(I-event) Tour(I-event) was(O) created(O) by(O) the(O) American(O) nu(B-music genre) metal(I-music genre) band(O) Korn(B-band) in(O) 1998(O) to(O) be(O) an(O) annual(O) Rock(B-music genre) music(I-music genre) and(O) Hip(B-music genre) hop(I-music genre) music(I-music genre) tour(O) .(O)"}}
{"id": "345", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "award", "song", "person", "location", "album", "musical instrument", "event", "country", "music genre", "musical artist", "organization"], "instance": {"id": "345", "words": ["225", "Other", "influential", "bands", "identified", "with", "metal", "emerged", "in", "the", "U.S.", ",", "such", "as", "Sir", "Lord", "Baltimore", "(", "Kingdom", "Come", ",", "1970", ")", ",", "Blue", "Öyster", "Cult", "(", "Blue", "Öyster", "Cult", ",", "1972", ")", ",", "Aerosmith", "(", "Aerosmith", ",", "1973", ")", "and", "Kiss", "(", "Kiss", ",", "1974", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "B-country", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-album", "I-album", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "B-band", "O", "B-album", "O", "O", "O", "O", "B-band", "O", "B-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, award, song, person, location, album, musical instrument, event, country, music genre, musical artist, organization and O.\nSentence: 225 Other influential bands identified with metal emerged in the U.S. , such as Sir Lord Baltimore ( Kingdom Come , 1970 ) , Blue Öyster Cult ( Blue Öyster Cult , 1972 ) , Aerosmith ( Aerosmith , 1973 ) and Kiss ( Kiss , 1974 ) .", "prompt_labels": "225(O) Other(O) influential(O) bands(O) identified(O) with(O) metal(B-music genre) emerged(O) in(O) the(O) U.S.(B-country) ,(O) such(O) as(O) Sir(B-band) Lord(I-band) Baltimore(I-band) ((O) Kingdom(B-album) Come(I-album) ,(O) 1970(O) )(O) ,(O) Blue(B-band) Öyster(I-band) Cult(I-band) ((O) Blue(B-album) Öyster(I-album) Cult(I-album) ,(O) 1972(O) )(O) ,(O) Aerosmith(B-band) ((O) Aerosmith(B-album) ,(O) 1973(O) )(O) and(O) Kiss(B-band) ((O) Kiss(B-album) ,(O) 1974(O) )(O) .(O)"}}
{"id": "365", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "musical instrument", "award", "music genre", "album", "country", "event", "organization", "location", "person", "song", "musical artist"], "instance": {"id": "365", "words": ["Leading", "artists", "in", "this", "genre", "included", "Jim", "Reeves", ",", "Skeeter", "Davis", ",", "Connie", "Smith", ",", "The", "Browns", ",", "The", "Nashville", "Sound", "collapsed", "in", "mainstream", "popularity", "in", "1964", ",", "a", "victim", "of", "both", "the", "British", "Invasion", "and", "the", "deaths", "of", "Reeves", "and", "Cline", "in", "separate", "airplane", "crashes", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-person", "I-person", "O", "B-band", "I-band", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "B-musical artist", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical instrument, award, music genre, album, country, event, organization, location, person, song, musical artist and O.\nSentence: Leading artists in this genre included Jim Reeves , Skeeter Davis , Connie Smith , The Browns , The Nashville Sound collapsed in mainstream popularity in 1964 , a victim of both the British Invasion and the deaths of Reeves and Cline in separate airplane crashes .", "prompt_labels": "Leading(O) artists(O) in(O) this(O) genre(O) included(O) Jim(B-musical artist) Reeves(I-musical artist) ,(O) Skeeter(B-musical artist) Davis(I-musical artist) ,(O) Connie(B-person) Smith(I-person) ,(O) The(B-band) Browns(I-band) ,(O) The(B-music genre) Nashville(I-music genre) Sound(I-music genre) collapsed(O) in(O) mainstream(O) popularity(O) in(O) 1964(O) ,(O) a(O) victim(O) of(O) both(O) the(O) British(O) Invasion(O) and(O) the(O) deaths(O) of(O) Reeves(B-musical artist) and(O) Cline(B-musical artist) in(O) separate(O) airplane(O) crashes(O) .(O)"}}
{"id": "29", "dataset": "crossner_music", "split": "dev", "label_list": ["musical artist", "musical instrument", "award", "person", "location", "country", "album", "music genre", "event", "organization", "band", "song"], "instance": {"id": "29", "words": ["In", "central", "Europe", ",", "Italo", "disco", "(", "a.k.a.", "1980s", "Euro", "disco", ")", "and", "Euro", "house", "were", "the", "predominant", "attempts", "by", "young", "musicians", "to", "have", "a", "hit", "record", "in", "and", "beyond", "the", "borders", "of", "their", "own", "country", "."], "labels": ["O", "B-location", "I-location", "O", "B-music genre", "I-music genre", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, musical instrument, award, person, location, country, album, music genre, event, organization, band, song and O.\nSentence: In central Europe , Italo disco ( a.k.a. 1980s Euro disco ) and Euro house were the predominant attempts by young musicians to have a hit record in and beyond the borders of their own country .", "prompt_labels": "In(O) central(B-location) Europe(I-location) ,(O) Italo(B-music genre) disco(I-music genre) ((O) a.k.a.(O) 1980s(O) Euro(B-music genre) disco(I-music genre) )(O) and(O) Euro(B-music genre) house(I-music genre) were(O) the(O) predominant(O) attempts(O) by(O) young(O) musicians(O) to(O) have(O) a(O) hit(O) record(O) in(O) and(O) beyond(O) the(O) borders(O) of(O) their(O) own(O) country(O) .(O)"}}
{"id": "165", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "band", "country", "music genre", "organization", "song", "musical artist", "award", "album", "musical instrument", "location", "event"], "instance": {"id": "165", "words": ["Top", "artists", "included", "Tammy", "Wynette", ",", "Lynn", "Anderson", "and", "Charlie", "Rich", ",", "as", "well", "as", "such", "former", "hard", "country", "artists", "as", "Ray", "Price", "and", "Marty", "Robbins", "."], "labels": ["O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, band, country, music genre, organization, song, musical artist, award, album, musical instrument, location, event and O.\nSentence: Top artists included Tammy Wynette , Lynn Anderson and Charlie Rich , as well as such former hard country artists as Ray Price and Marty Robbins .", "prompt_labels": "Top(O) artists(O) included(O) Tammy(B-musical artist) Wynette(I-musical artist) ,(O) Lynn(B-musical artist) Anderson(I-musical artist) and(O) Charlie(B-musical artist) Rich(I-musical artist) ,(O) as(O) well(O) as(O) such(O) former(O) hard(B-music genre) country(I-music genre) artists(O) as(O) Ray(B-musical artist) Price(I-musical artist) and(O) Marty(B-musical artist) Robbins(I-musical artist) .(O)"}}
{"id": "139", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "event", "album", "award", "location", "person", "song", "music genre", "musical instrument", "country", "band", "musical artist"], "instance": {"id": "139", "words": ["For", "the", "majority", "of", "its", "existence", ",", "the", "group", "was", "composed", "of", "Mike", "D", "(", "vocals", ",", "drums", ")", ",", "Adam", "Yauch", "(", "vocals", ",", "bass", ")", "and", "Ad-Rock", "(", "vocals", ",", "guitar", ",", "programming", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "B-musical instrument", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "B-musical instrument", "O", "O", "B-musical artist", "O", "O", "O", "B-musical instrument", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, album, award, location, person, song, music genre, musical instrument, country, band, musical artist and O.\nSentence: For the majority of its existence , the group was composed of Mike D ( vocals , drums ) , Adam Yauch ( vocals , bass ) and Ad-Rock ( vocals , guitar , programming ) .", "prompt_labels": "For(O) the(O) majority(O) of(O) its(O) existence(O) ,(O) the(O) group(O) was(O) composed(O) of(O) Mike(B-musical artist) D(I-musical artist) ((O) vocals(O) ,(O) drums(B-musical instrument) )(O) ,(O) Adam(B-musical artist) Yauch(I-musical artist) ((O) vocals(O) ,(O) bass(B-musical instrument) )(O) and(O) Ad-Rock(B-musical artist) ((O) vocals(O) ,(O) guitar(B-musical instrument) ,(O) programming(O) )(O) .(O)"}}
{"id": "122", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "country", "musical artist", "album", "organization", "person", "music genre", "musical instrument", "song", "band", "event", "location"], "instance": {"id": "122", "words": ["Kraftwerk", "'s", "musical", "style", "and", "image", "can", "be", "heard", "and", "seen", "in", "1980s", "synthpop", "groups", "such", "as", "Gary", "Numan", ",", "Ultravox", ",", "John", "Foxx", ",", "Orchestral", "Manoeuvres", "in", "the", "Dark", ",", "Human", "League", ",", "Depeche", "Mode", ",", "Visage", ",", "and", "Soft", "Cell", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, musical artist, album, organization, person, music genre, musical instrument, song, band, event, location and O.\nSentence: Kraftwerk 's musical style and image can be heard and seen in 1980s synthpop groups such as Gary Numan , Ultravox , John Foxx , Orchestral Manoeuvres in the Dark , Human League , Depeche Mode , Visage , and Soft Cell .", "prompt_labels": "Kraftwerk(B-musical artist) 's(O) musical(O) style(O) and(O) image(O) can(O) be(O) heard(O) and(O) seen(O) in(O) 1980s(O) synthpop(B-music genre) groups(O) such(O) as(O) Gary(B-musical artist) Numan(I-musical artist) ,(O) Ultravox(B-band) ,(O) John(B-musical artist) Foxx(I-musical artist) ,(O) Orchestral(B-band) Manoeuvres(I-band) in(I-band) the(I-band) Dark(I-band) ,(O) Human(B-band) League(I-band) ,(O) Depeche(B-band) Mode(I-band) ,(O) Visage(B-band) ,(O) and(O) Soft(B-band) Cell(I-band) .(O)"}}
{"id": "272", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "event", "musical artist", "award", "musical instrument", "person", "country", "band", "music genre", "organization", "song", "location"], "instance": {"id": "272", "words": ["Their", "first", "three", "black", "metal", "albums", "-", "A", "Blaze", "in", "the", "Northern", "Sky", "(", "1992", ")", ",", "Under", "a", "Funeral", "Moon", "(", "1993", ")", "and", "Transilvanian", "Hunger", "(", "1994", ")", "-", "are", "sometimes", "dubbed", "the", "Unholy", "Trinity", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, event, musical artist, award, musical instrument, person, country, band, music genre, organization, song, location and O.\nSentence: Their first three black metal albums - A Blaze in the Northern Sky ( 1992 ) , Under a Funeral Moon ( 1993 ) and Transilvanian Hunger ( 1994 ) - are sometimes dubbed the Unholy Trinity .", "prompt_labels": "Their(O) first(O) three(O) black(O) metal(O) albums(O) -(O) A(B-album) Blaze(I-album) in(I-album) the(I-album) Northern(I-album) Sky(I-album) ((O) 1992(O) )(O) ,(O) Under(B-album) a(I-album) Funeral(I-album) Moon(I-album) ((O) 1993(O) )(O) and(O) Transilvanian(B-album) Hunger(I-album) ((O) 1994(O) )(O) -(O) are(O) sometimes(O) dubbed(O) the(O) Unholy(O) Trinity(O) .(O)"}}
{"id": "253", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "album", "country", "location", "person", "event", "musical artist", "song", "award", "music genre", "organization", "musical instrument"], "instance": {"id": "253", "words": ["Iommi", "called", "vocalist", "Dave", "Walker", ",", "a", "longtime", "friend", "of", "the", "band", ",", "who", "had", "previously", "been", "a", "member", "of", "Fleetwood", "Mac", "and", "Savoy", "Brown", ",", "and", "informed", "him", "that", "Osbourne", "had", "left", "the", "band", "."], "labels": ["B-musical artist", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, album, country, location, person, event, musical artist, song, award, music genre, organization, musical instrument and O.\nSentence: Iommi called vocalist Dave Walker , a longtime friend of the band , who had previously been a member of Fleetwood Mac and Savoy Brown , and informed him that Osbourne had left the band .", "prompt_labels": "Iommi(B-musical artist) called(O) vocalist(O) Dave(B-musical artist) Walker(I-musical artist) ,(O) a(O) longtime(O) friend(O) of(O) the(O) band(O) ,(O) who(O) had(O) previously(O) been(O) a(O) member(O) of(O) Fleetwood(B-band) Mac(I-band) and(O) Savoy(B-band) Brown(I-band) ,(O) and(O) informed(O) him(O) that(O) Osbourne(B-musical artist) had(O) left(O) the(O) band(O) .(O)"}}
{"id": "265", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "musical artist", "organization", "album", "musical instrument", "song", "person", "music genre", "award", "country", "band", "location"], "instance": {"id": "265", "words": ["The", "game", "features", "the", "full", "albums", "of", "Dookie", ",", "American", "Idiot", ",", "and", "21st", "Century", "Breakdown", "as", "well", "as", "select", "songs", "from", "the", "rest", "of", "Green", "Day", "'s", "discography", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-album", "O", "B-album", "I-album", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical artist, organization, album, musical instrument, song, person, music genre, award, country, band, location and O.\nSentence: The game features the full albums of Dookie , American Idiot , and 21st Century Breakdown as well as select songs from the rest of Green Day 's discography .", "prompt_labels": "The(O) game(O) features(O) the(O) full(O) albums(O) of(O) Dookie(B-album) ,(O) American(B-album) Idiot(I-album) ,(O) and(O) 21st(B-album) Century(I-album) Breakdown(I-album) as(O) well(O) as(O) select(O) songs(O) from(O) the(O) rest(O) of(O) Green(B-band) Day(I-band) 's(O) discography(O) .(O)"}}
{"id": "136", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "song", "country", "person", "band", "music genre", "album", "location", "musical instrument", "award", "event", "musical artist"], "instance": {"id": "136", "words": ["Dalida", "employed", "various", "musical", "styles", "ranging", "from", "Pop", "music", "and", "easy", "listening", "to", "disco", "and", "adult", "contemporary", ",", "employing", "even", "Folk", "music", "and", "Rock", "music", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, song, country, person, band, music genre, album, location, musical instrument, award, event, musical artist and O.\nSentence: Dalida employed various musical styles ranging from Pop music and easy listening to disco and adult contemporary , employing even Folk music and Rock music .", "prompt_labels": "Dalida(B-musical artist) employed(O) various(O) musical(O) styles(O) ranging(O) from(O) Pop(B-music genre) music(I-music genre) and(O) easy(B-music genre) listening(I-music genre) to(O) disco(B-music genre) and(O) adult(B-music genre) contemporary(I-music genre) ,(O) employing(O) even(O) Folk(B-music genre) music(I-music genre) and(O) Rock(B-music genre) music(I-music genre) .(O)"}}
{"id": "295", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "organization", "album", "musical artist", "location", "country", "band", "music genre", "award", "person", "musical instrument", "event"], "instance": {"id": "295", "words": ["He", "was", "an", "immediate", "success", "at", "the", "Charing", "Cross", "Music", "Hall", "and", "the", "London", "Pavilion", ",", "venues", "at", "which", "the", "theatrical", "paper", "The", "Era", "reported", "that", "he", "had", "generated", "great", "furore", "among", "his", "audiences", "with", "three", "of", "his", "self-composed", "songs", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, organization, album, musical artist, location, country, band, music genre, award, person, musical instrument, event and O.\nSentence: He was an immediate success at the Charing Cross Music Hall and the London Pavilion , venues at which the theatrical paper The Era reported that he had generated great furore among his audiences with three of his self-composed songs .", "prompt_labels": "He(O) was(O) an(O) immediate(O) success(O) at(O) the(O) Charing(B-location) Cross(I-location) Music(I-location) Hall(I-location) and(O) the(O) London(B-location) Pavilion(I-location) ,(O) venues(O) at(O) which(O) the(O) theatrical(O) paper(O) The(O) Era(O) reported(O) that(O) he(O) had(O) generated(O) great(O) furore(O) among(O) his(O) audiences(O) with(O) three(O) of(O) his(O) self-composed(O) songs(O) .(O)"}}
{"id": "56", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "song", "location", "organization", "musical artist", "award", "country", "album", "event", "person", "band", "musical instrument"], "instance": {"id": "56", "words": ["Subsequent", "releases", ",", "such", "as", "The", "Force", "Behind", "the", "Power", "(", "1991", ")", ",", "Take", "Me", "Higher", "(", "1995", ")", ",", "and", "Every", "Day", "Is", "a", "New", "Day", "(", "1999", ")", "produced", "similar", "results", "."], "labels": ["O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, song, location, organization, musical artist, award, country, album, event, person, band, musical instrument and O.\nSentence: Subsequent releases , such as The Force Behind the Power ( 1991 ) , Take Me Higher ( 1995 ) , and Every Day Is a New Day ( 1999 ) produced similar results .", "prompt_labels": "Subsequent(O) releases(O) ,(O) such(O) as(O) The(B-album) Force(I-album) Behind(I-album) the(I-album) Power(I-album) ((O) 1991(O) )(O) ,(O) Take(B-album) Me(I-album) Higher(I-album) ((O) 1995(O) )(O) ,(O) and(O) Every(B-album) Day(I-album) Is(I-album) a(I-album) New(I-album) Day(I-album) ((O) 1999(O) )(O) produced(O) similar(O) results(O) .(O)"}}
{"id": "117", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "musical artist", "location", "country", "song", "musical instrument", "organization", "album", "band", "music genre", "event", "person"], "instance": {"id": "117", "words": ["Jones", "supports", "a", "number", "of", "other", "charities", ",", "including", "the", "NAACP", ",", "GLAAD", ",", "Peace", "Games", ",", "AmfAR", ",", "and", "the", "Maybach", "Foundation", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "B-organization", "O", "B-organization", "I-organization", "O", "B-organization", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical artist, location, country, song, musical instrument, organization, album, band, music genre, event, person and O.\nSentence: Jones supports a number of other charities , including the NAACP , GLAAD , Peace Games , AmfAR , and the Maybach Foundation .", "prompt_labels": "Jones(B-musical artist) supports(O) a(O) number(O) of(O) other(O) charities(O) ,(O) including(O) the(O) NAACP(B-organization) ,(O) GLAAD(B-organization) ,(O) Peace(B-organization) Games(I-organization) ,(O) AmfAR(B-organization) ,(O) and(O) the(O) Maybach(B-organization) Foundation(I-organization) .(O)"}}
{"id": "207", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "event", "song", "musical artist", "country", "organization", "band", "location", "person", "musical instrument", "album", "music genre"], "instance": {"id": "207", "words": ["In", "the", "twentieth", "century", ",", "Christian", "music", "has", "developed", "to", "reflect", "the", "emergence", "of", "a", "diverse", "array", "of", "musical", "genres", "including", "Rock", "music", ",", "metal", ",", "Pop", "music", ",", "jazz", ",", "Contemporary", "Christian", "music", ",", "rap", ",", "spiritual", ",", "Country", "music", ",", "blues", ",", "and", "Gospel", "music", "."], "labels": ["O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, song, musical artist, country, organization, band, location, person, musical instrument, album, music genre and O.\nSentence: In the twentieth century , Christian music has developed to reflect the emergence of a diverse array of musical genres including Rock music , metal , Pop music , jazz , Contemporary Christian music , rap , spiritual , Country music , blues , and Gospel music .", "prompt_labels": "In(O) the(O) twentieth(O) century(O) ,(O) Christian(B-music genre) music(I-music genre) has(O) developed(O) to(O) reflect(O) the(O) emergence(O) of(O) a(O) diverse(O) array(O) of(O) musical(O) genres(O) including(O) Rock(B-music genre) music(I-music genre) ,(O) metal(B-music genre) ,(O) Pop(B-music genre) music(I-music genre) ,(O) jazz(B-music genre) ,(O) Contemporary(B-music genre) Christian(I-music genre) music(I-music genre) ,(O) rap(B-music genre) ,(O) spiritual(B-music genre) ,(O) Country(B-music genre) music(I-music genre) ,(O) blues(B-music genre) ,(O) and(O) Gospel(B-music genre) music(I-music genre) .(O)"}}
{"id": "250", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "organization", "album", "music genre", "person", "band", "musical instrument", "musical artist", "award", "location", "song", "event"], "instance": {"id": "250", "words": ["Autechre", "released", "three", "records", "in", "1997", ":", "the", "full", "length", "Chiastic", "Slide", ",", "and", "the", "EPs", "Envane", ",", "and", "Cichlisuite", "(", "pronounced", "sickly", "sweet", ")", "."], "labels": ["B-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-album", "O", "O", "B-album", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, album, music genre, person, band, musical instrument, musical artist, award, location, song, event and O.\nSentence: Autechre released three records in 1997 : the full length Chiastic Slide , and the EPs Envane , and Cichlisuite ( pronounced sickly sweet ) .", "prompt_labels": "Autechre(B-band) released(O) three(O) records(O) in(O) 1997(O) :(O) the(O) full(O) length(O) Chiastic(B-album) Slide(I-album) ,(O) and(O) the(O) EPs(O) Envane(B-album) ,(O) and(O) Cichlisuite(B-album) ((O) pronounced(O) sickly(O) sweet(O) )(O) .(O)"}}
{"id": "252", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "song", "country", "band", "event", "award", "person", "album", "location", "musical artist", "organization", "music genre"], "instance": {"id": "252", "words": ["He", "liked", "to", "surround", "himself", "with", "jazz", "musicians", "such", "as", "John", "McLaughlin", "and", "Binky", "McKenzie", "and", "often", "performed", "with", "a", "horn", "section", "drawn", "from", "a", "pool", "that", "included", ",", "among", "others", ",", "saxophone", "players", "Art", "Themen", ",", "Mel", "Collins", ",", "Dick", "Heckstall-Smith", ",", "Lol", "Coxhill", ",", "Dick", "Morrissey", ",", "John", "Surman", "and", "trombonist", "Mike", "Zwerin", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-musical instrument", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical instrument", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, song, country, band, event, award, person, album, location, musical artist, organization, music genre and O.\nSentence: He liked to surround himself with jazz musicians such as John McLaughlin and Binky McKenzie and often performed with a horn section drawn from a pool that included , among others , saxophone players Art Themen , Mel Collins , Dick Heckstall-Smith , Lol Coxhill , Dick Morrissey , John Surman and trombonist Mike Zwerin .", "prompt_labels": "He(O) liked(O) to(O) surround(O) himself(O) with(O) jazz(B-music genre) musicians(O) such(O) as(O) John(B-musical artist) McLaughlin(I-musical artist) and(O) Binky(B-musical artist) McKenzie(I-musical artist) and(O) often(O) performed(O) with(O) a(O) horn(B-musical instrument) section(O) drawn(O) from(O) a(O) pool(O) that(O) included(O) ,(O) among(O) others(O) ,(O) saxophone(B-musical instrument) players(O) Art(B-musical artist) Themen(I-musical artist) ,(O) Mel(B-musical artist) Collins(I-musical artist) ,(O) Dick(B-musical artist) Heckstall-Smith(I-musical artist) ,(O) Lol(B-musical artist) Coxhill(I-musical artist) ,(O) Dick(B-musical artist) Morrissey(I-musical artist) ,(O) John(B-musical artist) Surman(I-musical artist) and(O) trombonist(O) Mike(B-musical artist) Zwerin(I-musical artist) .(O)"}}
{"id": "33", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "musical artist", "music genre", "person", "organization", "award", "song", "musical instrument", "event", "album", "location", "band"], "instance": {"id": "33", "words": ["In", "2007", ",", "Standing", "on", "the", "Outside", ":", "The", "Songs", "of", "Cold", "Chisel", "was", "released", ",", "featuring", "a", "collection", "of", "the", "band", "'s", "songs", "as", "performed", "by", "artists", "including", "The", "Living", "End", ",", "Evermore", ",", "Something", "for", "Kate", ",", "Pete", "Murray", ",", "Katie", "Noonan", ",", "You", "Am", "I", ",", "Paul", "Kelly", ",", "Alex", "Lloyd", ",", "Thirsty", "Merc", "and", "Ben", "Lee", ","], "labels": ["O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, musical artist, music genre, person, organization, award, song, musical instrument, event, album, location, band and O.\nSentence: In 2007 , Standing on the Outside : The Songs of Cold Chisel was released , featuring a collection of the band 's songs as performed by artists including The Living End , Evermore , Something for Kate , Pete Murray , Katie Noonan , You Am I , Paul Kelly , Alex Lloyd , Thirsty Merc and Ben Lee ,", "prompt_labels": "In(O) 2007(O) ,(O) Standing(B-album) on(I-album) the(I-album) Outside(I-album) :(I-album) The(I-album) Songs(I-album) of(I-album) Cold(I-album) Chisel(I-album) was(O) released(O) ,(O) featuring(O) a(O) collection(O) of(O) the(O) band(O) 's(O) songs(O) as(O) performed(O) by(O) artists(O) including(O) The(B-band) Living(I-band) End(I-band) ,(O) Evermore(B-band) ,(O) Something(B-band) for(I-band) Kate(I-band) ,(O) Pete(B-band) Murray(I-band) ,(O) Katie(B-musical artist) Noonan(I-musical artist) ,(O) You(B-band) Am(I-band) I(I-band) ,(O) Paul(B-musical artist) Kelly(I-musical artist) ,(O) Alex(B-band) Lloyd(I-band) ,(O) Thirsty(B-band) Merc(I-band) and(O) Ben(B-musical artist) Lee(I-musical artist) ,(O)"}}
{"id": "306", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "organization", "musical instrument", "country", "award", "album", "event", "location", "music genre", "musical artist", "band", "person"], "instance": {"id": "306", "words": ["Gurney", "spent", "the", "last", "15", "years", "of", "his", "life", "in", "psychiatric", "hospital", "s", ",", "first", "for", "a", "short", "period", "at", "Barnwood", "House", "Hospital", "in", "Gloucester", ",", "and", "then", "at", "the", "Stone", "House", "Hospital", ",", "Dartford", ",", "where", "he", "was", "diagnosed", "as", "suffering", "from", "delusional", "insanity", "(", "systematised", ")", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, organization, musical instrument, country, award, album, event, location, music genre, musical artist, band, person and O.\nSentence: Gurney spent the last 15 years of his life in psychiatric hospital s , first for a short period at Barnwood House Hospital in Gloucester , and then at the Stone House Hospital , Dartford , where he was diagnosed as suffering from delusional insanity ( systematised ) .", "prompt_labels": "Gurney(B-musical artist) spent(O) the(O) last(O) 15(O) years(O) of(O) his(O) life(O) in(O) psychiatric(O) hospital(O) s(O) ,(O) first(O) for(O) a(O) short(O) period(O) at(O) Barnwood(B-location) House(I-location) Hospital(I-location) in(O) Gloucester(B-location) ,(O) and(O) then(O) at(O) the(O) Stone(B-location) House(I-location) Hospital(I-location) ,(O) Dartford(B-location) ,(O) where(O) he(O) was(O) diagnosed(O) as(O) suffering(O) from(O) delusional(O) insanity(O) ((O) systematised(O) )(O) .(O)"}}
{"id": "358", "dataset": "crossner_music", "split": "dev", "label_list": ["musical artist", "musical instrument", "person", "event", "band", "music genre", "country", "award", "location", "song", "album", "organization"], "instance": {"id": "358", "words": ["The", "Elephant", "6", "Recording", "Company", "(", "or", "simply", "Elephant", "6", ")", "is", "a", "collective", "of", "American", "musicians", "that", "spawned", "many", "notable", "independent", "bands", "of", "the", "1990s", ",", "including", "the", "Apples", "in", "Stereo", ",", "the", "Olivia", "Tremor", "Control", ",", "Neutral", "Milk", "Hotel", ",", "Beulah", ",", "Elf", "Power", ",", "of", "Montreal", ",", "The", "Minders", "and", "Circulatory", "System", "."], "labels": ["B-band", "I-band", "I-band", "I-band", "I-band", "O", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, musical instrument, person, event, band, music genre, country, award, location, song, album, organization and O.\nSentence: The Elephant 6 Recording Company ( or simply Elephant 6 ) is a collective of American musicians that spawned many notable independent bands of the 1990s , including the Apples in Stereo , the Olivia Tremor Control , Neutral Milk Hotel , Beulah , Elf Power , of Montreal , The Minders and Circulatory System .", "prompt_labels": "The(B-band) Elephant(I-band) 6(I-band) Recording(I-band) Company(I-band) ((O) or(O) simply(O) Elephant(B-band) 6(I-band) )(O) is(O) a(O) collective(O) of(O) American(O) musicians(O) that(O) spawned(O) many(O) notable(O) independent(O) bands(O) of(O) the(O) 1990s(O) ,(O) including(O) the(B-band) Apples(I-band) in(I-band) Stereo(I-band) ,(O) the(B-band) Olivia(I-band) Tremor(I-band) Control(I-band) ,(O) Neutral(B-band) Milk(I-band) Hotel(I-band) ,(O) Beulah(B-band) ,(O) Elf(B-band) Power(I-band) ,(O) of(B-band) Montreal(I-band) ,(O) The(B-band) Minders(I-band) and(O) Circulatory(B-band) System(I-band) .(O)"}}
{"id": "79", "dataset": "crossner_music", "split": "dev", "label_list": ["musical artist", "person", "band", "musical instrument", "song", "location", "organization", "music genre", "event", "album", "award", "country"], "instance": {"id": "79", "words": ["Funds", "raised", "by", "the", "project", "will", "go", "to", "Amazon", "Watch", "and", "Extinction", "Rebellion", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, person, band, musical instrument, song, location, organization, music genre, event, album, award, country and O.\nSentence: Funds raised by the project will go to Amazon Watch and Extinction Rebellion .", "prompt_labels": "Funds(O) raised(O) by(O) the(O) project(O) will(O) go(O) to(O) Amazon(B-organization) Watch(I-organization) and(O) Extinction(B-organization) Rebellion(I-organization) .(O)"}}
{"id": "273", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "person", "album", "music genre", "event", "song", "award", "band", "country", "location", "musical artist", "organization"], "instance": {"id": "273", "words": ["This", "was", "followed", "by", "performances", "at", "the", "Teatro", "Valli", "in", "Reggio", "Emilia", ",", "Italy", ";", "the", "Barbican", "Centre", ",", "London", ";", "the", "Sony", "Centre", "for", "the", "Performing", "Arts", ",", "Toronto", ";", "the", "Brooklyn", "Academy", "of", "Music", "Brooklyn", ",", "New", "York", ";", "Zellerbach", "Hall", "at", "the", "University", "of", "California", ",", "Berkeley", ",", "California", ";", "the", "Teatro", "del", "Palacio", "de", "Bellas", "Artes", ",", "Mexico", "City", ":", "in", "2013", ",", "at", "Het", "Muziektheater", "/", "De", "Nederlandse", "Opera", ",", "Amsterdam", ";"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "B-country", "O", "O", "B-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "I-location", "I-location", "B-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "B-location", "O", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "I-location", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "I-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, person, album, music genre, event, song, award, band, country, location, musical artist, organization and O.\nSentence: This was followed by performances at the Teatro Valli in Reggio Emilia , Italy ; the Barbican Centre , London ; the Sony Centre for the Performing Arts , Toronto ; the Brooklyn Academy of Music Brooklyn , New York ; Zellerbach Hall at the University of California , Berkeley , California ; the Teatro del Palacio de Bellas Artes , Mexico City : in 2013 , at Het Muziektheater / De Nederlandse Opera , Amsterdam ;", "prompt_labels": "This(O) was(O) followed(O) by(O) performances(O) at(O) the(O) Teatro(B-location) Valli(I-location) in(O) Reggio(B-location) Emilia(I-location) ,(O) Italy(B-country) ;(O) the(O) Barbican(B-location) Centre(I-location) ,(O) London(B-location) ;(O) the(O) Sony(B-location) Centre(I-location) for(I-location) the(I-location) Performing(I-location) Arts(I-location) ,(O) Toronto(B-location) ;(O) the(O) Brooklyn(B-location) Academy(I-location) of(I-location) Music(I-location) Brooklyn(B-location) ,(O) New(B-location) York(I-location) ;(O) Zellerbach(B-location) Hall(I-location) at(O) the(O) University(B-location) of(I-location) California(I-location) ,(O) Berkeley(B-location) ,(O) California(B-location) ;(O) the(O) Teatro(B-location) del(I-location) Palacio(I-location) de(I-location) Bellas(I-location) Artes(I-location) ,(O) Mexico(B-location) City(I-location) :(O) in(O) 2013(O) ,(O) at(O) Het(B-location) Muziektheater(I-location) /(O) De(B-location) Nederlandse(I-location) Opera(I-location) ,(O) Amsterdam(B-location) ;(O)"}}
{"id": "216", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "song", "musical instrument", "country", "organization", "musical artist", "album", "event", "music genre", "award", "location", "person"], "instance": {"id": "216", "words": ["In", "addition", "to", "being", "named", "Top", "Female", "Vocalist", "by", "the", "Academy", "of", "Country", "Music", "(", "ACM", ")", "twice", "and", "Female", "Vocalist", "of", "the", "Year", "by", "the", "Country", "Music", "Association", "(", "CMA", ")", ",", "she", "also", "won", "a", "Grammy", "Award", "(", "earning", "seven", "nominations", ")", ",", "People", "'s", "Choice", "Award", "and", "an", "American", "Music", "Award", "(", "AMA", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "B-award", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, song, musical instrument, country, organization, musical artist, album, event, music genre, award, location, person and O.\nSentence: In addition to being named Top Female Vocalist by the Academy of Country Music ( ACM ) twice and Female Vocalist of the Year by the Country Music Association ( CMA ) , she also won a Grammy Award ( earning seven nominations ) , People 's Choice Award and an American Music Award ( AMA ) .", "prompt_labels": "In(O) addition(O) to(O) being(O) named(O) Top(B-award) Female(I-award) Vocalist(I-award) by(O) the(O) Academy(B-organization) of(I-organization) Country(I-organization) Music(I-organization) ((O) ACM(B-organization) )(O) twice(O) and(O) Female(B-award) Vocalist(I-award) of(I-award) the(I-award) Year(I-award) by(O) the(O) Country(B-organization) Music(I-organization) Association(I-organization) ((O) CMA(B-organization) )(O) ,(O) she(O) also(O) won(O) a(O) Grammy(B-award) Award(I-award) ((O) earning(O) seven(O) nominations(O) )(O) ,(O) People(B-award) 's(I-award) Choice(I-award) Award(I-award) and(O) an(O) American(B-award) Music(I-award) Award(I-award) ((O) AMA(B-award) )(O) .(O)"}}
{"id": "352", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "musical artist", "award", "album", "country", "location", "band", "musical instrument", "event", "music genre", "person", "song"], "instance": {"id": "352", "words": ["Tens", "of", "thousands", "of", "medieval", "tombstones", "called", "Stećci", "are", "found", "in", "Bosnia", "and", "Hercegovina", "and", "neighboring", "areas", "in", "Montenegro", ",", "Serbia", "and", "Croatia", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, musical artist, award, album, country, location, band, musical instrument, event, music genre, person, song and O.\nSentence: Tens of thousands of medieval tombstones called Stećci are found in Bosnia and Hercegovina and neighboring areas in Montenegro , Serbia and Croatia .", "prompt_labels": "Tens(O) of(O) thousands(O) of(O) medieval(O) tombstones(O) called(O) Stećci(O) are(O) found(O) in(O) Bosnia(B-country) and(I-country) Hercegovina(I-country) and(O) neighboring(O) areas(O) in(O) Montenegro(B-country) ,(O) Serbia(B-country) and(O) Croatia(B-country) .(O)"}}
{"id": "212", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "band", "album", "event", "organization", "country", "location", "award", "musical artist", "person", "music genre", "song"], "instance": {"id": "212", "words": ["The", "film", "won", "Academy", "Award", "for", "Best", "Picture", ",", "Academy", "Award", "for", "Best", "Director", "and", "Academy", "Award", "for", "Best", "Adapted", "Screenplay", ",", "and", "Hopkins", "also", "picked", "up", "his", "first", "BAFTA", "for", "Best", "Actor", "."], "labels": ["O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-person", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, band, album, event, organization, country, location, award, musical artist, person, music genre, song and O.\nSentence: The film won Academy Award for Best Picture , Academy Award for Best Director and Academy Award for Best Adapted Screenplay , and Hopkins also picked up his first BAFTA for Best Actor .", "prompt_labels": "The(O) film(O) won(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) ,(O) and(O) Hopkins(B-person) also(O) picked(O) up(O) his(O) first(O) BAFTA(B-award) for(I-award) Best(I-award) Actor(I-award) .(O)"}}
{"id": "335", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "country", "award", "band", "musical artist", "music genre", "event", "organization", "location", "song", "musical instrument", "album"], "instance": {"id": "335", "words": ["In", "addition", "to", "Underwood", ",", "American", "Idol", "launched", "the", "careers", "of", "Kellie", "Pickler", ",", "Josh", "Gracin", ",", "Bucky", "Covington", ",", "Kristy", "Lee", "Cook", ",", "Danny", "Gokey", ",", "Lauren", "Alaina", "and", "Scotty", "McCreery", "(", "as", "well", "as", "that", "of", "occasional", "country", "singer", "Kelly", "Clarkson", ")", "in", "the", "decade", ",", "and", "would", "continue", "to", "launch", "country", "careers", "in", "the", "2010s", "."], "labels": ["O", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, award, band, musical artist, music genre, event, organization, location, song, musical instrument, album and O.\nSentence: In addition to Underwood , American Idol launched the careers of Kellie Pickler , Josh Gracin , Bucky Covington , Kristy Lee Cook , Danny Gokey , Lauren Alaina and Scotty McCreery ( as well as that of occasional country singer Kelly Clarkson ) in the decade , and would continue to launch country careers in the 2010s .", "prompt_labels": "In(O) addition(O) to(O) Underwood(B-musical artist) ,(O) American(O) Idol(O) launched(O) the(O) careers(O) of(O) Kellie(B-musical artist) Pickler(I-musical artist) ,(O) Josh(B-musical artist) Gracin(I-musical artist) ,(O) Bucky(B-musical artist) Covington(I-musical artist) ,(O) Kristy(B-musical artist) Lee(I-musical artist) Cook(I-musical artist) ,(O) Danny(B-musical artist) Gokey(I-musical artist) ,(O) Lauren(B-musical artist) Alaina(I-musical artist) and(O) Scotty(B-musical artist) McCreery(I-musical artist) ((O) as(O) well(O) as(O) that(O) of(O) occasional(O) country(B-music genre) singer(O) Kelly(B-musical artist) Clarkson(I-musical artist) )(O) in(O) the(O) decade(O) ,(O) and(O) would(O) continue(O) to(O) launch(O) country(O) careers(O) in(O) the(O) 2010s(O) .(O)"}}
{"id": "350", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "award", "album", "song", "country", "event", "musical artist", "organization", "band", "musical instrument", "location", "person"], "instance": {"id": "350", "words": ["The", "Nordic", "countries", "are", "generally", "taken", "to", "include", "Iceland", ",", "Norway", ",", "Finland", ",", "Sweden", "and", "Denmark", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, award, album, song, country, event, musical artist, organization, band, musical instrument, location, person and O.\nSentence: The Nordic countries are generally taken to include Iceland , Norway , Finland , Sweden and Denmark .", "prompt_labels": "The(O) Nordic(O) countries(O) are(O) generally(O) taken(O) to(O) include(O) Iceland(B-country) ,(O) Norway(B-country) ,(O) Finland(B-country) ,(O) Sweden(B-country) and(O) Denmark(B-country) .(O)"}}
{"id": "279", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "person", "country", "musical instrument", "musical artist", "event", "song", "location", "organization", "album", "band", "music genre"], "instance": {"id": "279", "words": ["He", "received", "the", "Grammy", "Award", "for", "Grammy", "Award", "for", "Album", "of", "the", "Year", "in", "1987", "and", "also", "Grammy", "Award", "for", "Record", "of", "the", "Year", "for", "the", "title", "track", "one", "year", "later", "."], "labels": ["O", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, country, musical instrument, musical artist, event, song, location, organization, album, band, music genre and O.\nSentence: He received the Grammy Award for Grammy Award for Album of the Year in 1987 and also Grammy Award for Record of the Year for the title track one year later .", "prompt_labels": "He(O) received(O) the(O) Grammy(B-award) Award(I-award) for(O) Grammy(B-award) Award(I-award) for(I-award) Album(I-award) of(I-award) the(I-award) Year(I-award) in(O) 1987(O) and(O) also(O) Grammy(B-award) Award(I-award) for(I-award) Record(I-award) of(I-award) the(I-award) Year(I-award) for(O) the(O) title(O) track(O) one(O) year(O) later(O) .(O)"}}
{"id": "368", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "award", "person", "country", "band", "musical artist", "event", "music genre", "location", "organization", "album", "musical instrument"], "instance": {"id": "368", "words": ["He", "created", "a", "prodigious", "output", "by", "working", "for", "such", "bands", ",", "musicians", "and", "performers", "as", "Vivian", "Stanshall", ",", "Generation", "X", ",", "Big", "Star", ",", "Johnny", "Moped", ",", "Whirlwind", ",", "Billy", "Bragg", ",", "Clover", ",", "the", "Sinceros", ",", "Roger", "Chapman", ",", "Phillip", "Goodhand-Tait", ",", "Dr.", "Feelgood", ",", "Inner", "City", "Unit", "and", "the", "Psychedelic", "Furs", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O", "B-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, award, person, country, band, musical artist, event, music genre, location, organization, album, musical instrument and O.\nSentence: He created a prodigious output by working for such bands , musicians and performers as Vivian Stanshall , Generation X , Big Star , Johnny Moped , Whirlwind , Billy Bragg , Clover , the Sinceros , Roger Chapman , Phillip Goodhand-Tait , Dr. Feelgood , Inner City Unit and the Psychedelic Furs .", "prompt_labels": "He(O) created(O) a(O) prodigious(O) output(O) by(O) working(O) for(O) such(O) bands(O) ,(O) musicians(O) and(O) performers(O) as(O) Vivian(B-musical artist) Stanshall(I-musical artist) ,(O) Generation(B-band) X(I-band) ,(O) Big(B-band) Star(I-band) ,(O) Johnny(B-band) Moped(I-band) ,(O) Whirlwind(B-band) ,(O) Billy(B-musical artist) Bragg(I-musical artist) ,(O) Clover(B-band) ,(O) the(B-band) Sinceros(I-band) ,(O) Roger(B-musical artist) Chapman(I-musical artist) ,(O) Phillip(B-musical artist) Goodhand-Tait(I-musical artist) ,(O) Dr.(B-band) Feelgood(I-band) ,(O) Inner(B-band) City(I-band) Unit(I-band) and(O) the(B-band) Psychedelic(I-band) Furs(I-band) .(O)"}}
{"id": "34", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "musical instrument", "location", "event", "musical artist", "song", "award", "organization", "music genre", "band", "country", "album"], "instance": {"id": "34", "words": ["Heard", "released", "five", "albums", "for", "the", "label", ";", "1981", "'s", "Stop", "the", "Dominoes", ",", "1982", "'s", "Victims", "of", "the", "Age", ";", "1983", "'s", "Eye", "of", "the", "Storm", ";", "1984", "'s", "Ashes", "and", "Light", ";", "and", "1985", "'s", "Mosaics", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, musical instrument, location, event, musical artist, song, award, organization, music genre, band, country, album and O.\nSentence: Heard released five albums for the label ; 1981 's Stop the Dominoes , 1982 's Victims of the Age ; 1983 's Eye of the Storm ; 1984 's Ashes and Light ; and 1985 's Mosaics .", "prompt_labels": "Heard(B-musical artist) released(O) five(O) albums(O) for(O) the(O) label(O) ;(O) 1981(O) 's(O) Stop(B-album) the(I-album) Dominoes(I-album) ,(O) 1982(O) 's(O) Victims(B-album) of(I-album) the(I-album) Age(I-album) ;(O) 1983(O) 's(O) Eye(B-album) of(I-album) the(I-album) Storm(I-album) ;(O) 1984(O) 's(O) Ashes(B-album) and(I-album) Light(I-album) ;(O) and(O) 1985(O) 's(O) Mosaics(B-album) .(O)"}}
{"id": "77", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "award", "song", "musical artist", "musical instrument", "album", "band", "organization", "country", "person", "event", "music genre"], "instance": {"id": "77", "words": ["Schmidt", "was", "born", "in", "Pozsony", "(", "known", "in", "German", "as", "Pressburg", ")", ",", "in", "the", "Hungary", "part", "of", "the", "Austria-Hungary", "(", "the", "city", "is", "now", "Bratislava", ",", "capital", "of", "Slovakia", ")", "."], "labels": ["B-person", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "B-country", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, song, musical artist, musical instrument, album, band, organization, country, person, event, music genre and O.\nSentence: Schmidt was born in Pozsony ( known in German as Pressburg ) , in the Hungary part of the Austria-Hungary ( the city is now Bratislava , capital of Slovakia ) .", "prompt_labels": "Schmidt(B-person) was(O) born(O) in(O) Pozsony(B-location) ((O) known(O) in(O) German(O) as(O) Pressburg(B-location) )(O) ,(O) in(O) the(O) Hungary(B-country) part(O) of(O) the(O) Austria-Hungary(B-location) ((O) the(O) city(O) is(O) now(O) Bratislava(B-location) ,(O) capital(O) of(O) Slovakia(B-country) )(O) .(O)"}}
{"id": "86", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "band", "musical artist", "event", "award", "organization", "location", "person", "song", "album", "country", "music genre"], "instance": {"id": "86", "words": ["Other", "early", "examples", "include", "the", "Mercury", "Music", "Prize", "winning", "albums", "New", "Forms", "(", "1997", ")", "from", "Reprazent", "and", "OK", "(", "1998", ")", "from", "Talvin", "Singh", ",", "4hero", "'", "s", "Mercury-nominated", "Two", "Pages", "from", "1998", ",", "and", "Pendulum", "'", "s", "Hold", "Your", "Colour", "in", "2005", "(", "the", "best", "selling", "drum", "and", "bass", "album", "of", "all", "time", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-band", "O", "B-band", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-band", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "B-musical instrument", "O", "B-musical instrument", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, band, musical artist, event, award, organization, location, person, song, album, country, music genre and O.\nSentence: Other early examples include the Mercury Music Prize winning albums New Forms ( 1997 ) from Reprazent and OK ( 1998 ) from Talvin Singh , 4hero ' s Mercury-nominated Two Pages from 1998 , and Pendulum ' s Hold Your Colour in 2005 ( the best selling drum and bass album of all time ) .", "prompt_labels": "Other(O) early(O) examples(O) include(O) the(O) Mercury(B-award) Music(I-award) Prize(I-award) winning(O) albums(O) New(B-album) Forms(I-album) ((O) 1997(O) )(O) from(O) Reprazent(B-band) and(O) OK(B-band) ((O) 1998(O) )(O) from(O) Talvin(B-musical artist) Singh(I-musical artist) ,(O) 4hero(B-band) '(O) s(O) Mercury-nominated(O) Two(B-album) Pages(I-album) from(O) 1998(O) ,(O) and(O) Pendulum(B-band) '(O) s(O) Hold(B-album) Your(I-album) Colour(I-album) in(O) 2005(O) ((O) the(O) best(O) selling(O) drum(B-musical instrument) and(O) bass(B-musical instrument) album(O) of(O) all(O) time(O) )(O) .(O)"}}
{"id": "24", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "country", "musical instrument", "musical artist", "location", "album", "award", "person", "organization", "music genre", "song", "event"], "instance": {"id": "24", "words": ["Within", "the", "mainline", "United", "States", "drum", "and", "bugle", "corps", "can", "trace", "their", "origins", "to", "the", "many", "Veterans", "of", "Foreign", "Wars", "(", "VFW", ")", "and", "American", "Legion", "(", "AL", ")", "meeting", "halls", ",", "where", "First", "World", "War", "and", "Spanish-American", "War", "veterans", "met", "and", "formed", "musical", "ensemble", "s", "to", "entertain", "their", "communities", "."], "labels": ["O", "O", "O", "B-country", "I-country", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "B-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, country, musical instrument, musical artist, location, album, award, person, organization, music genre, song, event and O.\nSentence: Within the mainline United States drum and bugle corps can trace their origins to the many Veterans of Foreign Wars ( VFW ) and American Legion ( AL ) meeting halls , where First World War and Spanish-American War veterans met and formed musical ensemble s to entertain their communities .", "prompt_labels": "Within(O) the(O) mainline(O) United(B-country) States(I-country) drum(B-event) and(I-event) bugle(I-event) corps(I-event) can(O) trace(O) their(O) origins(O) to(O) the(O) many(O) Veterans(B-organization) of(I-organization) Foreign(I-organization) Wars(I-organization) ((O) VFW(B-organization) )(O) and(O) American(B-organization) Legion(I-organization) ((O) AL(B-organization) )(O) meeting(O) halls(O) ,(O) where(O) First(B-event) World(I-event) War(I-event) and(O) Spanish-American(B-event) War(I-event) veterans(O) met(O) and(O) formed(O) musical(O) ensemble(O) s(O) to(O) entertain(O) their(O) communities(O) .(O)"}}
{"id": "277", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "organization", "musical artist", "award", "band", "country", "music genre", "event", "song", "person", "album", "location"], "instance": {"id": "277", "words": ["Cobain", "was", "also", "a", "fan", "of", "protopunk", "acts", "like", "the", "Stooges", ",", "whose", "1973", "album", "Raw", "Power", "he", "listed", "as", "his", "favorite", "of", "all", "time", "in", "his", "journals", ",", "and", "The", "Velvet", "Underground", ",", "whose", "1968", "song", "Here", "She", "Comes", "Now", "the", "band", "covered", "both", "live", "and", "in", "the", "studio", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, organization, musical artist, award, band, country, music genre, event, song, person, album, location and O.\nSentence: Cobain was also a fan of protopunk acts like the Stooges , whose 1973 album Raw Power he listed as his favorite of all time in his journals , and The Velvet Underground , whose 1968 song Here She Comes Now the band covered both live and in the studio .", "prompt_labels": "Cobain(B-musical artist) was(O) also(O) a(O) fan(O) of(O) protopunk(B-music genre) acts(O) like(O) the(B-band) Stooges(I-band) ,(O) whose(O) 1973(O) album(O) Raw(B-album) Power(I-album) he(O) listed(O) as(O) his(O) favorite(O) of(O) all(O) time(O) in(O) his(O) journals(O) ,(O) and(O) The(B-band) Velvet(I-band) Underground(I-band) ,(O) whose(O) 1968(O) song(O) Here(B-song) She(I-song) Comes(I-song) Now(I-song) the(O) band(O) covered(O) both(O) live(O) and(O) in(O) the(O) studio(O) .(O)"}}
{"id": "171", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "country", "person", "music genre", "album", "organization", "musical artist", "musical instrument", "event", "award", "location", "band"], "instance": {"id": "171", "words": ["Juliana", "Hatfield", "is", "an", "American", "musician", "and", "singer-songwriter", "from", "the", "Boston", "area", ",", "formerly", "of", "the", "indie", "rock", "bands", "Blake", "Babies", ",", "Some", "Girls", ",", "and", "The", "Lemonheads", "."], "labels": ["B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "B-music genre", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, country, person, music genre, album, organization, musical artist, musical instrument, event, award, location, band and O.\nSentence: Juliana Hatfield is an American musician and singer-songwriter from the Boston area , formerly of the indie rock bands Blake Babies , Some Girls , and The Lemonheads .", "prompt_labels": "Juliana(B-musical artist) Hatfield(I-musical artist) is(O) an(O) American(O) musician(O) and(O) singer-songwriter(O) from(O) the(O) Boston(B-location) area(I-location) ,(O) formerly(O) of(O) the(O) indie(O) rock(B-music genre) bands(O) Blake(B-band) Babies(I-band) ,(O) Some(B-band) Girls(I-band) ,(O) and(O) The(B-band) Lemonheads(I-band) .(O)"}}
{"id": "332", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "album", "location", "award", "event", "organization", "musical instrument", "band", "person", "musical artist", "country", "song"], "instance": {"id": "332", "words": ["After", "releasing", "four", "albums", ",", "Crowded", "House", ",", "Temple", "of", "Low", "Men", ",", "Woodface", ",", "and", "Together", "Alone", ",", "the", "group", "broke", "up", "in", "1996", ",", "and", "followed", "this", "action", "by", "releasing", "a", "greatest", "hits", "album", "Recurring", "Dream", "."], "labels": ["O", "O", "O", "O", "O", "B-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-album", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, album, location, award, event, organization, musical instrument, band, person, musical artist, country, song and O.\nSentence: After releasing four albums , Crowded House , Temple of Low Men , Woodface , and Together Alone , the group broke up in 1996 , and followed this action by releasing a greatest hits album Recurring Dream .", "prompt_labels": "After(O) releasing(O) four(O) albums(O) ,(O) Crowded(B-album) House(I-album) ,(O) Temple(B-album) of(I-album) Low(I-album) Men(I-album) ,(O) Woodface(B-album) ,(O) and(O) Together(B-album) Alone(I-album) ,(O) the(O) group(O) broke(O) up(O) in(O) 1996(O) ,(O) and(O) followed(O) this(O) action(O) by(O) releasing(O) a(O) greatest(O) hits(O) album(O) Recurring(B-album) Dream(I-album) .(O)"}}
{"id": "0", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "band", "song", "location", "album", "musical artist", "country", "award", "music genre", "person", "organization", "event"], "instance": {"id": "0", "words": ["As", "part", "of", "the", "2010", "leg", "of", "the", "My", "Christmas", "Tour", ",", "Bocelli", "gave", "two", "concerts", "in", "The", "O2", "Arena", ",", "in", "London", ",", "and", "the", "Manchester", "Arena", ",", "in", "Manchester", ",", "and", "a", "concert", "at", "3Arena", ",", "in", "Dublin", ",", "in", "late", "November", "2010", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "B-musical artist", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "O", "O", "O", "B-location", "I-location", "O", "O", "B-location", "O", "O", "O", "O", "O", "B-location", "O", "O", "B-location", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, band, song, location, album, musical artist, country, award, music genre, person, organization, event and O.\nSentence: As part of the 2010 leg of the My Christmas Tour , Bocelli gave two concerts in The O2 Arena , in London , and the Manchester Arena , in Manchester , and a concert at 3Arena , in Dublin , in late November 2010 .", "prompt_labels": "As(O) part(O) of(O) the(O) 2010(O) leg(O) of(O) the(O) My(B-event) Christmas(I-event) Tour(I-event) ,(O) Bocelli(B-musical artist) gave(O) two(O) concerts(O) in(O) The(B-location) O2(I-location) Arena(I-location) ,(O) in(O) London(B-location) ,(O) and(O) the(O) Manchester(B-location) Arena(I-location) ,(O) in(O) Manchester(B-location) ,(O) and(O) a(O) concert(O) at(O) 3Arena(B-location) ,(O) in(O) Dublin(B-location) ,(O) in(O) late(O) November(O) 2010(O) .(O)"}}
{"id": "203", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "event", "music genre", "organization", "location", "musical artist", "award", "person", "musical instrument", "album", "country", "band"], "instance": {"id": "203", "words": ["Brad", "Shoup", "of", "Stereogum", "surmised", "that", ",", "thanks", "to", "the", "Ramones", "'", "praise", "for", "the", "group", ",", "many", "punk", ",", "pop", "punk", ",", "or", "punk-adjacent", "artists", "showed", "influence", "from", "the", "Beach", "Boys", ",", "noting", "cover", "versions", "of", "the", "band", "'s", "songs", "recorded", "by", "Slickee", "Boys", ",", "Agent", "Orange", ",", "Bad", "Religion", ",", "Shonen", "Knife", ",", "the", "Queers", ",", "Hi-Standard", ",", "the", "Descendents", ",", "the", "Donnas", ",", "M.O.D.", ",", "and", "the", "Vandals", "."], "labels": ["B-person", "I-person", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-band", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, music genre, organization, location, musical artist, award, person, musical instrument, album, country, band and O.\nSentence: Brad Shoup of Stereogum surmised that , thanks to the Ramones ' praise for the group , many punk , pop punk , or punk-adjacent artists showed influence from the Beach Boys , noting cover versions of the band 's songs recorded by Slickee Boys , Agent Orange , Bad Religion , Shonen Knife , the Queers , Hi-Standard , the Descendents , the Donnas , M.O.D. , and the Vandals .", "prompt_labels": "Brad(B-person) Shoup(I-person) of(O) Stereogum(B-organization) surmised(O) that(O) ,(O) thanks(O) to(O) the(O) Ramones(B-band) '(O) praise(O) for(O) the(O) group(O) ,(O) many(O) punk(B-music genre) ,(O) pop(B-music genre) punk(I-music genre) ,(O) or(O) punk-adjacent(B-music genre) artists(O) showed(O) influence(O) from(O) the(O) Beach(B-band) Boys(I-band) ,(O) noting(O) cover(O) versions(O) of(O) the(O) band(O) 's(O) songs(O) recorded(O) by(O) Slickee(B-band) Boys(I-band) ,(O) Agent(B-band) Orange(I-band) ,(O) Bad(B-band) Religion(I-band) ,(O) Shonen(B-band) Knife(I-band) ,(O) the(B-band) Queers(I-band) ,(O) Hi-Standard(B-band) ,(O) the(B-band) Descendents(I-band) ,(O) the(B-band) Donnas(I-band) ,(O) M.O.D.(B-band) ,(O) and(O) the(B-band) Vandals(I-band) .(O)"}}
{"id": "16", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "musical instrument", "country", "album", "song", "organization", "musical artist", "band", "person", "music genre", "award", "event"], "instance": {"id": "16", "words": ["Since", "the", "introduction", "of", "the", "50", "/", "50", "voting", "system", "in", "2009", ",", "the", "juries", "and", "the", "voters", "have", "disagreed", "on", "the", "winner", "on", "five", "occasions", ",", "in", "Eurovision", "Song", "Contest", "2011", ",", "Eurovision", "Song", "Contest", "2015", ",", "Eurovision", "Song", "Contest", "2016", ",", "Eurovision", "Song", "Contest", "2018", "and", "Eurovision", "Song", "Contest", "2019", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical instrument, country, album, song, organization, musical artist, band, person, music genre, award, event and O.\nSentence: Since the introduction of the 50 / 50 voting system in 2009 , the juries and the voters have disagreed on the winner on five occasions , in Eurovision Song Contest 2011 , Eurovision Song Contest 2015 , Eurovision Song Contest 2016 , Eurovision Song Contest 2018 and Eurovision Song Contest 2019 .", "prompt_labels": "Since(O) the(O) introduction(O) of(O) the(O) 50(O) /(O) 50(O) voting(O) system(O) in(O) 2009(O) ,(O) the(O) juries(O) and(O) the(O) voters(O) have(O) disagreed(O) on(O) the(O) winner(O) on(O) five(O) occasions(O) ,(O) in(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2011(I-event) ,(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2015(I-event) ,(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2016(I-event) ,(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2018(I-event) and(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2019(I-event) .(O)"}}
{"id": "11", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "album", "music genre", "organization", "person", "song", "musical artist", "location", "event", "country", "band", "award"], "instance": {"id": "11", "words": ["Some", "of", "his", "most", "celebrated", "designs", "adorned", "the", "sleeves", "of", "albums", "such", "as", "Midnight", "Blue", ",", "Out", "to", "Lunch", "!", ",", "Unity", ",", "Somethin", "'", "Else", ",", "Let", "Freedom", "Ring", ",", "Hub-Tones", ",", "No", "Room", "for", "Squares", ",", "Cool", "Struttin", "'", ",", "and", "The", "Sidewinder", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-album", "O", "B-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "O", "B-album", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "O", "O", "B-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, album, music genre, organization, person, song, musical artist, location, event, country, band, award and O.\nSentence: Some of his most celebrated designs adorned the sleeves of albums such as Midnight Blue , Out to Lunch ! , Unity , Somethin ' Else , Let Freedom Ring , Hub-Tones , No Room for Squares , Cool Struttin ' , and The Sidewinder .", "prompt_labels": "Some(O) of(O) his(O) most(O) celebrated(O) designs(O) adorned(O) the(O) sleeves(O) of(O) albums(O) such(O) as(O) Midnight(B-album) Blue(I-album) ,(O) Out(B-album) to(I-album) Lunch(I-album) !(I-album) ,(O) Unity(B-album) ,(O) Somethin(B-album) '(I-album) Else(I-album) ,(O) Let(B-album) Freedom(I-album) Ring(I-album) ,(O) Hub-Tones(B-album) ,(O) No(B-album) Room(I-album) for(I-album) Squares(I-album) ,(O) Cool(B-album) Struttin(I-album) '(I-album) ,(O) and(O) The(B-album) Sidewinder(I-album) .(O)"}}
{"id": "178", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "person", "country", "band", "location", "music genre", "event", "album", "musical artist", "award", "organization", "musical instrument"], "instance": {"id": "178", "words": ["Depeche", "Mode", "'s", "releases", "have", "been", "nominated", "for", "five", "Grammy", "Awards", ":", "Devotional", "for", "Grammy", "Award", "for", "Best", "Music", "Film", ";", "I", "Feel", "Loved", "and", "Suffer", "Well", ",", "both", "for", "Grammy", "Award", "for", "Best", "Dance", "Recording", ";", "Sounds", "of", "the", "Universe", "for", "Best", "Alternative", "Album", ";", "and", "Wrong", "for", "Grammy", "Award", "for", "Best", "Music", "Video", "."], "labels": ["B-band", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "B-song", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-award", "I-award", "I-award", "O", "O", "B-song", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, person, country, band, location, music genre, event, album, musical artist, award, organization, musical instrument and O.\nSentence: Depeche Mode 's releases have been nominated for five Grammy Awards : Devotional for Grammy Award for Best Music Film ; I Feel Loved and Suffer Well , both for Grammy Award for Best Dance Recording ; Sounds of the Universe for Best Alternative Album ; and Wrong for Grammy Award for Best Music Video .", "prompt_labels": "Depeche(B-band) Mode(O) 's(O) releases(O) have(O) been(O) nominated(O) for(O) five(O) Grammy(B-award) Awards(I-award) :(O) Devotional(B-song) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Music(I-award) Film(I-award) ;(O) I(B-song) Feel(I-song) Loved(I-song) and(O) Suffer(B-song) Well(I-song) ,(O) both(O) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Dance(I-award) Recording(I-award) ;(O) Sounds(B-album) of(I-album) the(I-album) Universe(I-album) for(O) Best(B-award) Alternative(I-award) Album(I-award) ;(O) and(O) Wrong(B-song) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Music(I-award) Video(I-award) .(O)"}}
{"id": "9", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "location", "organization", "country", "album", "song", "person", "music genre", "musical artist", "event", "award", "musical instrument"], "instance": {"id": "9", "words": ["Ernest", "Jennings", "Ford", "(", "February", "13", ",", "1919", "-", "October", "17", ",", "1991", ")", ",", "known", "professionally", "as", "Tennessee", "Ernie", "Ford", ",", "was", "an", "American", "singer", "and", "television", "host", "who", "enjoyed", "success", "in", "the", "Country", "music", ",", "Pop", "music", ",", "and", "Gospel", "music", "musical", "genres", "."], "labels": ["B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, location, organization, country, album, song, person, music genre, musical artist, event, award, musical instrument and O.\nSentence: Ernest Jennings Ford ( February 13 , 1919 - October 17 , 1991 ) , known professionally as Tennessee Ernie Ford , was an American singer and television host who enjoyed success in the Country music , Pop music , and Gospel music musical genres .", "prompt_labels": "Ernest(B-musical artist) Jennings(I-musical artist) Ford(I-musical artist) ((O) February(O) 13(O) ,(O) 1919(O) -(O) October(O) 17(O) ,(O) 1991(O) )(O) ,(O) known(O) professionally(O) as(O) Tennessee(B-musical artist) Ernie(I-musical artist) Ford(I-musical artist) ,(O) was(O) an(O) American(O) singer(O) and(O) television(O) host(O) who(O) enjoyed(O) success(O) in(O) the(O) Country(B-music genre) music(I-music genre) ,(O) Pop(B-music genre) music(I-music genre) ,(O) and(O) Gospel(B-music genre) music(I-music genre) musical(O) genres(O) .(O)"}}
{"id": "245", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "country", "song", "band", "musical artist", "event", "organization", "album", "musical instrument", "person", "location", "music genre"], "instance": {"id": "245", "words": ["In", "the", "1970s", "and", "1980s", ",", "tension", "and", "conflict", "emerged", "between", "Southern", "gospel", "and", "the", "newer", "developments", "of", "Jesus", "music", "and", "Contemporary", "Christian", "music", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, song, band, musical artist, event, organization, album, musical instrument, person, location, music genre and O.\nSentence: In the 1970s and 1980s , tension and conflict emerged between Southern gospel and the newer developments of Jesus music and Contemporary Christian music .", "prompt_labels": "In(O) the(O) 1970s(O) and(O) 1980s(O) ,(O) tension(O) and(O) conflict(O) emerged(O) between(O) Southern(B-music genre) gospel(I-music genre) and(O) the(O) newer(O) developments(O) of(O) Jesus(B-music genre) music(I-music genre) and(O) Contemporary(B-music genre) Christian(I-music genre) music(I-music genre) .(O)"}}
{"id": "176", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "event", "music genre", "location", "song", "musical artist", "organization", "award", "band", "musical instrument", "country", "album"], "instance": {"id": "176", "words": ["Besides", "fronting", "his", "own", "band", "and", "rap", "projects", ",", "Ice-T", "has", "also", "collaborated", "with", "other", "hard", "rock", "and", "metal", "bands", ",", "such", "as", "Icepick", ",", "Motörhead", ",", "Slayer", ",", "Pro-Pain", ",", "and", "Six", "Feet", "Under", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "B-musical artist", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "O", "O", "O", "B-band", "O", "B-band", "O", "B-band", "O", "B-band", "O", "O", "B-band", "I-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, music genre, location, song, musical artist, organization, award, band, musical instrument, country, album and O.\nSentence: Besides fronting his own band and rap projects , Ice-T has also collaborated with other hard rock and metal bands , such as Icepick , Motörhead , Slayer , Pro-Pain , and Six Feet Under .", "prompt_labels": "Besides(O) fronting(O) his(O) own(O) band(O) and(O) rap(B-music genre) projects(O) ,(O) Ice-T(B-musical artist) has(O) also(O) collaborated(O) with(O) other(O) hard(B-music genre) rock(I-music genre) and(O) metal(B-music genre) bands(O) ,(O) such(O) as(O) Icepick(B-band) ,(O) Motörhead(B-band) ,(O) Slayer(B-band) ,(O) Pro-Pain(B-band) ,(O) and(O) Six(B-band) Feet(I-band) Under(I-band) .(O)"}}
{"id": "146", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "music genre", "organization", "location", "award", "band", "album", "song", "musical instrument", "person", "country", "musical artist"], "instance": {"id": "146", "words": ["Major", "artists", "who", "have", "been", "influenced", "by", "Costello", "include", "Green", "Day", ",", "Prince", ",", "Billy", "Bragg", ",", "Goldfinger", ",", "the", "Pogues", ",", "Radiohead", ",", "Dexys", "Midnight", "Runners", ",", "Pulp", ",", "Crowded", "House", ",", "James", ",", "Suzanne", "Vega", ",", "Less", "than", "Jake", ",", "and", "Foo", "Fighters", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "B-band", "I-band", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "I-band", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, music genre, organization, location, award, band, album, song, musical instrument, person, country, musical artist and O.\nSentence: Major artists who have been influenced by Costello include Green Day , Prince , Billy Bragg , Goldfinger , the Pogues , Radiohead , Dexys Midnight Runners , Pulp , Crowded House , James , Suzanne Vega , Less than Jake , and Foo Fighters .", "prompt_labels": "Major(O) artists(O) who(O) have(O) been(O) influenced(O) by(O) Costello(B-musical artist) include(O) Green(B-band) Day(I-band) ,(O) Prince(B-band) ,(O) Billy(B-musical artist) Bragg(I-musical artist) ,(O) Goldfinger(B-band) ,(O) the(B-band) Pogues(I-band) ,(O) Radiohead(B-band) ,(O) Dexys(B-band) Midnight(I-band) Runners(I-band) ,(O) Pulp(B-band) ,(O) Crowded(B-band) House(I-band) ,(O) James(B-band) ,(O) Suzanne(B-musical artist) Vega(I-musical artist) ,(O) Less(B-band) than(I-band) Jake(I-band) ,(O) and(O) Foo(B-band) Fighters(I-band) .(O)"}}
{"id": "197", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "person", "musical instrument", "award", "music genre", "country", "musical artist", "organization", "band", "song", "event", "location"], "instance": {"id": "197", "words": ["Mercury", "wrote", "numerous", "hits", "for", "Queen", ",", "including", "Killer", "Queen", ",", "Bohemian", "Rhapsody", ",", "Somebody", "to", "Love", ",", "We", "Are", "the", "Champions", ",", "Don", "'t", "Stop", "Me", "Now", ",", "and", "Crazy", "Little", "Thing", "Called", "Love", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "B-band", "O", "O", "B-song", "I-song", "O", "B-song", "I-song", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, person, musical instrument, award, music genre, country, musical artist, organization, band, song, event, location and O.\nSentence: Mercury wrote numerous hits for Queen , including Killer Queen , Bohemian Rhapsody , Somebody to Love , We Are the Champions , Don 't Stop Me Now , and Crazy Little Thing Called Love .", "prompt_labels": "Mercury(B-musical artist) wrote(O) numerous(O) hits(O) for(O) Queen(B-band) ,(O) including(O) Killer(B-song) Queen(I-song) ,(O) Bohemian(B-song) Rhapsody(I-song) ,(O) Somebody(B-song) to(I-song) Love(I-song) ,(O) We(B-song) Are(I-song) the(I-song) Champions(I-song) ,(O) Don(B-song) 't(I-song) Stop(I-song) Me(I-song) Now(I-song) ,(O) and(O) Crazy(B-song) Little(I-song) Thing(I-song) Called(I-song) Love(I-song) .(O)"}}
{"id": "240", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "event", "organization", "award", "musical artist", "musical instrument", "song", "band", "album", "music genre", "person", "location"], "instance": {"id": "240", "words": ["She", "has", "garnered", "ten", "Grammy", "Award", "s", ",", "two", "Academy", "Awards", "nominations", ",", "ten", "Country", "Music", "Association", "Awards", ",", "seven", "Academy", "of", "Country", "Music", "Awards", ",", "three", "American", "Music", "Awards", ",", "and", "is", "one", "of", "only", "seven", "female", "artists", "to", "win", "the", "Country", "Music", "Association", "'s", "Entertainer", "of", "the", "Year", "Award", "."], "labels": ["O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, organization, award, musical artist, musical instrument, song, band, album, music genre, person, location and O.\nSentence: She has garnered ten Grammy Award s , two Academy Awards nominations , ten Country Music Association Awards , seven Academy of Country Music Awards , three American Music Awards , and is one of only seven female artists to win the Country Music Association 's Entertainer of the Year Award .", "prompt_labels": "She(O) has(O) garnered(O) ten(O) Grammy(B-award) Award(I-award) s(O) ,(O) two(O) Academy(B-award) Awards(I-award) nominations(O) ,(O) ten(O) Country(B-award) Music(I-award) Association(I-award) Awards(I-award) ,(O) seven(O) Academy(B-award) of(I-award) Country(I-award) Music(I-award) Awards(I-award) ,(O) three(O) American(B-award) Music(I-award) Awards(I-award) ,(O) and(O) is(O) one(O) of(O) only(O) seven(O) female(O) artists(O) to(O) win(O) the(O) Country(B-award) Music(I-award) Association(I-award) 's(I-award) Entertainer(I-award) of(I-award) the(I-award) Year(I-award) Award(I-award) .(O)"}}
{"id": "27", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "music genre", "song", "country", "organization", "award", "event", "musical artist", "person", "album", "musical instrument", "band"], "instance": {"id": "27", "words": ["Under", "the", "current", "voting", "system", ",", "in", "place", "since", "2016", ",", "the", "highest-scoring", "winner", "is", "Salvador", "Sobral", "of", "Portugal", "who", "won", "the", "Eurovision", "Song", "Contest", "2017", "in", "Kiev", ",", "Ukraine", ",", "with", "758", "points", ";", "under", "the", "previous", "system", ",", "the", "highest-scoring", "winner", "was", "Alexander", "Rybak", "of", "Norway", "with", "387", "points", "in", "Eurovision", "Song", "Contest", "2009", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-country", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-location", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-country", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, music genre, song, country, organization, award, event, musical artist, person, album, musical instrument, band and O.\nSentence: Under the current voting system , in place since 2016 , the highest-scoring winner is Salvador Sobral of Portugal who won the Eurovision Song Contest 2017 in Kiev , Ukraine , with 758 points ; under the previous system , the highest-scoring winner was Alexander Rybak of Norway with 387 points in Eurovision Song Contest 2009 .", "prompt_labels": "Under(O) the(O) current(O) voting(O) system(O) ,(O) in(O) place(O) since(O) 2016(O) ,(O) the(O) highest-scoring(O) winner(O) is(O) Salvador(B-musical artist) Sobral(I-musical artist) of(O) Portugal(B-country) who(O) won(O) the(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2017(I-event) in(O) Kiev(B-location) ,(O) Ukraine(B-country) ,(O) with(O) 758(O) points(O) ;(O) under(O) the(O) previous(O) system(O) ,(O) the(O) highest-scoring(O) winner(O) was(O) Alexander(B-musical artist) Rybak(I-musical artist) of(O) Norway(B-country) with(O) 387(O) points(O) in(O) Eurovision(B-event) Song(I-event) Contest(I-event) 2009(I-event) .(O)"}}
{"id": "43", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "music genre", "band", "event", "country", "organization", "location", "musical instrument", "song", "musical artist", "award", "album"], "instance": {"id": "43", "words": ["Olympia", "71", ",", "Olympia", "74", ",", "and", "Olympia", "77", "are", "live", "albums", "."], "labels": ["B-album", "I-album", "O", "B-album", "I-album", "O", "O", "B-album", "I-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, music genre, band, event, country, organization, location, musical instrument, song, musical artist, award, album and O.\nSentence: Olympia 71 , Olympia 74 , and Olympia 77 are live albums .", "prompt_labels": "Olympia(B-album) 71(I-album) ,(O) Olympia(B-album) 74(I-album) ,(O) and(O) Olympia(B-album) 77(I-album) are(O) live(O) albums(O) .(O)"}}
{"id": "45", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "music genre", "album", "band", "location", "event", "country", "award", "musical artist", "musical instrument", "song", "organization"], "instance": {"id": "45", "words": ["The", "venue", "hosted", "Badminton", "at", "the", "2012", "Summer", "Olympics", "and", "Gymnastics", "at", "the", "2012", "Summer", "Olympics", "at", "the", "2012", "Summer", "Olympics", "."], "labels": ["O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, music genre, album, band, location, event, country, award, musical artist, musical instrument, song, organization and O.\nSentence: The venue hosted Badminton at the 2012 Summer Olympics and Gymnastics at the 2012 Summer Olympics at the 2012 Summer Olympics .", "prompt_labels": "The(O) venue(O) hosted(O) Badminton(B-event) at(I-event) the(I-event) 2012(I-event) Summer(I-event) Olympics(I-event) and(O) Gymnastics(B-event) at(I-event) the(I-event) 2012(I-event) Summer(I-event) Olympics(I-event) at(O) the(O) 2012(B-event) Summer(I-event) Olympics(I-event) .(O)"}}
{"id": "363", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "musical artist", "person", "song", "musical instrument", "album", "event", "music genre", "location", "country", "award", "band"], "instance": {"id": "363", "words": ["She", "rose", "to", "fame", "portraying", "Jamie", "Buchman", "in", "the", "sitcom", "Mad", "About", "You", "(", "1992-1999", ",", "2019-present", ")", ",", "for", "which", "she", "won", "three", "Golden", "Globe", "Awards", "for", "Best", "Actress", "in", "a", "Television", "Series", "-", "Musical", "or", "Comedy", "and", "four", "Emmy", "Award", "s", "Primetime", "Emmy", "Award", "for", "Outstanding", "Lead", "Actress", "in", "a", "Comedy", "Series", "."], "labels": ["O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, musical artist, person, song, musical instrument, album, event, music genre, location, country, award, band and O.\nSentence: She rose to fame portraying Jamie Buchman in the sitcom Mad About You ( 1992-1999 , 2019-present ) , for which she won three Golden Globe Awards for Best Actress in a Television Series - Musical or Comedy and four Emmy Award s Primetime Emmy Award for Outstanding Lead Actress in a Comedy Series .", "prompt_labels": "She(O) rose(O) to(O) fame(O) portraying(O) Jamie(B-person) Buchman(I-person) in(O) the(O) sitcom(O) Mad(O) About(O) You(O) ((O) 1992-1999(O) ,(O) 2019-present(O) )(O) ,(O) for(O) which(O) she(O) won(O) three(O) Golden(B-award) Globe(I-award) Awards(I-award) for(I-award) Best(I-award) Actress(I-award) in(I-award) a(I-award) Television(I-award) Series(I-award) -(I-award) Musical(I-award) or(I-award) Comedy(I-award) and(O) four(O) Emmy(B-award) Award(I-award) s(I-award) Primetime(B-award) Emmy(I-award) Award(I-award) for(I-award) Outstanding(I-award) Lead(I-award) Actress(I-award) in(I-award) a(I-award) Comedy(I-award) Series(I-award) .(O)"}}
{"id": "223", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "song", "band", "person", "event", "musical instrument", "award", "location", "country", "organization", "musical artist", "album"], "instance": {"id": "223", "words": ["ZZ", "Top", "'s", "music", "videos", "won", "multiple", "MTV", "Video", "Music", "Award", "awards", "during", "the", "1980s", ",", "topping", "the", "categories", "of", "Best", "Group", "Video", ",", "MTV", "Video", "Music", "Award", "for", "Best", "Direction", ",", "and", "MTV", "Video", "Music", "Award", "for", "Best", "Art", "Direction", "for", "Legs", ",", "Sharp", "Dressed", "Man", "and", "Rough", "Boy", ",", "respectively.", "and", "induction", "into", "the", "Rock", "and", "Roll", "Hall", "of", "Fame", "in", "2004", "."], "labels": ["B-band", "I-band", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-song", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "I-location", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, song, band, person, event, musical instrument, award, location, country, organization, musical artist, album and O.\nSentence: ZZ Top 's music videos won multiple MTV Video Music Award awards during the 1980s , topping the categories of Best Group Video , MTV Video Music Award for Best Direction , and MTV Video Music Award for Best Art Direction for Legs , Sharp Dressed Man and Rough Boy , respectively. and induction into the Rock and Roll Hall of Fame in 2004 .", "prompt_labels": "ZZ(B-band) Top(I-band) 's(O) music(O) videos(O) won(O) multiple(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) awards(O) during(O) the(O) 1980s(O) ,(O) topping(O) the(O) categories(O) of(O) Best(B-award) Group(I-award) Video(I-award) ,(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) for(I-award) Best(I-award) Direction(I-award) ,(O) and(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) for(I-award) Best(I-award) Art(I-award) Direction(I-award) for(O) Legs(B-song) ,(O) Sharp(B-song) Dressed(I-song) Man(I-song) and(O) Rough(B-song) Boy(I-song) ,(O) respectively.(O) and(O) induction(O) into(O) the(O) Rock(B-location) and(I-location) Roll(I-location) Hall(I-location) of(I-location) Fame(I-location) in(O) 2004(O) .(O)"}}
{"id": "62", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "music genre", "event", "album", "person", "song", "organization", "musical instrument", "musical artist", "location", "award", "band"], "instance": {"id": "62", "words": ["142", "He", "returned", "to", "the", "stage", "on", "8", "September", "2005", ",", "appearing", "with", "Arcade", "Fire", "for", "the", "US", "nationally", "televised", "event", "Fashion", "Rocks", ",", "and", "performed", "with", "the", "Canadian", "band", "for", "the", "second", "time", "a", "week", "later", "during", "the", "CMJ", "Music", "Marathon", ".", "Thompson", "(", "2006", ")", ":", "pp.", "291-92", "He", "contributed", "backing", "vocals", "on", "TV", "on", "the", "Radio", "'", "s", "song", "Province", "for", "their", "album", "Return", "to", "Cookie", "Mountain", ","], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "B-country", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "O", "O", "O", "B-song", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, music genre, event, album, person, song, organization, musical instrument, musical artist, location, award, band and O.\nSentence: 142 He returned to the stage on 8 September 2005 , appearing with Arcade Fire for the US nationally televised event Fashion Rocks , and performed with the Canadian band for the second time a week later during the CMJ Music Marathon . Thompson ( 2006 ) : pp. 291-92 He contributed backing vocals on TV on the Radio ' s song Province for their album Return to Cookie Mountain ,", "prompt_labels": "142(O) He(O) returned(O) to(O) the(O) stage(O) on(O) 8(O) September(O) 2005(O) ,(O) appearing(O) with(O) Arcade(B-band) Fire(I-band) for(O) the(O) US(B-country) nationally(O) televised(O) event(O) Fashion(B-event) Rocks(I-event) ,(O) and(O) performed(O) with(O) the(O) Canadian(O) band(O) for(O) the(O) second(O) time(O) a(O) week(O) later(O) during(O) the(O) CMJ(B-event) Music(I-event) Marathon(I-event) .(O) Thompson(B-musical artist) ((O) 2006(O) )(O) :(O) pp.(O) 291-92(O) He(O) contributed(O) backing(O) vocals(O) on(O) TV(B-band) on(I-band) the(I-band) Radio(I-band) '(O) s(O) song(O) Province(B-song) for(O) their(O) album(O) Return(B-album) to(I-album) Cookie(I-album) Mountain(I-album) ,(O)"}}
{"id": "274", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "album", "musical artist", "music genre", "award", "organization", "musical instrument", "event", "person", "location", "song", "band"], "instance": {"id": "274", "words": ["Comprising", "25", "albums", ",", "the", "series", "includes", "music", "from", "regions", "that", "span", "the", "globe", ",", "including", "the", "Sudan", ",", "Nigeria", ",", "Tibet", ",", "Indonesia", ",", "Latvia", ",", "and", "Brazil", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-location", "O", "B-country", "O", "B-country", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, album, musical artist, music genre, award, organization, musical instrument, event, person, location, song, band and O.\nSentence: Comprising 25 albums , the series includes music from regions that span the globe , including the Sudan , Nigeria , Tibet , Indonesia , Latvia , and Brazil .", "prompt_labels": "Comprising(O) 25(O) albums(O) ,(O) the(O) series(O) includes(O) music(O) from(O) regions(O) that(O) span(O) the(O) globe(O) ,(O) including(O) the(O) Sudan(B-country) ,(O) Nigeria(B-country) ,(O) Tibet(B-location) ,(O) Indonesia(B-country) ,(O) Latvia(B-country) ,(O) and(O) Brazil(B-country) .(O)"}}
{"id": "22", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "music genre", "musical instrument", "organization", "event", "country", "album", "person", "band", "location", "song", "musical artist"], "instance": {"id": "22", "words": ["The", "band", "have", "received", "seven", "Grammy", "Award", "s", ",", "four", "Brit", "Awards", ",", "an", "Academy", "Award", "(", "for", "Best", "Original", "Song", "Score", "for", "the", "1970", "film", "Let", "It", "Be", ")", "and", "fifteen", "Ivor", "Novello", "Awards", "."], "labels": ["O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, music genre, musical instrument, organization, event, country, album, person, band, location, song, musical artist and O.\nSentence: The band have received seven Grammy Award s , four Brit Awards , an Academy Award ( for Best Original Song Score for the 1970 film Let It Be ) and fifteen Ivor Novello Awards .", "prompt_labels": "The(O) band(O) have(O) received(O) seven(O) Grammy(B-award) Award(I-award) s(O) ,(O) four(O) Brit(B-award) Awards(I-award) ,(O) an(O) Academy(B-award) Award(I-award) ((O) for(O) Best(B-award) Original(I-award) Song(I-award) Score(I-award) for(O) the(O) 1970(O) film(O) Let(O) It(O) Be(O) )(O) and(O) fifteen(O) Ivor(B-award) Novello(I-award) Awards(I-award) .(O)"}}
{"id": "112", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "award", "music genre", "song", "musical artist", "album", "country", "musical instrument", "band", "person", "event", "location"], "instance": {"id": "112", "words": ["some", "of", "his", "performances", "will", "finish", "without", "some", "blues", "and", "Tejano", "music", "tunes", "playing", "as", "well", "as", "Surf", "music", "instrumentals", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, music genre, song, musical artist, album, country, musical instrument, band, person, event, location and O.\nSentence: some of his performances will finish without some blues and Tejano music tunes playing as well as Surf music instrumentals .", "prompt_labels": "some(O) of(O) his(O) performances(O) will(O) finish(O) without(O) some(O) blues(B-music genre) and(O) Tejano(B-music genre) music(I-music genre) tunes(O) playing(O) as(O) well(O) as(O) Surf(B-music genre) music(I-music genre) instrumentals(O) .(O)"}}
{"id": "244", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "organization", "musical artist", "band", "person", "musical instrument", "award", "song", "location", "country", "album", "event"], "instance": {"id": "244", "words": ["The", "Jawaharlal", "Nehru", "Stadium", "was", "constructed", "by", "the", "Government", "of", "India", "to", "host", "the", "athletic", "events", "and", "ceremonies", "of", "the", "1982", "Asian", "Games", "."], "labels": ["O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, organization, musical artist, band, person, musical instrument, award, song, location, country, album, event and O.\nSentence: The Jawaharlal Nehru Stadium was constructed by the Government of India to host the athletic events and ceremonies of the 1982 Asian Games .", "prompt_labels": "The(O) Jawaharlal(B-location) Nehru(I-location) Stadium(I-location) was(O) constructed(O) by(O) the(O) Government(B-organization) of(I-organization) India(I-organization) to(O) host(O) the(O) athletic(O) events(O) and(O) ceremonies(O) of(O) the(O) 1982(B-event) Asian(I-event) Games(I-event) .(O)"}}
{"id": "63", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "event", "music genre", "award", "band", "album", "organization", "song", "musical artist", "musical instrument", "person", "country"], "instance": {"id": "63", "words": ["In", "December", "2011", "the", "band", "announced", "they", "would", "be", "performing", "Tellin", "'", "Stories", "in", "its", "entirety", "at", "London", "'s", "HMV", "Hammersmith", "Apollo", ",", "O2", "Apollo", "Manchester", "and", "Glasgow", "'s", "Barrowland", "Ballroom", "in", "June", "2012", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "B-location", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "B-location", "I-location", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, music genre, award, band, album, organization, song, musical artist, musical instrument, person, country and O.\nSentence: In December 2011 the band announced they would be performing Tellin ' Stories in its entirety at London 's HMV Hammersmith Apollo , O2 Apollo Manchester and Glasgow 's Barrowland Ballroom in June 2012 .", "prompt_labels": "In(O) December(O) 2011(O) the(O) band(O) announced(O) they(O) would(O) be(O) performing(O) Tellin(B-album) '(I-album) Stories(I-album) in(O) its(O) entirety(O) at(O) London(B-location) 's(O) HMV(B-location) Hammersmith(I-location) Apollo(I-location) ,(O) O2(B-location) Apollo(I-location) Manchester(I-location) and(O) Glasgow(B-location) 's(O) Barrowland(B-location) Ballroom(I-location) in(O) June(O) 2012(O) .(O)"}}
{"id": "262", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "award", "person", "musical artist", "music genre", "organization", "musical instrument", "location", "album", "band", "song", "event"], "instance": {"id": "262", "words": ["He", "won", "seven", "Grammy", "Award", "s", ",", "seven", "Brit", "Awards", ",", "six", "American", "Music", "Awards", ",", "four", "MTV", "Video", "Music", "Award", "s", ",", "an", "Academy", "Award", ",", "and", "a", "Golden", "Globe", "Award", "."], "labels": ["O", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, person, musical artist, music genre, organization, musical instrument, location, album, band, song, event and O.\nSentence: He won seven Grammy Award s , seven Brit Awards , six American Music Awards , four MTV Video Music Award s , an Academy Award , and a Golden Globe Award .", "prompt_labels": "He(O) won(O) seven(O) Grammy(B-award) Award(I-award) s(O) ,(O) seven(O) Brit(B-award) Awards(I-award) ,(O) six(O) American(B-award) Music(I-award) Awards(I-award) ,(O) four(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) s(O) ,(O) an(O) Academy(B-award) Award(I-award) ,(O) and(O) a(O) Golden(B-award) Globe(I-award) Award(I-award) .(O)"}}
{"id": "377", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "location", "song", "musical instrument", "organization", "award", "event", "person", "music genre", "country", "musical artist", "album"], "instance": {"id": "377", "words": ["In", "2001", ",", "he", "released", "Word", "of", "Mouf", ",", "followed", "by", "Chicken-n-Beer", "in", "2003", "and", "The", "Red", "Light", "District", "in", "2004", "."], "labels": ["O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "B-album", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, location, song, musical instrument, organization, award, event, person, music genre, country, musical artist, album and O.\nSentence: In 2001 , he released Word of Mouf , followed by Chicken-n-Beer in 2003 and The Red Light District in 2004 .", "prompt_labels": "In(O) 2001(O) ,(O) he(O) released(O) Word(B-album) of(I-album) Mouf(I-album) ,(O) followed(O) by(O) Chicken-n-Beer(B-album) in(O) 2003(O) and(O) The(B-album) Red(I-album) Light(I-album) District(I-album) in(O) 2004(O) .(O)"}}
{"id": "135", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "location", "person", "musical artist", "event", "song", "music genre", "country", "organization", "band", "album", "award"], "instance": {"id": "135", "words": ["Well-known", "disco", "artists", "include", "Donna", "Summer", ",", "Gloria", "Gaynor", ",", "the", "Bee", "Gees", ",", "Chic", ",", "KC", "and", "the", "Sunshine", "Band", ",", "Thelma", "Houston", ",", "Sister", "Sledge", ",", "The", "Trammps", ",", "the", "Village", "People", "and", "Michael", "Jackson", "."], "labels": ["O", "B-music genre", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "O", "B-band", "I-band", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, location, person, musical artist, event, song, music genre, country, organization, band, album, award and O.\nSentence: Well-known disco artists include Donna Summer , Gloria Gaynor , the Bee Gees , Chic , KC and the Sunshine Band , Thelma Houston , Sister Sledge , The Trammps , the Village People and Michael Jackson .", "prompt_labels": "Well-known(O) disco(B-music genre) artists(O) include(O) Donna(B-musical artist) Summer(I-musical artist) ,(O) Gloria(B-musical artist) Gaynor(I-musical artist) ,(O) the(O) Bee(B-band) Gees(I-band) ,(O) Chic(B-band) ,(O) KC(B-band) and(I-band) the(I-band) Sunshine(I-band) Band(I-band) ,(O) Thelma(B-musical artist) Houston(I-musical artist) ,(O) Sister(B-band) Sledge(I-band) ,(O) The(B-band) Trammps(I-band) ,(O) the(O) Village(B-band) People(I-band) and(O) Michael(B-person) Jackson(I-person) .(O)"}}
{"id": "157", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "person", "location", "event", "award", "album", "musical artist", "music genre", "organization", "country", "song", "band"], "instance": {"id": "157", "words": ["In", "the", "2010s", ",", "the", "alt-country", "genre", "saw", "an", "increase", "in", "its", "critical", "and", "commercial", "popularity", ",", "owing", "to", "the", "success", "of", "artists", "such", "as", "The", "Civil", "Wars", ",", "Chris", "Stapleton", ",", "Sturgill", "Simpson", ",", "Jason", "Isbell", ",", "Lydia", "Loveless", "and", "Margo", "Price", "."], "labels": ["O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, person, location, event, award, album, musical artist, music genre, organization, country, song, band and O.\nSentence: In the 2010s , the alt-country genre saw an increase in its critical and commercial popularity , owing to the success of artists such as The Civil Wars , Chris Stapleton , Sturgill Simpson , Jason Isbell , Lydia Loveless and Margo Price .", "prompt_labels": "In(O) the(O) 2010s(O) ,(O) the(O) alt-country(B-music genre) genre(I-music genre) saw(O) an(O) increase(O) in(O) its(O) critical(O) and(O) commercial(O) popularity(O) ,(O) owing(O) to(O) the(O) success(O) of(O) artists(O) such(O) as(O) The(B-band) Civil(I-band) Wars(I-band) ,(O) Chris(B-musical artist) Stapleton(I-musical artist) ,(O) Sturgill(B-musical artist) Simpson(I-musical artist) ,(O) Jason(B-musical artist) Isbell(I-musical artist) ,(O) Lydia(B-band) Loveless(I-band) and(O) Margo(B-musical artist) Price(I-musical artist) .(O)"}}
{"id": "128", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "musical artist", "music genre", "album", "person", "organization", "award", "musical instrument", "event", "location", "song", "band"], "instance": {"id": "128", "words": ["They", "restarted", "to", "perform", "live", "regularly", ",", "touring", "the", "world", "with", "rapturous", "feedbacks", ":", "they", "brought", "their", "distinguishable", "sound", "in", "great", "venues", "such", "as", "the", "Kings", "Place", "in", "London", ",", "the", "Soma", "Festival", "in", "Belfast", ",", "the", "Bolshoi", "Theatre", "in", "Moscow", "and", "the", "Star", "Pine", "'s", "cafe", "in", "Tokyo", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "O", "O", "B-event", "I-event", "O", "B-location", "O", "O", "B-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, musical artist, music genre, album, person, organization, award, musical instrument, event, location, song, band and O.\nSentence: They restarted to perform live regularly , touring the world with rapturous feedbacks : they brought their distinguishable sound in great venues such as the Kings Place in London , the Soma Festival in Belfast , the Bolshoi Theatre in Moscow and the Star Pine 's cafe in Tokyo .", "prompt_labels": "They(O) restarted(O) to(O) perform(O) live(O) regularly(O) ,(O) touring(O) the(O) world(O) with(O) rapturous(O) feedbacks(O) :(O) they(O) brought(O) their(O) distinguishable(O) sound(O) in(O) great(O) venues(O) such(O) as(O) the(O) Kings(B-location) Place(I-location) in(O) London(B-location) ,(O) the(O) Soma(B-event) Festival(I-event) in(O) Belfast(B-location) ,(O) the(O) Bolshoi(B-location) Theatre(I-location) in(O) Moscow(B-location) and(O) the(O) Star(B-location) Pine(I-location) 's(I-location) cafe(I-location) in(O) Tokyo(B-location) .(O)"}}
{"id": "64", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "country", "person", "band", "song", "musical artist", "award", "organization", "event", "music genre", "album", "musical instrument"], "instance": {"id": "64", "words": ["One", "of", "the", "best-selling", "music", "groups", "of", "the", "1960s", ",", "their", "biggest", "hits", "-", "including", "The", "Sound", "of", "Silence", "(", "1965", ")", ",", "Mrs.", "Robinson", "(", "1968", ")", ",", "The", "Boxer", "(", "1969", ")", ",", "and", "Bridge", "over", "Troubled", "Water", "(", "1970", ")", "-", "reached", "number", "one", "on", "singles", "charts", "worldwide", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "B-song", "I-song", "O", "O", "O", "O", "B-song", "I-song", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, person, band, song, musical artist, award, organization, event, music genre, album, musical instrument and O.\nSentence: One of the best-selling music groups of the 1960s , their biggest hits - including The Sound of Silence ( 1965 ) , Mrs. Robinson ( 1968 ) , The Boxer ( 1969 ) , and Bridge over Troubled Water ( 1970 ) - reached number one on singles charts worldwide .", "prompt_labels": "One(O) of(O) the(O) best-selling(O) music(O) groups(O) of(O) the(O) 1960s(O) ,(O) their(O) biggest(O) hits(O) -(O) including(O) The(B-song) Sound(I-song) of(I-song) Silence(I-song) ((O) 1965(O) )(O) ,(O) Mrs.(B-song) Robinson(I-song) ((O) 1968(O) )(O) ,(O) The(B-song) Boxer(I-song) ((O) 1969(O) )(O) ,(O) and(O) Bridge(B-album) over(I-album) Troubled(I-album) Water(I-album) ((O) 1970(O) )(O) -(O) reached(O) number(O) one(O) on(O) singles(O) charts(O) worldwide(O) .(O)"}}
{"id": "196", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "award", "musical instrument", "album", "musical artist", "country", "event", "song", "location", "music genre", "band", "person"], "instance": {"id": "196", "words": ["In", "July", "2010", ",", "Dayne", "released", "Facing", "a", "Miracle", ",", "the", "official", "theme", "song", "to", "the", "2010", "Gay", "Games", "."], "labels": ["O", "O", "O", "O", "B-musical artist", "O", "B-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, musical instrument, album, musical artist, country, event, song, location, music genre, band, person and O.\nSentence: In July 2010 , Dayne released Facing a Miracle , the official theme song to the 2010 Gay Games .", "prompt_labels": "In(O) July(O) 2010(O) ,(O) Dayne(B-musical artist) released(O) Facing(B-song) a(I-song) Miracle(I-song) ,(O) the(O) official(O) theme(O) song(O) to(O) the(O) 2010(B-event) Gay(I-event) Games(I-event) .(O)"}}
{"id": "132", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "award", "organization", "album", "musical instrument", "musical artist", "location", "music genre", "song", "event", "person", "country"], "instance": {"id": "132", "words": ["Since", "Between", "the", "Buttons", "(", "1967", ")", ",", "he", "has", "sung", "lead", "or", "co-lead", "on", "at", "least", "one", "track", "(", "see", "list", "below", ")", "of", "every", "Rolling", "Stones", "studio", "album", "except", "Their", "Satanic", "Majesties", "Request", ",", "Sticky", "Fingers", ",", "It", "'s", "Only", "Rock", "'", "n", "Roll", ",", "and", "Blue", "&", "Lonesome", "."], "labels": ["O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "B-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, award, organization, album, musical instrument, musical artist, location, music genre, song, event, person, country and O.\nSentence: Since Between the Buttons ( 1967 ) , he has sung lead or co-lead on at least one track ( see list below ) of every Rolling Stones studio album except Their Satanic Majesties Request , Sticky Fingers , It 's Only Rock ' n Roll , and Blue & Lonesome .", "prompt_labels": "Since(O) Between(B-album) the(I-album) Buttons(I-album) ((O) 1967(O) )(O) ,(O) he(O) has(O) sung(O) lead(O) or(O) co-lead(O) on(O) at(O) least(O) one(O) track(O) ((O) see(O) list(O) below(O) )(O) of(O) every(O) Rolling(B-band) Stones(I-band) studio(O) album(O) except(O) Their(B-album) Satanic(I-album) Majesties(I-album) Request(I-album) ,(O) Sticky(B-album) Fingers(I-album) ,(O) It(B-album) 's(I-album) Only(I-album) Rock(I-album) '(I-album) n(I-album) Roll(I-album) ,(O) and(O) Blue(B-album) &(I-album) Lonesome(I-album) .(O)"}}
{"id": "183", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "song", "album", "organization", "musical artist", "person", "music genre", "band", "musical instrument", "award", "country", "event"], "instance": {"id": "183", "words": ["In", "February", "1984", ",", "Queen", "released", "their", "eleventh", "studio", "album", ",", "The", "Works", ",", "which", "included", "the", "successful", "singles", "Radio", "Ga", "Ga", ",", "Hammer", "to", "Fall", "and", "I", "Want", "to", "Break", "Free", "."], "labels": ["O", "O", "O", "O", "B-band", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, song, album, organization, musical artist, person, music genre, band, musical instrument, award, country, event and O.\nSentence: In February 1984 , Queen released their eleventh studio album , The Works , which included the successful singles Radio Ga Ga , Hammer to Fall and I Want to Break Free .", "prompt_labels": "In(O) February(O) 1984(O) ,(O) Queen(B-band) released(O) their(O) eleventh(O) studio(O) album(O) ,(O) The(B-album) Works(I-album) ,(O) which(O) included(O) the(O) successful(O) singles(O) Radio(B-song) Ga(I-song) Ga(I-song) ,(O) Hammer(B-song) to(I-song) Fall(I-song) and(O) I(B-song) Want(I-song) to(I-song) Break(I-song) Free(I-song) .(O)"}}
{"id": "90", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "music genre", "location", "musical artist", "person", "organization", "award", "musical instrument", "event", "song", "band", "album"], "instance": {"id": "90", "words": ["Electropop", "pioneers", "Haruomi", "Hosono", "and", "Ryuichi", "Sakamoto", "of", "the", "Yellow", "Magic", "Orchestra", "produced", "a", "1978", "Electronic", "music", "album", ",", "Cochin", "Moon", ",", "based", "on", "an", "experimental", "fusion", "of", "electronic", "music", "and", "Bollywood-inspired", "Indian", "music", "."], "labels": ["B-music genre", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "B-band", "I-band", "I-band", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, music genre, location, musical artist, person, organization, award, musical instrument, event, song, band, album and O.\nSentence: Electropop pioneers Haruomi Hosono and Ryuichi Sakamoto of the Yellow Magic Orchestra produced a 1978 Electronic music album , Cochin Moon , based on an experimental fusion of electronic music and Bollywood-inspired Indian music .", "prompt_labels": "Electropop(B-music genre) pioneers(O) Haruomi(B-musical artist) Hosono(I-musical artist) and(O) Ryuichi(B-musical artist) Sakamoto(I-musical artist) of(O) the(O) Yellow(B-band) Magic(I-band) Orchestra(I-band) produced(O) a(O) 1978(O) Electronic(B-music genre) music(I-music genre) album(O) ,(O) Cochin(B-album) Moon(I-album) ,(O) based(O) on(O) an(O) experimental(O) fusion(O) of(O) electronic(B-music genre) music(I-music genre) and(O) Bollywood-inspired(B-music genre) Indian(I-music genre) music(I-music genre) .(O)"}}
{"id": "17", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "country", "event", "location", "award", "band", "musical artist", "person", "song", "album", "organization", "music genre"], "instance": {"id": "17", "words": ["Artists", "from", "outside", "California", "who", "were", "associated", "with", "early", "alternative", "country", "included", "singer-songwriters", "such", "as", "Lucinda", "Williams", ",", "Lyle", "Lovett", "and", "Steve", "Earle", ",", "the", "Nashville", "country", "rock", "band", "Jason", "and", "the", "Scorchers", "and", "the", "British", "post-punk", "band", "The", "Mekons", "."], "labels": ["O", "O", "O", "B-location", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "I-band", "O", "O", "O", "B-music genre", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, country, event, location, award, band, musical artist, person, song, album, organization, music genre and O.\nSentence: Artists from outside California who were associated with early alternative country included singer-songwriters such as Lucinda Williams , Lyle Lovett and Steve Earle , the Nashville country rock band Jason and the Scorchers and the British post-punk band The Mekons .", "prompt_labels": "Artists(O) from(O) outside(O) California(B-location) who(O) were(O) associated(O) with(O) early(O) alternative(B-music genre) country(I-music genre) included(O) singer-songwriters(O) such(O) as(O) Lucinda(B-musical artist) Williams(I-musical artist) ,(O) Lyle(B-musical artist) Lovett(I-musical artist) and(O) Steve(B-musical artist) Earle(I-musical artist) ,(O) the(O) Nashville(O) country(O) rock(O) band(O) Jason(B-band) and(I-band) the(I-band) Scorchers(I-band) and(O) the(O) British(O) post-punk(B-music genre) band(O) The(B-band) Mekons(I-band) .(O)"}}
{"id": "234", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "musical instrument", "song", "location", "event", "country", "person", "award", "band", "album", "musical artist", "organization"], "instance": {"id": "234", "words": ["Bonnie", "received", "two", "Grammy", "Award", "nominations", "-", "for", "Grammy", "Award", "for", "Best", "Female", "Pop", "Vocal", "Performance", "and", "Grammy", "Award", "for", "Best", "Female", "Rock", "Vocal", "Performance", "."], "labels": ["B-musical artist", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical instrument, song, location, event, country, person, award, band, album, musical artist, organization and O.\nSentence: Bonnie received two Grammy Award nominations - for Grammy Award for Best Female Pop Vocal Performance and Grammy Award for Best Female Rock Vocal Performance .", "prompt_labels": "Bonnie(B-musical artist) received(O) two(O) Grammy(B-award) Award(I-award) nominations(O) -(O) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Female(I-award) Pop(I-award) Vocal(I-award) Performance(I-award) and(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Female(I-award) Rock(I-award) Vocal(I-award) Performance(I-award) .(O)"}}
{"id": "42", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "band", "organization", "musical instrument", "musical artist", "award", "song", "country", "music genre", "location", "album", "person"], "instance": {"id": "42", "words": ["In", "1956", ",", "the", "arrival", "of", "rockabilly", "was", "underlined", "by", "the", "success", "of", "songs", "like", "Folsom", "Prison", "Blues", "by", "Johnny", "Cash", ",", "Blue", "Suede", "Shoes", "by", "Perkins", "and", "the", "No.", "1", "hit", "Heartbreak", "Hotel", "by", "Presley", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O", "B-musical artist", "I-musical artist", "O", "B-song", "I-song", "I-song", "O", "B-musical artist", "O", "O", "O", "O", "O", "B-song", "I-song", "O", "B-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, band, organization, musical instrument, musical artist, award, song, country, music genre, location, album, person and O.\nSentence: In 1956 , the arrival of rockabilly was underlined by the success of songs like Folsom Prison Blues by Johnny Cash , Blue Suede Shoes by Perkins and the No. 1 hit Heartbreak Hotel by Presley .", "prompt_labels": "In(O) 1956(O) ,(O) the(O) arrival(O) of(O) rockabilly(O) was(O) underlined(O) by(O) the(O) success(O) of(O) songs(O) like(O) Folsom(B-song) Prison(I-song) Blues(I-song) by(O) Johnny(B-musical artist) Cash(I-musical artist) ,(O) Blue(B-song) Suede(I-song) Shoes(I-song) by(O) Perkins(B-musical artist) and(O) the(O) No.(O) 1(O) hit(O) Heartbreak(B-song) Hotel(I-song) by(O) Presley(B-musical artist) .(O)"}}
{"id": "349", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "event", "musical artist", "album", "organization", "location", "music genre", "band", "musical instrument", "person", "award", "country"], "instance": {"id": "349", "words": ["In", "the", "days", "following", "his", "death", ",", "tributes", "were", "paid", "by", "then-President", "George", "W.", "Bush", ",", "the", "United", "States", "House", "of", "Representatives", ",", "and", "many", "musicians", "and", "performers", ",", "including", "B.", "B.", "King", ",", "Ronnie", "Hawkins", ",", "Mick", "Jagger", ",", "Ronnie", "Wood", ",", "George", "Thorogood", ",", "Eric", "Clapton", ",", "Tom", "Petty", ",", "Robert", "Plant", ",", "Elvis", "Costello", ",", "Bonnie", "Raitt", ",", "Robert", "Randolph", "and", "the", "Family", "Band", "and", "Eric", "Burdon", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "I-band", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, musical artist, album, organization, location, music genre, band, musical instrument, person, award, country and O.\nSentence: In the days following his death , tributes were paid by then-President George W. Bush , the United States House of Representatives , and many musicians and performers , including B. B. King , Ronnie Hawkins , Mick Jagger , Ronnie Wood , George Thorogood , Eric Clapton , Tom Petty , Robert Plant , Elvis Costello , Bonnie Raitt , Robert Randolph and the Family Band and Eric Burdon .", "prompt_labels": "In(O) the(O) days(O) following(O) his(O) death(O) ,(O) tributes(O) were(O) paid(O) by(O) then-President(O) George(B-person) W.(I-person) Bush(I-person) ,(O) the(O) United(O) States(O) House(O) of(O) Representatives(O) ,(O) and(O) many(O) musicians(O) and(O) performers(O) ,(O) including(O) B.(B-musical artist) B.(I-musical artist) King(I-musical artist) ,(O) Ronnie(B-musical artist) Hawkins(I-musical artist) ,(O) Mick(B-musical artist) Jagger(I-musical artist) ,(O) Ronnie(B-musical artist) Wood(I-musical artist) ,(O) George(B-musical artist) Thorogood(I-musical artist) ,(O) Eric(B-musical artist) Clapton(I-musical artist) ,(O) Tom(B-musical artist) Petty(I-musical artist) ,(O) Robert(B-musical artist) Plant(I-musical artist) ,(O) Elvis(B-musical artist) Costello(I-musical artist) ,(O) Bonnie(B-musical artist) Raitt(I-musical artist) ,(O) Robert(B-band) Randolph(I-band) and(I-band) the(I-band) Family(I-band) Band(I-band) and(O) Eric(B-musical artist) Burdon(I-musical artist) .(O)"}}
{"id": "48", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "event", "location", "country", "music genre", "song", "person", "musical instrument", "musical artist", "organization", "award", "album"], "instance": {"id": "48", "words": ["All", "ten", "of", "Cannibal", "Corpse", "'s", "albums", ",", "the", "live", "album", "Live", "Cannibalism", ",", "the", "boxed", "set", "15", "Year", "Killing", "Spree", ",", "the", "EP", "Worm", "Infested", ",", "and", "the", "single", "Hammer", "Smashed", "Face", "were", "re-released", "in", "Australia", "between", "2006", "and", "2007", ",", "finally", "classified", "by", "ARIA", "and", "allowed", "for", "sale", "in", "Australia", "."], "labels": ["O", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, event, location, country, music genre, song, person, musical instrument, musical artist, organization, award, album and O.\nSentence: All ten of Cannibal Corpse 's albums , the live album Live Cannibalism , the boxed set 15 Year Killing Spree , the EP Worm Infested , and the single Hammer Smashed Face were re-released in Australia between 2006 and 2007 , finally classified by ARIA and allowed for sale in Australia .", "prompt_labels": "All(O) ten(O) of(O) Cannibal(B-band) Corpse(I-band) 's(O) albums(O) ,(O) the(O) live(O) album(O) Live(B-album) Cannibalism(I-album) ,(O) the(O) boxed(O) set(O) 15(B-album) Year(I-album) Killing(I-album) Spree(I-album) ,(O) the(O) EP(O) Worm(B-album) Infested(I-album) ,(O) and(O) the(O) single(O) Hammer(B-song) Smashed(I-song) Face(I-song) were(O) re-released(O) in(O) Australia(B-country) between(O) 2006(O) and(O) 2007(O) ,(O) finally(O) classified(O) by(O) ARIA(B-organization) and(O) allowed(O) for(O) sale(O) in(O) Australia(B-country) .(O)"}}
{"id": "19", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "musical instrument", "music genre", "location", "musical artist", "band", "song", "person", "country", "event", "album", "award"], "instance": {"id": "19", "words": ["The", "film", "was", "nominated", "for", "the", "Academy", "Awards", "for", "Academy", "Award", "for", "Best", "Picture", ",", "as", "well", "as", "Academy", "Award", "for", "Best", "Production", "Design", "(", "Carroll", "Clark", "and", "Van", "Nest", "Polglase", ")", ",", "Academy", "Award", "for", "Best", "Original", "Song", "(", "Irving", "Berlin", "for", "Cheek", "to", "Cheek", ")", ",", "and", "Dance", "Direction", "(", "Hermes", "Pan", "for", "Piccolino", "and", "Top", "Hat", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-musical artist", "I-musical artist", "O", "B-song", "I-song", "I-song", "O", "O", "O", "B-award", "I-award", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, musical instrument, music genre, location, musical artist, band, song, person, country, event, album, award and O.\nSentence: The film was nominated for the Academy Awards for Academy Award for Best Picture , as well as Academy Award for Best Production Design ( Carroll Clark and Van Nest Polglase ) , Academy Award for Best Original Song ( Irving Berlin for Cheek to Cheek ) , and Dance Direction ( Hermes Pan for Piccolino and Top Hat ) .", "prompt_labels": "The(O) film(O) was(O) nominated(O) for(O) the(O) Academy(B-award) Awards(I-award) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ,(O) as(O) well(O) as(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Production(I-award) Design(I-award) ((O) Carroll(B-person) Clark(I-person) and(O) Van(B-person) Nest(I-person) Polglase(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Song(I-award) ((O) Irving(B-musical artist) Berlin(I-musical artist) for(O) Cheek(B-song) to(I-song) Cheek(I-song) )(O) ,(O) and(O) Dance(B-award) Direction(I-award) ((O) Hermes(B-person) Pan(I-person) for(O) Piccolino(O) and(O) Top(O) Hat(O) )(O) .(O)"}}
{"id": "247", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "event", "musical artist", "song", "location", "musical instrument", "country", "album", "band", "award", "organization", "person"], "instance": {"id": "247", "words": ["However", ",", "much", "of", "his", "output", ",", "which", "included", "A", "Poke", "in", "the", "Eye", "...", "With", "a", "Sharp", "Stick", "!", ",", "Praise", "the", "Lard", ",", "A", "Stroll", "in", "the", "Pork", "and", "The", "Swining", ",", "was", "scattered", "across", "numerous", "labels", "around", "the", "world", "and", "often", "proved", "difficult", "to", "find", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, event, musical artist, song, location, musical instrument, country, album, band, award, organization, person and O.\nSentence: However , much of his output , which included A Poke in the Eye ... With a Sharp Stick ! , Praise the Lard , A Stroll in the Pork and The Swining , was scattered across numerous labels around the world and often proved difficult to find .", "prompt_labels": "However(O) ,(O) much(O) of(O) his(O) output(O) ,(O) which(O) included(O) A(B-album) Poke(I-album) in(I-album) the(I-album) Eye(I-album) ...(I-album) With(I-album) a(I-album) Sharp(I-album) Stick(I-album) !(I-album) ,(O) Praise(B-album) the(I-album) Lard(I-album) ,(O) A(B-album) Stroll(I-album) in(I-album) the(I-album) Pork(I-album) and(O) The(B-album) Swining(I-album) ,(O) was(O) scattered(O) across(O) numerous(O) labels(O) around(O) the(O) world(O) and(O) often(O) proved(O) difficult(O) to(O) find(O) .(O)"}}
{"id": "219", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "event", "band", "song", "music genre", "location", "album", "musical instrument", "musical artist", "award", "person", "organization"], "instance": {"id": "219", "words": ["In", "1968", ",", "three", "of", "the", "genre", "'s", "most", "famous", "pioneers", ",", "Led", "Zeppelin", ",", "Black", "Sabbath", "and", "Deep", "Purple", "were", "founded", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, band, song, music genre, location, album, musical instrument, musical artist, award, person, organization and O.\nSentence: In 1968 , three of the genre 's most famous pioneers , Led Zeppelin , Black Sabbath and Deep Purple were founded .", "prompt_labels": "In(O) 1968(O) ,(O) three(O) of(O) the(O) genre(O) 's(O) most(O) famous(O) pioneers(O) ,(O) Led(B-band) Zeppelin(I-band) ,(O) Black(B-band) Sabbath(I-band) and(O) Deep(B-band) Purple(I-band) were(O) founded(O) .(O)"}}
{"id": "12", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "album", "organization", "country", "person", "event", "musical artist", "band", "musical instrument", "award", "location", "song"], "instance": {"id": "12", "words": ["Pop", "'s", "music", "has", "encompassed", "a", "number", "of", "styles", "over", "the", "course", "of", "his", "career", ",", "including", "garage", "rock", ",", "punk", "rock", ",", "hard", "rock", ",", "Heavy", "metal", "music", ",", "art", "rock", ",", "New", "wave", "music", ",", "jazz", ",", "blues", ",", "and", "Electronic", "music", "."], "labels": ["B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, album, organization, country, person, event, musical artist, band, musical instrument, award, location, song and O.\nSentence: Pop 's music has encompassed a number of styles over the course of his career , including garage rock , punk rock , hard rock , Heavy metal music , art rock , New wave music , jazz , blues , and Electronic music .", "prompt_labels": "Pop(B-music genre) 's(O) music(O) has(O) encompassed(O) a(O) number(O) of(O) styles(O) over(O) the(O) course(O) of(O) his(O) career(O) ,(O) including(O) garage(B-music genre) rock(I-music genre) ,(O) punk(B-music genre) rock(I-music genre) ,(O) hard(B-music genre) rock(I-music genre) ,(O) Heavy(B-music genre) metal(I-music genre) music(I-music genre) ,(O) art(B-music genre) rock(I-music genre) ,(O) New(B-music genre) wave(I-music genre) music(I-music genre) ,(O) jazz(B-music genre) ,(O) blues(B-music genre) ,(O) and(O) Electronic(B-music genre) music(I-music genre) .(O)"}}
{"id": "118", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "event", "musical artist", "album", "award", "music genre", "person", "song", "musical instrument", "organization", "band", "country"], "instance": {"id": "118", "words": ["Urban", "male", "performers", "included", "popular", "black", "musicians", "of", "the", "era", ",", "such", "as", "Tampa", "Red", ",", "Big", "Bill", "Broonzy", "and", "Leroy", "Carr", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, musical artist, album, award, music genre, person, song, musical instrument, organization, band, country and O.\nSentence: Urban male performers included popular black musicians of the era , such as Tampa Red , Big Bill Broonzy and Leroy Carr .", "prompt_labels": "Urban(O) male(O) performers(O) included(O) popular(O) black(O) musicians(O) of(O) the(O) era(O) ,(O) such(O) as(O) Tampa(B-musical artist) Red(I-musical artist) ,(O) Big(B-musical artist) Bill(I-musical artist) Broonzy(I-musical artist) and(O) Leroy(B-musical artist) Carr(I-musical artist) .(O)"}}
{"id": "69", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "award", "location", "musical artist", "country", "musical instrument", "organization", "person", "album", "event", "band", "music genre"], "instance": {"id": "69", "words": ["With", "the", "eventual", "commercial", "dominance", "of", "West", "Coast", "hip", "hop", "gangsta", "rap", ",", "particularly", "the", "emergence", "of", "the", "relaxed", "sounds", "of", "G-funk", "by", "the", "early", "nineties", ",", "the", "East", "Coast", "hip", "hop", "new", "school", "/", "golden", "age", "can", "be", "said", "to", "have", "ended", ",", "with", "hardcore", "rappers", "such", "as", "the", "Wu-Tang", "Clan", "and", "gangsta", "rappers", "such", "as", "Nas", "and", "The", "Notorious", "B.I.G.", "coming", "to", "dominate", "the", "East", "Coast", "scene", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "I-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "B-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, award, location, musical artist, country, musical instrument, organization, person, album, event, band, music genre and O.\nSentence: With the eventual commercial dominance of West Coast hip hop gangsta rap , particularly the emergence of the relaxed sounds of G-funk by the early nineties , the East Coast hip hop new school / golden age can be said to have ended , with hardcore rappers such as the Wu-Tang Clan and gangsta rappers such as Nas and The Notorious B.I.G. coming to dominate the East Coast scene .", "prompt_labels": "With(O) the(O) eventual(O) commercial(O) dominance(O) of(O) West(B-music genre) Coast(I-music genre) hip(I-music genre) hop(I-music genre) gangsta(I-music genre) rap(I-music genre) ,(O) particularly(O) the(O) emergence(O) of(O) the(O) relaxed(O) sounds(O) of(O) G-funk(B-music genre) by(O) the(O) early(O) nineties(O) ,(O) the(O) East(B-event) Coast(I-event) hip(I-event) hop(I-event) new(I-event) school(I-event) /(O) golden(O) age(O) can(O) be(O) said(O) to(O) have(O) ended(O) ,(O) with(O) hardcore(O) rappers(O) such(O) as(O) the(O) Wu-Tang(B-band) Clan(I-band) and(O) gangsta(O) rappers(O) such(O) as(O) Nas(B-musical artist) and(O) The(B-musical artist) Notorious(I-musical artist) B.I.G.(I-musical artist) coming(O) to(O) dominate(O) the(O) East(B-music genre) Coast(I-music genre) scene(O) .(O)"}}
{"id": "324", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "event", "music genre", "band", "organization", "person", "award", "musical instrument", "location", "country", "album", "musical artist"], "instance": {"id": "324", "words": ["The", "show", "began", "at", "Melbourne", "'s", "Rod", "Laver", "Arena", "and", "proceeded", "to", "Sydney", "Entertainment", "Centre", ",", "Brisbane", "Entertainment", "Centre", ",", "the", "Adelaide", "Entertainment", "Centre", ",", "and", "the", "Perth", "Arena", "."], "labels": ["O", "O", "O", "O", "B-location", "O", "B-location", "I-location", "I-location", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, music genre, band, organization, person, award, musical instrument, location, country, album, musical artist and O.\nSentence: The show began at Melbourne 's Rod Laver Arena and proceeded to Sydney Entertainment Centre , Brisbane Entertainment Centre , the Adelaide Entertainment Centre , and the Perth Arena .", "prompt_labels": "The(O) show(O) began(O) at(O) Melbourne(B-location) 's(O) Rod(B-location) Laver(I-location) Arena(I-location) and(O) proceeded(O) to(O) Sydney(B-location) Entertainment(I-location) Centre(I-location) ,(O) Brisbane(B-location) Entertainment(I-location) Centre(I-location) ,(O) the(O) Adelaide(B-location) Entertainment(I-location) Centre(I-location) ,(O) and(O) the(O) Perth(B-location) Arena(I-location) .(O)"}}
{"id": "180", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "album", "band", "award", "musical instrument", "organization", "person", "country", "event", "location", "song", "musical artist"], "instance": {"id": "180", "words": ["Alexander", "von", "Meilenwald", "from", "German", "band", "Nagelfar", "considers", "Ungod", "'", "s", "1993", "debut", "Circle", "of", "the", "Seven", "Infernal", "Pacts", ",", "Desaster", "'", "s", "1994", "demo", "Lost", "in", "the", "Ages", ",", "Tha-Norr", "'", "s", "1995", "album", "Wolfenzeitalter", ",", "Lunar", "Aurora", "'", "s", "1996", "debut", "Weltengänger", "and", "Katharsis", "'", "s", "2000", "debut", "666", "Alexander", "von", "Meilenwald", ":", "5", "Klassiker", "."], "labels": ["B-musical artist", "I-musical artist", "I-musical artist", "O", "B-country", "O", "B-band", "O", "B-band", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "I-album", "O", "B-band", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-band", "O", "O", "O", "O", "B-album", "O", "B-band", "I-band", "O", "O", "O", "O", "B-album", "O", "B-band", "O", "O", "O", "O", "B-album", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, album, band, award, musical instrument, organization, person, country, event, location, song, musical artist and O.\nSentence: Alexander von Meilenwald from German band Nagelfar considers Ungod ' s 1993 debut Circle of the Seven Infernal Pacts , Desaster ' s 1994 demo Lost in the Ages , Tha-Norr ' s 1995 album Wolfenzeitalter , Lunar Aurora ' s 1996 debut Weltengänger and Katharsis ' s 2000 debut 666 Alexander von Meilenwald : 5 Klassiker .", "prompt_labels": "Alexander(B-musical artist) von(I-musical artist) Meilenwald(I-musical artist) from(O) German(B-country) band(O) Nagelfar(B-band) considers(O) Ungod(B-band) '(O) s(O) 1993(O) debut(O) Circle(B-album) of(I-album) the(I-album) Seven(I-album) Infernal(I-album) Pacts(I-album) ,(O) Desaster(B-band) '(O) s(O) 1994(O) demo(O) Lost(B-album) in(I-album) the(I-album) Ages(I-album) ,(O) Tha-Norr(B-band) '(O) s(O) 1995(O) album(O) Wolfenzeitalter(B-album) ,(O) Lunar(B-band) Aurora(I-band) '(O) s(O) 1996(O) debut(O) Weltengänger(B-album) and(O) Katharsis(B-band) '(O) s(O) 2000(O) debut(O) 666(B-album) Alexander(B-musical artist) von(I-musical artist) Meilenwald(I-musical artist) :(O) 5(O) Klassiker(O) .(O)"}}
{"id": "211", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "album", "location", "song", "musical instrument", "country", "musical artist", "band", "organization", "award", "person", "music genre"], "instance": {"id": "211", "words": ["Sometimes", "the", "most", "successful", "solo", "star", "from", "a", "band", "is", "not", "the", "most", "popular", "member", "such", "as", "Robbie", "Williams", "as", "opposed", "to", "lead", "singer", "Gary", "Barlow", "from", "Take", "That", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, album, location, song, musical instrument, country, musical artist, band, organization, award, person, music genre and O.\nSentence: Sometimes the most successful solo star from a band is not the most popular member such as Robbie Williams as opposed to lead singer Gary Barlow from Take That .", "prompt_labels": "Sometimes(O) the(O) most(O) successful(O) solo(O) star(O) from(O) a(O) band(O) is(O) not(O) the(O) most(O) popular(O) member(O) such(O) as(O) Robbie(B-musical artist) Williams(I-musical artist) as(O) opposed(O) to(O) lead(O) singer(O) Gary(B-musical artist) Barlow(I-musical artist) from(O) Take(B-band) That(I-band) .(O)"}}
{"id": "361", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "album", "award", "musical instrument", "organization", "country", "location", "song", "person", "musical artist", "event", "band"], "instance": {"id": "361", "words": ["Featuring", "West", "Coast", "hip", "hop", ",", "G-funk", "and", "horrorcore", "musical", "styles", ",", "the", "majority", "of", "The", "Slim", "Shady", "LP", "nowiki", "/", "'", "s", "lyrical", "content", "is", "written", "from", "the", "perspective", "of", "Eminem", "'s", "alter", "ego", "Slim", "Shady", ",", "whom", "he", "created", "on", "the", "Slim", "Shady", "EP", "(", "1997", ")", "."], "labels": ["O", "B-music genre", "I-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, album, award, musical instrument, organization, country, location, song, person, musical artist, event, band and O.\nSentence: Featuring West Coast hip hop , G-funk and horrorcore musical styles , the majority of The Slim Shady LP nowiki / ' s lyrical content is written from the perspective of Eminem 's alter ego Slim Shady , whom he created on the Slim Shady EP ( 1997 ) .", "prompt_labels": "Featuring(O) West(B-music genre) Coast(I-music genre) hip(I-music genre) hop(I-music genre) ,(O) G-funk(B-music genre) and(O) horrorcore(B-music genre) musical(O) styles(O) ,(O) the(O) majority(O) of(O) The(B-album) Slim(I-album) Shady(I-album) LP(I-album) nowiki(O) /(O) '(O) s(O) lyrical(O) content(O) is(O) written(O) from(O) the(O) perspective(O) of(O) Eminem(B-musical artist) 's(O) alter(O) ego(O) Slim(B-musical artist) Shady(I-musical artist) ,(O) whom(O) he(O) created(O) on(O) the(O) Slim(B-album) Shady(I-album) EP(I-album) ((O) 1997(O) )(O) .(O)"}}
{"id": "129", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "award", "album", "song", "music genre", "organization", "person", "location", "musical instrument", "band", "event", "musical artist"], "instance": {"id": "129", "words": ["The", "Hall", "is", "flanked", "by", "The", "Royal", "Lyceum", "Theatre", "on", "the", "right", "and", "The", "Traverse", "Theatre", "on", "the", "left", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, album, song, music genre, organization, person, location, musical instrument, band, event, musical artist and O.\nSentence: The Hall is flanked by The Royal Lyceum Theatre on the right and The Traverse Theatre on the left .", "prompt_labels": "The(O) Hall(O) is(O) flanked(O) by(O) The(O) Royal(B-location) Lyceum(I-location) Theatre(I-location) on(O) the(O) right(O) and(O) The(O) Traverse(B-location) Theatre(I-location) on(O) the(O) left(O) .(O)"}}
{"id": "102", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "musical instrument", "song", "musical artist", "country", "event", "band", "music genre", "person", "award", "location", "organization"], "instance": {"id": "102", "words": ["Steve", "Young", ",", "David", "Allan", "Coe", ",", "John", "Prine", ",", "Billy", "Joe", "Shaver", ",", "Gary", "Stewart", ",", "Townes", "Van", "Zandt", ",", "Kris", "Kristofferson", ",", "Michael", "Martin", "Murphey", ",", "Tompall", "Glaser", ",", "Steve", "Earle", ",", "and", "the", "later", "career", "renaissance", "of", "Johnny", "Cash", ",", "along", "with", "a", "few", "female", "vocalists", "such", "as", "Jessi", "Colter", ",", "Sammi", "Smith", ",", "Tanya", "Tucker", ",", "and", "Rosanne", "Cash", "."], "labels": ["B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical instrument, song, musical artist, country, event, band, music genre, person, award, location, organization and O.\nSentence: Steve Young , David Allan Coe , John Prine , Billy Joe Shaver , Gary Stewart , Townes Van Zandt , Kris Kristofferson , Michael Martin Murphey , Tompall Glaser , Steve Earle , and the later career renaissance of Johnny Cash , along with a few female vocalists such as Jessi Colter , Sammi Smith , Tanya Tucker , and Rosanne Cash .", "prompt_labels": "Steve(B-musical artist) Young(I-musical artist) ,(O) David(B-musical artist) Allan(I-musical artist) Coe(I-musical artist) ,(O) John(B-musical artist) Prine(I-musical artist) ,(O) Billy(B-musical artist) Joe(I-musical artist) Shaver(I-musical artist) ,(O) Gary(B-musical artist) Stewart(I-musical artist) ,(O) Townes(B-musical artist) Van(I-musical artist) Zandt(I-musical artist) ,(O) Kris(B-musical artist) Kristofferson(I-musical artist) ,(O) Michael(B-musical artist) Martin(I-musical artist) Murphey(I-musical artist) ,(O) Tompall(B-musical artist) Glaser(I-musical artist) ,(O) Steve(B-musical artist) Earle(I-musical artist) ,(O) and(O) the(O) later(O) career(O) renaissance(O) of(O) Johnny(B-musical artist) Cash(I-musical artist) ,(O) along(O) with(O) a(O) few(O) female(O) vocalists(O) such(O) as(O) Jessi(B-musical artist) Colter(I-musical artist) ,(O) Sammi(B-musical artist) Smith(I-musical artist) ,(O) Tanya(B-musical artist) Tucker(I-musical artist) ,(O) and(O) Rosanne(B-musical artist) Cash(I-musical artist) .(O)"}}
{"id": "14", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "song", "person", "country", "band", "music genre", "musical artist", "organization", "album", "award", "location", "event"], "instance": {"id": "14", "words": ["Phoenix", "has", "long", "been", "a", "social", "activist", ",", "lending", "his", "support", "to", "a", "number", "of", "charities", "and", "humanitarian", "organizations", ",", "such", "as", "Amnesty", "International", ",", "The", "Art", "of", "Elysium", ",", "HEART", ",", "and", "the", "Peace", "Alliance", "(", "which", "campaigns", "for", "a", "United", "States", "Department", "of", "Peace", ")", "."], "labels": ["B-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "B-country", "I-country", "B-organization", "I-organization", "I-organization", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, song, person, country, band, music genre, musical artist, organization, album, award, location, event and O.\nSentence: Phoenix has long been a social activist , lending his support to a number of charities and humanitarian organizations , such as Amnesty International , The Art of Elysium , HEART , and the Peace Alliance ( which campaigns for a United States Department of Peace ) .", "prompt_labels": "Phoenix(B-band) has(O) long(O) been(O) a(O) social(O) activist(O) ,(O) lending(O) his(O) support(O) to(O) a(O) number(O) of(O) charities(O) and(O) humanitarian(O) organizations(O) ,(O) such(O) as(O) Amnesty(B-organization) International(I-organization) ,(O) The(B-organization) Art(I-organization) of(I-organization) Elysium(I-organization) ,(O) HEART(B-organization) ,(O) and(O) the(O) Peace(B-organization) Alliance(I-organization) ((O) which(O) campaigns(O) for(O) a(O) United(B-country) States(I-country) Department(B-organization) of(I-organization) Peace(I-organization) )(O) .(O)"}}
{"id": "299", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "award", "event", "musical artist", "album", "person", "musical instrument", "song", "band", "country", "organization", "location"], "instance": {"id": "299", "words": ["Tim", "Burton", "appeared", "at", "the", "2009", "Comic-Con", "in", "San", "Diego", ",", "California", ",", "to", "promote", "both", "9", "and", "Alice", "in", "Wonderland", ",", "the", "latter", "won", "two", "Academy", "Awards", ",", "for", "Academy", "Award", "for", "Best", "Production", "Design", "and", "Academy", "Award", "for", "Best", "Costume", "Design", "."], "labels": ["B-person", "I-person", "O", "O", "O", "B-event", "I-event", "O", "B-location", "I-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, award, event, musical artist, album, person, musical instrument, song, band, country, organization, location and O.\nSentence: Tim Burton appeared at the 2009 Comic-Con in San Diego , California , to promote both 9 and Alice in Wonderland , the latter won two Academy Awards , for Academy Award for Best Production Design and Academy Award for Best Costume Design .", "prompt_labels": "Tim(B-person) Burton(I-person) appeared(O) at(O) the(O) 2009(B-event) Comic-Con(I-event) in(O) San(B-location) Diego(I-location) ,(O) California(B-location) ,(O) to(O) promote(O) both(O) 9(O) and(O) Alice(O) in(O) Wonderland(O) ,(O) the(O) latter(O) won(O) two(O) Academy(B-award) Awards(I-award) ,(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Production(I-award) Design(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Costume(I-award) Design(I-award) .(O)"}}
{"id": "91", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "country", "organization", "music genre", "event", "location", "award", "musical instrument", "album", "musical artist", "person", "band"], "instance": {"id": "91", "words": ["Black", "Holes", "and", "Revelations", "(", "2006", ")", "incorporated", "Electronic", "music", "and", "Pop", "music", "elements", ",", "displayed", "in", "singles", "such", "as", "Supermassive", "Black", "Hole", ",", "Their", "seventh", "album", ",", "Drones", "(", "2015", ")", ",", "was", "a", "concept", "album", "about", "drone", "warfare", "and", "returned", "to", "a", "harder", "rock", "sound", "."], "labels": ["B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O", "O", "O", "O", "O", "B-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "B-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, country, organization, music genre, event, location, award, musical instrument, album, musical artist, person, band and O.\nSentence: Black Holes and Revelations ( 2006 ) incorporated Electronic music and Pop music elements , displayed in singles such as Supermassive Black Hole , Their seventh album , Drones ( 2015 ) , was a concept album about drone warfare and returned to a harder rock sound .", "prompt_labels": "Black(B-album) Holes(I-album) and(I-album) Revelations(I-album) ((O) 2006(O) )(O) incorporated(O) Electronic(B-music genre) music(I-music genre) and(O) Pop(B-music genre) music(I-music genre) elements(O) ,(O) displayed(O) in(O) singles(O) such(O) as(O) Supermassive(B-song) Black(I-song) Hole(I-song) ,(O) Their(O) seventh(O) album(O) ,(O) Drones(B-album) ((O) 2015(O) )(O) ,(O) was(O) a(O) concept(O) album(O) about(O) drone(B-album) warfare(I-album) and(O) returned(O) to(O) a(O) harder(O) rock(B-music genre) sound(O) .(O)"}}
{"id": "342", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "country", "location", "musical instrument", "award", "song", "person", "album", "organization", "music genre", "musical artist", "event"], "instance": {"id": "342", "words": ["During", "the", "'", "90s", ",", "Southern", "California", "Chicano", "artists", ",", "such", "as", "Kid", "Frost", ",", "A.L.T.", ",", "A", "Lighter", "Shade", "of", "Brown", ",", "B-Real", ",", "Psycho", "Realm", ",", "Gunter", ",", "Delinquent", "Habits", "and", "Jonny", "Z", "received", "mainstream", "success", "."], "labels": ["O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "O", "B-musical artist", "O", "B-band", "I-band", "O", "B-musical artist", "O", "B-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, country, location, musical instrument, award, song, person, album, organization, music genre, musical artist, event and O.\nSentence: During the ' 90s , Southern California Chicano artists , such as Kid Frost , A.L.T. , A Lighter Shade of Brown , B-Real , Psycho Realm , Gunter , Delinquent Habits and Jonny Z received mainstream success .", "prompt_labels": "During(O) the(O) '(O) 90s(O) ,(O) Southern(B-location) California(I-location) Chicano(O) artists(O) ,(O) such(O) as(O) Kid(B-musical artist) Frost(I-musical artist) ,(O) A.L.T.(B-musical artist) ,(O) A(B-band) Lighter(I-band) Shade(I-band) of(I-band) Brown(I-band) ,(O) B-Real(B-musical artist) ,(O) Psycho(B-band) Realm(I-band) ,(O) Gunter(B-musical artist) ,(O) Delinquent(B-band) Habits(I-band) and(O) Jonny(B-musical artist) Z(I-musical artist) received(O) mainstream(O) success(O) .(O)"}}
{"id": "54", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "music genre", "album", "musical instrument", "award", "event", "organization", "band", "song", "location", "person", "musical artist"], "instance": {"id": "54", "words": ["After", "the", "festival", ",", "the", "Experience", "was", "booked", "for", "five", "concerts", "at", "Bill", "Graham", "'s", "The", "Fillmore", ",", "with", "Big", "Brother", "and", "the", "Holding", "Company", "and", "Jefferson", "Airplane", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-location", "I-location", "O", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, music genre, album, musical instrument, award, event, organization, band, song, location, person, musical artist and O.\nSentence: After the festival , the Experience was booked for five concerts at Bill Graham 's The Fillmore , with Big Brother and the Holding Company and Jefferson Airplane .", "prompt_labels": "After(O) the(O) festival(O) ,(O) the(O) Experience(O) was(O) booked(O) for(O) five(O) concerts(O) at(O) Bill(B-person) Graham(I-person) 's(O) The(B-location) Fillmore(I-location) ,(O) with(O) Big(B-band) Brother(I-band) and(I-band) the(I-band) Holding(I-band) Company(I-band) and(O) Jefferson(B-band) Airplane(I-band) .(O)"}}
{"id": "287", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "song", "music genre", "album", "musical instrument", "location", "band", "country", "musical artist", "organization", "person", "award"], "instance": {"id": "287", "words": ["The", "subgenre", "was", "popularized", "by", "the", "Big", "Four", "of", "Thrash", ":", "Metallica", ",", "Anthrax", ",", "Megadeth", ",", "and", "Slayer", ".", "Walser", "(", "1993", ")", ",", "p.14", "Three", "German", "bands", ",", "Kreator", ",", "Sodom", ",", "and", "Destruction", ",", "played", "a", "central", "role", "in", "bringing", "the", "style", "to", "Europe", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "B-band", "O", "B-band", "O", "O", "B-band", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "B-band", "O", "O", "B-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, song, music genre, album, musical instrument, location, band, country, musical artist, organization, person, award and O.\nSentence: The subgenre was popularized by the Big Four of Thrash : Metallica , Anthrax , Megadeth , and Slayer . Walser ( 1993 ) , p.14 Three German bands , Kreator , Sodom , and Destruction , played a central role in bringing the style to Europe .", "prompt_labels": "The(O) subgenre(O) was(O) popularized(O) by(O) the(O) Big(O) Four(O) of(O) Thrash(O) :(O) Metallica(B-band) ,(O) Anthrax(B-band) ,(O) Megadeth(B-band) ,(O) and(O) Slayer(B-band) .(O) Walser(B-person) ((O) 1993(O) )(O) ,(O) p.14(O) Three(O) German(O) bands(O) ,(O) Kreator(B-band) ,(O) Sodom(B-band) ,(O) and(O) Destruction(B-band) ,(O) played(O) a(O) central(O) role(O) in(O) bringing(O) the(O) style(O) to(O) Europe(B-location) .(O)"}}
{"id": "57", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "person", "music genre", "album", "musical instrument", "organization", "location", "song", "musical artist", "band", "award", "event"], "instance": {"id": "57", "words": ["The", "arena", "also", "hosted", "the", "Ice", "hockey", "at", "the", "2010", "Winter", "Olympics", "events", "at", "the", "2010", "Winter", "Olympics", "."], "labels": ["O", "B-organization", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, music genre, album, musical instrument, organization, location, song, musical artist, band, award, event and O.\nSentence: The arena also hosted the Ice hockey at the 2010 Winter Olympics events at the 2010 Winter Olympics .", "prompt_labels": "The(O) arena(B-organization) also(O) hosted(O) the(O) Ice(B-event) hockey(I-event) at(I-event) the(I-event) 2010(I-event) Winter(I-event) Olympics(I-event) events(O) at(O) the(O) 2010(B-event) Winter(I-event) Olympics(I-event) .(O)"}}
{"id": "95", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "organization", "musical artist", "location", "country", "song", "music genre", "album", "award", "band", "person", "event"], "instance": {"id": "95", "words": ["In", "June", "1932", "he", "joined", "the", "British", "Astronomical", "Association", ",", "in", "November", "of", "the", "same", "year", "he", "became", "a", "Fellow", "of", "the", "Royal", "Astronomical", "Society", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, organization, musical artist, location, country, song, music genre, album, award, band, person, event and O.\nSentence: In June 1932 he joined the British Astronomical Association , in November of the same year he became a Fellow of the Royal Astronomical Society .", "prompt_labels": "In(O) June(O) 1932(O) he(O) joined(O) the(O) British(B-organization) Astronomical(I-organization) Association(I-organization) ,(O) in(O) November(O) of(O) the(O) same(O) year(O) he(O) became(O) a(O) Fellow(B-award) of(I-award) the(I-award) Royal(I-award) Astronomical(I-award) Society(I-award) .(O)"}}
{"id": "366", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "song", "award", "person", "organization", "country", "location", "event", "musical artist", "album", "music genre", "musical instrument"], "instance": {"id": "366", "words": ["blues", "-", ",", "Rock", "music", "-", "and", "Pop", "music", "-musicians", "such", "as"], "labels": ["B-music genre", "O", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, song, award, person, organization, country, location, event, musical artist, album, music genre, musical instrument and O.\nSentence: blues - , Rock music - and Pop music -musicians such as", "prompt_labels": "blues(B-music genre) -(O) ,(O) Rock(B-music genre) music(I-music genre) -(O) and(O) Pop(B-music genre) music(I-music genre) -musicians(O) such(O) as(O)"}}
{"id": "204", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "person", "song", "award", "country", "event", "musical artist", "organization", "location", "musical instrument", "music genre", "album"], "instance": {"id": "204", "words": ["The", "music", "of", "Honduras", "varies", "from", "Punta", "and", "Paranda", "(", "the", "local", "genre", "of", "the", "Garifunas", ")", "to", "Caribbean", "music", "such", "as", "Salsa", "music", ",", "Merengue", "music", ",", "reggae", "and", "reggaeton", "(", "all", "widely", "heard", ",", "especially", "in", "the", "north", ")", "."], "labels": ["O", "O", "O", "B-country", "O", "O", "B-music genre", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, person, song, award, country, event, musical artist, organization, location, musical instrument, music genre, album and O.\nSentence: The music of Honduras varies from Punta and Paranda ( the local genre of the Garifunas ) to Caribbean music such as Salsa music , Merengue music , reggae and reggaeton ( all widely heard , especially in the north ) .", "prompt_labels": "The(O) music(O) of(O) Honduras(B-country) varies(O) from(O) Punta(B-music genre) and(O) Paranda(B-music genre) ((O) the(O) local(O) genre(O) of(O) the(O) Garifunas(O) )(O) to(O) Caribbean(B-music genre) music(I-music genre) such(O) as(O) Salsa(B-music genre) music(I-music genre) ,(O) Merengue(B-music genre) music(I-music genre) ,(O) reggae(B-music genre) and(O) reggaeton(B-music genre) ((O) all(O) widely(O) heard(O) ,(O) especially(O) in(O) the(O) north(O) )(O) .(O)"}}
{"id": "367", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "award", "music genre", "musical instrument", "event", "album", "country", "person", "song", "musical artist", "location", "organization"], "instance": {"id": "367", "words": ["Lindsey", "Buckingham", "issued", "Go", "Insane", "in", "1984", ",", "the", "same", "year", "that", "Christine", "McVie", "made", "an", "eponymous", "album", "(", "yielding", "the", "Top", "10", "hit", "Got", "a", "Hold", "on", "Me", "and", "the", "Top", "40", "hit", "Love", "Will", "Show", "Us", "How", ")", "."], "labels": ["B-musical artist", "I-musical artist", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-album", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, award, music genre, musical instrument, event, album, country, person, song, musical artist, location, organization and O.\nSentence: Lindsey Buckingham issued Go Insane in 1984 , the same year that Christine McVie made an eponymous album ( yielding the Top 10 hit Got a Hold on Me and the Top 40 hit Love Will Show Us How ) .", "prompt_labels": "Lindsey(B-musical artist) Buckingham(I-musical artist) issued(O) Go(B-album) Insane(I-album) in(O) 1984(O) ,(O) the(O) same(O) year(O) that(O) Christine(B-musical artist) McVie(I-musical artist) made(O) an(O) eponymous(B-album) album(O) ((O) yielding(O) the(O) Top(O) 10(O) hit(O) Got(B-song) a(I-song) Hold(I-song) on(I-song) Me(I-song) and(O) the(O) Top(O) 40(O) hit(O) Love(B-song) Will(I-song) Show(I-song) Us(I-song) How(I-song) )(O) .(O)"}}
{"id": "260", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "musical instrument", "person", "song", "musical artist", "band", "album", "event", "award", "organization", "location", "music genre"], "instance": {"id": "260", "words": ["He", "later", "also", "became", "an", "ambassador", "for", "the", "2010", "Shanghai", "Expo", "and", "the", "2010", "Asian", "Games", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, musical instrument, person, song, musical artist, band, album, event, award, organization, location, music genre and O.\nSentence: He later also became an ambassador for the 2010 Shanghai Expo and the 2010 Asian Games .", "prompt_labels": "He(O) later(O) also(O) became(O) an(O) ambassador(O) for(O) the(O) 2010(B-event) Shanghai(I-event) Expo(I-event) and(O) the(O) 2010(B-event) Asian(I-event) Games(I-event) .(O)"}}
{"id": "256", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "country", "person", "musical instrument", "band", "award", "location", "album", "organization", "music genre", "event", "musical artist"], "instance": {"id": "256", "words": ["Western", "music", ",", "influenced", "by", "the", "cowboy", "ballads", ",", "New", "Mexico", "music", ",", "Texas", "country", "music", "and", "Tejano", "music", "rhythms", "of", "the", "Southwestern", "United", "States", "and", "Northern", "Mexico", ",", "reached", "its", "peak", "in", "popularity", "in", "the", "late", "1950s", ",", "most", "notably", "with", "the", "song", "El", "Paso", ",", "first", "recorded", "by", "Marty", "Robbins", "in", "September", "1959", "."], "labels": ["B-music genre", "I-music genre", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, country, person, musical instrument, band, award, location, album, organization, music genre, event, musical artist and O.\nSentence: Western music , influenced by the cowboy ballads , New Mexico music , Texas country music and Tejano music rhythms of the Southwestern United States and Northern Mexico , reached its peak in popularity in the late 1950s , most notably with the song El Paso , first recorded by Marty Robbins in September 1959 .", "prompt_labels": "Western(B-music genre) music(I-music genre) ,(O) influenced(O) by(O) the(O) cowboy(B-music genre) ballads(I-music genre) ,(O) New(B-music genre) Mexico(I-music genre) music(I-music genre) ,(O) Texas(B-music genre) country(I-music genre) music(I-music genre) and(O) Tejano(B-music genre) music(I-music genre) rhythms(O) of(O) the(O) Southwestern(B-location) United(I-location) States(I-location) and(O) Northern(B-location) Mexico(I-location) ,(O) reached(O) its(O) peak(O) in(O) popularity(O) in(O) the(O) late(O) 1950s(O) ,(O) most(O) notably(O) with(O) the(O) song(O) El(B-song) Paso(I-song) ,(O) first(O) recorded(O) by(O) Marty(B-musical artist) Robbins(I-musical artist) in(O) September(O) 1959(O) .(O)"}}
{"id": "198", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "music genre", "country", "event", "song", "award", "organization", "person", "musical artist", "musical instrument", "album", "location"], "instance": {"id": "198", "words": ["Also", ",", "Bon", "Jovi", "had", "a", "hit", "single", ",", "Who", "Says", "You", "Can", "'t", "Go", "Home", ",", "with", "Jennifer", "Nettles", "of", "Sugarland", "."], "labels": ["O", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, music genre, country, event, song, award, organization, person, musical artist, musical instrument, album, location and O.\nSentence: Also , Bon Jovi had a hit single , Who Says You Can 't Go Home , with Jennifer Nettles of Sugarland .", "prompt_labels": "Also(O) ,(O) Bon(B-band) Jovi(I-band) had(O) a(O) hit(O) single(O) ,(O) Who(B-song) Says(I-song) You(I-song) Can(I-song) 't(I-song) Go(I-song) Home(I-song) ,(O) with(O) Jennifer(B-musical artist) Nettles(I-musical artist) of(O) Sugarland(B-band) .(O)"}}
{"id": "153", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "event", "country", "band", "musical instrument", "award", "song", "location", "musical artist", "person", "album", "organization"], "instance": {"id": "153", "words": ["Muggs", "released", "Soul", "Assassins", ":", "Chapter", "1", "featuring", "contributions", "from", "Dr.", "Dre", ",", "KRS-One", ",", "Wyclef", "Jean", "and", "Mobb", "Deep", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, event, country, band, musical instrument, award, song, location, musical artist, person, album, organization and O.\nSentence: Muggs released Soul Assassins : Chapter 1 featuring contributions from Dr. Dre , KRS-One , Wyclef Jean and Mobb Deep .", "prompt_labels": "Muggs(B-musical artist) released(O) Soul(O) Assassins(O) :(O) Chapter(O) 1(O) featuring(O) contributions(O) from(O) Dr.(B-musical artist) Dre(I-musical artist) ,(O) KRS-One(B-musical artist) ,(O) Wyclef(B-musical artist) Jean(I-musical artist) and(O) Mobb(B-band) Deep(I-band) .(O)"}}
{"id": "6", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "album", "musical artist", "award", "country", "location", "band", "song", "event", "music genre", "musical instrument", "person"], "instance": {"id": "6", "words": ["Hildy", "tempts", "him", "into", "taking", "a", "tour", "of", "the", "city", ",", "but", "all", "the", "places", "he", "wants", "to", "go", "(", "New", "York", "Hippodrome", ",", "the", "Forrest", "Theatre", "to", "see", "Tobacco", "Road", ",", "the", "New", "York", "City", "Aquarium", ",", "and", "the", "Woolworth", "Building", ")", "are", "either", "no", "longer", "in", "existence", "or", "no", "longer", "notable", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, album, musical artist, award, country, location, band, song, event, music genre, musical instrument, person and O.\nSentence: Hildy tempts him into taking a tour of the city , but all the places he wants to go ( New York Hippodrome , the Forrest Theatre to see Tobacco Road , the New York City Aquarium , and the Woolworth Building ) are either no longer in existence or no longer notable .", "prompt_labels": "Hildy(B-person) tempts(O) him(O) into(O) taking(O) a(O) tour(O) of(O) the(O) city(O) ,(O) but(O) all(O) the(O) places(O) he(O) wants(O) to(O) go(O) ((O) New(B-location) York(I-location) Hippodrome(I-location) ,(O) the(O) Forrest(B-location) Theatre(I-location) to(O) see(O) Tobacco(B-location) Road(I-location) ,(O) the(O) New(B-location) York(I-location) City(I-location) Aquarium(I-location) ,(O) and(O) the(O) Woolworth(B-location) Building(I-location) )(O) are(O) either(O) no(O) longer(O) in(O) existence(O) or(O) no(O) longer(O) notable(O) .(O)"}}
{"id": "13", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "song", "location", "musical artist", "award", "country", "album", "music genre", "musical instrument", "band", "person", "event"], "instance": {"id": "13", "words": ["Although", "his", "bandmate", "Martin", "Gore", "continues", "to", "be", "the", "main", "songwriter", "for", "Depeche", "Mode", ",", "Gahan", "has", "contributed", "a", "number", "of", "songs", "to", "the", "albums", "Playing", "the", "Angel", "(", "2005", ")", ",", "Sounds", "of", "the", "Universe", "(", "2009", ")", ",", "Delta", "Machine", "(", "2013", ")", "and", "Spirit", "(", "2017", ")", "."], "labels": ["O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, song, location, musical artist, award, country, album, music genre, musical instrument, band, person, event and O.\nSentence: Although his bandmate Martin Gore continues to be the main songwriter for Depeche Mode , Gahan has contributed a number of songs to the albums Playing the Angel ( 2005 ) , Sounds of the Universe ( 2009 ) , Delta Machine ( 2013 ) and Spirit ( 2017 ) .", "prompt_labels": "Although(O) his(O) bandmate(O) Martin(B-musical artist) Gore(I-musical artist) continues(O) to(O) be(O) the(O) main(O) songwriter(O) for(O) Depeche(B-band) Mode(I-band) ,(O) Gahan(B-musical artist) has(O) contributed(O) a(O) number(O) of(O) songs(O) to(O) the(O) albums(O) Playing(B-album) the(I-album) Angel(I-album) ((O) 2005(O) )(O) ,(O) Sounds(B-album) of(I-album) the(I-album) Universe(I-album) ((O) 2009(O) )(O) ,(O) Delta(B-album) Machine(I-album) ((O) 2013(O) )(O) and(O) Spirit(B-album) ((O) 2017(O) )(O) .(O)"}}
{"id": "290", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "song", "event", "country", "location", "organization", "award", "album", "musical artist", "music genre", "person", "musical instrument"], "instance": {"id": "290", "words": ["The", "three", "then", "made", "a", "creative", "decision", "to", "change", "the", "musical", "style", "of", "the", "project", "from", "Hip", "house", "to", "Eurodance", "and", "Pop", "music", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, song, event, country, location, organization, award, album, musical artist, music genre, person, musical instrument and O.\nSentence: The three then made a creative decision to change the musical style of the project from Hip house to Eurodance and Pop music .", "prompt_labels": "The(O) three(O) then(O) made(O) a(O) creative(O) decision(O) to(O) change(O) the(O) musical(O) style(O) of(O) the(O) project(O) from(O) Hip(B-music genre) house(I-music genre) to(O) Eurodance(B-music genre) and(O) Pop(B-music genre) music(I-music genre) .(O)"}}
{"id": "74", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "person", "musical artist", "award", "musical instrument", "location", "organization", "event", "song", "album", "country", "music genre"], "instance": {"id": "74", "words": ["In", "2019", ",", "Fonsi", "performed", "Right", "Where", "I", "'m", "Supposed", "to", "Be", "as", "the", "Official", "Song", "of", "the", "2019", "Special", "Olympics", "World", "Summer", "Games", "in", "Abu", "Dhabi", ",", "United", "Arab", "Emirates", "in", "collaboration", "with", "Ryan", "Tedder", ",", "Avril", "Lavigne", "Hussain", "Al", "Jassmi", ",", "Assala", "Nasri", "and", "Tamer", "Hosny", "."], "labels": ["O", "O", "O", "B-musical artist", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "B-location", "I-location", "O", "B-country", "I-country", "I-country", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, person, musical artist, award, musical instrument, location, organization, event, song, album, country, music genre and O.\nSentence: In 2019 , Fonsi performed Right Where I 'm Supposed to Be as the Official Song of the 2019 Special Olympics World Summer Games in Abu Dhabi , United Arab Emirates in collaboration with Ryan Tedder , Avril Lavigne Hussain Al Jassmi , Assala Nasri and Tamer Hosny .", "prompt_labels": "In(O) 2019(O) ,(O) Fonsi(B-musical artist) performed(O) Right(B-song) Where(I-song) I(I-song) 'm(I-song) Supposed(I-song) to(I-song) Be(I-song) as(O) the(O) Official(O) Song(O) of(O) the(O) 2019(B-event) Special(I-event) Olympics(I-event) World(I-event) Summer(I-event) Games(I-event) in(O) Abu(B-location) Dhabi(I-location) ,(O) United(B-country) Arab(I-country) Emirates(I-country) in(O) collaboration(O) with(O) Ryan(B-musical artist) Tedder(I-musical artist) ,(O) Avril(B-musical artist) Lavigne(I-musical artist) Hussain(B-musical artist) Al(I-musical artist) Jassmi(I-musical artist) ,(O) Assala(B-musical artist) Nasri(I-musical artist) and(O) Tamer(B-musical artist) Hosny(I-musical artist) .(O)"}}
{"id": "78", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "person", "musical instrument", "musical artist", "music genre", "band", "organization", "album", "award", "song", "country", "event"], "instance": {"id": "78", "words": ["In", "2005", "Jamieson", "won", "Best", "Male", "Performer", "in", "the", "second", "annual", "Jack", "Awards", ",", "Jamieson", "showcased", "the", "sounds", "of", "Grinspoon", "to", "millions", "of", "viewers", "in", "March", "2006", ",", "playing", "live", "at", "Melbourne", "Cricket", "Ground", "as", "part", "of", "the", "closing", "ceremony", "of", "the", "2006", "Commonwealth", "Games", "."], "labels": ["O", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "B-musical artist", "O", "O", "O", "O", "B-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, musical instrument, musical artist, music genre, band, organization, album, award, song, country, event and O.\nSentence: In 2005 Jamieson won Best Male Performer in the second annual Jack Awards , Jamieson showcased the sounds of Grinspoon to millions of viewers in March 2006 , playing live at Melbourne Cricket Ground as part of the closing ceremony of the 2006 Commonwealth Games .", "prompt_labels": "In(O) 2005(O) Jamieson(B-musical artist) won(O) Best(O) Male(O) Performer(O) in(O) the(O) second(B-award) annual(I-award) Jack(I-award) Awards(I-award) ,(O) Jamieson(B-musical artist) showcased(O) the(O) sounds(O) of(O) Grinspoon(B-band) to(O) millions(O) of(O) viewers(O) in(O) March(O) 2006(O) ,(O) playing(O) live(O) at(O) Melbourne(B-location) Cricket(I-location) Ground(I-location) as(O) part(O) of(O) the(O) closing(O) ceremony(O) of(O) the(O) 2006(B-event) Commonwealth(I-event) Games(I-event) .(O)"}}
{"id": "301", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "person", "award", "album", "musical artist", "location", "event", "country", "band", "song", "music genre", "musical instrument"], "instance": {"id": "301", "words": ["He", "was", "the", "direct", "inspiration", "for", "three", "Beatles", "'", "songs", ":", "Lucy", "in", "the", "Sky", "with", "Diamonds", "(", "1967", ")", ",", "Hey", "Jude", "(", "1968", ")", ",", "and", "Good", "Night", "(", "1968", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-band", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "B-song", "I-song", "O", "O", "O", "O", "O", "B-song", "I-song", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, award, album, musical artist, location, event, country, band, song, music genre, musical instrument and O.\nSentence: He was the direct inspiration for three Beatles ' songs : Lucy in the Sky with Diamonds ( 1967 ) , Hey Jude ( 1968 ) , and Good Night ( 1968 ) .", "prompt_labels": "He(O) was(O) the(O) direct(O) inspiration(O) for(O) three(O) Beatles(B-band) '(O) songs(O) :(O) Lucy(B-song) in(I-song) the(I-song) Sky(I-song) with(I-song) Diamonds(I-song) ((O) 1967(O) )(O) ,(O) Hey(B-song) Jude(I-song) ((O) 1968(O) )(O) ,(O) and(O) Good(B-song) Night(I-song) ((O) 1968(O) )(O) .(O)"}}
{"id": "319", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "music genre", "musical artist", "band", "musical instrument", "location", "award", "organization", "country", "song", "album", "person"], "instance": {"id": "319", "words": ["Three", "more", "albums", "followed", ":", "1985", "'s", "Little", "Creatures", "(", "which", "featured", "the", "hit", "singles", "And", "She", "Was", "and", "Road", "to", "Nowhere", ")", ",", "During", "that", "time", ",", "the", "group", "was", "falling", "increasingly", "under", "David", "Byrne", "'s", "control", "and", ",", "after", "Naked", ",", "the", "band", "went", "on", "hiatus", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-album", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, music genre, musical artist, band, musical instrument, location, award, organization, country, song, album, person and O.\nSentence: Three more albums followed : 1985 's Little Creatures ( which featured the hit singles And She Was and Road to Nowhere ) , During that time , the group was falling increasingly under David Byrne 's control and , after Naked , the band went on hiatus .", "prompt_labels": "Three(O) more(O) albums(O) followed(O) :(O) 1985(O) 's(O) Little(B-album) Creatures(I-album) ((O) which(O) featured(O) the(O) hit(O) singles(O) And(B-song) She(I-song) Was(I-song) and(O) Road(B-song) to(I-song) Nowhere(I-song) )(O) ,(O) During(O) that(O) time(O) ,(O) the(O) group(O) was(O) falling(O) increasingly(O) under(O) David(B-musical artist) Byrne(I-musical artist) 's(O) control(O) and(O) ,(O) after(O) Naked(B-album) ,(O) the(O) band(O) went(O) on(O) hiatus(O) .(O)"}}
{"id": "109", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "album", "event", "organization", "band", "country", "award", "song", "person", "musical artist", "musical instrument", "music genre"], "instance": {"id": "109", "words": ["First", ",", "they", "played", "a", "series", "of", "concerts", "at", "the", "Glór", "Theatre", "in", "Ennis", ",", "County", "Clare", "(", "on", "23", "&", "amp", ";", "24", "January", "2004", ")", "and", "at", "Vicar", "Street", "in", "Dublin", "(", "on", "30", "&", "amp", ";", "31", "January", "and", "on", "4", "&", "amp", ";", "5", ",", "11", "&", "amp", ";", "12", "February", "2004", ")", ",", "which", "were", "recorded", "and", "from", "which", "selected", "material", "was", "released", "on", "the", "CD", "Live", "2004", "and", "its", "associated", "DVD", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "O", "B-location", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, album, event, organization, band, country, award, song, person, musical artist, musical instrument, music genre and O.\nSentence: First , they played a series of concerts at the Glór Theatre in Ennis , County Clare ( on 23 & amp ; 24 January 2004 ) and at Vicar Street in Dublin ( on 30 & amp ; 31 January and on 4 & amp ; 5 , 11 & amp ; 12 February 2004 ) , which were recorded and from which selected material was released on the CD Live 2004 and its associated DVD .", "prompt_labels": "First(O) ,(O) they(O) played(O) a(O) series(O) of(O) concerts(O) at(O) the(O) Glór(B-location) Theatre(I-location) in(O) Ennis(B-location) ,(O) County(B-location) Clare(B-location) ((O) on(O) 23(O) &(O) amp(O) ;(O) 24(O) January(O) 2004(O) )(O) and(O) at(O) Vicar(B-location) Street(I-location) in(O) Dublin(B-location) ((O) on(O) 30(O) &(O) amp(O) ;(O) 31(O) January(O) and(O) on(O) 4(O) &(O) amp(O) ;(O) 5(O) ,(O) 11(O) &(O) amp(O) ;(O) 12(O) February(O) 2004(O) )(O) ,(O) which(O) were(O) recorded(O) and(O) from(O) which(O) selected(O) material(O) was(O) released(O) on(O) the(O) CD(O) Live(B-album) 2004(I-album) and(O) its(O) associated(O) DVD(O) .(O)"}}
{"id": "130", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "band", "event", "musical artist", "location", "person", "music genre", "organization", "musical instrument", "country", "song", "award"], "instance": {"id": "130", "words": ["Following", "the", "number-one", "success", "of", "MARRS", "'", "Pump", "Up", "The", "Volume", "in", "October", ",", "in", "1987", "to", "1989", ",", "UK", "acts", "such", "as", "The", "Beatmasters", ",", "Krush", ",", "Coldcut", ",", "Yazz", ",", "Bomb", "The", "Bass", ",", "S-Express", ",", "and", "Italy", "'s", "Black", "Box", "opened", "the", "doors", "to", "house", "music", "success", "on", "the", "UK", "charts", "."], "labels": ["O", "O", "O", "O", "O", "B-band", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "O", "B-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "O", "O", "B-country", "O", "B-band", "I-band", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "B-country", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, band, event, musical artist, location, person, music genre, organization, musical instrument, country, song, award and O.\nSentence: Following the number-one success of MARRS ' Pump Up The Volume in October , in 1987 to 1989 , UK acts such as The Beatmasters , Krush , Coldcut , Yazz , Bomb The Bass , S-Express , and Italy 's Black Box opened the doors to house music success on the UK charts .", "prompt_labels": "Following(O) the(O) number-one(O) success(O) of(O) MARRS(B-band) '(O) Pump(B-song) Up(I-song) The(I-song) Volume(I-song) in(O) October(O) ,(O) in(O) 1987(O) to(O) 1989(O) ,(O) UK(B-country) acts(O) such(O) as(O) The(B-band) Beatmasters(I-band) ,(O) Krush(B-band) ,(O) Coldcut(B-band) ,(O) Yazz(B-band) ,(O) Bomb(B-band) The(I-band) Bass(I-band) ,(O) S-Express(B-band) ,(O) and(O) Italy(B-country) 's(O) Black(B-band) Box(I-band) opened(O) the(O) doors(O) to(O) house(B-music genre) music(I-music genre) success(O) on(O) the(O) UK(B-country) charts(O) .(O)"}}
{"id": "325", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "award", "musical instrument", "band", "person", "song", "musical artist", "music genre", "album", "country", "location", "event"], "instance": {"id": "325", "words": ["Stock", "Aitken", "Waterman", "(", "SAW", ")", "expensively-produced", "productions", "for", "Mel", "and", "Kim", ",", "including", "the", "number-one", "hit", "Respectable", ",", "added", "elements", "of", "house", "to", "their", "previous", "Europop", "sound", "."], "labels": ["B-band", "I-band", "I-band", "O", "B-band", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "O", "O", "O", "O", "B-song", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "B-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, musical instrument, band, person, song, musical artist, music genre, album, country, location, event and O.\nSentence: Stock Aitken Waterman ( SAW ) expensively-produced productions for Mel and Kim , including the number-one hit Respectable , added elements of house to their previous Europop sound .", "prompt_labels": "Stock(B-band) Aitken(I-band) Waterman(I-band) ((O) SAW(B-band) )(O) expensively-produced(O) productions(O) for(O) Mel(B-band) and(I-band) Kim(I-band) ,(O) including(O) the(O) number-one(O) hit(O) Respectable(B-song) ,(O) added(O) elements(O) of(O) house(B-music genre) to(O) their(O) previous(O) Europop(B-music genre) sound(O) .(O)"}}
{"id": "41", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "musical instrument", "music genre", "song", "award", "organization", "person", "country", "album", "event", "musical artist", "band"], "instance": {"id": "41", "words": ["In", "2009", ",", "Paltrow", "received", "a", "Grammy", "Award", "nomination", "for", "Grammy", "Award", "for", "Best", "Spoken", "Word", "Album", "for", "Children", "for", "the", "children", "'s", "audiobook", "Brown", "Bear", "and", "Friends", "and", "won", "the", "Primetime", "Emmy", "Award", "for", "Outstanding", "Guest", "Actress", "in", "a", "Comedy", "Series", "for", "her", "guest", "role", "as", "Holly", "Holliday", "on", "the", "Fox", "musical", "comedy-drama", "television", "series", "Glee", "in", "2011", "."], "labels": ["O", "O", "O", "B-musical artist", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical instrument, music genre, song, award, organization, person, country, album, event, musical artist, band and O.\nSentence: In 2009 , Paltrow received a Grammy Award nomination for Grammy Award for Best Spoken Word Album for Children for the children 's audiobook Brown Bear and Friends and won the Primetime Emmy Award for Outstanding Guest Actress in a Comedy Series for her guest role as Holly Holliday on the Fox musical comedy-drama television series Glee in 2011 .", "prompt_labels": "In(O) 2009(O) ,(O) Paltrow(B-musical artist) received(O) a(O) Grammy(B-award) Award(I-award) nomination(O) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Spoken(I-award) Word(I-award) Album(I-award) for(I-award) Children(I-award) for(O) the(O) children(O) 's(O) audiobook(O) Brown(O) Bear(O) and(O) Friends(O) and(O) won(O) the(O) Primetime(B-award) Emmy(I-award) Award(I-award) for(I-award) Outstanding(I-award) Guest(I-award) Actress(I-award) in(I-award) a(I-award) Comedy(I-award) Series(I-award) for(O) her(O) guest(O) role(O) as(O) Holly(B-person) Holliday(I-person) on(O) the(O) Fox(O) musical(O) comedy-drama(O) television(O) series(O) Glee(O) in(O) 2011(O) .(O)"}}
{"id": "182", "dataset": "crossner_music", "split": "dev", "label_list": ["person", "music genre", "organization", "event", "musical instrument", "album", "song", "band", "musical artist", "award", "location", "country"], "instance": {"id": "182", "words": ["In", "the", "early", "1990s", ",", "the", "rise", "of", "melodic", "death", "metal", "was", "recognized", ",", "with", "Swedish", "bands", "such", "as", "Dark", "Tranquillity", ",", "At", "the", "Gates", ",", "and", "In", "Flames", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, music genre, organization, event, musical instrument, album, song, band, musical artist, award, location, country and O.\nSentence: In the early 1990s , the rise of melodic death metal was recognized , with Swedish bands such as Dark Tranquillity , At the Gates , and In Flames .", "prompt_labels": "In(O) the(O) early(O) 1990s(O) ,(O) the(O) rise(O) of(O) melodic(B-music genre) death(I-music genre) metal(I-music genre) was(O) recognized(O) ,(O) with(O) Swedish(O) bands(O) such(O) as(O) Dark(B-band) Tranquillity(I-band) ,(O) At(B-band) the(I-band) Gates(I-band) ,(O) and(O) In(B-band) Flames(I-band) .(O)"}}
{"id": "175", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "musical artist", "album", "band", "musical instrument", "song", "organization", "award", "music genre", "event", "person", "location"], "instance": {"id": "175", "words": ["Dimmu", "Borgir", "'s", "following", "full-length", "albums", "Spiritual", "Black", "Dimensions", "in", "1999", "and", "2001", "'s", "Puritanical", "Euphoric", "Misanthropia", ",", "both", "met", "critical", "acclaim.", "only", "to", "be", "replaced", "with", "Nicholas", "Barker", "of", "Cradle", "of", "Filth", "."], "labels": ["B-band", "I-band", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, musical artist, album, band, musical instrument, song, organization, award, music genre, event, person, location and O.\nSentence: Dimmu Borgir 's following full-length albums Spiritual Black Dimensions in 1999 and 2001 's Puritanical Euphoric Misanthropia , both met critical acclaim. only to be replaced with Nicholas Barker of Cradle of Filth .", "prompt_labels": "Dimmu(B-band) Borgir(I-band) 's(O) following(O) full-length(O) albums(O) Spiritual(B-album) Black(I-album) Dimensions(I-album) in(O) 1999(O) and(O) 2001(O) 's(O) Puritanical(B-album) Euphoric(I-album) Misanthropia(I-album) ,(O) both(O) met(O) critical(O) acclaim.(O) only(O) to(O) be(O) replaced(O) with(O) Nicholas(B-musical artist) Barker(I-musical artist) of(O) Cradle(B-band) of(I-band) Filth(I-band) .(O)"}}
{"id": "341", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "band", "musical instrument", "location", "country", "music genre", "album", "song", "award", "organization", "musical artist", "person"], "instance": {"id": "341", "words": ["A", "favorite", "technique", "of", "contemporary", "jazz", "writers", ",", "ostinati", "are", "often", "used", "in", "Modal", "jazz", "and", "Latin", "jazz", "and", "traditional", "African", "music", "including", "Gnawa", "music", "."], "labels": ["O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, band, musical instrument, location, country, music genre, album, song, award, organization, musical artist, person and O.\nSentence: A favorite technique of contemporary jazz writers , ostinati are often used in Modal jazz and Latin jazz and traditional African music including Gnawa music .", "prompt_labels": "A(O) favorite(O) technique(O) of(O) contemporary(B-music genre) jazz(I-music genre) writers(O) ,(O) ostinati(O) are(O) often(O) used(O) in(O) Modal(B-music genre) jazz(I-music genre) and(O) Latin(B-music genre) jazz(I-music genre) and(O) traditional(O) African(B-music genre) music(I-music genre) including(O) Gnawa(B-music genre) music(I-music genre) .(O)"}}
{"id": "289", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "song", "musical artist", "location", "organization", "person", "album", "band", "country", "award", "music genre", "event"], "instance": {"id": "289", "words": ["The", "success", "of", "this", "performance", "lead", "to", "them", "embarking", "on", "a", "string", "of", "other", "live", "performances", "in", "2009", "and", "2010", ",", "selling", "out", "prestigious", "venues", ",", "such", "as", "the", "Queen", "Elizabeth", "Hall", "in", "London", ",", "Volksbühne", "in", "Berlin", "and", "La", "Cigale", "in", "Paris", "before", "they", "returned", "to", "their", "homeland", "for", "their", "performance", "at", "The", "Norwegian", "Opera", "House", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "B-location", "O", "B-band", "O", "B-location", "I-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, song, musical artist, location, organization, person, album, band, country, award, music genre, event and O.\nSentence: The success of this performance lead to them embarking on a string of other live performances in 2009 and 2010 , selling out prestigious venues , such as the Queen Elizabeth Hall in London , Volksbühne in Berlin and La Cigale in Paris before they returned to their homeland for their performance at The Norwegian Opera House .", "prompt_labels": "The(O) success(O) of(O) this(O) performance(O) lead(O) to(O) them(O) embarking(O) on(O) a(O) string(O) of(O) other(O) live(O) performances(O) in(O) 2009(O) and(O) 2010(O) ,(O) selling(O) out(O) prestigious(O) venues(O) ,(O) such(O) as(O) the(O) Queen(B-location) Elizabeth(I-location) Hall(I-location) in(O) London(B-location) ,(O) Volksbühne(B-location) in(O) Berlin(B-band) and(O) La(B-location) Cigale(I-location) in(O) Paris(B-location) before(O) they(O) returned(O) to(O) their(O) homeland(O) for(O) their(O) performance(O) at(O) The(B-location) Norwegian(I-location) Opera(I-location) House(I-location) .(O)"}}
{"id": "35", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "song", "award", "musical instrument", "band", "music genre", "country", "musical artist", "event", "organization", "location", "person"], "instance": {"id": "35", "words": ["Construction", "commenced", "in", "1975", "and", "the", "venue", "opened", "ahead", "of", "the", "1978", "Commonwealth", "Games", "(", "hence", "its", "name", ")", ",", "replacing", "the", "adjacent", "Clarke", "Stadium", "as", "the", "Eskimos", "home", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, song, award, musical instrument, band, music genre, country, musical artist, event, organization, location, person and O.\nSentence: Construction commenced in 1975 and the venue opened ahead of the 1978 Commonwealth Games ( hence its name ) , replacing the adjacent Clarke Stadium as the Eskimos home .", "prompt_labels": "Construction(O) commenced(O) in(O) 1975(O) and(O) the(O) venue(O) opened(O) ahead(O) of(O) the(O) 1978(B-event) Commonwealth(I-event) Games(I-event) ((O) hence(O) its(O) name(O) )(O) ,(O) replacing(O) the(O) adjacent(O) Clarke(B-location) Stadium(I-location) as(O) the(O) Eskimos(O) home(O) .(O)"}}
{"id": "232", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "musical instrument", "country", "band", "person", "organization", "award", "location", "music genre", "song", "musical artist", "album"], "instance": {"id": "232", "words": ["Examples", "of", "this", "include", "Michael", "Jackson", "from", "The", "Jackson", "5", ",", "Donny", "Osmond", "from", "The", "Osmonds", ",", "Ricky", "Martin", "from", "Menudo", ",", "Justin", "Timberlake", "from", "*", "NSYNC", ",", "and", "Ronan", "Keating", "from", "Boyzone", "."], "labels": ["O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "O", "B-band", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical instrument, country, band, person, organization, award, location, music genre, song, musical artist, album and O.\nSentence: Examples of this include Michael Jackson from The Jackson 5 , Donny Osmond from The Osmonds , Ricky Martin from Menudo , Justin Timberlake from * NSYNC , and Ronan Keating from Boyzone .", "prompt_labels": "Examples(O) of(O) this(O) include(O) Michael(B-musical artist) Jackson(I-musical artist) from(O) The(B-band) Jackson(I-band) 5(I-band) ,(O) Donny(B-musical artist) Osmond(I-musical artist) from(O) The(B-band) Osmonds(I-band) ,(O) Ricky(B-musical artist) Martin(I-musical artist) from(O) Menudo(B-band) ,(O) Justin(B-musical artist) Timberlake(I-musical artist) from(O) *(O) NSYNC(B-band) ,(O) and(O) Ronan(B-musical artist) Keating(I-musical artist) from(O) Boyzone(B-band) .(O)"}}
{"id": "225", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "band", "person", "musical instrument", "organization", "event", "location", "song", "music genre", "album", "musical artist", "award"], "instance": {"id": "225", "words": ["That", "year", ",", "he", "produced", "Death", "Row", "labelmate", "Snoop", "Doggy", "Dogg", "'", "s", "quadruple", "platinum", "debut", "Doggystyle", ",", "and", "mentored", "producers", "such", "as", "his", "step-brother", "Warren", "G", "(", "leading", "to", "the", "multi-platinum", "debut", "Regulate", "...", "G", "Funk", "Era", "in", "1994", ")", "and", "Snoop", "Dogg", "'s", "cousin", "Daz", "Dillinger", "(", "leading", "to", "the", "double-platinum", "debut", "Dogg", "Food", "by", "Tha", "Dogg", "Pound", "in", "1995", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-album", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "B-band", "I-band", "I-band", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, band, person, musical instrument, organization, event, location, song, music genre, album, musical artist, award and O.\nSentence: That year , he produced Death Row labelmate Snoop Doggy Dogg ' s quadruple platinum debut Doggystyle , and mentored producers such as his step-brother Warren G ( leading to the multi-platinum debut Regulate ... G Funk Era in 1994 ) and Snoop Dogg 's cousin Daz Dillinger ( leading to the double-platinum debut Dogg Food by Tha Dogg Pound in 1995 ) .", "prompt_labels": "That(O) year(O) ,(O) he(O) produced(O) Death(O) Row(O) labelmate(O) Snoop(B-musical artist) Doggy(I-musical artist) Dogg(I-musical artist) '(O) s(O) quadruple(O) platinum(O) debut(O) Doggystyle(B-album) ,(O) and(O) mentored(O) producers(O) such(O) as(O) his(O) step-brother(O) Warren(B-musical artist) G(I-musical artist) ((O) leading(O) to(O) the(O) multi-platinum(O) debut(O) Regulate(B-album) ...(I-album) G(I-album) Funk(I-album) Era(I-album) in(O) 1994(O) )(O) and(O) Snoop(B-musical artist) Dogg(I-musical artist) 's(O) cousin(O) Daz(B-musical artist) Dillinger(I-musical artist) ((O) leading(O) to(O) the(O) double-platinum(O) debut(O) Dogg(B-album) Food(I-album) by(O) Tha(B-band) Dogg(I-band) Pound(I-band) in(O) 1995(O) )(O) .(O)"}}
{"id": "205", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "country", "band", "song", "music genre", "musical instrument", "album", "location", "musical artist", "award", "person", "organization"], "instance": {"id": "205", "words": ["The", "elements", "of", "Cali-Style", "Salsa", "were", "strongly", "influenced", "by", "dances", "to", "Caribbean", "rhythms", "which", "preceded", "salsa", ",", "such", "as", "Pachanga", "and", "Boogaloo", "."], "labels": ["O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "B-music genre", "O", "O", "O", "B-music genre", "O", "B-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, band, song, music genre, musical instrument, album, location, musical artist, award, person, organization and O.\nSentence: The elements of Cali-Style Salsa were strongly influenced by dances to Caribbean rhythms which preceded salsa , such as Pachanga and Boogaloo .", "prompt_labels": "The(O) elements(O) of(O) Cali-Style(B-music genre) Salsa(I-music genre) were(O) strongly(O) influenced(O) by(O) dances(O) to(O) Caribbean(B-music genre) rhythms(I-music genre) which(O) preceded(O) salsa(B-music genre) ,(O) such(O) as(O) Pachanga(B-music genre) and(O) Boogaloo(B-music genre) .(O)"}}
{"id": "47", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "location", "song", "organization", "event", "person", "music genre", "musical artist", "musical instrument", "country", "band", "award"], "instance": {"id": "47", "words": ["One", "of", "the", "main", "differences", "between", "American", "and", "European", "pop", "is", "that", "Europop", "is", "generally", "more", "Dance", "music", "and", "Trance", "music", "oriented", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "B-music genre", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, location, song, organization, event, person, music genre, musical artist, musical instrument, country, band, award and O.\nSentence: One of the main differences between American and European pop is that Europop is generally more Dance music and Trance music oriented .", "prompt_labels": "One(O) of(O) the(O) main(O) differences(O) between(O) American(O) and(O) European(O) pop(B-music genre) is(O) that(O) Europop(B-music genre) is(O) generally(O) more(O) Dance(B-music genre) music(I-music genre) and(O) Trance(B-music genre) music(I-music genre) oriented(O) .(O)"}}
{"id": "15", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "musical instrument", "award", "person", "album", "organization", "song", "location", "music genre", "country", "band", "musical artist"], "instance": {"id": "15", "words": ["The", "best-selling", "album", "in", "the", "band", "'s", "catalog", ",", "I", "Against", "I", "is", "an", "album", "that", "mixes", "American", "hardcore", "punk", "with", "funk", ",", "Soul", "music", ",", "reggae", "and", "Heavy", "metal", "music", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-album", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "O", "B-music genre", "O", "B-music genre", "I-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical instrument, award, person, album, organization, song, location, music genre, country, band, musical artist and O.\nSentence: The best-selling album in the band 's catalog , I Against I is an album that mixes American hardcore punk with funk , Soul music , reggae and Heavy metal music .", "prompt_labels": "The(O) best-selling(O) album(O) in(O) the(O) band(O) 's(O) catalog(O) ,(O) I(O) Against(B-album) I(O) is(O) an(O) album(O) that(O) mixes(O) American(O) hardcore(B-music genre) punk(I-music genre) with(O) funk(B-music genre) ,(O) Soul(B-music genre) music(I-music genre) ,(O) reggae(B-music genre) and(O) Heavy(B-music genre) metal(I-music genre) music(I-music genre) .(O)"}}
{"id": "359", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "organization", "musical instrument", "location", "album", "country", "award", "music genre", "band", "song", "musical artist", "person"], "instance": {"id": "359", "words": ["In", "2012", ",", "the", "song", "topped", "an", "ITV", "poll", "in", "the", "UK", "to", "find", "The", "Nation", "'s", "Favourite", "Number", "One", "over", "60", "years", "of", "music", ",", "ahead", "of", "Michael", "Jackson", "'", "s", "Billie", "Jean", "(", "No.", "2", ")", ",", "Adele", "'", "s", "Someone", "like", "You", "(", "No.", "3", ")", ",", "Oasis", "'", "Don", "'t", "Look", "Back", "in", "Anger", "(", "No.", "4", ")", "and", "The", "Beatles", "'", "Hey", "Jude", "(", "No.", "5", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-song", "I-song", "O", "O", "O", "O", "O", "B-musical artist", "O", "O", "B-song", "I-song", "I-song", "O", "O", "O", "O", "O", "B-song", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-song", "I-song", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, musical instrument, location, album, country, award, music genre, band, song, musical artist, person and O.\nSentence: In 2012 , the song topped an ITV poll in the UK to find The Nation 's Favourite Number One over 60 years of music , ahead of Michael Jackson ' s Billie Jean ( No. 2 ) , Adele ' s Someone like You ( No. 3 ) , Oasis ' Don 't Look Back in Anger ( No. 4 ) and The Beatles ' Hey Jude ( No. 5 ) .", "prompt_labels": "In(O) 2012(O) ,(O) the(O) song(O) topped(O) an(O) ITV(O) poll(O) in(O) the(O) UK(B-country) to(O) find(O) The(O) Nation(B-award) 's(I-award) Favourite(I-award) Number(I-award) One(I-award) over(O) 60(O) years(O) of(O) music(O) ,(O) ahead(O) of(O) Michael(B-musical artist) Jackson(I-musical artist) '(O) s(O) Billie(B-song) Jean(I-song) ((O) No.(O) 2(O) )(O) ,(O) Adele(B-musical artist) '(O) s(O) Someone(B-song) like(I-song) You(I-song) ((O) No.(O) 3(O) )(O) ,(O) Oasis(B-song) '(O) Don(B-song) 't(I-song) Look(I-song) Back(I-song) in(I-song) Anger(I-song) ((O) No.(O) 4(O) )(O) and(O) The(B-band) Beatles(I-band) '(O) Hey(B-song) Jude(I-song) ((O) No.(O) 5(O) )(O) .(O)"}}
{"id": "308", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "song", "musical artist", "award", "person", "event", "location", "band", "album", "musical instrument", "music genre", "country"], "instance": {"id": "308", "words": ["Singer", "Marcia", "Barrett", "(", "from", "Jamaica", ")", "joined", "the", "group", ",", "who", "brought", "in", "Liz", "Mitchell", ",", "a", "former", "member", "of", "the", "Les", "Humphries", "Singers", "and", "Boney", "M.", "was", "finalised", "."], "labels": ["O", "B-musical artist", "I-musical artist", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, song, musical artist, award, person, event, location, band, album, musical instrument, music genre, country and O.\nSentence: Singer Marcia Barrett ( from Jamaica ) joined the group , who brought in Liz Mitchell , a former member of the Les Humphries Singers and Boney M. was finalised .", "prompt_labels": "Singer(O) Marcia(B-musical artist) Barrett(I-musical artist) ((O) from(O) Jamaica(B-country) )(O) joined(O) the(O) group(O) ,(O) who(O) brought(O) in(O) Liz(B-musical artist) Mitchell(I-musical artist) ,(O) a(O) former(O) member(O) of(O) the(O) Les(B-band) Humphries(I-band) Singers(I-band) and(O) Boney(B-band) M.(I-band) was(O) finalised(O) .(O)"}}
{"id": "318", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "person", "organization", "album", "event", "band", "musical artist", "award", "location", "song", "country", "musical instrument"], "instance": {"id": "318", "words": ["Brooks", "&", "amp", ";", "Dunn", "has", "17", "Country", "Music", "Association", "awards", ",", "26", "Academy", "of", "Country", "Music", "awards", "and", "2", "Grammy", "Award", "s", "."], "labels": ["B-band", "I-band", "I-band", "I-band", "I-band", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, person, organization, album, event, band, musical artist, award, location, song, country, musical instrument and O.\nSentence: Brooks & amp ; Dunn has 17 Country Music Association awards , 26 Academy of Country Music awards and 2 Grammy Award s .", "prompt_labels": "Brooks(B-band) &(I-band) amp(I-band) ;(I-band) Dunn(I-band) has(O) 17(O) Country(B-award) Music(I-award) Association(I-award) awards(I-award) ,(O) 26(O) Academy(B-award) of(I-award) Country(I-award) Music(I-award) awards(I-award) and(O) 2(O) Grammy(B-award) Award(I-award) s(O) .(O)"}}
{"id": "228", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "song", "location", "award", "music genre", "event", "organization", "musical instrument", "band", "musical artist", "person", "country"], "instance": {"id": "228", "words": ["Popular", "contemporary", "performers", "of", "Australian", "country", "music", "include", "John", "Williamson", "(", "who", "wrote", "the", "iconic", "TRUE", "Blue", ")", ",", "Lee", "Kernaghan", "(", "whose", "hits", "include", "Boys", "from", "the", "Bush", "and", "The", "Outback", "Club", ")", ",", "Gina", "Jeffreys", ",", "Forever", "Road", "and", "Sara", "Storer", "."], "labels": ["O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-song", "I-song", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-album", "I-album", "I-album", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, song, location, award, music genre, event, organization, musical instrument, band, musical artist, person, country and O.\nSentence: Popular contemporary performers of Australian country music include John Williamson ( who wrote the iconic TRUE Blue ) , Lee Kernaghan ( whose hits include Boys from the Bush and The Outback Club ) , Gina Jeffreys , Forever Road and Sara Storer .", "prompt_labels": "Popular(O) contemporary(O) performers(O) of(O) Australian(B-music genre) country(I-music genre) music(I-music genre) include(O) John(B-musical artist) Williamson(I-musical artist) ((O) who(O) wrote(O) the(O) iconic(O) TRUE(B-song) Blue(I-song) )(O) ,(O) Lee(B-musical artist) Kernaghan(I-musical artist) ((O) whose(O) hits(O) include(O) Boys(B-song) from(I-song) the(I-song) Bush(I-song) and(O) The(B-album) Outback(I-album) Club(I-album) )(O) ,(O) Gina(B-musical artist) Jeffreys(I-musical artist) ,(O) Forever(B-band) Road(I-band) and(O) Sara(B-musical artist) Storer(I-musical artist) .(O)"}}
{"id": "127", "dataset": "crossner_music", "split": "dev", "label_list": ["musical artist", "band", "song", "country", "event", "organization", "location", "musical instrument", "award", "person", "music genre", "album"], "instance": {"id": "127", "words": ["This", "period", "on", "the", "RCA", "label", "(", "1971-75", ")", "produced", "Muswell", "Hillbillies", ",", "Everybody", "'s", "in", "Show-Biz", ",", "Preservation", "Act", "1", "and", "Preservation", "Act", "2", ",", "Soap", "Opera", "and", "Schoolboys", "in", "Disgrace", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "B-album", "I-album", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "O", "B-album", "I-album", "I-album", "O", "B-album", "I-album", "O", "B-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, band, song, country, event, organization, location, musical instrument, award, person, music genre, album and O.\nSentence: This period on the RCA label ( 1971-75 ) produced Muswell Hillbillies , Everybody 's in Show-Biz , Preservation Act 1 and Preservation Act 2 , Soap Opera and Schoolboys in Disgrace .", "prompt_labels": "This(O) period(O) on(O) the(O) RCA(B-organization) label(I-organization) ((O) 1971-75(O) )(O) produced(O) Muswell(B-album) Hillbillies(I-album) ,(O) Everybody(B-album) 's(I-album) in(I-album) Show-Biz(I-album) ,(O) Preservation(B-album) Act(I-album) 1(I-album) and(O) Preservation(B-album) Act(I-album) 2(I-album) ,(O) Soap(B-album) Opera(I-album) and(O) Schoolboys(B-album) in(I-album) Disgrace(I-album) .(O)"}}
{"id": "356", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "award", "musical artist", "album", "country", "person", "song", "music genre", "organization", "band", "event", "location"], "instance": {"id": "356", "words": ["In", "Ireland", ",", "The", "Clancy", "Brothers", "and", "Tommy", "Makem", "(", "although", "its", "members", "were", "all", "Irish-born", ",", "the", "group", "became", "famous", "while", "based", "in", "New", "York", "'s", "Greenwich", "Village", ")", ",", "The", "Dubliners", ",", "Clannad", ",", "Planxty", ",", "The", "Chieftains", ",", "The", "Pogues", ",", "The", "Corrs", ",", "The", "Irish", "Rovers", ",", "and", "a", "variety", "of", "other", "folk", "bands", "have", "done", "much", "over", "the", "past", "few", "decades", "to", "revitalise", "and", "re-popularise", "Irish", "traditional", "music", "."], "labels": ["O", "B-country", "O", "B-band", "I-band", "I-band", "I-band", "I-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "O", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, award, musical artist, album, country, person, song, music genre, organization, band, event, location and O.\nSentence: In Ireland , The Clancy Brothers and Tommy Makem ( although its members were all Irish-born , the group became famous while based in New York 's Greenwich Village ) , The Dubliners , Clannad , Planxty , The Chieftains , The Pogues , The Corrs , The Irish Rovers , and a variety of other folk bands have done much over the past few decades to revitalise and re-popularise Irish traditional music .", "prompt_labels": "In(O) Ireland(B-country) ,(O) The(B-band) Clancy(I-band) Brothers(I-band) and(I-band) Tommy(I-band) Makem(I-band) ((O) although(O) its(O) members(O) were(O) all(O) Irish-born(O) ,(O) the(O) group(O) became(O) famous(O) while(O) based(O) in(O) New(B-location) York(I-location) 's(I-location) Greenwich(I-location) Village(I-location) )(O) ,(O) The(B-band) Dubliners(I-band) ,(O) Clannad(B-band) ,(O) Planxty(B-band) ,(O) The(B-band) Chieftains(I-band) ,(O) The(B-band) Pogues(I-band) ,(O) The(B-band) Corrs(I-band) ,(O) The(B-band) Irish(I-band) Rovers(I-band) ,(O) and(O) a(O) variety(O) of(O) other(O) folk(O) bands(O) have(O) done(O) much(O) over(O) the(O) past(O) few(O) decades(O) to(O) revitalise(O) and(O) re-popularise(O) Irish(B-music genre) traditional(I-music genre) music(I-music genre) .(O)"}}
{"id": "297", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "location", "album", "organization", "band", "musical instrument", "person", "musical artist", "song", "music genre", "country", "event"], "instance": {"id": "297", "words": ["With", "the", "continued", "success", "of", "Backstreet", "Boys", "and", "NSYNC", ",", "American", "and", "British", "groups", "like", "98", "Degrees", ",", "Westlife", ",", "O-Town", ",", "A1", ",", "Blue", ",", "and", "Busted", "gained", "quick", "popularity", "both", "domestically", "and", "internationally", "."], "labels": ["O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "O", "B-band", "O", "B-band", "O", "B-band", "O", "B-band", "O", "O", "B-band", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, album, organization, band, musical instrument, person, musical artist, song, music genre, country, event and O.\nSentence: With the continued success of Backstreet Boys and NSYNC , American and British groups like 98 Degrees , Westlife , O-Town , A1 , Blue , and Busted gained quick popularity both domestically and internationally .", "prompt_labels": "With(O) the(O) continued(O) success(O) of(O) Backstreet(B-band) Boys(I-band) and(O) NSYNC(B-band) ,(O) American(O) and(O) British(O) groups(O) like(O) 98(B-band) Degrees(I-band) ,(O) Westlife(B-band) ,(O) O-Town(B-band) ,(O) A1(B-band) ,(O) Blue(B-band) ,(O) and(O) Busted(B-band) gained(O) quick(O) popularity(O) both(O) domestically(O) and(O) internationally(O) .(O)"}}
{"id": "186", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "music genre", "event", "album", "country", "location", "person", "musical artist", "organization", "musical instrument", "song", "band"], "instance": {"id": "186", "words": ["Nova", "was", "selected", "as", "the", "official", "voice", "of", "the", "2013", "Central", "American", "Games", ",", "San", "José", "2013", ",", "with", "the", "song", "Arriba", "Arriba", "(", "Get", "Up", ",", "Get", "Up", ")", "and", "was", "also", "invited", "to", "participate", "in", "the", "TEDx", "Joven", "Pura", "Vida", "(", "Youth", "Pure", "Life", ")", "conference", ",", "where", "she", "shared", "her", "story", ",", "her", "music", "and", "encouraged", "young", "people", "to", "follow", "their", "dreams", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "B-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, music genre, event, album, country, location, person, musical artist, organization, musical instrument, song, band and O.\nSentence: Nova was selected as the official voice of the 2013 Central American Games , San José 2013 , with the song Arriba Arriba ( Get Up , Get Up ) and was also invited to participate in the TEDx Joven Pura Vida ( Youth Pure Life ) conference , where she shared her story , her music and encouraged young people to follow their dreams .", "prompt_labels": "Nova(B-person) was(O) selected(O) as(O) the(O) official(O) voice(O) of(O) the(O) 2013(B-event) Central(I-event) American(I-event) Games(I-event) ,(O) San(B-location) José(I-location) 2013(O) ,(O) with(O) the(O) song(O) Arriba(B-song) Arriba(I-song) ((O) Get(B-song) Up(I-song) ,(I-song) Get(I-song) Up(I-song) )(O) and(O) was(O) also(O) invited(O) to(O) participate(O) in(O) the(O) TEDx(B-organization) Joven(O) Pura(O) Vida(O) ((O) Youth(O) Pure(O) Life(O) )(O) conference(O) ,(O) where(O) she(O) shared(O) her(O) story(O) ,(O) her(O) music(O) and(O) encouraged(O) young(O) people(O) to(O) follow(O) their(O) dreams(O) .(O)"}}
{"id": "266", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "musical instrument", "person", "musical artist", "organization", "location", "country", "music genre", "band", "award", "album", "song"], "instance": {"id": "266", "words": ["Country", "influences", "combined", "with", "Punk", "rock", "and", "alternative", "rock", "to", "forge", "the", "cowpunk", "scene", "in", "Southern", "California", "during", "the", "1980s", ",", "which", "included", "bands", "such", "as", "The", "Long", "Ryders", ",", "Lone", "Justice", "and", "The", "Beat", "Farmers", ",", "as", "well", "as", "the", "established", "punk", "group", "X", ",", "whose", "music", "had", "begun", "to", "include", "country", "and", "rockabilly", "influences", "."], "labels": ["B-music genre", "O", "O", "O", "B-music genre", "I-music genre", "O", "B-music genre", "I-music genre", "O", "O", "O", "B-music genre", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-band", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "B-music genre", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical instrument, person, musical artist, organization, location, country, music genre, band, award, album, song and O.\nSentence: Country influences combined with Punk rock and alternative rock to forge the cowpunk scene in Southern California during the 1980s , which included bands such as The Long Ryders , Lone Justice and The Beat Farmers , as well as the established punk group X , whose music had begun to include country and rockabilly influences .", "prompt_labels": "Country(B-music genre) influences(O) combined(O) with(O) Punk(B-music genre) rock(I-music genre) and(O) alternative(B-music genre) rock(I-music genre) to(O) forge(O) the(O) cowpunk(B-music genre) scene(O) in(O) Southern(B-location) California(I-location) during(O) the(O) 1980s(O) ,(O) which(O) included(O) bands(O) such(O) as(O) The(B-band) Long(I-band) Ryders(I-band) ,(O) Lone(B-band) Justice(I-band) and(O) The(B-band) Beat(I-band) Farmers(I-band) ,(O) as(O) well(O) as(O) the(O) established(O) punk(B-music genre) group(O) X(B-band) ,(O) whose(O) music(O) had(O) begun(O) to(O) include(O) country(B-music genre) and(O) rockabilly(B-music genre) influences(O) .(O)"}}
{"id": "68", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "person", "organization", "country", "song", "band", "album", "location", "award", "event", "music genre", "musical artist"], "instance": {"id": "68", "words": ["They", "also", "visited", "South", "America", "for", "the", "second", "time", "(", "the", "first", "time", "being", "in", "1999", ")", ",", "arriving", "at", "Chile", ",", "Argentina", ",", "and", "Brazil", "."], "labels": ["O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, person, organization, country, song, band, album, location, award, event, music genre, musical artist and O.\nSentence: They also visited South America for the second time ( the first time being in 1999 ) , arriving at Chile , Argentina , and Brazil .", "prompt_labels": "They(O) also(O) visited(O) South(B-location) America(I-location) for(O) the(O) second(O) time(O) ((O) the(O) first(O) time(O) being(O) in(O) 1999(O) )(O) ,(O) arriving(O) at(O) Chile(B-country) ,(O) Argentina(B-country) ,(O) and(O) Brazil(B-country) .(O)"}}
{"id": "99", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "music genre", "location", "organization", "band", "album", "award", "person", "event", "song", "musical instrument", "musical artist"], "instance": {"id": "99", "words": ["Phish", ",", "Soul", "Rebels", "Brass", "Band", ",", "Galactic", ",", "and", "Soulive", "are", "all", "examples", "of", "funk", "bands", "that", "play", "funk", "jam", "."], "labels": ["B-band", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "O", "O", "B-band", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "B-music genre", "I-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, music genre, location, organization, band, album, award, person, event, song, musical instrument, musical artist and O.\nSentence: Phish , Soul Rebels Brass Band , Galactic , and Soulive are all examples of funk bands that play funk jam .", "prompt_labels": "Phish(B-band) ,(O) Soul(B-band) Rebels(I-band) Brass(I-band) Band(I-band) ,(O) Galactic(B-band) ,(O) and(O) Soulive(B-band) are(O) all(O) examples(O) of(O) funk(B-music genre) bands(O) that(O) play(O) funk(B-music genre) jam(I-music genre) .(O)"}}
{"id": "46", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "person", "song", "musical instrument", "organization", "country", "music genre", "band", "event", "musical artist", "award", "location"], "instance": {"id": "46", "words": ["The", "group", "gave", "two", "particularly", "noteworthy", "concerts", "in", "New", "York", "City", ":", "once", "in", "2000", "at", "the", "Zee", "Gold", "Bollywood", "Awards", "in", "the", "Nassau", "Coliseum", ",", "and", "again", "in", "2002", "at", "the", "Bollywood", "Music", "Awards", "in", "the", "Hammerstein", "Ballroom", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, person, song, musical instrument, organization, country, music genre, band, event, musical artist, award, location and O.\nSentence: The group gave two particularly noteworthy concerts in New York City : once in 2000 at the Zee Gold Bollywood Awards in the Nassau Coliseum , and again in 2002 at the Bollywood Music Awards in the Hammerstein Ballroom .", "prompt_labels": "The(O) group(O) gave(O) two(O) particularly(O) noteworthy(O) concerts(O) in(O) New(B-location) York(I-location) City(I-location) :(O) once(O) in(O) 2000(O) at(O) the(O) Zee(B-award) Gold(I-award) Bollywood(I-award) Awards(I-award) in(O) the(O) Nassau(B-location) Coliseum(I-location) ,(O) and(O) again(O) in(O) 2002(O) at(O) the(O) Bollywood(B-award) Music(I-award) Awards(I-award) in(O) the(O) Hammerstein(B-location) Ballroom(I-location) .(O)"}}
{"id": "83", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "person", "award", "location", "country", "organization", "musical artist", "song", "band", "album", "event", "musical instrument"], "instance": {"id": "83", "words": ["The", "film", "drew", "many", "of", "the", "biggest", "living", "influencers", "of", "the", "rhythm", "and", "blues", "genre", "together", ",", "such", "as", "Ray", "Charles", ",", "James", "Brown", ",", "Cab", "Calloway", ",", "Aretha", "Franklin", ",", "and", "John", "Lee", "Hooker", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, person, award, location, country, organization, musical artist, song, band, album, event, musical instrument and O.\nSentence: The film drew many of the biggest living influencers of the rhythm and blues genre together , such as Ray Charles , James Brown , Cab Calloway , Aretha Franklin , and John Lee Hooker .", "prompt_labels": "The(O) film(O) drew(O) many(O) of(O) the(O) biggest(O) living(O) influencers(O) of(O) the(O) rhythm(B-music genre) and(I-music genre) blues(I-music genre) genre(O) together(O) ,(O) such(O) as(O) Ray(B-musical artist) Charles(I-musical artist) ,(O) James(B-musical artist) Brown(I-musical artist) ,(O) Cab(B-musical artist) Calloway(I-musical artist) ,(O) Aretha(B-musical artist) Franklin(I-musical artist) ,(O) and(O) John(B-musical artist) Lee(I-musical artist) Hooker(I-musical artist) .(O)"}}
{"id": "340", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "organization", "country", "album", "band", "person", "award", "music genre", "event", "musical artist", "location", "song"], "instance": {"id": "340", "words": ["Rodgers", "and", "Hammerstein", "musicals", "earned", "a", "total", "of", "37", "Tony", "Award", "s", ",", "15", "Academy", "Awards", ",", "two", "Pulitzer", "Prize", "s", ",", "two", "Grammy", "Award", "s", ",", "and", "two", "Emmy", "Award", "s", "."], "labels": ["B-musical artist", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, organization, country, album, band, person, award, music genre, event, musical artist, location, song and O.\nSentence: Rodgers and Hammerstein musicals earned a total of 37 Tony Award s , 15 Academy Awards , two Pulitzer Prize s , two Grammy Award s , and two Emmy Award s .", "prompt_labels": "Rodgers(B-musical artist) and(O) Hammerstein(B-musical artist) musicals(O) earned(O) a(O) total(O) of(O) 37(O) Tony(B-award) Award(I-award) s(O) ,(O) 15(O) Academy(B-award) Awards(I-award) ,(O) two(O) Pulitzer(B-award) Prize(I-award) s(O) ,(O) two(O) Grammy(B-award) Award(I-award) s(O) ,(O) and(O) two(O) Emmy(B-award) Award(I-award) s(O) .(O)"}}
{"id": "103", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "song", "event", "band", "person", "location", "organization", "musical instrument", "country", "album", "award", "musical artist"], "instance": {"id": "103", "words": ["New", "York", "'s", "rock", "scene", "includes", "clubs", "such", "as", "Irving", "Plaza", ",", "while", "the", "city", "'s", "avant-garde", "downtown", "scene", "includes", "The", "Kitchen", ",", "Roulette", ",", "and", "Knitting", "Factory", "."], "labels": ["B-location", "I-location", "O", "B-music genre", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, song, event, band, person, location, organization, musical instrument, country, album, award, musical artist and O.\nSentence: New York 's rock scene includes clubs such as Irving Plaza , while the city 's avant-garde downtown scene includes The Kitchen , Roulette , and Knitting Factory .", "prompt_labels": "New(B-location) York(I-location) 's(O) rock(B-music genre) scene(O) includes(O) clubs(O) such(O) as(O) Irving(B-location) Plaza(I-location) ,(O) while(O) the(O) city(O) 's(O) avant-garde(O) downtown(O) scene(O) includes(O) The(B-location) Kitchen(I-location) ,(O) Roulette(B-location) ,(O) and(O) Knitting(B-location) Factory(I-location) .(O)"}}
{"id": "255", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "musical instrument", "musical artist", "album", "award", "location", "song", "event", "country", "music genre", "person", "band"], "instance": {"id": "255", "words": ["Since", "2001", ",", "the", "dominance", "of", "traditional", "boy", "bands", "on", "pop", "charts", "began", "to", "fade", "in", "the", "western", "hemisphere", ",", "although", "Gil", "Kaufman", "of", "MTV", "has", "described", "new", "boy", "bands", "that", "are", "more", "likely", "to", "resemble", "My", "Chemical", "Romance", ",", "Sum", "41", ",", "and", "Simple", "Plan", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "O", "B-band", "I-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, musical instrument, musical artist, album, award, location, song, event, country, music genre, person, band and O.\nSentence: Since 2001 , the dominance of traditional boy bands on pop charts began to fade in the western hemisphere , although Gil Kaufman of MTV has described new boy bands that are more likely to resemble My Chemical Romance , Sum 41 , and Simple Plan .", "prompt_labels": "Since(O) 2001(O) ,(O) the(O) dominance(O) of(O) traditional(O) boy(O) bands(O) on(O) pop(B-music genre) charts(O) began(O) to(O) fade(O) in(O) the(O) western(O) hemisphere(O) ,(O) although(O) Gil(B-musical artist) Kaufman(I-musical artist) of(O) MTV(O) has(O) described(O) new(O) boy(O) bands(O) that(O) are(O) more(O) likely(O) to(O) resemble(O) My(B-band) Chemical(I-band) Romance(I-band) ,(O) Sum(B-band) 41(I-band) ,(O) and(O) Simple(B-band) Plan(I-band) .(O)"}}
{"id": "163", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "musical instrument", "musical artist", "location", "event", "music genre", "band", "organization", "country", "award", "person", "song"], "instance": {"id": "163", "words": ["Minogue", "has", "sold", "70", "million", "records", "worldwide", "and", "has", "earned", "numerous", "awards", "and", "accolades", ",", "including", "a", "Grammy", "Award", ",", "three", "Brit", "Awards", ",", "17", "ARIA", "Music", "Awards", ",", "two", "MTV", "Europe", "Music", "Award", "and", "two", "MTV", "Video", "Music", "Award", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical instrument, musical artist, location, event, music genre, band, organization, country, award, person, song and O.\nSentence: Minogue has sold 70 million records worldwide and has earned numerous awards and accolades , including a Grammy Award , three Brit Awards , 17 ARIA Music Awards , two MTV Europe Music Award and two MTV Video Music Award .", "prompt_labels": "Minogue(B-musical artist) has(O) sold(O) 70(O) million(O) records(O) worldwide(O) and(O) has(O) earned(O) numerous(O) awards(O) and(O) accolades(O) ,(O) including(O) a(O) Grammy(B-award) Award(I-award) ,(O) three(O) Brit(B-award) Awards(I-award) ,(O) 17(O) ARIA(B-award) Music(I-award) Awards(I-award) ,(O) two(O) MTV(B-award) Europe(I-award) Music(I-award) Award(I-award) and(O) two(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) .(O)"}}
{"id": "7", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "band", "music genre", "organization", "award", "country", "event", "person", "song", "location", "album", "musical artist"], "instance": {"id": "7", "words": ["It", "was", "Recording", "Industry", "Association", "of", "America", "triple", "platinum", "in", "America", ",", "Two", "more", "singles", "from", "the", "album", "-", "Policy", "of", "Truth", "and", "World", "in", "My", "Eyes", "-", "were", "hits", "in", "the", "UK", ",", "with", "the", "former", "also", "charting", "in", "the", "US", "."], "labels": ["O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, band, music genre, organization, award, country, event, person, song, location, album, musical artist and O.\nSentence: It was Recording Industry Association of America triple platinum in America , Two more singles from the album - Policy of Truth and World in My Eyes - were hits in the UK , with the former also charting in the US .", "prompt_labels": "It(O) was(O) Recording(B-organization) Industry(I-organization) Association(I-organization) of(I-organization) America(I-organization) triple(O) platinum(O) in(O) America(B-country) ,(O) Two(O) more(O) singles(O) from(O) the(O) album(O) -(O) Policy(B-song) of(I-song) Truth(I-song) and(O) World(B-song) in(I-song) My(I-song) Eyes(I-song) -(O) were(O) hits(O) in(O) the(O) UK(B-country) ,(O) with(O) the(O) former(O) also(O) charting(O) in(O) the(O) US(B-country) .(O)"}}
{"id": "137", "dataset": "crossner_music", "split": "dev", "label_list": ["music genre", "event", "person", "location", "song", "organization", "musical artist", "country", "award", "musical instrument", "band", "album"], "instance": {"id": "137", "words": ["Weir", "is", "on", "the", "board", "of", "directors", "of", "the", "Rex", "Foundation", ",", "the", "Furthur", "Foundation", ",", "and", "HeadCount", "..", "He", "is", "also", "on", "the", "honorary", "board", "of", "directors", "of", "Little", "Kids", "Rock", ",", "a", "non-profit", "organization", "that", "provides", "free", "musical", "instruments", "and", "instruction", "to", "children", "in", "under-served", "public", "schools", "throughout", "the", "U.S", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, event, person, location, song, organization, musical artist, country, award, musical instrument, band, album and O.\nSentence: Weir is on the board of directors of the Rex Foundation , the Furthur Foundation , and HeadCount .. He is also on the honorary board of directors of Little Kids Rock , a non-profit organization that provides free musical instruments and instruction to children in under-served public schools throughout the U.S .", "prompt_labels": "Weir(B-musical artist) is(O) on(O) the(O) board(O) of(O) directors(O) of(O) the(O) Rex(B-organization) Foundation(I-organization) ,(O) the(O) Furthur(B-organization) Foundation(I-organization) ,(O) and(O) HeadCount(B-organization) ..(O) He(O) is(O) also(O) on(O) the(O) honorary(O) board(O) of(O) directors(O) of(O) Little(B-organization) Kids(I-organization) Rock(I-organization) ,(O) a(O) non-profit(O) organization(O) that(O) provides(O) free(O) musical(O) instruments(O) and(O) instruction(O) to(O) children(O) in(O) under-served(O) public(O) schools(O) throughout(O) the(O) U.S(B-country) .(O)"}}
{"id": "150", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "award", "country", "person", "organization", "band", "song", "location", "musical artist", "event", "album", "music genre"], "instance": {"id": "150", "words": ["These", "included", "depictions", "of", "local", "Civil", "Defence", "during", "World", "War", "II", "including", "St.", "John", "Ambulance", ",", "the", "British", "Red", "Cross", "and", "the", "fire", "services", "along", "with", "air", "raid", "wardens", ",", "police", "officers", ",", "the", "Home", "Guard", "and", "the", "Royal", "Voluntary", "Service", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, award, country, person, organization, band, song, location, musical artist, event, album, music genre and O.\nSentence: These included depictions of local Civil Defence during World War II including St. John Ambulance , the British Red Cross and the fire services along with air raid wardens , police officers , the Home Guard and the Royal Voluntary Service .", "prompt_labels": "These(O) included(O) depictions(O) of(O) local(O) Civil(O) Defence(O) during(O) World(B-event) War(I-event) II(I-event) including(O) St.(B-organization) John(I-organization) Ambulance(I-organization) ,(O) the(O) British(B-organization) Red(I-organization) Cross(I-organization) and(O) the(O) fire(O) services(O) along(O) with(O) air(O) raid(O) wardens(O) ,(O) police(O) officers(O) ,(O) the(O) Home(B-organization) Guard(I-organization) and(O) the(O) Royal(B-organization) Voluntary(I-organization) Service(I-organization) .(O)"}}
{"id": "166", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "country", "song", "award", "event", "organization", "location", "musical instrument", "musical artist", "band", "person", "music genre"], "instance": {"id": "166", "words": ["A", "critical", "and", "commercial", "success", "upon", "release", "and", "nominated", "for", "four", "Academy", "Awards", ",", "including", "for", "Academy", "Award", "for", "Best", "Picture", ",", "Academy", "Award", "for", "Best", "Actor", "(", "for", "De", "Niro", ")", "and", "Academy", "Award", "for", "Best", "Supporting", "Actress", "(", "for", "Foster", ")", ",", "Taxi", "Driver", "won", "the", "Palme", "d", "'Or", "at", "the", "1976", "Cannes", "Film", "Festival", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, country, song, award, event, organization, location, musical instrument, musical artist, band, person, music genre and O.\nSentence: A critical and commercial success upon release and nominated for four Academy Awards , including for Academy Award for Best Picture , Academy Award for Best Actor ( for De Niro ) and Academy Award for Best Supporting Actress ( for Foster ) , Taxi Driver won the Palme d 'Or at the 1976 Cannes Film Festival .", "prompt_labels": "A(O) critical(O) and(O) commercial(O) success(O) upon(O) release(O) and(O) nominated(O) for(O) four(O) Academy(B-award) Awards(I-award) ,(O) including(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) ((O) for(O) De(B-person) Niro(I-person) )(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actress(I-award) ((O) for(O) Foster(B-person) )(O) ,(O) Taxi(O) Driver(O) won(O) the(O) Palme(B-award) d(I-award) 'Or(I-award) at(O) the(O) 1976(B-event) Cannes(I-event) Film(I-event) Festival(I-event) .(O)"}}
{"id": "284", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "person", "award", "event", "musical artist", "location", "music genre", "album", "organization", "song", "country", "band"], "instance": {"id": "284", "words": ["Wills", "favored", "jazz-like", "arrangements", "and", "the", "band", "found", "national", "popularity", "into", "the", "1940s", "with", "such", "hits", "as", "Steel", "Guitar", "Rag", ",", "New", "San", "Antonio", "Rose", ",", "Smoke", "On", "The", "Water", ",", "Stars", "And", "Stripes", "On", "Iwo", "Jima", ",", "and", "New", "Spanish", "Two", "Step", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "I-song", "O", "O", "B-song", "I-song", "I-song", "I-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, person, award, event, musical artist, location, music genre, album, organization, song, country, band and O.\nSentence: Wills favored jazz-like arrangements and the band found national popularity into the 1940s with such hits as Steel Guitar Rag , New San Antonio Rose , Smoke On The Water , Stars And Stripes On Iwo Jima , and New Spanish Two Step .", "prompt_labels": "Wills(B-musical artist) favored(O) jazz-like(O) arrangements(O) and(O) the(O) band(O) found(O) national(O) popularity(O) into(O) the(O) 1940s(O) with(O) such(O) hits(O) as(O) Steel(B-song) Guitar(I-song) Rag(I-song) ,(O) New(B-song) San(I-song) Antonio(I-song) Rose(I-song) ,(O) Smoke(B-song) On(I-song) The(I-song) Water(I-song) ,(O) Stars(B-song) And(I-song) Stripes(I-song) On(I-song) Iwo(I-song) Jima(I-song) ,(O) and(O) New(B-song) Spanish(I-song) Two(I-song) Step(I-song) .(O)"}}
{"id": "36", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "organization", "band", "music genre", "person", "musical artist", "album", "country", "award", "event", "song", "location"], "instance": {"id": "36", "words": ["Rocks", "and", "Honey", "was", "released", "in", "2013", "and", "features", "the", "single", "Believe", "in", "Me", "which", "she", "performed", "representing", "the", "United", "Kingdom", "at", "the", "Eurovision", "Song", "Contest", "2013", "in", "Malmö", ",", "Sweden", "."], "labels": ["B-album", "I-album", "I-album", "O", "O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-location", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, organization, band, music genre, person, musical artist, album, country, award, event, song, location and O.\nSentence: Rocks and Honey was released in 2013 and features the single Believe in Me which she performed representing the United Kingdom at the Eurovision Song Contest 2013 in Malmö , Sweden .", "prompt_labels": "Rocks(B-album) and(I-album) Honey(I-album) was(O) released(O) in(O) 2013(O) and(O) features(O) the(O) single(O) Believe(B-song) in(I-song) Me(I-song) which(O) she(O) performed(O) representing(O) the(O) United(B-country) Kingdom(I-country) at(O) the(O) Eurovision(B-album) Song(I-album) Contest(I-album) 2013(I-album) in(O) Malmö(B-location) ,(O) Sweden(B-country) .(O)"}}
{"id": "76", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "award", "country", "organization", "musical artist", "song", "musical instrument", "album", "music genre", "location", "band", "person"], "instance": {"id": "76", "words": ["It", "comprises", "the", "music", "of", "Bosnia", "and", "Herzegovina", ",", "Bulgaria", ",", "Croatia", ",", "Music", "of", "Greece", ",", "Montenegro", ",", "Serbia", ",", "Romania", ",", "Republic", "of", "Macedonia", ",", "Albania", ",", "some", "of", "the", "historical", "states", "of", "Yugoslavia", "or", "the", "State", "Union", "of", "Serbia", "and", "Montenegro", "and", "geographical", "regions", "such", "as", "Thrace", "."], "labels": ["O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "I-music genre", "I-music genre", "O", "B-country", "O", "B-country", "O", "B-music genre", "I-music genre", "I-music genre", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "I-country", "I-country", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-country", "I-country", "I-country", "I-country", "I-country", "I-country", "O", "O", "O", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, country, organization, musical artist, song, musical instrument, album, music genre, location, band, person and O.\nSentence: It comprises the music of Bosnia and Herzegovina , Bulgaria , Croatia , Music of Greece , Montenegro , Serbia , Romania , Republic of Macedonia , Albania , some of the historical states of Yugoslavia or the State Union of Serbia and Montenegro and geographical regions such as Thrace .", "prompt_labels": "It(O) comprises(O) the(O) music(B-music genre) of(I-music genre) Bosnia(I-music genre) and(I-music genre) Herzegovina(I-music genre) ,(O) Bulgaria(B-country) ,(O) Croatia(B-country) ,(O) Music(B-music genre) of(I-music genre) Greece(I-music genre) ,(O) Montenegro(B-country) ,(O) Serbia(B-country) ,(O) Romania(B-country) ,(O) Republic(B-country) of(I-country) Macedonia(I-country) ,(O) Albania(B-country) ,(O) some(O) of(O) the(O) historical(O) states(O) of(O) Yugoslavia(B-country) or(O) the(O) State(B-country) Union(I-country) of(I-country) Serbia(I-country) and(I-country) Montenegro(I-country) and(O) geographical(O) regions(O) such(O) as(O) Thrace(B-location) .(O)"}}
{"id": "32", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "country", "award", "song", "organization", "location", "musical instrument", "event", "person", "album", "musical artist", "music genre"], "instance": {"id": "32", "words": ["Four", "singles", "-", "Until", "It", "Sleeps", ",", "Hero", "of", "the", "Day", ",", "Mama", "Said", ",", "and", "King", "Nothing", "-", "were", "released", "as", "part", "of", "the", "marketing", "campaign", "for", "the", "album", "."], "labels": ["O", "O", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "O", "O", "B-song", "I-song", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, country, award, song, organization, location, musical instrument, event, person, album, musical artist, music genre and O.\nSentence: Four singles - Until It Sleeps , Hero of the Day , Mama Said , and King Nothing - were released as part of the marketing campaign for the album .", "prompt_labels": "Four(O) singles(O) -(O) Until(B-song) It(I-song) Sleeps(I-song) ,(O) Hero(B-song) of(I-song) the(I-song) Day(I-song) ,(O) Mama(B-song) Said(I-song) ,(O) and(O) King(B-song) Nothing(I-song) -(O) were(O) released(O) as(O) part(O) of(O) the(O) marketing(O) campaign(O) for(O) the(O) album(O) .(O)"}}
{"id": "263", "dataset": "crossner_music", "split": "dev", "label_list": ["musical artist", "organization", "band", "music genre", "country", "song", "musical instrument", "person", "event", "location", "award", "album"], "instance": {"id": "263", "words": ["!", "--as", "per", "MOS", ":", "BIO", ";", "became", "notable", "as", "Welsh", ",", "and", "self", "identifies", "as", "Welsh", ";", "2012", "Guardian", "interview", ";", "I", "suppose", "it", "'s", "because", "we", "are", "both", "Welsh", "--", "He", "won", "the", "Academy", "Award", "for", "Best", "Actor", "in", "1992", "and", "has", "also", "received", "three", "British", "Academy", "Film", "Awards", "s", ",", "two", "Emmy", "Award", "and", "the", "Golden", "Globe", "Cecil", "B.", "DeMille", "Award", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, organization, band, music genre, country, song, musical instrument, person, event, location, award, album and O.\nSentence: ! --as per MOS : BIO ; became notable as Welsh , and self identifies as Welsh ; 2012 Guardian interview ; I suppose it 's because we are both Welsh -- He won the Academy Award for Best Actor in 1992 and has also received three British Academy Film Awards s , two Emmy Award and the Golden Globe Cecil B. DeMille Award .", "prompt_labels": "!(O) --as(O) per(O) MOS(O) :(O) BIO(O) ;(O) became(O) notable(O) as(O) Welsh(O) ,(O) and(O) self(O) identifies(O) as(O) Welsh(O) ;(O) 2012(B-event) Guardian(I-event) interview(I-event) ;(O) I(O) suppose(O) it(O) 's(O) because(O) we(O) are(O) both(O) Welsh(O) --(O) He(O) won(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) in(O) 1992(O) and(O) has(O) also(O) received(O) three(O) British(B-award) Academy(I-award) Film(I-award) Awards(I-award) s(O) ,(O) two(O) Emmy(B-award) Award(I-award) and(O) the(O) Golden(B-award) Globe(I-award) Cecil(I-award) B.(I-award) DeMille(I-award) Award(I-award) .(O)"}}
{"id": "278", "dataset": "crossner_music", "split": "dev", "label_list": ["event", "location", "award", "organization", "musical instrument", "musical artist", "album", "band", "person", "song", "country", "music genre"], "instance": {"id": "278", "words": ["Notable", "examples", "are", "Judas", "Priest", "'", "s", "Unleashed", "in", "the", "East", ",", "Deep", "Purple", "'", "s", "Made", "in", "Japan", ",", "Iron", "Maiden", "'", "s", "Maiden", "Japan", ",", "Michael", "Schenker", "Group", "'", "s", "One", "Night", "at", "Budokan", "and", "Dream", "Theater", "'", "s", "Live", "at", "Budokan", "."], "labels": ["O", "O", "O", "B-band", "I-band", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-band", "I-band", "O", "O", "B-album", "I-album", "I-album", "O", "B-band", "I-band", "O", "O", "B-album", "I-album", "O", "B-band", "I-band", "I-band", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-band", "I-band", "O", "O", "B-album", "I-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, award, organization, musical instrument, musical artist, album, band, person, song, country, music genre and O.\nSentence: Notable examples are Judas Priest ' s Unleashed in the East , Deep Purple ' s Made in Japan , Iron Maiden ' s Maiden Japan , Michael Schenker Group ' s One Night at Budokan and Dream Theater ' s Live at Budokan .", "prompt_labels": "Notable(O) examples(O) are(O) Judas(B-band) Priest(I-band) '(O) s(O) Unleashed(B-album) in(I-album) the(I-album) East(I-album) ,(O) Deep(B-band) Purple(I-band) '(O) s(O) Made(B-album) in(I-album) Japan(I-album) ,(O) Iron(B-band) Maiden(I-band) '(O) s(O) Maiden(B-album) Japan(I-album) ,(O) Michael(B-band) Schenker(I-band) Group(I-band) '(O) s(O) One(B-album) Night(I-album) at(I-album) Budokan(I-album) and(O) Dream(B-band) Theater(I-band) '(O) s(O) Live(B-album) at(I-album) Budokan(I-album) .(O)"}}
{"id": "338", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "country", "organization", "band", "location", "musical artist", "music genre", "musical instrument", "event", "person", "award", "song"], "instance": {"id": "338", "words": ["In", "2014", "he", "received", "the", "Golden", "Globe", "Cecil", "B.", "DeMille", "Award", "for", "Lifetime", "Achievement", "and", "a", "Tony", "Award", "nomination", "for", "Tony", "Award", "for", "Best", "Book", "of", "a", "Musical", "for", "Bullets", "over", "Broadway", "."], "labels": ["O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, country, organization, band, location, musical artist, music genre, musical instrument, event, person, award, song and O.\nSentence: In 2014 he received the Golden Globe Cecil B. DeMille Award for Lifetime Achievement and a Tony Award nomination for Tony Award for Best Book of a Musical for Bullets over Broadway .", "prompt_labels": "In(O) 2014(O) he(O) received(O) the(O) Golden(B-award) Globe(I-award) Cecil(I-award) B.(I-award) DeMille(I-award) Award(I-award) for(I-award) Lifetime(I-award) Achievement(I-award) and(O) a(O) Tony(B-award) Award(I-award) nomination(O) for(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Book(I-award) of(I-award) a(I-award) Musical(I-award) for(O) Bullets(O) over(O) Broadway(O) .(O)"}}
{"id": "65", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "music genre", "band", "person", "musical artist", "song", "event", "country", "location", "award", "organization", "musical instrument"], "instance": {"id": "65", "words": ["The", "Eurasian", "Economic", "Union", ",", "the", "Gulf", "Cooperation", "Council", ",", "CARICOM", "and", "the", "European", "Union", "are", "current", "examples", "of", "single", "markets", ",", "although", "the", "Gulf", "Cooperation", "Council", "'", "s", "single", "market", "has", "been", "described", "as", "malfunctioning", "in", "2014", "."], "labels": ["O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, music genre, band, person, musical artist, song, event, country, location, award, organization, musical instrument and O.\nSentence: The Eurasian Economic Union , the Gulf Cooperation Council , CARICOM and the European Union are current examples of single markets , although the Gulf Cooperation Council ' s single market has been described as malfunctioning in 2014 .", "prompt_labels": "The(O) Eurasian(B-organization) Economic(I-organization) Union(I-organization) ,(O) the(O) Gulf(B-organization) Cooperation(I-organization) Council(I-organization) ,(O) CARICOM(B-organization) and(O) the(O) European(B-organization) Union(I-organization) are(O) current(O) examples(O) of(O) single(O) markets(O) ,(O) although(O) the(O) Gulf(B-organization) Cooperation(I-organization) Council(I-organization) '(O) s(O) single(O) market(O) has(O) been(O) described(O) as(O) malfunctioning(O) in(O) 2014(O) .(O)"}}
{"id": "125", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "country", "music genre", "band", "album", "person", "location", "musical artist", "event", "award", "musical instrument", "song"], "instance": {"id": "125", "words": ["Rodgers", "was", "the", "first", "person", "to", "win", "what", "is", "considered", "the", "top", "American", "entertainment", "awards", "in", "television", ",", "recording", ",", "movies", ",", "and", "Broadway", "-", "an", "Emmy", "Award", ",", "a", "Grammy", "Award", ",", "an", "Academy", "Awards", ",", "and", "a", "Tony", "Award", "-", "now", "known", "collectively", "as", "an", "EGOT", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, music genre, band, album, person, location, musical artist, event, award, musical instrument, song and O.\nSentence: Rodgers was the first person to win what is considered the top American entertainment awards in television , recording , movies , and Broadway - an Emmy Award , a Grammy Award , an Academy Awards , and a Tony Award - now known collectively as an EGOT .", "prompt_labels": "Rodgers(B-musical artist) was(O) the(O) first(O) person(O) to(O) win(O) what(O) is(O) considered(O) the(O) top(O) American(O) entertainment(O) awards(O) in(O) television(O) ,(O) recording(O) ,(O) movies(O) ,(O) and(O) Broadway(B-organization) -(O) an(O) Emmy(B-award) Award(I-award) ,(O) a(O) Grammy(B-award) Award(I-award) ,(O) an(O) Academy(B-award) Awards(I-award) ,(O) and(O) a(O) Tony(B-award) Award(I-award) -(O) now(O) known(O) collectively(O) as(O) an(O) EGOT(O) .(O)"}}
{"id": "71", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "award", "music genre", "person", "musical instrument", "organization", "song", "country", "album", "event", "band", "musical artist"], "instance": {"id": "71", "words": ["The", "Bee", "Gees", "used", "Barry", "Gibb", "'", "s", "falsetto", "to", "garner", "hits", "such", "as", "You", "Should", "Be", "Dancing", ",", "Stayin", "'", "Alive", ",", "Night", "Fever", ",", "More", "Than", "A", "Woman", "and", "Love", "You", "Inside", "Out", "."], "labels": ["O", "B-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "O", "B-song", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "O", "B-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O", "B-song", "I-song", "I-song", "I-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, music genre, person, musical instrument, organization, song, country, album, event, band, musical artist and O.\nSentence: The Bee Gees used Barry Gibb ' s falsetto to garner hits such as You Should Be Dancing , Stayin ' Alive , Night Fever , More Than A Woman and Love You Inside Out .", "prompt_labels": "The(O) Bee(B-band) Gees(I-band) used(O) Barry(B-musical artist) Gibb(I-musical artist) '(O) s(O) falsetto(B-song) to(O) garner(O) hits(O) such(O) as(O) You(B-song) Should(I-song) Be(I-song) Dancing(I-song) ,(O) Stayin(B-song) '(I-song) Alive(I-song) ,(O) Night(B-song) Fever(I-song) ,(O) More(B-song) Than(I-song) A(I-song) Woman(I-song) and(O) Love(B-song) You(I-song) Inside(I-song) Out(I-song) .(O)"}}
{"id": "51", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "song", "band", "organization", "country", "music genre", "album", "musical artist", "award", "location", "event", "person"], "instance": {"id": "51", "words": ["This", "album", "featured", "vocal", "contributions", "by", "Vicotnik", "of", "Ved", "Buens", "Ende", "and", "Dødheimsgard", "and", "Aldrahn", "of", "Dødheimsgard", "and", "Zyklon-B", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-musical artist", "O", "B-band", "I-band", "I-band", "O", "B-band", "O", "B-musical artist", "O", "B-band", "O", "B-band", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, song, band, organization, country, music genre, album, musical artist, award, location, event, person and O.\nSentence: This album featured vocal contributions by Vicotnik of Ved Buens Ende and Dødheimsgard and Aldrahn of Dødheimsgard and Zyklon-B .", "prompt_labels": "This(O) album(O) featured(O) vocal(O) contributions(O) by(O) Vicotnik(B-musical artist) of(O) Ved(B-band) Buens(I-band) Ende(I-band) and(O) Dødheimsgard(B-band) and(O) Aldrahn(B-musical artist) of(O) Dødheimsgard(B-band) and(O) Zyklon-B(B-band) .(O)"}}
{"id": "155", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "musical artist", "person", "song", "music genre", "organization", "location", "album", "musical instrument", "event", "award", "band"], "instance": {"id": "155", "words": ["It", "also", "produced", "the", "Top", "5", "single", "Rage", "Hard", "(", "#", "1", "in", "Germany", ")", ",", "Top", "20", "single", "Warriors", "of", "the", "Wasteland", "and", "Top", "30", "single", "Watching", "the", "Wildlife", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-song", "I-song", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "O", "O", "O", "O", "B-song", "I-song", "I-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, musical artist, person, song, music genre, organization, location, album, musical instrument, event, award, band and O.\nSentence: It also produced the Top 5 single Rage Hard ( # 1 in Germany ) , Top 20 single Warriors of the Wasteland and Top 30 single Watching the Wildlife .", "prompt_labels": "It(O) also(O) produced(O) the(O) Top(O) 5(O) single(O) Rage(B-song) Hard(I-song) ((O) #(O) 1(O) in(O) Germany(B-country) )(O) ,(O) Top(O) 20(O) single(O) Warriors(B-song) of(I-song) the(I-song) Wasteland(I-song) and(O) Top(O) 30(O) single(O) Watching(B-song) the(I-song) Wildlife(I-song) .(O)"}}
{"id": "84", "dataset": "crossner_music", "split": "dev", "label_list": ["album", "musical instrument", "musical artist", "organization", "award", "music genre", "band", "person", "country", "event", "location", "song"], "instance": {"id": "84", "words": ["Other", "acts", "who", "became", "prominent", "in", "the", "alt-country", "genre", "during", "the", "1990s", "and", "2000s", "included", "The", "Bottle", "Rockets", ",", "The", "Handsome", "Family", ",", "Blue", "Mountain", ",", "Robbie", "Fulks", ",", "Blood", "Oranges", ",", "Bright", "Eyes", ",", "Drive-By", "Truckers", ",", "Old", "97", "'s", ",", "Old", "Crow", "Medicine", "Show", ",", "Nickel", "Creek", ",", "Neko", "Case", ",", "and", "Whiskeytown", ",", "whose", "lead", "singer", "Ryan", "Adams", "later", "had", "a", "successful", "solo-career", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "O", "B-band", "I-band", "I-band", "O", "B-band", "I-band", "I-band", "I-band", "O", "B-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "O", "B-band", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical instrument, musical artist, organization, award, music genre, band, person, country, event, location, song and O.\nSentence: Other acts who became prominent in the alt-country genre during the 1990s and 2000s included The Bottle Rockets , The Handsome Family , Blue Mountain , Robbie Fulks , Blood Oranges , Bright Eyes , Drive-By Truckers , Old 97 's , Old Crow Medicine Show , Nickel Creek , Neko Case , and Whiskeytown , whose lead singer Ryan Adams later had a successful solo-career .", "prompt_labels": "Other(O) acts(O) who(O) became(O) prominent(O) in(O) the(O) alt-country(B-music genre) genre(I-music genre) during(O) the(O) 1990s(O) and(O) 2000s(O) included(O) The(B-band) Bottle(I-band) Rockets(I-band) ,(O) The(B-band) Handsome(I-band) Family(I-band) ,(O) Blue(B-band) Mountain(I-band) ,(O) Robbie(B-band) Fulks(I-band) ,(O) Blood(B-band) Oranges(I-band) ,(O) Bright(B-band) Eyes(I-band) ,(O) Drive-By(B-band) Truckers(I-band) ,(O) Old(B-band) 97(I-band) 's(I-band) ,(O) Old(B-band) Crow(I-band) Medicine(I-band) Show(I-band) ,(O) Nickel(B-band) Creek(I-band) ,(O) Neko(B-musical artist) Case(I-musical artist) ,(O) and(O) Whiskeytown(B-band) ,(O) whose(O) lead(O) singer(O) Ryan(B-musical artist) Adams(I-musical artist) later(O) had(O) a(O) successful(O) solo-career(O) .(O)"}}
{"id": "168", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "location", "award", "band", "musical instrument", "organization", "person", "song", "musical artist", "album", "event", "music genre"], "instance": {"id": "168", "words": ["The", "album", "was", "certified", "gold", "by", "both", "British", "Phonographic", "Industry", "and", "the", "Recording", "Industry", "Association", "of", "America", "in", "December", "2007", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, award, band, musical instrument, organization, person, song, musical artist, album, event, music genre and O.\nSentence: The album was certified gold by both British Phonographic Industry and the Recording Industry Association of America in December 2007 .", "prompt_labels": "The(O) album(O) was(O) certified(O) gold(O) by(O) both(O) British(B-organization) Phonographic(I-organization) Industry(I-organization) and(O) the(O) Recording(B-organization) Industry(I-organization) Association(I-organization) of(I-organization) America(I-organization) in(O) December(O) 2007(O) .(O)"}}
{"id": "4", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "album", "location", "country", "musical artist", "person", "event", "music genre", "award", "organization", "musical instrument", "song"], "instance": {"id": "4", "words": ["ABBA", "were", "soon", "recognised", "and", "embraced", "by", "other", "acts", ":", "Evan", "Dando", "of", "the", "Lemonheads", "recorded", "a", "cover", "version", "of", "Knowing", "Me", ",", "Knowing", "You", ";", "Sinéad", "O", "'Connor", "and", "Boyzone", "'s", "Stephen", "Gately", "have", "recorded", "Chiquitita", ";", "Tanita", "Tikaram", ",", "Blancmange", "and", "Steven", "Wilson", "paid", "tribute", "to", "The", "Day", "Before", "You", "Came", "."], "labels": ["B-band", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "O", "B-song", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "I-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, album, location, country, musical artist, person, event, music genre, award, organization, musical instrument, song and O.\nSentence: ABBA were soon recognised and embraced by other acts : Evan Dando of the Lemonheads recorded a cover version of Knowing Me , Knowing You ; Sinéad O 'Connor and Boyzone 's Stephen Gately have recorded Chiquitita ; Tanita Tikaram , Blancmange and Steven Wilson paid tribute to The Day Before You Came .", "prompt_labels": "ABBA(B-band) were(O) soon(O) recognised(O) and(O) embraced(O) by(O) other(O) acts(O) :(O) Evan(B-musical artist) Dando(I-musical artist) of(O) the(B-band) Lemonheads(I-band) recorded(O) a(O) cover(O) version(O) of(O) Knowing(B-song) Me(I-song) ,(I-song) Knowing(I-song) You(I-song) ;(O) Sinéad(B-musical artist) O(I-musical artist) 'Connor(I-musical artist) and(O) Boyzone(B-band) 's(O) Stephen(B-musical artist) Gately(I-musical artist) have(O) recorded(O) Chiquitita(B-song) ;(O) Tanita(B-musical artist) Tikaram(I-musical artist) ,(O) Blancmange(B-band) and(O) Steven(B-musical artist) Wilson(I-musical artist) paid(O) tribute(O) to(O) The(B-song) Day(I-song) Before(I-song) You(I-song) Came(I-song) .(O)"}}
{"id": "374", "dataset": "crossner_music", "split": "dev", "label_list": ["country", "award", "music genre", "organization", "musical artist", "band", "musical instrument", "location", "person", "song", "event", "album"], "instance": {"id": "374", "words": ["The", "Hives", "have", "released", "five", "studio", "albums", ":", "Barely", "Legal", "(", "1997", ")", ",", "Veni", "Vidi", "Vicious", "(", "2000", ")", ",", "Tyrannosaurus", "Hives", "(", "2004", ")", ",", "The", "Black", "and", "White", "Album", "(", "2007", ")", "and", "Lex", "Hives", "(", "2012", ")", "."], "labels": ["B-band", "I-band", "O", "O", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, music genre, organization, musical artist, band, musical instrument, location, person, song, event, album and O.\nSentence: The Hives have released five studio albums : Barely Legal ( 1997 ) , Veni Vidi Vicious ( 2000 ) , Tyrannosaurus Hives ( 2004 ) , The Black and White Album ( 2007 ) and Lex Hives ( 2012 ) .", "prompt_labels": "The(B-band) Hives(I-band) have(O) released(O) five(O) studio(O) albums(O) :(O) Barely(B-album) Legal(I-album) ((O) 1997(O) )(O) ,(O) Veni(B-album) Vidi(I-album) Vicious(I-album) ((O) 2000(O) )(O) ,(O) Tyrannosaurus(B-album) Hives(I-album) ((O) 2004(O) )(O) ,(O) The(B-album) Black(I-album) and(I-album) White(I-album) Album(I-album) ((O) 2007(O) )(O) and(O) Lex(B-album) Hives(I-album) ((O) 2012(O) )(O) .(O)"}}
{"id": "371", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "country", "person", "event", "musical instrument", "music genre", "award", "organization", "location", "album", "musical artist", "song"], "instance": {"id": "371", "words": ["In", "2019", ",", "Netherlands", "placed", "third", "with", "the", "juries", "(", "North", "Macedonia", "first", ",", "Sweden", "second", ")", "and", "second", "with", "the", "televote", "(", "Norway", "first", ")", "."], "labels": ["O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, country, person, event, musical instrument, music genre, award, organization, location, album, musical artist, song and O.\nSentence: In 2019 , Netherlands placed third with the juries ( North Macedonia first , Sweden second ) and second with the televote ( Norway first ) .", "prompt_labels": "In(O) 2019(O) ,(O) Netherlands(B-country) placed(O) third(O) with(O) the(O) juries(O) ((O) North(B-country) Macedonia(I-country) first(O) ,(O) Sweden(B-country) second(O) )(O) and(O) second(O) with(O) the(O) televote(O) ((O) Norway(B-country) first(O) )(O) .(O)"}}
{"id": "238", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "band", "album", "event", "organization", "musical instrument", "person", "song", "country", "musical artist", "music genre", "location"], "instance": {"id": "238", "words": ["This", "was", "followed", "by", "Our", "Little", "Secret", "(", "1997", ")", ",", "Heaven", "Is", "an", "Orgasm", "(", "1998", ")", "and", "Expand", "Your", "Head", "(", "1999", ")", "."], "labels": ["O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "O", "O", "O", "B-album", "I-album", "I-album", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, band, album, event, organization, musical instrument, person, song, country, musical artist, music genre, location and O.\nSentence: This was followed by Our Little Secret ( 1997 ) , Heaven Is an Orgasm ( 1998 ) and Expand Your Head ( 1999 ) .", "prompt_labels": "This(O) was(O) followed(O) by(O) Our(B-album) Little(I-album) Secret(I-album) ((O) 1997(O) )(O) ,(O) Heaven(B-album) Is(I-album) an(I-album) Orgasm(I-album) ((O) 1998(O) )(O) and(O) Expand(B-album) Your(I-album) Head(I-album) ((O) 1999(O) )(O) .(O)"}}
{"id": "113", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "person", "country", "musical artist", "band", "event", "music genre", "location", "award", "album", "musical instrument", "song"], "instance": {"id": "113", "words": ["They", "have", "won", "two", "Grammy", "Award", "s", ",", "six", "American", "Music", "Awards", ",", "two", "Billboard", "Music", "Award", ",", "four", "MTV", "Video", "Music", "Award", "s", ",", "10", "MTV", "Europe", "Music", "Award", "and", "three", "World", "Music", "Awards", "."], "labels": ["O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, country, musical artist, band, event, music genre, location, award, album, musical instrument, song and O.\nSentence: They have won two Grammy Award s , six American Music Awards , two Billboard Music Award , four MTV Video Music Award s , 10 MTV Europe Music Award and three World Music Awards .", "prompt_labels": "They(O) have(O) won(O) two(O) Grammy(B-award) Award(I-award) s(O) ,(O) six(O) American(B-award) Music(I-award) Awards(I-award) ,(O) two(O) Billboard(B-award) Music(I-award) Award(I-award) ,(O) four(O) MTV(B-award) Video(I-award) Music(I-award) Award(I-award) s(O) ,(O) 10(O) MTV(B-award) Europe(I-award) Music(I-award) Award(I-award) and(O) three(O) World(B-award) Music(I-award) Awards(I-award) .(O)"}}
{"id": "316", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "event", "person", "song", "musical artist", "country", "location", "award", "organization", "musical instrument", "album", "music genre"], "instance": {"id": "316", "words": ["It", "was", "nominated", "for", "Academy", "Award", "for", "Best", "Supporting", "Actor", "(", "Walter", "Huston", ")", ",", "Academy", "Award", "for", "Best", "Director", ",", "Academy", "Award", "for", "Best", "Film", "Editing", "for", "George", "Amy", ",", "Academy", "Award", "for", "Best", "Picture", "and", "Best", "Writing", ",", "Original", "Story", "."], "labels": ["O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "O", "B-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, event, person, song, musical artist, country, location, award, organization, musical instrument, album, music genre and O.\nSentence: It was nominated for Academy Award for Best Supporting Actor ( Walter Huston ) , Academy Award for Best Director , Academy Award for Best Film Editing for George Amy , Academy Award for Best Picture and Best Writing , Original Story .", "prompt_labels": "It(O) was(O) nominated(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actor(I-award) ((O) Walter(B-person) Huston(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Film(I-award) Editing(I-award) for(O) George(B-person) Amy(I-person) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) and(O) Best(B-award) Writing(I-award) ,(O) Original(B-award) Story(I-award) .(O)"}}
{"id": "21", "dataset": "crossner_music", "split": "dev", "label_list": ["band", "album", "music genre", "location", "musical artist", "song", "person", "country", "event", "organization", "musical instrument", "award"], "instance": {"id": "21", "words": ["Western", "music", "artists", "such", "as", "Michael", "Martin", "Murphey", ",", "and", "artists", "within", "the", "aforementioned", "styles", "and", "genres", ",", "have", "seen", "continued", "success", "throughout", "their", "respective", "fields", ",", "including", "the", "likes", "of", "The", "Great", "Divide", ",", "Lorenzo", "Antonio", ",", "Sparx", ",", "Pat", "Green", ",", "and", "Jack", "Ingram", "."], "labels": ["B-music genre", "I-music genre", "O", "O", "O", "B-musical artist", "I-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O", "B-musical artist", "I-musical artist", "O", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, album, music genre, location, musical artist, song, person, country, event, organization, musical instrument, award and O.\nSentence: Western music artists such as Michael Martin Murphey , and artists within the aforementioned styles and genres , have seen continued success throughout their respective fields , including the likes of The Great Divide , Lorenzo Antonio , Sparx , Pat Green , and Jack Ingram .", "prompt_labels": "Western(B-music genre) music(I-music genre) artists(O) such(O) as(O) Michael(B-musical artist) Martin(I-musical artist) Murphey(I-musical artist) ,(O) and(O) artists(O) within(O) the(O) aforementioned(O) styles(O) and(O) genres(O) ,(O) have(O) seen(O) continued(O) success(O) throughout(O) their(O) respective(O) fields(O) ,(O) including(O) the(O) likes(O) of(O) The(B-band) Great(I-band) Divide(I-band) ,(O) Lorenzo(B-musical artist) Antonio(I-musical artist) ,(O) Sparx(B-band) ,(O) Pat(B-musical artist) Green(I-musical artist) ,(O) and(O) Jack(B-musical artist) Ingram(I-musical artist) .(O)"}}
{"id": "353", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "album", "person", "country", "music genre", "song", "musical artist", "award", "musical instrument", "band", "event", "organization"], "instance": {"id": "353", "words": ["Sokol", "Omaha", "sent", "Phil", "Cahoy", "and", "James", "Hartung", "as", "members", "of", "the", "1980", "Olympic", "team", ";", "Hartung", "competed", "again", "in", "United", "States", "at", "the", "1984", "Summer", "Olympics", "."], "labels": ["B-organization", "I-organization", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-person", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, album, person, country, music genre, song, musical artist, award, musical instrument, band, event, organization and O.\nSentence: Sokol Omaha sent Phil Cahoy and James Hartung as members of the 1980 Olympic team ; Hartung competed again in United States at the 1984 Summer Olympics .", "prompt_labels": "Sokol(B-organization) Omaha(I-organization) sent(O) Phil(B-person) Cahoy(I-person) and(O) James(B-person) Hartung(I-person) as(O) members(O) of(O) the(O) 1980(B-organization) Olympic(I-organization) team(I-organization) ;(O) Hartung(B-person) competed(O) again(O) in(O) United(B-event) States(I-event) at(I-event) the(I-event) 1984(I-event) Summer(I-event) Olympics(I-event) .(O)"}}
{"id": "337", "dataset": "crossner_music", "split": "dev", "label_list": ["musical instrument", "event", "person", "organization", "music genre", "band", "location", "album", "musical artist", "country", "song", "award"], "instance": {"id": "337", "words": ["Other", "groups", "in", "the", "British", "grindcore", "scene", ",", "such", "as", "Heresy", "and", "Unseen", "Terror", ",", "have", "emphasized", "the", "influence", "of", "American", "hardcore", "punk", ",", "including", "Septic", "Death", ",", "as", "well", "as", "Swedish", "D-beat", "."], "labels": ["O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "B-band", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "B-band", "I-band", "O", "O", "O", "O", "O", "B-music genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, event, person, organization, music genre, band, location, album, musical artist, country, song, award and O.\nSentence: Other groups in the British grindcore scene , such as Heresy and Unseen Terror , have emphasized the influence of American hardcore punk , including Septic Death , as well as Swedish D-beat .", "prompt_labels": "Other(O) groups(O) in(O) the(O) British(O) grindcore(B-music genre) scene(O) ,(O) such(O) as(O) Heresy(B-band) and(O) Unseen(B-band) Terror(I-band) ,(O) have(O) emphasized(O) the(O) influence(O) of(O) American(O) hardcore(B-music genre) punk(I-music genre) ,(O) including(O) Septic(B-band) Death(I-band) ,(O) as(O) well(O) as(O) Swedish(O) D-beat(B-music genre) .(O)"}}
{"id": "3", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "musical instrument", "event", "location", "country", "musical artist", "music genre", "organization", "song", "album", "band", "person"], "instance": {"id": "3", "words": ["He", "has", "also", "won", "three", "Grammy", "Awards", ",", "14", "Academy", "of", "Country", "Music", "awards", ",", "11", "Country", "Music", "Association", "(", "CMA", ")", "awards", ",", "10", "American", "Music", "Awards", ",", "and", "three", "People", "'s", "Choice", "Awards", "."], "labels": ["O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical instrument, event, location, country, musical artist, music genre, organization, song, album, band, person and O.\nSentence: He has also won three Grammy Awards , 14 Academy of Country Music awards , 11 Country Music Association ( CMA ) awards , 10 American Music Awards , and three People 's Choice Awards .", "prompt_labels": "He(O) has(O) also(O) won(O) three(O) Grammy(B-award) Awards(I-award) ,(O) 14(O) Academy(B-award) of(I-award) Country(I-award) Music(I-award) awards(I-award) ,(O) 11(O) Country(B-award) Music(I-award) Association(I-award) ((I-award) CMA(I-award) )(I-award) awards(I-award) ,(O) 10(O) American(B-award) Music(I-award) Awards(I-award) ,(O) and(O) three(O) People(B-award) 's(I-award) Choice(I-award) Awards(I-award) .(O)"}}
{"id": "264", "dataset": "crossner_music", "split": "dev", "label_list": ["musical artist", "album", "person", "country", "musical instrument", "location", "organization", "event", "award", "music genre", "band", "song"], "instance": {"id": "264", "words": ["Brooks", "is", "one", "of", "the", "few", "people", "who", "have", "received", "an", "Academy", "Awards", ",", "an", "Emmy", "Award", ",", "a", "Tony", "Award", ",", "and", "a", "Grammy", "Award", "."], "labels": ["B-musical artist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, album, person, country, musical instrument, location, organization, event, award, music genre, band, song and O.\nSentence: Brooks is one of the few people who have received an Academy Awards , an Emmy Award , a Tony Award , and a Grammy Award .", "prompt_labels": "Brooks(B-musical artist) is(O) one(O) of(O) the(O) few(O) people(O) who(O) have(O) received(O) an(O) Academy(B-award) Awards(I-award) ,(O) an(O) Emmy(B-award) Award(I-award) ,(O) a(O) Tony(B-award) Award(I-award) ,(O) and(O) a(O) Grammy(B-award) Award(I-award) .(O)"}}
{"id": "235", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "country", "organization", "band", "person", "musical artist", "musical instrument", "music genre", "song", "event", "location", "album"], "instance": {"id": "235", "words": ["With", "Parsons", "and", "Woolfson", ",", "the", "Project", "consisted", "of", "the", "group", "Pilot", ",", "with", "Ian", "Bairnson", "(", "guitar", ")", ",", "David", "Paton", "(", "bass", ")", "and", "Stuart", "Tosh", "(", "drums", ")", "."], "labels": ["O", "B-musical artist", "O", "B-musical artist", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, organization, band, person, musical artist, musical instrument, music genre, song, event, location, album and O.\nSentence: With Parsons and Woolfson , the Project consisted of the group Pilot , with Ian Bairnson ( guitar ) , David Paton ( bass ) and Stuart Tosh ( drums ) .", "prompt_labels": "With(O) Parsons(B-musical artist) and(O) Woolfson(B-musical artist) ,(O) the(O) Project(O) consisted(O) of(O) the(O) group(O) Pilot(B-band) ,(O) with(O) Ian(B-musical artist) Bairnson(I-musical artist) ((O) guitar(O) )(O) ,(O) David(B-musical artist) Paton(I-musical artist) ((O) bass(O) )(O) and(O) Stuart(B-musical artist) Tosh(I-musical artist) ((O) drums(O) )(O) .(O)"}}
{"id": "364", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "musical instrument", "person", "award", "album", "music genre", "event", "band", "country", "organization", "location", "musical artist"], "instance": {"id": "364", "words": ["Blue", "Öyster", "Cult", "'s", "longest-lasting", "and", "most", "commercially", "successful", "lineup", "included", "Buck", "Dharma", "(", "lead", "guitar", ",", "vocals", ")", ",", "Eric", "Bloom", "(", "lead", "vocals", ",", "stun", "guitar", ",", "keyboards", ",", "synthesizers", ")", ",", "Allen", "Lanier", "(", "keyboards", ",", "rhythm", "guitar", ",", "backing", "vocals", ")", ",", "Joe", "Bouchard", "(", "bass", ",", "vocals", ")", ",", "and", "Albert", "Bouchard", "(", "drums", ",", "percussion", ",", "vocals", ")", "."], "labels": ["B-band", "I-band", "I-band", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "B-musical instrument", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "B-musical instrument", "I-musical instrument", "O", "B-musical instrument", "O", "B-musical instrument", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical instrument", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical instrument", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "B-musical instrument", "O", "B-musical instrument", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, musical instrument, person, award, album, music genre, event, band, country, organization, location, musical artist and O.\nSentence: Blue Öyster Cult 's longest-lasting and most commercially successful lineup included Buck Dharma ( lead guitar , vocals ) , Eric Bloom ( lead vocals , stun guitar , keyboards , synthesizers ) , Allen Lanier ( keyboards , rhythm guitar , backing vocals ) , Joe Bouchard ( bass , vocals ) , and Albert Bouchard ( drums , percussion , vocals ) .", "prompt_labels": "Blue(B-band) Öyster(I-band) Cult(I-band) 's(O) longest-lasting(O) and(O) most(O) commercially(O) successful(O) lineup(O) included(O) Buck(B-musical artist) Dharma(I-musical artist) ((O) lead(O) guitar(B-musical instrument) ,(O) vocals(O) )(O) ,(O) Eric(B-musical artist) Bloom(I-musical artist) ((O) lead(O) vocals(O) ,(O) stun(B-musical instrument) guitar(I-musical instrument) ,(O) keyboards(B-musical instrument) ,(O) synthesizers(B-musical instrument) )(O) ,(O) Allen(B-musical artist) Lanier(I-musical artist) ((O) keyboards(B-musical instrument) ,(O) rhythm(B-musical artist) guitar(I-musical artist) ,(O) backing(O) vocals(O) )(O) ,(O) Joe(B-musical artist) Bouchard(I-musical artist) ((O) bass(B-musical instrument) ,(O) vocals(O) )(O) ,(O) and(O) Albert(B-musical artist) Bouchard(I-musical artist) ((O) drums(B-musical instrument) ,(O) percussion(B-musical instrument) ,(O) vocals(O) )(O) .(O)"}}
{"id": "40", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "musical artist", "band", "person", "musical instrument", "award", "location", "album", "event", "organization", "music genre", "country"], "instance": {"id": "40", "words": ["Notable", "rockabilly", "revivalists", "and", "psychobilly", "performers", "from", "the", "1990s", "and", "first", "decade", "of", "the", "21st", "century", "include", "Scott", "Owen", "(", "from", "the", "Australian", "band", "The", "Living", "End", ")", ",", "Jimbo", "Wallace", "(", "from", "the", "US", "band", "Reverend", "Horton", "Heat", ")", ",", "Kim", "Nekroman", "(", "Nekromantix", ")", ",", "Patricia", "Day", "(", "HorrorPops", ")", ",", "Geoff", "Kresge", "(", "Tiger", "Army", ",", "ex-", "AFI", ")", "."], "labels": ["O", "B-music genre", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-band", "I-band", "I-band", "O", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "B-country", "O", "B-band", "I-band", "I-band", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "O", "O", "B-musical artist", "I-musical artist", "O", "B-band", "I-band", "O", "O", "B-band", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, musical artist, band, person, musical instrument, award, location, album, event, organization, music genre, country and O.\nSentence: Notable rockabilly revivalists and psychobilly performers from the 1990s and first decade of the 21st century include Scott Owen ( from the Australian band The Living End ) , Jimbo Wallace ( from the US band Reverend Horton Heat ) , Kim Nekroman ( Nekromantix ) , Patricia Day ( HorrorPops ) , Geoff Kresge ( Tiger Army , ex- AFI ) .", "prompt_labels": "Notable(O) rockabilly(B-music genre) revivalists(O) and(O) psychobilly(B-music genre) performers(O) from(O) the(O) 1990s(O) and(O) first(O) decade(O) of(O) the(O) 21st(O) century(O) include(O) Scott(B-musical artist) Owen(I-musical artist) ((O) from(O) the(O) Australian(O) band(O) The(B-band) Living(I-band) End(I-band) )(O) ,(O) Jimbo(B-musical artist) Wallace(I-musical artist) ((O) from(O) the(O) US(B-country) band(O) Reverend(B-band) Horton(I-band) Heat(I-band) )(O) ,(O) Kim(B-musical artist) Nekroman(I-musical artist) ((O) Nekromantix(B-band) )(O) ,(O) Patricia(B-musical artist) Day(I-musical artist) ((O) HorrorPops(B-band) )(O) ,(O) Geoff(B-musical artist) Kresge(I-musical artist) ((O) Tiger(B-band) Army(I-band) ,(O) ex-(O) AFI(B-band) )(O) .(O)"}}
{"id": "329", "dataset": "crossner_music", "split": "dev", "label_list": ["song", "band", "location", "person", "event", "organization", "country", "musical instrument", "album", "musical artist", "music genre", "award"], "instance": {"id": "329", "words": ["Jimmie", "Rodgers", ",", "Moon", "Mullican", ",", "Bob", "Wills", ",", "Bill", "Monroe", "and", "Hank", "Williams", "have", "all", "described", "themselves", "as", "blues", "singers", "and", "their", "music", "has", "a", "blues", "feel", "that", "is", "different", ",", "at", "first", "glance", "at", "least", ",", "from", "the", "later", "country", "pop", "of", "artists", "like", "Eddy", "Arnold", "."], "labels": ["B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "B-musical artist", "I-musical artist", "O", "O", "O", "O", "O", "B-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "O", "O", "O", "B-musical artist", "I-musical artist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, band, location, person, event, organization, country, musical instrument, album, musical artist, music genre, award and O.\nSentence: Jimmie Rodgers , Moon Mullican , Bob Wills , Bill Monroe and Hank Williams have all described themselves as blues singers and their music has a blues feel that is different , at first glance at least , from the later country pop of artists like Eddy Arnold .", "prompt_labels": "Jimmie(B-musical artist) Rodgers(I-musical artist) ,(O) Moon(B-musical artist) Mullican(I-musical artist) ,(O) Bob(B-musical artist) Wills(I-musical artist) ,(O) Bill(B-musical artist) Monroe(I-musical artist) and(O) Hank(B-musical artist) Williams(I-musical artist) have(O) all(O) described(O) themselves(O) as(O) blues(B-music genre) singers(O) and(O) their(O) music(O) has(O) a(O) blues(O) feel(O) that(O) is(O) different(O) ,(O) at(O) first(O) glance(O) at(O) least(O) ,(O) from(O) the(O) later(O) country(B-music genre) pop(I-music genre) of(O) artists(O) like(O) Eddy(B-musical artist) Arnold(I-musical artist) .(O)"}}
{"id": "96", "dataset": "crossner_music", "split": "dev", "label_list": ["location", "song", "person", "organization", "musical instrument", "musical artist", "band", "award", "album", "country", "event", "music genre"], "instance": {"id": "96", "words": ["These", "were", "finished", "by", "early", "1984", ",", "with", "all", "night", "games", "in", "Adelaide", "moving", "from", "the", "suburban", "grounds", "(", "Norwood", "Oval", "and", "Thebarton", "Oval", ")", "to", "league", "headquarters", "for", "the", "next", "16", "years", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, song, person, organization, musical instrument, musical artist, band, award, album, country, event, music genre and O.\nSentence: These were finished by early 1984 , with all night games in Adelaide moving from the suburban grounds ( Norwood Oval and Thebarton Oval ) to league headquarters for the next 16 years .", "prompt_labels": "These(O) were(O) finished(O) by(O) early(O) 1984(O) ,(O) with(O) all(O) night(O) games(O) in(O) Adelaide(B-location) moving(O) from(O) the(O) suburban(O) grounds(O) ((O) Norwood(B-location) Oval(I-location) and(O) Thebarton(B-location) Oval(I-location) )(O) to(O) league(O) headquarters(O) for(O) the(O) next(O) 16(O) years(O) .(O)"}}
{"id": "37", "dataset": "crossner_music", "split": "dev", "label_list": ["organization", "event", "person", "music genre", "album", "award", "song", "country", "location", "musical artist", "musical instrument", "band"], "instance": {"id": "37", "words": ["Several", "albums", "that", "continued", "this", "style", ",", "which", "had", "come", "to", "be", "known", "as", "technical", "thrash", "metal", ",", "were", "released", "in", "1991", ",", "such", "as", "Overkill", "'s", "Horrorscope", ",", "Heathen", "'", "s", "Victims", "of", "Deception", ",", "Dark", "Angel", "'", "s", "Time", "Does", "Not", "Heal", ",", "Sepultura", "'s", "Arise", ",", "and", "Coroner", "'s", "Mental", "Vortex", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-music genre", "I-music genre", "I-music genre", "O", "O", "O", "O", "O", "O", "O", "O", "B-band", "O", "B-album", "O", "B-band", "O", "O", "B-album", "I-album", "I-album", "O", "B-band", "I-band", "O", "O", "B-album", "I-album", "I-album", "I-album", "O", "B-band", "O", "B-album", "O", "O", "B-band", "O", "B-album", "I-album", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, person, music genre, album, award, song, country, location, musical artist, musical instrument, band and O.\nSentence: Several albums that continued this style , which had come to be known as technical thrash metal , were released in 1991 , such as Overkill 's Horrorscope , Heathen ' s Victims of Deception , Dark Angel ' s Time Does Not Heal , Sepultura 's Arise , and Coroner 's Mental Vortex .", "prompt_labels": "Several(O) albums(O) that(O) continued(O) this(O) style(O) ,(O) which(O) had(O) come(O) to(O) be(O) known(O) as(O) technical(B-music genre) thrash(I-music genre) metal(I-music genre) ,(O) were(O) released(O) in(O) 1991(O) ,(O) such(O) as(O) Overkill(B-band) 's(O) Horrorscope(B-album) ,(O) Heathen(B-band) '(O) s(O) Victims(B-album) of(I-album) Deception(I-album) ,(O) Dark(B-band) Angel(I-band) '(O) s(O) Time(B-album) Does(I-album) Not(I-album) Heal(I-album) ,(O) Sepultura(B-band) 's(O) Arise(B-album) ,(O) and(O) Coroner(B-band) 's(O) Mental(B-album) Vortex(I-album) .(O)"}}
{"id": "220", "dataset": "crossner_music", "split": "dev", "label_list": ["award", "country", "musical artist", "album", "location", "music genre", "organization", "band", "song", "musical instrument", "event", "person"], "instance": {"id": "220", "words": ["The", "instrument", "is", "played", "in", "Guinea", ",", "Guinea-Bissau", ",", "Mali", ",", "Senegal", ",", "Burkina", "Faso", "and", "the", "Gambia", "."], "labels": ["O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "I-country", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, musical artist, album, location, music genre, organization, band, song, musical instrument, event, person and O.\nSentence: The instrument is played in Guinea , Guinea-Bissau , Mali , Senegal , Burkina Faso and the Gambia .", "prompt_labels": "The(O) instrument(O) is(O) played(O) in(O) Guinea(B-country) ,(O) Guinea-Bissau(B-country) ,(O) Mali(B-country) ,(O) Senegal(B-country) ,(O) Burkina(B-country) Faso(I-country) and(O) the(B-country) Gambia(I-country) .(O)"}}
{"id": "213", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "country", "location", "political party", "event", "politician", "person", "organization"], "instance": {"id": "213", "words": ["This", "election", "saw", "the", "peak", "of", "the", "Australian", "Democrats", "'", "popularity", "under", "Janine", "Haines", ",", "and", "a", "Greens", "Western", "Australia", "candidate", "won", "a", "seat", "in", "the", "Australian", "Senate", "for", "the", "first", "time", "-", "although", "the", "successful", "candidate", ",", "Jo", "Vallentine", ",", "was", "already", "a", "two-term", "senator", ",", "having", "previously", "won", "a", "seat", "for", "the", "Nuclear", "Disarmament", "Party", "at", "the", "1984", "Australian", "federal", "election", ",", "and", "the", "Vallentine", "Peace", "Group", "at", "the", "1987", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, country, location, political party, event, politician, person, organization and O.\nSentence: This election saw the peak of the Australian Democrats ' popularity under Janine Haines , and a Greens Western Australia candidate won a seat in the Australian Senate for the first time - although the successful candidate , Jo Vallentine , was already a two-term senator , having previously won a seat for the Nuclear Disarmament Party at the 1984 Australian federal election , and the Vallentine Peace Group at the 1987 election .", "prompt_labels": "This(O) election(O) saw(O) the(O) peak(O) of(O) the(O) Australian(B-political party) Democrats(I-political party) '(O) popularity(O) under(O) Janine(B-politician) Haines(I-politician) ,(O) and(O) a(O) Greens(B-political party) Western(I-political party) Australia(I-political party) candidate(O) won(O) a(O) seat(O) in(O) the(O) Australian(B-organization) Senate(I-organization) for(O) the(O) first(O) time(O) -(O) although(O) the(O) successful(O) candidate(O) ,(O) Jo(B-politician) Vallentine(I-politician) ,(O) was(O) already(O) a(O) two-term(O) senator(O) ,(O) having(O) previously(O) won(O) a(O) seat(O) for(O) the(O) Nuclear(B-political party) Disarmament(I-political party) Party(I-political party) at(O) the(O) 1984(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) and(O) the(O) Vallentine(B-political party) Peace(I-political party) Group(I-political party) at(O) the(O) 1987(O) election(O) .(O)"}}
{"id": "470", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "person", "country", "organization", "political party", "location", "event", "politician"], "instance": {"id": "470", "words": ["During", "the", "early", "stages", "of", "World", "War", "II", ",", "the", "United", "Kingdom", "and", "France", "Allies", "of", "World", "War", "II", "made", "a", "series", "of", "proposals", "to", "send", "troops", "to", "assist", "Finland", "against", "the", "Soviet", "Union", "in", "the", "Winter", "War", ",", "which", "started", "on", "30", "November", "1939", "."], "labels": ["O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-country", "I-country", "O", "B-country", "I-country", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, country, organization, political party, location, event, politician and O.\nSentence: During the early stages of World War II , the United Kingdom and France Allies of World War II made a series of proposals to send troops to assist Finland against the Soviet Union in the Winter War , which started on 30 November 1939 .", "prompt_labels": "During(O) the(O) early(O) stages(O) of(O) World(B-event) War(I-event) II(I-event) ,(O) the(O) United(B-country) Kingdom(I-country) and(O) France(B-country) Allies(I-country) of(O) World(B-event) War(I-event) II(I-event) made(O) a(O) series(O) of(O) proposals(O) to(O) send(O) troops(O) to(O) assist(O) Finland(O) against(O) the(O) Soviet(B-country) Union(I-country) in(O) the(O) Winter(B-event) War(I-event) ,(O) which(O) started(O) on(O) 30(O) November(O) 1939(O) .(O)"}}
{"id": "516", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "country", "person", "political party", "location", "organization", "event", "election"], "instance": {"id": "516", "words": ["The", "campaign", "is", "also", "known", "as", "Gaoyou", "Campaign", "(", "高邮战役", ")", "for", "short", ",", "and", "it", "was", "one", "of", "the", "Chinese", "Civil", "War", "in", "the", "immediate", "post", "World", "War", "II", "era", "resulted", "in", "Communist", "Party", "of", "China", "victory", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "B-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, person, political party, location, organization, event, election and O.\nSentence: The campaign is also known as Gaoyou Campaign ( 高邮战役 ) for short , and it was one of the Chinese Civil War in the immediate post World War II era resulted in Communist Party of China victory .", "prompt_labels": "The(O) campaign(O) is(O) also(O) known(O) as(O) Gaoyou(B-event) Campaign(I-event) ((O) 高邮战役(B-event) )(O) for(O) short(O) ,(O) and(O) it(O) was(O) one(O) of(O) the(O) Chinese(B-event) Civil(I-event) War(I-event) in(O) the(O) immediate(O) post(O) World(B-event) War(I-event) II(I-event) era(O) resulted(O) in(O) Communist(B-political party) Party(I-political party) of(I-political party) China(I-political party) victory(O) .(O)"}}
{"id": "457", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "location", "politician", "organization", "event", "political party", "person", "election"], "instance": {"id": "457", "words": ["The", "Suffragette", "Oak", "on", "Kelvinway", "was", "planted", "in", "1918", "to", "celebrate", "women", "'s", "first", "opportunity", "to", "vote", "in", "a", "general", "election", "and", "stands", "as", "a", "memorial", "to", "the", "likes", "of", "Helen", "Crawfurd", ",", "Dorothea", "Chalmers", "Smith", ",", "Jessie", "Stephen", "and", "Frances", "McPhun", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, politician, organization, event, political party, person, election and O.\nSentence: The Suffragette Oak on Kelvinway was planted in 1918 to celebrate women 's first opportunity to vote in a general election and stands as a memorial to the likes of Helen Crawfurd , Dorothea Chalmers Smith , Jessie Stephen and Frances McPhun .", "prompt_labels": "The(O) Suffragette(O) Oak(O) on(O) Kelvinway(O) was(O) planted(O) in(O) 1918(O) to(O) celebrate(O) women(O) 's(O) first(O) opportunity(O) to(O) vote(O) in(O) a(O) general(O) election(O) and(O) stands(O) as(O) a(O) memorial(O) to(O) the(O) likes(O) of(O) Helen(B-person) Crawfurd(I-person) ,(O) Dorothea(B-person) Chalmers(I-person) Smith(I-person) ,(O) Jessie(B-person) Stephen(I-person) and(O) Frances(B-person) McPhun(I-person) .(O)"}}
{"id": "22", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "organization", "political party", "election", "event", "location", "country", "person"], "instance": {"id": "22", "words": ["For", "the", "2016", "Belarusian", "parliamentary", "election", ",", "the", "party", "formed", "an", "alliance", "with", "the", "BPF", "Party", ",", "the", "Belarusian", "Christian", "Democracy", ",", "the", "Social", "Democratic", "Party", "(", "Assembly", ")", ",", "the", "'", "Za", "svabodu", "'", "movement", ",", "the", "Belarusian", "Green", "Party", ",", "the", "Belarusian", "Liberal", "Party", "of", "Freedom", "and", "Progress", ",", "the", "Trade", "Union", "of", "Electric", "Industry", "and", "independent", "candidates", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, political party, election, event, location, country, person and O.\nSentence: For the 2016 Belarusian parliamentary election , the party formed an alliance with the BPF Party , the Belarusian Christian Democracy , the Social Democratic Party ( Assembly ) , the ' Za svabodu ' movement , the Belarusian Green Party , the Belarusian Liberal Party of Freedom and Progress , the Trade Union of Electric Industry and independent candidates .", "prompt_labels": "For(O) the(O) 2016(B-election) Belarusian(I-election) parliamentary(I-election) election(I-election) ,(O) the(O) party(O) formed(O) an(O) alliance(O) with(O) the(O) BPF(B-political party) Party(I-political party) ,(O) the(O) Belarusian(B-political party) Christian(I-political party) Democracy(I-political party) ,(O) the(O) Social(B-political party) Democratic(I-political party) Party(I-political party) ((O) Assembly(B-political party) )(O) ,(O) the(O) '(B-political party) Za(I-political party) svabodu(I-political party) '(I-political party) movement(I-political party) ,(O) the(O) Belarusian(B-political party) Green(I-political party) Party(I-political party) ,(O) the(O) Belarusian(B-political party) Liberal(I-political party) Party(I-political party) of(I-political party) Freedom(I-political party) and(I-political party) Progress(I-political party) ,(O) the(O) Trade(B-political party) Union(I-political party) of(I-political party) Electric(I-political party) Industry(I-political party) and(O) independent(O) candidates(O) .(O)"}}
{"id": "73", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "event", "location", "political party", "country", "politician", "person", "election"], "instance": {"id": "73", "words": ["Liberalism", "is", "now", "represented", "by", "the", "mainly", "Turkish", "minority", "party", "Movement", "for", "Rights", "and", "Freedoms", "(", "Dviženie", "za", "prava", "i", "svobodi", ",", "observer", "Liberal", "International", ",", "member", "Alliance", "of", "Liberals", "and", "Democrats", "for", "Europe", "Party", ")", "and", "the", "National", "Movement", "for", "Stability", "and", "Progress", "(", "Nacionalno", "Dviženie", "Simeon", "Vtori", ",", "member", "Liberal", "International", ",", "Alliance", "of", "Liberals", "and", "Democrats", "for", "Europe", "Party", ")", ",", "both", "taking", "a", "more", "or", "less", "liberal", "position", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-organization", "I-organization", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-organization", "I-organization", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, location, political party, country, politician, person, election and O.\nSentence: Liberalism is now represented by the mainly Turkish minority party Movement for Rights and Freedoms ( Dviženie za prava i svobodi , observer Liberal International , member Alliance of Liberals and Democrats for Europe Party ) and the National Movement for Stability and Progress ( Nacionalno Dviženie Simeon Vtori , member Liberal International , Alliance of Liberals and Democrats for Europe Party ) , both taking a more or less liberal position .", "prompt_labels": "Liberalism(O) is(O) now(O) represented(O) by(O) the(O) mainly(O) Turkish(O) minority(O) party(O) Movement(B-political party) for(I-political party) Rights(I-political party) and(I-political party) Freedoms(I-political party) ((O) Dviženie(B-political party) za(I-political party) prava(I-political party) i(I-political party) svobodi(I-political party) ,(O) observer(O) Liberal(B-organization) International(I-organization) ,(O) member(O) Alliance(B-political party) of(I-political party) Liberals(I-political party) and(I-political party) Democrats(I-political party) for(I-political party) Europe(I-political party) Party(I-political party) )(O) and(O) the(O) National(B-political party) Movement(I-political party) for(I-political party) Stability(I-political party) and(I-political party) Progress(I-political party) ((O) Nacionalno(B-political party) Dviženie(I-political party) Simeon(I-political party) Vtori(I-political party) ,(O) member(O) Liberal(B-organization) International(I-organization) ,(O) Alliance(B-political party) of(I-political party) Liberals(I-political party) and(I-political party) Democrats(I-political party) for(I-political party) Europe(I-political party) Party(I-political party) )(O) ,(O) both(O) taking(O) a(O) more(O) or(O) less(O) liberal(O) position(O) .(O)"}}
{"id": "227", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "election", "person", "organization", "politician", "location", "country", "political party"], "instance": {"id": "227", "words": ["Hanni", "served", "as", "leader", "of", "the", "Reform", "Party", "of", "British", "Columbia", "from", "August", "30", ",", "1997", "to", "June", "1998", ",", "and", "later", "as", "leader", "of", "the", "British", "Columbia", "Party", ",", "and", "the", "British", "Columbia", "Conservative", "Party", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, person, organization, politician, location, country, political party and O.\nSentence: Hanni served as leader of the Reform Party of British Columbia from August 30 , 1997 to June 1998 , and later as leader of the British Columbia Party , and the British Columbia Conservative Party .", "prompt_labels": "Hanni(B-politician) served(O) as(O) leader(O) of(O) the(O) Reform(B-political party) Party(I-political party) of(I-political party) British(I-political party) Columbia(I-political party) from(O) August(O) 30(O) ,(O) 1997(O) to(O) June(O) 1998(O) ,(O) and(O) later(O) as(O) leader(O) of(O) the(O) British(B-political party) Columbia(I-political party) Party(I-political party) ,(O) and(O) the(O) British(B-political party) Columbia(I-political party) Conservative(I-political party) Party(I-political party) .(O)"}}
{"id": "46", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "politician", "organization", "political party", "person", "location", "event", "election"], "instance": {"id": "46", "words": ["Of", "the", "five", "main", "political", "parties", "in", "Northern", "Ireland", ",", "four", "(", "the", "Ulster", "Unionist", "Party", ",", "the", "Democratic", "Unionist", "Party", ",", "the", "Social", "Democratic", "and", "Labour", "Party", "and", "Sinn", "Féin", ")", "all", "have", "relatively", "strong", "support", "bases", "and", "routinely", "poll", "similar", "results", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, politician, organization, political party, person, location, event, election and O.\nSentence: Of the five main political parties in Northern Ireland , four ( the Ulster Unionist Party , the Democratic Unionist Party , the Social Democratic and Labour Party and Sinn Féin ) all have relatively strong support bases and routinely poll similar results .", "prompt_labels": "Of(O) the(O) five(O) main(O) political(O) parties(O) in(O) Northern(B-country) Ireland(I-country) ,(O) four(O) ((O) the(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) ,(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) ,(O) the(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) and(O) Sinn(B-political party) Féin(I-political party) )(O) all(O) have(O) relatively(O) strong(O) support(O) bases(O) and(O) routinely(O) poll(O) similar(O) results(O) .(O)"}}
{"id": "500", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "political party", "politician", "location", "organization", "election", "person", "event"], "instance": {"id": "500", "words": ["Linjiang", "Campaign", "(", "Chinese", ":", "临江战役", ")", ",", "also", "known", "by", "the", "communist", "s", "as", "the", "Campaign", "of", "Going", "South", "of", "the", "River", "for", "Three", "time", "to", "Guard", "Linjiang", "for", "Four", "Times", "(", "San", "Xia", "Jiang", "Nan", ",", "Si", "Bao", "Linjiang", "Zhanyi", ",", "三下江南四保临江战役", ")", ",", "was", "a", "series", "of", "four", "failed", "offensives", "launched", "by", "the", "Kuomintang", "in", "an", "attempt", "to", "eliminate", "the", "communist", "base", "south", "of", "Songhua", "River", "during", "the", "Chinese", "Civil", "War", "."], "labels": ["B-event", "I-event", "O", "O", "O", "B-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "B-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, politician, location, organization, election, person, event and O.\nSentence: Linjiang Campaign ( Chinese : 临江战役 ) , also known by the communist s as the Campaign of Going South of the River for Three time to Guard Linjiang for Four Times ( San Xia Jiang Nan , Si Bao Linjiang Zhanyi , 三下江南四保临江战役 ) , was a series of four failed offensives launched by the Kuomintang in an attempt to eliminate the communist base south of Songhua River during the Chinese Civil War .", "prompt_labels": "Linjiang(B-event) Campaign(I-event) ((O) Chinese(O) :(O) 临江战役(B-event) )(O) ,(O) also(O) known(O) by(O) the(O) communist(O) s(O) as(O) the(O) Campaign(B-event) of(I-event) Going(I-event) South(I-event) of(I-event) the(I-event) River(I-event) for(I-event) Three(I-event) time(I-event) to(I-event) Guard(I-event) Linjiang(I-event) for(I-event) Four(I-event) Times(I-event) ((O) San(B-event) Xia(I-event) Jiang(I-event) Nan(I-event) ,(I-event) Si(I-event) Bao(I-event) Linjiang(I-event) Zhanyi(I-event) ,(O) 三下江南四保临江战役(B-event) )(O) ,(O) was(O) a(O) series(O) of(O) four(O) failed(O) offensives(O) launched(O) by(O) the(O) Kuomintang(B-political party) in(O) an(O) attempt(O) to(O) eliminate(O) the(O) communist(O) base(O) south(O) of(O) Songhua(B-location) River(I-location) during(O) the(O) Chinese(B-event) Civil(I-event) War(I-event) .(O)"}}
{"id": "309", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "country", "event", "election", "organization", "political party", "politician", "person"], "instance": {"id": "309", "words": ["Labour", "won", "two", "more", "general", "elections", "under", "his", "leadership", "in", "2001", "United", "Kingdom", "general", "election", ",", "in", "which", "it", "won", "another", "landslide", "victory", "(", "albeit", "with", "the", "lowest", "turnout", "since", "1918", "United", "Kingdom", "general", "election", ")", ",", "and", "in", "2005", "United", "Kingdom", "general", "election", ",", "with", "a", "greatly", "reduced", "majority", "."], "labels": ["B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, event, election, organization, political party, politician, person and O.\nSentence: Labour won two more general elections under his leadership in 2001 United Kingdom general election , in which it won another landslide victory ( albeit with the lowest turnout since 1918 United Kingdom general election ) , and in 2005 United Kingdom general election , with a greatly reduced majority .", "prompt_labels": "Labour(B-political party) won(O) two(O) more(O) general(O) elections(O) under(O) his(O) leadership(O) in(O) 2001(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) in(O) which(O) it(O) won(O) another(O) landslide(O) victory(O) ((O) albeit(O) with(O) the(O) lowest(O) turnout(O) since(O) 1918(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) )(O) ,(O) and(O) in(O) 2005(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) with(O) a(O) greatly(O) reduced(O) majority(O) .(O)"}}
{"id": "36", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "location", "event", "country", "organization", "person", "politician", "political party"], "instance": {"id": "36", "words": ["It", "was", "set", "up", "by", "former", "left-leaning", "Christian", "Democrats", ",", "(", "former", "Italian", "Liberal", "Party", "and", "former", "Italian", "Republican", "Party", ")", ",", "as", "well", "as", "other", "left-wing", "politicians", "from", "the", "former", "Italian", "Socialist", "Party", "and", "Federation", "of", "the", "Greens", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, event, country, organization, person, politician, political party and O.\nSentence: It was set up by former left-leaning Christian Democrats , ( former Italian Liberal Party and former Italian Republican Party ) , as well as other left-wing politicians from the former Italian Socialist Party and Federation of the Greens .", "prompt_labels": "It(O) was(O) set(O) up(O) by(O) former(O) left-leaning(O) Christian(O) Democrats(O) ,(O) ((O) former(O) Italian(B-political party) Liberal(I-political party) Party(I-political party) and(O) former(O) Italian(B-political party) Republican(I-political party) Party(I-political party) )(O) ,(O) as(O) well(O) as(O) other(O) left-wing(O) politicians(O) from(O) the(O) former(O) Italian(B-political party) Socialist(I-political party) Party(I-political party) and(O) Federation(B-political party) of(I-political party) the(I-political party) Greens(I-political party) .(O)"}}
{"id": "210", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "event", "person", "location", "country", "politician", "organization", "election"], "instance": {"id": "210", "words": ["He", "unsuccessfully", "ran", "as", "a", "Progressive", "Conservative", "Party", "of", "Ontario", "candidate", "in", "the", "1981", "Ontario", "general", "election", "in", "Hamilton", "West", ",", "losing", "to", "provincial", "Ontario", "Liberal", "Party", "leader", "Stuart", "Lyon", "Smith", "."], "labels": ["O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-location", "I-location", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, person, location, country, politician, organization, election and O.\nSentence: He unsuccessfully ran as a Progressive Conservative Party of Ontario candidate in the 1981 Ontario general election in Hamilton West , losing to provincial Ontario Liberal Party leader Stuart Lyon Smith .", "prompt_labels": "He(O) unsuccessfully(O) ran(O) as(O) a(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Ontario(I-political party) candidate(O) in(O) the(O) 1981(B-election) Ontario(I-election) general(I-election) election(I-election) in(O) Hamilton(B-location) West(I-location) ,(O) losing(O) to(O) provincial(O) Ontario(B-political party) Liberal(I-political party) Party(I-political party) leader(O) Stuart(B-politician) Lyon(I-politician) Smith(I-politician) .(O)"}}
{"id": "1", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "location", "country", "organization", "event", "political party", "election", "politician"], "instance": {"id": "1", "words": ["For", "the", "2009", "European", "Parliament", "election", "in", "Italy", "the", "Greens", "formed", "a", "joint", "list", "with", "the", "Movement", "for", "the", "Left", "(", "MpS", ")", "-", "a", "moderate", "split", "from", "the", "PRC", "-", ",", "the", "Socialist", "Party", "(", "PS", ")", "-", "successor", "of", "the", "SDI", "-", ",", "SD", "and", "Unite", "the", "Left", "(", "UlS", ")", ":", "Left", "Ecology", "Freedom", "(", "SL", ")", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "B-political party", "O", "O", "B-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, country, organization, event, political party, election, politician and O.\nSentence: For the 2009 European Parliament election in Italy the Greens formed a joint list with the Movement for the Left ( MpS ) - a moderate split from the PRC - , the Socialist Party ( PS ) - successor of the SDI - , SD and Unite the Left ( UlS ) : Left Ecology Freedom ( SL ) .", "prompt_labels": "For(O) the(O) 2009(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) Italy(I-election) the(O) Greens(O) formed(O) a(O) joint(O) list(O) with(O) the(O) Movement(B-political party) for(I-political party) the(I-political party) Left(I-political party) ((O) MpS(B-political party) )(O) -(O) a(O) moderate(O) split(O) from(O) the(O) PRC(B-political party) -(O) ,(O) the(O) Socialist(B-political party) Party(I-political party) ((O) PS(B-political party) )(O) -(O) successor(O) of(O) the(O) SDI(B-political party) -(O) ,(O) SD(B-political party) and(O) Unite(B-political party) the(I-political party) Left(I-political party) ((O) UlS(B-political party) )(O) :(O) Left(B-political party) Ecology(I-political party) Freedom(I-political party) ((O) SL(B-political party) )(O) .(O)"}}
{"id": "395", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "politician", "person", "organization", "event", "election", "location", "country"], "instance": {"id": "395", "words": ["Despite", "Bob", "Dole", "'", "s", "victory", "over", "Bill", "Clinton", "and", "Ross", "Perot", "in", "the", "state", "1996", "United", "States", "presidential", "election", "in", "Montana", "in", "the", "presidential", "election", ",", "Baucus", "managed", "to", "narrowly", "win", "re-election", "over", "Rehberg", "to", "secure", "a", "fourth", "term", "in", "the", "Senate", "."], "labels": ["O", "B-politician", "I-politician", "O", "O", "O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, person, organization, event, election, location, country and O.\nSentence: Despite Bob Dole ' s victory over Bill Clinton and Ross Perot in the state 1996 United States presidential election in Montana in the presidential election , Baucus managed to narrowly win re-election over Rehberg to secure a fourth term in the Senate .", "prompt_labels": "Despite(O) Bob(B-politician) Dole(I-politician) '(O) s(O) victory(O) over(O) Bill(B-politician) Clinton(I-politician) and(O) Ross(B-politician) Perot(I-politician) in(O) the(O) state(O) 1996(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) in(I-election) Montana(I-election) in(O) the(O) presidential(O) election(O) ,(O) Baucus(B-politician) managed(O) to(O) narrowly(O) win(O) re-election(O) over(O) Rehberg(B-location) to(O) secure(O) a(O) fourth(O) term(O) in(O) the(O) Senate(B-organization) .(O)"}}
{"id": "400", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "person", "organization", "event", "location", "country", "political party", "election"], "instance": {"id": "400", "words": ["The", "tablets", "quote", "the", "Vermont", "Constitution", ",", "Ethan", "Allen", ",", "Calvin", "Coolidge", ",", "George", "Aiken", ",", "Warren", "Austin", ",", "and", "Dorothy", "Canfield", "Fisher", "among", "others", "."], "labels": ["O", "O", "O", "O", "B-location", "O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, organization, event, location, country, political party, election and O.\nSentence: The tablets quote the Vermont Constitution , Ethan Allen , Calvin Coolidge , George Aiken , Warren Austin , and Dorothy Canfield Fisher among others .", "prompt_labels": "The(O) tablets(O) quote(O) the(O) Vermont(B-location) Constitution(O) ,(O) Ethan(B-politician) Allen(I-politician) ,(O) Calvin(B-politician) Coolidge(I-politician) ,(O) George(B-politician) Aiken(I-politician) ,(O) Warren(B-politician) Austin(I-politician) ,(O) and(O) Dorothy(B-person) Canfield(I-person) Fisher(I-person) among(O) others(O) .(O)"}}
{"id": "473", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "country", "election", "person", "politician", "organization", "political party", "event"], "instance": {"id": "473", "words": ["During", "the", "Western", "Desert", "Campaign", "of", "World", "War", "II", ",", "two", "Allies", "of", "World", "War", "II", "officers", "in", "Egypt", "are", "interviewed", "to", "lead", "a", "dangerous", "commando", "mission", "far", "behind", "enemy", "lines", "in", "Benghazi", "."], "labels": ["O", "O", "B-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "O", "O", "B-country", "O", "B-event", "I-event", "I-event", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, election, person, politician, organization, political party, event and O.\nSentence: During the Western Desert Campaign of World War II , two Allies of World War II officers in Egypt are interviewed to lead a dangerous commando mission far behind enemy lines in Benghazi .", "prompt_labels": "During(O) the(O) Western(B-event) Desert(I-event) Campaign(I-event) of(O) World(B-event) War(I-event) II(I-event) ,(O) two(O) Allies(B-country) of(O) World(B-event) War(I-event) II(I-event) officers(O) in(O) Egypt(B-country) are(O) interviewed(O) to(O) lead(O) a(O) dangerous(O) commando(O) mission(O) far(O) behind(O) enemy(O) lines(O) in(O) Benghazi(B-location) .(O)"}}
{"id": "214", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "election", "organization", "person", "political party", "event", "politician", "country"], "instance": {"id": "214", "words": ["The", "demonstration", "was", "organised", "by", "the", "Stop", "the", "War", "Coalition", ",", "the", "Campaign", "for", "Nuclear", "Disarmament", "(", "CND", ")", "and", "the", "Muslim", "Association", "of", "Britain", "(", "MAB", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, election, organization, person, political party, event, politician, country and O.\nSentence: The demonstration was organised by the Stop the War Coalition , the Campaign for Nuclear Disarmament ( CND ) and the Muslim Association of Britain ( MAB ) .", "prompt_labels": "The(O) demonstration(O) was(O) organised(O) by(O) the(O) Stop(B-organization) the(I-organization) War(I-organization) Coalition(I-organization) ,(O) the(O) Campaign(B-organization) for(I-organization) Nuclear(I-organization) Disarmament(I-organization) ((O) CND(B-organization) )(O) and(O) the(O) Muslim(B-organization) Association(I-organization) of(I-organization) Britain(I-organization) ((O) MAB(B-organization) )(O) .(O)"}}
{"id": "118", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "politician", "political party", "organization", "election", "person", "country", "location"], "instance": {"id": "118", "words": ["Vice", "presidents", "Chen", "Cheng", ",", "Yen", "Chia-kan", ",", "and", "Lien", "Chan", "all", "served", "as", "premier", "concurrently", "as", "vice", "president", "during", "part", "of", "their", "terms", ",", "and", "vice", "president", "Annette", "Lu", "has", "at", "times", "been", "mentioned", "as", "a", "possible", "candidate", "for", "premiership", "."], "labels": ["O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, political party, organization, election, person, country, location and O.\nSentence: Vice presidents Chen Cheng , Yen Chia-kan , and Lien Chan all served as premier concurrently as vice president during part of their terms , and vice president Annette Lu has at times been mentioned as a possible candidate for premiership .", "prompt_labels": "Vice(O) presidents(O) Chen(B-politician) Cheng(I-politician) ,(O) Yen(B-politician) Chia-kan(I-politician) ,(O) and(O) Lien(B-politician) Chan(I-politician) all(O) served(O) as(O) premier(O) concurrently(O) as(O) vice(O) president(O) during(O) part(O) of(O) their(O) terms(O) ,(O) and(O) vice(O) president(O) Annette(B-politician) Lu(I-politician) has(O) at(O) times(O) been(O) mentioned(O) as(O) a(O) possible(O) candidate(O) for(O) premiership(O) .(O)"}}
{"id": "489", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "organization", "location", "election", "country", "person", "politician", "political party"], "instance": {"id": "489", "words": ["A", "special", "War", "Cabinet", "was", "created", "after", "war", "was", "declared", "-", "initially", "composed", "of", "Prime", "Minister", "Menzies", "and", "five", "senior", "ministers", "(", "RG", "Casey", ",", "GA", "Street", ",", "Senator", "McLeay", ",", "HS", "Gullet", "and", "World", "War", "I", "Prime", "Minister", "Billy", "Hughes", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "B-event", "I-event", "I-event", "O", "O", "B-politician", "I-politician", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, location, election, country, person, politician, political party and O.\nSentence: A special War Cabinet was created after war was declared - initially composed of Prime Minister Menzies and five senior ministers ( RG Casey , GA Street , Senator McLeay , HS Gullet and World War I Prime Minister Billy Hughes ) .", "prompt_labels": "A(O) special(O) War(O) Cabinet(O) was(O) created(O) after(O) war(O) was(O) declared(O) -(O) initially(O) composed(O) of(O) Prime(O) Minister(O) Menzies(B-politician) and(O) five(O) senior(O) ministers(O) ((O) RG(B-politician) Casey(I-politician) ,(O) GA(B-politician) Street(I-politician) ,(O) Senator(B-politician) McLeay(I-politician) ,(O) HS(B-politician) Gullet(I-politician) and(O) World(B-event) War(I-event) I(I-event) Prime(O) Minister(O) Billy(B-politician) Hughes(I-politician) )(O) .(O)"}}
{"id": "17", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "organization", "election", "person", "location", "political party", "country", "politician"], "instance": {"id": "17", "words": ["The", "Conservative", "Party", "of", "Canada", "took", "back", "this", "historically", "New", "Democratic", "Party", "(", "NDP", ")", "seat", "in", "2004", "Canadian", "federal", "election", "."], "labels": ["O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, election, person, location, political party, country, politician and O.\nSentence: The Conservative Party of Canada took back this historically New Democratic Party ( NDP ) seat in 2004 Canadian federal election .", "prompt_labels": "The(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) took(O) back(O) this(O) historically(O) New(B-political party) Democratic(I-political party) Party(I-political party) ((O) NDP(B-political party) )(O) seat(O) in(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)"}}
{"id": "519", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "political party", "country", "politician", "election", "person", "location", "organization"], "instance": {"id": "519", "words": ["U.S.", "President", "George", "W.", "Bush", "also", "set", "his", "hopes", "on", "leaders", "of", "the", "summit", "to", "back", "the", "2008", "G-20", "Washington", "summit", "and", "declaration", "to", "the", "financial", "crisis", "of", "2007-2008", "."], "labels": ["B-country", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, country, politician, election, person, location, organization and O.\nSentence: U.S. President George W. Bush also set his hopes on leaders of the summit to back the 2008 G-20 Washington summit and declaration to the financial crisis of 2007-2008 .", "prompt_labels": "U.S.(B-country) President(O) George(B-politician) W.(I-politician) Bush(I-politician) also(O) set(O) his(O) hopes(O) on(O) leaders(O) of(O) the(O) summit(O) to(O) back(O) the(O) 2008(B-event) G-20(I-event) Washington(I-event) summit(I-event) and(O) declaration(O) to(O) the(O) financial(B-event) crisis(I-event) of(I-event) 2007-2008(I-event) .(O)"}}
{"id": "248", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "politician", "country", "event", "political party", "person", "organization", "location"], "instance": {"id": "248", "words": ["She", "campaigned", "for", "the", "federal", "New", "Democratic", "Party", "in", "the", "1993", "Canadian", "federal", "election", "and", "received", "3,029", "votes", "(", "8.49", "%", ")", "in", "the", "riding", "of", "Portage", "-", "Interlake", ",", "finishing", "fourth", "against", "Liberal", "Party", "of", "Canada", "candidate", "Jon", "Gerrard", "."], "labels": ["O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, country, event, political party, person, organization, location and O.\nSentence: She campaigned for the federal New Democratic Party in the 1993 Canadian federal election and received 3,029 votes ( 8.49 % ) in the riding of Portage - Interlake , finishing fourth against Liberal Party of Canada candidate Jon Gerrard .", "prompt_labels": "She(O) campaigned(O) for(O) the(O) federal(O) New(B-political party) Democratic(I-political party) Party(I-political party) in(O) the(O) 1993(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) received(O) 3,029(O) votes(O) ((O) 8.49(O) %(O) )(O) in(O) the(O) riding(O) of(O) Portage(B-location) -(I-location) Interlake(I-location) ,(O) finishing(O) fourth(O) against(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) candidate(O) Jon(B-politician) Gerrard(I-politician) .(O)"}}
{"id": "2", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "politician", "political party", "event", "country", "person", "organization", "location"], "instance": {"id": "2", "words": ["Sitting", "as", "a", "Liberal", "Party", "of", "Canada", "Member", "of", "Parliament", "(", "MP", ")", "for", "Niagara", "Falls", ",", "she", "joined", "the", "Canadian", "Cabinet", "after", "the", "Liberals", "defeated", "the", "Progressive", "Conservative", "Party", "of", "Canada", "government", "of", "John", "Diefenbaker", "in", "the", "1963", "Canadian", "federal", "election", "."], "labels": ["O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-politician", "I-politician", "O", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, political party, event, country, person, organization, location and O.\nSentence: Sitting as a Liberal Party of Canada Member of Parliament ( MP ) for Niagara Falls , she joined the Canadian Cabinet after the Liberals defeated the Progressive Conservative Party of Canada government of John Diefenbaker in the 1963 Canadian federal election .", "prompt_labels": "Sitting(O) as(O) a(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) Member(O) of(O) Parliament(B-organization) ((O) MP(O) )(O) for(O) Niagara(O) Falls(O) ,(O) she(O) joined(O) the(O) Canadian(O) Cabinet(O) after(O) the(O) Liberals(O) defeated(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) government(O) of(O) John(B-politician) Diefenbaker(I-politician) in(O) the(O) 1963(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)"}}
{"id": "463", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "election", "political party", "location", "country", "person", "event", "organization"], "instance": {"id": "463", "words": ["In", "the", "biggest", "mobilisation", "of", "Australian", "Forces", "since", "the", "Vietnam", "War", ",", "the", "Government", "committed", "Australian", "naval", "forces", "to", "the", "1991", "Gulf", "War", "in", "support", "of", "the", "United", "States", "led", "coalition", "against", "the", "regime", "of", "Saddam", "Hussein", ",", "following", "the", "invasion", "of", "oil-rich", "Kuwait", "by", "Iraq", "on", "2", "August", "1990", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, political party, location, country, person, event, organization and O.\nSentence: In the biggest mobilisation of Australian Forces since the Vietnam War , the Government committed Australian naval forces to the 1991 Gulf War in support of the United States led coalition against the regime of Saddam Hussein , following the invasion of oil-rich Kuwait by Iraq on 2 August 1990 .", "prompt_labels": "In(O) the(O) biggest(O) mobilisation(O) of(O) Australian(B-organization) Forces(I-organization) since(O) the(O) Vietnam(B-event) War(I-event) ,(O) the(O) Government(O) committed(O) Australian(B-organization) naval(I-organization) forces(I-organization) to(O) the(O) 1991(O) Gulf(B-event) War(I-event) in(O) support(O) of(O) the(O) United(B-country) States(I-country) led(O) coalition(O) against(O) the(O) regime(O) of(O) Saddam(B-politician) Hussein(I-politician) ,(O) following(O) the(O) invasion(O) of(O) oil-rich(O) Kuwait(B-country) by(O) Iraq(B-country) on(O) 2(O) August(O) 1990(O) .(O)"}}
{"id": "245", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "location", "organization", "political party", "politician", "country", "election", "person"], "instance": {"id": "245", "words": ["They", "were", "told", "by", "organisations", "Disasters", "Emergency", "Committee", ",", "Amnesty", "International", ",", "Oxfam", ",", "Christian", "Aid", ",", "Save", "the", "Children", "Fund", "and", "the", "Catholic", "agency", "CAFOD", "that", "the", "topic", "was", "too", "sensitive", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, organization, political party, politician, country, election, person and O.\nSentence: They were told by organisations Disasters Emergency Committee , Amnesty International , Oxfam , Christian Aid , Save the Children Fund and the Catholic agency CAFOD that the topic was too sensitive .", "prompt_labels": "They(O) were(O) told(O) by(O) organisations(O) Disasters(B-organization) Emergency(I-organization) Committee(I-organization) ,(O) Amnesty(B-organization) International(I-organization) ,(O) Oxfam(B-organization) ,(O) Christian(B-organization) Aid(I-organization) ,(O) Save(B-organization) the(I-organization) Children(I-organization) Fund(I-organization) and(O) the(O) Catholic(O) agency(O) CAFOD(B-organization) that(O) the(O) topic(O) was(O) too(O) sensitive(O) .(O)"}}
{"id": "312", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "event", "location", "politician", "person", "organization", "country", "political party"], "instance": {"id": "312", "words": ["In", "the", "past", ",", "Harris", "served", "on", "the", "boards", "of", "the", "Girl", "Scouts", "of", "the", "USA", ",", "Independent", "Sector", ",", "Council", "on", "Foundations", ",", "National", "Organization", "for", "Women", ",", "National", "Urban", "League", ",", "Save", "the", "Children", ",", "National", "Committee", "Against", "Discrimination", "in", "Housing", ",", "and", "Overseas", "Development", "Corporation", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, location, politician, person, organization, country, political party and O.\nSentence: In the past , Harris served on the boards of the Girl Scouts of the USA , Independent Sector , Council on Foundations , National Organization for Women , National Urban League , Save the Children , National Committee Against Discrimination in Housing , and Overseas Development Corporation .", "prompt_labels": "In(O) the(O) past(O) ,(O) Harris(O) served(O) on(O) the(O) boards(O) of(O) the(O) Girl(B-organization) Scouts(I-organization) of(I-organization) the(I-organization) USA(I-organization) ,(O) Independent(B-organization) Sector(I-organization) ,(O) Council(B-organization) on(I-organization) Foundations(I-organization) ,(O) National(B-organization) Organization(I-organization) for(I-organization) Women(I-organization) ,(O) National(B-organization) Urban(I-organization) League(I-organization) ,(O) Save(B-organization) the(I-organization) Children(I-organization) ,(O) National(B-organization) Committee(I-organization) Against(I-organization) Discrimination(I-organization) in(I-organization) Housing(I-organization) ,(O) and(O) Overseas(B-organization) Development(I-organization) Corporation(I-organization) .(O)"}}
{"id": "478", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "location", "person", "political party", "election", "politician", "country", "organization"], "instance": {"id": "478", "words": ["He", "spent", "some", "time", "working", "abroad", ",", "notably", "in", "a", "field", "hospital", "from", "1951", "to", "1952", "during", "the", "Korean", "War", ",", "as", "leader", "of", "the", "Norwegian", "sanitary", "company", "stationed", "in", "Suez", "from", "1956", "to", "1957", "after", "the", "Suez", "Crisis", ",", "and", "as", "sanitary", "leader", "for", "United", "Nations", "Operation", "in", "the", "Congo", "(", "intervening", "in", "the", "Congo", "Crisis", ",", "State", "of", "Katanga", ")", "in", "1961", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "B-event", "I-event", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, person, political party, election, politician, country, organization and O.\nSentence: He spent some time working abroad , notably in a field hospital from 1951 to 1952 during the Korean War , as leader of the Norwegian sanitary company stationed in Suez from 1956 to 1957 after the Suez Crisis , and as sanitary leader for United Nations Operation in the Congo ( intervening in the Congo Crisis , State of Katanga ) in 1961 .", "prompt_labels": "He(O) spent(O) some(O) time(O) working(O) abroad(O) ,(O) notably(O) in(O) a(O) field(O) hospital(O) from(O) 1951(O) to(O) 1952(O) during(O) the(O) Korean(B-event) War(I-event) ,(O) as(O) leader(O) of(O) the(O) Norwegian(O) sanitary(O) company(O) stationed(O) in(O) Suez(B-location) from(O) 1956(O) to(O) 1957(O) after(O) the(O) Suez(B-event) Crisis(I-event) ,(O) and(O) as(O) sanitary(O) leader(O) for(O) United(B-event) Nations(I-event) Operation(I-event) in(I-event) the(I-event) Congo(I-event) ((O) intervening(O) in(O) the(O) Congo(B-event) Crisis(I-event) ,(O) State(B-country) of(I-country) Katanga(I-country) )(O) in(O) 1961(O) .(O)"}}
{"id": "133", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "person", "election", "location", "political party", "country", "politician", "organization"], "instance": {"id": "133", "words": ["Since", "1952", ",", "control", "of", "the", "House", "has", "changed", "hands", "five", "times", ",", "all", "of", "which", "were", "in", "midterm", "elections", "(", "1954", "United", "States", "House", "of", "Representatives", "elections", ",", "1994", "United", "States", "House", "of", "Representatives", "elections", ",", "2006", "United", "States", "House", "of", "Representatives", "elections", ",", "2010", "United", "States", "House", "of", "Representatives", "elections", "and", "2018", "United", "States", "House", "of", "Representatives", "elections", ")", "and", "all", "of", "which", "were", "at", "the", "expense", "of", "the", "incumbent", "President", "'s", "party", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, election, location, political party, country, politician, organization and O.\nSentence: Since 1952 , control of the House has changed hands five times , all of which were in midterm elections ( 1954 United States House of Representatives elections , 1994 United States House of Representatives elections , 2006 United States House of Representatives elections , 2010 United States House of Representatives elections and 2018 United States House of Representatives elections ) and all of which were at the expense of the incumbent President 's party .", "prompt_labels": "Since(O) 1952(O) ,(O) control(O) of(O) the(O) House(O) has(O) changed(O) hands(O) five(O) times(O) ,(O) all(O) of(O) which(O) were(O) in(O) midterm(O) elections(O) ((O) 1954(B-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) ,(O) 1994(B-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) ,(O) 2006(B-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) ,(O) 2010(B-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) and(O) 2018(B-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) )(O) and(O) all(O) of(O) which(O) were(O) at(O) the(O) expense(O) of(O) the(O) incumbent(O) President(O) 's(O) party(O) .(O)"}}
{"id": "195", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "political party", "country", "person", "organization", "event", "election", "politician"], "instance": {"id": "195", "words": ["In", "Belgian", "politics", ",", "the", "term", "Jamaica", "coalition", "refers", "to", "a", "coalition", "of", "Christian", "democrats", "(", "Christen-Democratisch", "en", "Vlaams", "and", "Centre", "démocrate", "humaniste", ")", ",", "liberals", "(", "Open", "Vlaamse", "Liberalen", "en", "Democraten", "and", "Mouvement", "Réformateur", ")", "and", "greens", "(", "Groen", "and", "Ecolo", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "O", "O", "B-political party", "O", "B-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, country, person, organization, event, election, politician and O.\nSentence: In Belgian politics , the term Jamaica coalition refers to a coalition of Christian democrats ( Christen-Democratisch en Vlaams and Centre démocrate humaniste ) , liberals ( Open Vlaamse Liberalen en Democraten and Mouvement Réformateur ) and greens ( Groen and Ecolo ) .", "prompt_labels": "In(O) Belgian(O) politics(O) ,(O) the(O) term(O) Jamaica(B-country) coalition(O) refers(O) to(O) a(O) coalition(O) of(O) Christian(O) democrats(O) ((O) Christen-Democratisch(B-political party) en(I-political party) Vlaams(I-political party) and(O) Centre(B-political party) démocrate(I-political party) humaniste(I-political party) )(O) ,(O) liberals(O) ((O) Open(B-political party) Vlaamse(I-political party) Liberalen(I-political party) en(I-political party) Democraten(I-political party) and(O) Mouvement(B-political party) Réformateur(I-political party) )(O) and(O) greens(O) ((O) Groen(B-political party) and(O) Ecolo(B-political party) )(O) .(O)"}}
{"id": "383", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "political party", "organization", "country", "event", "politician", "person", "location"], "instance": {"id": "383", "words": ["However", ",", "he", "did", "continue", "in", "the", "Norwegian", "Parliament", ",", "being", "elected", "in", "1930", "Norwegian", "parliamentary", "election", ",", "1933", "Norwegian", "parliamentary", "election", "and", "1936", "Norwegian", "parliamentary", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, organization, country, event, politician, person, location and O.\nSentence: However , he did continue in the Norwegian Parliament , being elected in 1930 Norwegian parliamentary election , 1933 Norwegian parliamentary election and 1936 Norwegian parliamentary election .", "prompt_labels": "However(O) ,(O) he(O) did(O) continue(O) in(O) the(O) Norwegian(B-organization) Parliament(I-organization) ,(O) being(O) elected(O) in(O) 1930(B-election) Norwegian(I-election) parliamentary(I-election) election(I-election) ,(O) 1933(B-election) Norwegian(I-election) parliamentary(I-election) election(I-election) and(O) 1936(B-election) Norwegian(I-election) parliamentary(I-election) election(I-election) .(O)"}}
{"id": "287", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "organization", "country", "political party", "event", "election", "location", "politician"], "instance": {"id": "287", "words": ["Although", "Barak", "won", "the", "Prime", "Ministerial", "election", "comfortably", ",", "his", "One", "Israel", "alliance", "won", "only", "26", "seats", ",", "meaning", "he", "had", "to", "form", "a", "convoluted", "coalition", "with", "Shas", ",", "Meretz", ",", "Yisrael", "BaAliyah", ",", "the", "Centre", "Party", ",", "the", "National", "Religious", "Party", "and", "United", "Torah", "Judaism", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "B-political party", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, country, political party, event, election, location, politician and O.\nSentence: Although Barak won the Prime Ministerial election comfortably , his One Israel alliance won only 26 seats , meaning he had to form a convoluted coalition with Shas , Meretz , Yisrael BaAliyah , the Centre Party , the National Religious Party and United Torah Judaism .", "prompt_labels": "Although(O) Barak(O) won(O) the(O) Prime(O) Ministerial(O) election(O) comfortably(O) ,(O) his(O) One(B-political party) Israel(I-political party) alliance(O) won(O) only(O) 26(O) seats(O) ,(O) meaning(O) he(O) had(O) to(O) form(O) a(O) convoluted(O) coalition(O) with(O) Shas(B-political party) ,(O) Meretz(B-political party) ,(O) Yisrael(B-political party) BaAliyah(I-political party) ,(O) the(O) Centre(B-political party) Party(I-political party) ,(O) the(O) National(B-political party) Religious(I-political party) Party(I-political party) and(O) United(B-political party) Torah(I-political party) Judaism(I-political party) .(O)"}}
{"id": "381", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "organization", "person", "political party", "election", "politician", "country", "event"], "instance": {"id": "381", "words": ["He", "was", "also", "elected", "as", "a", "deputy", "representative", "to", "the", "Parliament", "of", "Norway", "from", "Telemark", "in", "1965", "Norwegian", "parliamentary", "election", ",", "and", "was", "subsequently", "elected", "to", "four", "full", "terms", "in", "1969", "Norwegian", "parliamentary", "election", ",", "1973", "Norwegian", "parliamentary", "election", ",", "1977", "Norwegian", "parliamentary", "election", "and", "1981", "Norwegian", "parliamentary", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, person, political party, election, politician, country, event and O.\nSentence: He was also elected as a deputy representative to the Parliament of Norway from Telemark in 1965 Norwegian parliamentary election , and was subsequently elected to four full terms in 1969 Norwegian parliamentary election , 1973 Norwegian parliamentary election , 1977 Norwegian parliamentary election and 1981 Norwegian parliamentary election .", "prompt_labels": "He(O) was(O) also(O) elected(O) as(O) a(O) deputy(O) representative(O) to(O) the(O) Parliament(B-organization) of(I-organization) Norway(I-organization) from(O) Telemark(B-location) in(O) 1965(B-election) Norwegian(I-election) parliamentary(I-election) election(I-election) ,(O) and(O) was(O) subsequently(O) elected(O) to(O) four(O) full(O) terms(O) in(O) 1969(B-election) Norwegian(I-election) parliamentary(I-election) election(I-election) ,(O) 1973(B-election) Norwegian(I-election) parliamentary(I-election) election(I-election) ,(O) 1977(B-election) Norwegian(I-election) parliamentary(I-election) election(I-election) and(O) 1981(B-election) Norwegian(I-election) parliamentary(I-election) election(I-election) .(O)"}}
{"id": "16", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "country", "location", "election", "event", "politician", "political party", "person"], "instance": {"id": "16", "words": ["It", "emerged", "in", "1992", "when", "the", "old", "Sammarinese", "Communist", "Party", "evolved", "into", "the", "Sammarinese", "Democratic", "Progressive", "Party", "and", "some", "members", ",", "on", "the", "example", "of", "the", "Italian", "Communist", "Refoundation", "Party", ",", "decided", "not", "to", "join", "the", "new", "party", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, location, election, event, politician, political party, person and O.\nSentence: It emerged in 1992 when the old Sammarinese Communist Party evolved into the Sammarinese Democratic Progressive Party and some members , on the example of the Italian Communist Refoundation Party , decided not to join the new party .", "prompt_labels": "It(O) emerged(O) in(O) 1992(O) when(O) the(O) old(O) Sammarinese(B-political party) Communist(I-political party) Party(I-political party) evolved(O) into(O) the(O) Sammarinese(B-political party) Democratic(I-political party) Progressive(I-political party) Party(I-political party) and(O) some(O) members(O) ,(O) on(O) the(O) example(O) of(O) the(O) Italian(O) Communist(B-political party) Refoundation(I-political party) Party(I-political party) ,(O) decided(O) not(O) to(O) join(O) the(O) new(O) party(O) .(O)"}}
{"id": "190", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "politician", "organization", "election", "country", "political party", "event", "person"], "instance": {"id": "190", "words": ["Grande", "was", "elected", "to", "the", "Ontario", "legislature", "in", "the", "1975", "Ontario", "general", "election", ",", "and", "re-elected", "in", "1977", "Ontario", "general", "election", ",", "1981", "Ontario", "general", "election", "and", "1985", "Ontario", "general", "election", "."], "labels": ["B-person", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, organization, election, country, political party, event, person and O.\nSentence: Grande was elected to the Ontario legislature in the 1975 Ontario general election , and re-elected in 1977 Ontario general election , 1981 Ontario general election and 1985 Ontario general election .", "prompt_labels": "Grande(B-person) was(O) elected(O) to(O) the(O) Ontario(B-organization) legislature(I-organization) in(O) the(O) 1975(B-election) Ontario(I-election) general(I-election) election(I-election) ,(O) and(O) re-elected(O) in(O) 1977(B-election) Ontario(I-election) general(I-election) election(I-election) ,(O) 1981(B-election) Ontario(I-election) general(I-election) election(I-election) and(O) 1985(B-election) Ontario(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "482", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "country", "person", "politician", "political party", "event", "location", "election"], "instance": {"id": "482", "words": ["He", "spurred", "the", "ongoing", "revolt", "in", "Duchy", "of", "Saxony", ",", "which", "had", "forced", "Henry", "IV", "to", "retreat", "from", "that", "region", "(", "he", "crushed", "the", "revolt", "at", "the", "Battle", "of", "Langensalza", "soon", "thereafter", ")", ";", "the", "Polish", "king", "seized", "the", "occasion", "to", "launch", "an", "invasion", "against", "Henry", "IV", "'s", "vassal", ",", "Vratislaus", "II", "of", "Bohemia", ",", "alongside", "an", "ally", "from", "Grand", "Prince", "Vladimir", "II", "Monomakh", "of", "Kiev", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-person", "I-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, person, politician, political party, event, location, election and O.\nSentence: He spurred the ongoing revolt in Duchy of Saxony , which had forced Henry IV to retreat from that region ( he crushed the revolt at the Battle of Langensalza soon thereafter ) ; the Polish king seized the occasion to launch an invasion against Henry IV 's vassal , Vratislaus II of Bohemia , alongside an ally from Grand Prince Vladimir II Monomakh of Kiev .", "prompt_labels": "He(O) spurred(O) the(O) ongoing(O) revolt(O) in(O) Duchy(B-country) of(I-country) Saxony(I-country) ,(O) which(O) had(O) forced(O) Henry(B-person) IV(I-person) to(O) retreat(O) from(O) that(O) region(O) ((O) he(O) crushed(O) the(O) revolt(O) at(O) the(O) Battle(B-event) of(I-event) Langensalza(I-event) soon(O) thereafter(O) )(O) ;(O) the(O) Polish(O) king(O) seized(O) the(O) occasion(O) to(O) launch(O) an(O) invasion(O) against(O) Henry(B-person) IV(I-person) 's(O) vassal(O) ,(O) Vratislaus(B-person) II(I-person) of(I-person) Bohemia(I-person) ,(O) alongside(O) an(O) ally(O) from(O) Grand(O) Prince(O) Vladimir(B-person) II(I-person) Monomakh(I-person) of(O) Kiev(B-location) .(O)"}}
{"id": "47", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "country", "event", "person", "location", "organization", "election", "political party"], "instance": {"id": "47", "words": ["Other", "parties", "such", "as", "the", "Alliance", "Party", "of", "Northern", "Ireland", ",", "Progressive", "Unionist", "Party", ",", "Unionist", "Party", "of", "Northern", "Ireland", ",", "Conservatives", "and", "the", "Workers", "'", "Party", "have", "at", "times", "polled", "significantly", ",", "as", "have", "independent", "candidates", ",", "with", "the", "result", "that", "many", "elections", "have", "been", "won", "on", "comparatively", "low", "shares", "of", "the", "vote", "."], "labels": ["O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, event, person, location, organization, election, political party and O.\nSentence: Other parties such as the Alliance Party of Northern Ireland , Progressive Unionist Party , Unionist Party of Northern Ireland , Conservatives and the Workers ' Party have at times polled significantly , as have independent candidates , with the result that many elections have been won on comparatively low shares of the vote .", "prompt_labels": "Other(O) parties(O) such(O) as(O) the(O) Alliance(B-political party) Party(I-political party) of(I-political party) Northern(I-political party) Ireland(I-political party) ,(O) Progressive(B-political party) Unionist(I-political party) Party(I-political party) ,(O) Unionist(B-political party) Party(I-political party) of(I-political party) Northern(I-political party) Ireland(I-political party) ,(O) Conservatives(B-political party) and(I-political party) the(I-political party) Workers(I-political party) '(I-political party) Party(I-political party) have(O) at(O) times(O) polled(O) significantly(O) ,(O) as(O) have(O) independent(O) candidates(O) ,(O) with(O) the(O) result(O) that(O) many(O) elections(O) have(O) been(O) won(O) on(O) comparatively(O) low(O) shares(O) of(O) the(O) vote(O) .(O)"}}
{"id": "484", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "politician", "country", "person", "location", "event", "election", "organization"], "instance": {"id": "484", "words": ["During", "World", "War", "I", "Kähler", "supported", "left", "wing", "SPD", "politicians", ",", "that", "included", "Clara", "Zetkin", "and", "Rosa", "Luxemburg", "in", "rejecting", "the", "party", "'s", "policy", "of", "Burgfrieden", "(", "a", "truce", "with", "the", "government", ",", "promising", "to", "refrain", "from", "any", "strikes", "during", "the", "war", ")", "and", "attended", "an", "international", "socialist", "women", "'s", "anti-war", "conference", "in", "Berlin", "organised", "by", "Zetkin", "in", "1915", "."], "labels": ["O", "B-event", "I-event", "I-event", "B-politician", "O", "O", "O", "B-political party", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "B-location", "O", "O", "B-person", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, country, person, location, event, election, organization and O.\nSentence: During World War I Kähler supported left wing SPD politicians , that included Clara Zetkin and Rosa Luxemburg in rejecting the party 's policy of Burgfrieden ( a truce with the government , promising to refrain from any strikes during the war ) and attended an international socialist women 's anti-war conference in Berlin organised by Zetkin in 1915 .", "prompt_labels": "During(O) World(B-event) War(I-event) I(I-event) Kähler(B-politician) supported(O) left(O) wing(O) SPD(B-political party) politicians(O) ,(O) that(O) included(O) Clara(B-person) Zetkin(I-person) and(O) Rosa(B-person) Luxemburg(I-person) in(O) rejecting(O) the(O) party(O) 's(O) policy(O) of(O) Burgfrieden(O) ((O) a(O) truce(O) with(O) the(O) government(O) ,(O) promising(O) to(O) refrain(O) from(O) any(O) strikes(O) during(O) the(O) war(O) )(O) and(O) attended(O) an(O) international(B-event) socialist(I-event) women(I-event) 's(I-event) anti-war(I-event) conference(I-event) in(O) Berlin(B-location) organised(O) by(O) Zetkin(B-person) in(O) 1915(O) .(O)"}}
{"id": "7", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "person", "country", "politician", "organization", "event", "political party", "location"], "instance": {"id": "7", "words": ["Of", "the", "current", "first", "ministers", ",", "four", "are", "from", "a", "Liberal", "Party", ",", "four", "are", "from", "a", "Progressive", "Conservative", "Party", ",", "and", "one", "is", "from", "a", "New", "Democratic", "Party", ";", "three", "others", "are", "from", "local", "parties", "(", "the", "Coalition", "Avenir", "Québec", ",", "the", "Saskatchewan", "Party", ",", "and", "the", "United", "Conservative", "Party", ")", "and", "two", "are", "non-partisan", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, country, politician, organization, event, political party, location and O.\nSentence: Of the current first ministers , four are from a Liberal Party , four are from a Progressive Conservative Party , and one is from a New Democratic Party ; three others are from local parties ( the Coalition Avenir Québec , the Saskatchewan Party , and the United Conservative Party ) and two are non-partisan .", "prompt_labels": "Of(O) the(O) current(O) first(O) ministers(O) ,(O) four(O) are(O) from(O) a(O) Liberal(B-political party) Party(I-political party) ,(O) four(O) are(O) from(O) a(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) ,(O) and(O) one(O) is(O) from(O) a(O) New(B-political party) Democratic(I-political party) Party(I-political party) ;(O) three(O) others(O) are(O) from(O) local(O) parties(O) ((O) the(O) Coalition(B-political party) Avenir(I-political party) Québec(I-political party) ,(O) the(O) Saskatchewan(B-political party) Party(I-political party) ,(O) and(O) the(O) United(B-political party) Conservative(I-political party) Party(I-political party) )(O) and(O) two(O) are(O) non-partisan(O) .(O)"}}
{"id": "60", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "organization", "politician", "country", "location", "election", "person", "event"], "instance": {"id": "60", "words": ["In", "the", "2001", "United", "Kingdom", "general", "election", "the", "votes", "for", "the", "UUP", ",", "Democratic", "Unionist", "Party", "and", "Alliance", "Party", "of", "Northern", "Ireland", "all", "remained", "remarkably", "stable", "compared", "to", "significant", "shifts", "elsewhere", "in", "Northern", "Ireland", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, politician, country, location, election, person, event and O.\nSentence: In the 2001 United Kingdom general election the votes for the UUP , Democratic Unionist Party and Alliance Party of Northern Ireland all remained remarkably stable compared to significant shifts elsewhere in Northern Ireland .", "prompt_labels": "In(O) the(O) 2001(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) the(O) votes(O) for(O) the(O) UUP(B-political party) ,(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) and(O) Alliance(B-political party) Party(I-political party) of(I-political party) Northern(I-political party) Ireland(I-political party) all(O) remained(O) remarkably(O) stable(O) compared(O) to(O) significant(O) shifts(O) elsewhere(O) in(O) Northern(B-country) Ireland(I-country) .(O)"}}
{"id": "225", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "person", "politician", "country", "political party", "event", "organization", "election"], "instance": {"id": "225", "words": ["He", "made", "other", "unsuccessful", "attempts", "to", "be", "elected", "to", "the", "House", "of", "Commons", "at", "Glasgow", "Pollok", "in", "October", "1974", "United", "Kingdom", "general", "election", ",", "Roxburgh", ",", "Peebles", "and", "Selkirk", "at", "the", "1979", "United", "Kingdom", "general", "election", ",", "and", "the", "1982", "Glasgow", "Hillhead", "by-election", ";", "where", "he", "lost", "the", "traditionally", "Conservative", "seat", "to", "Roy", "Jenkins", "of", "the", "SDP", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "I-location", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, politician, country, political party, event, organization, election and O.\nSentence: He made other unsuccessful attempts to be elected to the House of Commons at Glasgow Pollok in October 1974 United Kingdom general election , Roxburgh , Peebles and Selkirk at the 1979 United Kingdom general election , and the 1982 Glasgow Hillhead by-election ; where he lost the traditionally Conservative seat to Roy Jenkins of the SDP .", "prompt_labels": "He(O) made(O) other(O) unsuccessful(O) attempts(O) to(O) be(O) elected(O) to(O) the(O) House(B-organization) of(I-organization) Commons(I-organization) at(O) Glasgow(B-location) Pollok(I-location) in(O) October(B-election) 1974(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) Roxburgh(B-location) ,(I-location) Peebles(I-location) and(I-location) Selkirk(I-location) at(O) the(O) 1979(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) and(O) the(O) 1982(B-election) Glasgow(I-election) Hillhead(I-election) by-election(I-election) ;(O) where(O) he(O) lost(O) the(O) traditionally(O) Conservative(O) seat(O) to(O) Roy(B-politician) Jenkins(I-politician) of(O) the(O) SDP(B-political party) .(O)"}}
{"id": "501", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "event", "political party", "location", "election", "person", "country", "politician"], "instance": {"id": "501", "words": ["The", "Persian", "Campaign", "or", "Invasion", "of", "Iran", "(", ")", "was", "a", "series", "of", "engagements", "in", "Iranian", "Azerbaijan", "region", "involving", "the", "forces", "of", "the", "Ottoman", "Empire", "against", "those", "of", "the", "British", "Empire", "and", "Russian", "Empire", ",", "and", "also", "involving", "local", "Persian", "population", "elements", ",", "beginning", "in", "December", "1914", "and", "ending", "with", "the", "Armistice", "of", "Mudros", "on", "October", "30", ",", "1918", ",", "as", "part", "of", "Middle", "Eastern", "theatre", "of", "World", "War", "I", "."], "labels": ["O", "B-event", "I-event", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "B-country", "I-country", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, political party, location, election, person, country, politician and O.\nSentence: The Persian Campaign or Invasion of Iran ( ) was a series of engagements in Iranian Azerbaijan region involving the forces of the Ottoman Empire against those of the British Empire and Russian Empire , and also involving local Persian population elements , beginning in December 1914 and ending with the Armistice of Mudros on October 30 , 1918 , as part of Middle Eastern theatre of World War I .", "prompt_labels": "The(O) Persian(B-event) Campaign(I-event) or(O) Invasion(B-event) of(I-event) Iran(I-event) ((O) )(O) was(O) a(O) series(O) of(O) engagements(O) in(O) Iranian(B-location) Azerbaijan(I-location) region(O) involving(O) the(O) forces(O) of(O) the(O) Ottoman(B-country) Empire(I-country) against(O) those(O) of(O) the(O) British(B-country) Empire(I-country) and(O) Russian(B-country) Empire(I-country) ,(O) and(O) also(O) involving(O) local(O) Persian(O) population(O) elements(O) ,(O) beginning(O) in(O) December(O) 1914(O) and(O) ending(O) with(O) the(O) Armistice(B-event) of(I-event) Mudros(I-event) on(O) October(O) 30(O) ,(O) 1918(O) ,(O) as(O) part(O) of(O) Middle(B-event) Eastern(I-event) theatre(I-event) of(I-event) World(I-event) War(I-event) I(I-event) .(O)"}}
{"id": "262", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "election", "location", "organization", "politician", "event", "political party", "person"], "instance": {"id": "262", "words": ["At", "the", "1922", "United", "Kingdom", "general", "election", "Paling", "was", "elected", "member", "of", "parliament", "(", "MP", ")", "for", "Doncaster", ",", "and", "was", "re-elected", "in", "1923", "United", "Kingdom", "general", "election", ",", "1924", "United", "Kingdom", "general", "election", "and", "1929", "United", "Kingdom", "general", "election", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, location, organization, politician, event, political party, person and O.\nSentence: At the 1922 United Kingdom general election Paling was elected member of parliament ( MP ) for Doncaster , and was re-elected in 1923 United Kingdom general election , 1924 United Kingdom general election and 1929 United Kingdom general election .", "prompt_labels": "At(O) the(O) 1922(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) Paling(B-politician) was(O) elected(O) member(O) of(O) parliament(O) ((O) MP(O) )(O) for(O) Doncaster(B-location) ,(O) and(O) was(O) re-elected(O) in(O) 1923(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 1924(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1929(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "244", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "event", "person", "election", "politician", "political party", "location", "country"], "instance": {"id": "244", "words": ["The", "Price-Anderson", "Act", "has", "been", "criticized", "by", "various", "think", "tanks", "and", "environmental", "organizations", ",", "including", "Union", "of", "Concerned", "Scientists", ",", "Greenpeace", "International", ",", "Public", "Citizen", "and", "the", "Cato", "Institute", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, person, election, politician, political party, location, country and O.\nSentence: The Price-Anderson Act has been criticized by various think tanks and environmental organizations , including Union of Concerned Scientists , Greenpeace International , Public Citizen and the Cato Institute .", "prompt_labels": "The(O) Price-Anderson(O) Act(O) has(O) been(O) criticized(O) by(O) various(O) think(O) tanks(O) and(O) environmental(O) organizations(O) ,(O) including(O) Union(B-organization) of(I-organization) Concerned(I-organization) Scientists(I-organization) ,(O) Greenpeace(B-organization) International(I-organization) ,(O) Public(B-organization) Citizen(I-organization) and(O) the(O) Cato(B-organization) Institute(I-organization) .(O)"}}
{"id": "283", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "country", "event", "politician", "person", "election", "political party", "location"], "instance": {"id": "283", "words": ["Shaw", "himself", "ran", "for", "the", "CCF", "in", "the", "1945", "Canadian", "federal", "election", "and", "1949", "Canadian", "federal", "election", "federal", "elections", "and", "in", "a", "federal", "byelection", "in", "1948", ",", "and", "for", "the", "NDP", "in", "the", "1974", "Canadian", "federal", "election", "federal", "election", ",", "but", "was", "never", "elected", "to", "the", "House", "of", "Commons", "of", "Canada", "."], "labels": ["B-politician", "O", "O", "O", "O", "B-organization", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, event, politician, person, election, political party, location and O.\nSentence: Shaw himself ran for the CCF in the 1945 Canadian federal election and 1949 Canadian federal election federal elections and in a federal byelection in 1948 , and for the NDP in the 1974 Canadian federal election federal election , but was never elected to the House of Commons of Canada .", "prompt_labels": "Shaw(B-politician) himself(O) ran(O) for(O) the(O) CCF(B-organization) in(O) the(O) 1945(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 1949(B-election) Canadian(I-election) federal(I-election) election(I-election) federal(O) elections(O) and(O) in(O) a(O) federal(O) byelection(O) in(O) 1948(O) ,(O) and(O) for(O) the(O) NDP(B-political party) in(O) the(O) 1974(B-election) Canadian(I-election) federal(I-election) election(I-election) federal(O) election(O) ,(O) but(O) was(O) never(O) elected(O) to(O) the(O) House(B-organization) of(I-organization) Commons(I-organization) of(I-organization) Canada(I-organization) .(O)"}}
{"id": "186", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "organization", "political party", "location", "politician", "country", "person", "event"], "instance": {"id": "186", "words": ["A", "significant", "number", "of", "Straight", "Left", "faction", "members", "had", "developed", "close", "personal", "friendships", "with", "members", "of", "fraternal", "communist", "parties", ",", "particularly", "the", "Tudeh", "Party", "of", "Iran", ",", "Iraqi", "Communist", "Party", ",", "South", "African", "and", "Communist", "Party", "of", "Greece", "parties", ",", "who", "were", "well", "organised", "on", "most", "British", "University", "campuses", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, political party, location, politician, country, person, event and O.\nSentence: A significant number of Straight Left faction members had developed close personal friendships with members of fraternal communist parties , particularly the Tudeh Party of Iran , Iraqi Communist Party , South African and Communist Party of Greece parties , who were well organised on most British University campuses .", "prompt_labels": "A(O) significant(O) number(O) of(O) Straight(B-organization) Left(I-organization) faction(O) members(O) had(O) developed(O) close(O) personal(O) friendships(O) with(O) members(O) of(O) fraternal(O) communist(O) parties(O) ,(O) particularly(O) the(O) Tudeh(B-political party) Party(I-political party) of(I-political party) Iran(I-political party) ,(O) Iraqi(B-political party) Communist(I-political party) Party(I-political party) ,(O) South(O) African(O) and(O) Communist(B-political party) Party(I-political party) of(I-political party) Greece(I-political party) parties(O) ,(O) who(O) were(O) well(O) organised(O) on(O) most(O) British(O) University(O) campuses(O) .(O)"}}
{"id": "75", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "election", "organization", "political party", "location", "country", "person", "politician"], "instance": {"id": "75", "words": ["She", "was", "elected", "to", "the", "Ontario", "legislature", "in", "the", "1995", "Ontario", "general", "election", ",", "defeating", "Ontario", "Liberal", "Party", "Joe", "Dickson", "and", "incumbent", "Ontario", "New", "Democratic", "Party", "Jim", "Wiseman", "by", "a", "significant", "margin", "in", "the", "riding", "of", "Durham", "West", ",", "east", "of", "Toronto", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-political party", "I-political party", "I-political party", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, organization, political party, location, country, person, politician and O.\nSentence: She was elected to the Ontario legislature in the 1995 Ontario general election , defeating Ontario Liberal Party Joe Dickson and incumbent Ontario New Democratic Party Jim Wiseman by a significant margin in the riding of Durham West , east of Toronto .", "prompt_labels": "She(O) was(O) elected(O) to(O) the(O) Ontario(B-organization) legislature(I-organization) in(O) the(O) 1995(B-election) Ontario(I-election) general(I-election) election(I-election) ,(O) defeating(O) Ontario(B-political party) Liberal(I-political party) Party(I-political party) Joe(B-politician) Dickson(I-politician) and(O) incumbent(O) Ontario(B-political party) New(I-political party) Democratic(I-political party) Party(I-political party) Jim(B-politician) Wiseman(I-politician) by(O) a(O) significant(O) margin(O) in(O) the(O) riding(O) of(O) Durham(B-location) West(I-location) ,(O) east(O) of(O) Toronto(B-location) .(O)"}}
{"id": "459", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "event", "organization", "politician", "person", "country", "political party", "location"], "instance": {"id": "459", "words": ["As", "Bangladesh", "'s", "representative", ",", "he", "spearheaded", "the", "country", "'s", "entry", "into", "the", "World", "Bank", ",", "International", "Monetary", "Fund", "and", "Asian", "Development", "Bank", "."], "labels": ["O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, organization, politician, person, country, political party, location and O.\nSentence: As Bangladesh 's representative , he spearheaded the country 's entry into the World Bank , International Monetary Fund and Asian Development Bank .", "prompt_labels": "As(O) Bangladesh(B-country) 's(O) representative(O) ,(O) he(O) spearheaded(O) the(O) country(O) 's(O) entry(O) into(O) the(O) World(B-organization) Bank(I-organization) ,(O) International(B-organization) Monetary(I-organization) Fund(I-organization) and(O) Asian(B-organization) Development(I-organization) Bank(I-organization) .(O)"}}
{"id": "6", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "organization", "country", "election", "political party", "location", "person", "event"], "instance": {"id": "6", "words": ["Their", "destinies", "are", "all", "altered", "and", "shaped", "by", "the", "historical", "event", "(", "for", "example", ":", "Simard", "becomes", "a", "sovereigntist", "and", "will", "leave", "the", "Quebec", "Liberal", "Party", "for", "the", "Parti", "Québécois", ";", "Dumont", "will", "also", "slam", "the", "Liberal", "door", "to", "later", "help", "create", "and", "finally", "become", "leader", "of", "the", "Action", "démocratique", "du", "Québec", ",", "or", "ADQ", ",", "and", "support", "the", "Yes", "side", "of", "the", "1995", "referendum", "on", "independence", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, country, election, political party, location, person, event and O.\nSentence: Their destinies are all altered and shaped by the historical event ( for example : Simard becomes a sovereigntist and will leave the Quebec Liberal Party for the Parti Québécois ; Dumont will also slam the Liberal door to later help create and finally become leader of the Action démocratique du Québec , or ADQ , and support the Yes side of the 1995 referendum on independence ) .", "prompt_labels": "Their(O) destinies(O) are(O) all(O) altered(O) and(O) shaped(O) by(O) the(O) historical(O) event(O) ((O) for(O) example(O) :(O) Simard(B-politician) becomes(O) a(O) sovereigntist(O) and(O) will(O) leave(O) the(O) Quebec(B-political party) Liberal(I-political party) Party(I-political party) for(O) the(O) Parti(B-political party) Québécois(I-political party) ;(O) Dumont(B-politician) will(O) also(O) slam(O) the(O) Liberal(O) door(O) to(O) later(O) help(O) create(O) and(O) finally(O) become(O) leader(O) of(O) the(O) Action(B-political party) démocratique(I-political party) du(I-political party) Québec(I-political party) ,(O) or(O) ADQ(B-political party) ,(O) and(O) support(O) the(O) Yes(O) side(O) of(O) the(O) 1995(B-event) referendum(I-event) on(O) independence(O) )(O) .(O)"}}
{"id": "207", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "organization", "location", "person", "event", "political party", "country", "politician"], "instance": {"id": "207", "words": ["NJA", "helped", "with", "the", "development", "of", "other", "left", "wing", "Jewish", "organizations", ",", "including", "Americans", "for", "Peace", "Now", ",", "The", "New", "Israel", "Fund", ",", "Jewish", "Fund", "for", "Justice", ",", "The", "Shalom", "Center", ",", "The", "Shefa", "Fund", ",", "Bridges", "Journal", ",", "American", "Friends", "of", "Neve", "Shalom", ",", "Brit", "Tzedek", "v", "'Shalom", ",", "Bat", "Shalom", ",", "and", "The", "Abraham", "Fund", "."], "labels": ["B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, location, person, event, political party, country, politician and O.\nSentence: NJA helped with the development of other left wing Jewish organizations , including Americans for Peace Now , The New Israel Fund , Jewish Fund for Justice , The Shalom Center , The Shefa Fund , Bridges Journal , American Friends of Neve Shalom , Brit Tzedek v 'Shalom , Bat Shalom , and The Abraham Fund .", "prompt_labels": "NJA(B-organization) helped(O) with(O) the(O) development(O) of(O) other(O) left(O) wing(O) Jewish(O) organizations(O) ,(O) including(O) Americans(B-organization) for(I-organization) Peace(I-organization) Now(I-organization) ,(O) The(O) New(B-organization) Israel(I-organization) Fund(I-organization) ,(O) Jewish(B-organization) Fund(I-organization) for(I-organization) Justice(I-organization) ,(O) The(O) Shalom(B-organization) Center(I-organization) ,(O) The(O) Shefa(B-organization) Fund(I-organization) ,(O) Bridges(B-organization) Journal(I-organization) ,(O) American(B-organization) Friends(I-organization) of(I-organization) Neve(I-organization) Shalom(I-organization) ,(O) Brit(B-organization) Tzedek(I-organization) v(I-organization) 'Shalom(I-organization) ,(O) Bat(B-organization) Shalom(I-organization) ,(O) and(O) The(B-organization) Abraham(I-organization) Fund(I-organization) .(O)"}}
{"id": "281", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "person", "country", "election", "location", "political party", "event", "organization"], "instance": {"id": "281", "words": ["As", "a", "member", "of", "the", "Scottish", "Conservatives", ",", "he", "was", "a", "Member", "of", "the", "Scottish", "Parliament", "(", "MSP", ")", "for", "the", "Mid", "Scotland", "and", "Fife", "region", "from", "1999", "Scottish", "Parliament", "election", "to", "2007", "Scottish", "Parliament", "election", "."], "labels": ["O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, country, election, location, political party, event, organization and O.\nSentence: As a member of the Scottish Conservatives , he was a Member of the Scottish Parliament ( MSP ) for the Mid Scotland and Fife region from 1999 Scottish Parliament election to 2007 Scottish Parliament election .", "prompt_labels": "As(O) a(O) member(O) of(O) the(O) Scottish(B-political party) Conservatives(I-political party) ,(O) he(O) was(O) a(O) Member(O) of(O) the(O) Scottish(B-organization) Parliament(I-organization) ((O) MSP(O) )(O) for(O) the(O) Mid(B-location) Scotland(I-location) and(I-location) Fife(I-location) region(O) from(O) 1999(B-election) Scottish(I-election) Parliament(I-election) election(I-election) to(O) 2007(B-election) Scottish(I-election) Parliament(I-election) election(I-election) .(O)"}}
{"id": "284", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "organization", "location", "event", "person", "country", "political party", "politician"], "instance": {"id": "284", "words": ["She", "previously", "represented", "the", "riding", "of", "Lotbinière", "from", "2003", "Quebec", "general", "election", "until", "2012", ",", "initially", "as", "a", "member", "of", "the", "now-defunct", "Action", "démocratique", "du", "Québec", "(", "ADQ", ")", "until", "the", "merger", "of", "that", "party", "into", "the", "Coalition", "Avenir", "Québec", "(", "CAQ", ")", "in", "2012", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-location", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, location, event, person, country, political party, politician and O.\nSentence: She previously represented the riding of Lotbinière from 2003 Quebec general election until 2012 , initially as a member of the now-defunct Action démocratique du Québec ( ADQ ) until the merger of that party into the Coalition Avenir Québec ( CAQ ) in 2012 .", "prompt_labels": "She(O) previously(O) represented(O) the(O) riding(O) of(O) Lotbinière(B-location) from(O) 2003(B-election) Quebec(I-election) general(I-election) election(I-election) until(O) 2012(O) ,(O) initially(O) as(O) a(O) member(O) of(O) the(O) now-defunct(O) Action(B-political party) démocratique(I-political party) du(I-political party) Québec(I-political party) ((O) ADQ(B-political party) )(O) until(O) the(O) merger(O) of(O) that(O) party(O) into(O) the(O) Coalition(B-political party) Avenir(I-political party) Québec(I-political party) ((O) CAQ(B-political party) )(O) in(O) 2012(O) .(O)"}}
{"id": "71", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "country", "person", "location", "politician", "organization", "election", "political party"], "instance": {"id": "71", "words": ["The", "Freedom", "Party", "was", "subsequently", "expelled", "from", "the", "Liberal", "International", ",", "and", "the", "remaining", "liberals", "seceded", "to", "found", "the", "Liberal", "Forum", "(", "Liberales", "Forum", ",", "member", "Liberal", "International", ",", "Alliance", "of", "Liberals", "and", "Democrats", "for", "Europe", "Party", ")", "in", "1993", "."], "labels": ["O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "B-organization", "I-organization", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, person, location, politician, organization, election, political party and O.\nSentence: The Freedom Party was subsequently expelled from the Liberal International , and the remaining liberals seceded to found the Liberal Forum ( Liberales Forum , member Liberal International , Alliance of Liberals and Democrats for Europe Party ) in 1993 .", "prompt_labels": "The(O) Freedom(B-political party) Party(I-political party) was(O) subsequently(O) expelled(O) from(O) the(O) Liberal(B-organization) International(I-organization) ,(O) and(O) the(O) remaining(O) liberals(O) seceded(O) to(O) found(O) the(O) Liberal(B-political party) Forum(I-political party) ((O) Liberales(B-political party) Forum(I-political party) ,(O) member(O) Liberal(B-organization) International(I-organization) ,(O) Alliance(B-political party) of(I-political party) Liberals(I-political party) and(I-political party) Democrats(I-political party) for(I-political party) Europe(I-political party) Party(I-political party) )(O) in(O) 1993(O) .(O)"}}
{"id": "348", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "election", "event", "person", "political party", "organization", "politician", "country"], "instance": {"id": "348", "words": ["Azerbaijani", "nationalism", ",", "the", "ruling", "New", "Azerbaijan", "Party", "as", "well", "as", "the", "main", "opposition", "parties", "Musavat", "and", "Azerbaijani", "Popular", "Front", "Party", "doesn", "'t", "see", "Azizbekov", "as", "a", "positive", "figure", "."], "labels": ["O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, election, event, person, political party, organization, politician, country and O.\nSentence: Azerbaijani nationalism , the ruling New Azerbaijan Party as well as the main opposition parties Musavat and Azerbaijani Popular Front Party doesn 't see Azizbekov as a positive figure .", "prompt_labels": "Azerbaijani(O) nationalism(O) ,(O) the(O) ruling(O) New(B-political party) Azerbaijan(I-political party) Party(I-political party) as(O) well(O) as(O) the(O) main(O) opposition(O) parties(O) Musavat(B-political party) and(O) Azerbaijani(B-political party) Popular(I-political party) Front(I-political party) Party(I-political party) doesn(O) 't(O) see(O) Azizbekov(B-politician) as(O) a(O) positive(O) figure(O) .(O)"}}
{"id": "326", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "organization", "event", "political party", "country", "politician", "person", "election"], "instance": {"id": "326", "words": ["He", "was", "elected", "as", "a", "Fianna", "Fáil", "Teachta", "Dála", "(", "TD", ")", "for", "the", "constituency", "of", "Dublin", "North-West", "at", "the", "1997", "Irish", "general", "election", ",", "defeating", "the", "sitting", "Fine", "Gael", "TD", "Mary", "Flaherty", "to", "win", "a", "second", "seat", "for", "the", "Fianna", "Fáil", "in", "the", "4-seater", "constituency", "."], "labels": ["O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, event, political party, country, politician, person, election and O.\nSentence: He was elected as a Fianna Fáil Teachta Dála ( TD ) for the constituency of Dublin North-West at the 1997 Irish general election , defeating the sitting Fine Gael TD Mary Flaherty to win a second seat for the Fianna Fáil in the 4-seater constituency .", "prompt_labels": "He(O) was(O) elected(O) as(O) a(O) Fianna(B-political party) Fáil(I-political party) Teachta(I-political party) Dála(I-political party) ((O) TD(B-political party) )(O) for(O) the(O) constituency(O) of(O) Dublin(B-location) North-West(I-location) at(O) the(O) 1997(B-election) Irish(I-election) general(I-election) election(I-election) ,(O) defeating(O) the(O) sitting(O) Fine(B-political party) Gael(I-political party) TD(I-political party) Mary(B-politician) Flaherty(I-politician) to(O) win(O) a(O) second(O) seat(O) for(O) the(O) Fianna(B-political party) Fáil(I-political party) in(O) the(O) 4-seater(O) constituency(O) .(O)"}}
{"id": "141", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "organization", "political party", "event", "politician", "location", "person", "election"], "instance": {"id": "141", "words": ["During", "her", "visit", "she", "met", "representatives", "from", "the", "National", "Center", "for", "Transgender", "Equality", ",", "the", "National", "Association", "of", "LGBT", "Community", "Centers", ",", "the", "National", "Gay", "and", "Lesbian", "Task", "Force", ",", "Freedom", "to", "Marry", "and", "the", "Stonewall", "Democrats", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, political party, event, politician, location, person, election and O.\nSentence: During her visit she met representatives from the National Center for Transgender Equality , the National Association of LGBT Community Centers , the National Gay and Lesbian Task Force , Freedom to Marry and the Stonewall Democrats .", "prompt_labels": "During(O) her(O) visit(O) she(O) met(O) representatives(O) from(O) the(O) National(B-organization) Center(I-organization) for(I-organization) Transgender(I-organization) Equality(I-organization) ,(O) the(O) National(B-organization) Association(I-organization) of(I-organization) LGBT(I-organization) Community(I-organization) Centers(I-organization) ,(O) the(O) National(B-organization) Gay(I-organization) and(I-organization) Lesbian(I-organization) Task(I-organization) Force(I-organization) ,(O) Freedom(B-organization) to(I-organization) Marry(I-organization) and(O) the(O) Stonewall(B-organization) Democrats(I-organization) .(O)"}}
{"id": "182", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "politician", "political party", "country", "event", "election", "organization", "location"], "instance": {"id": "182", "words": ["President", "John", "Adams", ",", "a", "Federalist", "Party", "elected", "two", "years", "prior", "in", "the", "1796", "United", "States", "presidential", "election", ",", "remained", "popular", "during", "a", "time", "of", "national", "economic", "growth", ",", "and", "the", "Federalists", "made", "a", "modest", "gain", "of", "three", "seats", "at", "the", "expense", "of", "the", "opposition", "Democratic-Republican", "Party", ",", "the", "party", "of", "Vice", "President", "and", "future", "President", "Thomas", "Jefferson", "."], "labels": ["O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, political party, country, event, election, organization, location and O.\nSentence: President John Adams , a Federalist Party elected two years prior in the 1796 United States presidential election , remained popular during a time of national economic growth , and the Federalists made a modest gain of three seats at the expense of the opposition Democratic-Republican Party , the party of Vice President and future President Thomas Jefferson .", "prompt_labels": "President(O) John(B-politician) Adams(I-politician) ,(O) a(O) Federalist(B-political party) Party(I-political party) elected(O) two(O) years(O) prior(O) in(O) the(O) 1796(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) remained(O) popular(O) during(O) a(O) time(O) of(O) national(O) economic(O) growth(O) ,(O) and(O) the(O) Federalists(O) made(O) a(O) modest(O) gain(O) of(O) three(O) seats(O) at(O) the(O) expense(O) of(O) the(O) opposition(O) Democratic-Republican(B-political party) Party(I-political party) ,(O) the(O) party(O) of(O) Vice(O) President(O) and(O) future(O) President(O) Thomas(B-politician) Jefferson(I-politician) .(O)"}}
{"id": "388", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "organization", "politician", "event", "election", "location", "person", "political party"], "instance": {"id": "388", "words": ["Democrat", "Ron", "Wyden", "won", "a", "1996", "United", "States", "Senate", "special", "election", "in", "Oregon", "to", "replace", "Republican", "Bob", "Packwood", ",", "leaving", "the", "balance", "at", "53-47", "before", "the", "next", "1996", "United", "States", "Senate", "elections", "."], "labels": ["O", "B-politician", "I-politician", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, politician, event, election, location, person, political party and O.\nSentence: Democrat Ron Wyden won a 1996 United States Senate special election in Oregon to replace Republican Bob Packwood , leaving the balance at 53-47 before the next 1996 United States Senate elections .", "prompt_labels": "Democrat(O) Ron(B-politician) Wyden(I-politician) won(O) a(O) 1996(B-election) United(I-election) States(I-election) Senate(I-election) special(I-election) election(I-election) in(I-election) Oregon(I-election) to(O) replace(O) Republican(O) Bob(B-politician) Packwood(I-politician) ,(O) leaving(O) the(O) balance(O) at(O) 53-47(O) before(O) the(O) next(O) 1996(B-election) United(I-election) States(I-election) Senate(I-election) elections(I-election) .(O)"}}
{"id": "390", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "organization", "location", "person", "country", "event", "political party", "politician"], "instance": {"id": "390", "words": ["Geraldine", "Ferraro", ",", "former", "U.S.", "Representative", "and", "nominee", "for", "Vice", "President", "in", "1984", "United", "States", "presidential", "election", ",", "was", "well", "known", "for", "having", "been", "the", "1984", "United", "States", "presidential", "election", "and", "had", "also", "run", "but", "lost", "in", "the", "Democratic", "primary", "in", "the", "1992", "United", "States", "Senate", "election", "in", "New", "York", "."], "labels": ["B-politician", "I-politician", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, location, person, country, event, political party, politician and O.\nSentence: Geraldine Ferraro , former U.S. Representative and nominee for Vice President in 1984 United States presidential election , was well known for having been the 1984 United States presidential election and had also run but lost in the Democratic primary in the 1992 United States Senate election in New York .", "prompt_labels": "Geraldine(B-politician) Ferraro(I-politician) ,(O) former(O) U.S.(B-country) Representative(O) and(O) nominee(O) for(O) Vice(O) President(O) in(O) 1984(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) was(O) well(O) known(O) for(O) having(O) been(O) the(O) 1984(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) had(O) also(O) run(O) but(O) lost(O) in(O) the(O) Democratic(O) primary(O) in(O) the(O) 1992(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) New(I-election) York(I-election) .(O)"}}
{"id": "327", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "organization", "political party", "location", "country", "person", "politician", "event"], "instance": {"id": "327", "words": ["Davis", "was", "the", "last", "Democrat", "until", "Jerry", "Brown", "in", "2010", "California", "gubernatorial", "election", "to", "carry", "Alpine", ",", "Del", "Norte", ",", "Sacramento", ",", "San", "Joaquin", ",", "and", "Santa", "Barbara", "Counties", ",", "the", "last", "until", "Jerry", "Brown", "in", "2014", "California", "gubernatorial", "election", "to", "carry", "Merced", ",", "Mono", ",", "San", "Diego", ",", "San", "Luis", "Obispo", ",", "Stanislaus", ",", "and", "Ventura", "Counties", ",", "and", "the", "last", "until", "Gavin", "Newsom", "in", "2018", "California", "gubernatorial", "election", "to", "carry", "San", "Bernardino", "County", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-location", "O", "B-location", "I-location", "O", "B-location", "O", "B-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "B-location", "I-location", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-location", "O", "B-location", "O", "B-location", "I-location", "O", "B-location", "I-location", "I-location", "O", "B-location", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, political party, location, country, person, politician, event and O.\nSentence: Davis was the last Democrat until Jerry Brown in 2010 California gubernatorial election to carry Alpine , Del Norte , Sacramento , San Joaquin , and Santa Barbara Counties , the last until Jerry Brown in 2014 California gubernatorial election to carry Merced , Mono , San Diego , San Luis Obispo , Stanislaus , and Ventura Counties , and the last until Gavin Newsom in 2018 California gubernatorial election to carry San Bernardino County .", "prompt_labels": "Davis(B-politician) was(O) the(O) last(O) Democrat(O) until(O) Jerry(B-politician) Brown(I-politician) in(O) 2010(B-election) California(I-election) gubernatorial(I-election) election(I-election) to(O) carry(O) Alpine(B-location) ,(O) Del(B-location) Norte(I-location) ,(O) Sacramento(B-location) ,(O) San(B-location) Joaquin(I-location) ,(O) and(O) Santa(B-location) Barbara(I-location) Counties(I-location) ,(O) the(O) last(O) until(O) Jerry(B-location) Brown(I-location) in(O) 2014(B-election) California(I-election) gubernatorial(I-election) election(I-election) to(O) carry(O) Merced(B-location) ,(O) Mono(B-location) ,(O) San(B-location) Diego(I-location) ,(O) San(B-location) Luis(I-location) Obispo(I-location) ,(O) Stanislaus(B-location) ,(O) and(O) Ventura(B-location) Counties(I-location) ,(O) and(O) the(O) last(O) until(O) Gavin(B-politician) Newsom(I-politician) in(O) 2018(B-election) California(I-election) gubernatorial(I-election) election(I-election) to(O) carry(O) San(B-location) Bernardino(I-location) County(I-location) .(O)"}}
{"id": "111", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "person", "politician", "country", "political party", "event", "election", "organization"], "instance": {"id": "111", "words": ["Mazowiecki", "remained", "a", "member", "of", "the", "Sejm", "until", "1971", ",", "serving", "his", "1965", "Polish", "legislative", "election", ",", "1969", "Polish", "legislative", "election", "and", "1972", "Polish", "legislative", "election", "as", "a", "member", "of", "the", "Catholic", "party", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, politician, country, political party, event, election, organization and O.\nSentence: Mazowiecki remained a member of the Sejm until 1971 , serving his 1965 Polish legislative election , 1969 Polish legislative election and 1972 Polish legislative election as a member of the Catholic party .", "prompt_labels": "Mazowiecki(B-politician) remained(O) a(O) member(O) of(O) the(O) Sejm(B-organization) until(O) 1971(O) ,(O) serving(O) his(O) 1965(B-election) Polish(I-election) legislative(I-election) election(I-election) ,(O) 1969(B-election) Polish(I-election) legislative(I-election) election(I-election) and(O) 1972(B-election) Polish(I-election) legislative(I-election) election(I-election) as(O) a(O) member(O) of(O) the(O) Catholic(B-political party) party(I-political party) .(O)"}}
{"id": "396", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "organization", "location", "country", "event", "person", "politician", "political party"], "instance": {"id": "396", "words": ["This", "was", "the", "second", "Senatorial", "race", "for", "Gordon", "Smith", "in", "1996", ";", "he", "had", "previously", "lost", "to", "Ron", "Wyden", "in", "the", "1996", "United", "States", "Senate", "special", "election", "in", "Oregon", "to", "fill", "Bob", "Packwood", "'", "s", "seat", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, location, country, event, person, politician, political party and O.\nSentence: This was the second Senatorial race for Gordon Smith in 1996 ; he had previously lost to Ron Wyden in the 1996 United States Senate special election in Oregon to fill Bob Packwood ' s seat .", "prompt_labels": "This(O) was(O) the(O) second(O) Senatorial(O) race(O) for(O) Gordon(B-politician) Smith(I-politician) in(O) 1996(O) ;(O) he(O) had(O) previously(O) lost(O) to(O) Ron(B-politician) Wyden(I-politician) in(O) the(O) 1996(B-election) United(I-election) States(I-election) Senate(I-election) special(I-election) election(I-election) in(I-election) Oregon(I-election) to(O) fill(O) Bob(B-politician) Packwood(I-politician) '(O) s(O) seat(O) .(O)"}}
{"id": "330", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "event", "location", "politician", "person", "country", "election", "organization"], "instance": {"id": "330", "words": ["The", "Privy", "Council", "as", "New", "Zealand", "'s", "highest", "court", "of", "appeal", "was", "replaced", "by", "the", "Supreme", "Court", "of", "New", "Zealand", "by", "a", "simple", "Act", "of", "Parliament", "despite", "calls", "from", "New", "Zealand", "First", ",", "New", "Zealand", "National", "Party", "and", "ACT", "New", "Zealand", "for", "a", "referendum", "to", "be", "called", "on", "the", "issue", "."], "labels": ["O", "B-organization", "I-organization", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, location, politician, person, country, election, organization and O.\nSentence: The Privy Council as New Zealand 's highest court of appeal was replaced by the Supreme Court of New Zealand by a simple Act of Parliament despite calls from New Zealand First , New Zealand National Party and ACT New Zealand for a referendum to be called on the issue .", "prompt_labels": "The(O) Privy(B-organization) Council(I-organization) as(O) New(B-country) Zealand(I-country) 's(O) highest(O) court(O) of(O) appeal(O) was(O) replaced(O) by(O) the(O) Supreme(B-organization) Court(I-organization) of(I-organization) New(I-organization) Zealand(I-organization) by(O) a(O) simple(O) Act(O) of(O) Parliament(O) despite(O) calls(O) from(O) New(B-political party) Zealand(I-political party) First(I-political party) ,(O) New(B-political party) Zealand(I-political party) National(I-political party) Party(I-political party) and(O) ACT(B-political party) New(I-political party) Zealand(I-political party) for(O) a(O) referendum(O) to(O) be(O) called(O) on(O) the(O) issue(O) .(O)"}}
{"id": "401", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "location", "country", "politician", "election", "political party", "person", "event"], "instance": {"id": "401", "words": ["During", "the", "1988", "United", "States", "presidential", "election", ",", "Republican", "nominee", "Vice", "President", "George", "H.", "W.", "Bush", "selected", "U.S.", "Senator", "Dan", "Quayle", "of", "Indiana", "as", "his", "vice", "presidential", "nominee", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-politician", "I-politician", "I-politician", "I-politician", "O", "B-country", "O", "B-politician", "I-politician", "O", "B-location", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, country, politician, election, political party, person, event and O.\nSentence: During the 1988 United States presidential election , Republican nominee Vice President George H. W. Bush selected U.S. Senator Dan Quayle of Indiana as his vice presidential nominee .", "prompt_labels": "During(O) the(O) 1988(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) Republican(O) nominee(O) Vice(O) President(O) George(B-politician) H.(I-politician) W.(I-politician) Bush(I-politician) selected(O) U.S.(B-country) Senator(O) Dan(B-politician) Quayle(I-politician) of(O) Indiana(B-location) as(O) his(O) vice(O) presidential(O) nominee(O) .(O)"}}
{"id": "31", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "politician", "election", "event", "person", "organization", "country", "political party"], "instance": {"id": "31", "words": ["Originally", "a", "member", "of", "the", "Centre", "of", "Social", "Democrats", "(", "CDS", ")", ",", "the", "Christian", "Democrat", "component", "of", "the", "Union", "for", "French", "Democracy", "(", "UDF", ")", "party", ",", "he", "later", "joined", "the", "Union", "for", "a", "Popular", "Movement", "."], "labels": ["O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, election, event, person, organization, country, political party and O.\nSentence: Originally a member of the Centre of Social Democrats ( CDS ) , the Christian Democrat component of the Union for French Democracy ( UDF ) party , he later joined the Union for a Popular Movement .", "prompt_labels": "Originally(O) a(O) member(O) of(O) the(O) Centre(B-political party) of(I-political party) Social(I-political party) Democrats(I-political party) ((O) CDS(B-political party) )(O) ,(O) the(O) Christian(O) Democrat(O) component(O) of(O) the(O) Union(B-political party) for(I-political party) French(I-political party) Democracy(I-political party) ((O) UDF(B-political party) )(O) party(O) ,(O) he(O) later(O) joined(O) the(O) Union(B-political party) for(I-political party) a(I-political party) Popular(I-political party) Movement(I-political party) .(O)"}}
{"id": "121", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "country", "political party", "person", "politician", "election", "event", "location"], "instance": {"id": "121", "words": ["It", "is", "a", "founding", "member", "of", "ASEAN", ",", "EAS", ",", "Organisation", "of", "Islamic", "Cooperation", "and", "a", "member", "of", "Asia-Pacific", "Economic", "Cooperation", ",", "the", "Commonwealth", "of", "Nations", "and", "the", "Non-Aligned", "Movement", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "O", "B-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, political party, person, politician, election, event, location and O.\nSentence: It is a founding member of ASEAN , EAS , Organisation of Islamic Cooperation and a member of Asia-Pacific Economic Cooperation , the Commonwealth of Nations and the Non-Aligned Movement .", "prompt_labels": "It(O) is(O) a(O) founding(O) member(O) of(O) ASEAN(B-organization) ,(O) EAS(B-organization) ,(O) Organisation(B-organization) of(I-organization) Islamic(I-organization) Cooperation(I-organization) and(O) a(O) member(O) of(O) Asia-Pacific(B-organization) Economic(I-organization) Cooperation(I-organization) ,(O) the(O) Commonwealth(B-organization) of(I-organization) Nations(I-organization) and(O) the(O) Non-Aligned(B-organization) Movement(I-organization) .(O)"}}
{"id": "153", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "election", "event", "location", "country", "person", "political party", "organization"], "instance": {"id": "153", "words": ["On", "August", "3", ",", "2012", "the", "party", "(", "which", "had", "no", "member", "in", "the", "lower", "house", ")", "in", "concert", "with", "six", "other", "minor", "opposition", "parties", "(", "People", "'s", "Life", "First", ",", "Kizuna", "Party", ",", "Social", "Democratic", "Party", ",", "Your", "Party", ",", "Japanese", "Communist", "Party", "and", "the", "New", "Renaissance", "Party", ")", "agreed", "to", "submit", "a", "no", "confidence", "motion", "against", "Prime", "Minister", "Yoshihiko", "Noda", "in", "an", "effort", "to", "block", "the", "passage", "of", "the", "bill", "raising", "Japan", "'s", "consumption", "tax", "from", "5", "%", "to", "10", "%", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, event, location, country, person, political party, organization and O.\nSentence: On August 3 , 2012 the party ( which had no member in the lower house ) in concert with six other minor opposition parties ( People 's Life First , Kizuna Party , Social Democratic Party , Your Party , Japanese Communist Party and the New Renaissance Party ) agreed to submit a no confidence motion against Prime Minister Yoshihiko Noda in an effort to block the passage of the bill raising Japan 's consumption tax from 5 % to 10 % .", "prompt_labels": "On(O) August(O) 3(O) ,(O) 2012(O) the(O) party(O) ((O) which(O) had(O) no(O) member(O) in(O) the(O) lower(O) house(O) )(O) in(O) concert(O) with(O) six(O) other(O) minor(O) opposition(O) parties(O) ((O) People(B-political party) 's(I-political party) Life(I-political party) First(I-political party) ,(O) Kizuna(B-political party) Party(I-political party) ,(O) Social(B-political party) Democratic(I-political party) Party(I-political party) ,(O) Your(B-political party) Party(I-political party) ,(O) Japanese(B-political party) Communist(I-political party) Party(I-political party) and(O) the(O) New(B-political party) Renaissance(I-political party) Party(I-political party) )(O) agreed(O) to(O) submit(O) a(O) no(O) confidence(O) motion(O) against(O) Prime(O) Minister(O) Yoshihiko(B-politician) Noda(I-politician) in(O) an(O) effort(O) to(O) block(O) the(O) passage(O) of(O) the(O) bill(O) raising(O) Japan(B-country) 's(O) consumption(O) tax(O) from(O) 5(O) %(O) to(O) 10(O) %(O) .(O)"}}
{"id": "282", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "country", "organization", "politician", "election", "person", "event", "political party"], "instance": {"id": "282", "words": ["In", "1984", "he", "was", "candidate", "for", "the", "European", "Parliament", "for", "the", "Green", "Progressive", "Accord", ",", "a", "combined", "list", "of", "Political", "Party", "of", "Radicals", ",", "Communist", "Party", "of", "the", "Netherlands", "(", "CPN", ")", "and", "the", "Pacifist", "Socialist", "Party", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, organization, politician, election, person, event, political party and O.\nSentence: In 1984 he was candidate for the European Parliament for the Green Progressive Accord , a combined list of Political Party of Radicals , Communist Party of the Netherlands ( CPN ) and the Pacifist Socialist Party .", "prompt_labels": "In(O) 1984(O) he(O) was(O) candidate(O) for(O) the(O) European(B-organization) Parliament(I-organization) for(O) the(O) Green(B-political party) Progressive(I-political party) Accord(I-political party) ,(O) a(O) combined(O) list(O) of(O) Political(B-political party) Party(I-political party) of(I-political party) Radicals(I-political party) ,(O) Communist(B-political party) Party(I-political party) of(I-political party) the(I-political party) Netherlands(I-political party) ((O) CPN(B-political party) )(O) and(O) the(O) Pacifist(B-political party) Socialist(I-political party) Party(I-political party) .(O)"}}
{"id": "514", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "person", "political party", "election", "organization", "country", "politician", "location"], "instance": {"id": "514", "words": ["The", "historian", "Jogesh", "Chandra", "Bagal", "describes", "the", "revolt", "as", "a", "non-violent", "revolution", "and", "gives", "this", "as", "a", "reason", "why", "the", "indigo", "revolt", "was", "a", "success", "compared", "to", "the", "Sepoy", "Revolt", "."], "labels": ["O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, political party, election, organization, country, politician, location and O.\nSentence: The historian Jogesh Chandra Bagal describes the revolt as a non-violent revolution and gives this as a reason why the indigo revolt was a success compared to the Sepoy Revolt .", "prompt_labels": "The(O) historian(O) Jogesh(B-person) Chandra(I-person) Bagal(I-person) describes(O) the(O) revolt(O) as(O) a(O) non-violent(O) revolution(O) and(O) gives(O) this(O) as(O) a(O) reason(O) why(O) the(O) indigo(O) revolt(O) was(O) a(O) success(O) compared(O) to(O) the(O) Sepoy(B-event) Revolt(I-event) .(O)"}}
{"id": "9", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "person", "country", "politician", "election", "political party", "location", "organization"], "instance": {"id": "9", "words": ["He", "stood", "for", "the", "Green", "party", "in", "Oxford", "West", "and", "Abingdon", "in", "the", "1992", "United", "Kingdom", "general", "election", ",", "1997", "United", "Kingdom", "general", "election", ",", "and", "2001", "United", "Kingdom", "general", "election", "general", "elections", "."], "labels": ["O", "O", "O", "O", "B-political party", "I-political party", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, country, politician, election, political party, location, organization and O.\nSentence: He stood for the Green party in Oxford West and Abingdon in the 1992 United Kingdom general election , 1997 United Kingdom general election , and 2001 United Kingdom general election general elections .", "prompt_labels": "He(O) stood(O) for(O) the(O) Green(B-political party) party(I-political party) in(O) Oxford(B-organization) West(I-organization) and(I-organization) Abingdon(I-organization) in(O) the(O) 1992(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 1997(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) and(O) 2001(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) general(O) elections(O) .(O)"}}
{"id": "206", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "political party", "location", "election", "person", "organization", "country", "politician"], "instance": {"id": "206", "words": ["He", "ran", "for", "President", "of", "Russia", "in", "1991", "Russian", "presidential", "election", ",", "1996", "Russian", "presidential", "election", "(", "withdrawing", "during", "the", "campaign", ")", "and", "2000", "Russian", "presidential", "election", ",", "coming", "fourth", "in", "both", "1991", "and", "2000", "."], "labels": ["O", "O", "O", "O", "O", "B-country", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, location, election, person, organization, country, politician and O.\nSentence: He ran for President of Russia in 1991 Russian presidential election , 1996 Russian presidential election ( withdrawing during the campaign ) and 2000 Russian presidential election , coming fourth in both 1991 and 2000 .", "prompt_labels": "He(O) ran(O) for(O) President(O) of(O) Russia(B-country) in(O) 1991(B-election) Russian(I-election) presidential(I-election) election(I-election) ,(O) 1996(B-election) Russian(I-election) presidential(I-election) election(I-election) ((O) withdrawing(O) during(O) the(O) campaign(O) )(O) and(O) 2000(B-election) Russian(I-election) presidential(I-election) election(I-election) ,(O) coming(O) fourth(O) in(O) both(O) 1991(O) and(O) 2000(O) .(O)"}}
{"id": "77", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "country", "organization", "person", "location", "political party", "election", "event"], "instance": {"id": "77", "words": ["In", "recent", "years", ",", "the", "Progressive", "Conservative", "Party", "of", "Canada", "has", "had", "the", "most", "success", "in", "the", "city", ":", "its", "members", "were", "elected", "in", "all", "but", "four", "elections", "since", "1953", ":", "1974", "Canadian", "federal", "election", ",", "1980", "Canadian", "federal", "election", ",", "2004", "Canadian", "federal", "election", ",", "and", "2006", "Canadian", "federal", "election", "."], "labels": ["O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, organization, person, location, political party, election, event and O.\nSentence: In recent years , the Progressive Conservative Party of Canada has had the most success in the city : its members were elected in all but four elections since 1953 : 1974 Canadian federal election , 1980 Canadian federal election , 2004 Canadian federal election , and 2006 Canadian federal election .", "prompt_labels": "In(O) recent(O) years(O) ,(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) has(O) had(O) the(O) most(O) success(O) in(O) the(O) city(O) :(O) its(O) members(O) were(O) elected(O) in(O) all(O) but(O) four(O) elections(O) since(O) 1953(O) :(O) 1974(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 1980(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) and(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)"}}
{"id": "165", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "event", "person", "politician", "election", "location", "country", "organization"], "instance": {"id": "165", "words": ["He", "was", "elected", "as", "a", "Social", "Credit", "MLA", "in", "Vancouver", "South", "in", "1975", "British", "Columbia", "general", "election", ",", "1979", "British", "Columbia", "general", "election", ",", "1983", "British", "Columbia", "general", "election", "and", "1986", "British", "Columbia", "general", "election", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "I-location", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, person, politician, election, location, country, organization and O.\nSentence: He was elected as a Social Credit MLA in Vancouver South in 1975 British Columbia general election , 1979 British Columbia general election , 1983 British Columbia general election and 1986 British Columbia general election .", "prompt_labels": "He(O) was(O) elected(O) as(O) a(O) Social(B-organization) Credit(I-organization) MLA(I-organization) in(O) Vancouver(B-location) South(I-location) in(O) 1975(B-election) British(I-election) Columbia(I-election) general(I-election) election(I-election) ,(O) 1979(B-election) British(I-election) Columbia(I-election) general(I-election) election(I-election) ,(O) 1983(B-election) British(I-election) Columbia(I-election) general(I-election) election(I-election) and(O) 1986(B-election) British(I-election) Columbia(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "275", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "location", "organization", "country", "politician", "event", "election", "political party"], "instance": {"id": "275", "words": ["MPs", "from", "the", "Democratic", "Unionist", "Party", ",", "Sinn", "Féin", ",", "Social", "Democratic", "and", "Labour", "Party", "or", "Ulster", "Unionist", "Party", ",", "including", "those", "Ulster", "Unionists", "who", "stood", "as", "part", "of", "the", "Conservative", "Party", ",", "are", "excluded", "."], "labels": ["O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, organization, country, politician, event, election, political party and O.\nSentence: MPs from the Democratic Unionist Party , Sinn Féin , Social Democratic and Labour Party or Ulster Unionist Party , including those Ulster Unionists who stood as part of the Conservative Party , are excluded .", "prompt_labels": "MPs(O) from(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) ,(O) Sinn(B-political party) Féin(I-political party) ,(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) or(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) ,(O) including(O) those(O) Ulster(O) Unionists(O) who(O) stood(O) as(O) part(O) of(O) the(O) Conservative(B-political party) Party(I-political party) ,(O) are(O) excluded(O) .(O)"}}
{"id": "267", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "country", "event", "organization", "election", "person", "location", "politician"], "instance": {"id": "267", "words": ["Archer", "volunteered", "for", "Progressive", "Conservative", "Party", "of", "Manitoba", "candidate", "Cecil", "Thorne", "for", "the", "1999", "Manitoba", "general", "election", "in", "the", "northern", "election", "division", "of", "Thompson", ",", "and", "was", "himself", "was", "the", "party", "'s", "candidate", "in", "2003", "Manitoba", "general", "election", "."], "labels": ["B-politician", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, event, organization, election, person, location, politician and O.\nSentence: Archer volunteered for Progressive Conservative Party of Manitoba candidate Cecil Thorne for the 1999 Manitoba general election in the northern election division of Thompson , and was himself was the party 's candidate in 2003 Manitoba general election .", "prompt_labels": "Archer(B-politician) volunteered(O) for(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Manitoba(I-political party) candidate(O) Cecil(B-politician) Thorne(I-politician) for(O) the(O) 1999(B-election) Manitoba(I-election) general(I-election) election(I-election) in(O) the(O) northern(O) election(O) division(O) of(O) Thompson(B-location) ,(O) and(O) was(O) himself(O) was(O) the(O) party(O) 's(O) candidate(O) in(O) 2003(B-election) Manitoba(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "359", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "politician", "event", "country", "political party", "organization", "person", "location"], "instance": {"id": "359", "words": ["Tassi", "'s", "first", "run", "for", "elective", "office", "was", "as", "a", "candidate", "for", "the", "Ontario", "Liberal", "Party", "in", "the", "1995", "Ontario", "general", "election", ",", "where", "she", "finished", "a", "narrow", "second", "to", "Ontario", "New", "Democratic", "Party", "incumbent", "David", "Christopherson", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, event, country, political party, organization, person, location and O.\nSentence: Tassi 's first run for elective office was as a candidate for the Ontario Liberal Party in the 1995 Ontario general election , where she finished a narrow second to Ontario New Democratic Party incumbent David Christopherson .", "prompt_labels": "Tassi(B-politician) 's(O) first(O) run(O) for(O) elective(O) office(O) was(O) as(O) a(O) candidate(O) for(O) the(O) Ontario(B-political party) Liberal(I-political party) Party(I-political party) in(O) the(O) 1995(B-election) Ontario(I-election) general(I-election) election(I-election) ,(O) where(O) she(O) finished(O) a(O) narrow(O) second(O) to(O) Ontario(B-political party) New(I-political party) Democratic(I-political party) Party(I-political party) incumbent(O) David(B-politician) Christopherson(I-politician) .(O)"}}
{"id": "220", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "organization", "event", "political party", "person", "country", "election", "location"], "instance": {"id": "220", "words": ["The", "Harrisons", "are", "among", "four", "families", "to", "have", "two", "presidents", "in", "their", "number", "with", "the", "same", "surname", ";", "the", "others", "are", "the", "Adams", "political", "family", ",", "Roosevelt", "family", ",", "and", "Bush", "family", "families", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, event, political party, person, country, election, location and O.\nSentence: The Harrisons are among four families to have two presidents in their number with the same surname ; the others are the Adams political family , Roosevelt family , and Bush family families .", "prompt_labels": "The(O) Harrisons(O) are(O) among(O) four(O) families(O) to(O) have(O) two(O) presidents(O) in(O) their(O) number(O) with(O) the(O) same(O) surname(O) ;(O) the(O) others(O) are(O) the(O) Adams(B-organization) political(I-organization) family(I-organization) ,(O) Roosevelt(B-organization) family(I-organization) ,(O) and(O) Bush(B-organization) family(I-organization) families(O) .(O)"}}
{"id": "462", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "politician", "person", "political party", "country", "event", "election", "location"], "instance": {"id": "462", "words": ["The", "audiobook", "version", "features", "Jim", "Parsons", ",", "Jesse", "Tyler", "Ferguson", ",", "Jeff", "Garlin", ",", "Ellie", "Kemper", ",", "John", "Lithgow", ",", "Jack", "McBrayer", ",", "and", "RuPaul", "."], "labels": ["O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, person, political party, country, event, election, location and O.\nSentence: The audiobook version features Jim Parsons , Jesse Tyler Ferguson , Jeff Garlin , Ellie Kemper , John Lithgow , Jack McBrayer , and RuPaul .", "prompt_labels": "The(O) audiobook(O) version(O) features(O) Jim(B-person) Parsons(I-person) ,(O) Jesse(B-person) Tyler(I-person) Ferguson(I-person) ,(O) Jeff(B-person) Garlin(I-person) ,(O) Ellie(B-person) Kemper(I-person) ,(O) John(B-person) Lithgow(I-person) ,(O) Jack(B-person) McBrayer(I-person) ,(O) and(O) RuPaul(B-person) .(O)"}}
{"id": "532", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "location", "organization", "event", "political party", "election", "politician", "country"], "instance": {"id": "532", "words": ["David", "Hackett", "Fischer", ",", "The", "Great", "Wave", ",", "OUP", "1996", ",", "p240", "But", "as", "of", "2017", ",", "the", "implied", "crisis", "he", "predicts", "has", "yet", "to", "happen", ";", "recent", "works", "by", "Steven", "Pinker", "The", "better", "angels", "of", "our", "nature", ",", "2011", ",", "chapters", "2-7", "and", "Joshua", "S", "Goldstein", "Joshua", "S", "Goldstein", "and", "Steven", "Pinker", ",", "'", "war", "really", "is", "going", "out", "of", "style", ",", "New", "York", "Times", ",", "17", "/", "12", "/", "2011", "suggest", "that", "war", "and", "violence", "are", "decreasing", "."], "labels": ["B-person", "I-person", "I-person", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "I-politician", "B-politician", "I-politician", "I-politician", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, organization, event, political party, election, politician, country and O.\nSentence: David Hackett Fischer , The Great Wave , OUP 1996 , p240 But as of 2017 , the implied crisis he predicts has yet to happen ; recent works by Steven Pinker The better angels of our nature , 2011 , chapters 2-7 and Joshua S Goldstein Joshua S Goldstein and Steven Pinker , ' war really is going out of style , New York Times , 17 / 12 / 2011 suggest that war and violence are decreasing .", "prompt_labels": "David(B-person) Hackett(I-person) Fischer(I-person) ,(O) The(O) Great(O) Wave(O) ,(O) OUP(B-organization) 1996(O) ,(O) p240(O) But(O) as(O) of(O) 2017(O) ,(O) the(O) implied(O) crisis(O) he(O) predicts(O) has(O) yet(O) to(O) happen(O) ;(O) recent(O) works(O) by(O) Steven(B-person) Pinker(I-person) The(O) better(O) angels(O) of(O) our(O) nature(O) ,(O) 2011(O) ,(O) chapters(O) 2-7(O) and(O) Joshua(B-politician) S(I-politician) Goldstein(I-politician) Joshua(B-politician) S(I-politician) Goldstein(I-politician) and(O) Steven(B-person) Pinker(I-person) ,(O) '(O) war(O) really(O) is(O) going(O) out(O) of(O) style(O) ,(O) New(B-organization) York(I-organization) Times(I-organization) ,(O) 17(O) /(O) 12(O) /(O) 2011(O) suggest(O) that(O) war(O) and(O) violence(O) are(O) decreasing(O) .(O)"}}
{"id": "228", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "country", "person", "event", "election", "political party", "politician", "location"], "instance": {"id": "228", "words": ["She", "wrote", "that", "NGOs", "like", "the", "World", "Wide", "Fund", "for", "Nature", ",", "Conservation", "International", "and", "Germanwatch", "had", "provided", "comments", "based", "on", "grey", "literature", ",", "to", "push", "a", "political", "agenda", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, person, event, election, political party, politician, location and O.\nSentence: She wrote that NGOs like the World Wide Fund for Nature , Conservation International and Germanwatch had provided comments based on grey literature , to push a political agenda .", "prompt_labels": "She(O) wrote(O) that(O) NGOs(O) like(O) the(O) World(B-organization) Wide(I-organization) Fund(I-organization) for(I-organization) Nature(I-organization) ,(O) Conservation(B-organization) International(I-organization) and(O) Germanwatch(B-organization) had(O) provided(O) comments(O) based(O) on(O) grey(O) literature(O) ,(O) to(O) push(O) a(O) political(O) agenda(O) .(O)"}}
{"id": "238", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "event", "country", "location", "organization", "person", "political party", "politician"], "instance": {"id": "238", "words": ["Barnett", "won", "re-election", "again", "in", "the", "1962", "Canadian", "federal", "election", ",", "becoming", "an", "NDP", "Member", "of", "Parliament", ",", "and", "was", "re-elected", "until", "his", "defeat", "in", "the", "1968", "Canadian", "federal", "election", "when", "he", "was", "defeated", "by", "Liberal", "Party", "of", "Canada", "candidate", "Richard", "Durante", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, country, location, organization, person, political party, politician and O.\nSentence: Barnett won re-election again in the 1962 Canadian federal election , becoming an NDP Member of Parliament , and was re-elected until his defeat in the 1968 Canadian federal election when he was defeated by Liberal Party of Canada candidate Richard Durante .", "prompt_labels": "Barnett(B-politician) won(O) re-election(O) again(O) in(O) the(O) 1962(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) becoming(O) an(O) NDP(B-political party) Member(O) of(O) Parliament(O) ,(O) and(O) was(O) re-elected(O) until(O) his(O) defeat(O) in(O) the(O) 1968(B-election) Canadian(I-election) federal(I-election) election(I-election) when(O) he(O) was(O) defeated(O) by(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) candidate(O) Richard(B-politician) Durante(I-politician) .(O)"}}
{"id": "385", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "election", "political party", "organization", "event", "country", "politician", "location"], "instance": {"id": "385", "words": ["During", "the", "First", "Hellenic", "Republic", "(", "1828-1832", ")", "and", "the", "reign", "of", "King", "Otto", "(", "1833-1863", ")", ",", "the", "political", "parties", "were", "essentially", "based", "on", "clientage", "of", "the", "Great", "Powers", ":", "the", "Russian", "Party", ",", "the", "English", "Party", ",", "and", "the", "French", "Party", "."], "labels": ["O", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, political party, organization, event, country, politician, location and O.\nSentence: During the First Hellenic Republic ( 1828-1832 ) and the reign of King Otto ( 1833-1863 ) , the political parties were essentially based on clientage of the Great Powers : the Russian Party , the English Party , and the French Party .", "prompt_labels": "During(O) the(O) First(B-country) Hellenic(I-country) Republic(I-country) ((O) 1828-1832(O) )(O) and(O) the(O) reign(O) of(O) King(B-politician) Otto(I-politician) ((O) 1833-1863(O) )(O) ,(O) the(O) political(O) parties(O) were(O) essentially(O) based(O) on(O) clientage(O) of(O) the(O) Great(O) Powers(O) :(O) the(O) Russian(B-political party) Party(I-political party) ,(O) the(O) English(B-political party) Party(I-political party) ,(O) and(O) the(O) French(B-political party) Party(I-political party) .(O)"}}
{"id": "155", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "organization", "politician", "event", "location", "political party", "election", "country"], "instance": {"id": "155", "words": ["He", "was", "elected", "to", "the", "Parliament", "of", "Norway", "from", "Vest-Agder", "in", "1993", "Norwegian", "parliamentary", "election", ",", "and", "was", "re-elected", "on", "the", "two", "following", "occasions", "in", "1997", "Norwegian", "parliamentary", "election", "and", "2001", "Norwegian", "parliamentary", "election", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, politician, event, location, political party, election, country and O.\nSentence: He was elected to the Parliament of Norway from Vest-Agder in 1993 Norwegian parliamentary election , and was re-elected on the two following occasions in 1997 Norwegian parliamentary election and 2001 Norwegian parliamentary election .", "prompt_labels": "He(O) was(O) elected(O) to(O) the(O) Parliament(B-organization) of(I-organization) Norway(I-organization) from(O) Vest-Agder(B-location) in(O) 1993(B-election) Norwegian(I-election) parliamentary(I-election) election(I-election) ,(O) and(O) was(O) re-elected(O) on(O) the(O) two(O) following(O) occasions(O) in(O) 1997(B-election) Norwegian(I-election) parliamentary(I-election) election(I-election) and(O) 2001(B-election) Norwegian(I-election) parliamentary(I-election) election(I-election) .(O)"}}
{"id": "290", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "location", "country", "organization", "event", "politician", "political party", "election"], "instance": {"id": "290", "words": ["After", "the", "April", "2009", "Moldovan", "parliamentary", "election", ",", "the", "2009", "Moldova", "civil", "unrest", ",", "the", "July", "2009", "Moldovan", "parliamentary", "election", "and", "the", "creation", "of", "the", "governing", "Alliance", "for", "European", "Integration", ",", "a", "new", "wave", "of", "speculations", "about", "the", "union", "followed", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, country, organization, event, politician, political party, election and O.\nSentence: After the April 2009 Moldovan parliamentary election , the 2009 Moldova civil unrest , the July 2009 Moldovan parliamentary election and the creation of the governing Alliance for European Integration , a new wave of speculations about the union followed .", "prompt_labels": "After(O) the(O) April(B-election) 2009(I-election) Moldovan(I-election) parliamentary(I-election) election(I-election) ,(O) the(O) 2009(B-event) Moldova(I-event) civil(I-event) unrest(I-event) ,(O) the(O) July(B-election) 2009(I-election) Moldovan(I-election) parliamentary(I-election) election(I-election) and(O) the(O) creation(O) of(O) the(O) governing(O) Alliance(B-organization) for(I-organization) European(I-organization) Integration(I-organization) ,(O) a(O) new(O) wave(O) of(O) speculations(O) about(O) the(O) union(O) followed(O) .(O)"}}
{"id": "260", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "politician", "event", "person", "organization", "election", "country", "location"], "instance": {"id": "260", "words": ["The", "centre-right", "(", "see", "House", "of", "Freedoms", ",", "The", "People", "of", "Freedom", ",", "centre-right", "coalition", "and", "Forza", "Italia", ")", "has", "held", "primary", "elections", "only", "at", "the", "local", "level", "."], "labels": ["O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, event, person, organization, election, country, location and O.\nSentence: The centre-right ( see House of Freedoms , The People of Freedom , centre-right coalition and Forza Italia ) has held primary elections only at the local level .", "prompt_labels": "The(O) centre-right(O) ((O) see(O) House(B-political party) of(I-political party) Freedoms(I-political party) ,(O) The(B-political party) People(I-political party) of(I-political party) Freedom(I-political party) ,(O) centre-right(O) coalition(O) and(O) Forza(B-political party) Italia(I-political party) )(O) has(O) held(O) primary(O) elections(O) only(O) at(O) the(O) local(O) level(O) .(O)"}}
{"id": "299", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "election", "political party", "organization", "person", "location", "politician", "country"], "instance": {"id": "299", "words": ["Starting", "from", "2014", "New", "Zealand", "general", "election", "under", "the", "MMP", "electoral", "system", ",", "71", "of", "the", "usually", "120", "seats", "in", "Parliament", "were", "filled", "by", "electorate", "members", ",", "with", "the", "remainder", "being", "filled", "from", "party", "list", "s", "in", "order", "to", "achieve", "proportional", "representation", "(", "there", "were", "69", "electorates", "in", "2005", "New", "Zealand", "general", "election", ",", "and", "70", "electorates", "in", "the", "2008", "New", "Zealand", "general", "election", "and", "2011", "New", "Zealand", "general", "election", ")", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, political party, organization, person, location, politician, country and O.\nSentence: Starting from 2014 New Zealand general election under the MMP electoral system , 71 of the usually 120 seats in Parliament were filled by electorate members , with the remainder being filled from party list s in order to achieve proportional representation ( there were 69 electorates in 2005 New Zealand general election , and 70 electorates in the 2008 New Zealand general election and 2011 New Zealand general election ) .", "prompt_labels": "Starting(O) from(O) 2014(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) under(O) the(O) MMP(O) electoral(O) system(O) ,(O) 71(O) of(O) the(O) usually(O) 120(O) seats(O) in(O) Parliament(O) were(O) filled(O) by(O) electorate(O) members(O) ,(O) with(O) the(O) remainder(O) being(O) filled(O) from(O) party(O) list(O) s(O) in(O) order(O) to(O) achieve(O) proportional(O) representation(O) ((O) there(O) were(O) 69(O) electorates(O) in(O) 2005(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) and(O) 70(O) electorates(O) in(O) the(O) 2008(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) and(O) 2011(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) )(O) .(O)"}}
{"id": "79", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "event", "location", "organization", "political party", "election", "politician", "country"], "instance": {"id": "79", "words": ["He", "was", "the", "American", "Independent", "Party", "vice", "presidential", "nominee", "under", "John", "G.", "Schmitz", "in", "1972", "United", "States", "presidential", "election", "and", "the", "American", "Party", "presidential", "nominee", "in", "1976", "United", "States", "presidential", "election", "."], "labels": ["O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-politician", "I-politician", "I-politician", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, location, organization, political party, election, politician, country and O.\nSentence: He was the American Independent Party vice presidential nominee under John G. Schmitz in 1972 United States presidential election and the American Party presidential nominee in 1976 United States presidential election .", "prompt_labels": "He(O) was(O) the(O) American(B-political party) Independent(I-political party) Party(I-political party) vice(O) presidential(O) nominee(O) under(O) John(B-politician) G.(I-politician) Schmitz(I-politician) in(O) 1972(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) the(O) American(O) Party(O) presidential(O) nominee(O) in(O) 1976(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) .(O)"}}
{"id": "412", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "organization", "political party", "event", "person", "politician", "location", "election"], "instance": {"id": "412", "words": ["For", "the", "2018", "Italian", "general", "election", "New", "Force", "Joined", "with", "Tricolour", "Flame", "to", "form", "the", "Italy", "for", "the", "Italians", "coalition", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, political party, event, person, politician, location, election and O.\nSentence: For the 2018 Italian general election New Force Joined with Tricolour Flame to form the Italy for the Italians coalition .", "prompt_labels": "For(O) the(O) 2018(B-election) Italian(I-election) general(I-election) election(I-election) New(B-political party) Force(I-political party) Joined(O) with(O) Tricolour(B-political party) Flame(I-political party) to(O) form(O) the(O) Italy(B-political party) for(I-political party) the(I-political party) Italians(I-political party) coalition(O) .(O)"}}
{"id": "413", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "location", "person", "country", "election", "political party", "organization", "event"], "instance": {"id": "413", "words": ["Bill", "de", "Blasio", ",", "the", "incumbent", "mayor", ",", "won", "re-election", "to", "a", "second", "term", ",", "on", "Democratic", "and", "Working", "Families", "Party", "lines", ",", "over", "challengers", "Nicole", "Malliotakis", "on", "the", "Republican", "and", "Conservative", "party", "lines", ",", "Sal", "Albanese", "on", "the", "Reform", "Party", "of", "the", "United", "States", "of", "America", "line", ",", "Akeem", "Browder", "on", "the", "Green", "Party", "of", "the", "United", "States", "line", ",", "independent", "candidates", "Mike", "Tolkin", "and", "Bo", "Dietl", ",", "and", "Libertarian", "Party", "candidate", "Aaron", "Commey", "."], "labels": ["B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-political party", "I-political party", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, person, country, election, political party, organization, event and O.\nSentence: Bill de Blasio , the incumbent mayor , won re-election to a second term , on Democratic and Working Families Party lines , over challengers Nicole Malliotakis on the Republican and Conservative party lines , Sal Albanese on the Reform Party of the United States of America line , Akeem Browder on the Green Party of the United States line , independent candidates Mike Tolkin and Bo Dietl , and Libertarian Party candidate Aaron Commey .", "prompt_labels": "Bill(B-politician) de(I-politician) Blasio(I-politician) ,(O) the(O) incumbent(O) mayor(O) ,(O) won(O) re-election(O) to(O) a(O) second(O) term(O) ,(O) on(O) Democratic(O) and(O) Working(B-political party) Families(I-political party) Party(I-political party) lines(O) ,(O) over(O) challengers(O) Nicole(B-politician) Malliotakis(I-politician) on(O) the(O) Republican(O) and(O) Conservative(B-political party) party(I-political party) lines(O) ,(O) Sal(B-politician) Albanese(I-politician) on(O) the(O) Reform(B-political party) Party(I-political party) of(I-political party) the(I-political party) United(I-political party) States(I-political party) of(I-political party) America(I-political party) line(O) ,(O) Akeem(B-politician) Browder(I-politician) on(O) the(O) Green(B-political party) Party(I-political party) of(I-political party) the(I-political party) United(I-political party) States(I-political party) line(O) ,(O) independent(O) candidates(O) Mike(B-person) Tolkin(I-person) and(O) Bo(B-person) Dietl(I-person) ,(O) and(O) Libertarian(B-political party) Party(I-political party) candidate(O) Aaron(B-politician) Commey(I-politician) .(O)"}}
{"id": "302", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "person", "political party", "location", "election", "organization", "politician", "country"], "instance": {"id": "302", "words": ["This", "region", "has", "been", "dominated", "by", "the", "Conservative", "Party", "of", "Canada", "and", "the", "former", "Reform", "Party", "of", "Canada", "and", "Canadian", "Alliance", "parties", "for", "most", "of", "the", "time", "from", "1993", "to", "2011", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, political party, location, election, organization, politician, country and O.\nSentence: This region has been dominated by the Conservative Party of Canada and the former Reform Party of Canada and Canadian Alliance parties for most of the time from 1993 to 2011 .", "prompt_labels": "This(O) region(O) has(O) been(O) dominated(O) by(O) the(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) the(O) former(O) Reform(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) Canadian(B-political party) Alliance(I-political party) parties(O) for(O) most(O) of(O) the(O) time(O) from(O) 1993(O) to(O) 2011(O) .(O)"}}
{"id": "152", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "country", "event", "organization", "political party", "politician", "person", "location"], "instance": {"id": "152", "words": ["Molloy", "ran", "for", "the", "Canadian", "House", "of", "Commons", "in", "the", "1921", "Canadian", "federal", "election", "as", "a", "candidate", "of", "the", "Liberal", "Party", "of", "Canada", ",", "and", "lost", "to", "Progressive", "Party", "of", "Canada", "candidate", "Robert", "Alexander", "Hoey", "by", "1,397", "votes", "in", "the", "riding", "of", "Springfield", "."], "labels": ["B-politician", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, country, event, organization, political party, politician, person, location and O.\nSentence: Molloy ran for the Canadian House of Commons in the 1921 Canadian federal election as a candidate of the Liberal Party of Canada , and lost to Progressive Party of Canada candidate Robert Alexander Hoey by 1,397 votes in the riding of Springfield .", "prompt_labels": "Molloy(B-politician) ran(O) for(O) the(O) Canadian(B-organization) House(I-organization) of(I-organization) Commons(I-organization) in(O) the(O) 1921(B-election) Canadian(I-election) federal(I-election) election(I-election) as(O) a(O) candidate(O) of(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) and(O) lost(O) to(O) Progressive(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) candidate(O) Robert(B-politician) Alexander(I-politician) Hoey(I-politician) by(O) 1,397(O) votes(O) in(O) the(O) riding(O) of(O) Springfield(B-location) .(O)"}}
{"id": "382", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "person", "election", "politician", "country", "political party", "event", "organization"], "instance": {"id": "382", "words": ["He", "was", "a", "Catholic", ",", "an", "Benevolent", "and", "Protective", "Order", "of", "Elks", "and", "a", "member", "of", "the", "Knights", "of", "Columbus", "and", "the", "Fraternal", "Order", "of", "Eagles", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, election, politician, country, political party, event, organization and O.\nSentence: He was a Catholic , an Benevolent and Protective Order of Elks and a member of the Knights of Columbus and the Fraternal Order of Eagles .", "prompt_labels": "He(O) was(O) a(O) Catholic(O) ,(O) an(O) Benevolent(B-organization) and(I-organization) Protective(I-organization) Order(I-organization) of(I-organization) Elks(I-organization) and(O) a(O) member(O) of(O) the(O) Knights(B-organization) of(I-organization) Columbus(I-organization) and(O) the(O) Fraternal(B-organization) Order(I-organization) of(I-organization) Eagles(I-organization) .(O)"}}
{"id": "177", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "location", "election", "country", "political party", "event", "organization", "politician"], "instance": {"id": "177", "words": ["Prominent", "politicians", "from", "Sinn", "Féin", ",", "the", "Social", "Democratic", "and", "Labour", "Party", "(", "SDLP", ")", "and", "the", "centrist", "Alliance", "Party", "of", "Northern", "Ireland", "joined", "the", "protest", "."], "labels": ["O", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, election, country, political party, event, organization, politician and O.\nSentence: Prominent politicians from Sinn Féin , the Social Democratic and Labour Party ( SDLP ) and the centrist Alliance Party of Northern Ireland joined the protest .", "prompt_labels": "Prominent(O) politicians(O) from(O) Sinn(B-political party) Féin(I-political party) ,(O) the(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) ((O) SDLP(B-political party) )(O) and(O) the(O) centrist(O) Alliance(B-political party) Party(I-political party) of(I-political party) Northern(I-political party) Ireland(I-political party) joined(O) the(O) protest(O) .(O)"}}
{"id": "237", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "politician", "location", "person", "political party", "election", "country", "organization"], "instance": {"id": "237", "words": ["The", "Foundation", "'s", "website", "includes", "the", "Cato", "Institute", ",", "Earhart", "Foundation", ",", "John", "M.", "Olin", "Foundation", "and", "Foreign", "Policy", "Research", "Institute", "among", "its", "donors", "and", "sponsors", ".", "Free", "Africa", "Foundation", ",", "accessed", "2", "August", "2010", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, location, person, political party, election, country, organization and O.\nSentence: The Foundation 's website includes the Cato Institute , Earhart Foundation , John M. Olin Foundation and Foreign Policy Research Institute among its donors and sponsors . Free Africa Foundation , accessed 2 August 2010 .", "prompt_labels": "The(O) Foundation(O) 's(O) website(O) includes(O) the(O) Cato(B-organization) Institute(I-organization) ,(O) Earhart(B-organization) Foundation(I-organization) ,(O) John(B-organization) M.(I-organization) Olin(I-organization) Foundation(I-organization) and(O) Foreign(B-organization) Policy(I-organization) Research(I-organization) Institute(I-organization) among(O) its(O) donors(O) and(O) sponsors(O) .(O) Free(B-organization) Africa(I-organization) Foundation(I-organization) ,(O) accessed(O) 2(O) August(O) 2010(O) .(O)"}}
{"id": "369", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "person", "politician", "event", "location", "organization", "political party", "election"], "instance": {"id": "369", "words": ["In", "2008", "Canadian", "federal", "election", ",", "Charlton", "faced", "another", "former", "Ward", "7", "councillor", ",", "Terry", "Anderson", "of", "the", "Conservative", "Party", "of", "Canada", "and", "local", "lawyer", "Tyler", "Banham", "of", "the", "Liberal", "Party", "of", "Canada", "."], "labels": ["O", "B-election", "I-election", "I-election", "I-election", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, politician, event, location, organization, political party, election and O.\nSentence: In 2008 Canadian federal election , Charlton faced another former Ward 7 councillor , Terry Anderson of the Conservative Party of Canada and local lawyer Tyler Banham of the Liberal Party of Canada .", "prompt_labels": "In(O) 2008(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) Charlton(B-politician) faced(O) another(O) former(O) Ward(O) 7(O) councillor(O) ,(O) Terry(B-politician) Anderson(I-politician) of(O) the(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) local(O) lawyer(O) Tyler(B-politician) Banham(I-politician) of(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) .(O)"}}
{"id": "243", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "person", "event", "politician", "election", "political party", "organization", "location"], "instance": {"id": "243", "words": ["Dickson", "campaigned", "for", "the", "federal", "New", "Democratic", "Party", "in", "the", "1974", "Canadian", "federal", "election", ",", "and", "finished", "third", "in", "St.", "Catharines", "against", "Liberal", "Party", "of", "Canada", "candidate", "Gilbert", "Parent", "."], "labels": ["B-politician", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, event, politician, election, political party, organization, location and O.\nSentence: Dickson campaigned for the federal New Democratic Party in the 1974 Canadian federal election , and finished third in St. Catharines against Liberal Party of Canada candidate Gilbert Parent .", "prompt_labels": "Dickson(B-politician) campaigned(O) for(O) the(O) federal(O) New(B-political party) Democratic(I-political party) Party(I-political party) in(O) the(O) 1974(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) and(O) finished(O) third(O) in(O) St.(B-location) Catharines(I-location) against(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) candidate(O) Gilbert(B-politician) Parent(I-politician) .(O)"}}
{"id": "221", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "organization", "event", "country", "location", "politician", "election", "political party"], "instance": {"id": "221", "words": ["It", "contested", "the", "2004", "Nigerien", "general", "election", "in", "an", "alliance", "with", "the", "Nigerien", "Party", "for", "Democracy", "and", "Socialism", "(", "PNDS", ")", "and", "Nigerien", "Self-Management", "Party", "(", "PNA", ")", "."], "labels": ["O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, event, country, location, politician, election, political party and O.\nSentence: It contested the 2004 Nigerien general election in an alliance with the Nigerien Party for Democracy and Socialism ( PNDS ) and Nigerien Self-Management Party ( PNA ) .", "prompt_labels": "It(O) contested(O) the(O) 2004(B-election) Nigerien(I-election) general(I-election) election(I-election) in(O) an(O) alliance(O) with(O) the(O) Nigerien(B-political party) Party(I-political party) for(I-political party) Democracy(I-political party) and(I-political party) Socialism(I-political party) ((O) PNDS(B-political party) )(O) and(O) Nigerien(B-political party) Self-Management(I-political party) Party(I-political party) ((O) PNA(B-political party) )(O) .(O)"}}
{"id": "139", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "political party", "country", "organization", "election", "location", "event", "person"], "instance": {"id": "139", "words": ["The", "Commune", "was", "formed", "by", "85", "Social", "Revolutionaries", "and", "Left", "Socialist-Revolutionaries", ",", "48", "Bolsheviks", ",", "36", "Dashnaks", ",", "18", "Musavat", "ists", "and", "13", "Mensheviks", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "B-political party", "O", "O", "B-political party", "O", "O", "B-political party", "O", "O", "O", "B-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, country, organization, election, location, event, person and O.\nSentence: The Commune was formed by 85 Social Revolutionaries and Left Socialist-Revolutionaries , 48 Bolsheviks , 36 Dashnaks , 18 Musavat ists and 13 Mensheviks .", "prompt_labels": "The(O) Commune(O) was(O) formed(O) by(O) 85(O) Social(B-political party) Revolutionaries(I-political party) and(O) Left(B-political party) Socialist-Revolutionaries(I-political party) ,(O) 48(O) Bolsheviks(B-political party) ,(O) 36(O) Dashnaks(B-political party) ,(O) 18(O) Musavat(B-political party) ists(O) and(O) 13(O) Mensheviks(B-political party) .(O)"}}
{"id": "392", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "country", "political party", "event", "organization", "person", "election", "politician"], "instance": {"id": "392", "words": ["Republicans", "nominated", "Guy", "Millner", ",", "a", "multi-millionaire", "businessman", "who", "was", "also", "the", "unsuccessful", "candidate", "who", "ran", "against", "Zell", "Miller", "in", "the", "1994", "Georgia", "gubernatorial", "election", "."], "labels": ["O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, political party, event, organization, person, election, politician and O.\nSentence: Republicans nominated Guy Millner , a multi-millionaire businessman who was also the unsuccessful candidate who ran against Zell Miller in the 1994 Georgia gubernatorial election .", "prompt_labels": "Republicans(O) nominated(O) Guy(B-politician) Millner(I-politician) ,(O) a(O) multi-millionaire(O) businessman(O) who(O) was(O) also(O) the(O) unsuccessful(O) candidate(O) who(O) ran(O) against(O) Zell(B-politician) Miller(I-politician) in(O) the(O) 1994(B-election) Georgia(I-election) gubernatorial(I-election) election(I-election) .(O)"}}
{"id": "523", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "event", "election", "person", "politician", "country", "organization", "location"], "instance": {"id": "523", "words": ["He", "wrote", ":", "Leave", "won", "because", "1", ")", "three", "big", "forces", "the", "immigration", "crisis", ",", "the", "2008", "financial", "crisis", "and", "the", "euro", "crisis", "created", "conditions", "in", "which", "the", "contest", "was", "competitive", ",", "AND", "2", ")", "Vote", "Leave", "exploited", "the", "situation", "imperfectly", "but", "effectively", ",", "AND", "3", ")", "David", "Cameron", "/", "George", "Osborne", "made", "big", "mistakes", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, election, person, politician, country, organization, location and O.\nSentence: He wrote : Leave won because 1 ) three big forces the immigration crisis , the 2008 financial crisis and the euro crisis created conditions in which the contest was competitive , AND 2 ) Vote Leave exploited the situation imperfectly but effectively , AND 3 ) David Cameron / George Osborne made big mistakes .", "prompt_labels": "He(O) wrote(O) :(O) Leave(O) won(O) because(O) 1(O) )(O) three(O) big(O) forces(O) the(O) immigration(O) crisis(O) ,(O) the(O) 2008(B-event) financial(I-event) crisis(I-event) and(O) the(O) euro(B-event) crisis(I-event) created(O) conditions(O) in(O) which(O) the(O) contest(O) was(O) competitive(O) ,(O) AND(O) 2(O) )(O) Vote(B-organization) Leave(I-organization) exploited(O) the(O) situation(O) imperfectly(O) but(O) effectively(O) ,(O) AND(O) 3(O) )(O) David(B-politician) Cameron(I-politician) /(O) George(B-politician) Osborne(I-politician) made(O) big(O) mistakes(O) .(O)"}}
{"id": "8", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "event", "person", "election", "political party", "politician", "country", "organization"], "instance": {"id": "8", "words": ["MoveOn", "also", "joined", "with", "14", "other", "organization", "to", "form", "the", "Win", "Without", "War", "coalition", ",", "which", "also", "included", "the", "National", "Council", "of", "Churches", ",", "the", "National", "Association", "for", "the", "Advancement", "of", "Colored", "People", ",", "and", "the", "National", "Organization", "for", "Women", "."], "labels": ["B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, person, election, political party, politician, country, organization and O.\nSentence: MoveOn also joined with 14 other organization to form the Win Without War coalition , which also included the National Council of Churches , the National Association for the Advancement of Colored People , and the National Organization for Women .", "prompt_labels": "MoveOn(B-organization) also(O) joined(O) with(O) 14(O) other(O) organization(O) to(O) form(O) the(O) Win(B-organization) Without(I-organization) War(I-organization) coalition(O) ,(O) which(O) also(O) included(O) the(O) National(B-organization) Council(I-organization) of(I-organization) Churches(I-organization) ,(O) the(O) National(B-organization) Association(I-organization) for(I-organization) the(I-organization) Advancement(I-organization) of(I-organization) Colored(I-organization) People(I-organization) ,(O) and(O) the(O) National(B-organization) Organization(I-organization) for(I-organization) Women(I-organization) .(O)"}}
{"id": "433", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "person", "organization", "country", "location", "event", "election", "politician"], "instance": {"id": "433", "words": ["He", "is", "also", "the", "Harry", "Chandler", "&", "amp", ";", "Norman", "Chandler", "Professor", "of", "Communication", "at", "Stanford", ",", "the", "director", "of", "Stanford", "'s", "Political", "Communication", "Lab", ",", "and", "a", "senior", "fellow", "at", "the", "Hoover", "Institution", "."], "labels": ["O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, organization, country, location, event, election, politician and O.\nSentence: He is also the Harry Chandler & amp ; Norman Chandler Professor of Communication at Stanford , the director of Stanford 's Political Communication Lab , and a senior fellow at the Hoover Institution .", "prompt_labels": "He(O) is(O) also(O) the(O) Harry(B-person) Chandler(I-person) &(O) amp(O) ;(O) Norman(B-person) Chandler(I-person) Professor(O) of(O) Communication(O) at(O) Stanford(B-organization) ,(O) the(O) director(O) of(O) Stanford(B-organization) 's(O) Political(O) Communication(O) Lab(O) ,(O) and(O) a(O) senior(O) fellow(O) at(O) the(O) Hoover(B-organization) Institution(I-organization) .(O)"}}
{"id": "366", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "location", "country", "organization", "person", "event", "election", "politician"], "instance": {"id": "366", "words": ["RL", "was", "founded", "in", "2005", "by", "a", "split", "from", "the", "Italian", "Radicals", "of", "those", "radicals", "that", "were", "opposed", "to", "the", "formation", "of", "the", "Rose", "in", "the", "Fist", "alliance", "together", "with", "the", "Italian", "Democratic", "Socialists", ",", "as", "a", "component", "of", "the", "wider", "centre-left", "The", "Union", ",", "and", "instead", "supported", "an", "alliance", "with", "the", "centre-right", "House", "of", "Freedoms", "."], "labels": ["B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, location, country, organization, person, event, election, politician and O.\nSentence: RL was founded in 2005 by a split from the Italian Radicals of those radicals that were opposed to the formation of the Rose in the Fist alliance together with the Italian Democratic Socialists , as a component of the wider centre-left The Union , and instead supported an alliance with the centre-right House of Freedoms .", "prompt_labels": "RL(B-political party) was(O) founded(O) in(O) 2005(O) by(O) a(O) split(O) from(O) the(O) Italian(B-political party) Radicals(I-political party) of(O) those(O) radicals(O) that(O) were(O) opposed(O) to(O) the(O) formation(O) of(O) the(O) Rose(B-political party) in(I-political party) the(I-political party) Fist(I-political party) alliance(O) together(O) with(O) the(O) Italian(B-political party) Democratic(I-political party) Socialists(I-political party) ,(O) as(O) a(O) component(O) of(O) the(O) wider(O) centre-left(O) The(B-political party) Union(I-political party) ,(O) and(O) instead(O) supported(O) an(O) alliance(O) with(O) the(O) centre-right(O) House(B-organization) of(I-organization) Freedoms(I-organization) .(O)"}}
{"id": "483", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "location", "organization", "country", "election", "politician", "person", "event"], "instance": {"id": "483", "words": ["Meanwhile", ",", "Bangon", "Pilipinas", "candidate", "Eddie", "Villanueva", "opened", "his", "campaign", "at", "Malolos", ",", "Bulacan", ",", "while", "independent", "candidate", "Edward", "Hagedorn", "started", "his", "campaign", "with", "a", "press", "conference", "surrounded", "by", "Miss", "Earth", "candidates", "to", "emphasize", "his", "pro-environment", "agenda", ",", "which", "followed", "with", "a", "motorcade", "that", "ended", "at", "the", "Santo", "Niño", "Parish", "Church", "at", "Tondo", ",", "Manila", "."], "labels": ["O", "O", "B-political party", "I-political party", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "B-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, location, organization, country, election, politician, person, event and O.\nSentence: Meanwhile , Bangon Pilipinas candidate Eddie Villanueva opened his campaign at Malolos , Bulacan , while independent candidate Edward Hagedorn started his campaign with a press conference surrounded by Miss Earth candidates to emphasize his pro-environment agenda , which followed with a motorcade that ended at the Santo Niño Parish Church at Tondo , Manila .", "prompt_labels": "Meanwhile(O) ,(O) Bangon(B-political party) Pilipinas(I-political party) candidate(O) Eddie(B-politician) Villanueva(I-politician) opened(O) his(O) campaign(O) at(O) Malolos(B-location) ,(O) Bulacan(B-location) ,(O) while(O) independent(O) candidate(O) Edward(B-politician) Hagedorn(I-politician) started(O) his(O) campaign(O) with(O) a(O) press(O) conference(O) surrounded(O) by(O) Miss(B-organization) Earth(I-organization) candidates(O) to(O) emphasize(O) his(O) pro-environment(O) agenda(O) ,(O) which(O) followed(O) with(O) a(O) motorcade(O) that(O) ended(O) at(O) the(O) Santo(B-location) Niño(I-location) Parish(I-location) Church(I-location) at(O) Tondo(B-location) ,(O) Manila(B-location) .(O)"}}
{"id": "218", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "organization", "person", "election", "politician", "political party", "country", "event"], "instance": {"id": "218", "words": ["He", "then", "served", "as", "Economic", "Minister", "at", "the", "British", "Embassy", "in", "Washington", "as", "well", "as", "holding", "executive", "directorships", "at", "the", "International", "Monetary", "Fund", ",", "the", "International", "Bank", "for", "Reconstruction", "and", "Development", ",", "and", "the", "International", "Finance", "Corporation", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, person, election, politician, political party, country, event and O.\nSentence: He then served as Economic Minister at the British Embassy in Washington as well as holding executive directorships at the International Monetary Fund , the International Bank for Reconstruction and Development , and the International Finance Corporation .", "prompt_labels": "He(O) then(O) served(O) as(O) Economic(O) Minister(O) at(O) the(O) British(B-organization) Embassy(I-organization) in(I-organization) Washington(I-organization) as(O) well(O) as(O) holding(O) executive(O) directorships(O) at(O) the(O) International(B-organization) Monetary(I-organization) Fund(I-organization) ,(O) the(O) International(B-organization) Bank(I-organization) for(I-organization) Reconstruction(I-organization) and(I-organization) Development(I-organization) ,(O) and(O) the(O) International(B-organization) Finance(I-organization) Corporation(I-organization) .(O)"}}
{"id": "506", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "country", "organization", "politician", "event", "election", "person", "location"], "instance": {"id": "506", "words": ["The", "Somaliland", "campaign", ",", "also", "called", "the", "Anglo-Somali", "War", "or", "the", "Dervish", "War", ",", "was", "a", "series", "of", "military", "expeditions", "that", "took", "place", "between", "1900", "and", "1920", "in", "the", "Horn", "of", "Africa", ",", "pitting", "the", "Dervishes", "led", "by", "Mohammed", "Abdullah", "Hassan", "(", "nicknamed", "the", "Mad", "Mullah", ",", "although", "he", "was", "neither", "mad", "nor", "a", "mullah", ")", "against", "the", "British", "Empire", "."], "labels": ["O", "B-event", "I-event", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, organization, politician, event, election, person, location and O.\nSentence: The Somaliland campaign , also called the Anglo-Somali War or the Dervish War , was a series of military expeditions that took place between 1900 and 1920 in the Horn of Africa , pitting the Dervishes led by Mohammed Abdullah Hassan ( nicknamed the Mad Mullah , although he was neither mad nor a mullah ) against the British Empire .", "prompt_labels": "The(O) Somaliland(B-event) campaign(I-event) ,(O) also(O) called(O) the(O) Anglo-Somali(B-event) War(I-event) or(O) the(O) Dervish(B-event) War(I-event) ,(O) was(O) a(O) series(O) of(O) military(O) expeditions(O) that(O) took(O) place(O) between(O) 1900(O) and(O) 1920(O) in(O) the(O) Horn(B-event) of(I-event) Africa(I-event) ,(O) pitting(O) the(O) Dervishes(O) led(O) by(O) Mohammed(B-person) Abdullah(I-person) Hassan(I-person) ((O) nicknamed(O) the(O) Mad(B-person) Mullah(I-person) ,(O) although(O) he(O) was(O) neither(O) mad(O) nor(O) a(O) mullah(O) )(O) against(O) the(O) British(B-country) Empire(I-country) .(O)"}}
{"id": "35", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "event", "person", "political party", "election", "politician", "country", "organization"], "instance": {"id": "35", "words": ["The", "party", "includes", "former", "Italian", "Communist", "Party", "and", "former", "Lega", "Nord", ",", "as", "well", "as", "former", "Italian", "Social", "Movement", "and", "several", "former", "Christian", "Democrats", "."], "labels": ["O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, person, political party, election, politician, country, organization and O.\nSentence: The party includes former Italian Communist Party and former Lega Nord , as well as former Italian Social Movement and several former Christian Democrats .", "prompt_labels": "The(O) party(O) includes(O) former(O) Italian(B-political party) Communist(I-political party) Party(I-political party) and(O) former(O) Lega(B-political party) Nord(I-political party) ,(O) as(O) well(O) as(O) former(O) Italian(B-political party) Social(I-political party) Movement(I-political party) and(O) several(O) former(O) Christian(O) Democrats(O) .(O)"}}
{"id": "352", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "organization", "election", "country", "person", "location", "political party", "politician"], "instance": {"id": "352", "words": ["Of", "the", "major", "federal", "political", "parties", ",", "the", "New", "Democratic", "Party", "has", "nominated", "the", "most", "female", "candidates", "in", "every", "election", "since", "its", "creation", ",", "except", "in", "the", "1962", "Canadian", "federal", "election", ",", "when", "it", "tied", "with", "the", "Progressive", "Conservative", "Party", "of", "Canada", ",", "and", "the", "2008", "election", ",", "when", "the", "Liberal", "Party", "of", "Canada", "nominated", "the", "most", "female", "candidates", "for", "the", "first", "time", "in", "their", "history", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, election, country, person, location, political party, politician and O.\nSentence: Of the major federal political parties , the New Democratic Party has nominated the most female candidates in every election since its creation , except in the 1962 Canadian federal election , when it tied with the Progressive Conservative Party of Canada , and the 2008 election , when the Liberal Party of Canada nominated the most female candidates for the first time in their history .", "prompt_labels": "Of(O) the(O) major(O) federal(O) political(O) parties(O) ,(O) the(O) New(B-political party) Democratic(I-political party) Party(I-political party) has(O) nominated(O) the(O) most(O) female(O) candidates(O) in(O) every(O) election(O) since(O) its(O) creation(O) ,(O) except(O) in(O) the(O) 1962(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) when(O) it(O) tied(O) with(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) and(O) the(O) 2008(O) election(O) ,(O) when(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) nominated(O) the(O) most(O) female(O) candidates(O) for(O) the(O) first(O) time(O) in(O) their(O) history(O) .(O)"}}
{"id": "222", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "event", "organization", "election", "person", "country", "politician", "location"], "instance": {"id": "222", "words": ["The", "Manitoba", "Cooperative", "Commonwealth", "Federation", "was", "the", "primary", "opposition", "party", "in", "the", "1949", "Manitoba", "general", "election", ",", "challenging", "the", "coalition", "government", "of", "Manitoba", "Liberal", "Party", "and", "Progressive", "Conservative", "Party", "of", "Manitoba", "."], "labels": ["O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, organization, election, person, country, politician, location and O.\nSentence: The Manitoba Cooperative Commonwealth Federation was the primary opposition party in the 1949 Manitoba general election , challenging the coalition government of Manitoba Liberal Party and Progressive Conservative Party of Manitoba .", "prompt_labels": "The(O) Manitoba(B-political party) Cooperative(I-political party) Commonwealth(I-political party) Federation(I-political party) was(O) the(O) primary(O) opposition(O) party(O) in(O) the(O) 1949(B-election) Manitoba(I-election) general(I-election) election(I-election) ,(O) challenging(O) the(O) coalition(O) government(O) of(O) Manitoba(B-political party) Liberal(I-political party) Party(I-political party) and(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Manitoba(I-political party) .(O)"}}
{"id": "184", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "person", "location", "organization", "election", "politician", "political party", "event"], "instance": {"id": "184", "words": ["The", "Queensland", "Greens", "enjoyed", "growing", "support", "in", "state", "elections", ",", "increasing", "their", "vote", "from", "2.5", "per", "cent", "at", "the", "2001", "Queensland", "state", "election", "(", "when", "they", "contested", "31", "of", "the", "Parliament", "'s", "89", "seats", ")", ",", "to", "6.76", "per", "cent", "in", "2004", "Queensland", "state", "election", "(", "from", "72", "seats", ")", ",", "to", "7.99", "per", "cent", "in", "2006", "Queensland", "state", "election", "(", "from", "75", "seats", ")", ","], "labels": ["O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, location, organization, election, politician, political party, event and O.\nSentence: The Queensland Greens enjoyed growing support in state elections , increasing their vote from 2.5 per cent at the 2001 Queensland state election ( when they contested 31 of the Parliament 's 89 seats ) , to 6.76 per cent in 2004 Queensland state election ( from 72 seats ) , to 7.99 per cent in 2006 Queensland state election ( from 75 seats ) ,", "prompt_labels": "The(O) Queensland(B-political party) Greens(I-political party) enjoyed(O) growing(O) support(O) in(O) state(O) elections(O) ,(O) increasing(O) their(O) vote(O) from(O) 2.5(O) per(O) cent(O) at(O) the(O) 2001(B-election) Queensland(I-election) state(I-election) election(I-election) ((O) when(O) they(O) contested(O) 31(O) of(O) the(O) Parliament(O) 's(O) 89(O) seats(O) )(O) ,(O) to(O) 6.76(O) per(O) cent(O) in(O) 2004(B-election) Queensland(I-election) state(I-election) election(I-election) ((O) from(O) 72(O) seats(O) )(O) ,(O) to(O) 7.99(O) per(O) cent(O) in(O) 2006(B-election) Queensland(I-election) state(I-election) election(I-election) ((O) from(O) 75(O) seats(O) )(O) ,(O)"}}
{"id": "149", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "politician", "election", "organization", "event", "person", "political party", "country"], "instance": {"id": "149", "words": ["Patteson", "was", "a", "member", "of", "the", "Board", "of", "Trustees", "of", "West", "Virginia", "Wesleyan", ",", "and", "of", "a", "number", "of", "societies", ":", "Free", "mason", "s", ",", "Knights", "Templar", ",", "Moose", "International", ",", "Lions", "Clubs", "International", ",", "Chamber", "of", "Commerce", ",", "American", "Legion", ",", "Sons", "of", "the", "American", "Revolution", "and", "Benevolent", "and", "Protective", "Order", "of", "Elks", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, election, organization, event, person, political party, country and O.\nSentence: Patteson was a member of the Board of Trustees of West Virginia Wesleyan , and of a number of societies : Free mason s , Knights Templar , Moose International , Lions Clubs International , Chamber of Commerce , American Legion , Sons of the American Revolution and Benevolent and Protective Order of Elks .", "prompt_labels": "Patteson(B-person) was(O) a(O) member(O) of(O) the(O) Board(O) of(O) Trustees(O) of(O) West(B-organization) Virginia(I-organization) Wesleyan(I-organization) ,(O) and(O) of(O) a(O) number(O) of(O) societies(O) :(O) Free(B-organization) mason(I-organization) s(O) ,(O) Knights(B-organization) Templar(I-organization) ,(O) Moose(B-organization) International(I-organization) ,(O) Lions(B-organization) Clubs(I-organization) International(I-organization) ,(O) Chamber(B-organization) of(I-organization) Commerce(I-organization) ,(O) American(B-organization) Legion(I-organization) ,(O) Sons(B-organization) of(I-organization) the(I-organization) American(I-organization) Revolution(I-organization) and(O) Benevolent(B-organization) and(I-organization) Protective(I-organization) Order(I-organization) of(I-organization) Elks(I-organization) .(O)"}}
{"id": "440", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "person", "politician", "election", "country", "location", "organization", "event"], "instance": {"id": "440", "words": ["On", "these", "pages", "Eddie", "Peng", ",", "Shu", "Qi", ",", "Joe", "Chen", "and", "Mark", "Chao", ",", "user", "Treysand", "keeps", "on", "adding", "English", "title", "to", "the", "filmography", "pages", "and", "accuses", "me", "of", "vandalism", "."], "labels": ["O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, politician, election, country, location, organization, event and O.\nSentence: On these pages Eddie Peng , Shu Qi , Joe Chen and Mark Chao , user Treysand keeps on adding English title to the filmography pages and accuses me of vandalism .", "prompt_labels": "On(O) these(O) pages(O) Eddie(B-person) Peng(I-person) ,(O) Shu(B-person) Qi(I-person) ,(O) Joe(B-person) Chen(I-person) and(O) Mark(B-person) Chao(I-person) ,(O) user(O) Treysand(B-person) keeps(O) on(O) adding(O) English(O) title(O) to(O) the(O) filmography(O) pages(O) and(O) accuses(O) me(O) of(O) vandalism(O) .(O)"}}
{"id": "250", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "event", "location", "political party", "politician", "election", "organization", "person"], "instance": {"id": "250", "words": ["In", "Northern", "Ireland", ",", "the", "term", "is", "not", "used", "by", "the", "Social", "Democratic", "and", "Labour", "Party", ",", "or", "by", "those", "parties", "which", "are", "not", "Irish-nationalist", "in", "outlook", ",", "such", "as", "the", "Alliance", "Party", "of", "Northern", "Ireland", "and", "Democratic", "Unionist", "Party", "."], "labels": ["O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, location, political party, politician, election, organization, person and O.\nSentence: In Northern Ireland , the term is not used by the Social Democratic and Labour Party , or by those parties which are not Irish-nationalist in outlook , such as the Alliance Party of Northern Ireland and Democratic Unionist Party .", "prompt_labels": "In(O) Northern(B-country) Ireland(I-country) ,(O) the(O) term(O) is(O) not(O) used(O) by(O) the(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) ,(O) or(O) by(O) those(O) parties(O) which(O) are(O) not(O) Irish-nationalist(O) in(O) outlook(O) ,(O) such(O) as(O) the(O) Alliance(B-political party) Party(I-political party) of(I-political party) Northern(I-political party) Ireland(I-political party) and(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) .(O)"}}
{"id": "405", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "event", "election", "location", "organization", "country", "politician", "person"], "instance": {"id": "405", "words": ["Incumbent", "Democrat", "Mike", "Gravel", "ran", "for", "a", "third", "term", ",", "but", "lost", "in", "the", "Democratic", "primary", "to", "Clark", "Gruening", ",", "a", "former", "state", "representative", "who", "was", "the", "grandson", "of", "Ernest", "Gruening", ",", "whom", "Gravel", "had", "defeated", "twelve", "years", "prior", "in", "an", "election", "for", "the", "same", "seat", "."], "labels": ["O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, election, location, organization, country, politician, person and O.\nSentence: Incumbent Democrat Mike Gravel ran for a third term , but lost in the Democratic primary to Clark Gruening , a former state representative who was the grandson of Ernest Gruening , whom Gravel had defeated twelve years prior in an election for the same seat .", "prompt_labels": "Incumbent(O) Democrat(O) Mike(B-politician) Gravel(I-politician) ran(O) for(O) a(O) third(O) term(O) ,(O) but(O) lost(O) in(O) the(O) Democratic(O) primary(O) to(O) Clark(B-politician) Gruening(I-politician) ,(O) a(O) former(O) state(O) representative(O) who(O) was(O) the(O) grandson(O) of(O) Ernest(B-politician) Gruening(I-politician) ,(O) whom(O) Gravel(O) had(O) defeated(O) twelve(O) years(O) prior(O) in(O) an(O) election(O) for(O) the(O) same(O) seat(O) .(O)"}}
{"id": "329", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "politician", "person", "country", "location", "organization", "political party", "event"], "instance": {"id": "329", "words": ["Alex", "T.", "Atamanenko", "MP", "(", "born", "January", "24", ",", "1945", ")", "is", "a", "Canadian", "politician", ",", "who", "was", "elected", "to", "the", "House", "of", "Commons", "in", "2006", ",", "winning", "the", "riding", "of", "British", "Columbia", "Southern", "Interior", "for", "the", "New", "Democratic", "Party", "in", "the", "2006", "Canadian", "federal", "election", ",", "and", "served", "in", "parliament", "until", "his", "retirement", "at", "the", "2015", "Canadian", "federal", "election", "."], "labels": ["B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, person, country, location, organization, political party, event and O.\nSentence: Alex T. Atamanenko MP ( born January 24 , 1945 ) is a Canadian politician , who was elected to the House of Commons in 2006 , winning the riding of British Columbia Southern Interior for the New Democratic Party in the 2006 Canadian federal election , and served in parliament until his retirement at the 2015 Canadian federal election .", "prompt_labels": "Alex(B-politician) T.(I-politician) Atamanenko(I-politician) MP(O) ((O) born(O) January(O) 24(O) ,(O) 1945(O) )(O) is(O) a(O) Canadian(O) politician(O) ,(O) who(O) was(O) elected(O) to(O) the(O) House(B-organization) of(I-organization) Commons(I-organization) in(O) 2006(O) ,(O) winning(O) the(O) riding(O) of(O) British(B-organization) Columbia(I-organization) Southern(I-organization) Interior(I-organization) for(O) the(O) New(B-political party) Democratic(I-political party) Party(I-political party) in(O) the(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) and(O) served(O) in(O) parliament(O) until(O) his(O) retirement(O) at(O) the(O) 2015(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)"}}
{"id": "407", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "political party", "location", "politician", "country", "event", "election", "organization"], "instance": {"id": "407", "words": ["He", "had", "run", "in", "the", "Republican", "primary", "in", "the", "1976", "United", "States", "Senate", "election", "in", "Pennsylvania", ",", "but", "was", "defeated", "by", "John", "Heinz", "and", "also", "ran", "in", "the", "1978", "Pennsylvania", "gubernatorial", "election", ",", "but", "was", "defeated", "by", "Dick", "Thornburgh", "in", "the", "primary", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, location, politician, country, event, election, organization and O.\nSentence: He had run in the Republican primary in the 1976 United States Senate election in Pennsylvania , but was defeated by John Heinz and also ran in the 1978 Pennsylvania gubernatorial election , but was defeated by Dick Thornburgh in the primary .", "prompt_labels": "He(O) had(O) run(O) in(O) the(O) Republican(O) primary(O) in(O) the(O) 1976(B-election) United(I-election) States(I-election) Senate(I-election) election(I-election) in(I-election) Pennsylvania(I-election) ,(O) but(O) was(O) defeated(O) by(O) John(B-politician) Heinz(I-politician) and(O) also(O) ran(O) in(O) the(O) 1978(B-election) Pennsylvania(I-election) gubernatorial(I-election) election(I-election) ,(O) but(O) was(O) defeated(O) by(O) Dick(B-politician) Thornburgh(I-politician) in(O) the(O) primary(O) .(O)"}}
{"id": "160", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "person", "country", "politician", "election", "location", "organization", "event"], "instance": {"id": "160", "words": ["The", "party", "ran", "in", "the", "1994", "Italian", "general", "election", "within", "the", "Alliance", "of", "Progressives", "and", "obtained", "a", "mere", "1.2", "%", "of", "the", "vote", ",", "due", "to", "the", "uneasy", "alliance", "with", "the", "traditional", "left", "and", "the", "competition", "by", "Silvio", "Berlusconi", "'", "s", "Forza", "Italia", ",", "which", "embraced", "most", "of", "AD", "'s", "policies", "."], "labels": ["O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, country, politician, election, location, organization, event and O.\nSentence: The party ran in the 1994 Italian general election within the Alliance of Progressives and obtained a mere 1.2 % of the vote , due to the uneasy alliance with the traditional left and the competition by Silvio Berlusconi ' s Forza Italia , which embraced most of AD 's policies .", "prompt_labels": "The(O) party(O) ran(O) in(O) the(O) 1994(B-election) Italian(I-election) general(I-election) election(I-election) within(O) the(O) Alliance(B-political party) of(I-political party) Progressives(I-political party) and(O) obtained(O) a(O) mere(O) 1.2(O) %(O) of(O) the(O) vote(O) ,(O) due(O) to(O) the(O) uneasy(O) alliance(O) with(O) the(O) traditional(O) left(O) and(O) the(O) competition(O) by(O) Silvio(B-politician) Berlusconi(I-politician) '(O) s(O) Forza(B-political party) Italia(I-political party) ,(O) which(O) embraced(O) most(O) of(O) AD(B-political party) 's(O) policies(O) .(O)"}}
{"id": "43", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "organization", "person", "election", "event", "politician", "location", "country"], "instance": {"id": "43", "words": ["Kardash", "was", "re-elected", "in", "the", "1945", ",", "1949", "Manitoba", "general", "election", "and", "1953", "Manitoba", "general", "election", "as", "a", "member", "of", "the", "Labor-Progressive", "Party", "(", "as", "the", "Communists", "had", "renamed", "themselves", ")", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, person, election, event, politician, location, country and O.\nSentence: Kardash was re-elected in the 1945 , 1949 Manitoba general election and 1953 Manitoba general election as a member of the Labor-Progressive Party ( as the Communists had renamed themselves ) .", "prompt_labels": "Kardash(B-politician) was(O) re-elected(O) in(O) the(O) 1945(O) ,(O) 1949(B-election) Manitoba(I-election) general(I-election) election(I-election) and(O) 1953(B-election) Manitoba(I-election) general(I-election) election(I-election) as(O) a(O) member(O) of(O) the(O) Labor-Progressive(B-political party) Party(I-political party) ((O) as(O) the(O) Communists(O) had(O) renamed(O) themselves(O) )(O) .(O)"}}
{"id": "127", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "election", "event", "country", "politician", "location", "political party", "organization"], "instance": {"id": "127", "words": ["They", "would", "win", "a", "small", "12-seat", "majority", "in", "2015", "United", "Kingdom", "general", "election", ",", "only", "to", "lose", "it", "again", "at", "the", "2017", "United", "Kingdom", "general", "election", "but", "would", "win", "in", "a", "landslide", "in", "the", "2019", "United", "Kingdom", "general", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, event, country, politician, location, political party, organization and O.\nSentence: They would win a small 12-seat majority in 2015 United Kingdom general election , only to lose it again at the 2017 United Kingdom general election but would win in a landslide in the 2019 United Kingdom general election .", "prompt_labels": "They(O) would(O) win(O) a(O) small(O) 12-seat(O) majority(O) in(O) 2015(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) only(O) to(O) lose(O) it(O) again(O) at(O) the(O) 2017(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) but(O) would(O) win(O) in(O) a(O) landslide(O) in(O) the(O) 2019(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "291", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "event", "politician", "political party", "election", "country", "person", "organization"], "instance": {"id": "291", "words": ["To", "date", ",", "it", "is", "the", "only", "party", "other", "than", "Mapai", "/", "Israeli", "Labor", "Party", "or", "Likud", "to", "have", "led", "a", "government", "in", "Israel", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, politician, political party, election, country, person, organization and O.\nSentence: To date , it is the only party other than Mapai / Israeli Labor Party or Likud to have led a government in Israel .", "prompt_labels": "To(O) date(O) ,(O) it(O) is(O) the(O) only(O) party(O) other(O) than(O) Mapai(B-political party) /(O) Israeli(B-political party) Labor(I-political party) Party(I-political party) or(O) Likud(B-political party) to(O) have(O) led(O) a(O) government(O) in(O) Israel(B-country) .(O)"}}
{"id": "88", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "person", "country", "location", "political party", "politician", "event", "organization"], "instance": {"id": "88", "words": ["He", "ran", "unsuccessfully", "as", "an", "independent", "Liberal", "in", "Saint-Jean", "-", "Iberville", "-", "Napierville", "in", "the", "1957", "Canadian", "federal", "election", ",", "but", "was", "successful", "as", "the", "Liberal", "Party", "of", "Canada", "candidate", "in", "the", "same", "riding", "in", "the", "1958", "Canadian", "federal", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, country, location, political party, politician, event, organization and O.\nSentence: He ran unsuccessfully as an independent Liberal in Saint-Jean - Iberville - Napierville in the 1957 Canadian federal election , but was successful as the Liberal Party of Canada candidate in the same riding in the 1958 Canadian federal election .", "prompt_labels": "He(O) ran(O) unsuccessfully(O) as(O) an(O) independent(O) Liberal(O) in(O) Saint-Jean(B-location) -(I-location) Iberville(I-location) -(I-location) Napierville(I-location) in(O) the(O) 1957(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) but(O) was(O) successful(O) as(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) candidate(O) in(O) the(O) same(O) riding(O) in(O) the(O) 1958(B-election) Canadian(I-election) federal(I-election) election(I-election) .(O)"}}
{"id": "450", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "organization", "person", "event", "election", "political party", "politician", "country"], "instance": {"id": "450", "words": ["Other", "talents", "include", "Omar", "Chaparro", ",", "who", "previously", "collaborated", "with", "Huevocartoon", "for", "Un", "gallo", "con", "muchos", "huevos", ",", "Eduardo", "Manzano", ",", "and", "ufologist", "Jaime", "Maussan", ",", "among", "others", "."], "labels": ["O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, person, event, election, political party, politician, country and O.\nSentence: Other talents include Omar Chaparro , who previously collaborated with Huevocartoon for Un gallo con muchos huevos , Eduardo Manzano , and ufologist Jaime Maussan , among others .", "prompt_labels": "Other(O) talents(O) include(O) Omar(B-person) Chaparro(I-person) ,(O) who(O) previously(O) collaborated(O) with(O) Huevocartoon(B-person) for(O) Un(O) gallo(O) con(O) muchos(O) huevos(O) ,(O) Eduardo(B-person) Manzano(I-person) ,(O) and(O) ufologist(O) Jaime(B-person) Maussan(I-person) ,(O) among(O) others(O) .(O)"}}
{"id": "69", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "country", "election", "politician", "event", "person", "organization", "political party"], "instance": {"id": "69", "words": ["However", ",", "with", "the", "onset", "of", "the", "Troubles", ",", "new", "parties", "emerged", "that", "appealed", "to", "the", "party", "'s", "support", "base", ",", "including", "the", "Social", "Democratic", "and", "Labour", "Party", "(", "SDLP", ")", ",", "the", "Alliance", "Party", "of", "Northern", "Ireland", "and", "the", "Democratic", "Unionist", "Party", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, election, politician, event, person, organization, political party and O.\nSentence: However , with the onset of the Troubles , new parties emerged that appealed to the party 's support base , including the Social Democratic and Labour Party ( SDLP ) , the Alliance Party of Northern Ireland and the Democratic Unionist Party .", "prompt_labels": "However(O) ,(O) with(O) the(O) onset(O) of(O) the(B-event) Troubles(I-event) ,(O) new(O) parties(O) emerged(O) that(O) appealed(O) to(O) the(O) party(O) 's(O) support(O) base(O) ,(O) including(O) the(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) ((O) SDLP(B-political party) )(O) ,(O) the(O) Alliance(B-political party) Party(I-political party) of(I-political party) Northern(I-political party) Ireland(I-political party) and(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) .(O)"}}
{"id": "408", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "event", "political party", "person", "location", "country", "election", "politician"], "instance": {"id": "408", "words": ["The", "election", "also", "produced", "other", "close", "results", ";", "Milton", "Young", "(", "R-ND", ")", "won", "reelection", "against", "Democrat", "William", "L.", "Guy", "by", "only", "186", "votes", "and", "Henry", "Bellmon", "(", "R-OK", ")", "won", "reelection", "against", "Democrat", "Ed", "Edmondson", "by", "half", "a", "percent", "of", "the", "vote", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, political party, person, location, country, election, politician and O.\nSentence: The election also produced other close results ; Milton Young ( R-ND ) won reelection against Democrat William L. Guy by only 186 votes and Henry Bellmon ( R-OK ) won reelection against Democrat Ed Edmondson by half a percent of the vote .", "prompt_labels": "The(O) election(O) also(O) produced(O) other(O) close(O) results(O) ;(O) Milton(B-politician) Young(I-politician) ((O) R-ND(O) )(O) won(O) reelection(O) against(O) Democrat(O) William(B-politician) L.(I-politician) Guy(I-politician) by(O) only(O) 186(O) votes(O) and(O) Henry(B-politician) Bellmon(I-politician) ((O) R-OK(O) )(O) won(O) reelection(O) against(O) Democrat(O) Ed(B-politician) Edmondson(I-politician) by(O) half(O) a(O) percent(O) of(O) the(O) vote(O) .(O)"}}
{"id": "180", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "organization", "person", "political party", "election", "event", "politician", "location"], "instance": {"id": "180", "words": ["The", "group", "'s", "School", "Risk", "Audit", "program", "was", "conducted", "jointly", "by", "Mission", "America", ",", "the", "American", "Family", "Association", ",", "Concerned", "Women", "for", "America", ",", "the", "Family", "Research", "Council", ",", "and", "other", "groups", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, person, political party, election, event, politician, location and O.\nSentence: The group 's School Risk Audit program was conducted jointly by Mission America , the American Family Association , Concerned Women for America , the Family Research Council , and other groups .", "prompt_labels": "The(O) group(O) 's(O) School(O) Risk(O) Audit(O) program(O) was(O) conducted(O) jointly(O) by(O) Mission(B-organization) America(I-organization) ,(O) the(O) American(B-organization) Family(I-organization) Association(I-organization) ,(O) Concerned(B-organization) Women(I-organization) for(I-organization) America(I-organization) ,(O) the(O) Family(B-organization) Research(I-organization) Council(I-organization) ,(O) and(O) other(O) groups(O) .(O)"}}
{"id": "387", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "political party", "organization", "event", "politician", "election", "country", "person"], "instance": {"id": "387", "words": ["This", "election", "marked", "the", "first", "time", "Republicans", "controlled", "the", "Senate", "since", "1986", "United", "States", "Senate", "elections", ",", "and", "coincided", "with", "the", "first", "1994", "United", "States", "House", "of", "Representatives", "elections", "since", "1954", "United", "States", "House", "of", "Representatives", "elections", "and", "a", "Republican", "1994", "United", "States", "gubernatorial", "elections", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-political party", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, organization, event, politician, election, country, person and O.\nSentence: This election marked the first time Republicans controlled the Senate since 1986 United States Senate elections , and coincided with the first 1994 United States House of Representatives elections since 1954 United States House of Representatives elections and a Republican 1994 United States gubernatorial elections .", "prompt_labels": "This(O) election(O) marked(O) the(O) first(O) time(O) Republicans(O) controlled(O) the(O) Senate(B-organization) since(O) 1986(B-election) United(I-election) States(I-election) Senate(I-election) elections(I-election) ,(O) and(O) coincided(O) with(O) the(O) first(O) 1994(B-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) since(O) 1954(B-election) United(I-election) States(I-election) House(I-election) of(I-election) Representatives(I-election) elections(I-election) and(O) a(O) Republican(B-political party) 1994(B-election) United(I-election) States(I-election) gubernatorial(I-election) elections(I-election) .(O)"}}
{"id": "202", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "politician", "election", "organization", "event", "political party", "location", "country"], "instance": {"id": "202", "words": ["She", "was", "re-elected", "to", "additional", "terms", "in", "1974", "Australian", "federal", "election", ",", "1975", "Australian", "federal", "election", ",", "and", "1980", "Australian", "federal", "election", ",", "retiring", "on", "5", "June", "1987", "at", "the", "end", "of", "her", "final", "term", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, election, organization, event, political party, location, country and O.\nSentence: She was re-elected to additional terms in 1974 Australian federal election , 1975 Australian federal election , and 1980 Australian federal election , retiring on 5 June 1987 at the end of her final term .", "prompt_labels": "She(O) was(O) re-elected(O) to(O) additional(O) terms(O) in(O) 1974(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) 1975(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) and(O) 1980(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) retiring(O) on(O) 5(O) June(O) 1987(O) at(O) the(O) end(O) of(O) her(O) final(O) term(O) .(O)"}}
{"id": "219", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "organization", "politician", "event", "person", "political party", "country", "location"], "instance": {"id": "219", "words": ["He", "twice", "ran", "for", "President", "of", "the", "United", "States", "on", "the", "Workers", "World", "Party", "(", "WWP", ")", "ticket", ",", "in", "1984", "United", "States", "presidential", "election", "and", "1988", "United", "States", "presidential", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, politician, event, person, political party, country, location and O.\nSentence: He twice ran for President of the United States on the Workers World Party ( WWP ) ticket , in 1984 United States presidential election and 1988 United States presidential election .", "prompt_labels": "He(O) twice(O) ran(O) for(O) President(O) of(O) the(O) United(B-country) States(I-country) on(O) the(O) Workers(B-political party) World(I-political party) Party(I-political party) ((O) WWP(B-political party) )(O) ticket(O) ,(O) in(O) 1984(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) 1988(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) .(O)"}}
{"id": "528", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "election", "location", "country", "politician", "organization", "person", "political party"], "instance": {"id": "528", "words": ["A", "certain", "rivalry", "between", "Walter", "Ulbricht", "and", "Franz", "Dahlem", "had", "arisen", "in", "the", "1930s", ",", "and", "after", "the", "war", "there", "were", "more", "general", "tensions", "within", "the", "national", "leadership", "between", "those", "-", "including", "Walter", "Ulbricht", "and", "President", "Pieck", "-", "who", "had", "spent", "the", "war", "years", "in", "Moscow", "and", "those", "(", "such", "as", "Dahlem", ")", "who", "had", "spent", "the", "war", "years", "in", "France", "."], "labels": ["O", "O", "O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, location, country, politician, organization, person, political party and O.\nSentence: A certain rivalry between Walter Ulbricht and Franz Dahlem had arisen in the 1930s , and after the war there were more general tensions within the national leadership between those - including Walter Ulbricht and President Pieck - who had spent the war years in Moscow and those ( such as Dahlem ) who had spent the war years in France .", "prompt_labels": "A(O) certain(O) rivalry(O) between(O) Walter(B-politician) Ulbricht(I-politician) and(O) Franz(B-politician) Dahlem(I-politician) had(O) arisen(O) in(O) the(O) 1930s(O) ,(O) and(O) after(O) the(O) war(O) there(O) were(O) more(O) general(O) tensions(O) within(O) the(O) national(O) leadership(O) between(O) those(O) -(O) including(O) Walter(B-politician) Ulbricht(I-politician) and(O) President(B-politician) Pieck(I-politician) -(O) who(O) had(O) spent(O) the(O) war(O) years(O) in(O) Moscow(B-location) and(O) those(O) ((O) such(O) as(O) Dahlem(B-location) )(O) who(O) had(O) spent(O) the(O) war(O) years(O) in(O) France(B-country) .(O)"}}
{"id": "122", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "political party", "election", "event", "country", "location", "organization", "politician"], "instance": {"id": "122", "words": ["Stanley", "(", "then", "known", "as", "the", "Honourable", "Edward", "Lyulph", "Stanley", ")", "contested", "Oldham", ",", "in", "the", "Liberal", "interest", ",", "at", "elections", "in", "1872", ",", "1874", "United", "Kingdom", "general", "election", ",", "1880", "United", "Kingdom", "general", "election", "and", "1885", "United", "Kingdom", "general", "election", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "B-location", "O", "O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, election, event, country, location, organization, politician and O.\nSentence: Stanley ( then known as the Honourable Edward Lyulph Stanley ) contested Oldham , in the Liberal interest , at elections in 1872 , 1874 United Kingdom general election , 1880 United Kingdom general election and 1885 United Kingdom general election .", "prompt_labels": "Stanley(B-politician) ((O) then(O) known(O) as(O) the(O) Honourable(O) Edward(B-politician) Lyulph(I-politician) Stanley(I-politician) )(O) contested(O) Oldham(B-location) ,(O) in(O) the(O) Liberal(B-political party) interest(O) ,(O) at(O) elections(O) in(O) 1872(O) ,(O) 1874(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) 1880(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 1885(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "120", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "politician", "location", "country", "person", "organization", "political party", "event"], "instance": {"id": "120", "words": ["Malaya", "united", "with", "Crown", "Colony", "of", "North", "Borneo", ",", "Crown", "Colony", "of", "Sarawak", ",", "and", "Colony", "of", "Singapore", "on", "16", "September", "1963", "to", "become", "Malaysia", "."], "labels": ["B-country", "O", "O", "B-country", "I-country", "I-country", "I-country", "I-country", "O", "B-country", "I-country", "I-country", "I-country", "O", "O", "B-country", "I-country", "I-country", "O", "O", "O", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, location, country, person, organization, political party, event and O.\nSentence: Malaya united with Crown Colony of North Borneo , Crown Colony of Sarawak , and Colony of Singapore on 16 September 1963 to become Malaysia .", "prompt_labels": "Malaya(B-country) united(O) with(O) Crown(B-country) Colony(I-country) of(I-country) North(I-country) Borneo(I-country) ,(O) Crown(B-country) Colony(I-country) of(I-country) Sarawak(I-country) ,(O) and(O) Colony(B-country) of(I-country) Singapore(I-country) on(O) 16(O) September(O) 1963(O) to(O) become(O) Malaysia(B-country) .(O)"}}
{"id": "354", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "election", "person", "location", "organization", "political party", "event", "country"], "instance": {"id": "354", "words": ["For", "example", ",", "in", "the", "four", "U.S.", "presidential", "elections", "contested", "from", "1988", "United", "States", "presidential", "election", "to", "2000", "United", "States", "presidential", "election", "inclusive", ",", "three", "sitting", "state", "governors", "were", "nominated", "for", "the", "presidency", ",", "these", "being", "Michael", "Dukakis", "in", "1988", ",", "Bill", "Clinton", "in", "1992", "United", "States", "presidential", "election", "and", "George", "W.", "Bush", "in", "2000", "(", "Clinton", "and", "Bush", "were", "elected", "president", ")", ",", "while", "in", "2016", "United", "States", "presidential", "election", "sitting", "governor", "Mike", "Pence", "was", "elected", "vice", "president", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "B-politician", "I-politician", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "B-politician", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, election, person, location, organization, political party, event, country and O.\nSentence: For example , in the four U.S. presidential elections contested from 1988 United States presidential election to 2000 United States presidential election inclusive , three sitting state governors were nominated for the presidency , these being Michael Dukakis in 1988 , Bill Clinton in 1992 United States presidential election and George W. Bush in 2000 ( Clinton and Bush were elected president ) , while in 2016 United States presidential election sitting governor Mike Pence was elected vice president .", "prompt_labels": "For(O) example(O) ,(O) in(O) the(O) four(O) U.S.(B-country) presidential(O) elections(O) contested(O) from(O) 1988(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) to(O) 2000(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) inclusive(O) ,(O) three(O) sitting(O) state(O) governors(O) were(O) nominated(O) for(O) the(O) presidency(O) ,(O) these(O) being(O) Michael(B-politician) Dukakis(I-politician) in(O) 1988(O) ,(O) Bill(B-politician) Clinton(I-politician) in(O) 1992(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) George(B-politician) W.(I-politician) Bush(I-politician) in(O) 2000(O) ((O) Clinton(B-politician) and(O) Bush(B-politician) were(O) elected(O) president(O) )(O) ,(O) while(O) in(O) 2016(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) sitting(O) governor(O) Mike(B-politician) Pence(I-politician) was(O) elected(O) vice(O) president(O) .(O)"}}
{"id": "353", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "country", "politician", "event", "political party", "location", "election", "person"], "instance": {"id": "353", "words": ["Campbell", "'s", "Progressive", "Conservatives", "and", "McLaughlin", "'s", "New", "Democratic", "Party", "were", "decimated", "in", "1993", ",", "both", "failing", "to", "reach", "official", "party", "status", ",", "and", "Lyn", "McLeod", "'", "s", "Ontario", "Liberal", "Party", "lost", "the", "1995", "Ontario", "general", "election", "despite", "having", "more", "than", "a", "10", "per", "cent", "lead", "in", "the", "polls", "when", "the", "election", "was", "called", "."], "labels": ["B-politician", "O", "B-political party", "I-political party", "O", "B-politician", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, politician, event, political party, location, election, person and O.\nSentence: Campbell 's Progressive Conservatives and McLaughlin 's New Democratic Party were decimated in 1993 , both failing to reach official party status , and Lyn McLeod ' s Ontario Liberal Party lost the 1995 Ontario general election despite having more than a 10 per cent lead in the polls when the election was called .", "prompt_labels": "Campbell(B-politician) 's(O) Progressive(B-political party) Conservatives(I-political party) and(O) McLaughlin(B-politician) 's(O) New(B-political party) Democratic(I-political party) Party(I-political party) were(O) decimated(O) in(O) 1993(O) ,(O) both(O) failing(O) to(O) reach(O) official(O) party(O) status(O) ,(O) and(O) Lyn(B-politician) McLeod(I-politician) '(O) s(O) Ontario(B-political party) Liberal(I-political party) Party(I-political party) lost(O) the(O) 1995(B-election) Ontario(I-election) general(I-election) election(I-election) despite(O) having(O) more(O) than(O) a(O) 10(O) per(O) cent(O) lead(O) in(O) the(O) polls(O) when(O) the(O) election(O) was(O) called(O) .(O)"}}
{"id": "242", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "location", "country", "political party", "person", "organization", "event", "election"], "instance": {"id": "242", "words": ["It", "was", "organised", "by", "Harry", "West", "and", "constituted", "a", "formal", "electoral", "pact", "between", "his", "Ulster", "Unionist", "Party", ",", "the", "Democratic", "Unionist", "Party", "and", "the", "Vanguard", "Unionist", "Progressive", "Party", "."], "labels": ["O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, country, political party, person, organization, event, election and O.\nSentence: It was organised by Harry West and constituted a formal electoral pact between his Ulster Unionist Party , the Democratic Unionist Party and the Vanguard Unionist Progressive Party .", "prompt_labels": "It(O) was(O) organised(O) by(O) Harry(B-politician) West(I-politician) and(O) constituted(O) a(O) formal(O) electoral(O) pact(O) between(O) his(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) ,(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) and(O) the(O) Vanguard(B-political party) Unionist(I-political party) Progressive(I-political party) Party(I-political party) .(O)"}}
{"id": "146", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "election", "event", "political party", "politician", "person", "country", "organization"], "instance": {"id": "146", "words": ["In", "Kota", "Kinabalu", ",", "United", "Pasokmomogun", "Kadazandusun", "Murut", "Organisation", "(", "UPKO", ")", "led", "by", "its", "Secretary-General", "Datuk", "Wilfred", "Madius", "Tangau", ",", "on", "23", "September", "2008", ",", "joined", "its", "3", "other", "Barisan", "Nasional", "(", "BN", ")", "counterparts", "Malaysian", "Chinese", "Association", ",", "Parti", "Gerakan", "Rakyat", "Malaysia", "and", "Malaysian", "Indian", "Congress", ",", "petitioning", "Government", "review", "of", "ISA", "."], "labels": ["O", "B-location", "I-location", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, election, event, political party, politician, person, country, organization and O.\nSentence: In Kota Kinabalu , United Pasokmomogun Kadazandusun Murut Organisation ( UPKO ) led by its Secretary-General Datuk Wilfred Madius Tangau , on 23 September 2008 , joined its 3 other Barisan Nasional ( BN ) counterparts Malaysian Chinese Association , Parti Gerakan Rakyat Malaysia and Malaysian Indian Congress , petitioning Government review of ISA .", "prompt_labels": "In(O) Kota(B-location) Kinabalu(I-location) ,(O) United(B-political party) Pasokmomogun(I-political party) Kadazandusun(I-political party) Murut(I-political party) Organisation(I-political party) ((O) UPKO(B-political party) )(O) led(O) by(O) its(O) Secretary-General(O) Datuk(O) Wilfred(B-politician) Madius(I-politician) Tangau(I-politician) ,(O) on(O) 23(O) September(O) 2008(O) ,(O) joined(O) its(O) 3(O) other(O) Barisan(B-political party) Nasional(I-political party) ((O) BN(B-political party) )(O) counterparts(O) Malaysian(B-political party) Chinese(I-political party) Association(I-political party) ,(O) Parti(B-political party) Gerakan(I-political party) Rakyat(I-political party) Malaysia(I-political party) and(O) Malaysian(B-political party) Indian(I-political party) Congress(I-political party) ,(O) petitioning(O) Government(O) review(O) of(O) ISA(O) .(O)"}}
{"id": "20", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "politician", "political party", "country", "event", "person", "election", "organization"], "instance": {"id": "20", "words": ["LIBRA", "then", "ran", "in", "the", "2003", "Croatian", "parliamentary", "election", "as", "a", "junior", "partner", "in", "a", "centre-left", "coalition", "with", "Social", "Democratic", "Party", "of", "Croatia", "(", "SDP", ")", ",", "Istrian", "Democratic", "Assembly", "(", "IDS", ")", "and", "the", "Liberal", "Party", "(", "LS", ")", "."], "labels": ["B-politician", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, political party, country, event, person, election, organization and O.\nSentence: LIBRA then ran in the 2003 Croatian parliamentary election as a junior partner in a centre-left coalition with Social Democratic Party of Croatia ( SDP ) , Istrian Democratic Assembly ( IDS ) and the Liberal Party ( LS ) .", "prompt_labels": "LIBRA(B-politician) then(O) ran(O) in(O) the(O) 2003(B-election) Croatian(I-election) parliamentary(I-election) election(I-election) as(O) a(O) junior(O) partner(O) in(O) a(O) centre-left(O) coalition(O) with(O) Social(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Croatia(I-political party) ((O) SDP(B-political party) )(O) ,(O) Istrian(B-political party) Democratic(I-political party) Assembly(I-political party) ((O) IDS(B-political party) )(O) and(O) the(O) Liberal(B-political party) Party(I-political party) ((O) LS(B-political party) )(O) .(O)"}}
{"id": "343", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "country", "politician", "person", "election", "location", "event", "organization"], "instance": {"id": "343", "words": ["In", "1800", "United", "States", "presidential", "election", ",", "1828", "United", "States", "presidential", "election", ",", "1840", "United", "States", "presidential", "election", "and", "1892", "United", "States", "presidential", "election", ",", "the", "victorious", "candidate", "had", "lost", "to", "the", "same", "opponent", "in", "the", "previous", "election", "."], "labels": ["O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, politician, person, election, location, event, organization and O.\nSentence: In 1800 United States presidential election , 1828 United States presidential election , 1840 United States presidential election and 1892 United States presidential election , the victorious candidate had lost to the same opponent in the previous election .", "prompt_labels": "In(O) 1800(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) 1828(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) 1840(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) 1892(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) the(O) victorious(O) candidate(O) had(O) lost(O) to(O) the(O) same(O) opponent(O) in(O) the(O) previous(O) election(O) .(O)"}}
{"id": "48", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "location", "person", "organization", "political party", "country", "politician", "election"], "instance": {"id": "48", "words": ["In", "the", "February", "1974", "United", "Kingdom", "general", "election", "the", "seat", "was", "won", "by", "John", "Carson", "of", "the", "Ulster", "Unionist", "Party", "with", "backing", "by", "the", "Vanguard", "Progressive", "Unionist", "Party", "and", "the", "Democratic", "Unionist", "Party", "on", "a", "united", "slate", "in", "opposition", "to", "the", "Sunningdale", "Agreement", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, person, organization, political party, country, politician, election and O.\nSentence: In the February 1974 United Kingdom general election the seat was won by John Carson of the Ulster Unionist Party with backing by the Vanguard Progressive Unionist Party and the Democratic Unionist Party on a united slate in opposition to the Sunningdale Agreement .", "prompt_labels": "In(O) the(O) February(B-election) 1974(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) the(O) seat(O) was(O) won(O) by(O) John(B-politician) Carson(I-politician) of(O) the(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) with(O) backing(O) by(O) the(O) Vanguard(B-political party) Progressive(I-political party) Unionist(I-political party) Party(I-political party) and(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) on(O) a(O) united(O) slate(O) in(O) opposition(O) to(O) the(O) Sunningdale(B-organization) Agreement(I-organization) .(O)"}}
{"id": "263", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "political party", "location", "person", "event", "organization", "election", "country"], "instance": {"id": "263", "words": ["Osborne", "was", "a", "free", "silver", "supporters", "and", "during", "the", "1896", "United", "States", "presidential", "election", ",", "1900", "United", "States", "presidential", "election", ",", "and", "1908", "United", "States", "presidential", "election", "he", "supported", "William", "Jennings", "Bryan", "."], "labels": ["B-person", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-politician", "I-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, location, person, event, organization, election, country and O.\nSentence: Osborne was a free silver supporters and during the 1896 United States presidential election , 1900 United States presidential election , and 1908 United States presidential election he supported William Jennings Bryan .", "prompt_labels": "Osborne(B-person) was(O) a(O) free(B-event) silver(I-event) supporters(O) and(O) during(O) the(O) 1896(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) 1900(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) and(O) 1908(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) he(O) supported(O) William(B-politician) Jennings(I-politician) Bryan(I-politician) .(O)"}}
{"id": "503", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "person", "event", "location", "election", "country", "politician", "organization"], "instance": {"id": "503", "words": ["Although", "the", "Kuomintang", "failed", "to", "eliminate", "the", "enemy", ",", "they", "did", "succeeded", "in", "eradicating", "the", "enemy", "and", "temporarily", "taking", "the", "enemy", "base", "in", "the", "region", ",", "though", "this", "success", "was", "negated", "by", "the", "political", "fallouts", "such", "as", "failing", "to", "achieve", "its", "original", "objective", "of", "exterminating", "the", "enemy", "within", "the", "region", "in", "this", "first", "campaign", "of", "the", "full", "scale", "Chinese", "Civil", "War", "after", "World", "War", "II", "."], "labels": ["O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, event, location, election, country, politician, organization and O.\nSentence: Although the Kuomintang failed to eliminate the enemy , they did succeeded in eradicating the enemy and temporarily taking the enemy base in the region , though this success was negated by the political fallouts such as failing to achieve its original objective of exterminating the enemy within the region in this first campaign of the full scale Chinese Civil War after World War II .", "prompt_labels": "Although(O) the(O) Kuomintang(B-political party) failed(O) to(O) eliminate(O) the(O) enemy(O) ,(O) they(O) did(O) succeeded(O) in(O) eradicating(O) the(O) enemy(O) and(O) temporarily(O) taking(O) the(O) enemy(O) base(O) in(O) the(O) region(O) ,(O) though(O) this(O) success(O) was(O) negated(O) by(O) the(O) political(O) fallouts(O) such(O) as(O) failing(O) to(O) achieve(O) its(O) original(O) objective(O) of(O) exterminating(O) the(O) enemy(O) within(O) the(O) region(O) in(O) this(O) first(O) campaign(O) of(O) the(O) full(O) scale(O) Chinese(B-event) Civil(I-event) War(I-event) after(O) World(B-event) War(I-event) II(I-event) .(O)"}}
{"id": "280", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "organization", "person", "politician", "political party", "event", "election", "location"], "instance": {"id": "280", "words": ["The", "day", "was", "organised", "by", "the", "Australian", "Council", "of", "Trade", "Unions", "(", "ACTU", ")", "and", "its", "state", ",", "territory", "and", "local", "affiliates", ",", "with", "the", "support", "of", "the", "Australian", "Labor", "Party", ",", "the", "Australian", "Greens", ",", "the", "Australian", "Democrats", ",", "and", "various", "other", "political", "and", "community", "organisations", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, person, politician, political party, event, election, location and O.\nSentence: The day was organised by the Australian Council of Trade Unions ( ACTU ) and its state , territory and local affiliates , with the support of the Australian Labor Party , the Australian Greens , the Australian Democrats , and various other political and community organisations .", "prompt_labels": "The(O) day(O) was(O) organised(O) by(O) the(O) Australian(B-organization) Council(I-organization) of(I-organization) Trade(I-organization) Unions(I-organization) ((O) ACTU(B-organization) )(O) and(O) its(O) state(O) ,(O) territory(O) and(O) local(O) affiliates(O) ,(O) with(O) the(O) support(O) of(O) the(O) Australian(B-political party) Labor(I-political party) Party(I-political party) ,(O) the(O) Australian(B-political party) Greens(I-political party) ,(O) the(O) Australian(B-political party) Democrats(I-political party) ,(O) and(O) various(O) other(O) political(O) and(O) community(O) organisations(O) .(O)"}}
{"id": "107", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "politician", "location", "organization", "election", "country", "event", "person"], "instance": {"id": "107", "words": ["The", "following", "parties", "have", "won", "the", "special", "seats", "reserved", "for", "national", "minority", "representatives", "(", "also", "in", "alphabetical", "order", ")", ":", "the", "Bosnian", "Democratic", "Party", "of", "Croatia", ",", "the", "Democratic", "Union", "of", "Hungarians", "of", "Croatia", ",", "the", "German", "People", "'s", "Union", "-", "National", "Association", "of", "Danube", "Swabians", "in", "Croatia", ",", "the", "Independent", "Democratic", "Serb", "Party", ",", "the", "Party", "of", "Democratic", "Action", "of", "Croatia", "and", "the", "Serb", "People", "'s", "Party", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, location, organization, election, country, event, person and O.\nSentence: The following parties have won the special seats reserved for national minority representatives ( also in alphabetical order ) : the Bosnian Democratic Party of Croatia , the Democratic Union of Hungarians of Croatia , the German People 's Union - National Association of Danube Swabians in Croatia , the Independent Democratic Serb Party , the Party of Democratic Action of Croatia and the Serb People 's Party .", "prompt_labels": "The(O) following(O) parties(O) have(O) won(O) the(O) special(O) seats(O) reserved(O) for(O) national(O) minority(O) representatives(O) ((O) also(O) in(O) alphabetical(O) order(O) )(O) :(O) the(O) Bosnian(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Croatia(I-political party) ,(O) the(O) Democratic(B-political party) Union(I-political party) of(I-political party) Hungarians(I-political party) of(I-political party) Croatia(I-political party) ,(O) the(O) German(B-political party) People(I-political party) 's(I-political party) Union(I-political party) -(I-political party) National(I-political party) Association(I-political party) of(I-political party) Danube(I-political party) Swabians(I-political party) in(I-political party) Croatia(I-political party) ,(O) the(O) Independent(B-political party) Democratic(I-political party) Serb(I-political party) Party(I-political party) ,(O) the(O) Party(B-political party) of(I-political party) Democratic(I-political party) Action(I-political party) of(I-political party) Croatia(I-political party) and(O) the(O) Serb(B-political party) People(I-political party) 's(I-political party) Party(I-political party) .(O)"}}
{"id": "431", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "person", "election", "organization", "political party", "country", "location", "event"], "instance": {"id": "431", "words": ["In", "the", "United", "States", ",", "organizations", "that", "promote", "accommodationism", "include", "The", "Becket", "Fund", "for", "Religious", "Liberty", ",", "Foundation", "for", "Moral", "Law", ",", "Lord", "'s", "Day", "Alliance", ",", "Alliance", "Defending", "Freedom", ",", "Christian", "Coalition", "of", "America", ",", "Woman", "'s", "Christian", "Temperance", "Union", ",", "and", "the", "First", "Liberty", "Institute", "."], "labels": ["O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, person, election, organization, political party, country, location, event and O.\nSentence: In the United States , organizations that promote accommodationism include The Becket Fund for Religious Liberty , Foundation for Moral Law , Lord 's Day Alliance , Alliance Defending Freedom , Christian Coalition of America , Woman 's Christian Temperance Union , and the First Liberty Institute .", "prompt_labels": "In(O) the(O) United(B-country) States(I-country) ,(O) organizations(O) that(O) promote(O) accommodationism(O) include(O) The(B-organization) Becket(I-organization) Fund(I-organization) for(I-organization) Religious(I-organization) Liberty(I-organization) ,(O) Foundation(B-organization) for(I-organization) Moral(I-organization) Law(I-organization) ,(O) Lord(B-organization) 's(I-organization) Day(I-organization) Alliance(I-organization) ,(O) Alliance(B-organization) Defending(I-organization) Freedom(I-organization) ,(O) Christian(B-organization) Coalition(I-organization) of(I-organization) America(I-organization) ,(O) Woman(B-organization) 's(I-organization) Christian(I-organization) Temperance(I-organization) Union(I-organization) ,(O) and(O) the(O) First(B-organization) Liberty(I-organization) Institute(I-organization) .(O)"}}
{"id": "361", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "organization", "politician", "election", "country", "person", "political party", "location"], "instance": {"id": "361", "words": ["Nova", "Civitas", "also", "proposed", "a", "regrouping", "of", "political", "forces", "into", "what", "would", "have", "become", "a", "de", "facto", "two-party", "system", ",", "with", "a", "big", "centre-right", "/", "right-wing", "political", "party", "alliance", "in", "Flanders", ",", "including", "VLD", ",", "Christen-Democratisch", "en", "Vlaams", ",", "New", "Flemish", "Alliance", "and", "Vlaams", "Belang", "."], "labels": ["B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "B-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, politician, election, country, person, political party, location and O.\nSentence: Nova Civitas also proposed a regrouping of political forces into what would have become a de facto two-party system , with a big centre-right / right-wing political party alliance in Flanders , including VLD , Christen-Democratisch en Vlaams , New Flemish Alliance and Vlaams Belang .", "prompt_labels": "Nova(B-politician) Civitas(I-politician) also(O) proposed(O) a(O) regrouping(O) of(O) political(O) forces(O) into(O) what(O) would(O) have(O) become(O) a(O) de(O) facto(O) two-party(O) system(O) ,(O) with(O) a(O) big(O) centre-right(O) /(O) right-wing(O) political(O) party(O) alliance(O) in(O) Flanders(B-location) ,(O) including(O) VLD(B-political party) ,(O) Christen-Democratisch(B-political party) en(I-political party) Vlaams(I-political party) ,(O) New(B-political party) Flemish(I-political party) Alliance(I-political party) and(O) Vlaams(B-political party) Belang(I-political party) .(O)"}}
{"id": "389", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "election", "organization", "politician", "person", "political party", "event", "location"], "instance": {"id": "389", "words": ["Dianne", "Feinstein", "won", "a", "1992", "United", "States", "Senate", "special", "election", "in", "California", "to", "fill", "the", "seat", "of", "Governor", "Pete", "Wilson", "."], "labels": ["B-politician", "I-politician", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, organization, politician, person, political party, event, location and O.\nSentence: Dianne Feinstein won a 1992 United States Senate special election in California to fill the seat of Governor Pete Wilson .", "prompt_labels": "Dianne(B-politician) Feinstein(I-politician) won(O) a(O) 1992(B-election) United(I-election) States(I-election) Senate(I-election) special(I-election) election(I-election) in(I-election) California(I-election) to(O) fill(O) the(O) seat(O) of(O) Governor(O) Pete(B-politician) Wilson(I-politician) .(O)"}}
{"id": "5", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "person", "location", "political party", "politician", "election", "organization", "event"], "instance": {"id": "5", "words": ["Prior", "to", "the", "1992", "elections", "Shinui", "merged", "with", "Shulamit", "Aloni", "'", "s", "Ratz", "and", "Zionist", "-", "socialist", "Mapam", "to", "form", "Meretz", ",", "a", "dovish", ",", "social-democratic", "liberal", "party", "."], "labels": ["O", "O", "O", "O", "O", "B-political party", "O", "O", "B-person", "I-person", "O", "O", "B-political party", "O", "O", "O", "O", "B-political party", "O", "O", "B-political party", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, location, political party, politician, election, organization, event and O.\nSentence: Prior to the 1992 elections Shinui merged with Shulamit Aloni ' s Ratz and Zionist - socialist Mapam to form Meretz , a dovish , social-democratic liberal party .", "prompt_labels": "Prior(O) to(O) the(O) 1992(O) elections(O) Shinui(B-political party) merged(O) with(O) Shulamit(B-person) Aloni(I-person) '(O) s(O) Ratz(B-political party) and(O) Zionist(O) -(O) socialist(O) Mapam(B-political party) to(O) form(O) Meretz(B-political party) ,(O) a(O) dovish(O) ,(O) social-democratic(O) liberal(B-political party) party(I-political party) .(O)"}}
{"id": "441", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "event", "politician", "location", "person", "organization", "political party", "election"], "instance": {"id": "441", "words": ["The", "group", "was", "one", "of", "a", "number", "of", "far-right", "Islamophobic", "groups", ",", "including", "the", "Q", "Society", ",", "Reclaim", "Australia", ",", "TRUE", "Blue", "Crew", "and", "the", "United", "Patriots", "Front", ",", "that", "opposed", "the", "construction", "of", "a", "$", "3", "million", "mosque", "and", "Islamic", "community", "centre", "in", "Bendigo", ",", "Victoria", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, politician, location, person, organization, political party, election and O.\nSentence: The group was one of a number of far-right Islamophobic groups , including the Q Society , Reclaim Australia , TRUE Blue Crew and the United Patriots Front , that opposed the construction of a $ 3 million mosque and Islamic community centre in Bendigo , Victoria .", "prompt_labels": "The(O) group(O) was(O) one(O) of(O) a(O) number(O) of(O) far-right(O) Islamophobic(B-organization) groups(O) ,(O) including(O) the(O) Q(B-organization) Society(I-organization) ,(O) Reclaim(B-organization) Australia(I-organization) ,(O) TRUE(B-organization) Blue(I-organization) Crew(I-organization) and(O) the(O) United(B-organization) Patriots(I-organization) Front(I-organization) ,(O) that(O) opposed(O) the(O) construction(O) of(O) a(O) $(O) 3(O) million(O) mosque(O) and(O) Islamic(O) community(O) centre(O) in(O) Bendigo(B-location) ,(O) Victoria(B-location) .(O)"}}
{"id": "307", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "organization", "political party", "event", "location", "election", "country", "person"], "instance": {"id": "307", "words": ["The", "bill", "was", "passed", "with", "the", "SLFP", "and", "the", "UNP", "supporting", "it", ",", "with", "the", "leftist", "LSSP", "and", "Communist", "Party", "of", "Sri", "Lanka", "as", "well", "as", "the", "Tamil", "nationalist", "parties", "(", "Illankai", "Tamil", "Arasu", "Kachchi", "and", "All", "Ceylon", "Tamil", "Congress", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, political party, event, location, election, country, person and O.\nSentence: The bill was passed with the SLFP and the UNP supporting it , with the leftist LSSP and Communist Party of Sri Lanka as well as the Tamil nationalist parties ( Illankai Tamil Arasu Kachchi and All Ceylon Tamil Congress ) .", "prompt_labels": "The(O) bill(O) was(O) passed(O) with(O) the(O) SLFP(B-political party) and(O) the(O) UNP(B-organization) supporting(O) it(O) ,(O) with(O) the(O) leftist(O) LSSP(B-political party) and(O) Communist(B-political party) Party(I-political party) of(I-political party) Sri(I-political party) Lanka(I-political party) as(O) well(O) as(O) the(O) Tamil(O) nationalist(O) parties(O) ((O) Illankai(B-political party) Tamil(I-political party) Arasu(I-political party) Kachchi(I-political party) and(O) All(B-political party) Ceylon(I-political party) Tamil(I-political party) Congress(I-political party) )(O) .(O)"}}
{"id": "148", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "country", "politician", "event", "election", "location", "organization", "political party"], "instance": {"id": "148", "words": ["Political", "scientists", "see", "European", "political", "parties", "such", "as", "Ecolo", "and", "Groen", "in", "Belgium", ",", "Alliance", "90", "/", "The", "Greens", "in", "Germany", ",", "or", "the", "Green", "Progressive", "Accord", "and", "GroenLinks", "in", "the", "Netherlands", "as", "coming", "out", "of", "the", "New", "Left", "and", "emphasizing", "spontaneous", "self-organization", ",", "participatory", "democracy", ",", "decentralization", "and", "voluntarism", ",", "being", "contrasted", "to", "the", "bureaucratic", "or", "statist", "approach", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "B-political party", "O", "B-country", "O", "B-political party", "I-political party", "O", "B-political party", "I-political party", "O", "B-country", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "B-country", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, politician, event, election, location, organization, political party and O.\nSentence: Political scientists see European political parties such as Ecolo and Groen in Belgium , Alliance 90 / The Greens in Germany , or the Green Progressive Accord and GroenLinks in the Netherlands as coming out of the New Left and emphasizing spontaneous self-organization , participatory democracy , decentralization and voluntarism , being contrasted to the bureaucratic or statist approach .", "prompt_labels": "Political(O) scientists(O) see(O) European(O) political(O) parties(O) such(O) as(O) Ecolo(B-political party) and(O) Groen(B-political party) in(O) Belgium(B-country) ,(O) Alliance(B-political party) 90(I-political party) /(O) The(B-political party) Greens(I-political party) in(O) Germany(B-country) ,(O) or(O) the(O) Green(B-political party) Progressive(I-political party) Accord(I-political party) and(O) GroenLinks(B-political party) in(O) the(O) Netherlands(B-country) as(O) coming(O) out(O) of(O) the(O) New(B-event) Left(I-event) and(O) emphasizing(O) spontaneous(O) self-organization(O) ,(O) participatory(O) democracy(O) ,(O) decentralization(O) and(O) voluntarism(O) ,(O) being(O) contrasted(O) to(O) the(O) bureaucratic(O) or(O) statist(O) approach(O) .(O)"}}
{"id": "511", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "political party", "organization", "election", "location", "event", "politician", "country"], "instance": {"id": "511", "words": ["Brimming", "with", "confidence", "after", "their", "victories", "over", "the", "Russians", "in", "1711", "(", "Pruth", "River", "Campaign", ")", "and", "over", "the", "Venetians", "in", "1715", "(", "Ottoman-Venetian", "War", "(", "1714-1718", ")", ")", ",", "the", "Ottomans", "declared", "war", "on", "the", "Habsburg", "Monarchy", "in", "1716", "and", "marched", "north", "from", "Belgrade", "in", "July", "under", "the", "command", "of", "Grand", "Vizier", "Silahdar", "Damat", "Ali", "Pasha", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "I-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, organization, election, location, event, politician, country and O.\nSentence: Brimming with confidence after their victories over the Russians in 1711 ( Pruth River Campaign ) and over the Venetians in 1715 ( Ottoman-Venetian War ( 1714-1718 ) ) , the Ottomans declared war on the Habsburg Monarchy in 1716 and marched north from Belgrade in July under the command of Grand Vizier Silahdar Damat Ali Pasha .", "prompt_labels": "Brimming(O) with(O) confidence(O) after(O) their(O) victories(O) over(O) the(O) Russians(O) in(O) 1711(O) ((O) Pruth(B-event) River(I-event) Campaign(I-event) )(O) and(O) over(O) the(O) Venetians(O) in(O) 1715(O) ((O) Ottoman-Venetian(B-event) War(I-event) ((O) 1714-1718(O) )(O) )(O) ,(O) the(O) Ottomans(B-politician) declared(O) war(O) on(O) the(O) Habsburg(O) Monarchy(O) in(O) 1716(O) and(O) marched(O) north(O) from(O) Belgrade(B-location) in(O) July(O) under(O) the(O) command(O) of(O) Grand(O) Vizier(O) Silahdar(B-politician) Damat(I-politician) Ali(I-politician) Pasha(I-politician) .(O)"}}
{"id": "85", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "person", "country", "politician", "organization", "event", "political party", "election"], "instance": {"id": "85", "words": ["She", "was", "a", "candidate", "in", "the", "1984", "United", "States", "presidential", "election", ",", "1992", "United", "States", "presidential", "election", "(", "339", "votes", ")", ",", "1996", "United", "States", "presidential", "election", ",", "and", "2004", "United", "States", "presidential", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, country, politician, organization, event, political party, election and O.\nSentence: She was a candidate in the 1984 United States presidential election , 1992 United States presidential election ( 339 votes ) , 1996 United States presidential election , and 2004 United States presidential election .", "prompt_labels": "She(O) was(O) a(O) candidate(O) in(O) the(O) 1984(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) 1992(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ((O) 339(O) votes(O) )(O) ,(O) 1996(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) and(O) 2004(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) .(O)"}}
{"id": "261", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "political party", "organization", "location", "event", "politician", "election", "person"], "instance": {"id": "261", "words": ["Since", "2008", "it", "was", "part", "of", "the", "Sammarinese", "Union", "of", "Moderates", "together", "with", "Sammarinese", "Populars", "and", "stood", "in", "opposition", "to", "the", "2006-2008", "coalition", "government", "consisting", "of", "the", "Party", "of", "Socialists", "and", "Democrats", ",", "the", "Popular", "Alliance", "and", "the", "United", "Left", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, organization, location, event, politician, election, person and O.\nSentence: Since 2008 it was part of the Sammarinese Union of Moderates together with Sammarinese Populars and stood in opposition to the 2006-2008 coalition government consisting of the Party of Socialists and Democrats , the Popular Alliance and the United Left .", "prompt_labels": "Since(O) 2008(O) it(O) was(O) part(O) of(O) the(O) Sammarinese(B-political party) Union(I-political party) of(I-political party) Moderates(I-political party) together(O) with(O) Sammarinese(B-political party) Populars(I-political party) and(O) stood(O) in(O) opposition(O) to(O) the(O) 2006-2008(O) coalition(O) government(O) consisting(O) of(O) the(O) Party(B-political party) of(I-political party) Socialists(I-political party) and(I-political party) Democrats(I-political party) ,(O) the(O) Popular(B-political party) Alliance(I-political party) and(O) the(O) United(B-organization) Left(I-organization) .(O)"}}
{"id": "23", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "country", "organization", "event", "person", "politician", "political party", "location"], "instance": {"id": "23", "words": ["It", "was", "succeeded", "in", "the", "Flemish", "Community", "of", "Belgium", "by", "the", "Open", "Vlaamse", "Liberalen", "en", "Democraten", "(", "VLD", ")", "and", "in", "the", "French", "Community", "by", "the", "Liberal", "Reformist", "Party", ",", "Parti", "des", "Réformes", "et", "des", "Libertés", "de", "Wallonie", "and", "the", "current-day", "Mouvement", "Réformateur", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-location", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, country, organization, event, person, politician, political party, location and O.\nSentence: It was succeeded in the Flemish Community of Belgium by the Open Vlaamse Liberalen en Democraten ( VLD ) and in the French Community by the Liberal Reformist Party , Parti des Réformes et des Libertés de Wallonie and the current-day Mouvement Réformateur .", "prompt_labels": "It(O) was(O) succeeded(O) in(O) the(O) Flemish(B-organization) Community(I-organization) of(O) Belgium(B-location) by(O) the(O) Open(B-political party) Vlaamse(I-political party) Liberalen(I-political party) en(I-political party) Democraten(I-political party) ((O) VLD(B-political party) )(O) and(O) in(O) the(O) French(B-country) Community(I-country) by(O) the(O) Liberal(B-political party) Reformist(I-political party) Party(I-political party) ,(O) Parti(B-political party) des(I-political party) Réformes(I-political party) et(I-political party) des(I-political party) Libertés(I-political party) de(I-political party) Wallonie(I-political party) and(O) the(O) current-day(O) Mouvement(B-political party) Réformateur(I-political party) .(O)"}}
{"id": "391", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "person", "election", "location", "country", "politician", "political party", "event"], "instance": {"id": "391", "words": ["Finally", ",", "the", "1998", "South", "Carolina", "GOP", "ticket", "was", "dragged", "down", "with", "unpopular", "Governor", "David", "Beasley", "at", "the", "top", "of", "the", "ticket", "who", "would", "go", "on", "to", "lose", "his", "1998", "South", "Carolina", "gubernatorial", "election", "to", "Jim", "Hodges", "."], "labels": ["O", "O", "O", "O", "B-location", "I-location", "B-political party", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-politician", "I-politician", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, election, location, country, politician, political party, event and O.\nSentence: Finally , the 1998 South Carolina GOP ticket was dragged down with unpopular Governor David Beasley at the top of the ticket who would go on to lose his 1998 South Carolina gubernatorial election to Jim Hodges .", "prompt_labels": "Finally(O) ,(O) the(O) 1998(O) South(B-location) Carolina(I-location) GOP(B-political party) ticket(O) was(O) dragged(O) down(O) with(O) unpopular(O) Governor(O) David(B-politician) Beasley(I-politician) at(O) the(O) top(O) of(O) the(O) ticket(O) who(O) would(O) go(O) on(O) to(O) lose(O) his(O) 1998(B-election) South(I-election) Carolina(I-election) gubernatorial(I-election) election(I-election) to(O) Jim(B-politician) Hodges(I-politician) .(O)"}}
{"id": "509", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "organization", "event", "election", "location", "country", "person", "politician"], "instance": {"id": "509", "words": ["The", "party", "seeks", "the", "removal", "of", "Northern", "Ireland", "from", "the", "United", "Kingdom", ",", "the", "Right2Water", "Campaign", ",", "the", "campaign", "to", "Repeal", "The", "8th", "Amendment", ",", "and", "their", "Public", "Housing", "For", "all", "campaign", ",", "which", "calls", "for", "the", "state", "to", "introduce", "a", "housing", "system", "where", "all", "citizens", "have", "the", "legal", "right", "to", "rent", "a", "high-quality", ",", "affordable", "home", "regardless", "of", "their", "income", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "B-country", "I-country", "O", "O", "B-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, event, election, location, country, person, politician and O.\nSentence: The party seeks the removal of Northern Ireland from the United Kingdom , the Right2Water Campaign , the campaign to Repeal The 8th Amendment , and their Public Housing For all campaign , which calls for the state to introduce a housing system where all citizens have the legal right to rent a high-quality , affordable home regardless of their income .", "prompt_labels": "The(O) party(O) seeks(O) the(O) removal(O) of(O) Northern(B-country) Ireland(I-country) from(O) the(O) United(B-country) Kingdom(I-country) ,(O) the(O) Right2Water(B-event) Campaign(I-event) ,(O) the(O) campaign(B-event) to(I-event) Repeal(I-event) The(I-event) 8th(I-event) Amendment(I-event) ,(O) and(O) their(O) Public(O) Housing(O) For(O) all(O) campaign(O) ,(O) which(O) calls(O) for(O) the(O) state(O) to(O) introduce(O) a(O) housing(O) system(O) where(O) all(O) citizens(O) have(O) the(O) legal(O) right(O) to(O) rent(O) a(O) high-quality(O) ,(O) affordable(O) home(O) regardless(O) of(O) their(O) income(O) .(O)"}}
{"id": "338", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "person", "location", "event", "politician", "country", "election", "political party"], "instance": {"id": "338", "words": ["The", "Party", "stood", "one", "candidate", ",", "John", "Morris", ",", "in", "the", "1997", "United", "Kingdom", "general", "election", "and", "2001", "United", "Kingdom", "general", "election", "in", "the", "Guildford", "constituency", ",", "and", "two", "in", "the", "2005", "United", "Kingdom", "general", "election", "with", "Caroline", "O", "'Reilly", "also", "standing", "in", "Brighton", "Kemptown", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, location, event, politician, country, election, political party and O.\nSentence: The Party stood one candidate , John Morris , in the 1997 United Kingdom general election and 2001 United Kingdom general election in the Guildford constituency , and two in the 2005 United Kingdom general election with Caroline O 'Reilly also standing in Brighton Kemptown .", "prompt_labels": "The(O) Party(O) stood(O) one(O) candidate(O) ,(O) John(B-politician) Morris(I-politician) ,(O) in(O) the(O) 1997(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) 2001(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) in(O) the(O) Guildford(B-location) constituency(O) ,(O) and(O) two(O) in(O) the(O) 2005(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) with(O) Caroline(O) O(O) 'Reilly(O) also(O) standing(O) in(O) Brighton(B-location) Kemptown(I-location) .(O)"}}
{"id": "15", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "organization", "election", "country", "person", "political party", "politician", "event"], "instance": {"id": "15", "words": ["The", "decline", "was", "evident", "even", "before", "the", "1981", "Northern", "Ireland", "local", "elections", "as", "4", "of", "the", "12", "UUUP", "councillors", "elected", "in", "1977", "Northern", "Ireland", "local", "elections", "had", "defected", "to", "other", "Unionist", "parties", "(", "2", "to", "UUP", ",", "1", "to", "DUP", "and", "1", "to", "the", "Ulster", "Popular", "Unionist", "Party", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "B-political party", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, election, country, person, political party, politician, event and O.\nSentence: The decline was evident even before the 1981 Northern Ireland local elections as 4 of the 12 UUUP councillors elected in 1977 Northern Ireland local elections had defected to other Unionist parties ( 2 to UUP , 1 to DUP and 1 to the Ulster Popular Unionist Party ) .", "prompt_labels": "The(O) decline(O) was(O) evident(O) even(O) before(O) the(O) 1981(B-election) Northern(I-election) Ireland(I-election) local(I-election) elections(I-election) as(O) 4(O) of(O) the(O) 12(O) UUUP(B-political party) councillors(O) elected(O) in(O) 1977(B-election) Northern(I-election) Ireland(I-election) local(I-election) elections(I-election) had(O) defected(O) to(O) other(O) Unionist(O) parties(O) ((O) 2(O) to(O) UUP(B-political party) ,(O) 1(O) to(O) DUP(B-political party) and(B-political party) 1(O) to(O) the(O) Ulster(B-political party) Popular(I-political party) Unionist(I-political party) Party(I-political party) )(O) .(O)"}}
{"id": "513", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "election", "event", "location", "country", "organization", "politician", "person"], "instance": {"id": "513", "words": ["The", "Long", "War", "is", "a", "name", "proposed", "by", "Philip", "Bobbitt", "in", "The", "Shield", "of", "Achilles", ":", "War", ",", "Peace", ",", "and", "the", "Course", "of", "History", "to", "describe", "the", "series", "of", "major", "conflicts", "fought", "from", "the", "start", "of", "the", "First", "World", "War", "in", "1914", "to", "the", "decline", "of", "the", "Soviet", "Union", "in", "1990", "."], "labels": ["O", "B-event", "I-event", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, event, location, country, organization, politician, person and O.\nSentence: The Long War is a name proposed by Philip Bobbitt in The Shield of Achilles : War , Peace , and the Course of History to describe the series of major conflicts fought from the start of the First World War in 1914 to the decline of the Soviet Union in 1990 .", "prompt_labels": "The(O) Long(B-event) War(I-event) is(O) a(O) name(O) proposed(O) by(O) Philip(B-person) Bobbitt(I-person) in(O) The(O) Shield(O) of(O) Achilles(O) :(O) War(O) ,(O) Peace(O) ,(O) and(O) the(O) Course(O) of(O) History(O) to(O) describe(O) the(O) series(O) of(O) major(O) conflicts(O) fought(O) from(O) the(O) start(O) of(O) the(O) First(B-event) World(I-event) War(I-event) in(O) 1914(O) to(O) the(O) decline(O) of(O) the(O) Soviet(B-country) Union(I-country) in(O) 1990(O) .(O)"}}
{"id": "265", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "location", "country", "political party", "person", "organization", "election", "event"], "instance": {"id": "265", "words": ["DeFeria", "also", "ran", "in", "the", "2000", "Canadian", "federal", "election", "in", "the", "riding", "of", "Mississauga", "East", "for", "the", "Progressive", "Conservative", "Party", "of", "Canada", "finishing", "a", "close", "third", "behind", "Jainstien", "Dookie", "of", "the", "Canadian", "Alliance", "and", "the", "winner", "Albina", "Guarnieri", "of", "the", "Liberal", "Party", "of", "Canada", "."], "labels": ["B-politician", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, country, political party, person, organization, election, event and O.\nSentence: DeFeria also ran in the 2000 Canadian federal election in the riding of Mississauga East for the Progressive Conservative Party of Canada finishing a close third behind Jainstien Dookie of the Canadian Alliance and the winner Albina Guarnieri of the Liberal Party of Canada .", "prompt_labels": "DeFeria(B-politician) also(O) ran(O) in(O) the(O) 2000(B-election) Canadian(I-election) federal(I-election) election(I-election) in(O) the(O) riding(O) of(O) Mississauga(B-location) East(I-location) for(O) the(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) finishing(O) a(O) close(O) third(O) behind(O) Jainstien(B-politician) Dookie(I-politician) of(O) the(O) Canadian(B-political party) Alliance(I-political party) and(O) the(O) winner(O) Albina(B-politician) Guarnieri(I-politician) of(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) .(O)"}}
{"id": "236", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "election", "organization", "location", "person", "politician", "country", "political party"], "instance": {"id": "236", "words": ["The", "California", "Democratic", "Party", ",", "the", "California", "Republican", "Party", ",", "the", "Libertarian", "Party", "of", "California", ",", "and", "the", "Peace", "and", "Freedom", "Party", "have", "historically", "prohibited", "nonmembers", "from", "voting", "in", "their", "party", "'s", "primary", "."], "labels": ["O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, organization, location, person, politician, country, political party and O.\nSentence: The California Democratic Party , the California Republican Party , the Libertarian Party of California , and the Peace and Freedom Party have historically prohibited nonmembers from voting in their party 's primary .", "prompt_labels": "The(O) California(B-political party) Democratic(I-political party) Party(I-political party) ,(O) the(O) California(B-political party) Republican(I-political party) Party(I-political party) ,(O) the(O) Libertarian(B-political party) Party(I-political party) of(I-political party) California(I-political party) ,(O) and(O) the(O) Peace(B-political party) and(I-political party) Freedom(I-political party) Party(I-political party) have(O) historically(O) prohibited(O) nonmembers(O) from(O) voting(O) in(O) their(O) party(O) 's(O) primary(O) .(O)"}}
{"id": "526", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "event", "person", "location", "organization", "politician", "political party", "country"], "instance": {"id": "526", "words": ["The", "National", "War", "Labor", "Board", ",", "commonly", "the", "War", "Labor", "Board", "(", "NWLB", "or", "WLB", ")", "was", "an", "agency", "of", "the", "United", "States", "government", "established", "January", "12", ",", "1942", "by", "executive", "order", "to", "mediate", "labor", "disputes", "during", "World", "War", "II", "."], "labels": ["O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, person, location, organization, politician, political party, country and O.\nSentence: The National War Labor Board , commonly the War Labor Board ( NWLB or WLB ) was an agency of the United States government established January 12 , 1942 by executive order to mediate labor disputes during World War II .", "prompt_labels": "The(O) National(B-organization) War(I-organization) Labor(I-organization) Board(I-organization) ,(O) commonly(O) the(O) War(B-organization) Labor(I-organization) Board(I-organization) ((O) NWLB(B-organization) or(O) WLB(B-organization) )(O) was(O) an(O) agency(O) of(O) the(O) United(B-country) States(I-country) government(O) established(O) January(O) 12(O) ,(O) 1942(O) by(O) executive(O) order(O) to(O) mediate(O) labor(O) disputes(O) during(O) World(B-event) War(I-event) II(I-event) .(O)"}}
{"id": "138", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "election", "politician", "location", "organization", "political party", "country", "person"], "instance": {"id": "138", "words": ["However", ",", "in", "the", "2004", "Canadian", "federal", "election", ",", "2006", "Canadian", "federal", "election", ",", "2008", "Canadian", "federal", "election", "and", "2011", "Canadian", "federal", "election", ",", "the", "entire", "region", ",", "incrementally", "swung", "away", "from", "the", "Liberals", "to", "support", "the", "Conservative", "Party", "of", "Canada", ",", "including", "in", "London", ",", "where", "three-way", "vote-splitting", "resulted", "in", "two", "ridings", "switching", "from", "Liberal", "to", "Conservative", "."], "labels": ["O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "B-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, politician, location, organization, political party, country, person and O.\nSentence: However , in the 2004 Canadian federal election , 2006 Canadian federal election , 2008 Canadian federal election and 2011 Canadian federal election , the entire region , incrementally swung away from the Liberals to support the Conservative Party of Canada , including in London , where three-way vote-splitting resulted in two ridings switching from Liberal to Conservative .", "prompt_labels": "However(O) ,(O) in(O) the(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2008(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 2011(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) the(O) entire(O) region(O) ,(O) incrementally(O) swung(O) away(O) from(O) the(O) Liberals(B-political party) to(O) support(O) the(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) ,(O) including(O) in(O) London(B-location) ,(O) where(O) three-way(O) vote-splitting(O) resulted(O) in(O) two(O) ridings(O) switching(O) from(O) Liberal(B-political party) to(O) Conservative(B-political party) .(O)"}}
{"id": "241", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "country", "political party", "election", "event", "location", "politician", "person"], "instance": {"id": "241", "words": ["Wolfe", "has", "been", "the", "recipient", "of", "grants", "from", "the", "Russell", "Sage", "Foundation", ",", "the", "Templeton", "Foundation", ",", "the", "Smith", "Richardson", "Foundation", ",", "the", "Carnegie", "Corporation", "of", "New", "York", ",", "and", "the", "Lilly", "Endowment", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, political party, election, event, location, politician, person and O.\nSentence: Wolfe has been the recipient of grants from the Russell Sage Foundation , the Templeton Foundation , the Smith Richardson Foundation , the Carnegie Corporation of New York , and the Lilly Endowment .", "prompt_labels": "Wolfe(B-person) has(O) been(O) the(O) recipient(O) of(O) grants(O) from(O) the(O) Russell(B-organization) Sage(I-organization) Foundation(I-organization) ,(O) the(O) Templeton(B-organization) Foundation(I-organization) ,(O) the(O) Smith(B-organization) Richardson(I-organization) Foundation(I-organization) ,(O) the(O) Carnegie(B-organization) Corporation(I-organization) of(I-organization) New(I-organization) York(I-organization) ,(O) and(O) the(O) Lilly(B-organization) Endowment(I-organization) .(O)"}}
{"id": "114", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "organization", "politician", "political party", "person", "location", "event", "election"], "instance": {"id": "114", "words": ["Hisense", "has", "13", "manufacturing", "facilities", "in", "China", "(", "located", "in", "the", "provinces", "/", "cities", "of", ":", "Guangdong", ",", "Guizhou", ",", "Huzhou", ",", "Jiangsu", ",", "Liaoning", ",", "Linyi", ",", "Shandong", ",", "Sichuan", ",", "Yangzhou", ",", "Yingkou", ",", "Xinjiang", ",", "Zibo", "and", "the", "municipality", "of", "Beijing", ")", "and", "several", "outside", "China", ",", "namely", "in", "Hungary", ",", "South", "Africa", ",", "Egypt", ",", "Algeria", ",", "Slovenia", ",", "France", "and", "Mexico", "."], "labels": ["B-organization", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "B-location", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-country", "I-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, politician, political party, person, location, event, election and O.\nSentence: Hisense has 13 manufacturing facilities in China ( located in the provinces / cities of : Guangdong , Guizhou , Huzhou , Jiangsu , Liaoning , Linyi , Shandong , Sichuan , Yangzhou , Yingkou , Xinjiang , Zibo and the municipality of Beijing ) and several outside China , namely in Hungary , South Africa , Egypt , Algeria , Slovenia , France and Mexico .", "prompt_labels": "Hisense(B-organization) has(O) 13(O) manufacturing(O) facilities(O) in(O) China(B-country) ((O) located(O) in(O) the(O) provinces(O) /(O) cities(O) of(O) :(O) Guangdong(B-location) ,(O) Guizhou(B-location) ,(O) Huzhou(B-location) ,(O) Jiangsu(B-location) ,(O) Liaoning(B-location) ,(O) Linyi(B-location) ,(O) Shandong(B-location) ,(O) Sichuan(B-location) ,(O) Yangzhou(B-location) ,(O) Yingkou(B-location) ,(O) Xinjiang(B-location) ,(O) Zibo(B-location) and(O) the(O) municipality(O) of(O) Beijing(B-location) )(O) and(O) several(O) outside(O) China(O) ,(O) namely(O) in(O) Hungary(B-country) ,(O) South(B-country) Africa(I-country) ,(O) Egypt(B-country) ,(O) Algeria(B-country) ,(O) Slovenia(B-country) ,(O) France(B-country) and(O) Mexico(B-country) .(O)"}}
{"id": "303", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "location", "political party", "country", "election", "person", "event", "organization"], "instance": {"id": "303", "words": ["Other", "parties", "at", "the", "time", "included", "the", "pan-Arabist", "Youth", "Congress", "Party", "and", "the", "Independence", "Party", "(", "Hizb", "al-Istiqlal", "al-", "'", "Arabi", ",", "also", "known", "as", "the", "Arab", "Istiqlal", "Party", ")", ",", "as", "well", "as", "the", "Reform", "Party", "and", "the", "National", "Bloc", ",", "established", "by", "public", "activists", "on", "a", "personal", "and", "local", "basis", ",", "and", "the", "National", "Liberation", "League", "in", "Palestine", ",", "an", "organization", "founded", "by", "the", "Palestine", "Communist", "Party", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, political party, country, election, person, event, organization and O.\nSentence: Other parties at the time included the pan-Arabist Youth Congress Party and the Independence Party ( Hizb al-Istiqlal al- ' Arabi , also known as the Arab Istiqlal Party ) , as well as the Reform Party and the National Bloc , established by public activists on a personal and local basis , and the National Liberation League in Palestine , an organization founded by the Palestine Communist Party .", "prompt_labels": "Other(O) parties(O) at(O) the(O) time(O) included(O) the(O) pan-Arabist(O) Youth(B-political party) Congress(I-political party) Party(I-political party) and(O) the(O) Independence(B-political party) Party(I-political party) ((O) Hizb(O) al-Istiqlal(O) al-(O) '(O) Arabi(O) ,(O) also(O) known(O) as(O) the(O) Arab(B-political party) Istiqlal(I-political party) Party(I-political party) )(O) ,(O) as(O) well(O) as(O) the(O) Reform(B-political party) Party(I-political party) and(O) the(O) National(B-political party) Bloc(I-political party) ,(O) established(O) by(O) public(O) activists(O) on(O) a(O) personal(O) and(O) local(O) basis(O) ,(O) and(O) the(O) National(B-political party) Liberation(I-political party) League(I-political party) in(I-political party) Palestine(I-political party) ,(O) an(O) organization(O) founded(O) by(O) the(O) Palestine(B-political party) Communist(I-political party) Party(I-political party) .(O)"}}
{"id": "104", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "political party", "location", "event", "country", "election", "person", "politician"], "instance": {"id": "104", "words": ["Those", "were", "the", "Croatian", "Democratic", "Union", ",", "the", "Croatian", "Peasant", "Party", ",", "the", "Croatian", "People", "'s", "Party", "-", "Liberal", "Democrats", ",", "the", "Croatian", "Social", "Liberal", "Party", ",", "Social", "Democratic", "Party", "of", "Croatia", "and", "the", "Bridge", "of", "Independent", "Lists", "."], "labels": ["O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, location, event, country, election, person, politician and O.\nSentence: Those were the Croatian Democratic Union , the Croatian Peasant Party , the Croatian People 's Party - Liberal Democrats , the Croatian Social Liberal Party , Social Democratic Party of Croatia and the Bridge of Independent Lists .", "prompt_labels": "Those(O) were(O) the(O) Croatian(B-political party) Democratic(I-political party) Union(I-political party) ,(O) the(O) Croatian(B-political party) Peasant(I-political party) Party(I-political party) ,(O) the(O) Croatian(B-political party) People(I-political party) 's(I-political party) Party(I-political party) -(I-political party) Liberal(I-political party) Democrats(I-political party) ,(O) the(O) Croatian(B-political party) Social(I-political party) Liberal(I-political party) Party(I-political party) ,(O) Social(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Croatia(I-political party) and(O) the(O) Bridge(B-political party) of(I-political party) Independent(I-political party) Lists(I-political party) .(O)"}}
{"id": "380", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "country", "person", "election", "location", "political party", "event", "organization"], "instance": {"id": "380", "words": ["In", "the", "parliamentary", "elections", "of", "2000", "Greek", "legislative", "election", "and", "2004", "Greek", "legislative", "election", ",", "and", "in", "the", "intervening", "local", "elections", ",", "Ecologists", "Greece", "formed", "an", "alliance", "with", "the", "Democratic", "Social", "Movement", "(", "DIKKI", ")", ",", "the", "Agro", "Party", "(", "PAEKE", ")", "and", "others", "."], "labels": ["O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, person, election, location, political party, event, organization and O.\nSentence: In the parliamentary elections of 2000 Greek legislative election and 2004 Greek legislative election , and in the intervening local elections , Ecologists Greece formed an alliance with the Democratic Social Movement ( DIKKI ) , the Agro Party ( PAEKE ) and others .", "prompt_labels": "In(O) the(O) parliamentary(O) elections(O) of(O) 2000(B-election) Greek(I-election) legislative(I-election) election(I-election) and(O) 2004(B-election) Greek(I-election) legislative(I-election) election(I-election) ,(O) and(O) in(O) the(O) intervening(O) local(O) elections(O) ,(O) Ecologists(B-political party) Greece(I-political party) formed(O) an(O) alliance(O) with(O) the(O) Democratic(B-political party) Social(I-political party) Movement(I-political party) ((O) DIKKI(B-political party) )(O) ,(O) the(O) Agro(B-political party) Party(I-political party) ((O) PAEKE(B-political party) )(O) and(O) others(O) .(O)"}}
{"id": "84", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "election", "organization", "country", "event", "location", "politician", "person"], "instance": {"id": "84", "words": ["Under", "Terre", "'Blanche", ",", "the", "AWB", "swore", "to", "use", "violence", "to", "preserve", "minority", "rule", ",", "opposing", "any", "concessions", "offered", "to", "the", "African", "National", "Congress", "-", "an", "organisation", "AWB", "supporters", "repeatedly", "branded", "as", "Marxist", "terrorist", "s", "Immediately", "prior", "to", "South", "Africa", "'s", "1994", "South", "African", "general", "election", ",", "Terre", "'Blanche", "'s", "followers", "were", "linked", "to", "a", "number", "of", "bombings", "and", "assassinations", "targeting", "the", "South", "African", "Communist", "Party", ";", "armed", "AWB", "commandos", "participated", "in", "the", "crisis", "in", "Bophuthatswana", "in", "1994", "."], "labels": ["O", "B-politician", "I-politician", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, organization, country, event, location, politician, person and O.\nSentence: Under Terre 'Blanche , the AWB swore to use violence to preserve minority rule , opposing any concessions offered to the African National Congress - an organisation AWB supporters repeatedly branded as Marxist terrorist s Immediately prior to South Africa 's 1994 South African general election , Terre 'Blanche 's followers were linked to a number of bombings and assassinations targeting the South African Communist Party ; armed AWB commandos participated in the crisis in Bophuthatswana in 1994 .", "prompt_labels": "Under(O) Terre(B-politician) 'Blanche(I-politician) ,(O) the(O) AWB(B-organization) swore(O) to(O) use(O) violence(O) to(O) preserve(O) minority(O) rule(O) ,(O) opposing(O) any(O) concessions(O) offered(O) to(O) the(O) African(B-political party) National(I-political party) Congress(I-political party) -(O) an(O) organisation(O) AWB(B-organization) supporters(O) repeatedly(O) branded(O) as(O) Marxist(O) terrorist(O) s(O) Immediately(O) prior(O) to(O) South(B-country) Africa(I-country) 's(O) 1994(B-election) South(I-election) African(I-election) general(I-election) election(I-election) ,(O) Terre(B-politician) 'Blanche(I-politician) 's(O) followers(O) were(O) linked(O) to(O) a(O) number(O) of(O) bombings(O) and(O) assassinations(O) targeting(O) the(O) South(B-political party) African(I-political party) Communist(I-political party) Party(I-political party) ;(O) armed(O) AWB(B-organization) commandos(O) participated(O) in(O) the(O) crisis(O) in(O) Bophuthatswana(B-country) in(O) 1994(O) .(O)"}}
{"id": "273", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "country", "political party", "location", "person", "event", "organization", "election"], "instance": {"id": "273", "words": ["Shinui", ",", "Hetz", ",", "Meretz", ",", "and", "Ale", "Yarok", "wish", "to", "promote", "what", "they", "see", "as", "key", "secular", "and", "democratic", "principles", ":"], "labels": ["B-political party", "O", "B-political party", "O", "B-political party", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, political party, location, person, event, organization, election and O.\nSentence: Shinui , Hetz , Meretz , and Ale Yarok wish to promote what they see as key secular and democratic principles :", "prompt_labels": "Shinui(B-political party) ,(O) Hetz(B-political party) ,(O) Meretz(B-political party) ,(O) and(O) Ale(B-political party) Yarok(I-political party) wish(O) to(O) promote(O) what(O) they(O) see(O) as(O) key(O) secular(O) and(O) democratic(O) principles(O) :(O)"}}
{"id": "336", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "location", "country", "politician", "political party", "election", "event", "person"], "instance": {"id": "336", "words": ["At", "the", "2013", "Australian", "federal", "election", ",", "LDP", "candidate", "David", "Leyonhjelm", "was", "elected", "to", "the", "Senate", "after", "polling", "the", "third", "highest", "vote", "in", "the", "state", "of", "New", "South", "Wales", "after", "the", "Liberal", "Party", "of", "Australia", "and", "the", "Australian", "Labor", "Party", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-political party", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, country, politician, political party, election, event, person and O.\nSentence: At the 2013 Australian federal election , LDP candidate David Leyonhjelm was elected to the Senate after polling the third highest vote in the state of New South Wales after the Liberal Party of Australia and the Australian Labor Party .", "prompt_labels": "At(O) the(O) 2013(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) LDP(B-political party) candidate(O) David(B-politician) Leyonhjelm(I-politician) was(O) elected(O) to(O) the(O) Senate(O) after(O) polling(O) the(O) third(O) highest(O) vote(O) in(O) the(O) state(O) of(O) New(B-location) South(I-location) Wales(I-location) after(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) and(O) the(O) Australian(B-political party) Labor(I-political party) Party(I-political party) .(O)"}}
{"id": "234", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "political party", "politician", "organization", "election", "country", "person", "location"], "instance": {"id": "234", "words": ["In", "1983", "United", "Kingdom", "general", "election", ",", "as", "a", "DUP", "candidate", ",", "Seawright", "finished", "second", "with", "8,260", "votes", "behind", "Cecil", "Walker", "of", "the", "UUP", ",", "whilst", "in", "1987", "United", "Kingdom", "general", "election", "he", "finished", "third", "behind", "Walker", "and", "Alban", "Maginness", "(", "Social", "Democratic", "and", "Labour", "Party", ")", "with", "5,671", "votes", "as", "a", "Protestant", "Unionist", "candidate", "(", "although", "the", "DUP", "did", "not", "contest", "the", "seat", "due", "to", "an", "electoral", "pact", "between", "the", "DUP", "and", "UUP", "at", "the", "time", ")", "."], "labels": ["O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-political party", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "B-politician", "O", "B-politician", "I-politician", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "B-political party", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, politician, organization, election, country, person, location and O.\nSentence: In 1983 United Kingdom general election , as a DUP candidate , Seawright finished second with 8,260 votes behind Cecil Walker of the UUP , whilst in 1987 United Kingdom general election he finished third behind Walker and Alban Maginness ( Social Democratic and Labour Party ) with 5,671 votes as a Protestant Unionist candidate ( although the DUP did not contest the seat due to an electoral pact between the DUP and UUP at the time ) .", "prompt_labels": "In(O) 1983(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) as(O) a(O) DUP(B-political party) candidate(O) ,(O) Seawright(B-politician) finished(O) second(O) with(O) 8,260(O) votes(O) behind(O) Cecil(B-politician) Walker(I-politician) of(O) the(O) UUP(B-political party) ,(O) whilst(O) in(O) 1987(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) he(O) finished(O) third(O) behind(O) Walker(B-politician) and(O) Alban(B-politician) Maginness(I-politician) ((O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) )(O) with(O) 5,671(O) votes(O) as(O) a(O) Protestant(B-political party) Unionist(I-political party) candidate(O) ((O) although(O) the(O) DUP(B-political party) did(O) not(O) contest(O) the(O) seat(O) due(O) to(O) an(O) electoral(O) pact(O) between(O) the(O) DUP(B-political party) and(O) UUP(B-political party) at(O) the(O) time(O) )(O) .(O)"}}
{"id": "539", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "political party", "person", "politician", "organization", "country", "location", "event"], "instance": {"id": "539", "words": ["In", "2003", ",", "she", "worked", "as", "Iowa", "field", "director", "for", "the", "John", "Edwards", "2004", "presidential", "campaign", ",", "and", ",", "after", "Edwards", "left", "the", "race", ",", "she", "became", "deputy", "campaign", "manager", "for", "Senator", "Tom", "Daschle", "'", "s", "re-election", "campaign", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, person, politician, organization, country, location, event and O.\nSentence: In 2003 , she worked as Iowa field director for the John Edwards 2004 presidential campaign , and , after Edwards left the race , she became deputy campaign manager for Senator Tom Daschle ' s re-election campaign .", "prompt_labels": "In(O) 2003(O) ,(O) she(O) worked(O) as(O) Iowa(O) field(O) director(O) for(O) the(O) John(B-event) Edwards(I-event) 2004(I-event) presidential(I-event) campaign(I-event) ,(O) and(O) ,(O) after(O) Edwards(B-politician) left(O) the(O) race(O) ,(O) she(O) became(O) deputy(O) campaign(O) manager(O) for(O) Senator(O) Tom(B-politician) Daschle(I-politician) '(O) s(O) re-election(O) campaign(O) .(O)"}}
{"id": "11", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "organization", "location", "country", "politician", "person", "election", "event"], "instance": {"id": "11", "words": ["In", "the", "February", "1974", "United", "Kingdom", "general", "election", "a", "number", "of", "Faulkner", "'s", "followers", "(", "including", "several", "sitting", "MPs", ")", "stood", "as", "Pro-Assembly", "Unionists", "against", "a", "coalition", "of", "the", "Ulster", "Unionist", "Party", ",", "the", "Vanguard", "Progressive", "Unionist", "Party", "and", "the", "Democratic", "Unionist", "Party", "."], "labels": ["O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, location, country, politician, person, election, event and O.\nSentence: In the February 1974 United Kingdom general election a number of Faulkner 's followers ( including several sitting MPs ) stood as Pro-Assembly Unionists against a coalition of the Ulster Unionist Party , the Vanguard Progressive Unionist Party and the Democratic Unionist Party .", "prompt_labels": "In(O) the(O) February(B-election) 1974(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) a(O) number(O) of(O) Faulkner(B-politician) 's(O) followers(O) ((O) including(O) several(O) sitting(O) MPs(O) )(O) stood(O) as(O) Pro-Assembly(O) Unionists(O) against(O) a(O) coalition(O) of(O) the(O) Ulster(B-political party) Unionist(I-political party) Party(I-political party) ,(O) the(O) Vanguard(B-political party) Progressive(I-political party) Unionist(I-political party) Party(I-political party) and(O) the(O) Democratic(B-political party) Unionist(I-political party) Party(I-political party) .(O)"}}
{"id": "487", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "country", "election", "location", "political party", "event", "organization", "person"], "instance": {"id": "487", "words": ["During", "World", "War", "I", ",", "he", "played", "a", "key", "role", "among", "those", "who", "advocated", "the", "Allies", "of", "World", "War", "I", "cause", ",", "arguing", "that", "Brazil", "should", "be", "more", "involved", "in", "the", "war", "."], "labels": ["O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, election, location, political party, event, organization, person and O.\nSentence: During World War I , he played a key role among those who advocated the Allies of World War I cause , arguing that Brazil should be more involved in the war .", "prompt_labels": "During(O) World(B-event) War(I-event) I(I-event) ,(O) he(O) played(O) a(O) key(O) role(O) among(O) those(O) who(O) advocated(O) the(O) Allies(B-country) of(O) World(B-event) War(I-event) I(I-event) cause(O) ,(O) arguing(O) that(O) Brazil(B-country) should(O) be(O) more(O) involved(O) in(O) the(O) war(O) .(O)"}}
{"id": "115", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "event", "person", "country", "organization", "politician", "election", "political party"], "instance": {"id": "115", "words": ["In", "1463", ",", "Bagrat", "allied", "himself", "with", "other", "oppositionist", "royal", "subjects", ",", "dukes", "(", "eristavi", ")", "of", "Principality", "of", "Mingrelia", ",", "Principality", "of", "Guria", ",", "Principality", "of", "Svaneti", "and", "Principality", "of", "Abkhazia", "."], "labels": ["O", "O", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "I-country", "I-country", "O", "B-country", "I-country", "I-country", "O", "B-country", "I-country", "I-country", "O", "B-country", "I-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, person, country, organization, politician, election, political party and O.\nSentence: In 1463 , Bagrat allied himself with other oppositionist royal subjects , dukes ( eristavi ) of Principality of Mingrelia , Principality of Guria , Principality of Svaneti and Principality of Abkhazia .", "prompt_labels": "In(O) 1463(O) ,(O) Bagrat(B-politician) allied(O) himself(O) with(O) other(O) oppositionist(O) royal(O) subjects(O) ,(O) dukes(O) ((O) eristavi(O) )(O) of(O) Principality(B-country) of(I-country) Mingrelia(I-country) ,(O) Principality(B-country) of(I-country) Guria(I-country) ,(O) Principality(B-country) of(I-country) Svaneti(I-country) and(O) Principality(B-country) of(I-country) Abkhazia(I-country) .(O)"}}
{"id": "455", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "location", "election", "political party", "person", "organization", "politician", "country"], "instance": {"id": "455", "words": ["Among", "the", "organization", "'s", "most", "prominent", "members", "are", "Alfonso", "Calderon", ",", "Sarah", "Chadwick", ",", "Jaclyn", "Corin", ",", "Ryan", "Deitsch", ",", "Emma", "González", ",", "David", "Hogg", ",", "Cameron", "Kasky", ",", "and", "Alex", "Wind", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, election, political party, person, organization, politician, country and O.\nSentence: Among the organization 's most prominent members are Alfonso Calderon , Sarah Chadwick , Jaclyn Corin , Ryan Deitsch , Emma González , David Hogg , Cameron Kasky , and Alex Wind .", "prompt_labels": "Among(O) the(O) organization(O) 's(O) most(O) prominent(O) members(O) are(O) Alfonso(B-person) Calderon(I-person) ,(O) Sarah(B-person) Chadwick(I-person) ,(O) Jaclyn(B-person) Corin(I-person) ,(O) Ryan(B-person) Deitsch(I-person) ,(O) Emma(B-person) González(I-person) ,(O) David(B-person) Hogg(I-person) ,(O) Cameron(B-person) Kasky(I-person) ,(O) and(O) Alex(B-person) Wind(I-person) .(O)"}}
{"id": "106", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "political party", "person", "event", "location", "country", "election", "politician"], "instance": {"id": "106", "words": ["Ante", "Starčević", "Croatian", "Party", "of", "Rights", "dr", ",", "Dalmatian", "Action", ",", "the", "Democratic", "Centre", ",", "the", "Istrian", "Democratic", "Assembly", ",", "the", "Liberal", "Party", ",", "the", "Party", "of", "Liberal", "Democrats", ",", "the", "Serb", "Democratic", "Party", ",", "the", "Slavonia-Baranja", "Croatian", "Party", "and", "the", "Social", "Democratic", "Action", "of", "Croatia", "."], "labels": ["B-politician", "I-politician", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, person, event, location, country, election, politician and O.\nSentence: Ante Starčević Croatian Party of Rights dr , Dalmatian Action , the Democratic Centre , the Istrian Democratic Assembly , the Liberal Party , the Party of Liberal Democrats , the Serb Democratic Party , the Slavonia-Baranja Croatian Party and the Social Democratic Action of Croatia .", "prompt_labels": "Ante(B-politician) Starčević(I-politician) Croatian(B-political party) Party(I-political party) of(I-political party) Rights(I-political party) dr(I-political party) ,(O) Dalmatian(B-political party) Action(I-political party) ,(O) the(O) Democratic(B-political party) Centre(I-political party) ,(O) the(O) Istrian(B-political party) Democratic(I-political party) Assembly(I-political party) ,(O) the(O) Liberal(B-political party) Party(I-political party) ,(O) the(O) Party(B-political party) of(I-political party) Liberal(I-political party) Democrats(I-political party) ,(O) the(O) Serb(B-political party) Democratic(I-political party) Party(I-political party) ,(O) the(O) Slavonia-Baranja(B-political party) Croatian(I-political party) Party(I-political party) and(O) the(O) Social(B-political party) Democratic(I-political party) Action(I-political party) of(I-political party) Croatia(I-political party) .(O)"}}
{"id": "424", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "location", "country", "event", "political party", "organization", "election", "person"], "instance": {"id": "424", "words": ["Sirivennela", "Seetharama", "Sastry", ",", "Swanand", "Kirkire", ",", "Madhan", "Karky", ",", "Azad", "Varadaraj", ",", "and", "Siju", "Thuravoor", "wrote", "the", "lyrics", "for", "the", "soundtrack", "in", "Telugu", ",", "Hindi", ",", "Tamil", ",", "Kannada", ",", "and", "Malayalam", "respectively", "."], "labels": ["B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, country, event, political party, organization, election, person and O.\nSentence: Sirivennela Seetharama Sastry , Swanand Kirkire , Madhan Karky , Azad Varadaraj , and Siju Thuravoor wrote the lyrics for the soundtrack in Telugu , Hindi , Tamil , Kannada , and Malayalam respectively .", "prompt_labels": "Sirivennela(B-person) Seetharama(I-person) Sastry(I-person) ,(O) Swanand(B-person) Kirkire(I-person) ,(O) Madhan(B-person) Karky(I-person) ,(O) Azad(B-person) Varadaraj(I-person) ,(O) and(O) Siju(B-person) Thuravoor(I-person) wrote(O) the(O) lyrics(O) for(O) the(O) soundtrack(O) in(O) Telugu(O) ,(O) Hindi(O) ,(O) Tamil(O) ,(O) Kannada(O) ,(O) and(O) Malayalam(O) respectively(O) .(O)"}}
{"id": "486", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "country", "location", "election", "organization", "event", "politician", "political party"], "instance": {"id": "486", "words": ["The", "national", "unity", "government", "(", "War", "cabinet", ")", "was", "the", "second", "of", "four", "war", "cabinets", "of", "the", "Dutch", "government-in-exile", "in", "London", "during", "World", "War", "II", "."], "labels": ["O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-location", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, location, election, organization, event, politician, political party and O.\nSentence: The national unity government ( War cabinet ) was the second of four war cabinets of the Dutch government-in-exile in London during World War II .", "prompt_labels": "The(O) national(B-organization) unity(I-organization) government(I-organization) ((O) War(B-organization) cabinet(I-organization) )(O) was(O) the(O) second(O) of(O) four(O) war(O) cabinets(O) of(O) the(O) Dutch(B-organization) government-in-exile(I-organization) in(O) London(B-location) during(O) World(B-event) War(I-event) II(I-event) .(O)"}}
{"id": "297", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "organization", "political party", "location", "event", "country", "politician", "person"], "instance": {"id": "297", "words": ["Edmonton", "is", "the", "only", "part", "of", "Alberta", "where", "the", "Liberal", "Party", "of", "Canada", "have", "consistently", "broken", "through", "in", "recent", "times", ",", "having", "won", "between", "two", "and", "four", "seats", "here", "from", "1993", "Canadian", "federal", "election", "to", "2004", "Canadian", "federal", "election", ",", "although", "never", "by", "large", "margins", "."], "labels": ["B-location", "O", "O", "O", "O", "O", "B-location", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, political party, location, event, country, politician, person and O.\nSentence: Edmonton is the only part of Alberta where the Liberal Party of Canada have consistently broken through in recent times , having won between two and four seats here from 1993 Canadian federal election to 2004 Canadian federal election , although never by large margins .", "prompt_labels": "Edmonton(B-location) is(O) the(O) only(O) part(O) of(O) Alberta(B-location) where(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) have(O) consistently(O) broken(O) through(O) in(O) recent(O) times(O) ,(O) having(O) won(O) between(O) two(O) and(O) four(O) seats(O) here(O) from(O) 1993(B-election) Canadian(I-election) federal(I-election) election(I-election) to(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) although(O) never(O) by(O) large(O) margins(O) .(O)"}}
{"id": "174", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "location", "country", "election", "political party", "politician", "person", "event"], "instance": {"id": "174", "words": ["The", "body", "thus", "oversaw", "the", "activities", "of", "the", "Bulgarian", "Communist", "Party", "(", "BKP", ")", ",", "the", "League", "of", "Communists", "of", "Yugoslavia", "(", "KPJ", ")", ",", "the", "Communist", "Party", "of", "Greece", "(", "KKE", ")", ",", "the", "Communist", "Party", "of", "Turkey", "(", "TKP", ")", ",", "and", ",", "to", "a", "certain", "measure", ",", "those", "of", "the", "Romanian", "Communist", "Party", "(", "PCdR", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, country, election, political party, politician, person, event and O.\nSentence: The body thus oversaw the activities of the Bulgarian Communist Party ( BKP ) , the League of Communists of Yugoslavia ( KPJ ) , the Communist Party of Greece ( KKE ) , the Communist Party of Turkey ( TKP ) , and , to a certain measure , those of the Romanian Communist Party ( PCdR ) .", "prompt_labels": "The(O) body(O) thus(O) oversaw(O) the(O) activities(O) of(O) the(O) Bulgarian(B-political party) Communist(I-political party) Party(I-political party) ((O) BKP(B-political party) )(O) ,(O) the(O) League(B-political party) of(I-political party) Communists(I-political party) of(I-political party) Yugoslavia(I-political party) ((O) KPJ(B-political party) )(O) ,(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Greece(I-political party) ((O) KKE(B-political party) )(O) ,(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Turkey(I-political party) ((O) TKP(B-political party) )(O) ,(O) and(O) ,(O) to(O) a(O) certain(O) measure(O) ,(O) those(O) of(O) the(O) Romanian(B-political party) Communist(I-political party) Party(I-political party) ((O) PCdR(B-political party) )(O) .(O)"}}
{"id": "194", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "event", "person", "country", "location", "organization", "politician", "political party"], "instance": {"id": "194", "words": ["In", "December", "2019", ",", "the", "party", "merged", "with", "the", "Afar", "National", "Democratic", "Party", "(", "ANDP", ")", ",", "the", "Amhara", "Democratic", "Party", "(", "ADP", ")", ",", "the", "Ethiopian", "Somali", "People", "'s", "Democratic", "Party", "(", "ESPDP", ")", ",", "the", "Gambela", "People", "'s", "Democratic", "Movement", "(", "GPDM", ")", ",", "the", "Hareri", "National", "League", "(", "HNL", ")", ",", "the", "Oromo", "Democratic", "Party", "(", "ODP", ")", "and", "the", "Southern", "Ethiopian", "People", "'s", "Democratic", "Movement", "(", "SEPDM", ")", "to", "form", "the", "Prosperity", "Party", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "O", "B-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, person, country, location, organization, politician, political party and O.\nSentence: In December 2019 , the party merged with the Afar National Democratic Party ( ANDP ) , the Amhara Democratic Party ( ADP ) , the Ethiopian Somali People 's Democratic Party ( ESPDP ) , the Gambela People 's Democratic Movement ( GPDM ) , the Hareri National League ( HNL ) , the Oromo Democratic Party ( ODP ) and the Southern Ethiopian People 's Democratic Movement ( SEPDM ) to form the Prosperity Party .", "prompt_labels": "In(O) December(O) 2019(O) ,(O) the(O) party(O) merged(O) with(O) the(O) Afar(B-political party) National(I-political party) Democratic(I-political party) Party(I-political party) ((O) ANDP(B-political party) )(O) ,(O) the(O) Amhara(B-political party) Democratic(I-political party) Party(I-political party) ((O) ADP(B-political party) )(O) ,(O) the(O) Ethiopian(B-political party) Somali(I-political party) People(I-political party) 's(I-political party) Democratic(I-political party) Party(I-political party) ((O) ESPDP(B-political party) )(O) ,(O) the(O) Gambela(B-political party) People(I-political party) 's(I-political party) Democratic(I-political party) Movement(I-political party) ((O) GPDM(B-political party) )(O) ,(O) the(O) Hareri(B-political party) National(I-political party) League(I-political party) ((O) HNL(B-political party) )(O) ,(O) the(O) Oromo(B-political party) Democratic(I-political party) Party(I-political party) ((O) ODP(B-political party) )(O) and(O) the(O) Southern(B-political party) Ethiopian(I-political party) People(I-political party) 's(I-political party) Democratic(I-political party) Movement(I-political party) ((O) SEPDM(B-political party) )(O) to(O) form(O) the(O) Prosperity(B-political party) Party(I-political party) .(O)"}}
{"id": "510", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "election", "political party", "event", "organization", "politician", "location", "person"], "instance": {"id": "510", "words": ["Prior", "to", "his", "time", "in", "Congress", ",", "Chapin", "would", "also", "serve", "as", "a", "delegate", "of", "the", "Massachusetts", "Constitutional", "Convention", "of", "1853", "and", ",", "as", "a", "War", "Democrat", ",", "purchased", "the", "uniforms", "of", "the", "10th", "Regiment", "Massachusetts", "Volunteer", "Infantry", "at", "the", "outset", "of", "the", "American", "Civil", "War", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "B-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, election, political party, event, organization, politician, location, person and O.\nSentence: Prior to his time in Congress , Chapin would also serve as a delegate of the Massachusetts Constitutional Convention of 1853 and , as a War Democrat , purchased the uniforms of the 10th Regiment Massachusetts Volunteer Infantry at the outset of the American Civil War .", "prompt_labels": "Prior(O) to(O) his(O) time(O) in(O) Congress(B-organization) ,(O) Chapin(B-politician) would(O) also(O) serve(O) as(O) a(O) delegate(O) of(O) the(O) Massachusetts(B-event) Constitutional(I-event) Convention(I-event) of(I-event) 1853(I-event) and(O) ,(O) as(O) a(O) War(B-political party) Democrat(I-political party) ,(O) purchased(O) the(O) uniforms(O) of(O) the(O) 10th(B-event) Regiment(I-event) Massachusetts(I-event) Volunteer(I-event) Infantry(I-event) at(O) the(O) outset(O) of(O) the(O) American(B-event) Civil(I-event) War(I-event) .(O)"}}
{"id": "82", "dataset": "crossner_politics", "split": "dev", "label_list": ["event", "political party", "organization", "election", "person", "location", "politician", "country"], "instance": {"id": "82", "words": ["Kōmeitō", "did", "quite", "well", ",", "and", "in", "1993", ",", "when", "the", "LDP", "was", "for", "the", "first", "time", "declared", "an", "opposition", "party", ",", "the", "Kōmeitō", "became", "one", "of", "the", "ruling", "parties", ",", "headed", "by", "the", "liberal", "Japan", "New", "Party", ",", "but", "which", "also", "included", "the", "Democratic", "Socialist", "Party", ",", "Japan", "Renewal", "Party", ",", "the", "New", "Party", "Sakigake", ",", "and", "the", "Japan", "Socialist", "Party", "."], "labels": ["B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, organization, election, person, location, politician, country and O.\nSentence: Kōmeitō did quite well , and in 1993 , when the LDP was for the first time declared an opposition party , the Kōmeitō became one of the ruling parties , headed by the liberal Japan New Party , but which also included the Democratic Socialist Party , Japan Renewal Party , the New Party Sakigake , and the Japan Socialist Party .", "prompt_labels": "Kōmeitō(B-politician) did(O) quite(O) well(O) ,(O) and(O) in(O) 1993(O) ,(O) when(O) the(O) LDP(B-political party) was(O) for(O) the(O) first(O) time(O) declared(O) an(O) opposition(O) party(O) ,(O) the(O) Kōmeitō(B-political party) became(O) one(O) of(O) the(O) ruling(O) parties(O) ,(O) headed(O) by(O) the(O) liberal(O) Japan(B-political party) New(I-political party) Party(I-political party) ,(O) but(O) which(O) also(O) included(O) the(O) Democratic(B-political party) Socialist(I-political party) Party(I-political party) ,(O) Japan(B-political party) Renewal(I-political party) Party(I-political party) ,(O) the(O) New(B-political party) Party(I-political party) Sakigake(I-political party) ,(O) and(O) the(O) Japan(B-political party) Socialist(I-political party) Party(I-political party) .(O)"}}
{"id": "252", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "person", "country", "event", "organization", "location", "politician", "political party"], "instance": {"id": "252", "words": ["He", "was", "chairman", "of", "the", "Executive", "Committee", "of", "the", "Board", "of", "the", "Public", "Agenda", "Foundation", ";", "chairman", "of", "the", "Executive", "Committee", "of", "the", "Board", "of", "Business", "Executives", "for", "National", "Security", ";", "a", "member", "of", "the", "Council", "on", "Foreign", "Relations", "and", "the", "U.S.", "Council", "on", "Competitiveness", ";", "and", "a", "member", "of", "the", "Board", "of", "the", "Leadership", "Institute", "of", "the", "University", "of", "Southern", "California", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, country, event, organization, location, politician, political party and O.\nSentence: He was chairman of the Executive Committee of the Board of the Public Agenda Foundation ; chairman of the Executive Committee of the Board of Business Executives for National Security ; a member of the Council on Foreign Relations and the U.S. Council on Competitiveness ; and a member of the Board of the Leadership Institute of the University of Southern California .", "prompt_labels": "He(O) was(O) chairman(O) of(O) the(O) Executive(B-organization) Committee(I-organization) of(O) the(O) Board(O) of(O) the(O) Public(B-organization) Agenda(I-organization) Foundation(I-organization) ;(O) chairman(O) of(O) the(O) Executive(B-organization) Committee(I-organization) of(O) the(O) Board(O) of(O) Business(B-organization) Executives(I-organization) for(I-organization) National(I-organization) Security(I-organization) ;(O) a(O) member(O) of(O) the(O) Council(B-organization) on(I-organization) Foreign(I-organization) Relations(I-organization) and(O) the(O) U.S.(B-organization) Council(I-organization) on(I-organization) Competitiveness(I-organization) ;(O) and(O) a(O) member(O) of(O) the(O) Board(O) of(O) the(O) Leadership(O) Institute(O) of(O) the(O) University(B-organization) of(I-organization) Southern(I-organization) California(I-organization) .(O)"}}
{"id": "443", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "location", "country", "politician", "political party", "person", "election", "event"], "instance": {"id": "443", "words": ["The", "film", "features", "Ranjith", "and", "Anamika", "in", "lead", "roles", ",", "with", "Ravichandran", ",", "Pyramid", "Natarajan", ",", "Manivannan", ",", "Vasu", "Vikram", ",", "Rajesh", ",", "Raj", "Kapoor", ",", "T.", "P.", "Gajendran", ",", "Alex", "and", "Pandu", "playing", "supporting", "roles", "."], "labels": ["O", "O", "O", "B-person", "O", "B-person", "O", "O", "O", "O", "O", "B-person", "O", "B-person", "I-person", "O", "B-person", "O", "B-person", "I-person", "O", "B-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "O", "B-person", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, country, politician, political party, person, election, event and O.\nSentence: The film features Ranjith and Anamika in lead roles , with Ravichandran , Pyramid Natarajan , Manivannan , Vasu Vikram , Rajesh , Raj Kapoor , T. P. Gajendran , Alex and Pandu playing supporting roles .", "prompt_labels": "The(O) film(O) features(O) Ranjith(B-person) and(O) Anamika(B-person) in(O) lead(O) roles(O) ,(O) with(O) Ravichandran(B-person) ,(O) Pyramid(B-person) Natarajan(I-person) ,(O) Manivannan(B-person) ,(O) Vasu(B-person) Vikram(I-person) ,(O) Rajesh(B-person) ,(O) Raj(B-person) Kapoor(I-person) ,(O) T.(B-person) P.(I-person) Gajendran(I-person) ,(O) Alex(B-person) and(O) Pandu(B-person) playing(O) supporting(O) roles(O) .(O)"}}
{"id": "102", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "organization", "politician", "location", "election", "political party", "event", "person"], "instance": {"id": "102", "words": ["The", "five", "largest", "political", "parties", "were", "Golkar", ",", "Nahdlatul", "Ulama", ",", "the", "Muslim", "Party", "of", "Indonesia", "(", "Parmusi", ")", ",", "the", "Indonesian", "National", "Party", "and", "the", "Indonesian", "Islamic", "Union", "Party", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-political party", "O", "B-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, politician, location, election, political party, event, person and O.\nSentence: The five largest political parties were Golkar , Nahdlatul Ulama , the Muslim Party of Indonesia ( Parmusi ) , the Indonesian National Party and the Indonesian Islamic Union Party .", "prompt_labels": "The(O) five(O) largest(O) political(O) parties(O) were(O) Golkar(B-political party) ,(O) Nahdlatul(B-political party) Ulama(I-political party) ,(O) the(O) Muslim(B-political party) Party(I-political party) of(I-political party) Indonesia(I-political party) ((O) Parmusi(B-political party) )(O) ,(O) the(O) Indonesian(B-political party) National(I-political party) Party(I-political party) and(O) the(O) Indonesian(B-political party) Islamic(I-political party) Union(I-political party) Party(I-political party) .(O)"}}
{"id": "249", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "political party", "country", "location", "event", "person", "election", "politician"], "instance": {"id": "249", "words": ["He", "subsequently", "was", "dropped", "as", "UMNO", "candidate", "in", "the", "1990", "Malaysian", "general", "election", ",", "1995", "Malaysian", "general", "election", "and", "1999", "Malaysian", "general", "election", "general", "elections", "."], "labels": ["O", "O", "O", "O", "O", "B-political party", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, political party, country, location, event, person, election, politician and O.\nSentence: He subsequently was dropped as UMNO candidate in the 1990 Malaysian general election , 1995 Malaysian general election and 1999 Malaysian general election general elections .", "prompt_labels": "He(O) subsequently(O) was(O) dropped(O) as(O) UMNO(B-political party) candidate(O) in(O) the(O) 1990(B-election) Malaysian(I-election) general(I-election) election(I-election) ,(O) 1995(B-election) Malaysian(I-election) general(I-election) election(I-election) and(O) 1999(B-election) Malaysian(I-election) general(I-election) election(I-election) general(O) elections(O) .(O)"}}
{"id": "268", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "election", "person", "politician", "organization", "event", "country", "political party"], "instance": {"id": "268", "words": ["Michael", "Bloomberg", ",", "formerly", "a", "Democrat", ",", "was", "elected", "as", "a", "Republican", "in", "2001", "New", "York", "City", "mayoral", "election", "and", "2005", "New", "York", "City", "mayoral", "election", ",", "succeeding", "another", "Republican", "mayor", ",", "Rudy", "Giuliani", ",", "elected", "in", "1993", "and", "1997", "New", "York", "City", "mayoral", "election", "."], "labels": ["B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, election, person, politician, organization, event, country, political party and O.\nSentence: Michael Bloomberg , formerly a Democrat , was elected as a Republican in 2001 New York City mayoral election and 2005 New York City mayoral election , succeeding another Republican mayor , Rudy Giuliani , elected in 1993 and 1997 New York City mayoral election .", "prompt_labels": "Michael(B-politician) Bloomberg(I-politician) ,(O) formerly(O) a(O) Democrat(O) ,(O) was(O) elected(O) as(O) a(O) Republican(O) in(O) 2001(B-election) New(I-election) York(I-election) City(I-election) mayoral(I-election) election(I-election) and(O) 2005(B-election) New(I-election) York(I-election) City(I-election) mayoral(I-election) election(I-election) ,(O) succeeding(O) another(O) Republican(O) mayor(O) ,(O) Rudy(B-politician) Giuliani(I-politician) ,(O) elected(O) in(O) 1993(B-election) and(I-election) 1997(I-election) New(I-election) York(I-election) City(I-election) mayoral(I-election) election(I-election) .(O)"}}
{"id": "188", "dataset": "crossner_politics", "split": "dev", "label_list": ["politician", "political party", "location", "country", "organization", "person", "event", "election"], "instance": {"id": "188", "words": ["He", "was", "voted", "into", "Parliament", "with", "the", "Liberal", "Democratic", "Union", ",", "in", "the", "1956", "Greek", "legislative", "election", "s", ",", "but", "in", "the", "1958", "Greek", "legislative", "election", "s", ",", "as", "head", "of", "the", "Union", "of", "Populars", ",", "he", "failed", "to", "be", "elected", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, location, country, organization, person, event, election and O.\nSentence: He was voted into Parliament with the Liberal Democratic Union , in the 1956 Greek legislative election s , but in the 1958 Greek legislative election s , as head of the Union of Populars , he failed to be elected .", "prompt_labels": "He(O) was(O) voted(O) into(O) Parliament(O) with(O) the(O) Liberal(B-political party) Democratic(I-political party) Union(I-political party) ,(O) in(O) the(O) 1956(B-election) Greek(I-election) legislative(I-election) election(I-election) s(O) ,(O) but(O) in(O) the(O) 1958(B-election) Greek(I-election) legislative(I-election) election(I-election) s(O) ,(O) as(O) head(O) of(O) the(O) Union(B-political party) of(I-political party) Populars(I-political party) ,(O) he(O) failed(O) to(O) be(O) elected(O) .(O)"}}
{"id": "74", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "event", "politician", "location", "person", "election", "organization", "country"], "instance": {"id": "74", "words": ["The", "main", "line", "of", "conflict", "in", "France", "during", "the", "19th", "century", "was", "between", "monarchists", "(", "mainly", "Legitimists", "and", "Orléanist", "s", ",", "but", "also", "Bonapartism", ")", "and", "republicans", "(", "Radical-Socialists", ",", "Opportunist", "Republicans", ",", "and", "later", "socialists", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, politician, location, person, election, organization, country and O.\nSentence: The main line of conflict in France during the 19th century was between monarchists ( mainly Legitimists and Orléanist s , but also Bonapartism ) and republicans ( Radical-Socialists , Opportunist Republicans , and later socialists ) .", "prompt_labels": "The(O) main(O) line(O) of(O) conflict(O) in(O) France(B-country) during(O) the(O) 19th(O) century(O) was(O) between(O) monarchists(O) ((O) mainly(O) Legitimists(O) and(O) Orléanist(O) s(O) ,(O) but(O) also(O) Bonapartism(O) )(O) and(O) republicans(O) ((O) Radical-Socialists(O) ,(O) Opportunist(B-organization) Republicans(I-organization) ,(O) and(O) later(O) socialists(O) )(O) .(O)"}}
{"id": "199", "dataset": "crossner_politics", "split": "dev", "label_list": ["location", "person", "event", "politician", "organization", "country", "election", "political party"], "instance": {"id": "199", "words": ["1885", "boundaries", "were", "also", "used", "in", "the", "1886", "United", "Kingdom", "general", "election", ",", "the", "1892", "United", "Kingdom", "general", "election", ",", "the", "1895", "United", "Kingdom", "general", "election", ",", "the", "1900", "United", "Kingdom", "general", "election", ",", "the", "1906", "United", "Kingdom", "general", "election", ",", "the", "January", "1910", "United", "Kingdom", "general", "election", "and", "the", "December", "1910", "United", "Kingdom", "general", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, event, politician, organization, country, election, political party and O.\nSentence: 1885 boundaries were also used in the 1886 United Kingdom general election , the 1892 United Kingdom general election , the 1895 United Kingdom general election , the 1900 United Kingdom general election , the 1906 United Kingdom general election , the January 1910 United Kingdom general election and the December 1910 United Kingdom general election .", "prompt_labels": "1885(O) boundaries(O) were(O) also(O) used(O) in(O) the(O) 1886(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1892(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1895(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1900(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1906(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) January(B-election) 1910(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) the(O) December(B-election) 1910(I-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "488", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "organization", "politician", "location", "event", "political party", "person", "country"], "instance": {"id": "488", "words": ["1940", "-", "World", "War", "II", ":", "Soviet", "Union", "leader", "Joseph", "Stalin", "and", "the", "Politburo", "signed", "an", "order", "for", "the", "execution", "of", "about", "22,000", "Polish", "military", "officers", ",", "policemen", ",", "intellectuals", "and", "civilian", "prisoners", "of", "war", "that", "were", "captured", "during", "the", "Soviet", "invasion", "of", "Poland", "."], "labels": ["O", "O", "B-event", "I-event", "I-event", "O", "B-country", "I-country", "O", "B-politician", "I-politician", "O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, politician, location, event, political party, person, country and O.\nSentence: 1940 - World War II : Soviet Union leader Joseph Stalin and the Politburo signed an order for the execution of about 22,000 Polish military officers , policemen , intellectuals and civilian prisoners of war that were captured during the Soviet invasion of Poland .", "prompt_labels": "1940(O) -(O) World(B-event) War(I-event) II(I-event) :(O) Soviet(B-country) Union(I-country) leader(O) Joseph(B-politician) Stalin(I-politician) and(O) the(O) Politburo(B-organization) signed(O) an(O) order(O) for(O) the(O) execution(O) of(O) about(O) 22,000(O) Polish(O) military(O) officers(O) ,(O) policemen(O) ,(O) intellectuals(O) and(O) civilian(O) prisoners(O) of(O) war(O) that(O) were(O) captured(O) during(O) the(O) Soviet(B-event) invasion(I-event) of(I-event) Poland(I-event) .(O)"}}
{"id": "196", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "politician", "country", "election", "political party", "location", "person", "event"], "instance": {"id": "196", "words": ["The", "same", "boundaries", "were", "used", "in", "the", "1922", "United", "Kingdom", "general", "election", ",", "the", "1923", "United", "Kingdom", "general", "election", ",", "the", "1924", "United", "Kingdom", "general", "election", ",", "the", "1929", "United", "Kingdom", "general", "election", ",", "the", "1931", "United", "Kingdom", "general", "election", ",", "the", "1935", "United", "Kingdom", "general", "election", "and", "the", "1945", "United", "Kingdom", "general", "election", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, country, election, political party, location, person, event and O.\nSentence: The same boundaries were used in the 1922 United Kingdom general election , the 1923 United Kingdom general election , the 1924 United Kingdom general election , the 1929 United Kingdom general election , the 1931 United Kingdom general election , the 1935 United Kingdom general election and the 1945 United Kingdom general election .", "prompt_labels": "The(O) same(O) boundaries(O) were(O) used(O) in(O) the(O) 1922(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1923(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1924(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1929(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1931(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) ,(O) the(O) 1935(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) the(O) 1945(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)"}}
{"id": "212", "dataset": "crossner_politics", "split": "dev", "label_list": ["political party", "person", "election", "event", "location", "country", "politician", "organization"], "instance": {"id": "212", "words": ["The", "incumbent", "Australian", "Labor", "Party", "led", "by", "Bob", "Hawke", "defeated", "the", "opposition", "Liberal", "Party", "of", "Australia", "led", "by", "Andrew", "Peacock", "with", "coalition", "partner", "the", "National", "Party", "of", "Australia", "led", "by", "Charles", "Blunt", "despite", "losing", "the", "two", "party", "preferred", "popular", "vote", "."], "labels": ["O", "O", "B-political party", "I-political party", "I-political party", "O", "O", "B-politician", "I-politician", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, person, election, event, location, country, politician, organization and O.\nSentence: The incumbent Australian Labor Party led by Bob Hawke defeated the opposition Liberal Party of Australia led by Andrew Peacock with coalition partner the National Party of Australia led by Charles Blunt despite losing the two party preferred popular vote .", "prompt_labels": "The(O) incumbent(O) Australian(B-political party) Labor(I-political party) Party(I-political party) led(O) by(O) Bob(B-politician) Hawke(I-politician) defeated(O) the(O) opposition(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) led(O) by(O) Andrew(B-politician) Peacock(I-politician) with(O) coalition(O) partner(O) the(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) led(O) by(O) Charles(B-politician) Blunt(I-politician) despite(O) losing(O) the(O) two(O) party(O) preferred(O) popular(O) vote(O) .(O)"}}
{"id": "376", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "election", "country", "politician", "location", "political party", "organization", "event"], "instance": {"id": "376", "words": ["The", "Bill", "was", "conceived", "in", "1989", "by", "5", "MPs", "from", "all", "five", "groups", "of", "the", "French", "Parliament", "(", "French", "Communist", "Party", ",", "French", "Socialist", "Party", ",", "and", "the", "conservative", "Union", "for", "French", "Democracy", ",", "Rally", "for", "the", "Republic", ",", "and", "UDC", ")", "."], "labels": ["O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "O", "O", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-political party", "I-political party", "I-political party", "I-political party", "O", "O", "B-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, country, politician, location, political party, organization, event and O.\nSentence: The Bill was conceived in 1989 by 5 MPs from all five groups of the French Parliament ( French Communist Party , French Socialist Party , and the conservative Union for French Democracy , Rally for the Republic , and UDC ) .", "prompt_labels": "The(O) Bill(B-politician) was(O) conceived(O) in(O) 1989(O) by(O) 5(O) MPs(O) from(O) all(O) five(O) groups(O) of(O) the(O) French(O) Parliament(O) ((O) French(B-political party) Communist(I-political party) Party(I-political party) ,(O) French(B-political party) Socialist(I-political party) Party(I-political party) ,(O) and(O) the(O) conservative(O) Union(B-political party) for(I-political party) French(I-political party) Democracy(I-political party) ,(O) Rally(B-political party) for(I-political party) the(I-political party) Republic(I-political party) ,(O) and(O) UDC(B-political party) )(O) .(O)"}}
{"id": "535", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "election", "organization", "country", "location", "political party", "politician", "event"], "instance": {"id": "535", "words": ["The", "Greek", "hyperinflation", "is", "the", "fifth", "worst", "in", "economic", "history", ",", "after", "Hungary", "'s", "following", "World", "War", "II", ",", "Zimbabwe", "'", "s", "in", "the", "late", "2000s", ",", "Yugoslavia", "'", "s", "in", "the", "middle", "1990s", ",", "and", "Germany", "'s", "following", "World", "War", "I.", "This", "was", "compounded", "by", "the", "country", "'s", "disastrous", "civil", "war", "from", "1944", "to", "1950", "."], "labels": ["O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-event", "I-event", "I-event", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, election, organization, country, location, political party, politician, event and O.\nSentence: The Greek hyperinflation is the fifth worst in economic history , after Hungary 's following World War II , Zimbabwe ' s in the late 2000s , Yugoslavia ' s in the middle 1990s , and Germany 's following World War I. This was compounded by the country 's disastrous civil war from 1944 to 1950 .", "prompt_labels": "The(O) Greek(B-event) hyperinflation(I-event) is(O) the(O) fifth(O) worst(O) in(O) economic(O) history(O) ,(O) after(O) Hungary(B-country) 's(O) following(O) World(B-event) War(I-event) II(I-event) ,(O) Zimbabwe(B-country) '(O) s(O) in(O) the(O) late(O) 2000s(O) ,(O) Yugoslavia(B-country) '(O) s(O) in(O) the(O) middle(O) 1990s(O) ,(O) and(O) Germany(B-country) 's(O) following(O) World(B-event) War(I-event) I.(I-event) This(O) was(O) compounded(O) by(O) the(O) country(O) 's(O) disastrous(O) civil(O) war(O) from(O) 1944(O) to(O) 1950(O) .(O)"}}
{"id": "437", "dataset": "crossner_politics", "split": "dev", "label_list": ["country", "politician", "election", "organization", "person", "political party", "location", "event"], "instance": {"id": "437", "words": ["The", "2017", "purge", "of", "the", "Saudi", "political", "and", "business", "elite", "was", "followed", "in", "2018", "by", "arrests", "of", "17", "women", "'s", "rights", "activists", ",", "including", "Aziza", "al-Yousef", ",", "Loujain", "al-Hathloul", ",", "Eman", "al-Nafjan", ",", "Aisha", "al-Mana", "and", "Madeha", "al-Ajroush"], "labels": ["O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, politician, election, organization, person, political party, location, event and O.\nSentence: The 2017 purge of the Saudi political and business elite was followed in 2018 by arrests of 17 women 's rights activists , including Aziza al-Yousef , Loujain al-Hathloul , Eman al-Nafjan , Aisha al-Mana and Madeha al-Ajroush", "prompt_labels": "The(O) 2017(O) purge(O) of(O) the(O) Saudi(B-country) political(O) and(O) business(O) elite(O) was(O) followed(O) in(O) 2018(O) by(O) arrests(O) of(O) 17(O) women(O) 's(O) rights(O) activists(O) ,(O) including(O) Aziza(B-person) al-Yousef(I-person) ,(O) Loujain(B-person) al-Hathloul(I-person) ,(O) Eman(B-person) al-Nafjan(I-person) ,(O) Aisha(B-person) al-Mana(I-person) and(O) Madeha(B-person) al-Ajroush(I-person)"}}
{"id": "157", "dataset": "crossner_politics", "split": "dev", "label_list": ["organization", "country", "political party", "location", "event", "politician", "person", "election"], "instance": {"id": "157", "words": ["Despite", "the", "unsuccessful", "attempt", "at", "the", "1987", "Sarawak", "state", "election", ",", "Abdul", "Rahman", "continued", "his", "struggle", "with", "his", "allies", ",", "Sarawak", "Dayak", "People", "'s", "Party", "against", "Taib", "'s", "led", "Sarawak", "Barisan", "Nasional", "until", "1991", "Sarawak", "state", "election", "when", "Taib", "'s", "coalition", "won", "an", "overwhelming", "majority", "of", "49", "out", "of", "56", "seats", "in", "the", "state", "assembly", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "B-location", "B-political party", "I-political party", "I-political party", "I-political party", "O", "B-politician", "O", "O", "B-location", "B-political party", "I-political party", "O", "B-election", "I-election", "I-election", "I-election", "O", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, political party, location, event, politician, person, election and O.\nSentence: Despite the unsuccessful attempt at the 1987 Sarawak state election , Abdul Rahman continued his struggle with his allies , Sarawak Dayak People 's Party against Taib 's led Sarawak Barisan Nasional until 1991 Sarawak state election when Taib 's coalition won an overwhelming majority of 49 out of 56 seats in the state assembly .", "prompt_labels": "Despite(O) the(O) unsuccessful(O) attempt(O) at(O) the(O) 1987(B-election) Sarawak(I-election) state(I-election) election(I-election) ,(O) Abdul(B-politician) Rahman(I-politician) continued(O) his(O) struggle(O) with(O) his(O) allies(O) ,(O) Sarawak(B-location) Dayak(B-political party) People(I-political party) 's(I-political party) Party(I-political party) against(O) Taib(B-politician) 's(O) led(O) Sarawak(B-location) Barisan(B-political party) Nasional(I-political party) until(O) 1991(B-election) Sarawak(I-election) state(I-election) election(I-election) when(O) Taib(B-politician) 's(O) coalition(O) won(O) an(O) overwhelming(O) majority(O) of(O) 49(O) out(O) of(O) 56(O) seats(O) in(O) the(O) state(O) assembly(O) .(O)"}}
{"id": "50", "dataset": "crossner_politics", "split": "dev", "label_list": ["person", "country", "politician", "political party", "election", "event", "location", "organization"], "instance": {"id": "50", "words": ["In", "1987", "United", "Kingdom", "general", "election", "Adams", "narrowly", "held", "his", "seat", ",", "but", "lost", "it", "in", "the", "1992", "United", "Kingdom", "general", "election", "amidst", "a", "strong", "tactical", "voting", "campaign", "in", "favour", "of", "Joe", "Hendron", "of", "the", "Social", "Democratic", "and", "Labour", "Party", "by", "unionists"], "labels": ["O", "B-election", "I-election", "I-election", "I-election", "I-election", "B-politician", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "B-political party", "I-political party", "I-political party", "I-political party", "I-political party", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, politician, political party, election, event, location, organization and O.\nSentence: In 1987 United Kingdom general election Adams narrowly held his seat , but lost it in the 1992 United Kingdom general election amidst a strong tactical voting campaign in favour of Joe Hendron of the Social Democratic and Labour Party by unionists", "prompt_labels": "In(O) 1987(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) Adams(B-politician) narrowly(O) held(O) his(O) seat(O) ,(O) but(O) lost(O) it(O) in(O) the(O) 1992(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) amidst(O) a(O) strong(O) tactical(O) voting(O) campaign(O) in(O) favour(O) of(O) Joe(B-politician) Hendron(I-politician) of(O) the(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) by(O) unionists(O)"}}
{"id": "259", "dataset": "crossner_politics", "split": "dev", "label_list": ["election", "location", "politician", "country", "political party", "event", "person", "organization"], "instance": {"id": "259", "words": ["Although", "Republican", "Dwight", "D.", "Eisenhower", "carried", "half", "the", "South", "in", "1952", "United", "States", "presidential", "election", "and", "1956", "United", "States", "presidential", "election", "and", "Senator", "Barry", "Goldwater", "also", "carried", "five", "Southern", "states", "in", "1964", "United", "States", "presidential", "election", ",", "Democrat", "Jimmy", "Carter", "carried", "all", "of", "the", "South", "except", "Virginia", "and", "there", "was", "no", "long-term", "realignment", "until", "Ronald", "Reagan", "'", "s", "sweeping", "victories", "in", "the", "South", "in", "1980", "United", "States", "presidential", "election", "and", "1984", "United", "States", "presidential", "election", "."], "labels": ["O", "O", "B-politician", "I-politician", "I-politician", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "B-politician", "I-politician", "O", "O", "O", "O", "O", "O", "O", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O", "B-election", "I-election", "I-election", "I-election", "I-election", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, politician, country, political party, event, person, organization and O.\nSentence: Although Republican Dwight D. Eisenhower carried half the South in 1952 United States presidential election and 1956 United States presidential election and Senator Barry Goldwater also carried five Southern states in 1964 United States presidential election , Democrat Jimmy Carter carried all of the South except Virginia and there was no long-term realignment until Ronald Reagan ' s sweeping victories in the South in 1980 United States presidential election and 1984 United States presidential election .", "prompt_labels": "Although(O) Republican(O) Dwight(B-politician) D.(I-politician) Eisenhower(I-politician) carried(O) half(O) the(O) South(O) in(O) 1952(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) 1956(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) Senator(O) Barry(B-politician) Goldwater(I-politician) also(O) carried(O) five(O) Southern(O) states(O) in(O) 1964(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) Democrat(O) Jimmy(B-politician) Carter(I-politician) carried(O) all(O) of(O) the(O) South(O) except(O) Virginia(B-location) and(O) there(O) was(O) no(O) long-term(O) realignment(O) until(O) Ronald(B-politician) Reagan(I-politician) '(O) s(O) sweeping(O) victories(O) in(O) the(O) South(O) in(O) 1980(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) and(O) 1984(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) .(O)"}}
{"id": "101", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "chemical element", "organization", "country", "protein", "location", "enzyme", "person", "award", "theory", "academic journal", "university", "scientist", "event", "discipline", "chemical compound"], "instance": {"id": "101", "words": ["Eggleton", "is", "the", "author", "and", "coauthor", "of", "more", "than", "480", "journal", "publications", ",", "including", "articles", "in", "Nature", "Photonics", ",", "Nature", "Physics", ",", "Nature", "Communications", ",", "Physical", "Review", "Letters", "and", "Optica", "and", "over", "200", "invited", "presentations", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, chemical element, organization, country, protein, location, enzyme, person, award, theory, academic journal, university, scientist, event, discipline, chemical compound and O.\nSentence: Eggleton is the author and coauthor of more than 480 journal publications , including articles in Nature Photonics , Nature Physics , Nature Communications , Physical Review Letters and Optica and over 200 invited presentations .", "prompt_labels": "Eggleton(B-scientist) is(O) the(O) author(O) and(O) coauthor(O) of(O) more(O) than(O) 480(O) journal(O) publications(O) ,(O) including(O) articles(O) in(O) Nature(B-academic journal) Photonics(I-academic journal) ,(O) Nature(B-academic journal) Physics(I-academic journal) ,(O) Nature(B-academic journal) Communications(I-academic journal) ,(O) Physical(B-academic journal) Review(I-academic journal) Letters(I-academic journal) and(O) Optica(B-academic journal) and(O) over(O) 200(O) invited(O) presentations(O) .(O)"}}
{"id": "128", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "chemical element", "university", "person", "event", "astronomical object", "scientist", "academic journal", "location", "organization", "chemical compound", "award", "protein", "theory", "enzyme", "country"], "instance": {"id": "128", "words": ["This", "range", ",", "as", "well", "as", "the", "relative", "speeds", "between", "the", "planets", ",", "led", "Kepler", "to", "conclude", "that", "the", "Solar", "System", "was", "composed", "of", "two", "basses", "(", "Saturn", "and", "Jupiter", ")", ",", "a", "tenor", "(", "Mars", ")", ",", "two", "altos", "(", "Venus", "and", "Earth", ")", ",", "and", "a", "soprano", "(", "Mercury", ")", ",", "which", "had", "sung", "in", "perfect", "concord", ",", "at", "the", "beginning", "of", "time", ",", "and", "could", "potentially", "arrange", "themselves", "to", "do", "so", "again", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical element, university, person, event, astronomical object, scientist, academic journal, location, organization, chemical compound, award, protein, theory, enzyme, country and O.\nSentence: This range , as well as the relative speeds between the planets , led Kepler to conclude that the Solar System was composed of two basses ( Saturn and Jupiter ) , a tenor ( Mars ) , two altos ( Venus and Earth ) , and a soprano ( Mercury ) , which had sung in perfect concord , at the beginning of time , and could potentially arrange themselves to do so again .", "prompt_labels": "This(O) range(O) ,(O) as(O) well(O) as(O) the(O) relative(O) speeds(O) between(O) the(O) planets(O) ,(O) led(O) Kepler(B-scientist) to(O) conclude(O) that(O) the(O) Solar(O) System(O) was(O) composed(O) of(O) two(O) basses(O) ((O) Saturn(B-astronomical object) and(O) Jupiter(B-astronomical object) )(O) ,(O) a(O) tenor(O) ((O) Mars(B-astronomical object) )(O) ,(O) two(O) altos(O) ((O) Venus(B-astronomical object) and(O) Earth(B-astronomical object) )(O) ,(O) and(O) a(O) soprano(O) ((O) Mercury(B-astronomical object) )(O) ,(O) which(O) had(O) sung(O) in(O) perfect(O) concord(O) ,(O) at(O) the(O) beginning(O) of(O) time(O) ,(O) and(O) could(O) potentially(O) arrange(O) themselves(O) to(O) do(O) so(O) again(O) .(O)"}}
{"id": "411", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "protein", "theory", "chemical compound", "discipline", "enzyme", "scientist", "university", "academic journal", "person", "chemical element", "award", "event", "country", "organization", "location"], "instance": {"id": "411", "words": ["Many", "different", "types", "of", "nanowires", "exist", ",", "including", "superconducting", "(", "e.g.", "Yttrium", "barium", "copper", "oxide", ")", ",", "metallic", "(", "e.g.", "Ni", ",", "Pt", ",", "Au", ")", ",", "semiconducting", "(", "e.g.", "silicon", "nanowires", "(", "SiNWs", ")", ",", "InP", ",", "GaN", ")", "and", "insulating", "(", "e.g.", "Silicon", "dioxide", ",", "Titanium", "dioxide", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "I-chemical compound", "O", "O", "O", "O", "O", "B-chemical element", "O", "B-chemical element", "O", "B-chemical element", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, protein, theory, chemical compound, discipline, enzyme, scientist, university, academic journal, person, chemical element, award, event, country, organization, location and O.\nSentence: Many different types of nanowires exist , including superconducting ( e.g. Yttrium barium copper oxide ) , metallic ( e.g. Ni , Pt , Au ) , semiconducting ( e.g. silicon nanowires ( SiNWs ) , InP , GaN ) and insulating ( e.g. Silicon dioxide , Titanium dioxide ) .", "prompt_labels": "Many(O) different(O) types(O) of(O) nanowires(O) exist(O) ,(O) including(O) superconducting(O) ((O) e.g.(O) Yttrium(B-chemical compound) barium(I-chemical compound) copper(I-chemical compound) oxide(I-chemical compound) )(O) ,(O) metallic(O) ((O) e.g.(O) Ni(B-chemical element) ,(O) Pt(B-chemical element) ,(O) Au(B-chemical element) )(O) ,(O) semiconducting(O) ((O) e.g.(O) silicon(O) nanowires(O) ((O) SiNWs(O) )(O) ,(O) InP(B-chemical compound) ,(O) GaN(B-chemical compound) )(O) and(O) insulating(O) ((O) e.g.(O) Silicon(B-chemical compound) dioxide(I-chemical compound) ,(O) Titanium(B-chemical compound) dioxide(I-chemical compound) )(O) .(O)"}}
{"id": "320", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "chemical element", "university", "enzyme", "astronomical object", "person", "discipline", "chemical compound", "organization", "theory", "award", "protein", "location", "scientist", "country", "event"], "instance": {"id": "320", "words": ["The", "society", "has", "published", "articles", "in", "journals", "such", "as", "Acta", "Chemica", "Scandinavia", "and", ",", "since", "2000", ",", "British", "journals", "including", "Dalton", "Transactions", "(", "Inorganic", "Chemistry", ")", "and", "Perkin", "Transactions", "(", "Organic", "Chemistry", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "O", "B-discipline", "I-discipline", "O", "O", "B-academic journal", "I-academic journal", "O", "B-discipline", "I-discipline", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, chemical element, university, enzyme, astronomical object, person, discipline, chemical compound, organization, theory, award, protein, location, scientist, country, event and O.\nSentence: The society has published articles in journals such as Acta Chemica Scandinavia and , since 2000 , British journals including Dalton Transactions ( Inorganic Chemistry ) and Perkin Transactions ( Organic Chemistry ) .", "prompt_labels": "The(O) society(O) has(O) published(O) articles(O) in(O) journals(O) such(O) as(O) Acta(B-academic journal) Chemica(I-academic journal) Scandinavia(I-academic journal) and(O) ,(O) since(O) 2000(O) ,(O) British(O) journals(O) including(O) Dalton(B-academic journal) Transactions(I-academic journal) ((O) Inorganic(B-discipline) Chemistry(I-discipline) )(O) and(O) Perkin(B-academic journal) Transactions(I-academic journal) ((O) Organic(B-discipline) Chemistry(I-discipline) )(O) .(O)"}}
{"id": "385", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "theory", "protein", "astronomical object", "chemical compound", "chemical element", "scientist", "event", "person", "country", "location", "award", "university", "academic journal", "discipline", "organization"], "instance": {"id": "385", "words": ["She", "competed", "in", "Cross-country", "skiing", "at", "the", "1976", "Winter", "Olympics", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, theory, protein, astronomical object, chemical compound, chemical element, scientist, event, person, country, location, award, university, academic journal, discipline, organization and O.\nSentence: She competed in Cross-country skiing at the 1976 Winter Olympics .", "prompt_labels": "She(O) competed(O) in(O) Cross-country(O) skiing(O) at(O) the(O) 1976(B-event) Winter(I-event) Olympics(I-event) .(O)"}}
{"id": "119", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "organization", "chemical element", "person", "event", "scientist", "chemical compound", "protein", "country", "enzyme", "university", "academic journal", "astronomical object", "award", "theory", "discipline"], "instance": {"id": "119", "words": ["Thus", ",", "Jupiter", "and", "Saturn", "are", "gas", "giants", ",", "and", "Uranus", "and", "Neptune", "are", "ice", "giant", "s", ",", "even", "though", "the", "vast", "majority", "of", "the", "gas", "and", "ice", "in", "their", "interiors", "is", "a", "hot", ",", "highly", "dense", "fluid", "that", "gets", "denser", "as", "the", "center", "of", "the", "planet", "is", "approached", "."], "labels": ["O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, chemical element, person, event, scientist, chemical compound, protein, country, enzyme, university, academic journal, astronomical object, award, theory, discipline and O.\nSentence: Thus , Jupiter and Saturn are gas giants , and Uranus and Neptune are ice giant s , even though the vast majority of the gas and ice in their interiors is a hot , highly dense fluid that gets denser as the center of the planet is approached .", "prompt_labels": "Thus(O) ,(O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) are(O) gas(O) giants(O) ,(O) and(O) Uranus(B-astronomical object) and(O) Neptune(B-astronomical object) are(O) ice(O) giant(O) s(O) ,(O) even(O) though(O) the(O) vast(O) majority(O) of(O) the(O) gas(O) and(O) ice(O) in(O) their(O) interiors(O) is(O) a(O) hot(O) ,(O) highly(O) dense(O) fluid(O) that(O) gets(O) denser(O) as(O) the(O) center(O) of(O) the(O) planet(O) is(O) approached(O) .(O)"}}
{"id": "166", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical compound", "university", "scientist", "astronomical object", "academic journal", "discipline", "protein", "chemical element", "person", "enzyme", "award", "location", "event", "country", "theory", "organization"], "instance": {"id": "166", "words": ["19126", "Ottohahn", "named", "in", "his", "honor", ",", "as", "were", "the", "Otto", "Hahn", "Prize", "of", "both", "the", "German", "Chemical", "and", "Physical", "Societies", "and", "the", "city", "of", "Frankfurt", "/", "Main", ",", "the", "Otto", "Hahn", "Medal", ",", "and", "the", "Otto", "Hahn", "Award", "of", "the", "Max", "Planck", "Society", "and", ",", "since", "1988", ",", "the", "Otto", "Hahn", "Peace", "Medal", "in", "Gold", "of", "the", "United", "Nations", "Association", "of", "Germany", "(", "DGVN", ")", "in", "Berlin", "."], "labels": ["B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-organization", "O", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, university, scientist, astronomical object, academic journal, discipline, protein, chemical element, person, enzyme, award, location, event, country, theory, organization and O.\nSentence: 19126 Ottohahn named in his honor , as were the Otto Hahn Prize of both the German Chemical and Physical Societies and the city of Frankfurt / Main , the Otto Hahn Medal , and the Otto Hahn Award of the Max Planck Society and , since 1988 , the Otto Hahn Peace Medal in Gold of the United Nations Association of Germany ( DGVN ) in Berlin .", "prompt_labels": "19126(B-astronomical object) Ottohahn(I-astronomical object) named(O) in(O) his(O) honor(O) ,(O) as(O) were(O) the(O) Otto(B-award) Hahn(I-award) Prize(I-award) of(I-award) both(I-award) the(I-award) German(I-award) Chemical(I-award) and(I-award) Physical(I-award) Societies(I-award) and(O) the(O) city(O) of(O) Frankfurt(B-location) /(O) Main(O) ,(O) the(O) Otto(B-award) Hahn(I-award) Medal(I-award) ,(O) and(O) the(O) Otto(B-award) Hahn(I-award) Award(I-award) of(I-award) the(I-award) Max(I-award) Planck(I-award) Society(I-award) and(O) ,(O) since(O) 1988(O) ,(O) the(O) Otto(B-award) Hahn(I-award) Peace(I-award) Medal(I-award) in(I-award) Gold(I-award) of(I-award) the(I-award) United(I-award) Nations(I-award) Association(I-award) of(I-award) Germany(I-award) ((O) DGVN(B-organization) )(O) in(O) Berlin(B-location) .(O)"}}
{"id": "444", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "discipline", "country", "award", "protein", "event", "location", "astronomical object", "organization", "scientist", "academic journal", "university", "enzyme", "person", "theory", "chemical compound"], "instance": {"id": "444", "words": ["Certain", "alleles", "of", "the", "human", "major", "histocompatibility", "complex", "(", "Major", "histocompatibility", "complex", ",", "called", "Human", "leukocyte", "antigen", "in", "humans", ")", "have", "been", "associated", "with", "the", "presence", "of", "Anti-Ro", "antibodies", "and", "the", "spread", "of", "the", "immune", "response", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, discipline, country, award, protein, event, location, astronomical object, organization, scientist, academic journal, university, enzyme, person, theory, chemical compound and O.\nSentence: Certain alleles of the human major histocompatibility complex ( Major histocompatibility complex , called Human leukocyte antigen in humans ) have been associated with the presence of Anti-Ro antibodies and the spread of the immune response .", "prompt_labels": "Certain(O) alleles(O) of(O) the(O) human(O) major(O) histocompatibility(O) complex(O) ((O) Major(O) histocompatibility(O) complex(O) ,(O) called(O) Human(O) leukocyte(O) antigen(O) in(O) humans(O) )(O) have(O) been(O) associated(O) with(O) the(O) presence(O) of(O) Anti-Ro(B-protein) antibodies(I-protein) and(O) the(O) spread(O) of(O) the(O) immune(O) response(O) .(O)"}}
{"id": "432", "dataset": "crossner_science", "split": "dev", "label_list": ["person", "enzyme", "scientist", "location", "chemical compound", "discipline", "astronomical object", "protein", "theory", "university", "event", "organization", "award", "academic journal", "country", "chemical element"], "instance": {"id": "432", "words": ["He", "was", "awarded", "the", "1999", "Nobel", "Prize", "in", "Chemistry", "for", "his", "work", "on", "femtochemistry", "and", "became", "the", "first", "Egyptian", "to", "win", "a", "Nobel", "Prize", "in", "a", "scientific", "field", "."], "labels": ["O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-discipline", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, enzyme, scientist, location, chemical compound, discipline, astronomical object, protein, theory, university, event, organization, award, academic journal, country, chemical element and O.\nSentence: He was awarded the 1999 Nobel Prize in Chemistry for his work on femtochemistry and became the first Egyptian to win a Nobel Prize in a scientific field .", "prompt_labels": "He(O) was(O) awarded(O) the(O) 1999(O) Nobel(B-award) Prize(I-award) in(I-award) Chemistry(I-award) for(O) his(O) work(O) on(O) femtochemistry(B-discipline) and(O) became(O) the(O) first(O) Egyptian(O) to(O) win(O) a(O) Nobel(B-award) Prize(I-award) in(O) a(O) scientific(O) field(O) .(O)"}}
{"id": "181", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "location", "university", "protein", "chemical element", "enzyme", "discipline", "chemical compound", "organization", "award", "astronomical object", "academic journal", "event", "scientist", "person", "country"], "instance": {"id": "181", "words": ["He", "was", "elected", "a", "foreign", "member", "of", "the", "Royal", "Swedish", "Academy", "of", "Sciences", "in", "1836", ",", "and", "a", "Foreign", "Honorary", "Member", "of", "the", "American", "Academy", "of", "Arts", "and", "Sciences", "in", "1849", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, location, university, protein, chemical element, enzyme, discipline, chemical compound, organization, award, astronomical object, academic journal, event, scientist, person, country and O.\nSentence: He was elected a foreign member of the Royal Swedish Academy of Sciences in 1836 , and a Foreign Honorary Member of the American Academy of Arts and Sciences in 1849 .", "prompt_labels": "He(O) was(O) elected(O) a(O) foreign(O) member(O) of(O) the(O) Royal(B-organization) Swedish(I-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) in(O) 1836(O) ,(O) and(O) a(O) Foreign(O) Honorary(O) Member(O) of(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) in(O) 1849(O) .(O)"}}
{"id": "383", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "astronomical object", "country", "enzyme", "chemical compound", "award", "event", "academic journal", "university", "organization", "discipline", "location", "scientist", "protein", "theory", "person"], "instance": {"id": "383", "words": ["The", "following", "asteroids", "were", "named", "in", "memory", "of", "the", "other", "six", "members", "of", "STS-107", ":", "51823", "Rickhusband", ",", "51825", "Davidbrown", ",", "51826", "Kalpanachawla", ",", "51827", "Laurelclark", ",", "51828", "Ilanramon", "and", "51829", "Williemccool", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, astronomical object, country, enzyme, chemical compound, award, event, academic journal, university, organization, discipline, location, scientist, protein, theory, person and O.\nSentence: The following asteroids were named in memory of the other six members of STS-107 : 51823 Rickhusband , 51825 Davidbrown , 51826 Kalpanachawla , 51827 Laurelclark , 51828 Ilanramon and 51829 Williemccool .", "prompt_labels": "The(O) following(O) asteroids(O) were(O) named(O) in(O) memory(O) of(O) the(O) other(O) six(O) members(O) of(O) STS-107(B-event) :(O) 51823(B-astronomical object) Rickhusband(I-astronomical object) ,(O) 51825(B-astronomical object) Davidbrown(I-astronomical object) ,(O) 51826(B-astronomical object) Kalpanachawla(I-astronomical object) ,(O) 51827(B-astronomical object) Laurelclark(I-astronomical object) ,(O) 51828(B-astronomical object) Ilanramon(I-astronomical object) and(O) 51829(B-astronomical object) Williemccool(I-astronomical object) .(O)"}}
{"id": "118", "dataset": "crossner_science", "split": "dev", "label_list": ["country", "theory", "protein", "scientist", "discipline", "university", "chemical compound", "person", "astronomical object", "academic journal", "enzyme", "location", "award", "organization", "chemical element", "event"], "instance": {"id": "118", "words": ["The", "Dirac", "Medal", "of", "the", "ICTP", "is", "not", "awarded", "to", "Nobel", "Prize", ",", "Fields", "Medal", "ists", ",", "or", "Wolf", "Prize", "winners", "."], "labels": ["O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "O", "O", "O", "B-award", "I-award", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, theory, protein, scientist, discipline, university, chemical compound, person, astronomical object, academic journal, enzyme, location, award, organization, chemical element, event and O.\nSentence: The Dirac Medal of the ICTP is not awarded to Nobel Prize , Fields Medal ists , or Wolf Prize winners .", "prompt_labels": "The(O) Dirac(B-award) Medal(I-award) of(I-award) the(I-award) ICTP(I-award) is(O) not(O) awarded(O) to(O) Nobel(B-award) Prize(I-award) ,(O) Fields(B-award) Medal(I-award) ists(O) ,(O) or(O) Wolf(B-award) Prize(I-award) winners(O) .(O)"}}
{"id": "131", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "theory", "person", "enzyme", "chemical element", "event", "protein", "organization", "academic journal", "astronomical object", "award", "chemical compound", "scientist", "country", "university", "location"], "instance": {"id": "131", "words": ["Wasserburg", "completed", "his", "Ph.D.", "from", "the", "University", "of", "Chicago", "in", "1954", ",", "with", "a", "thesis", "on", "the", "development", "of", "K-Ar", "dating", ",", "done", "under", "the", "sponsorship", "of", "Harold", "Urey", "and", "Mark", "Inghram", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-theory", "I-theory", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, theory, person, enzyme, chemical element, event, protein, organization, academic journal, astronomical object, award, chemical compound, scientist, country, university, location and O.\nSentence: Wasserburg completed his Ph.D. from the University of Chicago in 1954 , with a thesis on the development of K-Ar dating , done under the sponsorship of Harold Urey and Mark Inghram .", "prompt_labels": "Wasserburg(B-scientist) completed(O) his(O) Ph.D.(O) from(O) the(O) University(B-university) of(I-university) Chicago(I-university) in(O) 1954(O) ,(O) with(O) a(O) thesis(O) on(O) the(O) development(O) of(O) K-Ar(B-theory) dating(I-theory) ,(O) done(O) under(O) the(O) sponsorship(O) of(O) Harold(B-scientist) Urey(I-scientist) and(O) Mark(B-scientist) Inghram(I-scientist) .(O)"}}
{"id": "145", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "chemical element", "scientist", "event", "enzyme", "country", "location", "chemical compound", "award", "astronomical object", "discipline", "protein", "university", "academic journal", "organization", "person"], "instance": {"id": "145", "words": ["He", "was", "a", "professor", "of", "microbiology", ",", "and", "of", "public", "health", "at", "the", "Prince", "Leopold", "Institute", "of", "Tropical", "Medicine", ",", "in", "Antwerp", ",", "and", "at", "the", "University", "of", "Nairobi", ",", "Vrije", "Universiteit", "Brussel", ",", "the", "University", "of", "Lausanne", ",", "and", "a", "visiting", "professor", "at", "the", "London", "School", "of", "Economics", "."], "labels": ["O", "O", "O", "O", "O", "B-discipline", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-location", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "B-university", "I-university", "I-university", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, chemical element, scientist, event, enzyme, country, location, chemical compound, award, astronomical object, discipline, protein, university, academic journal, organization, person and O.\nSentence: He was a professor of microbiology , and of public health at the Prince Leopold Institute of Tropical Medicine , in Antwerp , and at the University of Nairobi , Vrije Universiteit Brussel , the University of Lausanne , and a visiting professor at the London School of Economics .", "prompt_labels": "He(O) was(O) a(O) professor(O) of(O) microbiology(B-discipline) ,(O) and(O) of(O) public(O) health(O) at(O) the(O) Prince(B-organization) Leopold(I-organization) Institute(I-organization) of(I-organization) Tropical(I-organization) Medicine(I-organization) ,(O) in(O) Antwerp(B-location) ,(O) and(O) at(O) the(O) University(B-university) of(I-university) Nairobi(I-university) ,(O) Vrije(B-university) Universiteit(I-university) Brussel(I-university) ,(O) the(O) University(B-university) of(I-university) Lausanne(I-university) ,(O) and(O) a(O) visiting(O) professor(O) at(O) the(O) London(B-organization) School(I-organization) of(I-organization) Economics(I-organization) .(O)"}}
{"id": "426", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "academic journal", "chemical compound", "astronomical object", "discipline", "organization", "enzyme", "award", "protein", "country", "event", "person", "location", "university", "chemical element", "scientist"], "instance": {"id": "426", "words": ["Arkadelphia", "is", "home", "to", "two", "liberal", "arts", "institutions", ":", "Henderson", "State", "University", "(", "founded", "in", "1890", "as", "Arkadelphia", "Methodist", "College", ")", ",", "which", "is", "the", "only", "member", "of", "the", "Council", "of", "Public", "Liberal", "Arts", "Colleges", "based", "in", "Arkansas", "and", "announced", "plans", "to", "join", "the", "Arkansas", "State", "University", "System", "in", "October", "2019", ",", "and", "Ouachita", "Baptist", "University", ",", "a", "private", ",", "Baptist", "college", "affiliated", "with", "the", "Arkansas", "Baptist", "State", "Convention", "(", "opened", "in", "1886", ")", "."], "labels": ["B-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-location", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, academic journal, chemical compound, astronomical object, discipline, organization, enzyme, award, protein, country, event, person, location, university, chemical element, scientist and O.\nSentence: Arkadelphia is home to two liberal arts institutions : Henderson State University ( founded in 1890 as Arkadelphia Methodist College ) , which is the only member of the Council of Public Liberal Arts Colleges based in Arkansas and announced plans to join the Arkansas State University System in October 2019 , and Ouachita Baptist University , a private , Baptist college affiliated with the Arkansas Baptist State Convention ( opened in 1886 ) .", "prompt_labels": "Arkadelphia(B-location) is(O) home(O) to(O) two(O) liberal(O) arts(O) institutions(O) :(O) Henderson(B-university) State(I-university) University(I-university) ((O) founded(O) in(O) 1890(O) as(O) Arkadelphia(B-university) Methodist(I-university) College(I-university) )(O) ,(O) which(O) is(O) the(O) only(O) member(O) of(O) the(O) Council(B-organization) of(I-organization) Public(I-organization) Liberal(I-organization) Arts(I-organization) Colleges(I-organization) based(O) in(O) Arkansas(B-location) and(O) announced(O) plans(O) to(O) join(O) the(O) Arkansas(B-university) State(I-university) University(I-university) System(I-university) in(O) October(O) 2019(O) ,(O) and(O) Ouachita(B-university) Baptist(I-university) University(I-university) ,(O) a(O) private(O) ,(O) Baptist(O) college(O) affiliated(O) with(O) the(O) Arkansas(B-organization) Baptist(I-organization) State(I-organization) Convention(I-organization) ((O) opened(O) in(O) 1886(O) )(O) .(O)"}}
{"id": "361", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "organization", "chemical element", "discipline", "person", "enzyme", "protein", "location", "country", "theory", "astronomical object", "event", "chemical compound", "award", "academic journal", "scientist"], "instance": {"id": "361", "words": ["She", "has", "been", "on", "the", "editorial", "board", "of", "Biochimica", "et", "Biophysica", "Acta", "and", "an", "editor", "of", "European", "Biophysics", "Journal", "and", "a", "member", "of", "the", "editorial", "advisory", "board", "of", "Accounts", "of", "Chemical", "Research", "and", "Chemical", "Reviews", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, organization, chemical element, discipline, person, enzyme, protein, location, country, theory, astronomical object, event, chemical compound, award, academic journal, scientist and O.\nSentence: She has been on the editorial board of Biochimica et Biophysica Acta and an editor of European Biophysics Journal and a member of the editorial advisory board of Accounts of Chemical Research and Chemical Reviews .", "prompt_labels": "She(O) has(O) been(O) on(O) the(O) editorial(O) board(O) of(O) Biochimica(B-academic journal) et(I-academic journal) Biophysica(I-academic journal) Acta(I-academic journal) and(O) an(O) editor(O) of(O) European(B-academic journal) Biophysics(I-academic journal) Journal(I-academic journal) and(O) a(O) member(O) of(O) the(O) editorial(O) advisory(O) board(O) of(O) Accounts(B-academic journal) of(I-academic journal) Chemical(I-academic journal) Research(I-academic journal) and(O) Chemical(B-academic journal) Reviews(I-academic journal) .(O)"}}
{"id": "393", "dataset": "crossner_science", "split": "dev", "label_list": ["event", "award", "country", "theory", "academic journal", "scientist", "person", "astronomical object", "chemical element", "protein", "discipline", "organization", "enzyme", "university", "location", "chemical compound"], "instance": {"id": "393", "words": ["The", "RCS", "and", "the", "Royal", "School", "of", "Mines", "subsequently", "merged", "in", "1907", "with", "the", "City", "and", "Guilds", "Central", "Technical", "College", "to", "form", "the", "Imperial", "College", "of", "Science", "and", "Technology", ",", "each", "continuing", "as", "a", "Constituent", "College", "of", "Imperial", ",", "which", "then", "joined", "the", "University", "of", "London", "in", "1929", "."], "labels": ["O", "B-university", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "I-university", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "I-university", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, country, theory, academic journal, scientist, person, astronomical object, chemical element, protein, discipline, organization, enzyme, university, location, chemical compound and O.\nSentence: The RCS and the Royal School of Mines subsequently merged in 1907 with the City and Guilds Central Technical College to form the Imperial College of Science and Technology , each continuing as a Constituent College of Imperial , which then joined the University of London in 1929 .", "prompt_labels": "The(O) RCS(B-university) and(O) the(O) Royal(B-university) School(I-university) of(I-university) Mines(I-university) subsequently(O) merged(O) in(O) 1907(O) with(O) the(O) City(B-university) and(I-university) Guilds(I-university) Central(I-university) Technical(I-university) College(I-university) to(O) form(O) the(O) Imperial(B-university) College(I-university) of(I-university) Science(I-university) and(I-university) Technology(I-university) ,(O) each(O) continuing(O) as(O) a(O) Constituent(B-university) College(I-university) of(I-university) Imperial(I-university) ,(O) which(O) then(O) joined(O) the(O) University(B-university) of(I-university) London(I-university) in(O) 1929(O) .(O)"}}
{"id": "70", "dataset": "crossner_science", "split": "dev", "label_list": ["organization", "event", "award", "chemical element", "discipline", "person", "location", "scientist", "university", "academic journal", "chemical compound", "protein", "enzyme", "astronomical object", "theory", "country"], "instance": {"id": "70", "words": ["FastPP", "can", "be", "used", "on", "unpurified", ",", "complex", "mixtures", "of", "proteins", "and", "proteins", "fused", "with", "other", "proteins", ",", "such", "as", "Glutathione", "S-transferase", "or", "Green", "fluorescent", "protein", ",", "as", "long", "as", "the", "sequence", "that", "is", "the", "target", "of", "the", "western", "blot", ",", "e.g.", ",", "His-tag", ",", "is", "directly", "linked", "to", "the", "protein", "of", "interest", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O", "B-protein", "I-protein", "I-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, award, chemical element, discipline, person, location, scientist, university, academic journal, chemical compound, protein, enzyme, astronomical object, theory, country and O.\nSentence: FastPP can be used on unpurified , complex mixtures of proteins and proteins fused with other proteins , such as Glutathione S-transferase or Green fluorescent protein , as long as the sequence that is the target of the western blot , e.g. , His-tag , is directly linked to the protein of interest .", "prompt_labels": "FastPP(O) can(O) be(O) used(O) on(O) unpurified(O) ,(O) complex(O) mixtures(O) of(O) proteins(O) and(O) proteins(O) fused(O) with(O) other(O) proteins(O) ,(O) such(O) as(O) Glutathione(B-protein) S-transferase(I-protein) or(O) Green(B-protein) fluorescent(I-protein) protein(I-protein) ,(O) as(O) long(O) as(O) the(O) sequence(O) that(O) is(O) the(O) target(O) of(O) the(O) western(O) blot(O) ,(O) e.g.(O) ,(O) His-tag(O) ,(O) is(O) directly(O) linked(O) to(O) the(O) protein(O) of(O) interest(O) .(O)"}}
{"id": "155", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "organization", "award", "scientist", "enzyme", "event", "discipline", "country", "location", "astronomical object", "university", "person", "chemical element", "chemical compound", "protein", "academic journal"], "instance": {"id": "155", "words": ["Known", "for", "his", "research", "on", "Mitogen-activated", "protein", "kinase", "(", "MAPK", ")", "cascade", "in", "plants", ",", "he", "is", "a", "three-time", "Alexander", "von", "Humboldt", "Fellow", "and", "an", "elected", "fellow", "of", "the", "National", "Academy", "of", "Sciences", ",", "India", "."], "labels": ["O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "I-enzyme", "O", "B-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, organization, award, scientist, enzyme, event, discipline, country, location, astronomical object, university, person, chemical element, chemical compound, protein, academic journal and O.\nSentence: Known for his research on Mitogen-activated protein kinase ( MAPK ) cascade in plants , he is a three-time Alexander von Humboldt Fellow and an elected fellow of the National Academy of Sciences , India .", "prompt_labels": "Known(O) for(O) his(O) research(O) on(O) Mitogen-activated(B-enzyme) protein(I-enzyme) kinase(I-enzyme) ((O) MAPK(B-enzyme) )(O) cascade(O) in(O) plants(O) ,(O) he(O) is(O) a(O) three-time(O) Alexander(B-award) von(I-award) Humboldt(I-award) Fellow(I-award) and(O) an(O) elected(O) fellow(B-award) of(I-award) the(I-award) National(I-award) Academy(I-award) of(I-award) Sciences(I-award) ,(O) India(B-country) .(O)"}}
{"id": "198", "dataset": "crossner_science", "split": "dev", "label_list": ["award", "scientist", "enzyme", "protein", "person", "theory", "university", "discipline", "chemical compound", "country", "astronomical object", "event", "organization", "location", "chemical element", "academic journal"], "instance": {"id": "198", "words": ["Phthalocyanine", "s", ",", "which", ",", "like", "Phthalocyanine", "Blue", "BN", "and", "Phthalocyanine", "Green", "G", ",", "often", "contain", "a", "transition", "metal", "ion", ",", "exchange", "an", "electron", "with", "the", "complexed", "transition", "metal", "ion", "that", "easily", "changes", "its", "oxidation", "state", "."], "labels": ["B-chemical compound", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, scientist, enzyme, protein, person, theory, university, discipline, chemical compound, country, astronomical object, event, organization, location, chemical element, academic journal and O.\nSentence: Phthalocyanine s , which , like Phthalocyanine Blue BN and Phthalocyanine Green G , often contain a transition metal ion , exchange an electron with the complexed transition metal ion that easily changes its oxidation state .", "prompt_labels": "Phthalocyanine(B-chemical compound) s(O) ,(O) which(O) ,(O) like(O) Phthalocyanine(B-chemical compound) Blue(I-chemical compound) BN(I-chemical compound) and(O) Phthalocyanine(B-chemical compound) Green(I-chemical compound) G(I-chemical compound) ,(O) often(O) contain(O) a(O) transition(O) metal(O) ion(O) ,(O) exchange(O) an(O) electron(O) with(O) the(O) complexed(O) transition(O) metal(O) ion(O) that(O) easily(O) changes(O) its(O) oxidation(O) state(O) .(O)"}}
{"id": "229", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "astronomical object", "country", "person", "university", "protein", "theory", "chemical compound", "organization", "enzyme", "scientist", "discipline", "academic journal", "award", "event", "location"], "instance": {"id": "229", "words": ["Other", "proteins", "that", "are", "active", "outside", "the", "cell", "are", "various", "enzymes", ",", "including", "digestive", "enzymes", "(", "Trypsin", ",", "Pepsin", ")", ",", "extracellular", "proteinases", "(", "Matrix", "metalloproteinase", "s", ",", "ADAMTS", "s", ",", "Cathepsin", "s", ")", "and", "antioxidant", "enzymes", "(", "extracellular", "superoxide", "dismutase", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "O", "B-enzyme", "O", "O", "O", "O", "O", "B-enzyme", "B-enzyme", "O", "O", "B-enzyme", "O", "O", "B-enzyme", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, astronomical object, country, person, university, protein, theory, chemical compound, organization, enzyme, scientist, discipline, academic journal, award, event, location and O.\nSentence: Other proteins that are active outside the cell are various enzymes , including digestive enzymes ( Trypsin , Pepsin ) , extracellular proteinases ( Matrix metalloproteinase s , ADAMTS s , Cathepsin s ) and antioxidant enzymes ( extracellular superoxide dismutase ) .", "prompt_labels": "Other(O) proteins(O) that(O) are(O) active(O) outside(O) the(O) cell(O) are(O) various(O) enzymes(O) ,(O) including(O) digestive(O) enzymes(O) ((O) Trypsin(B-enzyme) ,(O) Pepsin(B-enzyme) )(O) ,(O) extracellular(O) proteinases(O) ((O) Matrix(B-enzyme) metalloproteinase(B-enzyme) s(O) ,(O) ADAMTS(B-enzyme) s(O) ,(O) Cathepsin(B-enzyme) s(O) )(O) and(O) antioxidant(O) enzymes(O) ((O) extracellular(O) superoxide(B-enzyme) dismutase(I-enzyme) )(O) .(O)"}}
{"id": "8", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "discipline", "astronomical object", "academic journal", "university", "theory", "award", "scientist", "chemical compound", "chemical element", "protein", "person", "event", "enzyme", "country", "organization"], "instance": {"id": "8", "words": ["Removing", "a", "TAD", "boundary", "(", "for", "example", ",", "using", "CRISPR", "to", "delete", "the", "relevant", "region", "of", "the", "genome", ")", "can", "allow", "new", "promoter-enhancer", "contacts", "to", "form", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, discipline, astronomical object, academic journal, university, theory, award, scientist, chemical compound, chemical element, protein, person, event, enzyme, country, organization and O.\nSentence: Removing a TAD boundary ( for example , using CRISPR to delete the relevant region of the genome ) can allow new promoter-enhancer contacts to form .", "prompt_labels": "Removing(O) a(O) TAD(O) boundary(O) ((O) for(O) example(O) ,(O) using(O) CRISPR(O) to(O) delete(O) the(O) relevant(O) region(O) of(O) the(O) genome(O) )(O) can(O) allow(O) new(O) promoter-enhancer(O) contacts(O) to(O) form(O) .(O)"}}
{"id": "225", "dataset": "crossner_science", "split": "dev", "label_list": ["country", "award", "location", "chemical element", "scientist", "event", "person", "chemical compound", "academic journal", "university", "theory", "enzyme", "organization", "protein", "discipline", "astronomical object"], "instance": {"id": "225", "words": ["He", "is", "also", "on", "the", "editorial", "board", "of", "multiple", "scientific", "journals", ",", "including", "Cell", ",", "Cell", "Stem", "Cell", ",", "Stem", "Cell", "Reports", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, location, chemical element, scientist, event, person, chemical compound, academic journal, university, theory, enzyme, organization, protein, discipline, astronomical object and O.\nSentence: He is also on the editorial board of multiple scientific journals , including Cell , Cell Stem Cell , Stem Cell Reports .", "prompt_labels": "He(O) is(O) also(O) on(O) the(O) editorial(O) board(O) of(O) multiple(O) scientific(O) journals(O) ,(O) including(O) Cell(B-academic journal) ,(O) Cell(B-academic journal) Stem(I-academic journal) Cell(I-academic journal) ,(O) Stem(B-academic journal) Cell(I-academic journal) Reports(I-academic journal) .(O)"}}
{"id": "73", "dataset": "crossner_science", "split": "dev", "label_list": ["organization", "astronomical object", "protein", "country", "discipline", "person", "chemical compound", "theory", "award", "location", "university", "academic journal", "event", "chemical element", "scientist", "enzyme"], "instance": {"id": "73", "words": ["Glenn", "was", "an", "honorary", "member", "of", "the", "International", "Academy", "of", "Astronautics", "and", "a", "member", "of", "the", "Society", "of", "Experimental", "Test", "Pilots", ",", "Marine", "Corps", "Aviation", "Association", ",", "Order", "of", "Daedalians", ",", "National", "Space", "Club", "board", "of", "trustees", ",", "National", "Space", "Society", "board", "of", "governors", ",", "International", "Association", "of", "Holiday", "Inns", ",", "Ohio", "Democratic", "Party", ",", "State", "Democratic", "Executive", "Committee", ",", "Franklin", "County", "(", "Ohio", ")", "Democratic", "Party", "and", "the", "10th", "District", "(", "Ohio", ")", "Democratic", "Action", "Club", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-location", "I-location", "O", "B-location", "O", "B-organization", "I-organization", "O", "O", "B-location", "I-location", "O", "B-location", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, astronomical object, protein, country, discipline, person, chemical compound, theory, award, location, university, academic journal, event, chemical element, scientist, enzyme and O.\nSentence: Glenn was an honorary member of the International Academy of Astronautics and a member of the Society of Experimental Test Pilots , Marine Corps Aviation Association , Order of Daedalians , National Space Club board of trustees , National Space Society board of governors , International Association of Holiday Inns , Ohio Democratic Party , State Democratic Executive Committee , Franklin County ( Ohio ) Democratic Party and the 10th District ( Ohio ) Democratic Action Club .", "prompt_labels": "Glenn(B-scientist) was(O) an(O) honorary(O) member(O) of(O) the(O) International(B-organization) Academy(I-organization) of(I-organization) Astronautics(I-organization) and(O) a(O) member(O) of(O) the(O) Society(B-organization) of(I-organization) Experimental(I-organization) Test(I-organization) Pilots(I-organization) ,(O) Marine(B-organization) Corps(I-organization) Aviation(I-organization) Association(I-organization) ,(O) Order(B-organization) of(I-organization) Daedalians(I-organization) ,(O) National(B-organization) Space(I-organization) Club(I-organization) board(O) of(O) trustees(O) ,(O) National(B-organization) Space(I-organization) Society(I-organization) board(O) of(O) governors(O) ,(O) International(B-organization) Association(I-organization) of(I-organization) Holiday(I-organization) Inns(I-organization) ,(O) Ohio(B-organization) Democratic(I-organization) Party(I-organization) ,(O) State(B-organization) Democratic(I-organization) Executive(I-organization) Committee(I-organization) ,(O) Franklin(B-location) County(I-location) ((O) Ohio(B-location) )(O) Democratic(B-organization) Party(I-organization) and(O) the(O) 10th(B-location) District(I-location) ((O) Ohio(B-location) )(O) Democratic(B-organization) Action(I-organization) Club(I-organization) .(O)"}}
{"id": "104", "dataset": "crossner_science", "split": "dev", "label_list": ["country", "scientist", "theory", "award", "event", "enzyme", "university", "organization", "location", "academic journal", "person", "astronomical object", "discipline", "chemical compound", "chemical element", "protein"], "instance": {"id": "104", "words": ["The", "discovery", "of", "several", "other", "trans-Neptunian", "object", "s", ",", "such", "as", "50000", "Quaoar", "and", "90377", "Sedna", ",", "continued", "to", "erode", "arguments", "that", "Pluto", "was", "exceptional", "from", "the", "rest", "of", "the", "trans-Neptunian", "population", "."], "labels": ["O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, scientist, theory, award, event, enzyme, university, organization, location, academic journal, person, astronomical object, discipline, chemical compound, chemical element, protein and O.\nSentence: The discovery of several other trans-Neptunian object s , such as 50000 Quaoar and 90377 Sedna , continued to erode arguments that Pluto was exceptional from the rest of the trans-Neptunian population .", "prompt_labels": "The(O) discovery(O) of(O) several(O) other(O) trans-Neptunian(B-astronomical object) object(O) s(O) ,(O) such(O) as(O) 50000(B-astronomical object) Quaoar(I-astronomical object) and(O) 90377(B-astronomical object) Sedna(I-astronomical object) ,(O) continued(O) to(O) erode(O) arguments(O) that(O) Pluto(B-astronomical object) was(O) exceptional(O) from(O) the(O) rest(O) of(O) the(O) trans-Neptunian(B-astronomical object) population(O) .(O)"}}
{"id": "82", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "protein", "enzyme", "chemical compound", "university", "person", "country", "astronomical object", "discipline", "organization", "event", "location", "academic journal", "scientist", "award", "theory"], "instance": {"id": "82", "words": ["They", "are", "made", "up", "of", "monomers", "called", "nucleotide", "s", "which", "consist", "of", "an", "organic", "base", ":", "Adenine", ",", "Guanine", ",", "Cytosine", "and", "Tyrosine", "or", "Uracil", ",", "a", "pentose", "sugar", ",", "and", "a", "Phosphate", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O", "O", "B-chemical compound", "I-chemical compound", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, protein, enzyme, chemical compound, university, person, country, astronomical object, discipline, organization, event, location, academic journal, scientist, award, theory and O.\nSentence: They are made up of monomers called nucleotide s which consist of an organic base : Adenine , Guanine , Cytosine and Tyrosine or Uracil , a pentose sugar , and a Phosphate .", "prompt_labels": "They(O) are(O) made(O) up(O) of(O) monomers(O) called(O) nucleotide(B-chemical compound) s(O) which(O) consist(O) of(O) an(O) organic(O) base(O) :(O) Adenine(B-chemical compound) ,(O) Guanine(B-chemical compound) ,(O) Cytosine(B-chemical compound) and(O) Tyrosine(B-chemical compound) or(O) Uracil(B-chemical compound) ,(O) a(O) pentose(B-chemical compound) sugar(I-chemical compound) ,(O) and(O) a(O) Phosphate(O) .(O)"}}
{"id": "84", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "protein", "location", "person", "academic journal", "organization", "theory", "enzyme", "scientist", "chemical compound", "event", "discipline", "country", "award", "chemical element", "astronomical object"], "instance": {"id": "84", "words": ["In", "the", "first", "half", "of", "the", "20th", "century", ",", "advances", "in", "electronics", "enabled", "investigation", "of", "the", "electrical", "properties", "of", "nerve", "cells", ",", "culminating", "in", "work", "by", "Alan", "Hodgkin", ",", "Andrew", "Huxley", ",", "and", "others", "on", "the", "biophysics", "of", "the", "action", "potential", ",", "and", "the", "work", "of", "Bernard", "Katz", "and", "others", "on", "the", "electrochemistry", "of", "the", "synapse", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "B-discipline", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, protein, location, person, academic journal, organization, theory, enzyme, scientist, chemical compound, event, discipline, country, award, chemical element, astronomical object and O.\nSentence: In the first half of the 20th century , advances in electronics enabled investigation of the electrical properties of nerve cells , culminating in work by Alan Hodgkin , Andrew Huxley , and others on the biophysics of the action potential , and the work of Bernard Katz and others on the electrochemistry of the synapse .", "prompt_labels": "In(O) the(O) first(O) half(O) of(O) the(O) 20th(O) century(O) ,(O) advances(O) in(O) electronics(O) enabled(O) investigation(O) of(O) the(O) electrical(O) properties(O) of(O) nerve(O) cells(O) ,(O) culminating(O) in(O) work(O) by(O) Alan(B-scientist) Hodgkin(I-scientist) ,(O) Andrew(B-scientist) Huxley(I-scientist) ,(O) and(O) others(O) on(O) the(O) biophysics(O) of(O) the(O) action(O) potential(O) ,(O) and(O) the(O) work(O) of(O) Bernard(B-scientist) Katz(I-scientist) and(O) others(O) on(O) the(O) electrochemistry(B-discipline) of(O) the(O) synapse(O) .(O)"}}
{"id": "205", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "organization", "enzyme", "university", "person", "scientist", "chemical element", "location", "event", "discipline", "academic journal", "astronomical object", "award", "country", "chemical compound", "theory"], "instance": {"id": "205", "words": ["Among", "exoplanets", ",", "a", "review", "in", "2015", "came", "to", "the", "conclusion", "that", "Kepler-62f", ",", "Kepler-186f", "and", "Kepler-442b", "were", "likely", "the", "best", "candidates", "for", "being", "potentially", "habitable", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, organization, enzyme, university, person, scientist, chemical element, location, event, discipline, academic journal, astronomical object, award, country, chemical compound, theory and O.\nSentence: Among exoplanets , a review in 2015 came to the conclusion that Kepler-62f , Kepler-186f and Kepler-442b were likely the best candidates for being potentially habitable .", "prompt_labels": "Among(O) exoplanets(O) ,(O) a(O) review(O) in(O) 2015(O) came(O) to(O) the(O) conclusion(O) that(O) Kepler-62f(B-astronomical object) ,(O) Kepler-186f(B-astronomical object) and(O) Kepler-442b(B-astronomical object) were(O) likely(O) the(O) best(O) candidates(O) for(O) being(O) potentially(O) habitable(O) .(O)"}}
{"id": "202", "dataset": "crossner_science", "split": "dev", "label_list": ["person", "discipline", "event", "protein", "country", "organization", "academic journal", "scientist", "location", "university", "award", "chemical element", "chemical compound", "astronomical object", "theory", "enzyme"], "instance": {"id": "202", "words": ["For", "his", "study", "in", "irreversible", "thermodynamics", ",", "he", "received", "the", "Rumford", "Medal", "in", "1976", ",", "and", "in", "1977", ",", "the", "Nobel", "Prize", "in", "Nobel", "Prize", "in", "Chemistry", "."], "labels": ["O", "O", "O", "O", "B-discipline", "B-discipline", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, discipline, event, protein, country, organization, academic journal, scientist, location, university, award, chemical element, chemical compound, astronomical object, theory, enzyme and O.\nSentence: For his study in irreversible thermodynamics , he received the Rumford Medal in 1976 , and in 1977 , the Nobel Prize in Nobel Prize in Chemistry .", "prompt_labels": "For(O) his(O) study(O) in(O) irreversible(B-discipline) thermodynamics(B-discipline) ,(O) he(O) received(O) the(O) Rumford(B-award) Medal(I-award) in(O) 1976(O) ,(O) and(O) in(O) 1977(O) ,(O) the(O) Nobel(B-award) Prize(I-award) in(O) Nobel(B-award) Prize(I-award) in(I-award) Chemistry(I-award) .(O)"}}
{"id": "339", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "university", "event", "astronomical object", "enzyme", "theory", "academic journal", "country", "location", "protein", "organization", "person", "award", "chemical element", "chemical compound", "scientist"], "instance": {"id": "339", "words": ["The", "group", "included", "the", "physicists", "Walther", "Bothe", ",", "Robert", "Döpel", ",", "Hans", "Geiger", ",", "Wolfgang", "Gentner", "(", "probably", "sent", "by", "Walther", "Bothe", ")", ",", "Wilhelm", "Hanle", ",", "Gerhard", "Hoffmann", ",", "and", "Georg", "Joos", ";", "Peter", "Debye", "was", "invited", ",", "but", "he", "did", "not", "attend", "."], "labels": ["O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, university, event, astronomical object, enzyme, theory, academic journal, country, location, protein, organization, person, award, chemical element, chemical compound, scientist and O.\nSentence: The group included the physicists Walther Bothe , Robert Döpel , Hans Geiger , Wolfgang Gentner ( probably sent by Walther Bothe ) , Wilhelm Hanle , Gerhard Hoffmann , and Georg Joos ; Peter Debye was invited , but he did not attend .", "prompt_labels": "The(O) group(O) included(O) the(O) physicists(O) Walther(B-scientist) Bothe(I-scientist) ,(O) Robert(B-scientist) Döpel(I-scientist) ,(O) Hans(B-scientist) Geiger(I-scientist) ,(O) Wolfgang(B-scientist) Gentner(I-scientist) ((O) probably(O) sent(O) by(O) Walther(B-scientist) Bothe(I-scientist) )(O) ,(O) Wilhelm(B-scientist) Hanle(I-scientist) ,(O) Gerhard(B-scientist) Hoffmann(I-scientist) ,(O) and(O) Georg(B-scientist) Joos(I-scientist) ;(O) Peter(B-scientist) Debye(I-scientist) was(O) invited(O) ,(O) but(O) he(O) did(O) not(O) attend(O) .(O)"}}
{"id": "25", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "university", "protein", "scientist", "chemical element", "theory", "astronomical object", "academic journal", "award", "enzyme", "person", "chemical compound", "country", "organization", "event", "location"], "instance": {"id": "25", "words": ["Such", "mass", "wasting", "occurs", "on", "both", "terrestrial", "and", "submarine", "slopes", ",", "and", "has", "been", "observed", "on", "Earth", ",", "Mars", ",", "Venus", ",", "Titan", "and", "Iapetus", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, university, protein, scientist, chemical element, theory, astronomical object, academic journal, award, enzyme, person, chemical compound, country, organization, event, location and O.\nSentence: Such mass wasting occurs on both terrestrial and submarine slopes , and has been observed on Earth , Mars , Venus , Titan and Iapetus .", "prompt_labels": "Such(O) mass(O) wasting(O) occurs(O) on(O) both(O) terrestrial(O) and(O) submarine(O) slopes(O) ,(O) and(O) has(O) been(O) observed(O) on(O) Earth(B-astronomical object) ,(O) Mars(B-astronomical object) ,(O) Venus(B-astronomical object) ,(O) Titan(B-astronomical object) and(O) Iapetus(B-astronomical object) .(O)"}}
{"id": "91", "dataset": "crossner_science", "split": "dev", "label_list": ["scientist", "discipline", "organization", "enzyme", "chemical element", "theory", "location", "chemical compound", "country", "person", "event", "academic journal", "award", "protein", "university", "astronomical object"], "instance": {"id": "91", "words": ["Various", "acids", "were", "used", "in", "the", "past", "such", "as", "Phosphoric", "acid", "as", "used", "in", "Calcium", "Lime", "Rust", "Remover", "(", "CLR", ")", "and", "Hydrofluoric", "acid", "as", "used", "in", "the", "Australian", "product", "made", "in", "Queensland", "called", "Rustiban", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, discipline, organization, enzyme, chemical element, theory, location, chemical compound, country, person, event, academic journal, award, protein, university, astronomical object and O.\nSentence: Various acids were used in the past such as Phosphoric acid as used in Calcium Lime Rust Remover ( CLR ) and Hydrofluoric acid as used in the Australian product made in Queensland called Rustiban .", "prompt_labels": "Various(O) acids(O) were(O) used(O) in(O) the(O) past(O) such(O) as(O) Phosphoric(B-chemical compound) acid(I-chemical compound) as(O) used(O) in(O) Calcium(O) Lime(O) Rust(O) Remover(O) ((O) CLR(O) )(O) and(O) Hydrofluoric(B-chemical compound) acid(I-chemical compound) as(O) used(O) in(O) the(O) Australian(O) product(O) made(O) in(O) Queensland(B-location) called(O) Rustiban(B-location) .(O)"}}
{"id": "107", "dataset": "crossner_science", "split": "dev", "label_list": ["organization", "chemical compound", "country", "location", "scientist", "university", "academic journal", "discipline", "astronomical object", "chemical element", "award", "theory", "event", "person", "protein", "enzyme"], "instance": {"id": "107", "words": ["The", "Haldane", "Lecture", "at", "the", "John", "Innes", "Centre", ",", "of", "The", "Genetics", "Society", "is", "also", "named", "in", "his", "honour", "."], "labels": ["O", "B-award", "I-award", "O", "O", "B-location", "I-location", "I-location", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, chemical compound, country, location, scientist, university, academic journal, discipline, astronomical object, chemical element, award, theory, event, person, protein, enzyme and O.\nSentence: The Haldane Lecture at the John Innes Centre , of The Genetics Society is also named in his honour .", "prompt_labels": "The(O) Haldane(B-award) Lecture(I-award) at(O) the(O) John(B-location) Innes(I-location) Centre(I-location) ,(O) of(O) The(B-organization) Genetics(I-organization) Society(I-organization) is(O) also(O) named(O) in(O) his(O) honour(O) .(O)"}}
{"id": "402", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "astronomical object", "enzyme", "discipline", "location", "academic journal", "chemical compound", "country", "award", "university", "event", "protein", "person", "scientist", "organization", "theory"], "instance": {"id": "402", "words": ["He", "was", "sent", "to", "Sheppard", "Air", "Force", "Base", "in", "Wichita", "Falls", ",", "Texas", ",", "for", "five", "weeks", "of", "basic", "flight", "training", ",", "and", "was", "later", "stationed", "at", "Brooks", "Air", "Force", "Base", "in", "San", "Antonio", ",", "Texas", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-location", "I-location", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-location", "I-location", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, astronomical object, enzyme, discipline, location, academic journal, chemical compound, country, award, university, event, protein, person, scientist, organization, theory and O.\nSentence: He was sent to Sheppard Air Force Base in Wichita Falls , Texas , for five weeks of basic flight training , and was later stationed at Brooks Air Force Base in San Antonio , Texas .", "prompt_labels": "He(O) was(O) sent(O) to(O) Sheppard(B-organization) Air(I-organization) Force(I-organization) Base(I-organization) in(O) Wichita(B-location) Falls(I-location) ,(O) Texas(B-location) ,(O) for(O) five(O) weeks(O) of(O) basic(O) flight(O) training(O) ,(O) and(O) was(O) later(O) stationed(O) at(O) Brooks(B-organization) Air(I-organization) Force(I-organization) Base(I-organization) in(O) San(B-location) Antonio(I-location) ,(O) Texas(B-location) .(O)"}}
{"id": "347", "dataset": "crossner_science", "split": "dev", "label_list": ["scientist", "location", "event", "theory", "organization", "astronomical object", "academic journal", "chemical compound", "university", "country", "discipline", "enzyme", "person", "award", "protein", "chemical element"], "instance": {"id": "347", "words": ["Other", "members", "of", "the", "Nuclear", "Physics", "Working", "Group", "in", "both", "1956", "and", "1957", "were", ":", "Werner", "Heisenberg", "(", "chairman", ")", ",", "Hans", "Kopfermann", "(", "vice-chairman", ")", ",", "Fritz", "Bopp", ",", "Walther", "Bothe", ",", "Wolfgang", "Gentner", ",", "Otto", "Haxel", ",", "Willibald", "Jentschke", ",", "Heinz", "Maier-Leibnitz", ",", "Josef", "Mattauch", ",", "Wolfgang", "Riezler", ",", "Wilhelm", "Walcher", ",", "and", "Carl", "Friedrich", "von", "Weizsäcker", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, location, event, theory, organization, astronomical object, academic journal, chemical compound, university, country, discipline, enzyme, person, award, protein, chemical element and O.\nSentence: Other members of the Nuclear Physics Working Group in both 1956 and 1957 were : Werner Heisenberg ( chairman ) , Hans Kopfermann ( vice-chairman ) , Fritz Bopp , Walther Bothe , Wolfgang Gentner , Otto Haxel , Willibald Jentschke , Heinz Maier-Leibnitz , Josef Mattauch , Wolfgang Riezler , Wilhelm Walcher , and Carl Friedrich von Weizsäcker .", "prompt_labels": "Other(O) members(O) of(O) the(O) Nuclear(B-organization) Physics(I-organization) Working(I-organization) Group(I-organization) in(O) both(O) 1956(O) and(O) 1957(O) were(O) :(O) Werner(B-scientist) Heisenberg(I-scientist) ((O) chairman(O) )(O) ,(O) Hans(B-scientist) Kopfermann(I-scientist) ((O) vice-chairman(O) )(O) ,(O) Fritz(B-scientist) Bopp(I-scientist) ,(O) Walther(B-scientist) Bothe(I-scientist) ,(O) Wolfgang(B-scientist) Gentner(I-scientist) ,(O) Otto(B-scientist) Haxel(I-scientist) ,(O) Willibald(B-scientist) Jentschke(I-scientist) ,(O) Heinz(B-scientist) Maier-Leibnitz(I-scientist) ,(O) Josef(B-scientist) Mattauch(I-scientist) ,(O) Wolfgang(B-scientist) Riezler(I-scientist) ,(O) Wilhelm(B-scientist) Walcher(I-scientist) ,(O) and(O) Carl(B-scientist) Friedrich(I-scientist) von(I-scientist) Weizsäcker(I-scientist) .(O)"}}
{"id": "253", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "enzyme", "academic journal", "person", "protein", "chemical compound", "location", "organization", "event", "award", "university", "discipline", "chemical element", "astronomical object", "scientist", "country"], "instance": {"id": "253", "words": ["In", "2005", ",", "he", "won", "Grammy", "Award", "for", "Best", "Pop", "Instrumental", "Performance", "for", "Caravan", "and", "Grammy", "Award", "for", "Best", "Rock", "Instrumental", "Performance", "for", "69", "Freedom", "Special", "."], "labels": ["O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, enzyme, academic journal, person, protein, chemical compound, location, organization, event, award, university, discipline, chemical element, astronomical object, scientist, country and O.\nSentence: In 2005 , he won Grammy Award for Best Pop Instrumental Performance for Caravan and Grammy Award for Best Rock Instrumental Performance for 69 Freedom Special .", "prompt_labels": "In(O) 2005(O) ,(O) he(O) won(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Pop(I-award) Instrumental(I-award) Performance(I-award) for(O) Caravan(O) and(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Rock(I-award) Instrumental(I-award) Performance(I-award) for(O) 69(O) Freedom(O) Special(O) .(O)"}}
{"id": "208", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "theory", "location", "chemical element", "discipline", "event", "award", "country", "scientist", "chemical compound", "academic journal", "protein", "enzyme", "organization", "person", "university"], "instance": {"id": "208", "words": ["In", "1974", "Beatrice", "Mintz", "and", "Rudolf", "Jaenisch", "created", "the", "first", "genetically", "modified", "animal", "by", "inserting", "a", "DNA", "virus", "into", "an", "early-stage", "mouse", "embryo", "and", "showing", "that", "the", "inserted", "genes", "were", "present", "in", "every", "cell", "."], "labels": ["O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, theory, location, chemical element, discipline, event, award, country, scientist, chemical compound, academic journal, protein, enzyme, organization, person, university and O.\nSentence: In 1974 Beatrice Mintz and Rudolf Jaenisch created the first genetically modified animal by inserting a DNA virus into an early-stage mouse embryo and showing that the inserted genes were present in every cell .", "prompt_labels": "In(O) 1974(O) Beatrice(B-scientist) Mintz(I-scientist) and(O) Rudolf(B-scientist) Jaenisch(I-scientist) created(O) the(O) first(O) genetically(O) modified(O) animal(O) by(O) inserting(O) a(O) DNA(O) virus(O) into(O) an(O) early-stage(O) mouse(O) embryo(O) and(O) showing(O) that(O) the(O) inserted(O) genes(O) were(O) present(O) in(O) every(O) cell(O) .(O)"}}
{"id": "323", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "location", "theory", "event", "chemical compound", "award", "protein", "university", "scientist", "discipline", "chemical element", "astronomical object", "academic journal", "organization", "country", "person"], "instance": {"id": "323", "words": ["9", "km", ")", ",", "985", "Rosina", "(", "8.18", "km", ")", "1310", "Villigera", "(", "15.24", "km", ")", ",", "and", "1468", "Zomba", "(", "7", "km", ")", ";", "but", "still", "smaller", "than", "the", "largest", "members", "of", "this", "group", ",", "namely", ",", "132", "Aethra", ",", "323", "Brucia", ",", "1508", "Kemi", ",", "2204", "Lyyli", "and", "512", "Taurinensis", ",", "which", "are", "larger", "than", "20", "kilometers", "in", "diameter", "(", "in", "one", "or", "other", "given", "source", ")", "."], "labels": ["O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, location, theory, event, chemical compound, award, protein, university, scientist, discipline, chemical element, astronomical object, academic journal, organization, country, person and O.\nSentence: 9 km ) , 985 Rosina ( 8.18 km ) 1310 Villigera ( 15.24 km ) , and 1468 Zomba ( 7 km ) ; but still smaller than the largest members of this group , namely , 132 Aethra , 323 Brucia , 1508 Kemi , 2204 Lyyli and 512 Taurinensis , which are larger than 20 kilometers in diameter ( in one or other given source ) .", "prompt_labels": "9(O) km(O) )(O) ,(O) 985(B-astronomical object) Rosina(I-astronomical object) ((O) 8.18(O) km(O) )(O) 1310(B-astronomical object) Villigera(I-astronomical object) ((O) 15.24(O) km(O) )(O) ,(O) and(O) 1468(B-astronomical object) Zomba(I-astronomical object) ((O) 7(O) km(O) )(O) ;(O) but(O) still(O) smaller(O) than(O) the(O) largest(O) members(O) of(O) this(O) group(O) ,(O) namely(O) ,(O) 132(B-astronomical object) Aethra(I-astronomical object) ,(O) 323(B-astronomical object) Brucia(I-astronomical object) ,(O) 1508(B-astronomical object) Kemi(I-astronomical object) ,(O) 2204(B-astronomical object) Lyyli(I-astronomical object) and(O) 512(B-astronomical object) Taurinensis(I-astronomical object) ,(O) which(O) are(O) larger(O) than(O) 20(O) kilometers(O) in(O) diameter(O) ((O) in(O) one(O) or(O) other(O) given(O) source(O) )(O) .(O)"}}
{"id": "20", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "award", "organization", "country", "scientist", "theory", "university", "astronomical object", "event", "chemical compound", "person", "protein", "chemical element", "discipline", "location", "academic journal"], "instance": {"id": "20", "words": ["Generally", ",", "Potassium", "cyanide", "or", "its", "less", "toxic", "surrogate", "Zinc", "cyanide", "are", "used", "as", "nucleophilic", "cyanide", "sources", "."], "labels": ["O", "O", "B-chemical compound", "I-chemical compound", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, award, organization, country, scientist, theory, university, astronomical object, event, chemical compound, person, protein, chemical element, discipline, location, academic journal and O.\nSentence: Generally , Potassium cyanide or its less toxic surrogate Zinc cyanide are used as nucleophilic cyanide sources .", "prompt_labels": "Generally(O) ,(O) Potassium(B-chemical compound) cyanide(I-chemical compound) or(O) its(O) less(O) toxic(O) surrogate(O) Zinc(B-chemical compound) cyanide(I-chemical compound) are(O) used(O) as(O) nucleophilic(B-chemical compound) cyanide(I-chemical compound) sources(O) .(O)"}}
{"id": "222", "dataset": "crossner_science", "split": "dev", "label_list": ["scientist", "person", "academic journal", "enzyme", "country", "award", "event", "university", "chemical compound", "discipline", "organization", "location", "astronomical object", "protein", "chemical element", "theory"], "instance": {"id": "222", "words": ["Directors", "(", "superintendents", ")", "of", "the", "observatory", "included", "Stephen", "Demainbray", ",", "Francis", "Ronalds", ",", "John", "Welsh", ",", "Balfour", "Stewart", ",", "Francis", "John", "Welsh", "Whipple", ",", "Charles", "Chree", ",", "and", "George", "Clarke", "Simpson", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, person, academic journal, enzyme, country, award, event, university, chemical compound, discipline, organization, location, astronomical object, protein, chemical element, theory and O.\nSentence: Directors ( superintendents ) of the observatory included Stephen Demainbray , Francis Ronalds , John Welsh , Balfour Stewart , Francis John Welsh Whipple , Charles Chree , and George Clarke Simpson .", "prompt_labels": "Directors(O) ((O) superintendents(O) )(O) of(O) the(O) observatory(O) included(O) Stephen(B-scientist) Demainbray(I-scientist) ,(O) Francis(B-scientist) Ronalds(I-scientist) ,(O) John(B-scientist) Welsh(I-scientist) ,(O) Balfour(B-scientist) Stewart(I-scientist) ,(O) Francis(B-scientist) John(I-scientist) Welsh(I-scientist) Whipple(I-scientist) ,(O) Charles(B-scientist) Chree(I-scientist) ,(O) and(O) George(B-scientist) Clarke(I-scientist) Simpson(I-scientist) .(O)"}}
{"id": "79", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "country", "award", "chemical element", "organization", "location", "astronomical object", "discipline", "enzyme", "person", "scientist", "theory", "event", "chemical compound", "university", "academic journal"], "instance": {"id": "79", "words": ["He", "served", "as", "President", "of", "the", "Ecological", "Society", "of", "America", "in", "1917", ",", "the", "Association", "of", "American", "Geographers", "in", "1923", "and", "President", "of", "the", "Board", "of", "Directors", "of", "the", "Society", "for", "Biodemography", "and", "Social", "Biology", "from", "1934", "to", "1938", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, country, award, chemical element, organization, location, astronomical object, discipline, enzyme, person, scientist, theory, event, chemical compound, university, academic journal and O.\nSentence: He served as President of the Ecological Society of America in 1917 , the Association of American Geographers in 1923 and President of the Board of Directors of the Society for Biodemography and Social Biology from 1934 to 1938 .", "prompt_labels": "He(O) served(O) as(O) President(O) of(O) the(O) Ecological(B-organization) Society(I-organization) of(I-organization) America(I-organization) in(O) 1917(O) ,(O) the(O) Association(B-organization) of(I-organization) American(I-organization) Geographers(I-organization) in(O) 1923(O) and(O) President(O) of(O) the(O) Board(O) of(O) Directors(O) of(O) the(O) Society(B-organization) for(I-organization) Biodemography(I-organization) and(I-organization) Social(I-organization) Biology(I-organization) from(O) 1934(O) to(O) 1938(O) .(O)"}}
{"id": "255", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "theory", "organization", "university", "astronomical object", "discipline", "protein", "award", "academic journal", "scientist", "location", "event", "chemical compound", "country", "person", "enzyme"], "instance": {"id": "255", "words": ["Under", "the", "IAU", "definition", "approved", "on", "August", "24", ",", "2006", ",", "Eris", "is", "a", "dwarf", "planet", ",", "along", "with", "objects", "such", "as", "Pluto", ",", "Ceres", ",", "Haumea", "and", "Makemake", ","], "labels": ["O", "O", "B-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, theory, organization, university, astronomical object, discipline, protein, award, academic journal, scientist, location, event, chemical compound, country, person, enzyme and O.\nSentence: Under the IAU definition approved on August 24 , 2006 , Eris is a dwarf planet , along with objects such as Pluto , Ceres , Haumea and Makemake ,", "prompt_labels": "Under(O) the(O) IAU(B-organization) definition(O) approved(O) on(O) August(O) 24(O) ,(O) 2006(O) ,(O) Eris(B-astronomical object) is(O) a(O) dwarf(O) planet(O) ,(O) along(O) with(O) objects(O) such(O) as(O) Pluto(B-astronomical object) ,(O) Ceres(B-astronomical object) ,(O) Haumea(B-astronomical object) and(O) Makemake(B-astronomical object) ,(O)"}}
{"id": "448", "dataset": "crossner_science", "split": "dev", "label_list": ["award", "organization", "theory", "protein", "astronomical object", "person", "chemical element", "discipline", "chemical compound", "enzyme", "country", "location", "academic journal", "event", "university", "scientist"], "instance": {"id": "448", "words": ["She", "is", "a", "member", "of", "the", "Australian", "Antarctic", "Research", "Advisory", "Committee", "and", "serves", "on", "the", "Editorial", "Boards", "of", "the", "international", "journals", ":", "Ecosystems", ",", "Global", "Change", "Biology", ",", "Oecologia", ",", "Plant", ",", "Cell", ";", "Environment", ",", "and", "Tree", "Physiology", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "O", "B-academic journal", "O", "B-academic journal", "O", "B-academic journal", "O", "O", "B-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, organization, theory, protein, astronomical object, person, chemical element, discipline, chemical compound, enzyme, country, location, academic journal, event, university, scientist and O.\nSentence: She is a member of the Australian Antarctic Research Advisory Committee and serves on the Editorial Boards of the international journals : Ecosystems , Global Change Biology , Oecologia , Plant , Cell ; Environment , and Tree Physiology .", "prompt_labels": "She(O) is(O) a(O) member(O) of(O) the(O) Australian(B-organization) Antarctic(I-organization) Research(I-organization) Advisory(I-organization) Committee(I-organization) and(O) serves(O) on(O) the(O) Editorial(O) Boards(O) of(O) the(O) international(O) journals(O) :(O) Ecosystems(B-academic journal) ,(O) Global(B-academic journal) Change(I-academic journal) Biology(I-academic journal) ,(O) Oecologia(B-academic journal) ,(O) Plant(B-academic journal) ,(O) Cell(B-academic journal) ;(O) Environment(B-academic journal) ,(O) and(O) Tree(B-academic journal) Physiology(I-academic journal) .(O)"}}
{"id": "364", "dataset": "crossner_science", "split": "dev", "label_list": ["organization", "enzyme", "person", "event", "university", "chemical compound", "chemical element", "academic journal", "theory", "discipline", "award", "country", "protein", "scientist", "astronomical object", "location"], "instance": {"id": "364", "words": ["Max", "Volmer", "and", "Robert", "Döpel", "were", "assigned", "to", "this", "facility", "."], "labels": ["B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, enzyme, person, event, university, chemical compound, chemical element, academic journal, theory, discipline, award, country, protein, scientist, astronomical object, location and O.\nSentence: Max Volmer and Robert Döpel were assigned to this facility .", "prompt_labels": "Max(B-scientist) Volmer(I-scientist) and(O) Robert(B-scientist) Döpel(I-scientist) were(O) assigned(O) to(O) this(O) facility(O) .(O)"}}
{"id": "135", "dataset": "crossner_science", "split": "dev", "label_list": ["award", "protein", "event", "enzyme", "discipline", "scientist", "astronomical object", "university", "chemical element", "country", "organization", "academic journal", "person", "chemical compound", "location", "theory"], "instance": {"id": "135", "words": ["In", "Oct", "2002", "and", "after", ",", "Science", ",", "Physical", "Review", ",", "and", "Applied", "Physics", "Letters", "withdrew", "more", "than", "a", "dozen", "papers", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-academic journal", "O", "B-academic journal", "I-academic journal", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, protein, event, enzyme, discipline, scientist, astronomical object, university, chemical element, country, organization, academic journal, person, chemical compound, location, theory and O.\nSentence: In Oct 2002 and after , Science , Physical Review , and Applied Physics Letters withdrew more than a dozen papers .", "prompt_labels": "In(O) Oct(O) 2002(O) and(O) after(O) ,(O) Science(B-academic journal) ,(O) Physical(B-academic journal) Review(I-academic journal) ,(O) and(O) Applied(B-academic journal) Physics(I-academic journal) Letters(I-academic journal) withdrew(O) more(O) than(O) a(O) dozen(O) papers(O) .(O)"}}
{"id": "185", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "location", "discipline", "scientist", "enzyme", "astronomical object", "organization", "university", "theory", "event", "chemical compound", "person", "protein", "award", "country", "academic journal"], "instance": {"id": "185", "words": ["Cooper", "studied", "with", "Oliver", "Coleman", "and", "Walter", "Dyett", "in", "the", "late", "1950s", "and", "early", "1960s", ",", "then", "studied", "at", "the", "American", "Conservatory", "and", "Loop", "College", "."], "labels": ["O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, location, discipline, scientist, enzyme, astronomical object, organization, university, theory, event, chemical compound, person, protein, award, country, academic journal and O.\nSentence: Cooper studied with Oliver Coleman and Walter Dyett in the late 1950s and early 1960s , then studied at the American Conservatory and Loop College .", "prompt_labels": "Cooper(O) studied(O) with(O) Oliver(B-person) Coleman(I-person) and(O) Walter(B-person) Dyett(I-person) in(O) the(O) late(O) 1950s(O) and(O) early(O) 1960s(O) ,(O) then(O) studied(O) at(O) the(O) American(B-organization) Conservatory(I-organization) and(O) Loop(B-university) College(I-university) .(O)"}}
{"id": "433", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "event", "astronomical object", "theory", "person", "organization", "enzyme", "chemical compound", "academic journal", "protein", "country", "scientist", "university", "location", "discipline", "award"], "instance": {"id": "433", "words": ["Other", "astronomical", "bodies", "are", "also", "known", "to", "have", "polar", "vortices", ",", "including", "Venus", "(", "double", "vortex", "-", "that", "is", ",", "two", "polar", "vortices", "at", "a", "pole", ")", ",", "Mars", ",", "Jupiter", ",", "Saturn", ",", "and", "Saturn", "'s", "moon", "Titan", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, event, astronomical object, theory, person, organization, enzyme, chemical compound, academic journal, protein, country, scientist, university, location, discipline, award and O.\nSentence: Other astronomical bodies are also known to have polar vortices , including Venus ( double vortex - that is , two polar vortices at a pole ) , Mars , Jupiter , Saturn , and Saturn 's moon Titan .", "prompt_labels": "Other(O) astronomical(O) bodies(O) are(O) also(O) known(O) to(O) have(O) polar(O) vortices(O) ,(O) including(O) Venus(B-astronomical object) ((O) double(O) vortex(O) -(O) that(O) is(O) ,(O) two(O) polar(O) vortices(O) at(O) a(O) pole(O) )(O) ,(O) Mars(B-astronomical object) ,(O) Jupiter(B-astronomical object) ,(O) Saturn(B-astronomical object) ,(O) and(O) Saturn(B-astronomical object) 's(O) moon(O) Titan(B-astronomical object) .(O)"}}
{"id": "373", "dataset": "crossner_science", "split": "dev", "label_list": ["country", "event", "location", "discipline", "organization", "academic journal", "chemical compound", "protein", "scientist", "award", "university", "astronomical object", "person", "enzyme", "theory", "chemical element"], "instance": {"id": "373", "words": ["CDC", "assays", "are", "used", "to", "find", "a", "suitable", "donor", "for", "organ", "or", "bone", "marrow", "transplantation", ",", "namely", "donor", "with", "matching", "phenotype", "of", "Major", "histocompatibility", "complex", "Human", "leukocyte", "antigen", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "I-protein", "I-protein", "I-protein", "I-protein", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, location, discipline, organization, academic journal, chemical compound, protein, scientist, award, university, astronomical object, person, enzyme, theory, chemical element and O.\nSentence: CDC assays are used to find a suitable donor for organ or bone marrow transplantation , namely donor with matching phenotype of Major histocompatibility complex Human leukocyte antigen .", "prompt_labels": "CDC(O) assays(O) are(O) used(O) to(O) find(O) a(O) suitable(O) donor(O) for(O) organ(O) or(O) bone(O) marrow(O) transplantation(O) ,(O) namely(O) donor(O) with(O) matching(O) phenotype(O) of(O) Major(B-protein) histocompatibility(I-protein) complex(I-protein) Human(I-protein) leukocyte(I-protein) antigen(I-protein) .(O)"}}
{"id": "108", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "country", "theory", "organization", "university", "scientist", "enzyme", "astronomical object", "chemical compound", "event", "chemical element", "protein", "person", "discipline", "academic journal", "award"], "instance": {"id": "108", "words": ["The", "pyrimidines", ",", "thymine", ",", "cytosine", "and", "uracil", ",", "form", "the", "complementary", "bases", "to", "the", "purine", "bases", "in", "DNA", "and", "RNA", ",", "and", "are", "also", "components", "of", "Cytidine", "triphosphate", ",", "Uridine", "monophosphate", ",", "Uridine", "diphosphate", "and", "Uridine", "triphosphate", "."], "labels": ["O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, theory, organization, university, scientist, enzyme, astronomical object, chemical compound, event, chemical element, protein, person, discipline, academic journal, award and O.\nSentence: The pyrimidines , thymine , cytosine and uracil , form the complementary bases to the purine bases in DNA and RNA , and are also components of Cytidine triphosphate , Uridine monophosphate , Uridine diphosphate and Uridine triphosphate .", "prompt_labels": "The(O) pyrimidines(B-chemical compound) ,(O) thymine(B-chemical compound) ,(O) cytosine(B-chemical compound) and(O) uracil(B-chemical compound) ,(O) form(O) the(O) complementary(O) bases(O) to(O) the(O) purine(B-chemical compound) bases(O) in(O) DNA(O) and(O) RNA(O) ,(O) and(O) are(O) also(O) components(O) of(O) Cytidine(B-chemical compound) triphosphate(I-chemical compound) ,(O) Uridine(B-chemical compound) monophosphate(I-chemical compound) ,(O) Uridine(B-chemical compound) diphosphate(I-chemical compound) and(O) Uridine(B-chemical compound) triphosphate(I-chemical compound) .(O)"}}
{"id": "112", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "scientist", "chemical element", "event", "academic journal", "person", "protein", "country", "chemical compound", "organization", "theory", "astronomical object", "award", "enzyme", "location", "university"], "instance": {"id": "112", "words": ["It", "was", "discovered", "on", "24", "September", "1960", ",", "by", "astronomers", "Cornelis", "Johannes", "van", "Houten", ",", "Ingrid", "van", "Houten-Groeneveld", "and", "Tom", "Gehrels", "at", "Palomar", "Observatory", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, scientist, chemical element, event, academic journal, person, protein, country, chemical compound, organization, theory, astronomical object, award, enzyme, location, university and O.\nSentence: It was discovered on 24 September 1960 , by astronomers Cornelis Johannes van Houten , Ingrid van Houten-Groeneveld and Tom Gehrels at Palomar Observatory .", "prompt_labels": "It(O) was(O) discovered(O) on(O) 24(O) September(O) 1960(O) ,(O) by(O) astronomers(O) Cornelis(B-scientist) Johannes(I-scientist) van(I-scientist) Houten(I-scientist) ,(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Tom(B-scientist) Gehrels(I-scientist) at(O) Palomar(B-location) Observatory(I-location) .(O)"}}
{"id": "249", "dataset": "crossner_science", "split": "dev", "label_list": ["award", "astronomical object", "scientist", "theory", "enzyme", "person", "organization", "event", "university", "chemical element", "discipline", "protein", "location", "country", "academic journal", "chemical compound"], "instance": {"id": "249", "words": ["Observation", "of", "perception", "related", "neurons", "in", "prefrontal", "cortex", "is", "consistent", "with", "the", "theory", "of", "Christof", "Koch", "and", "Francis", "Crick", "who", "postulated", "that", "neural", "correlate", "of", "consciousness", "resides", "in", "prefrontal", "cortex", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, astronomical object, scientist, theory, enzyme, person, organization, event, university, chemical element, discipline, protein, location, country, academic journal, chemical compound and O.\nSentence: Observation of perception related neurons in prefrontal cortex is consistent with the theory of Christof Koch and Francis Crick who postulated that neural correlate of consciousness resides in prefrontal cortex .", "prompt_labels": "Observation(O) of(O) perception(O) related(O) neurons(O) in(O) prefrontal(O) cortex(O) is(O) consistent(O) with(O) the(O) theory(O) of(O) Christof(B-scientist) Koch(I-scientist) and(O) Francis(B-scientist) Crick(I-scientist) who(O) postulated(O) that(O) neural(O) correlate(O) of(O) consciousness(O) resides(O) in(O) prefrontal(O) cortex(O) .(O)"}}
{"id": "269", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "award", "organization", "scientist", "person", "astronomical object", "university", "location", "country", "event", "chemical compound", "theory", "discipline", "enzyme", "academic journal", "chemical element"], "instance": {"id": "269", "words": ["A", "proton", "gradient", "(", "unrelated", "to", "the", "natural", "temperature", "and", "energy", "gradients", "aforementioned", ")", "drives", "a", "remarkable", ",", "turbine", "like", "protein", ",", "ATP", "synthase", "to", "rotate", ",", "capturing", "energy", "in", "usable", "chemical", "form", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, award, organization, scientist, person, astronomical object, university, location, country, event, chemical compound, theory, discipline, enzyme, academic journal, chemical element and O.\nSentence: A proton gradient ( unrelated to the natural temperature and energy gradients aforementioned ) drives a remarkable , turbine like protein , ATP synthase to rotate , capturing energy in usable chemical form .", "prompt_labels": "A(O) proton(O) gradient(O) ((O) unrelated(O) to(O) the(O) natural(O) temperature(O) and(O) energy(O) gradients(O) aforementioned(O) )(O) drives(O) a(O) remarkable(O) ,(O) turbine(O) like(O) protein(O) ,(O) ATP(B-enzyme) synthase(I-enzyme) to(O) rotate(O) ,(O) capturing(O) energy(O) in(O) usable(O) chemical(O) form(O) .(O)"}}
{"id": "85", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "location", "award", "theory", "scientist", "chemical compound", "event", "discipline", "country", "astronomical object", "chemical element", "person", "university", "academic journal", "enzyme", "organization"], "instance": {"id": "85", "words": ["Jupiter", "rarely", "occults", "Saturn", "."], "labels": ["B-astronomical object", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, location, award, theory, scientist, chemical compound, event, discipline, country, astronomical object, chemical element, person, university, academic journal, enzyme, organization and O.\nSentence: Jupiter rarely occults Saturn .", "prompt_labels": "Jupiter(B-astronomical object) rarely(O) occults(O) Saturn(B-astronomical object) .(O)"}}
{"id": "350", "dataset": "crossner_science", "split": "dev", "label_list": ["country", "university", "chemical compound", "protein", "enzyme", "theory", "organization", "discipline", "event", "chemical element", "person", "astronomical object", "academic journal", "award", "location", "scientist"], "instance": {"id": "350", "words": ["A", "second", "meeting", "was", "held", "soon", "thereafter", "and", "included", "Klaus", "Clusius", ",", "Robert", "Döpel", ",", "Werner", "Heisenberg", ",", "and", "Carl", "Friedrich", "von", "Weizsäcker", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, chemical compound, protein, enzyme, theory, organization, discipline, event, chemical element, person, astronomical object, academic journal, award, location, scientist and O.\nSentence: A second meeting was held soon thereafter and included Klaus Clusius , Robert Döpel , Werner Heisenberg , and Carl Friedrich von Weizsäcker .", "prompt_labels": "A(O) second(O) meeting(O) was(O) held(O) soon(O) thereafter(O) and(O) included(O) Klaus(B-scientist) Clusius(I-scientist) ,(O) Robert(B-scientist) Döpel(I-scientist) ,(O) Werner(B-scientist) Heisenberg(I-scientist) ,(O) and(O) Carl(B-scientist) Friedrich(I-scientist) von(I-scientist) Weizsäcker(I-scientist) .(O)"}}
{"id": "46", "dataset": "crossner_science", "split": "dev", "label_list": ["organization", "enzyme", "event", "person", "location", "chemical compound", "theory", "chemical element", "discipline", "protein", "astronomical object", "university", "award", "country", "academic journal", "scientist"], "instance": {"id": "46", "words": ["She", "competed", "in", "the", "4", "×", "100", "metres", "relay", "event", "at", "the", "2015", "World", "Championships", "in", "Athletics", "in", "Beijing", ",", "China", "."], "labels": ["O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "B-location", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, enzyme, event, person, location, chemical compound, theory, chemical element, discipline, protein, astronomical object, university, award, country, academic journal, scientist and O.\nSentence: She competed in the 4 × 100 metres relay event at the 2015 World Championships in Athletics in Beijing , China .", "prompt_labels": "She(O) competed(O) in(O) the(O) 4(B-event) ×(I-event) 100(I-event) metres(I-event) relay(I-event) event(I-event) at(O) the(O) 2015(B-event) World(I-event) Championships(I-event) in(O) Athletics(O) in(O) Beijing(B-location) ,(O) China(B-country) .(O)"}}
{"id": "388", "dataset": "crossner_science", "split": "dev", "label_list": ["person", "academic journal", "organization", "country", "scientist", "enzyme", "protein", "chemical compound", "location", "university", "award", "theory", "event", "discipline", "chemical element", "astronomical object"], "instance": {"id": "388", "words": ["He", "was", "educated", "at", "the", "Odessa", "University", "(", "1922-23", ")", "and", "at", "the", "Saint", "Petersburg", "State", "University", "(", "1923-1929", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-university", "I-university", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, academic journal, organization, country, scientist, enzyme, protein, chemical compound, location, university, award, theory, event, discipline, chemical element, astronomical object and O.\nSentence: He was educated at the Odessa University ( 1922-23 ) and at the Saint Petersburg State University ( 1923-1929 ) .", "prompt_labels": "He(O) was(O) educated(O) at(O) the(O) Odessa(B-university) University(I-university) ((O) 1922-23(O) )(O) and(O) at(O) the(O) Saint(B-university) Petersburg(I-university) State(I-university) University(I-university) ((O) 1923-1929(O) )(O) .(O)"}}
{"id": "430", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "astronomical object", "discipline", "event", "award", "chemical element", "country", "protein", "person", "academic journal", "location", "scientist", "chemical compound", "theory", "university", "organization"], "instance": {"id": "430", "words": ["The", "invitees", "included", "Walther", "Bothe", ",", "Siegfried", "Flügge", ",", "Hans", "Geiger", ",", "Otto", "Hahn", ",", "Paul", "Harteck", ",", "Gerhard", "Hoffmann", ",", "Josef", "Mattauch", ",", "and", "Georg", "Stetter", "."], "labels": ["O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, astronomical object, discipline, event, award, chemical element, country, protein, person, academic journal, location, scientist, chemical compound, theory, university, organization and O.\nSentence: The invitees included Walther Bothe , Siegfried Flügge , Hans Geiger , Otto Hahn , Paul Harteck , Gerhard Hoffmann , Josef Mattauch , and Georg Stetter .", "prompt_labels": "The(O) invitees(O) included(O) Walther(B-scientist) Bothe(I-scientist) ,(O) Siegfried(B-scientist) Flügge(I-scientist) ,(O) Hans(B-scientist) Geiger(I-scientist) ,(O) Otto(B-scientist) Hahn(I-scientist) ,(O) Paul(B-scientist) Harteck(I-scientist) ,(O) Gerhard(B-scientist) Hoffmann(I-scientist) ,(O) Josef(B-scientist) Mattauch(I-scientist) ,(O) and(O) Georg(B-scientist) Stetter(I-scientist) .(O)"}}
{"id": "142", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "event", "academic journal", "chemical compound", "discipline", "person", "chemical element", "enzyme", "scientist", "organization", "location", "country", "theory", "astronomical object", "protein", "award"], "instance": {"id": "142", "words": ["In", "2013", ",", "the", "exoplanet", "Kepler-62f", "was", "discovered", ",", "along", "with", "Kepler-62e", "and", "Kepler-62c", "."], "labels": ["O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, event, academic journal, chemical compound, discipline, person, chemical element, enzyme, scientist, organization, location, country, theory, astronomical object, protein, award and O.\nSentence: In 2013 , the exoplanet Kepler-62f was discovered , along with Kepler-62e and Kepler-62c .", "prompt_labels": "In(O) 2013(O) ,(O) the(O) exoplanet(O) Kepler-62f(B-astronomical object) was(O) discovered(O) ,(O) along(O) with(O) Kepler-62e(B-astronomical object) and(O) Kepler-62c(B-astronomical object) .(O)"}}
{"id": "354", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "academic journal", "enzyme", "location", "person", "country", "theory", "discipline", "chemical compound", "chemical element", "event", "award", "university", "astronomical object", "organization", "scientist"], "instance": {"id": "354", "words": ["Ceres", ",", "2", "Pallas", ",", "3", "Juno", "and", "4", "Vesta", "lost", "their", "planet", "status", "after", "the", "discovery", "of", "many", "other", "asteroid", "s", "."], "labels": ["B-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, academic journal, enzyme, location, person, country, theory, discipline, chemical compound, chemical element, event, award, university, astronomical object, organization, scientist and O.\nSentence: Ceres , 2 Pallas , 3 Juno and 4 Vesta lost their planet status after the discovery of many other asteroid s .", "prompt_labels": "Ceres(B-astronomical object) ,(O) 2(B-astronomical object) Pallas(I-astronomical object) ,(O) 3(B-astronomical object) Juno(I-astronomical object) and(O) 4(B-astronomical object) Vesta(I-astronomical object) lost(O) their(O) planet(O) status(O) after(O) the(O) discovery(O) of(O) many(O) other(O) asteroid(O) s(O) .(O)"}}
{"id": "436", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "protein", "country", "chemical compound", "discipline", "academic journal", "event", "university", "scientist", "chemical element", "theory", "award", "organization", "person", "astronomical object", "location"], "instance": {"id": "436", "words": ["The", "film", "was", "also", "nominated", "for", "Academy", "Award", "for", "Best", "Supporting", "Actor", "(", "Sam", "Shepard", ")", ",", "Academy", "Award", "for", "Best", "Production", "Design", "(", "Art", "Direction", ":", "Geoffrey", "Kirkland", ",", "Richard", "Lawrence", ",", "W.", "Stewart", "Campbell", "and", "Peter", "R.", "Romero", ";", "Set", "Decoration", ":", "George", "R.", "Nelson", ")", ",", "Academy", "Award", "for", "Best", "Cinematography", "(", "Caleb", "Deschanel", ")", "and", "Academy", "Award", "for", "Best", "Picture", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "B-person", "I-person", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, protein, country, chemical compound, discipline, academic journal, event, university, scientist, chemical element, theory, award, organization, person, astronomical object, location and O.\nSentence: The film was also nominated for Academy Award for Best Supporting Actor ( Sam Shepard ) , Academy Award for Best Production Design ( Art Direction : Geoffrey Kirkland , Richard Lawrence , W. Stewart Campbell and Peter R. Romero ; Set Decoration : George R. Nelson ) , Academy Award for Best Cinematography ( Caleb Deschanel ) and Academy Award for Best Picture .", "prompt_labels": "The(O) film(O) was(O) also(O) nominated(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actor(I-award) ((O) Sam(B-person) Shepard(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Production(I-award) Design(I-award) ((O) Art(B-person) Direction(I-person) :(O) Geoffrey(B-person) Kirkland(I-person) ,(O) Richard(B-person) Lawrence(I-person) ,(O) W.(B-person) Stewart(I-person) Campbell(I-person) and(O) Peter(B-person) R.(I-person) Romero(I-person) ;(O) Set(B-person) Decoration(I-person) :(O) George(B-person) R.(I-person) Nelson(I-person) )(O) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Cinematography(I-award) ((O) Caleb(B-person) Deschanel(I-person) )(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) .(O)"}}
{"id": "296", "dataset": "crossner_science", "split": "dev", "label_list": ["award", "scientist", "university", "chemical compound", "discipline", "person", "event", "theory", "country", "protein", "astronomical object", "academic journal", "location", "chemical element", "enzyme", "organization"], "instance": {"id": "296", "words": ["Gilbert", "Gil", "Jerome", "Perlow", "(", "10", "February", "1916", "-", "17", "February", "2007", ")", ",", "was", "an", "American", "physicist", "famous", "for", "his", "work", "related", "to", "the", "Mössbauer", "effect", ",", "and", "an", "editor", "of", "the", "Journal", "of", "Applied", "Physics", "and", "Applied", "Physics", "Letters", "."], "labels": ["B-scientist", "I-scientist", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-theory", "I-theory", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, scientist, university, chemical compound, discipline, person, event, theory, country, protein, astronomical object, academic journal, location, chemical element, enzyme, organization and O.\nSentence: Gilbert Gil Jerome Perlow ( 10 February 1916 - 17 February 2007 ) , was an American physicist famous for his work related to the Mössbauer effect , and an editor of the Journal of Applied Physics and Applied Physics Letters .", "prompt_labels": "Gilbert(B-scientist) Gil(I-scientist) Jerome(B-scientist) Perlow(I-scientist) ((O) 10(O) February(O) 1916(O) -(O) 17(O) February(O) 2007(O) )(O) ,(O) was(O) an(O) American(O) physicist(O) famous(O) for(O) his(O) work(O) related(O) to(O) the(O) Mössbauer(B-theory) effect(I-theory) ,(O) and(O) an(O) editor(O) of(O) the(O) Journal(B-academic journal) of(I-academic journal) Applied(I-academic journal) Physics(I-academic journal) and(O) Applied(B-academic journal) Physics(I-academic journal) Letters(I-academic journal) .(O)"}}
{"id": "103", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "award", "chemical element", "university", "organization", "chemical compound", "theory", "protein", "country", "discipline", "astronomical object", "event", "person", "scientist", "enzyme", "academic journal"], "instance": {"id": "103", "words": ["The", "Council", "of", "Europe", "also", "has", "a", "Congress", "of", "the", "Council", "of", "Europe", ",", "similar", "to", "the", "EU", "'s", "Committee", "of", "the", "Regions", "."], "labels": ["O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, chemical element, university, organization, chemical compound, theory, protein, country, discipline, astronomical object, event, person, scientist, enzyme, academic journal and O.\nSentence: The Council of Europe also has a Congress of the Council of Europe , similar to the EU 's Committee of the Regions .", "prompt_labels": "The(O) Council(B-organization) of(I-organization) Europe(I-organization) also(O) has(O) a(O) Congress(B-organization) of(I-organization) the(I-organization) Council(I-organization) of(I-organization) Europe(I-organization) ,(O) similar(O) to(O) the(O) EU(B-organization) 's(I-organization) Committee(I-organization) of(I-organization) the(I-organization) Regions(I-organization) .(O)"}}
{"id": "7", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "protein", "university", "scientist", "location", "theory", "discipline", "enzyme", "academic journal", "country", "astronomical object", "award", "person", "organization", "event", "chemical compound"], "instance": {"id": "7", "words": ["The", "Olympic", "golf", "course", "is", "a", "new", "venue", "built", "for", "the", "Golf", "at", "the", "2016", "Summer", "Olympics", "."], "labels": ["O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, protein, university, scientist, location, theory, discipline, enzyme, academic journal, country, astronomical object, award, person, organization, event, chemical compound and O.\nSentence: The Olympic golf course is a new venue built for the Golf at the 2016 Summer Olympics .", "prompt_labels": "The(O) Olympic(B-location) golf(I-location) course(I-location) is(O) a(O) new(O) venue(O) built(O) for(O) the(O) Golf(O) at(O) the(O) 2016(B-event) Summer(I-event) Olympics(I-event) .(O)"}}
{"id": "120", "dataset": "crossner_science", "split": "dev", "label_list": ["scientist", "person", "country", "chemical compound", "enzyme", "theory", "organization", "university", "protein", "event", "academic journal", "award", "discipline", "astronomical object", "location", "chemical element"], "instance": {"id": "120", "words": ["GSK-3", "has", "been", "implicated", "in", "bipolar", "disorder", ",", "as", "bipolar", "medications", "lithium", "and", "valproate", "have", "been", "shown", "to", "increase", "its", "phosphorylation", ",", "thereby", "inhibiting", "it", "."], "labels": ["B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical element", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, person, country, chemical compound, enzyme, theory, organization, university, protein, event, academic journal, award, discipline, astronomical object, location, chemical element and O.\nSentence: GSK-3 has been implicated in bipolar disorder , as bipolar medications lithium and valproate have been shown to increase its phosphorylation , thereby inhibiting it .", "prompt_labels": "GSK-3(B-protein) has(O) been(O) implicated(O) in(O) bipolar(O) disorder(O) ,(O) as(O) bipolar(O) medications(O) lithium(B-chemical element) and(O) valproate(B-chemical compound) have(O) been(O) shown(O) to(O) increase(O) its(O) phosphorylation(O) ,(O) thereby(O) inhibiting(O) it(O) .(O)"}}
{"id": "334", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "enzyme", "award", "scientist", "chemical compound", "academic journal", "theory", "organization", "discipline", "event", "country", "person", "chemical element", "protein", "university", "location"], "instance": {"id": "334", "words": ["Starch", ";", "(", "fermentation", ")", ";", "Lactic", "acid", ";", "Polylactic", "acid", "(", "PLA", ")", "."], "labels": ["B-chemical compound", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, enzyme, award, scientist, chemical compound, academic journal, theory, organization, discipline, event, country, person, chemical element, protein, university, location and O.\nSentence: Starch ; ( fermentation ) ; Lactic acid ; Polylactic acid ( PLA ) .", "prompt_labels": "Starch(B-chemical compound) ;(O) ((O) fermentation(O) )(O) ;(O) Lactic(B-chemical compound) acid(I-chemical compound) ;(O) Polylactic(B-chemical compound) acid(I-chemical compound) ((O) PLA(B-chemical compound) )(O) .(O)"}}
{"id": "167", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "protein", "scientist", "award", "theory", "chemical element", "university", "location", "person", "astronomical object", "event", "country", "organization", "enzyme", "discipline", "chemical compound"], "instance": {"id": "167", "words": ["She", "also", "played", "at", "1986", ",", "1989", ",", "1991", ",", "1993", ",", "1995", "AFC", "Championship", ",", "1990", "and", "Football", "at", "the", "1994", "Asian", "Games", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, protein, scientist, award, theory, chemical element, university, location, person, astronomical object, event, country, organization, enzyme, discipline, chemical compound and O.\nSentence: She also played at 1986 , 1989 , 1991 , 1993 , 1995 AFC Championship , 1990 and Football at the 1994 Asian Games .", "prompt_labels": "She(O) also(O) played(O) at(O) 1986(O) ,(O) 1989(O) ,(O) 1991(O) ,(O) 1993(O) ,(O) 1995(O) AFC(B-event) Championship(I-event) ,(O) 1990(O) and(O) Football(O) at(O) the(O) 1994(O) Asian(B-event) Games(I-event) .(O)"}}
{"id": "415", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "protein", "theory", "enzyme", "country", "astronomical object", "university", "organization", "academic journal", "event", "award", "person", "scientist", "chemical element", "discipline", "chemical compound"], "instance": {"id": "415", "words": ["ref", "name", "=", "necrop", "/", "Other", "groups", "first", "observed", "that", "the", "stimulation", "of", "Fas", "/", "TNFR", "family", "of", "Death", "domain", "receptors", "(", "DR", ")", "activated", "a", "canonical", "apoptotic", "pathway", ";", "however", ",", "in", "many", "cell", "types", ",", "not", "only", "did", "caspase", "inhibition", "fail", "to", "inhibit", "cell", "death", ",", "as", "would", "be", "expected", "of", "canonical", "apoptosis", ",", "but", "stimulated", "cells", "experienced", "a", "form", "of", "cell", "death", "that", "more", "closely", "resembled", "necrosis", "than", "apoptosis", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, protein, theory, enzyme, country, astronomical object, university, organization, academic journal, event, award, person, scientist, chemical element, discipline, chemical compound and O.\nSentence: ref name = necrop / Other groups first observed that the stimulation of Fas / TNFR family of Death domain receptors ( DR ) activated a canonical apoptotic pathway ; however , in many cell types , not only did caspase inhibition fail to inhibit cell death , as would be expected of canonical apoptosis , but stimulated cells experienced a form of cell death that more closely resembled necrosis than apoptosis .", "prompt_labels": "ref(O) name(O) =(O) necrop(O) /(O) Other(O) groups(O) first(O) observed(O) that(O) the(O) stimulation(O) of(O) Fas(O) /(O) TNFR(O) family(O) of(O) Death(O) domain(O) receptors(O) ((O) DR(O) )(O) activated(O) a(O) canonical(O) apoptotic(O) pathway(O) ;(O) however(O) ,(O) in(O) many(O) cell(O) types(O) ,(O) not(O) only(O) did(O) caspase(O) inhibition(O) fail(O) to(O) inhibit(O) cell(O) death(O) ,(O) as(O) would(O) be(O) expected(O) of(O) canonical(O) apoptosis(O) ,(O) but(O) stimulated(O) cells(O) experienced(O) a(O) form(O) of(O) cell(O) death(O) that(O) more(O) closely(O) resembled(O) necrosis(O) than(O) apoptosis(O) .(O)"}}
{"id": "321", "dataset": "crossner_science", "split": "dev", "label_list": ["event", "person", "chemical element", "university", "award", "chemical compound", "discipline", "theory", "organization", "academic journal", "location", "astronomical object", "scientist", "protein", "country", "enzyme"], "instance": {"id": "321", "words": ["The", "song", "was", "nominated", "for", "various", "Grammy", "Award", ",", "winning", "Best", "Pop", "Duo", "/", "Group", "Performance", "and", "Grammy", "Award", "for", "Best", "Music", "Video", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, chemical element, university, award, chemical compound, discipline, theory, organization, academic journal, location, astronomical object, scientist, protein, country, enzyme and O.\nSentence: The song was nominated for various Grammy Award , winning Best Pop Duo / Group Performance and Grammy Award for Best Music Video .", "prompt_labels": "The(O) song(O) was(O) nominated(O) for(O) various(O) Grammy(B-award) Award(I-award) ,(O) winning(O) Best(B-award) Pop(I-award) Duo(I-award) /(I-award) Group(I-award) Performance(I-award) and(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Music(I-award) Video(I-award) .(O)"}}
{"id": "122", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "country", "astronomical object", "university", "award", "person", "academic journal", "discipline", "event", "chemical compound", "enzyme", "protein", "organization", "scientist", "theory", "location"], "instance": {"id": "122", "words": ["Since", "then", ",", "names", "have", "been", "given", "to", "134", "additional", "satellites", ":", "57", "satellites", "of", "Jupiter", ",", "43", "of", "Saturn", ",", "22", "of", "Uranus", ",", "12", "of", "Neptune", ",", "5", "of", "Pluto", ",", "1", "of", "Eris", ",", "and", "2", "of", "Haumea", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "B-astronomical object", "O", "O", "O", "B-astronomical object", "O", "O", "O", "B-astronomical object", "O", "O", "O", "B-astronomical object", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, country, astronomical object, university, award, person, academic journal, discipline, event, chemical compound, enzyme, protein, organization, scientist, theory, location and O.\nSentence: Since then , names have been given to 134 additional satellites : 57 satellites of Jupiter , 43 of Saturn , 22 of Uranus , 12 of Neptune , 5 of Pluto , 1 of Eris , and 2 of Haumea .", "prompt_labels": "Since(O) then(O) ,(O) names(O) have(O) been(O) given(O) to(O) 134(O) additional(O) satellites(O) :(O) 57(O) satellites(O) of(O) Jupiter(B-astronomical object) ,(O) 43(O) of(O) Saturn(B-astronomical object) ,(O) 22(O) of(O) Uranus(B-astronomical object) ,(O) 12(O) of(O) Neptune(B-astronomical object) ,(O) 5(O) of(O) Pluto(B-astronomical object) ,(O) 1(O) of(O) Eris(B-astronomical object) ,(O) and(O) 2(O) of(O) Haumea(B-astronomical object) .(O)"}}
{"id": "392", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "country", "discipline", "protein", "award", "academic journal", "scientist", "chemical element", "location", "astronomical object", "chemical compound", "organization", "event", "theory", "university", "person"], "instance": {"id": "392", "words": ["2006", ",", "281", ",", "1426-1431", "Substrates", "of", "glycosynthase", "include", "Glucose", ",", "Galactose", ",", "Mannose", ",", "Xylose", ",", "and", "Glucuronic", "acid", ".", "Wilkinson", ",", "S.", ";", "Liew", ",", "C.", ";", "Mackay", ",", "J.", ";", "Salleh", ",", "H.", ";", "Withers", ",", "S.", ";", "McLeod", ",", "M.", "Org", "Lett", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-enzyme", "O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "I-person", "B-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, country, discipline, protein, award, academic journal, scientist, chemical element, location, astronomical object, chemical compound, organization, event, theory, university, person and O.\nSentence: 2006 , 281 , 1426-1431 Substrates of glycosynthase include Glucose , Galactose , Mannose , Xylose , and Glucuronic acid . Wilkinson , S. ; Liew , C. ; Mackay , J. ; Salleh , H. ; Withers , S. ; McLeod , M. Org Lett .", "prompt_labels": "2006(O) ,(O) 281(O) ,(O) 1426-1431(O) Substrates(O) of(O) glycosynthase(B-enzyme) include(O) Glucose(B-chemical compound) ,(O) Galactose(B-chemical compound) ,(O) Mannose(B-chemical compound) ,(O) Xylose(B-chemical compound) ,(O) and(O) Glucuronic(B-chemical compound) acid(I-chemical compound) .(O) Wilkinson(B-person) ,(I-person) S.(I-person) ;(O) Liew(B-person) ,(I-person) C.(I-person) ;(O) Mackay(B-person) ,(I-person) J.(I-person) ;(O) Salleh(B-person) ,(I-person) H.(I-person) ;(O) Withers(B-person) ,(I-person) S.(I-person) ;(O) McLeod(B-person) ,(I-person) M.(I-person) Org(B-academic journal) Lett(I-academic journal) .(O)"}}
{"id": "170", "dataset": "crossner_science", "split": "dev", "label_list": ["organization", "academic journal", "chemical element", "astronomical object", "person", "award", "chemical compound", "discipline", "university", "location", "enzyme", "country", "protein", "scientist", "theory", "event"], "instance": {"id": "170", "words": ["These", "usually", "are", "limited", "to", "the", "planets", "from", "Mercury", "to", "Saturn", ",", "although", "some", "include", "Uranus", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, academic journal, chemical element, astronomical object, person, award, chemical compound, discipline, university, location, enzyme, country, protein, scientist, theory, event and O.\nSentence: These usually are limited to the planets from Mercury to Saturn , although some include Uranus .", "prompt_labels": "These(O) usually(O) are(O) limited(O) to(O) the(O) planets(O) from(O) Mercury(B-astronomical object) to(O) Saturn(B-astronomical object) ,(O) although(O) some(O) include(O) Uranus(B-astronomical object) .(O)"}}
{"id": "188", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "astronomical object", "event", "country", "theory", "discipline", "chemical compound", "protein", "chemical element", "award", "enzyme", "university", "location", "organization", "scientist", "person"], "instance": {"id": "188", "words": ["Michel", "Adanson", "(", "1763", ")", ",", "Antoine", "Laurent", "de", "Jussieu", "(", "1789", ")", ",", "and", "Augustin", "Pyramus", "de", "Candolle", "(", "1819", ")", "all", "proposed", "various", "alternative", "natural", "systems", "of", "classification", "that", "grouped", "plants", "using", "a", "wider", "range", "of", "shared", "characters", "and", "were", "widely", "followed", "."], "labels": ["B-scientist", "I-scientist", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, astronomical object, event, country, theory, discipline, chemical compound, protein, chemical element, award, enzyme, university, location, organization, scientist, person and O.\nSentence: Michel Adanson ( 1763 ) , Antoine Laurent de Jussieu ( 1789 ) , and Augustin Pyramus de Candolle ( 1819 ) all proposed various alternative natural systems of classification that grouped plants using a wider range of shared characters and were widely followed .", "prompt_labels": "Michel(B-scientist) Adanson(I-scientist) ((O) 1763(O) )(O) ,(O) Antoine(B-scientist) Laurent(I-scientist) de(I-scientist) Jussieu(I-scientist) ((O) 1789(O) )(O) ,(O) and(O) Augustin(B-scientist) Pyramus(I-scientist) de(I-scientist) Candolle(I-scientist) ((O) 1819(O) )(O) all(O) proposed(O) various(O) alternative(O) natural(O) systems(O) of(O) classification(O) that(O) grouped(O) plants(O) using(O) a(O) wider(O) range(O) of(O) shared(O) characters(O) and(O) were(O) widely(O) followed(O) .(O)"}}
{"id": "251", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "theory", "astronomical object", "country", "university", "award", "discipline", "person", "organization", "protein", "enzyme", "chemical element", "academic journal", "scientist", "event", "chemical compound"], "instance": {"id": "251", "words": ["He", "speculated", "that", "the", "planet", "Venus", "is", "a", "former", "comet", "which", "was", "ejected", "from", "Jupiter", "and", "subsequently", "3,500", "years", "ago", "made", "two", "catastrophic", "close", "passes", "by", "Earth", ",", "52", "years", "apart", ",", "and", "later", "interacted", "with", "Mars", ",", "which", "then", "had", "a", "series", "of", "near", "collisions", "with", "Earth", "which", "ended", "in", "687", "BCE", ",", "before", "settling", "into", "its", "current", "orbit", "."], "labels": ["O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, theory, astronomical object, country, university, award, discipline, person, organization, protein, enzyme, chemical element, academic journal, scientist, event, chemical compound and O.\nSentence: He speculated that the planet Venus is a former comet which was ejected from Jupiter and subsequently 3,500 years ago made two catastrophic close passes by Earth , 52 years apart , and later interacted with Mars , which then had a series of near collisions with Earth which ended in 687 BCE , before settling into its current orbit .", "prompt_labels": "He(O) speculated(O) that(O) the(O) planet(O) Venus(B-astronomical object) is(O) a(O) former(O) comet(B-astronomical object) which(O) was(O) ejected(O) from(O) Jupiter(B-astronomical object) and(O) subsequently(O) 3,500(O) years(O) ago(O) made(O) two(O) catastrophic(O) close(O) passes(O) by(O) Earth(B-astronomical object) ,(O) 52(O) years(O) apart(O) ,(O) and(O) later(O) interacted(O) with(O) Mars(B-astronomical object) ,(O) which(O) then(O) had(O) a(O) series(O) of(O) near(O) collisions(O) with(O) Earth(B-astronomical object) which(O) ended(O) in(O) 687(O) BCE(O) ,(O) before(O) settling(O) into(O) its(O) current(O) orbit(O) .(O)"}}
{"id": "400", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "chemical compound", "award", "event", "organization", "location", "astronomical object", "country", "discipline", "protein", "theory", "university", "scientist", "enzyme", "chemical element", "person"], "instance": {"id": "400", "words": ["In", "the", "finale", "episode", "The", "End", ",", "recurring", "guest", "stars", "Sam", "Anderson", ";", "L.", "Scott", "Caldwell", ";", "Francois", "Chau", ";", "Fionnula", "Flanagan", ";", "Sonya", "Walger", ";", "and", "John", "Terry", "were", "credited", "under", "the", "starring", "rubric", "alongside", "the", "principal", "cast", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, chemical compound, award, event, organization, location, astronomical object, country, discipline, protein, theory, university, scientist, enzyme, chemical element, person and O.\nSentence: In the finale episode The End , recurring guest stars Sam Anderson ; L. Scott Caldwell ; Francois Chau ; Fionnula Flanagan ; Sonya Walger ; and John Terry were credited under the starring rubric alongside the principal cast .", "prompt_labels": "In(O) the(O) finale(O) episode(O) The(O) End(O) ,(O) recurring(O) guest(O) stars(O) Sam(B-person) Anderson(I-person) ;(O) L.(B-person) Scott(I-person) Caldwell(I-person) ;(O) Francois(B-person) Chau(I-person) ;(O) Fionnula(B-person) Flanagan(I-person) ;(O) Sonya(B-person) Walger(I-person) ;(O) and(O) John(B-person) Terry(I-person) were(O) credited(O) under(O) the(O) starring(O) rubric(O) alongside(O) the(O) principal(O) cast(O) .(O)"}}
{"id": "153", "dataset": "crossner_science", "split": "dev", "label_list": ["award", "university", "discipline", "person", "enzyme", "event", "academic journal", "country", "protein", "theory", "organization", "chemical compound", "scientist", "chemical element", "location", "astronomical object"], "instance": {"id": "153", "words": ["In", "this", "respect", "he", "was", "the", "equivalent", "of", "Mars", ",", "Janus", ",", "Saturn", "and", "even", "Jupiter", "among", "Latin", "tribes", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, university, discipline, person, enzyme, event, academic journal, country, protein, theory, organization, chemical compound, scientist, chemical element, location, astronomical object and O.\nSentence: In this respect he was the equivalent of Mars , Janus , Saturn and even Jupiter among Latin tribes .", "prompt_labels": "In(O) this(O) respect(O) he(O) was(O) the(O) equivalent(O) of(O) Mars(B-astronomical object) ,(O) Janus(B-astronomical object) ,(O) Saturn(B-astronomical object) and(O) even(O) Jupiter(B-astronomical object) among(O) Latin(O) tribes(O) .(O)"}}
{"id": "355", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "country", "discipline", "protein", "organization", "university", "scientist", "chemical compound", "theory", "award", "event", "chemical element", "location", "person", "enzyme", "academic journal"], "instance": {"id": "355", "words": ["Her", "lab", "currently", "has", "three", "main", "projects", ":", "identifying", "targets", "and", "mechanisms", "of", "expression", "regulation", "of", "the", "Nocturnin", "gene", ";", "identifying", "the", "mechanism", "of", "metabolic", "control", "of", "Nocturnin", "knockout", "lean", "mice", ";", "and", "defining", "structural", "components", "of", "the", "repressor", "protein", "Cryptochrome", "and", "how", "regulation", "of", "the", "nuclear", "entry", "of", "the", "protein", "contributes", "to", "circadian", "period", "length", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, country, discipline, protein, organization, university, scientist, chemical compound, theory, award, event, chemical element, location, person, enzyme, academic journal and O.\nSentence: Her lab currently has three main projects : identifying targets and mechanisms of expression regulation of the Nocturnin gene ; identifying the mechanism of metabolic control of Nocturnin knockout lean mice ; and defining structural components of the repressor protein Cryptochrome and how regulation of the nuclear entry of the protein contributes to circadian period length .", "prompt_labels": "Her(O) lab(O) currently(O) has(O) three(O) main(O) projects(O) :(O) identifying(O) targets(O) and(O) mechanisms(O) of(O) expression(O) regulation(O) of(O) the(O) Nocturnin(B-enzyme) gene(O) ;(O) identifying(O) the(O) mechanism(O) of(O) metabolic(O) control(O) of(O) Nocturnin(O) knockout(O) lean(O) mice(O) ;(O) and(O) defining(O) structural(O) components(O) of(O) the(O) repressor(O) protein(O) Cryptochrome(B-protein) and(O) how(O) regulation(O) of(O) the(O) nuclear(O) entry(O) of(O) the(O) protein(O) contributes(O) to(O) circadian(O) period(O) length(O) .(O)"}}
{"id": "329", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "academic journal", "country", "enzyme", "chemical element", "theory", "event", "organization", "location", "chemical compound", "award", "astronomical object", "scientist", "person", "university", "discipline"], "instance": {"id": "329", "words": ["In", "2015", ",", "Hill", "was", "a", "Professor", "at", "the", "Ontario", "Cancer", "Institute", "and", "a", "Senior", "Scientist", "at", "the", "Princess", "Margaret", "Cancer", "Centre", "within", "the", "University", "Health", "Network", ",", "which", "is", "affiliated", "with", "the", "University", "of", "Toronto", "Faculty", "of", "Medicine", "."], "labels": ["O", "O", "O", "B-scientist", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, academic journal, country, enzyme, chemical element, theory, event, organization, location, chemical compound, award, astronomical object, scientist, person, university, discipline and O.\nSentence: In 2015 , Hill was a Professor at the Ontario Cancer Institute and a Senior Scientist at the Princess Margaret Cancer Centre within the University Health Network , which is affiliated with the University of Toronto Faculty of Medicine .", "prompt_labels": "In(O) 2015(O) ,(O) Hill(B-scientist) was(O) a(O) Professor(O) at(O) the(O) Ontario(B-organization) Cancer(I-organization) Institute(I-organization) and(O) a(O) Senior(O) Scientist(O) at(O) the(O) Princess(B-organization) Margaret(I-organization) Cancer(I-organization) Centre(I-organization) within(O) the(O) University(B-university) Health(I-university) Network(I-university) ,(O) which(O) is(O) affiliated(O) with(O) the(O) University(B-organization) of(I-organization) Toronto(I-organization) Faculty(I-organization) of(I-organization) Medicine(I-organization) .(O)"}}
{"id": "110", "dataset": "crossner_science", "split": "dev", "label_list": ["scientist", "location", "event", "protein", "enzyme", "person", "country", "academic journal", "discipline", "award", "chemical element", "university", "organization", "astronomical object", "chemical compound", "theory"], "instance": {"id": "110", "words": ["He", "was", "also", "awarded", "the", "Eddington", "Medal", "of", "the", "Royal", "Astronomical", "Society", "in", "1969", "."], "labels": ["O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, location, event, protein, enzyme, person, country, academic journal, discipline, award, chemical element, university, organization, astronomical object, chemical compound, theory and O.\nSentence: He was also awarded the Eddington Medal of the Royal Astronomical Society in 1969 .", "prompt_labels": "He(O) was(O) also(O) awarded(O) the(O) Eddington(B-award) Medal(I-award) of(O) the(O) Royal(B-organization) Astronomical(I-organization) Society(I-organization) in(O) 1969(O) .(O)"}}
{"id": "127", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "enzyme", "event", "university", "chemical compound", "organization", "academic journal", "award", "discipline", "scientist", "theory", "protein", "chemical element", "astronomical object", "person", "country"], "instance": {"id": "127", "words": ["Portions", "of", "Galveston", "County", "are", "served", "by", "College", "of", "the", "Mainland", "and", "Galveston", "College", "."], "labels": ["O", "O", "B-location", "I-location", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "B-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, enzyme, event, university, chemical compound, organization, academic journal, award, discipline, scientist, theory, protein, chemical element, astronomical object, person, country and O.\nSentence: Portions of Galveston County are served by College of the Mainland and Galveston College .", "prompt_labels": "Portions(O) of(O) Galveston(B-location) County(I-location) are(O) served(O) by(O) College(B-university) of(I-university) the(I-university) Mainland(I-university) and(O) Galveston(B-university) College(I-university) .(O)"}}
{"id": "429", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "academic journal", "award", "event", "chemical element", "astronomical object", "person", "country", "university", "scientist", "chemical compound", "discipline", "theory", "location", "enzyme", "organization"], "instance": {"id": "429", "words": ["Her", "work", "on", "ATP-binding", "cassette", "transporter", "includes", "investigating", "their", "role", "in", "resistance", "to", "chemotherapy", "drugs", ";", "antigen", "presentation", "in", "adaptive", "immunity", "and", "viral", "infection", ";", "cystic", "fibrosis", ";", "and", "bacterial", "nutrition", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, academic journal, award, event, chemical element, astronomical object, person, country, university, scientist, chemical compound, discipline, theory, location, enzyme, organization and O.\nSentence: Her work on ATP-binding cassette transporter includes investigating their role in resistance to chemotherapy drugs ; antigen presentation in adaptive immunity and viral infection ; cystic fibrosis ; and bacterial nutrition .", "prompt_labels": "Her(O) work(O) on(O) ATP-binding(O) cassette(O) transporter(O) includes(O) investigating(O) their(O) role(O) in(O) resistance(O) to(O) chemotherapy(O) drugs(O) ;(O) antigen(O) presentation(O) in(O) adaptive(O) immunity(O) and(O) viral(O) infection(O) ;(O) cystic(O) fibrosis(O) ;(O) and(O) bacterial(O) nutrition(O) .(O)"}}
{"id": "99", "dataset": "crossner_science", "split": "dev", "label_list": ["person", "location", "award", "organization", "chemical compound", "theory", "event", "discipline", "astronomical object", "scientist", "academic journal", "protein", "chemical element", "enzyme", "university", "country"], "instance": {"id": "99", "words": ["They", "spent", "the", "winter", "of", "1866-67", "in", "Paris", ",", "where", "Gibbs", "attended", "lectures", "at", "the", "University", "of", "Paris", "and", "the", "Collège", "de", "France", ",", "given", "by", "such", "distinguished", "mathematical", "scientists", "as", "Joseph", "Liouville", "and", "Michel", "Chasles", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "B-scientist", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, award, organization, chemical compound, theory, event, discipline, astronomical object, scientist, academic journal, protein, chemical element, enzyme, university, country and O.\nSentence: They spent the winter of 1866-67 in Paris , where Gibbs attended lectures at the University of Paris and the Collège de France , given by such distinguished mathematical scientists as Joseph Liouville and Michel Chasles .", "prompt_labels": "They(O) spent(O) the(O) winter(O) of(O) 1866-67(O) in(O) Paris(B-location) ,(O) where(O) Gibbs(B-scientist) attended(O) lectures(O) at(O) the(O) University(B-university) of(I-university) Paris(I-university) and(O) the(O) Collège(B-university) de(I-university) France(I-university) ,(O) given(O) by(O) such(O) distinguished(O) mathematical(O) scientists(O) as(O) Joseph(B-scientist) Liouville(I-scientist) and(O) Michel(B-scientist) Chasles(I-scientist) .(O)"}}
{"id": "387", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "university", "academic journal", "astronomical object", "person", "protein", "chemical compound", "discipline", "scientist", "award", "location", "chemical element", "event", "theory", "country", "organization"], "instance": {"id": "387", "words": ["For", "example", ",", "the", "first", "neuroligin", "(", "NLGN1", ")", "discovered", "was", "identified", "by", "its", "PDZ", "domain", "which", "binds", "to", "PSD95", ",", "a", "well-known", "a", "scaffold", "protein", "at", "glutamatergic", "synapses", "that", "functionally", "links", "NMDA", "receptor", "nowiki", "/", "s", "to", "the", "proper", "post-synaptic", "locale", "."], "labels": ["O", "O", "O", "O", "O", "B-protein", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, university, academic journal, astronomical object, person, protein, chemical compound, discipline, scientist, award, location, chemical element, event, theory, country, organization and O.\nSentence: For example , the first neuroligin ( NLGN1 ) discovered was identified by its PDZ domain which binds to PSD95 , a well-known a scaffold protein at glutamatergic synapses that functionally links NMDA receptor nowiki / s to the proper post-synaptic locale .", "prompt_labels": "For(O) example(O) ,(O) the(O) first(O) neuroligin(B-protein) ((O) NLGN1(B-protein) )(O) discovered(O) was(O) identified(O) by(O) its(O) PDZ(O) domain(O) which(O) binds(O) to(O) PSD95(B-protein) ,(O) a(O) well-known(O) a(O) scaffold(O) protein(O) at(O) glutamatergic(O) synapses(O) that(O) functionally(O) links(O) NMDA(B-protein) receptor(I-protein) nowiki(O) /(O) s(O) to(O) the(O) proper(O) post-synaptic(O) locale(O) .(O)"}}
{"id": "130", "dataset": "crossner_science", "split": "dev", "label_list": ["event", "scientist", "academic journal", "astronomical object", "enzyme", "country", "university", "person", "chemical element", "award", "organization", "chemical compound", "discipline", "theory", "location", "protein"], "instance": {"id": "130", "words": ["Seven", "other", "objects", "are", "classified", "as", "both", "periodic", "comets", "and", "numbered", "asteroids", ":", "2060", "Chiron", "(", "95P", "/", "Chiron", ")", ",", "4015", "Wilson-Harrington", "(", "107P", "/", "Wilson-Harrington", ")", ",", "7968", "Elst-Pizarro", "(", "133P", "/", "Elst-Pizarro", ")", ",", "60558", "Echeclus", "(", "174P", "/", "Echeclus", ")", ",", "(", "362P", "/", "2008", "GOsub98", "/", "sub", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "I-astronomical object", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "I-astronomical object", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "I-astronomical object", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "I-astronomical object", "O", "O", "O", "B-astronomical object", "I-astronomical object", "I-astronomical object", "I-astronomical object", "I-astronomical object", "I-astronomical object", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, scientist, academic journal, astronomical object, enzyme, country, university, person, chemical element, award, organization, chemical compound, discipline, theory, location, protein and O.\nSentence: Seven other objects are classified as both periodic comets and numbered asteroids : 2060 Chiron ( 95P / Chiron ) , 4015 Wilson-Harrington ( 107P / Wilson-Harrington ) , 7968 Elst-Pizarro ( 133P / Elst-Pizarro ) , 60558 Echeclus ( 174P / Echeclus ) , ( 362P / 2008 GOsub98 / sub ) .", "prompt_labels": "Seven(O) other(O) objects(O) are(O) classified(O) as(O) both(O) periodic(O) comets(O) and(O) numbered(O) asteroids(O) :(O) 2060(B-astronomical object) Chiron(I-astronomical object) ((O) 95P(B-astronomical object) /(I-astronomical object) Chiron(I-astronomical object) )(O) ,(O) 4015(B-astronomical object) Wilson-Harrington(I-astronomical object) ((O) 107P(B-astronomical object) /(I-astronomical object) Wilson-Harrington(I-astronomical object) )(O) ,(O) 7968(B-astronomical object) Elst-Pizarro(I-astronomical object) ((O) 133P(B-astronomical object) /(I-astronomical object) Elst-Pizarro(I-astronomical object) )(O) ,(O) 60558(B-astronomical object) Echeclus(I-astronomical object) ((O) 174P(B-astronomical object) /(I-astronomical object) Echeclus(I-astronomical object) )(O) ,(O) ((O) 362P(B-astronomical object) /(I-astronomical object) 2008(I-astronomical object) GOsub98(I-astronomical object) /(I-astronomical object) sub(I-astronomical object) )(O) .(O)"}}
{"id": "422", "dataset": "crossner_science", "split": "dev", "label_list": ["person", "protein", "location", "enzyme", "theory", "discipline", "astronomical object", "chemical compound", "organization", "academic journal", "award", "university", "chemical element", "country", "event", "scientist"], "instance": {"id": "422", "words": ["CCR1", "was", "the", "first", "CC", "chemokine", "receptor", "identified", "and", "binds", "multiple", "inflammatory", "/", "inducible", "(", "see", "inducible", "gene", ")", "CC", "chemokines", "(", "including", "CCL4", ",", "CCL5", ",", "CCL6", ",", "CCL14", ",", "CCL15", ",", "CCL16", "and", "CCL23", ")", "."], "labels": ["B-protein", "O", "O", "O", "B-protein", "I-protein", "I-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, protein, location, enzyme, theory, discipline, astronomical object, chemical compound, organization, academic journal, award, university, chemical element, country, event, scientist and O.\nSentence: CCR1 was the first CC chemokine receptor identified and binds multiple inflammatory / inducible ( see inducible gene ) CC chemokines ( including CCL4 , CCL5 , CCL6 , CCL14 , CCL15 , CCL16 and CCL23 ) .", "prompt_labels": "CCR1(B-protein) was(O) the(O) first(O) CC(B-protein) chemokine(I-protein) receptor(I-protein) identified(O) and(O) binds(O) multiple(O) inflammatory(O) /(O) inducible(O) ((O) see(O) inducible(O) gene(O) )(O) CC(B-protein) chemokines(I-protein) ((O) including(O) CCL4(B-protein) ,(O) CCL5(B-protein) ,(O) CCL6(B-protein) ,(O) CCL14(B-protein) ,(O) CCL15(B-protein) ,(O) CCL16(B-protein) and(O) CCL23(B-protein) )(O) .(O)"}}
{"id": "90", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "academic journal", "university", "country", "award", "scientist", "enzyme", "protein", "location", "astronomical object", "discipline", "event", "person", "chemical compound", "chemical element", "organization"], "instance": {"id": "90", "words": ["He", "was", "a", "fellow", "of", "the", "Society", "of", "Experimental", "Test", "Pilots", "(", "SETP", ")", "and", "the", "American", "Astronautical", "Society", ",", "as", "well", "as", "an", "associate", "fellow", "of", "the", "American", "Institute", "of", "Aeronautics", "and", "Astronautics", "."], "labels": ["O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "B-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, academic journal, university, country, award, scientist, enzyme, protein, location, astronomical object, discipline, event, person, chemical compound, chemical element, organization and O.\nSentence: He was a fellow of the Society of Experimental Test Pilots ( SETP ) and the American Astronautical Society , as well as an associate fellow of the American Institute of Aeronautics and Astronautics .", "prompt_labels": "He(O) was(O) a(O) fellow(B-award) of(I-award) the(I-award) Society(I-award) of(I-award) Experimental(I-award) Test(I-award) Pilots(I-award) ((O) SETP(B-organization) )(O) and(O) the(O) American(B-organization) Astronautical(I-organization) Society(I-organization) ,(O) as(O) well(O) as(O) an(O) associate(O) fellow(B-award) of(I-award) the(I-award) American(I-award) Institute(I-award) of(I-award) Aeronautics(I-award) and(I-award) Astronautics(I-award) .(O)"}}
{"id": "235", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "award", "discipline", "country", "enzyme", "event", "theory", "academic journal", "chemical element", "chemical compound", "organization", "university", "scientist", "protein", "person", "astronomical object"], "instance": {"id": "235", "words": ["It", "was", "nominated", "for", "the", "2002", "Hugo", "Award", "for", "Best", "Novel", "and", "tied", "for", "the", "2002", "John", "W.", "Campbell", "Memorial", "Award", "for", "Best", "Science", "Fiction", "Novel", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, discipline, country, enzyme, event, theory, academic journal, chemical element, chemical compound, organization, university, scientist, protein, person, astronomical object and O.\nSentence: It was nominated for the 2002 Hugo Award for Best Novel and tied for the 2002 John W. Campbell Memorial Award for Best Science Fiction Novel .", "prompt_labels": "It(O) was(O) nominated(O) for(O) the(O) 2002(O) Hugo(B-award) Award(I-award) for(I-award) Best(I-award) Novel(I-award) and(O) tied(O) for(O) the(O) 2002(O) John(B-award) W.(I-award) Campbell(I-award) Memorial(I-award) Award(I-award) for(I-award) Best(I-award) Science(I-award) Fiction(I-award) Novel(I-award) .(O)"}}
{"id": "410", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "location", "award", "astronomical object", "organization", "country", "chemical element", "person", "theory", "discipline", "event", "protein", "chemical compound", "university", "scientist", "enzyme"], "instance": {"id": "410", "words": ["Morley", "was", "the", "president", "of", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", "in", "1895", "and", "he", "was", "the", "president", "of", "the", "American", "Chemical", "Society", "in", "1899", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, location, award, astronomical object, organization, country, chemical element, person, theory, discipline, event, protein, chemical compound, university, scientist, enzyme and O.\nSentence: Morley was the president of the American Association for the Advancement of Science in 1895 and he was the president of the American Chemical Society in 1899 .", "prompt_labels": "Morley(B-scientist) was(O) the(O) president(O) of(O) the(O) American(B-organization) Association(I-organization) for(I-organization) the(I-organization) Advancement(I-organization) of(I-organization) Science(I-organization) in(O) 1895(O) and(O) he(O) was(O) the(O) president(O) of(O) the(O) American(B-organization) Chemical(I-organization) Society(I-organization) in(O) 1899(O) .(O)"}}
{"id": "438", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "protein", "location", "event", "scientist", "discipline", "chemical element", "theory", "person", "chemical compound", "organization", "country", "astronomical object", "award", "enzyme", "university"], "instance": {"id": "438", "words": ["She", "won", "the", "silver", "medal", "at", "the", "2018", "European", "Championships", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, protein, location, event, scientist, discipline, chemical element, theory, person, chemical compound, organization, country, astronomical object, award, enzyme, university and O.\nSentence: She won the silver medal at the 2018 European Championships .", "prompt_labels": "She(O) won(O) the(O) silver(O) medal(O) at(O) the(O) 2018(B-event) European(I-event) Championships(I-event) .(O)"}}
{"id": "287", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "enzyme", "event", "discipline", "chemical compound", "academic journal", "university", "organization", "theory", "award", "protein", "country", "location", "scientist", "person", "astronomical object"], "instance": {"id": "287", "words": ["In", "the", "United", "States", ",", "forensic", "pathologists", "typically", "complete", "at", "least", "one", "year", "of", "additional", "training", "(", "a", "fellowship", ")", "after", "completing", "an", "anatomical", "pathology", "residency", "and", "having", "passed", "the", "board", "examination", "administered", "by", "The", "American", "Board", "of", "Pathology", "or", "The", "American", "Osteopathic", "Board", "of", "Pathology", "(", "board-certified", ")", "."], "labels": ["O", "O", "B-country", "I-country", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, enzyme, event, discipline, chemical compound, academic journal, university, organization, theory, award, protein, country, location, scientist, person, astronomical object and O.\nSentence: In the United States , forensic pathologists typically complete at least one year of additional training ( a fellowship ) after completing an anatomical pathology residency and having passed the board examination administered by The American Board of Pathology or The American Osteopathic Board of Pathology ( board-certified ) .", "prompt_labels": "In(O) the(O) United(B-country) States(I-country) ,(O) forensic(O) pathologists(O) typically(O) complete(O) at(O) least(O) one(O) year(O) of(O) additional(O) training(O) ((O) a(O) fellowship(O) )(O) after(O) completing(O) an(O) anatomical(O) pathology(O) residency(O) and(O) having(O) passed(O) the(O) board(O) examination(O) administered(O) by(O) The(O) American(B-organization) Board(I-organization) of(I-organization) Pathology(I-organization) or(O) The(O) American(B-organization) Osteopathic(I-organization) Board(I-organization) of(I-organization) Pathology(I-organization) ((O) board-certified(O) )(O) .(O)"}}
{"id": "126", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "person", "university", "event", "location", "organization", "discipline", "country", "scientist", "chemical compound", "academic journal", "protein", "award", "astronomical object", "theory", "enzyme"], "instance": {"id": "126", "words": ["The", "maximum", "and", "minimum", "brightness", "of", "Jupiter", "differ", "by", "only", "a", "factor", "of", "3.3", "times", ",", "whilst", "those", "of", "Uranus", ";", "which", "is", "the", "most", "distant", "Solar", "System", "body", "visible", "to", "the", "naked", "eye", ";", "differ", "by", "a", "factor", "of", "1.7", "times", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, person, university, event, location, organization, discipline, country, scientist, chemical compound, academic journal, protein, award, astronomical object, theory, enzyme and O.\nSentence: The maximum and minimum brightness of Jupiter differ by only a factor of 3.3 times , whilst those of Uranus ; which is the most distant Solar System body visible to the naked eye ; differ by a factor of 1.7 times .", "prompt_labels": "The(O) maximum(O) and(O) minimum(O) brightness(O) of(O) Jupiter(B-astronomical object) differ(O) by(O) only(O) a(O) factor(O) of(O) 3.3(O) times(O) ,(O) whilst(O) those(O) of(O) Uranus(B-astronomical object) ;(O) which(O) is(O) the(O) most(O) distant(O) Solar(O) System(O) body(O) visible(O) to(O) the(O) naked(O) eye(O) ;(O) differ(O) by(O) a(O) factor(O) of(O) 1.7(O) times(O) .(O)"}}
{"id": "69", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "award", "scientist", "chemical compound", "country", "academic journal", "location", "university", "person", "astronomical object", "chemical element", "organization", "enzyme", "protein", "event", "theory"], "instance": {"id": "69", "words": ["For", "some", "articles", "I", "am", "interested", "in", "I", "did", "some", "images", ":", "actin", ",", "Arp2", "/", "3", "complex", ",", "MreB", ",", "Profilin", ",", "Zinc", "finger", "(", "Protein", "+", "DNA", ")"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "B-protein", "I-protein", "I-protein", "I-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "I-protein", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, award, scientist, chemical compound, country, academic journal, location, university, person, astronomical object, chemical element, organization, enzyme, protein, event, theory and O.\nSentence: For some articles I am interested in I did some images : actin , Arp2 / 3 complex , MreB , Profilin , Zinc finger ( Protein + DNA )", "prompt_labels": "For(O) some(O) articles(O) I(O) am(O) interested(O) in(O) I(O) did(O) some(O) images(O) :(O) actin(B-protein) ,(O) Arp2(B-protein) /(I-protein) 3(I-protein) complex(I-protein) ,(O) MreB(B-protein) ,(O) Profilin(B-protein) ,(O) Zinc(B-protein) finger(I-protein) ((O) Protein(O) +(O) DNA(O) )(O)"}}
{"id": "313", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "organization", "event", "chemical compound", "scientist", "chemical element", "theory", "person", "location", "discipline", "country", "academic journal", "protein", "award", "university", "enzyme"], "instance": {"id": "313", "words": ["Wang", "has", "published", "over", "300", "articles", ",", "which", "have", "been", "featured", "in", "publications", "such", "as", "Nature", "Magazine", ",", "Science", ",", "Physical", "Review", "Letters", ",", "Angewandte", "Chemie", ",", "and", "the", "Journal", "of", "the", "American", "Chemical", "Society", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, organization, event, chemical compound, scientist, chemical element, theory, person, location, discipline, country, academic journal, protein, award, university, enzyme and O.\nSentence: Wang has published over 300 articles , which have been featured in publications such as Nature Magazine , Science , Physical Review Letters , Angewandte Chemie , and the Journal of the American Chemical Society .", "prompt_labels": "Wang(B-scientist) has(O) published(O) over(O) 300(O) articles(O) ,(O) which(O) have(O) been(O) featured(O) in(O) publications(O) such(O) as(O) Nature(B-academic journal) Magazine(I-academic journal) ,(O) Science(B-academic journal) ,(O) Physical(B-academic journal) Review(I-academic journal) Letters(I-academic journal) ,(O) Angewandte(B-academic journal) Chemie(I-academic journal) ,(O) and(O) the(O) Journal(B-academic journal) of(I-academic journal) the(I-academic journal) American(I-academic journal) Chemical(I-academic journal) Society(I-academic journal) .(O)"}}
{"id": "36", "dataset": "crossner_science", "split": "dev", "label_list": ["event", "astronomical object", "person", "academic journal", "enzyme", "scientist", "discipline", "chemical compound", "location", "university", "chemical element", "award", "organization", "protein", "theory", "country"], "instance": {"id": "36", "words": ["Lincoln", "looks", "much", "as", "it", "did", "during", "the", "Lincoln", "County", "War", "(", "1878-1881", ")", "when", "its", "single", "street", "was", "peopled", "with", "characters", "like", "Billy", "the", "Kid", ",", "John", "Chisum", "and", "Lawrence", "Murphy", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, astronomical object, person, academic journal, enzyme, scientist, discipline, chemical compound, location, university, chemical element, award, organization, protein, theory, country and O.\nSentence: Lincoln looks much as it did during the Lincoln County War ( 1878-1881 ) when its single street was peopled with characters like Billy the Kid , John Chisum and Lawrence Murphy .", "prompt_labels": "Lincoln(B-person) looks(O) much(O) as(O) it(O) did(O) during(O) the(O) Lincoln(B-event) County(I-event) War(I-event) ((O) 1878-1881(O) )(O) when(O) its(O) single(O) street(O) was(O) peopled(O) with(O) characters(O) like(O) Billy(B-person) the(I-person) Kid(I-person) ,(O) John(B-person) Chisum(I-person) and(O) Lawrence(B-person) Murphy(I-person) .(O)"}}
{"id": "307", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "chemical element", "location", "event", "academic journal", "astronomical object", "scientist", "person", "university", "organization", "chemical compound", "discipline", "theory", "award", "country", "protein"], "instance": {"id": "307", "words": ["In", "the", "2010", "Bandy", "World", "Championship", ",", "he", "scored", "the", "golden", "goal", "in", "the", "final", ",", "giving", "the", "game", "to", "Sweden", "with", "6-5", "against", "Russia", "."], "labels": ["O", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical element, location, event, academic journal, astronomical object, scientist, person, university, organization, chemical compound, discipline, theory, award, country, protein and O.\nSentence: In the 2010 Bandy World Championship , he scored the golden goal in the final , giving the game to Sweden with 6-5 against Russia .", "prompt_labels": "In(O) the(O) 2010(B-event) Bandy(I-event) World(I-event) Championship(I-event) ,(O) he(O) scored(O) the(O) golden(O) goal(O) in(O) the(O) final(O) ,(O) giving(O) the(O) game(O) to(O) Sweden(B-country) with(O) 6-5(O) against(O) Russia(B-country) .(O)"}}
{"id": "404", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "protein", "astronomical object", "academic journal", "university", "person", "scientist", "event", "chemical compound", "enzyme", "organization", "award", "country", "chemical element", "discipline", "location"], "instance": {"id": "404", "words": ["The", "Australian", "leg", "began", "on", "February", "28", ",", "2013", ",", "in", "Perth", "at", "the", "Perth", "Arena", "and", "ran", "through", "March", "16", "in", "Mackay", "at", "Stadium", "Mackay", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "O", "B-location", "O", "B-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, protein, astronomical object, academic journal, university, person, scientist, event, chemical compound, enzyme, organization, award, country, chemical element, discipline, location and O.\nSentence: The Australian leg began on February 28 , 2013 , in Perth at the Perth Arena and ran through March 16 in Mackay at Stadium Mackay .", "prompt_labels": "The(O) Australian(O) leg(O) began(O) on(O) February(O) 28(O) ,(O) 2013(O) ,(O) in(O) Perth(B-location) at(O) the(O) Perth(B-location) Arena(I-location) and(O) ran(O) through(O) March(O) 16(O) in(O) Mackay(B-location) at(O) Stadium(B-location) Mackay(I-location) .(O)"}}
{"id": "94", "dataset": "crossner_science", "split": "dev", "label_list": ["country", "theory", "organization", "university", "protein", "chemical compound", "scientist", "person", "academic journal", "astronomical object", "event", "chemical element", "enzyme", "discipline", "award", "location"], "instance": {"id": "94", "words": ["Some", "ligninolytic", "enzymes", "include", "Haem", "peroxidase", "such", "as", "lignin", "peroxidase", "s", ",", "manganese", "peroxidase", "s", ",", "versatile", "peroxidase", "s", ",", "and", "Dye", "decolorizing", "peroxidase", "as", "well", "as", "copper-based", "laccase", "s", "."], "labels": ["O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "O", "B-enzyme", "I-enzyme", "I-enzyme", "O", "O", "O", "O", "B-enzyme", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, theory, organization, university, protein, chemical compound, scientist, person, academic journal, astronomical object, event, chemical element, enzyme, discipline, award, location and O.\nSentence: Some ligninolytic enzymes include Haem peroxidase such as lignin peroxidase s , manganese peroxidase s , versatile peroxidase s , and Dye decolorizing peroxidase as well as copper-based laccase s .", "prompt_labels": "Some(O) ligninolytic(O) enzymes(O) include(O) Haem(B-enzyme) peroxidase(I-enzyme) such(O) as(O) lignin(B-enzyme) peroxidase(I-enzyme) s(O) ,(O) manganese(B-enzyme) peroxidase(I-enzyme) s(O) ,(O) versatile(B-enzyme) peroxidase(I-enzyme) s(O) ,(O) and(O) Dye(B-enzyme) decolorizing(I-enzyme) peroxidase(I-enzyme) as(O) well(O) as(O) copper-based(O) laccase(B-enzyme) s(O) .(O)"}}
{"id": "367", "dataset": "crossner_science", "split": "dev", "label_list": ["award", "discipline", "academic journal", "astronomical object", "scientist", "enzyme", "location", "person", "chemical element", "event", "university", "organization", "country", "theory", "chemical compound", "protein"], "instance": {"id": "367", "words": ["This", "led", "also", "to", "group", "'s", "various", "theoretical", "efforts", ",", "such", "as", "development", "of", "electric", "field-driven", "torque", "models", "of", "the", "mitochondrial", "motor", "ATP", "synthase", "and", "efforts", "to", "understand", "mechanisms", "of", "disease-implicated", "mitochondrial", "mutations", "in", "the", "Electron", "transport", "chain", ",", "E.", "Prodan", ",", "C.", "Prodan", ",", "and", "J.", "H.", "Miller", ",", "Jr", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, discipline, academic journal, astronomical object, scientist, enzyme, location, person, chemical element, event, university, organization, country, theory, chemical compound, protein and O.\nSentence: This led also to group 's various theoretical efforts , such as development of electric field-driven torque models of the mitochondrial motor ATP synthase and efforts to understand mechanisms of disease-implicated mitochondrial mutations in the Electron transport chain , E. Prodan , C. Prodan , and J. H. Miller , Jr .", "prompt_labels": "This(O) led(O) also(O) to(O) group(O) 's(O) various(O) theoretical(O) efforts(O) ,(O) such(O) as(O) development(O) of(O) electric(O) field-driven(O) torque(O) models(O) of(O) the(O) mitochondrial(O) motor(O) ATP(B-enzyme) synthase(I-enzyme) and(O) efforts(O) to(O) understand(O) mechanisms(O) of(O) disease-implicated(O) mitochondrial(O) mutations(O) in(O) the(O) Electron(O) transport(O) chain(O) ,(O) E.(B-scientist) Prodan(I-scientist) ,(O) C.(B-scientist) Prodan(I-scientist) ,(O) and(O) J.(B-scientist) H.(I-scientist) Miller(I-scientist) ,(I-scientist) Jr(I-scientist) .(O)"}}
{"id": "395", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "scientist", "award", "university", "event", "chemical element", "chemical compound", "person", "astronomical object", "organization", "location", "enzyme", "protein", "theory", "country", "discipline"], "instance": {"id": "395", "words": ["The", "discoveries", "of", "a", "second", "non-transiting", "planet", "in", "the", "same", "system", ",", "CoRoT-7c", ",", "and", "of", "a", "new", "Hot", "Jupiter", ",", "CoRoT-6b", ",", "were", "also", "announced", "at", "the", "Symposium", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, scientist, award, university, event, chemical element, chemical compound, person, astronomical object, organization, location, enzyme, protein, theory, country, discipline and O.\nSentence: The discoveries of a second non-transiting planet in the same system , CoRoT-7c , and of a new Hot Jupiter , CoRoT-6b , were also announced at the Symposium .", "prompt_labels": "The(O) discoveries(O) of(O) a(O) second(O) non-transiting(O) planet(O) in(O) the(O) same(O) system(O) ,(O) CoRoT-7c(B-astronomical object) ,(O) and(O) of(O) a(O) new(O) Hot(B-astronomical object) Jupiter(I-astronomical object) ,(O) CoRoT-6b(B-astronomical object) ,(O) were(O) also(O) announced(O) at(O) the(O) Symposium(O) .(O)"}}
{"id": "312", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "chemical element", "university", "scientist", "protein", "person", "chemical compound", "enzyme", "event", "astronomical object", "award", "location", "organization", "discipline", "country", "academic journal"], "instance": {"id": "312", "words": ["Some", "asteroid", "s", ",", "also", "named", "after", "the", "same", "Shakespearean", "characters", ",", "share", "names", "with", "moons", "of", "Uranus", ":", "171", "Ophelia", ",", "218", "Bianca", ",", "593", "Titania", ",", "666", "Desdemona", ",", "763", "Cupido", ",", "and", "2758", "Cordelia", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "B-astronomical object", "I-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, chemical element, university, scientist, protein, person, chemical compound, enzyme, event, astronomical object, award, location, organization, discipline, country, academic journal and O.\nSentence: Some asteroid s , also named after the same Shakespearean characters , share names with moons of Uranus : 171 Ophelia , 218 Bianca , 593 Titania , 666 Desdemona , 763 Cupido , and 2758 Cordelia .", "prompt_labels": "Some(O) asteroid(O) s(O) ,(O) also(O) named(O) after(O) the(O) same(O) Shakespearean(B-person) characters(O) ,(O) share(O) names(O) with(O) moons(O) of(O) Uranus(B-astronomical object) :(O) 171(B-astronomical object) Ophelia(I-astronomical object) ,(O) 218(B-astronomical object) Bianca(I-astronomical object) ,(O) 593(B-astronomical object) Titania(I-astronomical object) ,(O) 666(B-astronomical object) Desdemona(I-astronomical object) ,(O) 763(B-astronomical object) Cupido(I-astronomical object) ,(O) and(O) 2758(B-astronomical object) Cordelia(I-astronomical object) .(O)"}}
{"id": "5", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "astronomical object", "event", "person", "discipline", "country", "scientist", "theory", "chemical compound", "academic journal", "award", "organization", "university", "location", "enzyme", "protein"], "instance": {"id": "5", "words": ["In", "addition", ",", "there", "would", "probably", "have", "been", "simple", "hydride", "s", "such", "as", "those", "now", "found", "in", "gas", "giants", "like", "Jupiter", "and", "Saturn", ",", "notably", "water", "vapor", ",", "methane", ",", "and", "ammonia", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O", "O", "B-chemical compound", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, astronomical object, event, person, discipline, country, scientist, theory, chemical compound, academic journal, award, organization, university, location, enzyme, protein and O.\nSentence: In addition , there would probably have been simple hydride s such as those now found in gas giants like Jupiter and Saturn , notably water vapor , methane , and ammonia .", "prompt_labels": "In(O) addition(O) ,(O) there(O) would(O) probably(O) have(O) been(O) simple(O) hydride(B-chemical compound) s(O) such(O) as(O) those(O) now(O) found(O) in(O) gas(O) giants(O) like(O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) ,(O) notably(O) water(B-chemical compound) vapor(I-chemical compound) ,(O) methane(B-chemical compound) ,(O) and(O) ammonia(B-chemical compound) .(O)"}}
{"id": "168", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "astronomical object", "enzyme", "protein", "chemical element", "academic journal", "location", "event", "award", "person", "scientist", "organization", "university", "chemical compound", "discipline", "country"], "instance": {"id": "168", "words": ["A", "physician", "and", "professor", "of", "public", "health", ",", "he", "worked", "first", "in", "social", "medicine", "at", "the", "University", "of", "Sassari", "(", "1969-1974", ")", "and", "then", "in", "occupational", "health", "at", "the", "Sapienza", "University", "of", "Rome", "(", "1975-1999", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "I-discipline", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, astronomical object, enzyme, protein, chemical element, academic journal, location, event, award, person, scientist, organization, university, chemical compound, discipline, country and O.\nSentence: A physician and professor of public health , he worked first in social medicine at the University of Sassari ( 1969-1974 ) and then in occupational health at the Sapienza University of Rome ( 1975-1999 ) .", "prompt_labels": "A(O) physician(O) and(O) professor(O) of(O) public(O) health(O) ,(O) he(O) worked(O) first(O) in(O) social(B-discipline) medicine(I-discipline) at(O) the(O) University(B-university) of(I-university) Sassari(I-university) ((O) 1969-1974(O) )(O) and(O) then(O) in(O) occupational(O) health(O) at(O) the(O) Sapienza(B-university) University(I-university) of(I-university) Rome(I-university) ((O) 1975-1999(O) )(O) .(O)"}}
{"id": "389", "dataset": "crossner_science", "split": "dev", "label_list": ["organization", "chemical element", "award", "chemical compound", "enzyme", "event", "theory", "astronomical object", "discipline", "person", "protein", "country", "academic journal", "university", "location", "scientist"], "instance": {"id": "389", "words": ["The", "International", "Astronomical", "Union", "named", "6216", "San", "Jose", "to", "honor", "the", "city", "'s", "efforts", "toward", "reducing", "light", "pollution", "."], "labels": ["O", "B-organization", "I-organization", "I-organization", "O", "B-astronomical object", "I-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, chemical element, award, chemical compound, enzyme, event, theory, astronomical object, discipline, person, protein, country, academic journal, university, location, scientist and O.\nSentence: The International Astronomical Union named 6216 San Jose to honor the city 's efforts toward reducing light pollution .", "prompt_labels": "The(O) International(B-organization) Astronomical(I-organization) Union(I-organization) named(O) 6216(B-astronomical object) San(I-astronomical object) Jose(I-astronomical object) to(O) honor(O) the(O) city(O) 's(O) efforts(O) toward(O) reducing(O) light(O) pollution(O) .(O)"}}
{"id": "348", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "astronomical object", "location", "organization", "enzyme", "academic journal", "award", "scientist", "country", "university", "chemical compound", "discipline", "person", "event", "protein", "chemical element"], "instance": {"id": "348", "words": ["The", "northwestern", "through", "northeastern", "parts", "of", "the", "metropolitan", "area", "are", "served", "by", "various", "campuses", "of", "the", "Lone", "Star", "College", "System", ",", "while", "the", "southeastern", "portion", "of", "the", "city", "and", "some", "surrounding", "areas", "are", "served", "by", "San", "Jacinto", "College", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, astronomical object, location, organization, enzyme, academic journal, award, scientist, country, university, chemical compound, discipline, person, event, protein, chemical element and O.\nSentence: The northwestern through northeastern parts of the metropolitan area are served by various campuses of the Lone Star College System , while the southeastern portion of the city and some surrounding areas are served by San Jacinto College .", "prompt_labels": "The(O) northwestern(O) through(O) northeastern(O) parts(O) of(O) the(O) metropolitan(O) area(O) are(O) served(O) by(O) various(O) campuses(O) of(O) the(O) Lone(B-university) Star(I-university) College(I-university) System(I-university) ,(O) while(O) the(O) southeastern(O) portion(O) of(O) the(O) city(O) and(O) some(O) surrounding(O) areas(O) are(O) served(O) by(O) San(B-university) Jacinto(I-university) College(I-university) .(O)"}}
{"id": "87", "dataset": "crossner_science", "split": "dev", "label_list": ["event", "protein", "award", "chemical element", "university", "discipline", "scientist", "theory", "academic journal", "enzyme", "chemical compound", "location", "person", "country", "astronomical object", "organization"], "instance": {"id": "87", "words": ["He", "is", "known", "for", "his", "studies", "on", "the", "Pore-forming", "toxin", "and", "T-cell", "costimulatory", "molecules", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, protein, award, chemical element, university, discipline, scientist, theory, academic journal, enzyme, chemical compound, location, person, country, astronomical object, organization and O.\nSentence: He is known for his studies on the Pore-forming toxin and T-cell costimulatory molecules .", "prompt_labels": "He(O) is(O) known(O) for(O) his(O) studies(O) on(O) the(O) Pore-forming(B-protein) toxin(I-protein) and(O) T-cell(O) costimulatory(O) molecules(O) .(O)"}}
{"id": "63", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "chemical compound", "enzyme", "astronomical object", "award", "scientist", "event", "university", "discipline", "chemical element", "country", "organization", "theory", "location", "academic journal", "person"], "instance": {"id": "63", "words": ["On", "the", "same", "night", ",", "the", "trio", "of", "astronomers", "also", "discovered", "the", "minor", "planets", "1912", "Anubis", ",", "1923", "Osiris", "and", "1924", "Horus", ",", "which", "were", "also", "named", "after", "Ancient", "Egyptian", "deities", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, chemical compound, enzyme, astronomical object, award, scientist, event, university, discipline, chemical element, country, organization, theory, location, academic journal, person and O.\nSentence: On the same night , the trio of astronomers also discovered the minor planets 1912 Anubis , 1923 Osiris and 1924 Horus , which were also named after Ancient Egyptian deities .", "prompt_labels": "On(O) the(O) same(O) night(O) ,(O) the(O) trio(O) of(O) astronomers(O) also(O) discovered(O) the(O) minor(O) planets(O) 1912(B-astronomical object) Anubis(I-astronomical object) ,(O) 1923(B-astronomical object) Osiris(I-astronomical object) and(O) 1924(B-astronomical object) Horus(I-astronomical object) ,(O) which(O) were(O) also(O) named(O) after(O) Ancient(O) Egyptian(O) deities(O) .(O)"}}
{"id": "396", "dataset": "crossner_science", "split": "dev", "label_list": ["scientist", "chemical compound", "astronomical object", "person", "chemical element", "award", "academic journal", "location", "enzyme", "discipline", "country", "organization", "university", "protein", "event", "theory"], "instance": {"id": "396", "words": ["There", "were", "probably", "simple", "hydrides", "such", "as", "those", "now", "found", "in", "the", "gas", "giant", "s", "(", "Jupiter", "and", "Saturn", ")", ",", "notably", "water", "vapor", ",", "methane", "and", "ammonia", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, chemical compound, astronomical object, person, chemical element, award, academic journal, location, enzyme, discipline, country, organization, university, protein, event, theory and O.\nSentence: There were probably simple hydrides such as those now found in the gas giant s ( Jupiter and Saturn ) , notably water vapor , methane and ammonia .", "prompt_labels": "There(O) were(O) probably(O) simple(O) hydrides(O) such(O) as(O) those(O) now(O) found(O) in(O) the(O) gas(B-astronomical object) giant(I-astronomical object) s(O) ((O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) )(O) ,(O) notably(O) water(B-chemical compound) vapor(I-chemical compound) ,(O) methane(B-chemical compound) and(O) ammonia(B-chemical compound) .(O)"}}
{"id": "315", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical compound", "award", "academic journal", "event", "astronomical object", "chemical element", "scientist", "discipline", "country", "person", "organization", "location", "protein", "university", "theory", "enzyme"], "instance": {"id": "315", "words": ["Various", "organizations", "gave", "evidence", "to", "the", "Scottish", "Executive", "during", "the", "consultation", "process", ",", "including", "the", "Royal", "College", "of", "Surgeons", "of", "Edinburgh", ",", "the", "Wellcome", "Trust", ",", "and", "the", "Museums", "Association", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, award, academic journal, event, astronomical object, chemical element, scientist, discipline, country, person, organization, location, protein, university, theory, enzyme and O.\nSentence: Various organizations gave evidence to the Scottish Executive during the consultation process , including the Royal College of Surgeons of Edinburgh , the Wellcome Trust , and the Museums Association .", "prompt_labels": "Various(O) organizations(O) gave(O) evidence(O) to(O) the(O) Scottish(B-person) Executive(I-person) during(O) the(O) consultation(O) process(O) ,(O) including(O) the(O) Royal(B-organization) College(I-organization) of(I-organization) Surgeons(I-organization) of(I-organization) Edinburgh(I-organization) ,(O) the(O) Wellcome(B-organization) Trust(I-organization) ,(O) and(O) the(O) Museums(B-organization) Association(I-organization) .(O)"}}
{"id": "61", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "theory", "chemical element", "astronomical object", "event", "country", "location", "university", "academic journal", "scientist", "organization", "chemical compound", "person", "discipline", "award", "enzyme"], "instance": {"id": "61", "words": ["Robot", "designer", "Hans", "Moravec", ",", "cyberneticist", "Kevin", "Warwick", "and", "inventor", "Ray", "Kurzweil", "have", "predicted", "that", "humans", "and", "machines", "will", "merge", "in", "the", "future", "into", "cyborg", "s", "that", "are", "more", "capable", "and", "powerful", "than", "either", "."], "labels": ["O", "O", "B-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, theory, chemical element, astronomical object, event, country, location, university, academic journal, scientist, organization, chemical compound, person, discipline, award, enzyme and O.\nSentence: Robot designer Hans Moravec , cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines will merge in the future into cyborg s that are more capable and powerful than either .", "prompt_labels": "Robot(O) designer(O) Hans(B-scientist) Moravec(I-scientist) ,(O) cyberneticist(O) Kevin(B-scientist) Warwick(I-scientist) and(O) inventor(O) Ray(B-person) Kurzweil(I-person) have(O) predicted(O) that(O) humans(O) and(O) machines(O) will(O) merge(O) in(O) the(O) future(O) into(O) cyborg(O) s(O) that(O) are(O) more(O) capable(O) and(O) powerful(O) than(O) either(O) .(O)"}}
{"id": "39", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical compound", "enzyme", "protein", "university", "location", "award", "event", "scientist", "theory", "academic journal", "astronomical object", "country", "organization", "chemical element", "discipline", "person"], "instance": {"id": "39", "words": ["The", "team", "that", "she", "manages", "has", "specially", "studied", "the", "role", "of", "Proinsulin", "/", "insulin", "in", "the", "development", "of", "the", "central", "nervous", "system", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, enzyme, protein, university, location, award, event, scientist, theory, academic journal, astronomical object, country, organization, chemical element, discipline, person and O.\nSentence: The team that she manages has specially studied the role of Proinsulin / insulin in the development of the central nervous system .", "prompt_labels": "The(O) team(O) that(O) she(O) manages(O) has(O) specially(O) studied(O) the(O) role(O) of(O) Proinsulin(B-protein) /(O) insulin(B-protein) in(O) the(O) development(O) of(O) the(O) central(O) nervous(O) system(O) .(O)"}}
{"id": "77", "dataset": "crossner_science", "split": "dev", "label_list": ["event", "discipline", "protein", "enzyme", "university", "chemical element", "location", "person", "academic journal", "astronomical object", "theory", "award", "organization", "chemical compound", "scientist", "country"], "instance": {"id": "77", "words": ["In", "the", "second", "half", "of", "the", "20th", "century", ",", "plate", "tectonics", "theory", "was", "developed", "by", "several", "contributors", "including", "Alfred", "Wegener", ",", "Maurice", "Ewing", ",", "Robert", "S.", "Dietz", ",", "Harry", "Hammond", "Hess", ",", "Hugo", "Benioff", ",", "Walter", "C.", "Pitman", ",", "III", ",", "Frederick", "Vine", ",", "Drummond", "Matthews", ",", "Keith", "Runcorn", ",", "Bryan", "L.", "Isacks", ",", "Edward", "Bullard", ",", "Xavier", "Le", "Pichon", ",", "Dan", "McKenzie", ",", "W.", "Jason", "Morgan", "and", "John", "Tuzo", "Wilson", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-theory", "I-theory", "I-theory", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, discipline, protein, enzyme, university, chemical element, location, person, academic journal, astronomical object, theory, award, organization, chemical compound, scientist, country and O.\nSentence: In the second half of the 20th century , plate tectonics theory was developed by several contributors including Alfred Wegener , Maurice Ewing , Robert S. Dietz , Harry Hammond Hess , Hugo Benioff , Walter C. Pitman , III , Frederick Vine , Drummond Matthews , Keith Runcorn , Bryan L. Isacks , Edward Bullard , Xavier Le Pichon , Dan McKenzie , W. Jason Morgan and John Tuzo Wilson .", "prompt_labels": "In(O) the(O) second(O) half(O) of(O) the(O) 20th(O) century(O) ,(O) plate(B-theory) tectonics(I-theory) theory(I-theory) was(O) developed(O) by(O) several(O) contributors(O) including(O) Alfred(B-scientist) Wegener(I-scientist) ,(O) Maurice(B-scientist) Ewing(I-scientist) ,(O) Robert(B-scientist) S.(I-scientist) Dietz(I-scientist) ,(O) Harry(B-scientist) Hammond(I-scientist) Hess(I-scientist) ,(O) Hugo(B-scientist) Benioff(I-scientist) ,(O) Walter(B-scientist) C.(I-scientist) Pitman(I-scientist) ,(I-scientist) III(I-scientist) ,(O) Frederick(B-scientist) Vine(I-scientist) ,(O) Drummond(B-scientist) Matthews(I-scientist) ,(O) Keith(B-scientist) Runcorn(I-scientist) ,(O) Bryan(B-scientist) L.(I-scientist) Isacks(I-scientist) ,(O) Edward(B-scientist) Bullard(I-scientist) ,(O) Xavier(B-scientist) Le(I-scientist) Pichon(I-scientist) ,(O) Dan(B-scientist) McKenzie(I-scientist) ,(O) W.(B-scientist) Jason(I-scientist) Morgan(I-scientist) and(O) John(B-scientist) Tuzo(I-scientist) Wilson(I-scientist) .(O)"}}
{"id": "96", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "theory", "chemical element", "astronomical object", "academic journal", "award", "location", "discipline", "event", "chemical compound", "person", "organization", "university", "country", "protein", "scientist"], "instance": {"id": "96", "words": ["Gas", "giants", "with", "a", "large", "radius", "and", "very", "low", "density", "are", "sometimes", "called", "puffy", "planets", "COROT-1b", ",", "TrES-4", ",", "WASP-12b", ",", "WASP-17b", ",", "and", "Kepler-7b", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, theory, chemical element, astronomical object, academic journal, award, location, discipline, event, chemical compound, person, organization, university, country, protein, scientist and O.\nSentence: Gas giants with a large radius and very low density are sometimes called puffy planets COROT-1b , TrES-4 , WASP-12b , WASP-17b , and Kepler-7b .", "prompt_labels": "Gas(O) giants(O) with(O) a(O) large(O) radius(O) and(O) very(O) low(O) density(O) are(O) sometimes(O) called(O) puffy(O) planets(O) COROT-1b(B-astronomical object) ,(O) TrES-4(B-astronomical object) ,(O) WASP-12b(B-astronomical object) ,(O) WASP-17b(B-astronomical object) ,(O) and(O) Kepler-7b(B-astronomical object) .(O)"}}
{"id": "327", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "person", "academic journal", "protein", "discipline", "country", "enzyme", "organization", "university", "event", "scientist", "location", "astronomical object", "theory", "award", "chemical compound"], "instance": {"id": "327", "words": ["Kandler", "was", "the", "founder", "and", "editor", "of", "Systematic", "and", "Applied", "Microbiology", ",", "co-editor", "of", "the", "Archives", "of", "Microbiology", "and", "of", "Zeitschrift", "für", "Pflanzenphysiologie", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, person, academic journal, protein, discipline, country, enzyme, organization, university, event, scientist, location, astronomical object, theory, award, chemical compound and O.\nSentence: Kandler was the founder and editor of Systematic and Applied Microbiology , co-editor of the Archives of Microbiology and of Zeitschrift für Pflanzenphysiologie .", "prompt_labels": "Kandler(B-scientist) was(O) the(O) founder(O) and(O) editor(O) of(O) Systematic(B-academic journal) and(I-academic journal) Applied(I-academic journal) Microbiology(I-academic journal) ,(O) co-editor(O) of(O) the(O) Archives(B-academic journal) of(I-academic journal) Microbiology(I-academic journal) and(O) of(O) Zeitschrift(B-academic journal) für(I-academic journal) Pflanzenphysiologie(I-academic journal) .(O)"}}
{"id": "163", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "location", "event", "award", "chemical element", "university", "enzyme", "academic journal", "theory", "chemical compound", "astronomical object", "scientist", "country", "organization", "person", "protein"], "instance": {"id": "163", "words": ["Within", "the", "Solar", "System", "there", "are", "five", "candidates", "for", "Schumann", "resonance", "detection", "besides", "the", "Earth", ":", "Venus", ",", "Mars", ",", "Jupiter", ",", "Saturn", ",", "and", "Saturn", "'s", "biggest", "moon", "Titan", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, location, event, award, chemical element, university, enzyme, academic journal, theory, chemical compound, astronomical object, scientist, country, organization, person, protein and O.\nSentence: Within the Solar System there are five candidates for Schumann resonance detection besides the Earth : Venus , Mars , Jupiter , Saturn , and Saturn 's biggest moon Titan .", "prompt_labels": "Within(O) the(O) Solar(O) System(O) there(O) are(O) five(O) candidates(O) for(O) Schumann(O) resonance(O) detection(O) besides(O) the(O) Earth(B-astronomical object) :(O) Venus(B-astronomical object) ,(O) Mars(B-astronomical object) ,(O) Jupiter(B-astronomical object) ,(O) Saturn(B-astronomical object) ,(O) and(O) Saturn(B-astronomical object) 's(O) biggest(O) moon(O) Titan(B-astronomical object) .(O)"}}
{"id": "232", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "enzyme", "protein", "chemical element", "theory", "organization", "discipline", "academic journal", "university", "chemical compound", "person", "award", "location", "scientist", "country", "event"], "instance": {"id": "232", "words": ["Thermales-", "rpoB", "motif", "RNAs", "likely", "function", "as", "cis-regulatory", "element", "s", ",", "in", "view", "of", "their", "positions", "upstream", "of", "protein-coding", "gene", "s", ",", "which", "invariably", "encode", "subunits", "of", "RNA", "polymerase", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, enzyme, protein, chemical element, theory, organization, discipline, academic journal, university, chemical compound, person, award, location, scientist, country, event and O.\nSentence: Thermales- rpoB motif RNAs likely function as cis-regulatory element s , in view of their positions upstream of protein-coding gene s , which invariably encode subunits of RNA polymerase .", "prompt_labels": "Thermales-(O) rpoB(O) motif(O) RNAs(O) likely(O) function(O) as(O) cis-regulatory(O) element(O) s(O) ,(O) in(O) view(O) of(O) their(O) positions(O) upstream(O) of(O) protein-coding(O) gene(O) s(O) ,(O) which(O) invariably(O) encode(O) subunits(O) of(O) RNA(B-enzyme) polymerase(I-enzyme) .(O)"}}
{"id": "346", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "event", "protein", "chemical element", "scientist", "award", "chemical compound", "enzyme", "academic journal", "astronomical object", "organization", "person", "university", "location", "discipline", "country"], "instance": {"id": "346", "words": ["Analysis", "of", "the", "elastic", "deformation", "happening", "when", "a", "pollinator", "lands", "on", "the", "sheath-like", "perch", "part", "of", "the", "flower", "Strelitzia", "reginae", "(", "known", "as", "Bird-of-Paradise", "flower", ")", "has", "inspired", "architects", "and", "scientists", "from", "the", "University", "of", "Freiburg", "and", "University", "of", "Stuttgart", "to", "create", "hingeless", "shading", "systems", "that", "can", "react", "to", "their", "environment", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, event, protein, chemical element, scientist, award, chemical compound, enzyme, academic journal, astronomical object, organization, person, university, location, discipline, country and O.\nSentence: Analysis of the elastic deformation happening when a pollinator lands on the sheath-like perch part of the flower Strelitzia reginae ( known as Bird-of-Paradise flower ) has inspired architects and scientists from the University of Freiburg and University of Stuttgart to create hingeless shading systems that can react to their environment .", "prompt_labels": "Analysis(O) of(O) the(O) elastic(O) deformation(O) happening(O) when(O) a(O) pollinator(O) lands(O) on(O) the(O) sheath-like(O) perch(O) part(O) of(O) the(O) flower(O) Strelitzia(O) reginae(O) ((O) known(O) as(O) Bird-of-Paradise(O) flower(O) )(O) has(O) inspired(O) architects(O) and(O) scientists(O) from(O) the(O) University(B-university) of(I-university) Freiburg(I-university) and(O) University(B-university) of(I-university) Stuttgart(I-university) to(O) create(O) hingeless(O) shading(O) systems(O) that(O) can(O) react(O) to(O) their(O) environment(O) .(O)"}}
{"id": "322", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "academic journal", "astronomical object", "country", "theory", "location", "person", "enzyme", "scientist", "event", "protein", "university", "award", "chemical compound", "chemical element", "organization"], "instance": {"id": "322", "words": ["Camilla", "is", "the", "6th", "trinary", "asteroid", "that", "has", "been", "discovered", "in", "the", "asteroid", "belt", ",", "after", "87", "Sylvia", ",", "45", "Eugenia", ",", "216", "Kleopatra", ",", "93", "Minerva", "and", "130", "Elektra", "."], "labels": ["B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, academic journal, astronomical object, country, theory, location, person, enzyme, scientist, event, protein, university, award, chemical compound, chemical element, organization and O.\nSentence: Camilla is the 6th trinary asteroid that has been discovered in the asteroid belt , after 87 Sylvia , 45 Eugenia , 216 Kleopatra , 93 Minerva and 130 Elektra .", "prompt_labels": "Camilla(B-astronomical object) is(O) the(O) 6th(O) trinary(O) asteroid(O) that(O) has(O) been(O) discovered(O) in(O) the(O) asteroid(O) belt(O) ,(O) after(O) 87(B-astronomical object) Sylvia(I-astronomical object) ,(O) 45(B-astronomical object) Eugenia(I-astronomical object) ,(O) 216(B-astronomical object) Kleopatra(I-astronomical object) ,(O) 93(B-astronomical object) Minerva(I-astronomical object) and(O) 130(B-astronomical object) Elektra(I-astronomical object) .(O)"}}
{"id": "54", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "scientist", "protein", "country", "organization", "location", "discipline", "award", "academic journal", "person", "event", "enzyme", "university", "chemical compound", "astronomical object", "chemical element"], "instance": {"id": "54", "words": ["In", "August", "2017", "a", "group", "of", "scientists", "from", "Oregon", "published", "an", "article", "in", "Nature", "journal", "detailing", "the", "successful", "use", "of", "CRISPR", "to", "edit", "out", "a", "mutation", "responsible", "for", "congenital", "heart", "disease", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-location", "O", "O", "O", "O", "B-academic journal", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, scientist, protein, country, organization, location, discipline, award, academic journal, person, event, enzyme, university, chemical compound, astronomical object, chemical element and O.\nSentence: In August 2017 a group of scientists from Oregon published an article in Nature journal detailing the successful use of CRISPR to edit out a mutation responsible for congenital heart disease .", "prompt_labels": "In(O) August(O) 2017(O) a(O) group(O) of(O) scientists(O) from(O) Oregon(B-location) published(O) an(O) article(O) in(O) Nature(B-academic journal) journal(O) detailing(O) the(O) successful(O) use(O) of(O) CRISPR(O) to(O) edit(O) out(O) a(O) mutation(O) responsible(O) for(O) congenital(O) heart(O) disease(O) .(O)"}}
{"id": "158", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "university", "scientist", "country", "person", "enzyme", "academic journal", "award", "theory", "location", "chemical compound", "event", "organization", "discipline", "astronomical object", "protein"], "instance": {"id": "158", "words": ["In", "1966", "he", "won", "the", "silver", "medal", "of", "the", "Football", "at", "the", "1966", "Asian", "Games", ",", "in", "1974", "he", "captained", "the", "Iranian", "team", "to", "win", "the", "football", "tournament", "of", "the", "Football", "at", "the", "1974", "Asian", "Games", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, university, scientist, country, person, enzyme, academic journal, award, theory, location, chemical compound, event, organization, discipline, astronomical object, protein and O.\nSentence: In 1966 he won the silver medal of the Football at the 1966 Asian Games , in 1974 he captained the Iranian team to win the football tournament of the Football at the 1974 Asian Games .", "prompt_labels": "In(O) 1966(O) he(O) won(O) the(O) silver(O) medal(O) of(O) the(O) Football(O) at(O) the(O) 1966(B-event) Asian(I-event) Games(I-event) ,(O) in(O) 1974(O) he(O) captained(O) the(O) Iranian(O) team(O) to(O) win(O) the(O) football(O) tournament(O) of(O) the(O) Football(O) at(O) the(O) 1974(B-event) Asian(I-event) Games(I-event) .(O)"}}
{"id": "31", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "person", "university", "event", "enzyme", "astronomical object", "scientist", "academic journal", "chemical element", "organization", "award", "location", "country", "theory", "protein", "chemical compound"], "instance": {"id": "31", "words": ["Usually", ",", "in", "the", "presence", "of", "NADPH", "dehydrogenase", "or", "NADH", "dehydrogenase", "as", "the", "enzyme", ",", "NADPH", "or", "NADH", "is", "the", "reductant", "that", "converts", "resazurin", "to", "resorufin", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "B-enzyme", "I-enzyme", "O", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, person, university, event, enzyme, astronomical object, scientist, academic journal, chemical element, organization, award, location, country, theory, protein, chemical compound and O.\nSentence: Usually , in the presence of NADPH dehydrogenase or NADH dehydrogenase as the enzyme , NADPH or NADH is the reductant that converts resazurin to resorufin .", "prompt_labels": "Usually(O) ,(O) in(O) the(O) presence(O) of(O) NADPH(B-enzyme) dehydrogenase(I-enzyme) or(O) NADH(B-enzyme) dehydrogenase(I-enzyme) as(O) the(O) enzyme(O) ,(O) NADPH(B-chemical compound) or(O) NADH(B-chemical compound) is(O) the(O) reductant(O) that(O) converts(O) resazurin(O) to(O) resorufin(O) .(O)"}}
{"id": "408", "dataset": "crossner_science", "split": "dev", "label_list": ["scientist", "astronomical object", "organization", "enzyme", "theory", "protein", "location", "chemical compound", "person", "chemical element", "academic journal", "award", "country", "discipline", "university", "event"], "instance": {"id": "408", "words": ["In", "March", "2019", ",", "European", "Southern", "Observatory", "astronomers", ",", "employing", "the", "GRAVITY", "instrument", "on", "their", "Very", "Large", "Telescope", "Interferometer", "(", "VLTI", ")", ",", "announced", "the", "first", "direct", "detection", "of", "an", "exoplanet", ",", "HR", "8799", "e", ",", "using", "optical", "interferometry", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, astronomical object, organization, enzyme, theory, protein, location, chemical compound, person, chemical element, academic journal, award, country, discipline, university, event and O.\nSentence: In March 2019 , European Southern Observatory astronomers , employing the GRAVITY instrument on their Very Large Telescope Interferometer ( VLTI ) , announced the first direct detection of an exoplanet , HR 8799 e , using optical interferometry .", "prompt_labels": "In(O) March(O) 2019(O) ,(O) European(B-organization) Southern(I-organization) Observatory(I-organization) astronomers(O) ,(O) employing(O) the(O) GRAVITY(O) instrument(O) on(O) their(O) Very(O) Large(O) Telescope(O) Interferometer(O) ((O) VLTI(O) )(O) ,(O) announced(O) the(O) first(O) direct(O) detection(O) of(O) an(O) exoplanet(O) ,(O) HR(B-astronomical object) 8799(I-astronomical object) e(I-astronomical object) ,(O) using(O) optical(O) interferometry(O) .(O)"}}
{"id": "284", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical compound", "academic journal", "protein", "award", "person", "organization", "country", "chemical element", "scientist", "theory", "event", "university", "discipline", "astronomical object", "enzyme", "location"], "instance": {"id": "284", "words": ["He", "is", "a", "past", "Chair", "of", "the", "Division", "for", "Planetary", "Sciences", "(", "DPS", ")", "of", "the", "American", "Astronomical", "Society", "and", "was", "the", "first", "editor", "of", "the", "Journal", "of", "Geophysical", "Research", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, academic journal, protein, award, person, organization, country, chemical element, scientist, theory, event, university, discipline, astronomical object, enzyme, location and O.\nSentence: He is a past Chair of the Division for Planetary Sciences ( DPS ) of the American Astronomical Society and was the first editor of the Journal of Geophysical Research .", "prompt_labels": "He(O) is(O) a(O) past(O) Chair(O) of(O) the(O) Division(B-organization) for(I-organization) Planetary(I-organization) Sciences(I-organization) ((O) DPS(B-organization) )(O) of(O) the(O) American(B-organization) Astronomical(I-organization) Society(I-organization) and(O) was(O) the(O) first(O) editor(O) of(O) the(O) Journal(B-academic journal) of(I-academic journal) Geophysical(I-academic journal) Research(I-academic journal) .(O)"}}
{"id": "285", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "award", "event", "university", "enzyme", "astronomical object", "academic journal", "person", "protein", "location", "scientist", "country", "chemical element", "chemical compound", "organization", "discipline"], "instance": {"id": "285", "words": ["The", "comet", "was", "discovered", "by", "astronomers", "Carolyn", "S.", "Shoemaker", "and", "Eugene", "Merle", "Shoemaker", "and", "David", "H.", "Levy", "in", "1993", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, award, event, university, enzyme, astronomical object, academic journal, person, protein, location, scientist, country, chemical element, chemical compound, organization, discipline and O.\nSentence: The comet was discovered by astronomers Carolyn S. Shoemaker and Eugene Merle Shoemaker and David H. Levy in 1993 .", "prompt_labels": "The(O) comet(O) was(O) discovered(O) by(O) astronomers(O) Carolyn(B-scientist) S.(I-scientist) Shoemaker(I-scientist) and(O) Eugene(B-scientist) Merle(I-scientist) Shoemaker(I-scientist) and(O) David(B-scientist) H.(I-scientist) Levy(I-scientist) in(O) 1993(O) .(O)"}}
{"id": "81", "dataset": "crossner_science", "split": "dev", "label_list": ["event", "award", "discipline", "chemical element", "astronomical object", "organization", "enzyme", "academic journal", "location", "person", "theory", "chemical compound", "protein", "scientist", "country", "university"], "instance": {"id": "81", "words": ["ESA", "'s", "Advanced", "Concepts", "Team", "has", "also", "demonstrated", "theoretically", "that", "a", "deflection", "of", "99942", "Apophis", "could", "be", "achieved", "by", "sending", "a", "simple", "spacecraft"], "labels": ["B-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, discipline, chemical element, astronomical object, organization, enzyme, academic journal, location, person, theory, chemical compound, protein, scientist, country, university and O.\nSentence: ESA 's Advanced Concepts Team has also demonstrated theoretically that a deflection of 99942 Apophis could be achieved by sending a simple spacecraft", "prompt_labels": "ESA(B-organization) 's(O) Advanced(B-organization) Concepts(I-organization) Team(I-organization) has(O) also(O) demonstrated(O) theoretically(O) that(O) a(O) deflection(O) of(O) 99942(B-astronomical object) Apophis(I-astronomical object) could(O) be(O) achieved(O) by(O) sending(O) a(O) simple(O) spacecraft(O)"}}
{"id": "177", "dataset": "crossner_science", "split": "dev", "label_list": ["country", "event", "university", "theory", "protein", "chemical element", "location", "chemical compound", "astronomical object", "discipline", "award", "organization", "academic journal", "scientist", "enzyme", "person"], "instance": {"id": "177", "words": ["He", "was", "internationally", "recognized", "with", "membership", "in", "the", "Japan", "Academy", "and", "the", "Brazilian", "Academy", "of", "Sciences", ",", "and", "in", "1959", "was", "appointed", "a", "member", "of", "the", "Board", "of", "Governors", "of", "the", "Weizmann", "Institute", "of", "Science", "in", "Israel", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "B-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, university, theory, protein, chemical element, location, chemical compound, astronomical object, discipline, award, organization, academic journal, scientist, enzyme, person and O.\nSentence: He was internationally recognized with membership in the Japan Academy and the Brazilian Academy of Sciences , and in 1959 was appointed a member of the Board of Governors of the Weizmann Institute of Science in Israel .", "prompt_labels": "He(O) was(O) internationally(O) recognized(O) with(O) membership(O) in(O) the(O) Japan(B-organization) Academy(I-organization) and(O) the(O) Brazilian(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) and(O) in(O) 1959(O) was(O) appointed(O) a(O) member(O) of(O) the(O) Board(O) of(O) Governors(O) of(O) the(O) Weizmann(B-university) Institute(I-university) of(I-university) Science(I-university) in(O) Israel(B-country) .(O)"}}
{"id": "212", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "discipline", "astronomical object", "academic journal", "event", "location", "university", "award", "chemical compound", "enzyme", "organization", "scientist", "person", "country", "chemical element", "theory"], "instance": {"id": "212", "words": ["CRISPR", "interference", "(", "CRISPRi", ")", "on", "the", "other", "hand", "utilizes", "a", "catalytically", "inactive", "nuclease", "to", "physically", "block", "RNA", "polymerase", ",", "effectively", "preventing", "or", "halting", "transcription", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "I-enzyme", "O", "O", "O", "B-enzyme", "I-enzyme", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, discipline, astronomical object, academic journal, event, location, university, award, chemical compound, enzyme, organization, scientist, person, country, chemical element, theory and O.\nSentence: CRISPR interference ( CRISPRi ) on the other hand utilizes a catalytically inactive nuclease to physically block RNA polymerase , effectively preventing or halting transcription .", "prompt_labels": "CRISPR(O) interference(O) ((O) CRISPRi(O) )(O) on(O) the(O) other(O) hand(O) utilizes(O) a(O) catalytically(B-enzyme) inactive(I-enzyme) nuclease(I-enzyme) to(O) physically(O) block(O) RNA(B-enzyme) polymerase(I-enzyme) ,(O) effectively(O) preventing(O) or(O) halting(O) transcription(O) .(O)"}}
{"id": "374", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "award", "country", "academic journal", "discipline", "organization", "chemical element", "astronomical object", "person", "location", "chemical compound", "enzyme", "scientist", "university", "event", "theory"], "instance": {"id": "374", "words": ["Sinope", "was", "the", "outermost", "known", "moon", "of", "Jupiter", "until", "the", "discovery", "of", "Megaclite", "in", "2000", "."], "labels": ["B-astronomical object", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, award, country, academic journal, discipline, organization, chemical element, astronomical object, person, location, chemical compound, enzyme, scientist, university, event, theory and O.\nSentence: Sinope was the outermost known moon of Jupiter until the discovery of Megaclite in 2000 .", "prompt_labels": "Sinope(B-astronomical object) was(O) the(O) outermost(O) known(O) moon(B-astronomical object) of(O) Jupiter(B-astronomical object) until(O) the(O) discovery(O) of(O) Megaclite(B-astronomical object) in(O) 2000(O) .(O)"}}
{"id": "289", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "country", "organization", "person", "location", "chemical element", "discipline", "astronomical object", "protein", "enzyme", "award", "scientist", "chemical compound", "academic journal", "event", "theory"], "instance": {"id": "289", "words": ["Mandelbrot", "'s", "awards", "include", "the", "Wolf", "Prize", "for", "Physics", "in", "1993", ",", "the", "Lewis", "Fry", "Richardson", "Prize", "of", "the", "European", "Geophysical", "Society", "in", "2000", ",", "the", "Japan", "Prize", "in", "2003", ",", "in", "2006", "."], "labels": ["B-award", "I-award", "I-award", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, country, organization, person, location, chemical element, discipline, astronomical object, protein, enzyme, award, scientist, chemical compound, academic journal, event, theory and O.\nSentence: Mandelbrot 's awards include the Wolf Prize for Physics in 1993 , the Lewis Fry Richardson Prize of the European Geophysical Society in 2000 , the Japan Prize in 2003 , in 2006 .", "prompt_labels": "Mandelbrot(B-award) 's(I-award) awards(I-award) include(O) the(O) Wolf(B-award) Prize(I-award) for(I-award) Physics(I-award) in(O) 1993(O) ,(O) the(O) Lewis(B-award) Fry(I-award) Richardson(I-award) Prize(I-award) of(I-award) the(I-award) European(I-award) Geophysical(I-award) Society(I-award) in(O) 2000(O) ,(O) the(O) Japan(B-award) Prize(I-award) in(O) 2003(O) ,(O) in(O) 2006(O) .(O)"}}
{"id": "301", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "academic journal", "country", "person", "astronomical object", "event", "scientist", "chemical compound", "theory", "discipline", "award", "protein", "university", "enzyme", "organization", "location"], "instance": {"id": "301", "words": ["To", "be", "recognized", "as", "a", "board-certified", "subspecialist", "by", "the", "American", "Board", "of", "Obstetrics", "and", "Gynecology", "or", "the", "American", "Osteopathic", "Board", "of", "Obstetrics", "and", "Gynecology", ",", "a", "practitioner", "must", "have", "completed", "an", "Accreditation", "Council", "for", "Graduate", "Medical", "Education", "or", "American", "Osteopathic", "Association", "-accredited", "residency", "and", "obtained", "a", "Certificate", "of", "Added", "Qualifications", "(", "CAQ", ")", "which", "requires", "an", "additional", "standardized", "examination", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "B-award", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, academic journal, country, person, astronomical object, event, scientist, chemical compound, theory, discipline, award, protein, university, enzyme, organization, location and O.\nSentence: To be recognized as a board-certified subspecialist by the American Board of Obstetrics and Gynecology or the American Osteopathic Board of Obstetrics and Gynecology , a practitioner must have completed an Accreditation Council for Graduate Medical Education or American Osteopathic Association -accredited residency and obtained a Certificate of Added Qualifications ( CAQ ) which requires an additional standardized examination .", "prompt_labels": "To(O) be(O) recognized(O) as(O) a(O) board-certified(O) subspecialist(O) by(O) the(O) American(B-organization) Board(I-organization) of(I-organization) Obstetrics(I-organization) and(I-organization) Gynecology(I-organization) or(O) the(O) American(B-organization) Osteopathic(I-organization) Board(I-organization) of(I-organization) Obstetrics(I-organization) and(I-organization) Gynecology(I-organization) ,(O) a(O) practitioner(O) must(O) have(O) completed(O) an(O) Accreditation(B-organization) Council(I-organization) for(I-organization) Graduate(I-organization) Medical(I-organization) Education(I-organization) or(O) American(B-organization) Osteopathic(I-organization) Association(I-organization) -accredited(O) residency(O) and(O) obtained(O) a(O) Certificate(B-award) of(I-award) Added(I-award) Qualifications(I-award) ((O) CAQ(B-award) )(O) which(O) requires(O) an(O) additional(O) standardized(O) examination(O) .(O)"}}
{"id": "49", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "astronomical object", "country", "event", "organization", "university", "chemical element", "person", "scientist", "location", "award", "enzyme", "discipline", "chemical compound", "protein", "academic journal"], "instance": {"id": "49", "words": ["He", "also", "received", "the", "Rumford", "Medal", "of", "the", "British", "Royal", "Society", "in", "1896", ",", "jointly", "with", "Philipp", "Lenard", ",", "who", "had", "already", "shown", "that", "a", "portion", "of", "the", "cathode", "rays", "could", "pass", "through", "a", "thin", "film", "of", "a", "metal", "such", "as", "aluminum", "."], "labels": ["O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical element", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, astronomical object, country, event, organization, university, chemical element, person, scientist, location, award, enzyme, discipline, chemical compound, protein, academic journal and O.\nSentence: He also received the Rumford Medal of the British Royal Society in 1896 , jointly with Philipp Lenard , who had already shown that a portion of the cathode rays could pass through a thin film of a metal such as aluminum .", "prompt_labels": "He(O) also(O) received(O) the(O) Rumford(B-award) Medal(I-award) of(I-award) the(I-award) British(I-award) Royal(I-award) Society(I-award) in(O) 1896(O) ,(O) jointly(O) with(O) Philipp(B-scientist) Lenard(I-scientist) ,(O) who(O) had(O) already(O) shown(O) that(O) a(O) portion(O) of(O) the(O) cathode(O) rays(O) could(O) pass(O) through(O) a(O) thin(O) film(O) of(O) a(O) metal(O) such(O) as(O) aluminum(B-chemical element) .(O)"}}
{"id": "308", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "academic journal", "chemical element", "university", "location", "theory", "protein", "award", "enzyme", "discipline", "country", "event", "organization", "chemical compound", "scientist", "person"], "instance": {"id": "308", "words": ["He", "is", "a", "recipient", "of", "the", "Bolton", "S.", "Corson", "Medal", "in", "1980", ",", "Tyler", "Prize", "for", "Environmental", "Achievement", "in", "1985", ",", "the", "Japan", "Prize", "in", "1997", ",", "the", "National", "Medal", "of", "Science", "in", "1998", "and", "the", "Thomas", "Hunt", "Morgan", "Medal", "in", "2004", ",", "among", "many", "others", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, academic journal, chemical element, university, location, theory, protein, award, enzyme, discipline, country, event, organization, chemical compound, scientist, person and O.\nSentence: He is a recipient of the Bolton S. Corson Medal in 1980 , Tyler Prize for Environmental Achievement in 1985 , the Japan Prize in 1997 , the National Medal of Science in 1998 and the Thomas Hunt Morgan Medal in 2004 , among many others .", "prompt_labels": "He(O) is(O) a(O) recipient(O) of(O) the(O) Bolton(B-award) S.(I-award) Corson(I-award) Medal(I-award) in(O) 1980(O) ,(O) Tyler(B-award) Prize(I-award) for(I-award) Environmental(I-award) Achievement(I-award) in(O) 1985(O) ,(O) the(O) Japan(B-award) Prize(I-award) in(O) 1997(O) ,(O) the(O) National(B-award) Medal(I-award) of(I-award) Science(I-award) in(O) 1998(O) and(O) the(O) Thomas(B-award) Hunt(I-award) Morgan(I-award) Medal(I-award) in(O) 2004(O) ,(O) among(O) many(O) others(O) .(O)"}}
{"id": "192", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "academic journal", "award", "event", "person", "discipline", "enzyme", "location", "country", "scientist", "university", "chemical compound", "organization", "protein", "chemical element", "astronomical object"], "instance": {"id": "192", "words": ["LH", "is", "released", "from", "the", "pituitary", "gland", "along", "with", "Follicle-stimulating", "hormone", "in", "response", "to", "GnRH", "release", "into", "the", "hypophyseal", "portal", "system", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, academic journal, award, event, person, discipline, enzyme, location, country, scientist, university, chemical compound, organization, protein, chemical element, astronomical object and O.\nSentence: LH is released from the pituitary gland along with Follicle-stimulating hormone in response to GnRH release into the hypophyseal portal system .", "prompt_labels": "LH(O) is(O) released(O) from(O) the(O) pituitary(O) gland(O) along(O) with(O) Follicle-stimulating(O) hormone(O) in(O) response(O) to(O) GnRH(O) release(O) into(O) the(O) hypophyseal(O) portal(O) system(O) .(O)"}}
{"id": "242", "dataset": "crossner_science", "split": "dev", "label_list": ["award", "scientist", "country", "astronomical object", "event", "protein", "chemical compound", "university", "chemical element", "discipline", "theory", "location", "organization", "enzyme", "person", "academic journal"], "instance": {"id": "242", "words": ["Two", "asteroid", "s", "share", "the", "same", "names", "as", "moons", "of", "Neptune", ":", "74", "Galatea", "and", "1162", "Larissa", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, scientist, country, astronomical object, event, protein, chemical compound, university, chemical element, discipline, theory, location, organization, enzyme, person, academic journal and O.\nSentence: Two asteroid s share the same names as moons of Neptune : 74 Galatea and 1162 Larissa .", "prompt_labels": "Two(O) asteroid(O) s(O) share(O) the(O) same(O) names(O) as(O) moons(O) of(O) Neptune(B-astronomical object) :(O) 74(B-astronomical object) Galatea(I-astronomical object) and(O) 1162(B-astronomical object) Larissa(I-astronomical object) .(O)"}}
{"id": "136", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "chemical compound", "award", "astronomical object", "location", "person", "event", "enzyme", "organization", "discipline", "scientist", "theory", "chemical element", "academic journal", "country", "university"], "instance": {"id": "136", "words": ["Gas", "giants", "with", "a", "large", "radius", "and", "very", "low", "density", "are", "sometimes", "called", "puffy", "planets", "COROT-1b", ",", "TrES-4", ",", "WASP-12b", ",", "WASP-17b", ",", "and", "Kepler-7b", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, chemical compound, award, astronomical object, location, person, event, enzyme, organization, discipline, scientist, theory, chemical element, academic journal, country, university and O.\nSentence: Gas giants with a large radius and very low density are sometimes called puffy planets COROT-1b , TrES-4 , WASP-12b , WASP-17b , and Kepler-7b .", "prompt_labels": "Gas(O) giants(O) with(O) a(O) large(O) radius(O) and(O) very(O) low(O) density(O) are(O) sometimes(O) called(O) puffy(O) planets(O) COROT-1b(B-astronomical object) ,(O) TrES-4(B-astronomical object) ,(O) WASP-12b(B-astronomical object) ,(O) WASP-17b(B-astronomical object) ,(O) and(O) Kepler-7b(B-astronomical object) .(O)"}}
{"id": "286", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "scientist", "event", "theory", "country", "academic journal", "person", "location", "chemical element", "award", "chemical compound", "university", "organization", "protein", "enzyme", "discipline"], "instance": {"id": "286", "words": ["This", "receptor", "has", "several", "CC", "chemokine", "ligands", "including", "CCL2", ",", "CCL3", ",", "CCL4", ",", "CCL5", ",", "CCL11", ",", "CCL13", ",", "CCL14", "and", "CCL16", ".ref", "name", "=", "nomiyama", "/", "ref", "name", "=", "ogilvie", "/"], "labels": ["O", "O", "O", "O", "B-protein", "I-protein", "I-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "B-protein", "O", "O", "O", "B-protein", "O", "O", "O", "O", "B-protein", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, scientist, event, theory, country, academic journal, person, location, chemical element, award, chemical compound, university, organization, protein, enzyme, discipline and O.\nSentence: This receptor has several CC chemokine ligands including CCL2 , CCL3 , CCL4 , CCL5 , CCL11 , CCL13 , CCL14 and CCL16 .ref name = nomiyama / ref name = ogilvie /", "prompt_labels": "This(O) receptor(O) has(O) several(O) CC(B-protein) chemokine(I-protein) ligands(I-protein) including(O) CCL2(B-protein) ,(O) CCL3(B-protein) ,(O) CCL4(B-protein) ,(O) CCL5(B-protein) ,(O) CCL11(B-protein) ,(O) CCL13(B-protein) ,(O) CCL14(B-protein) and(O) CCL16(B-protein) .ref(O) name(O) =(O) nomiyama(B-protein) /(O) ref(O) name(O) =(O) ogilvie(B-protein) /(O)"}}
{"id": "146", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "organization", "event", "theory", "country", "enzyme", "protein", "scientist", "astronomical object", "award", "academic journal", "location", "person", "chemical element", "chemical compound", "discipline"], "instance": {"id": "146", "words": ["Chiron", "'s", "orbit", "was", "found", "to", "be", "highly", "eccentric", "(", "0.37", ")", ",", "with", "perihelion", "just", "inside", "the", "orbit", "of", "Saturn", "and", "aphelion", "just", "outside", "the", "perihelion", "of", "Uranus", "(", "it", "does", "not", "reach", "the", "average", "distance", "of", "Uranus", ",", "however", ")", "."], "labels": ["B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, organization, event, theory, country, enzyme, protein, scientist, astronomical object, award, academic journal, location, person, chemical element, chemical compound, discipline and O.\nSentence: Chiron 's orbit was found to be highly eccentric ( 0.37 ) , with perihelion just inside the orbit of Saturn and aphelion just outside the perihelion of Uranus ( it does not reach the average distance of Uranus , however ) .", "prompt_labels": "Chiron(B-astronomical object) 's(O) orbit(O) was(O) found(O) to(O) be(O) highly(O) eccentric(O) ((O) 0.37(O) )(O) ,(O) with(O) perihelion(O) just(O) inside(O) the(O) orbit(O) of(O) Saturn(B-astronomical object) and(O) aphelion(O) just(O) outside(O) the(O) perihelion(O) of(O) Uranus(B-astronomical object) ((O) it(O) does(O) not(O) reach(O) the(O) average(O) distance(O) of(O) Uranus(B-astronomical object) ,(O) however(O) )(O) .(O)"}}
{"id": "319", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "organization", "country", "discipline", "chemical compound", "theory", "scientist", "award", "event", "person", "protein", "university", "chemical element", "enzyme", "location", "academic journal"], "instance": {"id": "319", "words": ["Michalak", "'s", "estimate", "depends", "on", "the", "masses", "of", "19", "Fortuna", ",", "29", "Amphitrite", ",", "and", "16", "Psyche", ";", "thus", "this", "mass", "was", "obtained", "assuming", "an", "incomplete", "dynamical", "model", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, organization, country, discipline, chemical compound, theory, scientist, award, event, person, protein, university, chemical element, enzyme, location, academic journal and O.\nSentence: Michalak 's estimate depends on the masses of 19 Fortuna , 29 Amphitrite , and 16 Psyche ; thus this mass was obtained assuming an incomplete dynamical model .", "prompt_labels": "Michalak(B-scientist) 's(O) estimate(O) depends(O) on(O) the(O) masses(O) of(O) 19(B-astronomical object) Fortuna(I-astronomical object) ,(O) 29(B-astronomical object) Amphitrite(I-astronomical object) ,(O) and(O) 16(B-astronomical object) Psyche(I-astronomical object) ;(O) thus(O) this(O) mass(O) was(O) obtained(O) assuming(O) an(O) incomplete(O) dynamical(O) model(O) .(O)"}}
{"id": "144", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "country", "award", "protein", "organization", "chemical element", "university", "event", "theory", "astronomical object", "academic journal", "chemical compound", "scientist", "enzyme", "location", "person"], "instance": {"id": "144", "words": ["DNA", "cytosine", "methylation", "is", "catalyzed", "by", "DNA", "methyltransferase", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, country, award, protein, organization, chemical element, university, event, theory, astronomical object, academic journal, chemical compound, scientist, enzyme, location, person and O.\nSentence: DNA cytosine methylation is catalyzed by DNA methyltransferase .", "prompt_labels": "DNA(O) cytosine(O) methylation(O) is(O) catalyzed(O) by(O) DNA(B-enzyme) methyltransferase(I-enzyme) .(O)"}}
{"id": "446", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "enzyme", "award", "event", "protein", "country", "chemical element", "chemical compound", "location", "university", "astronomical object", "academic journal", "scientist", "person", "organization", "theory"], "instance": {"id": "446", "words": ["Nottingham", "City", "Hospital", ",", "Nottingham", "University", "Hospitals", "NHS", "Trust", "(", "now", "one", "of", "the", "biggest", "in", "the", "country", ")", "and", "Queen", "'s", "Medical", "Centre", "articles", "all", "need", "lots", "of", "work", "too", "and", "are", "quite", "important", "to", "the", "city", "."], "labels": ["B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, enzyme, award, event, protein, country, chemical element, chemical compound, location, university, astronomical object, academic journal, scientist, person, organization, theory and O.\nSentence: Nottingham City Hospital , Nottingham University Hospitals NHS Trust ( now one of the biggest in the country ) and Queen 's Medical Centre articles all need lots of work too and are quite important to the city .", "prompt_labels": "Nottingham(B-organization) City(I-organization) Hospital(I-organization) ,(O) Nottingham(B-organization) University(I-organization) Hospitals(I-organization) NHS(I-organization) Trust(I-organization) ((O) now(O) one(O) of(O) the(O) biggest(O) in(O) the(O) country(O) )(O) and(O) Queen(B-organization) 's(I-organization) Medical(I-organization) Centre(I-organization) articles(O) all(O) need(O) lots(O) of(O) work(O) too(O) and(O) are(O) quite(O) important(O) to(O) the(O) city(O) .(O)"}}
{"id": "305", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "country", "protein", "chemical element", "discipline", "theory", "award", "location", "scientist", "person", "astronomical object", "event", "organization", "chemical compound", "enzyme", "university"], "instance": {"id": "305", "words": ["It", "won", "Gabriel", "a", "Grammy", "Award", "for", "Grammy", "Award", "for", "Best", "New", "Age", "Album", "and", "a", "nomination", "for", "a", "Golden", "Globe", "for", "Golden", "Globe", "Award", "for", "Best", "Original", "Score", "."], "labels": ["O", "O", "B-person", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, country, protein, chemical element, discipline, theory, award, location, scientist, person, astronomical object, event, organization, chemical compound, enzyme, university and O.\nSentence: It won Gabriel a Grammy Award for Grammy Award for Best New Age Album and a nomination for a Golden Globe for Golden Globe Award for Best Original Score .", "prompt_labels": "It(O) won(O) Gabriel(B-person) a(O) Grammy(B-award) Award(I-award) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) New(I-award) Age(I-award) Album(I-award) and(O) a(O) nomination(O) for(O) a(O) Golden(B-award) Globe(I-award) for(O) Golden(B-award) Globe(I-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) .(O)"}}
{"id": "304", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "academic journal", "discipline", "theory", "chemical compound", "chemical element", "enzyme", "event", "astronomical object", "award", "scientist", "country", "organization", "person", "protein", "university"], "instance": {"id": "304", "words": ["The", "Green", "Lab", "is", "currently", "focused", "on", "understanding", "the", "circadian", "function", "of", "Nocturnin", ",", "the", "circadian", "regulation", "of", "metabolism", ",", "and", "the", "circadian", "structure", "and", "function", "of", "Cryptochrome", "s", "core", "components", "."], "labels": ["O", "B-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, academic journal, discipline, theory, chemical compound, chemical element, enzyme, event, astronomical object, award, scientist, country, organization, person, protein, university and O.\nSentence: The Green Lab is currently focused on understanding the circadian function of Nocturnin , the circadian regulation of metabolism , and the circadian structure and function of Cryptochrome s core components .", "prompt_labels": "The(O) Green(B-organization) Lab(I-organization) is(O) currently(O) focused(O) on(O) understanding(O) the(O) circadian(O) function(O) of(O) Nocturnin(B-enzyme) ,(O) the(O) circadian(O) regulation(O) of(O) metabolism(O) ,(O) and(O) the(O) circadian(O) structure(O) and(O) function(O) of(O) Cryptochrome(B-protein) s(O) core(O) components(O) .(O)"}}
{"id": "274", "dataset": "crossner_science", "split": "dev", "label_list": ["organization", "event", "scientist", "university", "award", "country", "chemical element", "location", "astronomical object", "discipline", "person", "protein", "enzyme", "theory", "academic journal", "chemical compound"], "instance": {"id": "274", "words": ["Another", ",", "the", "fission", "model", ",", "was", "developed", "by", "George", "Darwin", "(", "son", "of", "Charles", "Darwin", ")", ",", "who", "noted", "that", ",", "as", "the", "Moon", "is", "gradually", "receding", "from", "the", "Earth", "at", "a", "rate", "of", "about", "4", "cm", "per", "year", ",", "so", "at", "one", "point", "in", "the", "distant", "past", "it", "must", "have", "been", "part", "of", "the", "Earth", ",", "but", "was", "flung", "outward", "by", "the", "momentum", "of", "Earth", "'s", "then-much", "faster", "rotation", "."], "labels": ["O", "O", "O", "B-theory", "I-theory", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, scientist, university, award, country, chemical element, location, astronomical object, discipline, person, protein, enzyme, theory, academic journal, chemical compound and O.\nSentence: Another , the fission model , was developed by George Darwin ( son of Charles Darwin ) , who noted that , as the Moon is gradually receding from the Earth at a rate of about 4 cm per year , so at one point in the distant past it must have been part of the Earth , but was flung outward by the momentum of Earth 's then-much faster rotation .", "prompt_labels": "Another(O) ,(O) the(O) fission(B-theory) model(I-theory) ,(O) was(O) developed(O) by(O) George(B-scientist) Darwin(I-scientist) ((O) son(O) of(O) Charles(B-scientist) Darwin(I-scientist) )(O) ,(O) who(O) noted(O) that(O) ,(O) as(O) the(O) Moon(B-astronomical object) is(O) gradually(O) receding(O) from(O) the(O) Earth(B-astronomical object) at(O) a(O) rate(O) of(O) about(O) 4(O) cm(O) per(O) year(O) ,(O) so(O) at(O) one(O) point(O) in(O) the(O) distant(O) past(O) it(O) must(O) have(O) been(O) part(O) of(O) the(O) Earth(B-astronomical object) ,(O) but(O) was(O) flung(O) outward(O) by(O) the(O) momentum(O) of(O) Earth(B-astronomical object) 's(O) then-much(O) faster(O) rotation(O) .(O)"}}
{"id": "233", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "chemical element", "enzyme", "university", "astronomical object", "location", "chemical compound", "theory", "award", "country", "academic journal", "organization", "scientist", "protein", "event", "person"], "instance": {"id": "233", "words": ["The", "anterior", "pituitary", "releases", "the", "gonadotropin", "s", "Luteinizing", "hormone", "into", "the", "ovaries", ",", "which", "produce", "estrogen", ",", "and", "follicle-stimulating", "hormone", "(", "FSH", ")", "into", "the", "testes", ",", "which", "produce", "testosterone", "."], "labels": ["O", "O", "O", "O", "O", "B-protein", "I-protein", "I-protein", "I-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical element, enzyme, university, astronomical object, location, chemical compound, theory, award, country, academic journal, organization, scientist, protein, event, person and O.\nSentence: The anterior pituitary releases the gonadotropin s Luteinizing hormone into the ovaries , which produce estrogen , and follicle-stimulating hormone ( FSH ) into the testes , which produce testosterone .", "prompt_labels": "The(O) anterior(O) pituitary(O) releases(O) the(O) gonadotropin(B-protein) s(I-protein) Luteinizing(I-protein) hormone(I-protein) into(O) the(O) ovaries(O) ,(O) which(O) produce(O) estrogen(O) ,(O) and(O) follicle-stimulating(B-protein) hormone(I-protein) ((O) FSH(B-protein) )(O) into(O) the(O) testes(O) ,(O) which(O) produce(O) testosterone(O) .(O)"}}
{"id": "132", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical compound", "astronomical object", "location", "organization", "chemical element", "scientist", "country", "person", "award", "university", "enzyme", "discipline", "event", "academic journal", "theory", "protein"], "instance": {"id": "132", "words": ["At", "Rockefeller", "University", "he", "worked", "on", "macrophage", "Fc", "receptor", "s", "and", "lysosomal", "proteases", "."], "labels": ["O", "B-university", "I-university", "O", "O", "O", "B-protein", "I-protein", "I-protein", "O", "O", "B-enzyme", "I-enzyme", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, astronomical object, location, organization, chemical element, scientist, country, person, award, university, enzyme, discipline, event, academic journal, theory, protein and O.\nSentence: At Rockefeller University he worked on macrophage Fc receptor s and lysosomal proteases .", "prompt_labels": "At(O) Rockefeller(B-university) University(I-university) he(O) worked(O) on(O) macrophage(B-protein) Fc(I-protein) receptor(I-protein) s(O) and(O) lysosomal(B-enzyme) proteases(I-enzyme) .(O)"}}
{"id": "237", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "organization", "location", "discipline", "enzyme", "person", "protein", "chemical element", "event", "scientist", "country", "academic journal", "award", "university", "theory", "chemical compound"], "instance": {"id": "237", "words": ["The", "Institute", "is", "a", "partnership", "among", "Harvard", "University", ",", "its", "major", "affiliated", "hospitals", "(", "Beth", "Israel", "Deaconess", "Medical", "Center", ",", "Brigham", "and", "Women", "'s", "Hospital", ",", "Boston", "Children", "'s", "Hospital", ",", "Dana", "Farber", "Cancer", "Institute", ",", "Massachusetts", "General", "Hospital", ",", "Spaulding", "Rehabilitation", "Hospital", ")", ",", "Boston", "University", ",", "Massachusetts", "Institute", "of", "Technology", ",", "Tufts", "University", ",", "University", "of", "Massachusetts", "Medical", "School", ",", "Charité", "-", "Universitätsmedizin", "Berlin", ",", "and", "University", "of", "Zurich", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-university", "I-university", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-university", "I-university", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-university", "I-university", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "O", "B-university", "I-university", "I-university", "I-university", "O", "O", "B-university", "I-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, organization, location, discipline, enzyme, person, protein, chemical element, event, scientist, country, academic journal, award, university, theory, chemical compound and O.\nSentence: The Institute is a partnership among Harvard University , its major affiliated hospitals ( Beth Israel Deaconess Medical Center , Brigham and Women 's Hospital , Boston Children 's Hospital , Dana Farber Cancer Institute , Massachusetts General Hospital , Spaulding Rehabilitation Hospital ) , Boston University , Massachusetts Institute of Technology , Tufts University , University of Massachusetts Medical School , Charité - Universitätsmedizin Berlin , and University of Zurich .", "prompt_labels": "The(O) Institute(O) is(O) a(O) partnership(O) among(O) Harvard(B-university) University(I-university) ,(O) its(O) major(O) affiliated(O) hospitals(O) ((O) Beth(B-organization) Israel(I-organization) Deaconess(I-organization) Medical(I-organization) Center(I-organization) ,(O) Brigham(B-organization) and(I-organization) Women(I-organization) 's(I-organization) Hospital(I-organization) ,(O) Boston(B-organization) Children(I-organization) 's(I-organization) Hospital(I-organization) ,(O) Dana(B-organization) Farber(I-organization) Cancer(I-organization) Institute(I-organization) ,(O) Massachusetts(B-organization) General(I-organization) Hospital(I-organization) ,(O) Spaulding(B-organization) Rehabilitation(I-organization) Hospital(I-organization) )(O) ,(O) Boston(B-university) University(I-university) ,(O) Massachusetts(B-organization) Institute(I-organization) of(I-organization) Technology(I-organization) ,(O) Tufts(B-university) University(I-university) ,(O) University(B-university) of(I-university) Massachusetts(I-university) Medical(I-university) School(I-university) ,(O) Charité(B-university) -(I-university) Universitätsmedizin(I-university) Berlin(I-university) ,(O) and(O) University(B-university) of(I-university) Zurich(I-university) .(O)"}}
{"id": "154", "dataset": "crossner_science", "split": "dev", "label_list": ["person", "discipline", "country", "academic journal", "theory", "organization", "chemical element", "astronomical object", "event", "university", "award", "protein", "scientist", "chemical compound", "enzyme", "location"], "instance": {"id": "154", "words": ["His", "work", "has", "been", "published", "in", "international", "refereed", "journals", ",", "including", "American", "Economic", "Review", ",", "Journal", "of", "European", "Economic", "Association", ",", "Journal", "of", "Economic", "Perspectives", ",", "Economic", "Journal", "and", "American", "Political", "Science", "Review", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, discipline, country, academic journal, theory, organization, chemical element, astronomical object, event, university, award, protein, scientist, chemical compound, enzyme, location and O.\nSentence: His work has been published in international refereed journals , including American Economic Review , Journal of European Economic Association , Journal of Economic Perspectives , Economic Journal and American Political Science Review .", "prompt_labels": "His(O) work(O) has(O) been(O) published(O) in(O) international(O) refereed(O) journals(O) ,(O) including(O) American(B-academic journal) Economic(I-academic journal) Review(I-academic journal) ,(O) Journal(O) of(O) European(B-academic journal) Economic(I-academic journal) Association(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Economic(I-academic journal) Perspectives(I-academic journal) ,(O) Economic(B-academic journal) Journal(I-academic journal) and(O) American(B-academic journal) Political(I-academic journal) Science(I-academic journal) Review(I-academic journal) .(O)"}}
{"id": "194", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "chemical element", "protein", "scientist", "country", "university", "location", "organization", "academic journal", "astronomical object", "person", "chemical compound", "discipline", "theory", "event", "award"], "instance": {"id": "194", "words": ["The", "manufacturing", "process", "of", "Technora", "reacts", "P-Phenylenediamine", "and", "3,4", "'", "-diaminodiphenylether", "(", "3,4", "'", "-ODA", ")", "with", "Terephthaloyl", "chloride", "."], "labels": ["O", "O", "O", "O", "B-chemical compound", "O", "B-chemical compound", "O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "O", "O", "B-chemical compound", "I-chemical compound", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical element, protein, scientist, country, university, location, organization, academic journal, astronomical object, person, chemical compound, discipline, theory, event, award and O.\nSentence: The manufacturing process of Technora reacts P-Phenylenediamine and 3,4 ' -diaminodiphenylether ( 3,4 ' -ODA ) with Terephthaloyl chloride .", "prompt_labels": "The(O) manufacturing(O) process(O) of(O) Technora(B-chemical compound) reacts(O) P-Phenylenediamine(B-chemical compound) and(O) 3,4(B-chemical compound) '(I-chemical compound) -diaminodiphenylether(I-chemical compound) ((O) 3,4(B-chemical compound) '(I-chemical compound) -ODA(I-chemical compound) )(O) with(O) Terephthaloyl(B-chemical compound) chloride(I-chemical compound) .(O)"}}
{"id": "277", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "country", "astronomical object", "theory", "university", "scientist", "organization", "discipline", "person", "protein", "chemical element", "academic journal", "award", "location", "chemical compound", "event"], "instance": {"id": "277", "words": ["Point", "Loma", "is", "home", "to", "several", "major", "military", "installations", "including", "the", "US", "Navy", "'s", "SPAWAR", "program", ",", "the", "Marine", "Corps", "Recruit", "Depot", "San", "Diego", "(", "MCRD", "San", "Diego", ")", "and", "Naval", "Base", "Point", "Loma", "."], "labels": ["B-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "I-location", "I-location", "O", "B-location", "I-location", "I-location", "O", "O", "B-location", "I-location", "I-location", "I-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, country, astronomical object, theory, university, scientist, organization, discipline, person, protein, chemical element, academic journal, award, location, chemical compound, event and O.\nSentence: Point Loma is home to several major military installations including the US Navy 's SPAWAR program , the Marine Corps Recruit Depot San Diego ( MCRD San Diego ) and Naval Base Point Loma .", "prompt_labels": "Point(B-location) Loma(I-location) is(O) home(O) to(O) several(O) major(O) military(O) installations(O) including(O) the(O) US(B-organization) Navy(I-organization) 's(O) SPAWAR(B-organization) program(O) ,(O) the(O) Marine(B-location) Corps(I-location) Recruit(I-location) Depot(I-location) San(I-location) Diego(I-location) ((O) MCRD(B-location) San(I-location) Diego(I-location) )(O) and(O) Naval(B-location) Base(I-location) Point(I-location) Loma(I-location) .(O)"}}
{"id": "68", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "discipline", "event", "university", "country", "award", "protein", "enzyme", "person", "chemical element", "astronomical object", "scientist", "academic journal", "chemical compound", "theory", "organization"], "instance": {"id": "68", "words": ["The", "concept", "that", "the", "composition", "of", "plant", "communities", "such", "as", "temperate", "broadleaf", "forest", "changes", "by", "a", "process", "of", "ecological", "succession", "was", "developed", "by", "Henry", "Chandler", "Cowles", ",", "Arthur", "Tansley", "and", "Frederic", "Clements", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, discipline, event, university, country, award, protein, enzyme, person, chemical element, astronomical object, scientist, academic journal, chemical compound, theory, organization and O.\nSentence: The concept that the composition of plant communities such as temperate broadleaf forest changes by a process of ecological succession was developed by Henry Chandler Cowles , Arthur Tansley and Frederic Clements .", "prompt_labels": "The(O) concept(O) that(O) the(O) composition(O) of(O) plant(O) communities(O) such(O) as(O) temperate(O) broadleaf(O) forest(O) changes(O) by(O) a(O) process(O) of(O) ecological(O) succession(O) was(O) developed(O) by(O) Henry(B-scientist) Chandler(I-scientist) Cowles(I-scientist) ,(O) Arthur(B-scientist) Tansley(I-scientist) and(O) Frederic(B-scientist) Clements(I-scientist) .(O)"}}
{"id": "245", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "scientist", "academic journal", "location", "enzyme", "award", "organization", "astronomical object", "theory", "country", "event", "person", "discipline", "chemical element", "protein", "chemical compound"], "instance": {"id": "245", "words": ["There", "are", "global", "climate", "simulation", "models", "that", "have", "been", "written", "for", "Jupiter", ",", "Saturn", ",", "Neptune", "and", "Venus", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, scientist, academic journal, location, enzyme, award, organization, astronomical object, theory, country, event, person, discipline, chemical element, protein, chemical compound and O.\nSentence: There are global climate simulation models that have been written for Jupiter , Saturn , Neptune and Venus .", "prompt_labels": "There(O) are(O) global(O) climate(O) simulation(O) models(O) that(O) have(O) been(O) written(O) for(O) Jupiter(B-astronomical object) ,(O) Saturn(B-astronomical object) ,(O) Neptune(B-astronomical object) and(O) Venus(B-astronomical object) .(O)"}}
{"id": "173", "dataset": "crossner_science", "split": "dev", "label_list": ["scientist", "chemical compound", "person", "enzyme", "protein", "award", "chemical element", "country", "theory", "location", "university", "astronomical object", "academic journal", "discipline", "organization", "event"], "instance": {"id": "173", "words": ["Among", "the", "researchers", "who", "laid", "the", "foundations", "of", "AI", "were", "Alan", "Turing", ",", "John", "von", "Neumann", ",", "Norbert", "Wiener", ",", "Claude", "Shannon", ",", "Warren", "McCullough", ",", "Walter", "Pitts", "and", "Donald", "Hebb", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, chemical compound, person, enzyme, protein, award, chemical element, country, theory, location, university, astronomical object, academic journal, discipline, organization, event and O.\nSentence: Among the researchers who laid the foundations of AI were Alan Turing , John von Neumann , Norbert Wiener , Claude Shannon , Warren McCullough , Walter Pitts and Donald Hebb .", "prompt_labels": "Among(O) the(O) researchers(O) who(O) laid(O) the(O) foundations(O) of(O) AI(O) were(O) Alan(B-scientist) Turing(I-scientist) ,(O) John(B-scientist) von(I-scientist) Neumann(I-scientist) ,(O) Norbert(B-scientist) Wiener(I-scientist) ,(O) Claude(B-scientist) Shannon(I-scientist) ,(O) Warren(B-scientist) McCullough(I-scientist) ,(O) Walter(B-scientist) Pitts(I-scientist) and(O) Donald(B-scientist) Hebb(I-scientist) .(O)"}}
{"id": "121", "dataset": "crossner_science", "split": "dev", "label_list": ["award", "scientist", "discipline", "chemical compound", "chemical element", "country", "location", "academic journal", "event", "astronomical object", "theory", "organization", "enzyme", "protein", "university", "person"], "instance": {"id": "121", "words": ["This", "subcategory", "includes", "Pluto", ",", "Haumea", ",", "Makemake", "and", "Eris", "."], "labels": ["O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, scientist, discipline, chemical compound, chemical element, country, location, academic journal, event, astronomical object, theory, organization, enzyme, protein, university, person and O.\nSentence: This subcategory includes Pluto , Haumea , Makemake and Eris .", "prompt_labels": "This(O) subcategory(O) includes(O) Pluto(B-astronomical object) ,(O) Haumea(B-astronomical object) ,(O) Makemake(B-astronomical object) and(O) Eris(B-astronomical object) .(O)"}}
{"id": "88", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "country", "enzyme", "scientist", "theory", "award", "academic journal", "chemical compound", "person", "event", "discipline", "university", "chemical element", "organization", "protein", "astronomical object"], "instance": {"id": "88", "words": ["The", "name", "was", "suggested", "by", "John", "Herschel", "(", "son", "of", "William", "Herschel", ",", "discoverer", "of", "Mimas", "and", "Enceladus", ")", "in", "his", "1847", "publication", "Results", "of", "Astronomical", "Observations", "made", "at", "the", "Cape", "of", "Good", "Hope", ",", "in", "which", "he", "advocated", "naming", "the", "moons", "of", "Saturn", "after", "the", "Titans", ",", "brothers", "and", "sisters", "of", "the", "Titan", "Cronus", "(", "whom", "the", "Romans", "equated", "with", "their", "god", "Saturn", ")", "."], "labels": ["O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "B-scientist", "I-scientist", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "I-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, enzyme, scientist, theory, award, academic journal, chemical compound, person, event, discipline, university, chemical element, organization, protein, astronomical object and O.\nSentence: The name was suggested by John Herschel ( son of William Herschel , discoverer of Mimas and Enceladus ) in his 1847 publication Results of Astronomical Observations made at the Cape of Good Hope , in which he advocated naming the moons of Saturn after the Titans , brothers and sisters of the Titan Cronus ( whom the Romans equated with their god Saturn ) .", "prompt_labels": "The(O) name(O) was(O) suggested(O) by(O) John(B-scientist) Herschel(I-scientist) ((O) son(O) of(O) William(B-scientist) Herschel(I-scientist) ,(O) discoverer(O) of(O) Mimas(B-astronomical object) and(O) Enceladus(B-astronomical object) )(O) in(O) his(O) 1847(O) publication(O) Results(O) of(O) Astronomical(O) Observations(O) made(O) at(O) the(O) Cape(B-location) of(I-location) Good(I-location) Hope(I-location) ,(O) in(O) which(O) he(O) advocated(O) naming(O) the(O) moons(O) of(O) Saturn(B-astronomical object) after(O) the(O) Titans(B-astronomical object) ,(O) brothers(O) and(O) sisters(O) of(O) the(O) Titan(B-person) Cronus(I-person) ((O) whom(O) the(O) Romans(O) equated(O) with(O) their(O) god(O) Saturn(B-astronomical object) )(O) .(O)"}}
{"id": "365", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "location", "scientist", "chemical element", "enzyme", "discipline", "university", "academic journal", "award", "event", "chemical compound", "country", "organization", "astronomical object", "person", "theory"], "instance": {"id": "365", "words": ["All", "three", "have", "streets", "named", "in", "their", "honor", "as", "does", "President", "Gerald", "Ford", ",", "a", "longtime", "Rancho", "Mirage", "resident", "and", "benefactor", "of", "the", "substance", "abuse", "center", "that", "bears", "his", "wife", "'s", "name", ",", "the", "Betty", "Ford", "Center", "on", "the", "campus", "of", "the", "Eisenhower", "Medical", "Center", ",", "named", "for", "general", ",", "U.S.", "president", "and", "part-time", "resident", "Dwight", "Eisenhower", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-location", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "B-location", "I-location", "I-location", "O", "O", "O", "O", "O", "B-country", "O", "O", "O", "O", "B-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, location, scientist, chemical element, enzyme, discipline, university, academic journal, award, event, chemical compound, country, organization, astronomical object, person, theory and O.\nSentence: All three have streets named in their honor as does President Gerald Ford , a longtime Rancho Mirage resident and benefactor of the substance abuse center that bears his wife 's name , the Betty Ford Center on the campus of the Eisenhower Medical Center , named for general , U.S. president and part-time resident Dwight Eisenhower .", "prompt_labels": "All(O) three(O) have(O) streets(O) named(O) in(O) their(O) honor(O) as(O) does(O) President(O) Gerald(B-person) Ford(I-person) ,(O) a(O) longtime(O) Rancho(B-location) Mirage(B-location) resident(O) and(O) benefactor(O) of(O) the(O) substance(O) abuse(O) center(O) that(O) bears(O) his(O) wife(O) 's(O) name(O) ,(O) the(O) Betty(B-location) Ford(I-location) Center(I-location) on(O) the(O) campus(O) of(O) the(O) Eisenhower(B-location) Medical(I-location) Center(I-location) ,(O) named(O) for(O) general(O) ,(O) U.S.(B-country) president(O) and(O) part-time(O) resident(O) Dwight(B-person) Eisenhower(I-person) .(O)"}}
{"id": "139", "dataset": "crossner_science", "split": "dev", "label_list": ["event", "person", "award", "country", "astronomical object", "enzyme", "organization", "chemical element", "academic journal", "scientist", "university", "chemical compound", "discipline", "protein", "theory", "location"], "instance": {"id": "139", "words": ["Some", "of", "these", "mechanisms", "include", "ATP-dependent", "chromatin", "remodeling", ",", "LINE1", ",", "and", "prion", "protein-based", "modifications", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "O", "B-protein", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, award, country, astronomical object, enzyme, organization, chemical element, academic journal, scientist, university, chemical compound, discipline, protein, theory, location and O.\nSentence: Some of these mechanisms include ATP-dependent chromatin remodeling , LINE1 , and prion protein-based modifications .", "prompt_labels": "Some(O) of(O) these(O) mechanisms(O) include(O) ATP-dependent(O) chromatin(O) remodeling(O) ,(O) LINE1(B-protein) ,(O) and(O) prion(B-protein) protein-based(O) modifications(O) .(O)"}}
{"id": "134", "dataset": "crossner_science", "split": "dev", "label_list": ["organization", "award", "scientist", "academic journal", "event", "theory", "astronomical object", "university", "location", "enzyme", "person", "protein", "discipline", "country", "chemical element", "chemical compound"], "instance": {"id": "134", "words": ["The", "Montreal", "Neurological", "Institute", ",", "the", "former", "Royal", "Victoria", "Hospital", ",", "Allan", "Memorial", "Institute", "and", "the", "Montreal", "General", "Hospital", "of", "McGill", "University", "are", "on", "Pine", "Avenue", ",", "as", "is", "Cormier", "House", ",", "the", "former", "residence", "of", "Pierre", "Elliott", "Trudeau", "."], "labels": ["O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-location", "I-location", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "O", "O", "B-person", "I-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, scientist, academic journal, event, theory, astronomical object, university, location, enzyme, person, protein, discipline, country, chemical element, chemical compound and O.\nSentence: The Montreal Neurological Institute , the former Royal Victoria Hospital , Allan Memorial Institute and the Montreal General Hospital of McGill University are on Pine Avenue , as is Cormier House , the former residence of Pierre Elliott Trudeau .", "prompt_labels": "The(O) Montreal(B-organization) Neurological(I-organization) Institute(I-organization) ,(O) the(O) former(O) Royal(B-organization) Victoria(I-organization) Hospital(I-organization) ,(O) Allan(B-organization) Memorial(I-organization) Institute(I-organization) and(O) the(O) Montreal(B-organization) General(I-organization) Hospital(I-organization) of(I-organization) McGill(I-organization) University(I-organization) are(O) on(O) Pine(B-location) Avenue(I-location) ,(O) as(O) is(O) Cormier(B-location) House(I-location) ,(O) the(O) former(O) residence(O) of(O) Pierre(B-person) Elliott(I-person) Trudeau(I-person) .(O)"}}
{"id": "344", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "event", "discipline", "country", "award", "university", "person", "organization", "chemical element", "academic journal", "theory", "chemical compound", "astronomical object", "scientist", "location", "protein"], "instance": {"id": "344", "words": ["In", "December", "2015", ",", "it", "was", "recognized", "as", "one", "of", "four", "new", "elements", "by", "the", "Joint", "Working", "Party", "of", "the", "international", "scientific", "bodies", "International", "Union", "of", "Pure", "and", "Applied", "Chemistry", "and", "International", "Union", "of", "Pure", "and", "Applied", "Physics", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, event, discipline, country, award, university, person, organization, chemical element, academic journal, theory, chemical compound, astronomical object, scientist, location, protein and O.\nSentence: In December 2015 , it was recognized as one of four new elements by the Joint Working Party of the international scientific bodies International Union of Pure and Applied Chemistry and International Union of Pure and Applied Physics .", "prompt_labels": "In(O) December(O) 2015(O) ,(O) it(O) was(O) recognized(O) as(O) one(O) of(O) four(O) new(O) elements(O) by(O) the(O) Joint(B-organization) Working(I-organization) Party(I-organization) of(O) the(O) international(O) scientific(O) bodies(O) International(B-organization) Union(I-organization) of(I-organization) Pure(I-organization) and(I-organization) Applied(I-organization) Chemistry(I-organization) and(O) International(B-organization) Union(I-organization) of(I-organization) Pure(I-organization) and(I-organization) Applied(I-organization) Physics(I-organization) .(O)"}}
{"id": "156", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "chemical element", "scientist", "academic journal", "enzyme", "location", "award", "chemical compound", "discipline", "organization", "protein", "theory", "person", "event", "astronomical object", "country"], "instance": {"id": "156", "words": ["An", "approved", "residency", "program", "and", "certification", "(", "in", "the", "U.S.", ",", "the", "American", "Board", "of", "Pathology", "or", "the", "American", "Osteopathic", "Board", "of", "Pathology", ")", "is", "usually", "required", "to", "obtain", "employment", "or", "hospital", "privileges", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-country", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, chemical element, scientist, academic journal, enzyme, location, award, chemical compound, discipline, organization, protein, theory, person, event, astronomical object, country and O.\nSentence: An approved residency program and certification ( in the U.S. , the American Board of Pathology or the American Osteopathic Board of Pathology ) is usually required to obtain employment or hospital privileges .", "prompt_labels": "An(O) approved(O) residency(O) program(O) and(O) certification(O) ((O) in(O) the(O) U.S.(B-country) ,(O) the(O) American(B-organization) Board(I-organization) of(I-organization) Pathology(I-organization) or(O) the(O) American(B-organization) Osteopathic(I-organization) Board(I-organization) of(I-organization) Pathology(I-organization) )(O) is(O) usually(O) required(O) to(O) obtain(O) employment(O) or(O) hospital(O) privileges(O) .(O)"}}
{"id": "52", "dataset": "crossner_science", "split": "dev", "label_list": ["person", "chemical compound", "chemical element", "organization", "location", "country", "theory", "enzyme", "event", "discipline", "academic journal", "protein", "astronomical object", "university", "award", "scientist"], "instance": {"id": "52", "words": ["From", "1884", "to", "1888", "he", "studied", "at", "the", "universities", "of", "Humboldt", "University", "of", "Berlin", "and", "University", "of", "Strasbourg", ",", "after", "which", "he", "became", "an", "assistant", "at", "the", "Academy", "of", "Münster", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, chemical compound, chemical element, organization, location, country, theory, enzyme, event, discipline, academic journal, protein, astronomical object, university, award, scientist and O.\nSentence: From 1884 to 1888 he studied at the universities of Humboldt University of Berlin and University of Strasbourg , after which he became an assistant at the Academy of Münster .", "prompt_labels": "From(O) 1884(O) to(O) 1888(O) he(O) studied(O) at(O) the(O) universities(O) of(O) Humboldt(B-university) University(I-university) of(I-university) Berlin(I-university) and(O) University(B-university) of(I-university) Strasbourg(I-university) ,(O) after(O) which(O) he(O) became(O) an(O) assistant(O) at(O) the(O) Academy(B-organization) of(I-organization) Münster(I-organization) .(O)"}}
{"id": "378", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "person", "chemical element", "location", "astronomical object", "theory", "enzyme", "award", "scientist", "protein", "university", "organization", "event", "discipline", "chemical compound", "country"], "instance": {"id": "378", "words": ["The", "town", "was", "frequented", "by", "notable", "Old", "West", "personalities", ",", "including", "Dave", "Rudabaugh", ",", "Billy", "the", "Kid", ",", "Pat", "Garrett", ",", "and", "Shotgun", "John", "Collins", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "I-person", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, person, chemical element, location, astronomical object, theory, enzyme, award, scientist, protein, university, organization, event, discipline, chemical compound, country and O.\nSentence: The town was frequented by notable Old West personalities , including Dave Rudabaugh , Billy the Kid , Pat Garrett , and Shotgun John Collins .", "prompt_labels": "The(O) town(O) was(O) frequented(O) by(O) notable(O) Old(O) West(O) personalities(O) ,(O) including(O) Dave(B-person) Rudabaugh(I-person) ,(O) Billy(B-person) the(I-person) Kid(I-person) ,(O) Pat(B-person) Garrett(I-person) ,(O) and(O) Shotgun(B-person) John(I-person) Collins(I-person) .(O)"}}
{"id": "13", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "chemical element", "event", "academic journal", "country", "scientist", "location", "protein", "person", "astronomical object", "award", "organization", "discipline", "chemical compound", "enzyme", "theory"], "instance": {"id": "13", "words": ["In", "addition", "to", "his", "steady", "research", "output", ",", "Naqvi", "has", "manifested", "his", "commitment", "to", "teaching", "by", "contributing", "to", "journals", "devoted", "to", "didactical", "aspects", "of", "science", "(", "American", "Journal", "of", "Physics", ",", "European", "Journal", "of", "Physics", ",", "Journal", "of", "Chemical", "Education", ")", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, chemical element, event, academic journal, country, scientist, location, protein, person, astronomical object, award, organization, discipline, chemical compound, enzyme, theory and O.\nSentence: In addition to his steady research output , Naqvi has manifested his commitment to teaching by contributing to journals devoted to didactical aspects of science ( American Journal of Physics , European Journal of Physics , Journal of Chemical Education ) .", "prompt_labels": "In(O) addition(O) to(O) his(O) steady(O) research(O) output(O) ,(O) Naqvi(B-scientist) has(O) manifested(O) his(O) commitment(O) to(O) teaching(O) by(O) contributing(O) to(O) journals(O) devoted(O) to(O) didactical(O) aspects(O) of(O) science(O) ((O) American(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physics(I-academic journal) ,(O) European(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physics(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Chemical(I-academic journal) Education(I-academic journal) )(O) .(O)"}}
{"id": "140", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "country", "organization", "chemical compound", "discipline", "university", "event", "academic journal", "enzyme", "chemical element", "award", "astronomical object", "person", "location", "scientist", "theory"], "instance": {"id": "140", "words": ["Later", ",", "scientists", "such", "as", "Ludwig", "Boltzmann", ",", "Josiah", "Willard", "Gibbs", ",", "and", "James", "Clerk", "Maxwell", "gave", "entropy", "a", "statistical", "basis", "."], "labels": ["O", "O", "O", "O", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, country, organization, chemical compound, discipline, university, event, academic journal, enzyme, chemical element, award, astronomical object, person, location, scientist, theory and O.\nSentence: Later , scientists such as Ludwig Boltzmann , Josiah Willard Gibbs , and James Clerk Maxwell gave entropy a statistical basis .", "prompt_labels": "Later(O) ,(O) scientists(O) such(O) as(O) Ludwig(B-scientist) Boltzmann(I-scientist) ,(O) Josiah(B-scientist) Willard(I-scientist) Gibbs(I-scientist) ,(O) and(O) James(B-scientist) Clerk(I-scientist) Maxwell(I-scientist) gave(O) entropy(O) a(O) statistical(O) basis(O) .(O)"}}
{"id": "370", "dataset": "crossner_science", "split": "dev", "label_list": ["academic journal", "chemical element", "astronomical object", "chemical compound", "person", "scientist", "enzyme", "protein", "organization", "theory", "country", "location", "event", "award", "university", "discipline"], "instance": {"id": "370", "words": ["Silindo", "may", "be", "Jupiter", ";", "Carnil", ",", "Mars", ";", "Elemmire", ",", "Mercury", ";", "Luinil", ",", "Uranus", ";", "Lumbar", ",", "Saturn", ";", "and", "Nenar", ",", "Neptune", "."], "labels": ["O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, chemical element, astronomical object, chemical compound, person, scientist, enzyme, protein, organization, theory, country, location, event, award, university, discipline and O.\nSentence: Silindo may be Jupiter ; Carnil , Mars ; Elemmire , Mercury ; Luinil , Uranus ; Lumbar , Saturn ; and Nenar , Neptune .", "prompt_labels": "Silindo(O) may(O) be(O) Jupiter(B-astronomical object) ;(O) Carnil(B-astronomical object) ,(O) Mars(B-astronomical object) ;(O) Elemmire(B-astronomical object) ,(O) Mercury(B-astronomical object) ;(O) Luinil(B-astronomical object) ,(O) Uranus(B-astronomical object) ;(O) Lumbar(B-astronomical object) ,(O) Saturn(B-astronomical object) ;(O) and(O) Nenar(B-astronomical object) ,(O) Neptune(B-astronomical object) .(O)"}}
{"id": "190", "dataset": "crossner_science", "split": "dev", "label_list": ["organization", "location", "award", "academic journal", "astronomical object", "person", "enzyme", "event", "theory", "scientist", "chemical compound", "discipline", "country", "protein", "university", "chemical element"], "instance": {"id": "190", "words": ["Assuming", "its", "TRUE", "mass", "is", "comparable", "to", "those", "of", "Neptune", "and", "Gliese", "436", "b", ",", "14", "Earth", "masses", "is", "theoretically", "the", "maximum", "size", "for", "a", "terrestrial", "planet", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "I-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, award, academic journal, astronomical object, person, enzyme, event, theory, scientist, chemical compound, discipline, country, protein, university, chemical element and O.\nSentence: Assuming its TRUE mass is comparable to those of Neptune and Gliese 436 b , 14 Earth masses is theoretically the maximum size for a terrestrial planet .", "prompt_labels": "Assuming(O) its(O) TRUE(O) mass(O) is(O) comparable(O) to(O) those(O) of(O) Neptune(B-astronomical object) and(O) Gliese(B-astronomical object) 436(I-astronomical object) b(I-astronomical object) ,(O) 14(O) Earth(O) masses(O) is(O) theoretically(O) the(O) maximum(O) size(O) for(O) a(O) terrestrial(O) planet(O) .(O)"}}
{"id": "179", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "chemical element", "protein", "scientist", "chemical compound", "academic journal", "location", "person", "award", "astronomical object", "organization", "theory", "university", "event", "country", "enzyme"], "instance": {"id": "179", "words": ["In", "1955", ",", "he", "was", "elected", "a", "Fellow", "of", "the", "Royal", "Society", "and", "served", "on", "the", "Council", "of", "the", "Royal", "Society", "from", "1960", "to", "1962", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical element, protein, scientist, chemical compound, academic journal, location, person, award, astronomical object, organization, theory, university, event, country, enzyme and O.\nSentence: In 1955 , he was elected a Fellow of the Royal Society and served on the Council of the Royal Society from 1960 to 1962 .", "prompt_labels": "In(O) 1955(O) ,(O) he(O) was(O) elected(O) a(O) Fellow(B-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) and(O) served(O) on(O) the(O) Council(B-organization) of(I-organization) the(I-organization) Royal(I-organization) Society(I-organization) from(O) 1960(O) to(O) 1962(O) .(O)"}}
{"id": "298", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "chemical compound", "person", "enzyme", "location", "discipline", "chemical element", "academic journal", "award", "scientist", "university", "event", "protein", "organization", "country", "theory"], "instance": {"id": "298", "words": ["Activated", "DHPR", "'s", "open", ",", "forming", "a", "L-type", "calcium", "channel", ",", "that", "allows", "Casup2", "+", "/", "sup", "to", "pass", "into", "the", "cell", "."], "labels": ["O", "B-chemical compound", "O", "O", "O", "O", "O", "B-protein", "I-protein", "I-protein", "O", "O", "O", "B-chemical element", "I-chemical element", "I-chemical element", "I-chemical element", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, chemical compound, person, enzyme, location, discipline, chemical element, academic journal, award, scientist, university, event, protein, organization, country, theory and O.\nSentence: Activated DHPR 's open , forming a L-type calcium channel , that allows Casup2 + / sup to pass into the cell .", "prompt_labels": "Activated(O) DHPR(B-chemical compound) 's(O) open(O) ,(O) forming(O) a(O) L-type(B-protein) calcium(I-protein) channel(I-protein) ,(O) that(O) allows(O) Casup2(B-chemical element) +(I-chemical element) /(I-chemical element) sup(I-chemical element) to(O) pass(O) into(O) the(O) cell(O) .(O)"}}
{"id": "18", "dataset": "crossner_science", "split": "dev", "label_list": ["scientist", "academic journal", "event", "organization", "location", "chemical compound", "protein", "theory", "country", "astronomical object", "university", "person", "award", "discipline", "chemical element", "enzyme"], "instance": {"id": "18", "words": ["Gerty", "Theresa", "Cori", "(", "née", "Radnitz", ";", "August", "15", ",", "1896", "-", "October", "26", ",", "1957", ")", "was", "a", "Austria-Hungary", "-", "American", "biochemist", "who", "in", "1947", "was", "the", "third", "woman", "-", "and", "first", "American", "woman", "-", "to", "win", "a", "Nobel", "Prize", "in", "science", ",", "and", "the", "first", "woman", "to", "be", "awarded", "the", "Nobel", "Prize", "in", "Physiology", "or", "Medicine", ",", "for", "her", "role", "in", "the", "discovery", "of", "glycogen", "metabolism", "."], "labels": ["B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, academic journal, event, organization, location, chemical compound, protein, theory, country, astronomical object, university, person, award, discipline, chemical element, enzyme and O.\nSentence: Gerty Theresa Cori ( née Radnitz ; August 15 , 1896 - October 26 , 1957 ) was a Austria-Hungary - American biochemist who in 1947 was the third woman - and first American woman - to win a Nobel Prize in science , and the first woman to be awarded the Nobel Prize in Physiology or Medicine , for her role in the discovery of glycogen metabolism .", "prompt_labels": "Gerty(B-scientist) Theresa(I-scientist) Cori(I-scientist) ((O) née(B-scientist) Radnitz(I-scientist) ;(O) August(O) 15(O) ,(O) 1896(O) -(O) October(O) 26(O) ,(O) 1957(O) )(O) was(O) a(O) Austria-Hungary(O) -(O) American(O) biochemist(O) who(O) in(O) 1947(O) was(O) the(O) third(O) woman(O) -(O) and(O) first(O) American(O) woman(O) -(O) to(O) win(O) a(O) Nobel(B-award) Prize(I-award) in(O) science(O) ,(O) and(O) the(O) first(O) woman(O) to(O) be(O) awarded(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Physiology(I-award) or(I-award) Medicine(I-award) ,(O) for(O) her(O) role(O) in(O) the(O) discovery(O) of(O) glycogen(O) metabolism(O) .(O)"}}
{"id": "406", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "university", "discipline", "astronomical object", "scientist", "event", "person", "protein", "location", "award", "country", "chemical compound", "enzyme", "organization", "chemical element", "academic journal"], "instance": {"id": "406", "words": ["By", "being", "an", "AMP-activated", "protein", "kinase", "activator", "like", "Metformin", ",", "it", "acts", "similar", ",", "affecting", "metabolism", "in", "a", "way", "that", "may", "reveal", "useful", "applications", "to", "treat", "."], "labels": ["O", "O", "O", "B-enzyme", "I-enzyme", "I-enzyme", "O", "O", "B-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, university, discipline, astronomical object, scientist, event, person, protein, location, award, country, chemical compound, enzyme, organization, chemical element, academic journal and O.\nSentence: By being an AMP-activated protein kinase activator like Metformin , it acts similar , affecting metabolism in a way that may reveal useful applications to treat .", "prompt_labels": "By(O) being(O) an(O) AMP-activated(B-enzyme) protein(I-enzyme) kinase(I-enzyme) activator(O) like(O) Metformin(B-enzyme) ,(O) it(O) acts(O) similar(O) ,(O) affecting(O) metabolism(O) in(O) a(O) way(O) that(O) may(O) reveal(O) useful(O) applications(O) to(O) treat(O) .(O)"}}
{"id": "117", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "award", "theory", "discipline", "event", "country", "academic journal", "astronomical object", "person", "enzyme", "location", "organization", "scientist", "university", "chemical compound", "protein"], "instance": {"id": "117", "words": ["Next", "stations", "were", "the", "University", "of", "Vienna", ",", "the", "University", "of", "Kraków", "and", "finally", "the", "University", "of", "Göttingen", ",", "where", "he", "studied", "mathematics", "under", "David", "Hilbert", ",", "Woldemar", "Voigt", ",", "Walther", "Nernst", ",", "Karl", "Schwarzschild", "and", "Hermann", "Minkowski", "."], "labels": ["O", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "B-university", "I-university", "I-university", "O", "O", "O", "O", "B-discipline", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, award, theory, discipline, event, country, academic journal, astronomical object, person, enzyme, location, organization, scientist, university, chemical compound, protein and O.\nSentence: Next stations were the University of Vienna , the University of Kraków and finally the University of Göttingen , where he studied mathematics under David Hilbert , Woldemar Voigt , Walther Nernst , Karl Schwarzschild and Hermann Minkowski .", "prompt_labels": "Next(O) stations(O) were(O) the(O) University(B-university) of(I-university) Vienna(I-university) ,(O) the(O) University(B-university) of(I-university) Kraków(I-university) and(O) finally(O) the(O) University(B-university) of(I-university) Göttingen(I-university) ,(O) where(O) he(O) studied(O) mathematics(B-discipline) under(O) David(B-scientist) Hilbert(I-scientist) ,(O) Woldemar(B-scientist) Voigt(I-scientist) ,(O) Walther(B-scientist) Nernst(I-scientist) ,(O) Karl(B-scientist) Schwarzschild(I-scientist) and(O) Hermann(B-scientist) Minkowski(I-scientist) .(O)"}}
{"id": "157", "dataset": "crossner_science", "split": "dev", "label_list": ["university", "chemical compound", "theory", "enzyme", "country", "astronomical object", "chemical element", "organization", "protein", "discipline", "academic journal", "person", "scientist", "award", "event", "location"], "instance": {"id": "157", "words": ["Singer", "is", "a", "member", "of", "the", "National", "Academy", "of", "Sciences", "and", "the", "American", "Academy", "of", "Arts", "and", "Sciences", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, chemical compound, theory, enzyme, country, astronomical object, chemical element, organization, protein, discipline, academic journal, person, scientist, award, event, location and O.\nSentence: Singer is a member of the National Academy of Sciences and the American Academy of Arts and Sciences .", "prompt_labels": "Singer(B-scientist) is(O) a(O) member(O) of(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) and(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) .(O)"}}
{"id": "160", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "university", "chemical compound", "scientist", "theory", "astronomical object", "enzyme", "country", "person", "location", "protein", "organization", "event", "award", "discipline", "academic journal"], "instance": {"id": "160", "words": ["Models", "of", "heat", "retention", "and", "heating", "via", "radioactive", "decay", "in", "smaller", "icy", "Solar", "System", "bodies", "suggest", "that", "Rhea", ",", "Titania", ",", "Oberon", ",", "Triton", ",", "Pluto", ",", "Eris", ",", "90377", "Sedna", ",", "and", "90482", "Orcus", "may", "have", "oceans", "underneath", "solid", "icy", "crusts", "approximately", "100", "km", "thick", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "O", "B-astronomical object", "I-astronomical object", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, university, chemical compound, scientist, theory, astronomical object, enzyme, country, person, location, protein, organization, event, award, discipline, academic journal and O.\nSentence: Models of heat retention and heating via radioactive decay in smaller icy Solar System bodies suggest that Rhea , Titania , Oberon , Triton , Pluto , Eris , 90377 Sedna , and 90482 Orcus may have oceans underneath solid icy crusts approximately 100 km thick .", "prompt_labels": "Models(O) of(O) heat(O) retention(O) and(O) heating(O) via(O) radioactive(O) decay(O) in(O) smaller(O) icy(O) Solar(O) System(O) bodies(O) suggest(O) that(O) Rhea(B-astronomical object) ,(O) Titania(B-astronomical object) ,(O) Oberon(B-astronomical object) ,(O) Triton(B-astronomical object) ,(O) Pluto(B-astronomical object) ,(O) Eris(B-astronomical object) ,(O) 90377(B-astronomical object) Sedna(I-astronomical object) ,(O) and(O) 90482(B-astronomical object) Orcus(I-astronomical object) may(O) have(O) oceans(O) underneath(O) solid(O) icy(O) crusts(O) approximately(O) 100(O) km(O) thick(O) .(O)"}}
{"id": "357", "dataset": "crossner_science", "split": "dev", "label_list": ["discipline", "organization", "person", "event", "theory", "chemical element", "protein", "academic journal", "university", "astronomical object", "country", "location", "enzyme", "award", "chemical compound", "scientist"], "instance": {"id": "357", "words": ["Cyclin", "and", "Cyclin-dependent", "kinase", "(", "CDK", ")", "are", "major", "positive", "regulators", ",", "and", "appear", "throughout", "the", "cell", "cycle", "."], "labels": ["B-protein", "O", "B-protein", "I-protein", "O", "B-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, organization, person, event, theory, chemical element, protein, academic journal, university, astronomical object, country, location, enzyme, award, chemical compound, scientist and O.\nSentence: Cyclin and Cyclin-dependent kinase ( CDK ) are major positive regulators , and appear throughout the cell cycle .", "prompt_labels": "Cyclin(B-protein) and(O) Cyclin-dependent(B-protein) kinase(I-protein) ((O) CDK(B-protein) )(O) are(O) major(O) positive(O) regulators(O) ,(O) and(O) appear(O) throughout(O) the(O) cell(O) cycle(O) .(O)"}}
{"id": "409", "dataset": "crossner_science", "split": "dev", "label_list": ["event", "person", "scientist", "protein", "academic journal", "organization", "award", "university", "theory", "location", "chemical element", "astronomical object", "discipline", "chemical compound", "country", "enzyme"], "instance": {"id": "409", "words": ["He", "won", "silver", "medals", "at", "the", "1989", "Southeast", "Asian", "Games", ",", "1991", "Southeast", "Asian", "Games", ",", "and", "1993", "Southeast", "Asian", "Games", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "I-event", "O", "O", "B-event", "I-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, scientist, protein, academic journal, organization, award, university, theory, location, chemical element, astronomical object, discipline, chemical compound, country, enzyme and O.\nSentence: He won silver medals at the 1989 Southeast Asian Games , 1991 Southeast Asian Games , and 1993 Southeast Asian Games .", "prompt_labels": "He(O) won(O) silver(O) medals(O) at(O) the(O) 1989(B-event) Southeast(I-event) Asian(I-event) Games(I-event) ,(O) 1991(B-event) Southeast(I-event) Asian(I-event) Games(I-event) ,(O) and(O) 1993(B-event) Southeast(I-event) Asian(I-event) Games(I-event) .(O)"}}
{"id": "174", "dataset": "crossner_science", "split": "dev", "label_list": ["organization", "theory", "event", "protein", "country", "discipline", "university", "person", "award", "chemical element", "location", "chemical compound", "scientist", "academic journal", "astronomical object", "enzyme"], "instance": {"id": "174", "words": ["The", "two", "factions", ",", "Lawrence", "Murphy", "-", "Dolan", "and", "John", "Tunstall", "-", "McSween", ",", "fought", "a", "series", "of", "escalating", "battles", "with", "such", "murderous", "ferocity", "that", "the", "repercussions", "were", "felt", "as", "far", "away", "as", "the", "state", "capital", "Santa", "Fe", "and", "even", "in", "Washington", ",", "D.C."], "labels": ["O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "O", "B-person", "I-person", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-location", "I-location", "O", "O", "O", "B-location", "I-location", "I-location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, theory, event, protein, country, discipline, university, person, award, chemical element, location, chemical compound, scientist, academic journal, astronomical object, enzyme and O.\nSentence: The two factions , Lawrence Murphy - Dolan and John Tunstall - McSween , fought a series of escalating battles with such murderous ferocity that the repercussions were felt as far away as the state capital Santa Fe and even in Washington , D.C.", "prompt_labels": "The(O) two(O) factions(O) ,(O) Lawrence(B-person) Murphy(I-person) -(O) Dolan(B-person) and(O) John(B-person) Tunstall(I-person) -(O) McSween(B-person) ,(O) fought(O) a(O) series(O) of(O) escalating(O) battles(O) with(O) such(O) murderous(O) ferocity(O) that(O) the(O) repercussions(O) were(O) felt(O) as(O) far(O) away(O) as(O) the(O) state(O) capital(O) Santa(B-location) Fe(I-location) and(O) even(O) in(O) Washington(B-location) ,(I-location) D.C.(I-location)"}}
{"id": "418", "dataset": "crossner_science", "split": "dev", "label_list": ["location", "award", "organization", "discipline", "university", "enzyme", "country", "event", "protein", "chemical compound", "astronomical object", "person", "theory", "scientist", "chemical element", "academic journal"], "instance": {"id": "418", "words": ["The", "disease", "is", "characterized", "behaviorally", "by", "chronic", "and", "progressive", "decline", "in", "cognitive", "function", ",", "beginning", "with", "short", "term", "memory", "loss", ",", "and", "neurologically", "by", "buildup", "of", "misfolded", "tau", "protein", "and", "associated", "neurofibrillary", "tangles", ",", "and", "by", "amyloid-beta", "senile", "plaques", "Amyloid", "beta", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, organization, discipline, university, enzyme, country, event, protein, chemical compound, astronomical object, person, theory, scientist, chemical element, academic journal and O.\nSentence: The disease is characterized behaviorally by chronic and progressive decline in cognitive function , beginning with short term memory loss , and neurologically by buildup of misfolded tau protein and associated neurofibrillary tangles , and by amyloid-beta senile plaques Amyloid beta .", "prompt_labels": "The(O) disease(O) is(O) characterized(O) behaviorally(O) by(O) chronic(O) and(O) progressive(O) decline(O) in(O) cognitive(O) function(O) ,(O) beginning(O) with(O) short(O) term(O) memory(O) loss(O) ,(O) and(O) neurologically(O) by(O) buildup(O) of(O) misfolded(O) tau(B-protein) protein(I-protein) and(O) associated(O) neurofibrillary(O) tangles(O) ,(O) and(O) by(O) amyloid-beta(O) senile(O) plaques(O) Amyloid(B-protein) beta(I-protein) .(O)"}}
{"id": "80", "dataset": "crossner_science", "split": "dev", "label_list": ["organization", "university", "event", "chemical compound", "enzyme", "country", "scientist", "person", "astronomical object", "theory", "location", "chemical element", "discipline", "protein", "academic journal", "award"], "instance": {"id": "80", "words": ["(", "1976", ")", ";", "Guggenheim", "Fellow", "(", "1977", ")", ";", "Member", "National", "Academy", "of", "Sciences", "(", "1979", ")", ";", "Member", "International", "Academy", "of", "Science", ",", "Member", "Academia", "Sinica", "(", "1980", ")", ";", "E.O.", "Lawrence", "Award", "(", "1981", ")", ";", "Miller", "Professor", ",", "Berkeley", "(", "1981", ")", ";", "Fairchild", "Distinguished", "Scholar", "(", "1983", ")", ";", "Harrison", "Howe", "Award", "(", "1983", ")", ";", "Peter", "Debye", "Award", "(", "1986", ")", ";", "National", "Medal", "of", "Science", "(", "1986", ")", "."], "labels": ["O", "O", "O", "O", "B-award", "I-award", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "B-scientist", "O", "O", "B-university", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, university, event, chemical compound, enzyme, country, scientist, person, astronomical object, theory, location, chemical element, discipline, protein, academic journal, award and O.\nSentence: ( 1976 ) ; Guggenheim Fellow ( 1977 ) ; Member National Academy of Sciences ( 1979 ) ; Member International Academy of Science , Member Academia Sinica ( 1980 ) ; E.O. Lawrence Award ( 1981 ) ; Miller Professor , Berkeley ( 1981 ) ; Fairchild Distinguished Scholar ( 1983 ) ; Harrison Howe Award ( 1983 ) ; Peter Debye Award ( 1986 ) ; National Medal of Science ( 1986 ) .", "prompt_labels": "((O) 1976(O) )(O) ;(O) Guggenheim(B-award) Fellow(I-award) ((O) 1977(O) )(O) ;(O) Member(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ((O) 1979(O) )(O) ;(O) Member(O) International(B-organization) Academy(I-organization) of(I-organization) Science(I-organization) ,(O) Member(O) Academia(B-organization) Sinica(I-organization) ((O) 1980(O) )(O) ;(O) E.O.(B-award) Lawrence(I-award) Award(I-award) ((O) 1981(O) )(O) ;(O) Miller(B-scientist) Professor(O) ,(O) Berkeley(B-university) ((O) 1981(O) )(O) ;(O) Fairchild(B-award) Distinguished(I-award) Scholar(I-award) ((O) 1983(O) )(O) ;(O) Harrison(B-award) Howe(I-award) Award(I-award) ((O) 1983(O) )(O) ;(O) Peter(B-award) Debye(I-award) Award(I-award) ((O) 1986(O) )(O) ;(O) National(B-award) Medal(I-award) of(I-award) Science(I-award) ((O) 1986(O) )(O) .(O)"}}
{"id": "246", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "organization", "discipline", "chemical compound", "astronomical object", "enzyme", "chemical element", "award", "location", "theory", "event", "scientist", "person", "academic journal", "country", "university"], "instance": {"id": "246", "words": ["Each", "chain", "has", "a", "PAS", "domain", ",", "GAF", "domain", "and", "PHY", "domain", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, organization, discipline, chemical compound, astronomical object, enzyme, chemical element, award, location, theory, event, scientist, person, academic journal, country, university and O.\nSentence: Each chain has a PAS domain , GAF domain and PHY domain .", "prompt_labels": "Each(O) chain(O) has(O) a(O) PAS(O) domain(O) ,(O) GAF(O) domain(O) and(O) PHY(O) domain(O) .(O)"}}
{"id": "437", "dataset": "crossner_science", "split": "dev", "label_list": ["organization", "academic journal", "astronomical object", "discipline", "chemical compound", "country", "person", "chemical element", "event", "location", "scientist", "enzyme", "award", "university", "protein", "theory"], "instance": {"id": "437", "words": ["It", "was", "discovered", "on", "26", "September", "1960", ",", "by", "Ingrid", "van", "Houten-Groeneveld", "and", "Cornelis", "van", "Houten", "at", "Leiden", ",", "and", "Tom", "Gehrels", "at", "Palomar", "Observatory", "in", "California", ",", "United", "States", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-scientist", "I-scientist", "I-scientist", "O", "B-location", "O", "O", "B-scientist", "I-scientist", "O", "B-location", "I-location", "O", "B-location", "O", "B-country", "I-country", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, academic journal, astronomical object, discipline, chemical compound, country, person, chemical element, event, location, scientist, enzyme, award, university, protein, theory and O.\nSentence: It was discovered on 26 September 1960 , by Ingrid van Houten-Groeneveld and Cornelis van Houten at Leiden , and Tom Gehrels at Palomar Observatory in California , United States .", "prompt_labels": "It(O) was(O) discovered(O) on(O) 26(O) September(O) 1960(O) ,(O) by(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Cornelis(B-scientist) van(I-scientist) Houten(I-scientist) at(O) Leiden(B-location) ,(O) and(O) Tom(B-scientist) Gehrels(I-scientist) at(O) Palomar(B-location) Observatory(I-location) in(O) California(B-location) ,(O) United(B-country) States(I-country) .(O)"}}
{"id": "345", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "academic journal", "person", "chemical compound", "theory", "country", "protein", "award", "university", "enzyme", "location", "discipline", "scientist", "organization", "event", "chemical element"], "instance": {"id": "345", "words": ["Finkelnburg", "invited", "five", "representatives", "to", "make", "arguments", "for", "theoretical", "physics", "and", "academic", "decisions", "based", "on", "ability", ",", "rather", "than", "politics", ":", "Carl", "Friedrich", "von", "Weizsäcker", ",", "Otto", "Scherzer", ",", "Georg", "Joos", ",", "Otto", "Heckmann", ",", "and", "Hans", "Kopfermann", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "I-discipline", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-scientist", "I-scientist", "I-scientist", "I-scientist", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-scientist", "I-scientist", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, academic journal, person, chemical compound, theory, country, protein, award, university, enzyme, location, discipline, scientist, organization, event, chemical element and O.\nSentence: Finkelnburg invited five representatives to make arguments for theoretical physics and academic decisions based on ability , rather than politics : Carl Friedrich von Weizsäcker , Otto Scherzer , Georg Joos , Otto Heckmann , and Hans Kopfermann .", "prompt_labels": "Finkelnburg(B-scientist) invited(O) five(O) representatives(O) to(O) make(O) arguments(O) for(O) theoretical(B-discipline) physics(I-discipline) and(O) academic(O) decisions(O) based(O) on(O) ability(O) ,(O) rather(O) than(O) politics(O) :(O) Carl(B-scientist) Friedrich(I-scientist) von(I-scientist) Weizsäcker(I-scientist) ,(O) Otto(B-person) Scherzer(I-person) ,(O) Georg(B-person) Joos(I-person) ,(O) Otto(B-person) Heckmann(I-person) ,(O) and(O) Hans(B-scientist) Kopfermann(I-scientist) .(O)"}}
{"id": "263", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "organization", "chemical element", "country", "university", "scientist", "chemical compound", "location", "enzyme", "event", "award", "academic journal", "person", "theory", "protein", "discipline"], "instance": {"id": "263", "words": ["These", "can", "be", "further", "subdivided", "into", "the", "gas", "giant", "s", "(", "Jupiter", "and", "Saturn", ")", "and", "the", "ice", "giant", "s", "(", "Uranus", "and", "Neptune", ")", "that", "have", "large", "icy", "cores", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, organization, chemical element, country, university, scientist, chemical compound, location, enzyme, event, award, academic journal, person, theory, protein, discipline and O.\nSentence: These can be further subdivided into the gas giant s ( Jupiter and Saturn ) and the ice giant s ( Uranus and Neptune ) that have large icy cores .", "prompt_labels": "These(O) can(O) be(O) further(O) subdivided(O) into(O) the(O) gas(O) giant(O) s(O) ((O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) )(O) and(O) the(O) ice(O) giant(O) s(O) ((O) Uranus(B-astronomical object) and(O) Neptune(B-astronomical object) )(O) that(O) have(O) large(O) icy(O) cores(O) .(O)"}}
{"id": "416", "dataset": "crossner_science", "split": "dev", "label_list": ["theory", "event", "country", "chemical element", "scientist", "university", "astronomical object", "organization", "chemical compound", "protein", "location", "person", "award", "academic journal", "enzyme", "discipline"], "instance": {"id": "416", "words": ["H3K4me3", "is", "an", "epigenetic", "modification", "to", "the", "DNA", "packaging", "protein", "Histone", "H3", "."], "labels": ["B-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-protein", "I-protein", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, event, country, chemical element, scientist, university, astronomical object, organization, chemical compound, protein, location, person, award, academic journal, enzyme, discipline and O.\nSentence: H3K4me3 is an epigenetic modification to the DNA packaging protein Histone H3 .", "prompt_labels": "H3K4me3(B-chemical compound) is(O) an(O) epigenetic(O) modification(O) to(O) the(O) DNA(O) packaging(O) protein(O) Histone(B-protein) H3(I-protein) .(O)"}}
{"id": "292", "dataset": "crossner_science", "split": "dev", "label_list": ["person", "location", "protein", "event", "chemical compound", "theory", "enzyme", "university", "award", "organization", "astronomical object", "country", "discipline", "chemical element", "academic journal", "scientist"], "instance": {"id": "292", "words": ["The", "citric", "acid", "cycle", "produces", "Nicotinamide", "adenine", "dinucleotide", "and", "Flavin", "adenine", "dinucleotide", "through", "oxidation", "that", "will", "be", "reduced", "in", "oxidative", "phosphorylation", "to", "produce", "Adenosine", "triphosphate", "."], "labels": ["O", "B-chemical compound", "I-chemical compound", "O", "O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "O", "B-chemical compound", "I-chemical compound", "I-chemical compound", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-chemical compound", "I-chemical compound", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, protein, event, chemical compound, theory, enzyme, university, award, organization, astronomical object, country, discipline, chemical element, academic journal, scientist and O.\nSentence: The citric acid cycle produces Nicotinamide adenine dinucleotide and Flavin adenine dinucleotide through oxidation that will be reduced in oxidative phosphorylation to produce Adenosine triphosphate .", "prompt_labels": "The(O) citric(B-chemical compound) acid(I-chemical compound) cycle(O) produces(O) Nicotinamide(B-chemical compound) adenine(I-chemical compound) dinucleotide(I-chemical compound) and(O) Flavin(B-chemical compound) adenine(I-chemical compound) dinucleotide(I-chemical compound) through(O) oxidation(O) that(O) will(O) be(O) reduced(O) in(O) oxidative(O) phosphorylation(O) to(O) produce(O) Adenosine(B-chemical compound) triphosphate(I-chemical compound) .(O)"}}
{"id": "58", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "event", "location", "award", "scientist", "chemical compound", "astronomical object", "discipline", "chemical element", "protein", "person", "university", "academic journal", "country", "organization", "theory"], "instance": {"id": "58", "words": ["Thompson", "endowed", "the", "Rumford", "medal", "s", "of", "the", "Royal", "Society", "and", "the", "American", "Academy", "of", "Arts", "and", "Sciences", ",", "and", "endowed", "a", "professorship", "at", "Harvard", "University", "."], "labels": ["B-scientist", "O", "O", "B-award", "I-award", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, event, location, award, scientist, chemical compound, astronomical object, discipline, chemical element, protein, person, university, academic journal, country, organization, theory and O.\nSentence: Thompson endowed the Rumford medal s of the Royal Society and the American Academy of Arts and Sciences , and endowed a professorship at Harvard University .", "prompt_labels": "Thompson(B-scientist) endowed(O) the(O) Rumford(B-award) medal(I-award) s(O) of(O) the(O) Royal(B-organization) Society(I-organization) and(O) the(O) American(B-organization) Academy(I-organization) of(I-organization) Arts(I-organization) and(I-organization) Sciences(I-organization) ,(O) and(O) endowed(O) a(O) professorship(O) at(O) Harvard(B-university) University(I-university) .(O)"}}
{"id": "3", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "organization", "chemical compound", "astronomical object", "country", "location", "award", "discipline", "person", "academic journal", "university", "scientist", "event", "chemical element", "theory", "enzyme"], "instance": {"id": "3", "words": ["He", "attended", "the", "U.S.", "Air", "Force", "Institute", "of", "Technology", "for", "a", "year", ",", "earning", "a", "bachelor", "'s", "degree", "in", "aeromechanics", ",", "and", "received", "his", "test", "pilot", "training", "at", "Edwards", "Air", "Force", "Base", "in", "California", "before", "his", "assignment", "as", "a", "test", "pilot", "at", "Wright-Patterson", "Air", "Force", "Base", "in", "Ohio", "."], "labels": ["O", "O", "O", "B-university", "I-university", "I-university", "I-university", "I-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-discipline", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-location", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, organization, chemical compound, astronomical object, country, location, award, discipline, person, academic journal, university, scientist, event, chemical element, theory, enzyme and O.\nSentence: He attended the U.S. Air Force Institute of Technology for a year , earning a bachelor 's degree in aeromechanics , and received his test pilot training at Edwards Air Force Base in California before his assignment as a test pilot at Wright-Patterson Air Force Base in Ohio .", "prompt_labels": "He(O) attended(O) the(O) U.S.(B-university) Air(I-university) Force(I-university) Institute(I-university) of(I-university) Technology(I-university) for(O) a(O) year(O) ,(O) earning(O) a(O) bachelor(O) 's(O) degree(O) in(O) aeromechanics(B-discipline) ,(O) and(O) received(O) his(O) test(O) pilot(O) training(O) at(O) Edwards(B-organization) Air(I-organization) Force(I-organization) Base(I-organization) in(O) California(B-location) before(O) his(O) assignment(O) as(O) a(O) test(O) pilot(O) at(O) Wright-Patterson(B-organization) Air(I-organization) Force(I-organization) Base(I-organization) in(O) Ohio(B-location) .(O)"}}
{"id": "76", "dataset": "crossner_science", "split": "dev", "label_list": ["event", "award", "location", "theory", "enzyme", "astronomical object", "academic journal", "scientist", "chemical element", "university", "discipline", "person", "chemical compound", "country", "protein", "organization"], "instance": {"id": "76", "words": ["It", "is", "also", "the", "first", "Doctor", "Who", "appearance", "of", "Gwen", "Cooper", "(", "Eve", "Myles", ")", ";", "Ianto", "Jones", "(", "Gareth", "David-Lloyd", ")", ";", "Luke", "Smith", "(", "Tommy", "Knight", ")", ";", "and", "Mr", "Smith", "(", "voiced", "by", "Alexander", "Armstrong", ")", ",", "though", "Myles", "and", "Armstrong", "appeared", "in", "other", "episodes", "playing", "different", "roles", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "B-person", "I-person", "O", "B-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-person", "I-person", "O", "O", "O", "B-person", "O", "B-person", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, location, theory, enzyme, astronomical object, academic journal, scientist, chemical element, university, discipline, person, chemical compound, country, protein, organization and O.\nSentence: It is also the first Doctor Who appearance of Gwen Cooper ( Eve Myles ) ; Ianto Jones ( Gareth David-Lloyd ) ; Luke Smith ( Tommy Knight ) ; and Mr Smith ( voiced by Alexander Armstrong ) , though Myles and Armstrong appeared in other episodes playing different roles .", "prompt_labels": "It(O) is(O) also(O) the(O) first(O) Doctor(O) Who(O) appearance(O) of(O) Gwen(B-person) Cooper(I-person) ((O) Eve(B-person) Myles(I-person) )(O) ;(O) Ianto(B-person) Jones(I-person) ((O) Gareth(B-person) David-Lloyd(I-person) )(O) ;(O) Luke(B-person) Smith(I-person) ((O) Tommy(B-person) Knight(I-person) )(O) ;(O) and(O) Mr(B-person) Smith(I-person) ((O) voiced(O) by(O) Alexander(B-person) Armstrong(I-person) )(O) ,(O) though(O) Myles(B-person) and(O) Armstrong(B-person) appeared(O) in(O) other(O) episodes(O) playing(O) different(O) roles(O) .(O)"}}
{"id": "288", "dataset": "crossner_science", "split": "dev", "label_list": ["enzyme", "astronomical object", "event", "country", "award", "location", "person", "organization", "chemical element", "scientist", "discipline", "university", "academic journal", "protein", "chemical compound", "theory"], "instance": {"id": "288", "words": ["His", "engagements", "have", "included", "the", "Staatsoper", "Berlin", ",", "Bolshoi", "Theater", "in", "Moscow", ",", "Opéra", "National", "de", "Paris", ",", "Palau", "de", "les", "Arts", "in", "Valencia", ",", "Arena", "di", "Verona", ",", "Teatro", "Carlo", "Felice", "in", "Genoa", ",", "Teatro", "La", "Fenice", "in", "Venice", ",", "Staatsoper", "Hamburg", ",", "Teatro", "Real", "in", "Madrid", ",", "Théâtre", "du", "Châtelet", ",", "Semperoper", "Dresden", ",", "Teatro", "Massimo", "in", "Palermo", ",", "Teatro", "alla", "Scala", "amongst", "other", "acclaimed", "opera", "companies", "/", "houses", "around", "the", "world", "and", "international", "music", "festivals", "."], "labels": ["O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-location", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-location", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "O", "B-organization", "I-organization", "I-organization", "O", "B-location", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-location", "O", "B-organization", "I-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-organization", "I-organization", "O", "B-location", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, astronomical object, event, country, award, location, person, organization, chemical element, scientist, discipline, university, academic journal, protein, chemical compound, theory and O.\nSentence: His engagements have included the Staatsoper Berlin , Bolshoi Theater in Moscow , Opéra National de Paris , Palau de les Arts in Valencia , Arena di Verona , Teatro Carlo Felice in Genoa , Teatro La Fenice in Venice , Staatsoper Hamburg , Teatro Real in Madrid , Théâtre du Châtelet , Semperoper Dresden , Teatro Massimo in Palermo , Teatro alla Scala amongst other acclaimed opera companies / houses around the world and international music festivals .", "prompt_labels": "His(O) engagements(O) have(O) included(O) the(O) Staatsoper(B-organization) Berlin(I-organization) ,(O) Bolshoi(B-organization) Theater(I-organization) in(O) Moscow(B-location) ,(O) Opéra(B-organization) National(I-organization) de(I-organization) Paris(I-organization) ,(O) Palau(B-organization) de(I-organization) les(I-organization) Arts(I-organization) in(O) Valencia(B-location) ,(O) Arena(B-organization) di(I-organization) Verona(I-organization) ,(O) Teatro(B-organization) Carlo(I-organization) Felice(I-organization) in(O) Genoa(B-location) ,(O) Teatro(B-organization) La(I-organization) Fenice(I-organization) in(O) Venice(B-location) ,(O) Staatsoper(B-organization) Hamburg(I-organization) ,(O) Teatro(B-organization) Real(I-organization) in(O) Madrid(B-location) ,(O) Théâtre(B-organization) du(I-organization) Châtelet(I-organization) ,(O) Semperoper(B-organization) Dresden(I-organization) ,(O) Teatro(B-organization) Massimo(I-organization) in(O) Palermo(B-location) ,(O) Teatro(B-organization) alla(I-organization) Scala(I-organization) amongst(O) other(O) acclaimed(O) opera(O) companies(O) /(O) houses(O) around(O) the(O) world(O) and(O) international(O) music(O) festivals(O) .(O)"}}
{"id": "17", "dataset": "crossner_science", "split": "dev", "label_list": ["country", "person", "event", "academic journal", "enzyme", "discipline", "university", "chemical element", "award", "location", "chemical compound", "protein", "organization", "scientist", "theory", "astronomical object"], "instance": {"id": "17", "words": ["Schirra", "was", "a", "33rd", "Degree", "Mason", "and", "part", "of", "the", "American", "Institute", "of", "Aeronautics", "and", "Astronautics", ",", "as", "well", "as", "a", "fellow", "of", "the", "American", "Astronautical", "Society", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, event, academic journal, enzyme, discipline, university, chemical element, award, location, chemical compound, protein, organization, scientist, theory, astronomical object and O.\nSentence: Schirra was a 33rd Degree Mason and part of the American Institute of Aeronautics and Astronautics , as well as a fellow of the American Astronautical Society .", "prompt_labels": "Schirra(B-person) was(O) a(O) 33rd(O) Degree(O) Mason(O) and(O) part(O) of(O) the(O) American(B-organization) Institute(I-organization) of(I-organization) Aeronautics(I-organization) and(I-organization) Astronautics(I-organization) ,(O) as(O) well(O) as(O) a(O) fellow(B-award) of(I-award) the(I-award) American(I-award) Astronautical(I-award) Society(I-award) .(O)"}}
{"id": "42", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "discipline", "theory", "country", "chemical element", "academic journal", "chemical compound", "location", "award", "event", "person", "organization", "enzyme", "university", "scientist", "protein"], "instance": {"id": "42", "words": ["Michelson", "was", "a", "member", "of", "the", "Royal", "Society", ",", "the", "National", "Academy", "of", "Sciences", ",", "the", "American", "Physical", "Society", "and", "the", "American", "Association", "for", "the", "Advancement", "of", "Science", "."], "labels": ["B-scientist", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, discipline, theory, country, chemical element, academic journal, chemical compound, location, award, event, person, organization, enzyme, university, scientist, protein and O.\nSentence: Michelson was a member of the Royal Society , the National Academy of Sciences , the American Physical Society and the American Association for the Advancement of Science .", "prompt_labels": "Michelson(B-scientist) was(O) a(O) member(O) of(O) the(O) Royal(B-organization) Society(I-organization) ,(O) the(O) National(B-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) ,(O) the(O) American(B-organization) Physical(I-organization) Society(I-organization) and(O) the(O) American(B-organization) Association(I-organization) for(I-organization) the(I-organization) Advancement(I-organization) of(I-organization) Science(I-organization) .(O)"}}
{"id": "449", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical compound", "theory", "organization", "country", "chemical element", "enzyme", "academic journal", "astronomical object", "discipline", "university", "person", "protein", "location", "award", "event", "scientist"], "instance": {"id": "449", "words": ["He", "serves", "as", "the", "Associate", "Editor", "of", "Advances", "in", "Space", "Research", ",", "and", "as", "a", "member", "of", "Editorial", "Board", "of", "the", "European", "journal", "Space", "Science", "Reviews", "and", "Molecular", "Astrophysics", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, theory, organization, country, chemical element, enzyme, academic journal, astronomical object, discipline, university, person, protein, location, award, event, scientist and O.\nSentence: He serves as the Associate Editor of Advances in Space Research , and as a member of Editorial Board of the European journal Space Science Reviews and Molecular Astrophysics .", "prompt_labels": "He(O) serves(O) as(O) the(O) Associate(O) Editor(O) of(O) Advances(B-academic journal) in(I-academic journal) Space(I-academic journal) Research(I-academic journal) ,(O) and(O) as(O) a(O) member(O) of(O) Editorial(O) Board(O) of(O) the(O) European(O) journal(O) Space(B-academic journal) Science(I-academic journal) Reviews(I-academic journal) and(O) Molecular(B-academic journal) Astrophysics(I-academic journal) .(O)"}}
{"id": "34", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical compound", "country", "event", "person", "scientist", "organization", "location", "academic journal", "discipline", "university", "theory", "protein", "astronomical object", "chemical element", "enzyme", "award"], "instance": {"id": "34", "words": ["Helicase", "s", "unwind", "the", "strands", "to", "facilitate", "the", "advance", "of", "sequence-reading", "enzymes", "such", "as", "DNA", "polymerase", "."], "labels": ["B-enzyme", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-enzyme", "I-enzyme", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, country, event, person, scientist, organization, location, academic journal, discipline, university, theory, protein, astronomical object, chemical element, enzyme, award and O.\nSentence: Helicase s unwind the strands to facilitate the advance of sequence-reading enzymes such as DNA polymerase .", "prompt_labels": "Helicase(B-enzyme) s(O) unwind(O) the(O) strands(O) to(O) facilitate(O) the(O) advance(O) of(O) sequence-reading(O) enzymes(O) such(O) as(O) DNA(B-enzyme) polymerase(I-enzyme) .(O)"}}
{"id": "43", "dataset": "crossner_science", "split": "dev", "label_list": ["person", "location", "event", "discipline", "chemical compound", "protein", "country", "award", "theory", "university", "scientist", "academic journal", "chemical element", "enzyme", "astronomical object", "organization"], "instance": {"id": "43", "words": ["Heartstone", "has", "also", "been", "a", "part", "of", "a", "number", "of", "esport", "demonstration", "event", "s", "at", "international", "competitions", ",", "such", "as", "the", "2017", "Asian", "Indoor", "and", "Martial", "Arts", "Games", "and", "2018", "Asian", "Games", "."], "labels": ["B-event", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-event", "I-event", "I-event", "I-event", "I-event", "I-event", "I-event", "O", "B-event", "I-event", "I-event", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, event, discipline, chemical compound, protein, country, award, theory, university, scientist, academic journal, chemical element, enzyme, astronomical object, organization and O.\nSentence: Heartstone has also been a part of a number of esport demonstration event s at international competitions , such as the 2017 Asian Indoor and Martial Arts Games and 2018 Asian Games .", "prompt_labels": "Heartstone(B-event) has(O) also(O) been(O) a(O) part(O) of(O) a(O) number(O) of(O) esport(O) demonstration(O) event(O) s(O) at(O) international(O) competitions(O) ,(O) such(O) as(O) the(O) 2017(B-event) Asian(I-event) Indoor(I-event) and(I-event) Martial(I-event) Arts(I-event) Games(I-event) and(O) 2018(B-event) Asian(I-event) Games(I-event) .(O)"}}
{"id": "317", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "chemical compound", "theory", "enzyme", "discipline", "event", "academic journal", "award", "organization", "university", "chemical element", "astronomical object", "scientist", "country", "location", "person"], "instance": {"id": "317", "words": ["Juno", "had", "the", "most", "eccentric", "orbit", "of", "any", "known", "body", "until", "33", "Polyhymnia", "was", "discovered", "in", "1854", ",", "and", "of", "asteroids", "over", "200", "km", "in", "diameter", "only", "324", "Bamberga", "has", "a", "more", "eccentric", "orbit", "."], "labels": ["B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "I-astronomical object", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, chemical compound, theory, enzyme, discipline, event, academic journal, award, organization, university, chemical element, astronomical object, scientist, country, location, person and O.\nSentence: Juno had the most eccentric orbit of any known body until 33 Polyhymnia was discovered in 1854 , and of asteroids over 200 km in diameter only 324 Bamberga has a more eccentric orbit .", "prompt_labels": "Juno(B-astronomical object) had(O) the(O) most(O) eccentric(O) orbit(O) of(O) any(O) known(O) body(O) until(O) 33(B-astronomical object) Polyhymnia(I-astronomical object) was(O) discovered(O) in(O) 1854(O) ,(O) and(O) of(O) asteroids(O) over(O) 200(O) km(O) in(O) diameter(O) only(O) 324(B-astronomical object) Bamberga(I-astronomical object) has(O) a(O) more(O) eccentric(O) orbit(O) .(O)"}}
{"id": "35", "dataset": "crossner_science", "split": "dev", "label_list": ["scientist", "university", "academic journal", "protein", "organization", "chemical element", "enzyme", "award", "location", "astronomical object", "event", "chemical compound", "country", "discipline", "person", "theory"], "instance": {"id": "35", "words": ["The", "album", "was", "nominated", "at", "the", "41st", "Annual", "Grammy", "Awards", "for", "Grammy", "Award", "for", "Best", "Rap", "Album", "."], "labels": ["O", "O", "O", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "I-award", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, university, academic journal, protein, organization, chemical element, enzyme, award, location, astronomical object, event, chemical compound, country, discipline, person, theory and O.\nSentence: The album was nominated at the 41st Annual Grammy Awards for Grammy Award for Best Rap Album .", "prompt_labels": "The(O) album(O) was(O) nominated(O) at(O) the(O) 41st(B-award) Annual(I-award) Grammy(I-award) Awards(I-award) for(O) Grammy(B-award) Award(I-award) for(I-award) Best(I-award) Rap(I-award) Album(I-award) .(O)"}}
{"id": "421", "dataset": "crossner_science", "split": "dev", "label_list": ["person", "event", "astronomical object", "chemical element", "country", "award", "chemical compound", "academic journal", "location", "theory", "enzyme", "scientist", "organization", "university", "protein", "discipline"], "instance": {"id": "421", "words": ["Low", "used", "the", "Learjet", "to", "make", "the", "discovery", "that", "both", "Jupiter", "and", "Saturn", "were", "emitting", "more", "energy", "than", "what", "they", "receive", "as", "solar", "radiation", ",", "demonstrating", "that", "both", "of", "these", "planets", "must", "have", "an", "internal", "source", "of", "energy", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, astronomical object, chemical element, country, award, chemical compound, academic journal, location, theory, enzyme, scientist, organization, university, protein, discipline and O.\nSentence: Low used the Learjet to make the discovery that both Jupiter and Saturn were emitting more energy than what they receive as solar radiation , demonstrating that both of these planets must have an internal source of energy .", "prompt_labels": "Low(B-person) used(O) the(O) Learjet(O) to(O) make(O) the(O) discovery(O) that(O) both(O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) were(O) emitting(O) more(O) energy(O) than(O) what(O) they(O) receive(O) as(O) solar(O) radiation(O) ,(O) demonstrating(O) that(O) both(O) of(O) these(O) planets(O) must(O) have(O) an(O) internal(O) source(O) of(O) energy(O) .(O)"}}
{"id": "424", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "scientist", "country", "academic journal", "person", "award", "chemical element", "theory", "chemical compound", "university", "event", "location", "discipline", "astronomical object", "organization", "enzyme"], "instance": {"id": "424", "words": ["In", "March", "1737", "Short", "was", "elected", "a", "Fellow", "of", "the", "Royal", "Society", "and", "in", "1758", "became", "a", "foreign", "member", "of", "the", "Royal", "Swedish", "Academy", "of", "Sciences", "."], "labels": ["O", "O", "O", "B-scientist", "O", "O", "O", "B-award", "I-award", "I-award", "I-award", "I-award", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "I-organization", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, scientist, country, academic journal, person, award, chemical element, theory, chemical compound, university, event, location, discipline, astronomical object, organization, enzyme and O.\nSentence: In March 1737 Short was elected a Fellow of the Royal Society and in 1758 became a foreign member of the Royal Swedish Academy of Sciences .", "prompt_labels": "In(O) March(O) 1737(O) Short(B-scientist) was(O) elected(O) a(O) Fellow(B-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) and(O) in(O) 1758(O) became(O) a(O) foreign(O) member(O) of(O) the(O) Royal(B-organization) Swedish(I-organization) Academy(I-organization) of(I-organization) Sciences(I-organization) .(O)"}}
{"id": "113", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical element", "protein", "discipline", "award", "location", "event", "academic journal", "astronomical object", "person", "chemical compound", "scientist", "country", "enzyme", "university", "theory", "organization"], "instance": {"id": "113", "words": ["Major", "species", "assessors", "include", "BirdLife", "International", ",", "the", "Institute", "of", "Zoology", "(", "the", "research", "division", "of", "the", "Zoological", "Society", "of", "London", ")", ",", "the", "World", "Conservation", "Monitoring", "Centre", ",", "and", "many", "Specialist", "Groups", "within", "the", "IUCN", "Species", "Survival", "Commission", "(", "SSC", ")", "."], "labels": ["O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "I-organization", "O", "B-organization", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, protein, discipline, award, location, event, academic journal, astronomical object, person, chemical compound, scientist, country, enzyme, university, theory, organization and O.\nSentence: Major species assessors include BirdLife International , the Institute of Zoology ( the research division of the Zoological Society of London ) , the World Conservation Monitoring Centre , and many Specialist Groups within the IUCN Species Survival Commission ( SSC ) .", "prompt_labels": "Major(O) species(O) assessors(O) include(O) BirdLife(B-organization) International(I-organization) ,(O) the(O) Institute(B-organization) of(I-organization) Zoology(I-organization) ((O) the(O) research(O) division(O) of(O) the(O) Zoological(B-organization) Society(I-organization) of(I-organization) London(I-organization) )(O) ,(O) the(O) World(B-organization) Conservation(I-organization) Monitoring(I-organization) Centre(I-organization) ,(O) and(O) many(O) Specialist(O) Groups(O) within(O) the(O) IUCN(B-organization) Species(I-organization) Survival(I-organization) Commission(I-organization) ((O) SSC(B-organization) )(O) .(O)"}}
{"id": "28", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "award", "theory", "scientist", "chemical compound", "discipline", "enzyme", "protein", "organization", "event", "chemical element", "person", "academic journal", "university", "country", "location"], "instance": {"id": "28", "words": ["He", "has", "served", "on", "scientific", "journal", "editorial", "boards", "including", "American", "Scientist", ",", "Physics", "of", "Fluids", ",", "Journal", "of", "Fluid", "Mechanics", ",", "Physical", "Review", "E", ",", "Physical", "Review", "Letters", ",", "Journal", "of", "Theoretical", "and", "Computational", "Fluid", "Dynamics", ","], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "O", "B-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "I-academic journal", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, award, theory, scientist, chemical compound, discipline, enzyme, protein, organization, event, chemical element, person, academic journal, university, country, location and O.\nSentence: He has served on scientific journal editorial boards including American Scientist , Physics of Fluids , Journal of Fluid Mechanics , Physical Review E , Physical Review Letters , Journal of Theoretical and Computational Fluid Dynamics ,", "prompt_labels": "He(O) has(O) served(O) on(O) scientific(O) journal(O) editorial(O) boards(O) including(O) American(B-academic journal) Scientist(I-academic journal) ,(O) Physics(B-academic journal) of(I-academic journal) Fluids(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Fluid(I-academic journal) Mechanics(I-academic journal) ,(O) Physical(B-academic journal) Review(I-academic journal) E(I-academic journal) ,(O) Physical(B-academic journal) Review(I-academic journal) Letters(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Theoretical(I-academic journal) and(I-academic journal) Computational(I-academic journal) Fluid(I-academic journal) Dynamics(I-academic journal) ,(O)"}}
{"id": "41", "dataset": "crossner_science", "split": "dev", "label_list": ["astronomical object", "scientist", "event", "theory", "organization", "academic journal", "person", "university", "award", "country", "chemical compound", "protein", "enzyme", "location", "discipline", "chemical element"], "instance": {"id": "41", "words": ["Hamilton", "was", "a", "visiting", "professor", "at", "Harvard", "University", "and", "later", "spent", "nine", "months", "with", "the", "Royal", "Society", "'", "s", "and", "the", "Royal", "Geographical", "Society", "'", "s", "Xavantina-Cachimbo", "Expedition", "as", "a", "visiting", "professor", "at", "the", "University", "of", "São", "Paulo", "."], "labels": ["B-person", "O", "O", "O", "O", "O", "B-university", "I-university", "O", "O", "O", "O", "O", "O", "O", "B-organization", "I-organization", "O", "O", "O", "O", "B-organization", "I-organization", "I-organization", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-university", "I-university", "I-university", "I-university", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, scientist, event, theory, organization, academic journal, person, university, award, country, chemical compound, protein, enzyme, location, discipline, chemical element and O.\nSentence: Hamilton was a visiting professor at Harvard University and later spent nine months with the Royal Society ' s and the Royal Geographical Society ' s Xavantina-Cachimbo Expedition as a visiting professor at the University of São Paulo .", "prompt_labels": "Hamilton(B-person) was(O) a(O) visiting(O) professor(O) at(O) Harvard(B-university) University(I-university) and(O) later(O) spent(O) nine(O) months(O) with(O) the(O) Royal(B-organization) Society(I-organization) '(O) s(O) and(O) the(O) Royal(B-organization) Geographical(I-organization) Society(I-organization) '(O) s(O) Xavantina-Cachimbo(O) Expedition(O) as(O) a(O) visiting(O) professor(O) at(O) the(O) University(B-university) of(I-university) São(I-university) Paulo(I-university) .(O)"}}
{"id": "16", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical compound", "theory", "country", "location", "chemical element", "award", "enzyme", "astronomical object", "person", "organization", "scientist", "university", "discipline", "academic journal", "event", "protein"], "instance": {"id": "16", "words": ["In", "most", "cases", ",", "planets", "named", "with", "Bayer", ",", "Flamsteed", ",", "and", "or", "Variable", "star", "designation", "have", "a", "space", ",", "but", "usage", "with", "other", "designations", "varies", "e.g.", "WASP-12b", "but", "HD", "209458", "b", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "I-astronomical object", "I-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, theory, country, location, chemical element, award, enzyme, astronomical object, person, organization, scientist, university, discipline, academic journal, event, protein and O.\nSentence: In most cases , planets named with Bayer , Flamsteed , and or Variable star designation have a space , but usage with other designations varies e.g. WASP-12b but HD 209458 b .", "prompt_labels": "In(O) most(O) cases(O) ,(O) planets(O) named(O) with(O) Bayer(O) ,(O) Flamsteed(O) ,(O) and(O) or(O) Variable(O) star(O) designation(O) have(O) a(O) space(O) ,(O) but(O) usage(O) with(O) other(O) designations(O) varies(O) e.g.(O) WASP-12b(B-astronomical object) but(O) HD(B-astronomical object) 209458(I-astronomical object) b(I-astronomical object) .(O)"}}
{"id": "261", "dataset": "crossner_science", "split": "dev", "label_list": ["chemical compound", "organization", "award", "scientist", "person", "discipline", "location", "event", "astronomical object", "country", "enzyme", "chemical element", "academic journal", "university", "theory", "protein"], "instance": {"id": "261", "words": ["Methone", "is", "a", "very", "small", "natural", "satellite", "of", "Saturn", "orbiting", "between", "the", "orbits", "of", "Mimas", "and", "Enceladus", "."], "labels": ["B-astronomical object", "O", "O", "O", "O", "O", "O", "O", "B-astronomical object", "O", "O", "O", "O", "O", "B-astronomical object", "O", "B-astronomical object", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, organization, award, scientist, person, discipline, location, event, astronomical object, country, enzyme, chemical element, academic journal, university, theory, protein and O.\nSentence: Methone is a very small natural satellite of Saturn orbiting between the orbits of Mimas and Enceladus .", "prompt_labels": "Methone(B-astronomical object) is(O) a(O) very(O) small(O) natural(O) satellite(O) of(O) Saturn(B-astronomical object) orbiting(O) between(O) the(O) orbits(O) of(O) Mimas(B-astronomical object) and(O) Enceladus(B-astronomical object) .(O)"}}
{"id": "428", "dataset": "crossner_science", "split": "dev", "label_list": ["protein", "chemical compound", "discipline", "theory", "scientist", "award", "astronomical object", "location", "country", "person", "chemical element", "enzyme", "academic journal", "event", "organization", "university"], "instance": {"id": "428", "words": ["The", "scientists", "found", "homologous", "recombination-mediated", "alteration", "in", "HBB", "and", "Glucose-6-phosphate", "dehydrogenase", "."], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-protein", "O", "B-enzyme", "I-enzyme", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, chemical compound, discipline, theory, scientist, award, astronomical object, location, country, person, chemical element, enzyme, academic journal, event, organization, university and O.\nSentence: The scientists found homologous recombination-mediated alteration in HBB and Glucose-6-phosphate dehydrogenase .", "prompt_labels": "The(O) scientists(O) found(O) homologous(O) recombination-mediated(O) alteration(O) in(O) HBB(B-protein) and(O) Glucose-6-phosphate(B-enzyme) dehydrogenase(I-enzyme) .(O)"}}
{"id": "826", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "character", "director", "rating", "trailer", "plot", "year", "genre", "song", "review", "actor", "title"], "instance": {"id": "826", "words": ["show", "me", "a", "movie", "with", "the", "song", "making", "christmas"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-song", "I-song"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, character, director, rating, trailer, plot, year, genre, song, review, actor, title and O.\nSentence: show me a movie with the song making christmas", "prompt_labels": "show(O) me(O) a(O) movie(O) with(O) the(O) song(O) making(B-song) christmas(I-song)"}}
{"id": "1516", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "song", "review", "trailer", "year", "rating", "actor", "genre", "average ratings", "director", "character", "title"], "instance": {"id": "1516", "words": ["is", "there", "a", "movie", "called", "the", "night", "on", "earth"], "labels": ["O", "O", "O", "O", "O", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, song, review, trailer, year, rating, actor, genre, average ratings, director, character, title and O.\nSentence: is there a movie called the night on earth", "prompt_labels": "is(O) there(O) a(O) movie(O) called(O) the(O) night(B-title) on(I-title) earth(I-title)"}}
{"id": "1371", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "character", "year", "title", "song", "rating", "trailer", "director", "genre", "review", "actor", "plot"], "instance": {"id": "1371", "words": ["im", "looking", "for", "an", "nc", "17", "biography", "about", "a", "mental", "patient", "released", "in", "the", "last", "eight", "years", "and", "directed", "by", "strathford", "hamilton"], "labels": ["O", "O", "O", "O", "B-rating", "I-rating", "B-genre", "O", "O", "B-plot", "I-plot", "O", "O", "O", "B-year", "I-year", "I-year", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, character, year, title, song, rating, trailer, director, genre, review, actor, plot and O.\nSentence: im looking for an nc 17 biography about a mental patient released in the last eight years and directed by strathford hamilton", "prompt_labels": "im(O) looking(O) for(O) an(O) nc(B-rating) 17(I-rating) biography(B-genre) about(O) a(O) mental(B-plot) patient(I-plot) released(O) in(O) the(O) last(B-year) eight(I-year) years(I-year) and(O) directed(O) by(O) strathford(B-director) hamilton(I-director)"}}
{"id": "647", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "year", "song", "trailer", "average ratings", "plot", "review", "actor", "character", "title", "director", "rating"], "instance": {"id": "647", "words": ["who", "directed", "the", "legend", "of", "bagger", "vance"], "labels": ["O", "O", "B-director", "I-director", "I-director", "I-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, year, song, trailer, average ratings, plot, review, actor, character, title, director, rating and O.\nSentence: who directed the legend of bagger vance", "prompt_labels": "who(O) directed(O) the(B-director) legend(I-director) of(I-director) bagger(I-director) vance(I-director)"}}
{"id": "2329", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "director", "rating", "average ratings", "plot", "actor", "title", "trailer", "year", "song", "character", "review"], "instance": {"id": "2329", "words": ["who", "stars", "in", "beatdown"], "labels": ["O", "O", "O", "B-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, director, rating, average ratings, plot, actor, title, trailer, year, song, character, review and O.\nSentence: who stars in beatdown", "prompt_labels": "who(O) stars(O) in(O) beatdown(B-title)"}}
{"id": "410", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "title", "year", "genre", "review", "trailer", "average ratings", "actor", "director", "plot", "song", "character"], "instance": {"id": "410", "words": ["whats", "a", "film", "based", "on", "a", "best", "selling", "contemporary", "novel"], "labels": ["O", "O", "O", "O", "O", "O", "B-plot", "I-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, title, year, genre, review, trailer, average ratings, actor, director, plot, song, character and O.\nSentence: whats a film based on a best selling contemporary novel", "prompt_labels": "whats(O) a(O) film(O) based(O) on(O) a(O) best(B-plot) selling(I-plot) contemporary(I-plot) novel(I-plot)"}}
{"id": "2257", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "trailer", "rating", "song", "character", "director", "review", "actor", "title", "year", "average ratings", "genre"], "instance": {"id": "2257", "words": ["whats", "a", "good", "drama", "that", "is", "well", "rated"], "labels": ["O", "O", "O", "B-genre", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, trailer, rating, song, character, director, review, actor, title, year, average ratings, genre and O.\nSentence: whats a good drama that is well rated", "prompt_labels": "whats(O) a(O) good(O) drama(B-genre) that(O) is(O) well(B-average ratings) rated(I-average ratings)"}}
{"id": "1256", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "review", "character", "trailer", "genre", "song", "title", "actor", "director", "plot", "rating", "average ratings"], "instance": {"id": "1256", "words": ["has", "billy", "wilder", "directed", "and", "rated", "r", "horror", "films"], "labels": ["O", "B-director", "I-director", "O", "O", "O", "B-rating", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, review, character, trailer, genre, song, title, actor, director, plot, rating, average ratings and O.\nSentence: has billy wilder directed and rated r horror films", "prompt_labels": "has(O) billy(B-director) wilder(I-director) directed(O) and(O) rated(O) r(B-rating) horror(B-genre) films(O)"}}
{"id": "1502", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "song", "year", "actor", "plot", "director", "title", "genre", "review", "character", "trailer", "average ratings"], "instance": {"id": "1502", "words": ["is", "there", "a", "historical", "movie", "starring", "morgan", "freeman"], "labels": ["O", "O", "O", "B-genre", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, song, year, actor, plot, director, title, genre, review, character, trailer, average ratings and O.\nSentence: is there a historical movie starring morgan freeman", "prompt_labels": "is(O) there(O) a(O) historical(B-genre) movie(O) starring(O) morgan(B-actor) freeman(I-actor)"}}
{"id": "1785", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "average ratings", "character", "year", "genre", "rating", "trailer", "director", "plot", "review", "title", "song"], "instance": {"id": "1785", "words": ["wanted", "to", "know", "if", "in", "the", "past", "two", "years", "has", "there", "been", "an", "all", "right", "action", "movie", "that", "came", "to", "have", "a", "standoff"], "labels": ["O", "O", "O", "O", "O", "O", "B-year", "I-year", "I-year", "O", "O", "O", "O", "B-average ratings", "I-average ratings", "B-genre", "O", "O", "O", "O", "O", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, average ratings, character, year, genre, rating, trailer, director, plot, review, title, song and O.\nSentence: wanted to know if in the past two years has there been an all right action movie that came to have a standoff", "prompt_labels": "wanted(O) to(O) know(O) if(O) in(O) the(O) past(B-year) two(I-year) years(I-year) has(O) there(O) been(O) an(O) all(B-average ratings) right(I-average ratings) action(B-genre) movie(O) that(O) came(O) to(O) have(O) a(O) standoff(B-plot)"}}
{"id": "643", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "actor", "plot", "review", "song", "character", "trailer", "title", "rating", "average ratings", "director", "genre"], "instance": {"id": "643", "words": ["is", "there", "a", "movie", "with", "kate", "hudson", "as", "a", "love", "struck", "columnist"], "labels": ["O", "O", "O", "O", "O", "B-actor", "I-actor", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, actor, plot, review, song, character, trailer, title, rating, average ratings, director, genre and O.\nSentence: is there a movie with kate hudson as a love struck columnist", "prompt_labels": "is(O) there(O) a(O) movie(O) with(O) kate(B-actor) hudson(I-actor) as(O) a(O) love(O) struck(O) columnist(O)"}}
{"id": "1777", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "title", "review", "year", "actor", "character", "average ratings", "genre", "plot", "rating", "director", "song"], "instance": {"id": "1777", "words": ["the", "movie", "blue", "steel"], "labels": ["O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, title, review, year, actor, character, average ratings, genre, plot, rating, director, song and O.\nSentence: the movie blue steel", "prompt_labels": "the(O) movie(O) blue(B-title) steel(I-title)"}}
{"id": "1781", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "year", "title", "review", "average ratings", "plot", "actor", "song", "rating", "character", "genre", "trailer"], "instance": {"id": "1781", "words": ["this", "1950", "horror", "film", "was", "directed", "by", "alex", "chapple"], "labels": ["O", "B-year", "B-genre", "O", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, year, title, review, average ratings, plot, actor, song, rating, character, genre, trailer and O.\nSentence: this 1950 horror film was directed by alex chapple", "prompt_labels": "this(O) 1950(B-year) horror(B-genre) film(O) was(O) directed(O) by(O) alex(B-director) chapple(I-director)"}}
{"id": "113", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "plot", "genre", "director", "title", "review", "trailer", "song", "year", "average ratings", "rating", "actor"], "instance": {"id": "113", "words": ["are", "there", "any", "pg", "movies", "with", "car", "chases"], "labels": ["O", "O", "O", "B-rating", "I-rating", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, plot, genre, director, title, review, trailer, song, year, average ratings, rating, actor and O.\nSentence: are there any pg movies with car chases", "prompt_labels": "are(O) there(O) any(O) pg(B-rating) movies(I-rating) with(O) car(B-plot) chases(I-plot)"}}
{"id": "148", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "year", "trailer", "character", "review", "actor", "genre", "director", "title", "rating", "song", "plot"], "instance": {"id": "148", "words": ["which", "animated", "childrens", "movies", "are", "considered", "timeless"], "labels": ["O", "B-genre", "I-genre", "I-genre", "O", "B-review", "I-review"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, year, trailer, character, review, actor, genre, director, title, rating, song, plot and O.\nSentence: which animated childrens movies are considered timeless", "prompt_labels": "which(O) animated(B-genre) childrens(I-genre) movies(I-genre) are(O) considered(B-review) timeless(I-review)"}}
{"id": "1185", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "character", "title", "rating", "review", "trailer", "year", "song", "director", "average ratings", "plot", "genre"], "instance": {"id": "1185", "words": ["do", "the", "coen", "brothers", "ever", "direct", "action", "movies"], "labels": ["O", "B-director", "I-director", "I-director", "O", "O", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, character, title, rating, review, trailer, year, song, director, average ratings, plot, genre and O.\nSentence: do the coen brothers ever direct action movies", "prompt_labels": "do(O) the(B-director) coen(I-director) brothers(I-director) ever(O) direct(O) action(B-genre) movies(O)"}}
{"id": "1409", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "song", "character", "genre", "review", "rating", "average ratings", "trailer", "director", "year", "title", "actor"], "instance": {"id": "1409", "words": ["in", "the", "2000", "s", "what", "movies", "was", "sheree", "j", "wilson", "in"], "labels": ["O", "O", "B-year", "I-year", "O", "O", "O", "B-actor", "I-actor", "I-actor", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, song, character, genre, review, rating, average ratings, trailer, director, year, title, actor and O.\nSentence: in the 2000 s what movies was sheree j wilson in", "prompt_labels": "in(O) the(O) 2000(B-year) s(I-year) what(O) movies(O) was(O) sheree(B-actor) j(I-actor) wilson(I-actor) in(O)"}}
{"id": "1508", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "trailer", "song", "actor", "genre", "plot", "title", "review", "director", "rating", "year", "average ratings"], "instance": {"id": "1508", "words": ["is", "there", "a", "movie", "the", "shining"], "labels": ["O", "O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, trailer, song, actor, genre, plot, title, review, director, rating, year, average ratings and O.\nSentence: is there a movie the shining", "prompt_labels": "is(O) there(O) a(O) movie(O) the(B-title) shining(I-title)"}}
{"id": "1312", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "director", "genre", "average ratings", "year", "trailer", "review", "song", "title", "plot", "character", "rating"], "instance": {"id": "1312", "words": ["i", "am", "looking", "for", "a", "sci", "fi", "from", "1970", "with", "excellent", "ratings"], "labels": ["O", "O", "O", "O", "O", "B-genre", "I-genre", "O", "B-year", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, director, genre, average ratings, year, trailer, review, song, title, plot, character, rating and O.\nSentence: i am looking for a sci fi from 1970 with excellent ratings", "prompt_labels": "i(O) am(O) looking(O) for(O) a(O) sci(B-genre) fi(I-genre) from(O) 1970(B-year) with(O) excellent(B-average ratings) ratings(I-average ratings)"}}
{"id": "2241", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "genre", "review", "rating", "song", "actor", "title", "director", "average ratings", "character", "trailer", "year"], "instance": {"id": "2241", "words": ["what", "year", "did", "we", "are", "what", "we", "are", "come", "out"], "labels": ["O", "O", "O", "B-title", "I-title", "I-title", "I-title", "I-title", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, genre, review, rating, song, actor, title, director, average ratings, character, trailer, year and O.\nSentence: what year did we are what we are come out", "prompt_labels": "what(O) year(O) did(O) we(B-title) are(I-title) what(I-title) we(I-title) are(I-title) come(O) out(O)"}}
{"id": "1874", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "director", "rating", "song", "genre", "trailer", "year", "actor", "character", "title", "plot", "average ratings"], "instance": {"id": "1874", "words": ["what", "spike", "lee", "movie", "stars", "clint", "eastwood"], "labels": ["O", "B-director", "I-director", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, director, rating, song, genre, trailer, year, actor, character, title, plot, average ratings and O.\nSentence: what spike lee movie stars clint eastwood", "prompt_labels": "what(O) spike(B-director) lee(I-director) movie(O) stars(O) clint(B-actor) eastwood(I-actor)"}}
{"id": "2070", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "song", "director", "genre", "trailer", "average ratings", "year", "character", "title", "actor", "rating", "plot"], "instance": {"id": "2070", "words": ["what", "is", "the", "last", "science", "fiction", "film", "that", "was", "rated", "r", "and", "directed", "by", "hayao", "miyazaki"], "labels": ["O", "O", "O", "O", "B-genre", "I-genre", "O", "O", "O", "O", "B-rating", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, director, genre, trailer, average ratings, year, character, title, actor, rating, plot and O.\nSentence: what is the last science fiction film that was rated r and directed by hayao miyazaki", "prompt_labels": "what(O) is(O) the(O) last(O) science(B-genre) fiction(I-genre) film(O) that(O) was(O) rated(O) r(B-rating) and(O) directed(O) by(O) hayao(B-director) miyazaki(I-director)"}}
{"id": "429", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "title", "year", "character", "rating", "plot", "trailer", "average ratings", "director", "genre", "song", "review"], "instance": {"id": "429", "words": ["find", "movies", "with", "the", "stones", "songs", "performed", "in", "it"], "labels": ["O", "O", "O", "O", "B-song", "I-song", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, title, year, character, rating, plot, trailer, average ratings, director, genre, song, review and O.\nSentence: find movies with the stones songs performed in it", "prompt_labels": "find(O) movies(O) with(O) the(O) stones(B-song) songs(I-song) performed(O) in(O) it(O)"}}
{"id": "1947", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "trailer", "plot", "song", "director", "average ratings", "title", "actor", "year", "character", "genre", "rating"], "instance": {"id": "1947", "words": ["what", "decent", "drama", "of", "the", "2000", "s", "featured", "a", "plot", "about", "a", "secret", "society"], "labels": ["O", "B-average ratings", "B-genre", "O", "O", "B-year", "I-year", "O", "O", "O", "O", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, trailer, plot, song, director, average ratings, title, actor, year, character, genre, rating and O.\nSentence: what decent drama of the 2000 s featured a plot about a secret society", "prompt_labels": "what(O) decent(B-average ratings) drama(B-genre) of(O) the(O) 2000(B-year) s(I-year) featured(O) a(O) plot(O) about(O) a(O) secret(B-plot) society(I-plot)"}}
{"id": "2292", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "song", "director", "character", "year", "rating", "genre", "plot", "trailer", "title", "review", "average ratings"], "instance": {"id": "2292", "words": ["which", "documentary", "received", "a", "rating", "of", "six"], "labels": ["O", "B-genre", "O", "O", "O", "O", "B-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, song, director, character, year, rating, genre, plot, trailer, title, review, average ratings and O.\nSentence: which documentary received a rating of six", "prompt_labels": "which(O) documentary(B-genre) received(O) a(O) rating(O) of(O) six(B-average ratings)"}}
{"id": "121", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "plot", "genre", "director", "trailer", "year", "song", "character", "title", "review", "rating", "average ratings"], "instance": {"id": "121", "words": ["who", "directed", "the", "help"], "labels": ["O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, plot, genre, director, trailer, year, song, character, title, review, rating, average ratings and O.\nSentence: who directed the help", "prompt_labels": "who(O) directed(O) the(B-title) help(I-title)"}}
{"id": "575", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "trailer", "song", "genre", "average ratings", "rating", "plot", "actor", "year", "review", "character", "director"], "instance": {"id": "575", "words": ["which", "tom", "hanks", "film", "features", "him", "as", "a", "stranded", "fed", "ex", "employee"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "O", "O", "B-plot", "I-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, trailer, song, genre, average ratings, rating, plot, actor, year, review, character, director and O.\nSentence: which tom hanks film features him as a stranded fed ex employee", "prompt_labels": "which(O) tom(B-actor) hanks(I-actor) film(O) features(O) him(O) as(O) a(O) stranded(B-plot) fed(I-plot) ex(I-plot) employee(I-plot)"}}
{"id": "594", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "average ratings", "title", "trailer", "review", "character", "plot", "year", "genre", "director", "song", "rating"], "instance": {"id": "594", "words": ["are", "there", "any", "movies", "about", "bank", "robberies"], "labels": ["O", "O", "O", "O", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, average ratings, title, trailer, review, character, plot, year, genre, director, song, rating and O.\nSentence: are there any movies about bank robberies", "prompt_labels": "are(O) there(O) any(O) movies(O) about(O) bank(B-plot) robberies(I-plot)"}}
{"id": "547", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "year", "song", "review", "director", "character", "actor", "average ratings", "title", "trailer", "genre", "rating"], "instance": {"id": "547", "words": ["what", "was", "the", "name", "of", "ariels", "prince", "in", "the", "little", "mermaid"], "labels": ["O", "O", "O", "O", "O", "B-character", "O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, year, song, review, director, character, actor, average ratings, title, trailer, genre, rating and O.\nSentence: what was the name of ariels prince in the little mermaid", "prompt_labels": "what(O) was(O) the(O) name(O) of(O) ariels(B-character) prince(O) in(O) the(O) little(B-title) mermaid(I-title)"}}
{"id": "1792", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "actor", "song", "average ratings", "plot", "genre", "review", "character", "title", "trailer", "rating", "year"], "instance": {"id": "1792", "words": ["was", "gene", "hackman", "ever", "a", "narrator", "for", "a", "biographical", "film"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "O", "O", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, song, average ratings, plot, genre, review, character, title, trailer, rating, year and O.\nSentence: was gene hackman ever a narrator for a biographical film", "prompt_labels": "was(O) gene(B-actor) hackman(I-actor) ever(O) a(O) narrator(O) for(O) a(O) biographical(B-genre) film(O)"}}
{"id": "472", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "song", "actor", "title", "genre", "average ratings", "trailer", "year", "director", "review", "plot", "character"], "instance": {"id": "472", "words": ["who", "directed", "dracula", "dead", "and", "loving", "it"], "labels": ["O", "B-director", "B-title", "I-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, song, actor, title, genre, average ratings, trailer, year, director, review, plot, character and O.\nSentence: who directed dracula dead and loving it", "prompt_labels": "who(O) directed(B-director) dracula(B-title) dead(I-title) and(I-title) loving(I-title) it(I-title)"}}
{"id": "1464", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "actor", "song", "director", "genre", "year", "rating", "plot", "review", "trailer", "title", "average ratings"], "instance": {"id": "1464", "words": ["is", "there", "a", "pg", "13", "film", "out", "there", "from", "the", "2000", "s", "that", "focuses", "on", "an", "evil", "character"], "labels": ["O", "O", "O", "B-rating", "I-rating", "O", "O", "O", "O", "O", "B-year", "I-year", "O", "O", "O", "O", "B-plot", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, actor, song, director, genre, year, rating, plot, review, trailer, title, average ratings and O.\nSentence: is there a pg 13 film out there from the 2000 s that focuses on an evil character", "prompt_labels": "is(O) there(O) a(O) pg(B-rating) 13(I-rating) film(O) out(O) there(O) from(O) the(O) 2000(B-year) s(I-year) that(O) focuses(O) on(O) an(O) evil(B-plot) character(O)"}}
{"id": "31", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "actor", "average ratings", "rating", "plot", "character", "title", "genre", "director", "trailer", "song", "year"], "instance": {"id": "31", "words": ["find", "movies", "with", "robert", "diniero", "in", "it"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, actor, average ratings, rating, plot, character, title, genre, director, trailer, song, year and O.\nSentence: find movies with robert diniero in it", "prompt_labels": "find(O) movies(O) with(O) robert(B-actor) diniero(I-actor) in(O) it(O)"}}
{"id": "171", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "average ratings", "year", "review", "trailer", "plot", "director", "character", "genre", "title", "actor", "rating"], "instance": {"id": "171", "words": ["show", "me", "a", "deborah", "harry", "movie"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, average ratings, year, review, trailer, plot, director, character, genre, title, actor, rating and O.\nSentence: show me a deborah harry movie", "prompt_labels": "show(O) me(O) a(O) deborah(B-actor) harry(I-actor) movie(O)"}}
{"id": "2223", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "actor", "average ratings", "year", "rating", "review", "trailer", "genre", "character", "director", "plot", "title"], "instance": {"id": "2223", "words": ["what", "was", "the", "last", "science", "fiction", "movie", "that", "liam", "neeson", "was", "in"], "labels": ["O", "O", "O", "O", "B-genre", "I-genre", "O", "O", "B-actor", "I-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, actor, average ratings, year, rating, review, trailer, genre, character, director, plot, title and O.\nSentence: what was the last science fiction movie that liam neeson was in", "prompt_labels": "what(O) was(O) the(O) last(O) science(B-genre) fiction(I-genre) movie(O) that(O) liam(B-actor) neeson(I-actor) was(O) in(O)"}}
{"id": "1370", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "rating", "trailer", "character", "actor", "plot", "genre", "average ratings", "title", "song", "director", "year"], "instance": {"id": "1370", "words": ["im", "looking", "for", "a", "war", "film", "from", "the", "past", "ten", "years", "rated", "nc", "17", "and", "directed", "by", "putipong", "saisikaew"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O", "O", "B-year", "I-year", "I-year", "O", "B-rating", "I-rating", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, rating, trailer, character, actor, plot, genre, average ratings, title, song, director, year and O.\nSentence: im looking for a war film from the past ten years rated nc 17 and directed by putipong saisikaew", "prompt_labels": "im(O) looking(O) for(O) a(O) war(B-genre) film(O) from(O) the(O) past(B-year) ten(I-year) years(I-year) rated(O) nc(B-rating) 17(I-rating) and(O) directed(O) by(O) putipong(B-director) saisikaew(I-director)"}}
{"id": "859", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "average ratings", "trailer", "director", "year", "genre", "rating", "title", "song", "plot", "character", "review"], "instance": {"id": "859", "words": ["is", "there", "a", "burt", "lancaster", "political", "movie"], "labels": ["O", "O", "O", "B-actor", "I-actor", "B-plot", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, average ratings, trailer, director, year, genre, rating, title, song, plot, character, review and O.\nSentence: is there a burt lancaster political movie", "prompt_labels": "is(O) there(O) a(O) burt(B-actor) lancaster(I-actor) political(B-plot) movie(O)"}}
{"id": "2186", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "review", "average ratings", "trailer", "song", "rating", "actor", "plot", "title", "director", "genre", "character"], "instance": {"id": "2186", "words": ["what", "science", "fiction", "movie", "of", "the", "past", "ten", "decades", "starred", "vincent", "gallo"], "labels": ["O", "B-genre", "I-genre", "O", "O", "O", "B-year", "I-year", "I-year", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, review, average ratings, trailer, song, rating, actor, plot, title, director, genre, character and O.\nSentence: what science fiction movie of the past ten decades starred vincent gallo", "prompt_labels": "what(O) science(B-genre) fiction(I-genre) movie(O) of(O) the(O) past(B-year) ten(I-year) decades(I-year) starred(O) vincent(B-actor) gallo(I-actor)"}}
{"id": "1291", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "rating", "average ratings", "actor", "director", "year", "genre", "title", "trailer", "plot", "review", "song"], "instance": {"id": "1291", "words": ["has", "traci", "bingham", "made", "any", "movies", "in", "the", "last", "five", "years"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, rating, average ratings, actor, director, year, genre, title, trailer, plot, review, song and O.\nSentence: has traci bingham made any movies in the last five years", "prompt_labels": "has(O) traci(B-actor) bingham(I-actor) made(O) any(O) movies(O) in(O) the(O) last(B-year) five(I-year) years(I-year)"}}
{"id": "1519", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "rating", "song", "character", "genre", "trailer", "director", "average ratings", "year", "plot", "actor", "title"], "instance": {"id": "1519", "words": ["is", "there", "a", "movie", "from", "the", "1990", "s", "about", "romance", "and", "rated", "r", "starring", "julie", "andrews", "and", "has", "excellent", "ratings"], "labels": ["O", "O", "O", "O", "O", "O", "B-year", "I-year", "O", "B-genre", "O", "O", "B-rating", "O", "B-actor", "I-actor", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, rating, song, character, genre, trailer, director, average ratings, year, plot, actor, title and O.\nSentence: is there a movie from the 1990 s about romance and rated r starring julie andrews and has excellent ratings", "prompt_labels": "is(O) there(O) a(O) movie(O) from(O) the(O) 1990(B-year) s(I-year) about(O) romance(B-genre) and(O) rated(O) r(B-rating) starring(O) julie(B-actor) andrews(I-actor) and(O) has(O) excellent(B-average ratings) ratings(I-average ratings)"}}
{"id": "165", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "genre", "average ratings", "plot", "year", "director", "song", "character", "title", "actor", "rating", "review"], "instance": {"id": "165", "words": ["what", "was", "the", "fog", "rated"], "labels": ["O", "O", "B-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, genre, average ratings, plot, year, director, song, character, title, actor, rating, review and O.\nSentence: what was the fog rated", "prompt_labels": "what(O) was(O) the(B-title) fog(I-title) rated(O)"}}
{"id": "2420", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "rating", "review", "song", "trailer", "actor", "plot", "character", "title", "year", "average ratings", "genre"], "instance": {"id": "2420", "words": ["oliver", "twist"], "labels": ["B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, rating, review, song, trailer, actor, plot, character, title, year, average ratings, genre and O.\nSentence: oliver twist", "prompt_labels": "oliver(B-title) twist(I-title)"}}
{"id": "402", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "average ratings", "director", "genre", "review", "actor", "trailer", "title", "rating", "song", "year", "plot"], "instance": {"id": "402", "words": ["in", "which", "movies", "did", "leonardo", "dicaprio", "play", "a", "cop", "or", "federal", "agent"], "labels": ["O", "O", "O", "O", "B-actor", "I-actor", "O", "O", "B-character", "O", "B-character", "I-character"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, average ratings, director, genre, review, actor, trailer, title, rating, song, year, plot and O.\nSentence: in which movies did leonardo dicaprio play a cop or federal agent", "prompt_labels": "in(O) which(O) movies(O) did(O) leonardo(B-actor) dicaprio(I-actor) play(O) a(O) cop(B-character) or(O) federal(B-character) agent(I-character)"}}
{"id": "1558", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "actor", "character", "rating", "average ratings", "review", "trailer", "director", "year", "song", "plot", "genre"], "instance": {"id": "1558", "words": ["is", "there", "any", "r", "rated", "portrait", "film", "starring", "billy", "bob", "thornton"], "labels": ["O", "O", "O", "B-rating", "O", "B-genre", "O", "O", "B-actor", "I-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, actor, character, rating, average ratings, review, trailer, director, year, song, plot, genre and O.\nSentence: is there any r rated portrait film starring billy bob thornton", "prompt_labels": "is(O) there(O) any(O) r(B-rating) rated(O) portrait(B-genre) film(O) starring(O) billy(B-actor) bob(I-actor) thornton(I-actor)"}}
{"id": "2108", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "genre", "year", "review", "average ratings", "actor", "director", "plot", "trailer", "song", "character", "rating"], "instance": {"id": "2108", "words": ["what", "is", "the", "plot", "of", "the", "film", "titled", "dakota"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, genre, year, review, average ratings, actor, director, plot, trailer, song, character, rating and O.\nSentence: what is the plot of the film titled dakota", "prompt_labels": "what(O) is(O) the(O) plot(O) of(O) the(O) film(O) titled(O) dakota(B-title)"}}
{"id": "1373", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "average ratings", "genre", "actor", "title", "rating", "review", "song", "character", "trailer", "director", "plot"], "instance": {"id": "1373", "words": ["im", "looking", "for", "an", "ok", "movie", "rated", "pg", "13", "featuring", "william", "hurt", "and", "made", "in", "1970"], "labels": ["O", "O", "O", "O", "B-average ratings", "O", "O", "B-rating", "I-rating", "O", "B-actor", "I-actor", "O", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, average ratings, genre, actor, title, rating, review, song, character, trailer, director, plot and O.\nSentence: im looking for an ok movie rated pg 13 featuring william hurt and made in 1970", "prompt_labels": "im(O) looking(O) for(O) an(O) ok(B-average ratings) movie(O) rated(O) pg(B-rating) 13(I-rating) featuring(O) william(B-actor) hurt(I-actor) and(O) made(O) in(O) 1970(B-year)"}}
{"id": "1212", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "song", "genre", "rating", "plot", "character", "actor", "review", "trailer", "average ratings", "year", "title"], "instance": {"id": "1212", "words": ["do", "you", "have", "the", "movie", "the", "level"], "labels": ["O", "O", "O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, song, genre, rating, plot, character, actor, review, trailer, average ratings, year, title and O.\nSentence: do you have the movie the level", "prompt_labels": "do(O) you(O) have(O) the(O) movie(O) the(B-title) level(I-title)"}}
{"id": "259", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "director", "average ratings", "rating", "title", "genre", "character", "song", "year", "trailer", "review", "actor"], "instance": {"id": "259", "words": ["what", "is", "the", "plot", "of", "the", "wild", "bunch"], "labels": ["O", "O", "O", "O", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, director, average ratings, rating, title, genre, character, song, year, trailer, review, actor and O.\nSentence: what is the plot of the wild bunch", "prompt_labels": "what(O) is(O) the(O) plot(O) of(O) the(B-title) wild(I-title) bunch(I-title)"}}
{"id": "612", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "director", "review", "year", "plot", "title", "trailer", "character", "genre", "rating", "average ratings", "actor"], "instance": {"id": "612", "words": ["show", "me", "the", "movie", "by", "peter", "jackson"], "labels": ["O", "O", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, director, review, year, plot, title, trailer, character, genre, rating, average ratings, actor and O.\nSentence: show me the movie by peter jackson", "prompt_labels": "show(O) me(O) the(O) movie(O) by(O) peter(B-director) jackson(I-director)"}}
{"id": "1984", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "plot", "title", "actor", "year", "director", "trailer", "song", "rating", "character", "average ratings", "review"], "instance": {"id": "1984", "words": ["what", "is", "the", "rock", "about"], "labels": ["O", "O", "B-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, plot, title, actor, year, director, trailer, song, rating, character, average ratings, review and O.\nSentence: what is the rock about", "prompt_labels": "what(O) is(O) the(B-title) rock(I-title) about(O)"}}
{"id": "1486", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "character", "rating", "plot", "year", "average ratings", "trailer", "title", "director", "genre", "actor", "song"], "instance": {"id": "1486", "words": ["is", "there", "a", "good", "comedy", "that", "is", "rated", "pg", "13", "about", "sexual", "desire"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O", "O", "B-rating", "I-rating", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, character, rating, plot, year, average ratings, trailer, title, director, genre, actor, song and O.\nSentence: is there a good comedy that is rated pg 13 about sexual desire", "prompt_labels": "is(O) there(O) a(O) good(O) comedy(B-genre) that(O) is(O) rated(O) pg(B-rating) 13(I-rating) about(O) sexual(B-plot) desire(I-plot)"}}
{"id": "2351", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "year", "actor", "review", "rating", "plot", "character", "average ratings", "trailer", "title", "genre", "song"], "instance": {"id": "2351", "words": ["does", "meryl", "streep", "star", "in", "a", "western"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "B-genre"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, year, actor, review, rating, plot, character, average ratings, trailer, title, genre, song and O.\nSentence: does meryl streep star in a western", "prompt_labels": "does(O) meryl(B-actor) streep(I-actor) star(O) in(O) a(O) western(B-genre)"}}
{"id": "2199", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "trailer", "genre", "average ratings", "song", "character", "review", "plot", "title", "year", "actor", "rating"], "instance": {"id": "2199", "words": ["what", "warsaw", "poland", "movie", "directed", "by", "carl", "bessai", "received", "two", "thumbs", "up"], "labels": ["O", "B-plot", "I-plot", "O", "O", "O", "B-director", "I-director", "O", "B-average ratings", "I-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, trailer, genre, average ratings, song, character, review, plot, title, year, actor, rating and O.\nSentence: what warsaw poland movie directed by carl bessai received two thumbs up", "prompt_labels": "what(O) warsaw(B-plot) poland(I-plot) movie(O) directed(O) by(O) carl(B-director) bessai(I-director) received(O) two(B-average ratings) thumbs(I-average ratings) up(I-average ratings)"}}
{"id": "1178", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "plot", "genre", "song", "director", "actor", "year", "character", "title", "average ratings", "rating", "trailer"], "instance": {"id": "1178", "words": ["did", "tom", "jones", "starring", "in", "pg", "13", "film", "centers", "on", "human", "in", "the", "past", "eight", "years"], "labels": ["O", "B-actor", "I-actor", "O", "O", "B-rating", "I-rating", "O", "O", "O", "B-plot", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, genre, song, director, actor, year, character, title, average ratings, rating, trailer and O.\nSentence: did tom jones starring in pg 13 film centers on human in the past eight years", "prompt_labels": "did(O) tom(B-actor) jones(I-actor) starring(O) in(O) pg(B-rating) 13(I-rating) film(O) centers(O) on(O) human(B-plot) in(O) the(O) past(B-year) eight(I-year) years(I-year)"}}
{"id": "664", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "plot", "actor", "trailer", "song", "rating", "title", "review", "year", "director", "genre", "character"], "instance": {"id": "664", "words": ["is", "there", "a", "science", "fiction", "film", "that", "starts", "on", "a", "tower"], "labels": ["O", "O", "O", "B-genre", "I-genre", "O", "O", "B-plot", "I-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, plot, actor, trailer, song, rating, title, review, year, director, genre, character and O.\nSentence: is there a science fiction film that starts on a tower", "prompt_labels": "is(O) there(O) a(O) science(B-genre) fiction(I-genre) film(O) that(O) starts(B-plot) on(I-plot) a(I-plot) tower(I-plot)"}}
{"id": "218", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "review", "rating", "director", "plot", "year", "title", "actor", "song", "genre", "character", "average ratings"], "instance": {"id": "218", "words": ["todd"], "labels": ["B-character"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, review, rating, director, plot, year, title, actor, song, genre, character, average ratings and O.\nSentence: todd", "prompt_labels": "todd(B-character)"}}
{"id": "1755", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "genre", "average ratings", "actor", "trailer", "review", "character", "title", "director", "plot", "rating", "year"], "instance": {"id": "1755", "words": ["ray", "brady", "directed", "sci", "fi", "pg", "13", "this", "year", "with", "ratings", "of", "eight"], "labels": ["B-director", "I-director", "O", "B-genre", "I-genre", "B-rating", "I-rating", "B-year", "I-year", "O", "O", "O", "B-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, genre, average ratings, actor, trailer, review, character, title, director, plot, rating, year and O.\nSentence: ray brady directed sci fi pg 13 this year with ratings of eight", "prompt_labels": "ray(B-director) brady(I-director) directed(O) sci(B-genre) fi(I-genre) pg(B-rating) 13(I-rating) this(B-year) year(I-year) with(O) ratings(O) of(O) eight(B-average ratings)"}}
{"id": "1244", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "average ratings", "director", "trailer", "song", "character", "review", "genre", "title", "actor", "plot", "year"], "instance": {"id": "1244", "words": ["find", "a", "romantic", "comedy", "starring", "hugh", "jackman"], "labels": ["O", "O", "B-genre", "I-genre", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, average ratings, director, trailer, song, character, review, genre, title, actor, plot, year and O.\nSentence: find a romantic comedy starring hugh jackman", "prompt_labels": "find(O) a(O) romantic(B-genre) comedy(I-genre) starring(O) hugh(B-actor) jackman(I-actor)"}}
{"id": "356", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "genre", "director", "average ratings", "year", "title", "plot", "song", "character", "rating", "actor", "review"], "instance": {"id": "356", "words": ["what", "is", "the", "movie", "made", "by", "james", "cameron", "in", "2008"], "labels": ["O", "O", "O", "O", "O", "O", "B-director", "I-director", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, genre, director, average ratings, year, title, plot, song, character, rating, actor, review and O.\nSentence: what is the movie made by james cameron in 2008", "prompt_labels": "what(O) is(O) the(O) movie(O) made(O) by(O) james(B-director) cameron(I-director) in(O) 2008(B-year)"}}
{"id": "119", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "average ratings", "director", "actor", "title", "genre", "song", "rating", "trailer", "review", "year", "plot"], "instance": {"id": "119", "words": ["what", "popular", "films", "released", "last", "month"], "labels": ["O", "B-review", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, average ratings, director, actor, title, genre, song, rating, trailer, review, year, plot and O.\nSentence: what popular films released last month", "prompt_labels": "what(O) popular(B-review) films(O) released(O) last(B-year) month(I-year)"}}
{"id": "1086", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "director", "plot", "year", "character", "genre", "average ratings", "trailer", "song", "rating", "review", "title"], "instance": {"id": "1086", "words": ["are", "there", "any", "crime", "movies", "with", "jennifer", "love", "hewitt", "in", "them"], "labels": ["O", "O", "O", "B-genre", "O", "O", "B-actor", "I-actor", "I-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, director, plot, year, character, genre, average ratings, trailer, song, rating, review, title and O.\nSentence: are there any crime movies with jennifer love hewitt in them", "prompt_labels": "are(O) there(O) any(O) crime(B-genre) movies(O) with(O) jennifer(B-actor) love(I-actor) hewitt(I-actor) in(O) them(O)"}}
{"id": "391", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "review", "actor", "trailer", "year", "average ratings", "genre", "title", "plot", "director", "rating", "song"], "instance": {"id": "391", "words": ["what", "was", "thirteen", "candles", "rated"], "labels": ["O", "O", "B-title", "I-title", "B-rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, review, actor, trailer, year, average ratings, genre, title, plot, director, rating, song and O.\nSentence: what was thirteen candles rated", "prompt_labels": "what(O) was(O) thirteen(B-title) candles(I-title) rated(B-rating)"}}
{"id": "2222", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "director", "average ratings", "plot", "year", "actor", "title", "trailer", "genre", "character", "rating", "review"], "instance": {"id": "2222", "words": ["what", "was", "the", "last", "movie", "that", "ridley", "scott", "directed"], "labels": ["O", "O", "O", "O", "O", "O", "B-director", "I-director", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, director, average ratings, plot, year, actor, title, trailer, genre, character, rating, review and O.\nSentence: what was the last movie that ridley scott directed", "prompt_labels": "what(O) was(O) the(O) last(O) movie(O) that(O) ridley(B-director) scott(I-director) directed(O)"}}
{"id": "1520", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "year", "song", "actor", "title", "genre", "rating", "average ratings", "character", "plot", "review", "trailer"], "instance": {"id": "1520", "words": ["is", "there", "a", "movie", "set", "in", "the", "1940", "s", "that", "has", "a", "plot", "of", "a", "runaway", "that", "might", "be", "really", "popular"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-year", "I-year", "O", "O", "O", "O", "O", "O", "B-plot", "O", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, year, song, actor, title, genre, rating, average ratings, character, plot, review, trailer and O.\nSentence: is there a movie set in the 1940 s that has a plot of a runaway that might be really popular", "prompt_labels": "is(O) there(O) a(O) movie(O) set(O) in(O) the(O) 1940(B-year) s(I-year) that(O) has(O) a(O) plot(O) of(O) a(O) runaway(B-plot) that(O) might(O) be(O) really(B-average ratings) popular(I-average ratings)"}}
{"id": "81", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "character", "trailer", "actor", "plot", "average ratings", "title", "genre", "rating", "director", "review", "year"], "instance": {"id": "81", "words": ["i", "would", "like", "a", "list", "of", "movies", "about", "dancing", "from", "the", "past", "10", "years"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-plot", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, character, trailer, actor, plot, average ratings, title, genre, rating, director, review, year and O.\nSentence: i would like a list of movies about dancing from the past 10 years", "prompt_labels": "i(O) would(O) like(O) a(O) list(O) of(O) movies(O) about(O) dancing(B-plot) from(O) the(O) past(B-year) 10(I-year) years(I-year)"}}
{"id": "168", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "year", "song", "title", "actor", "review", "rating", "genre", "director", "plot", "character", "trailer"], "instance": {"id": "168", "words": ["what", "was", "the", "name", "of", "the", "donkey", "in", "shrek"], "labels": ["O", "O", "O", "O", "O", "O", "B-character", "O", "B-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, year, song, title, actor, review, rating, genre, director, plot, character, trailer and O.\nSentence: what was the name of the donkey in shrek", "prompt_labels": "what(O) was(O) the(O) name(O) of(O) the(O) donkey(B-character) in(O) shrek(B-title)"}}
{"id": "2287", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "trailer", "song", "plot", "year", "rating", "review", "character", "actor", "genre", "average ratings", "director"], "instance": {"id": "2287", "words": ["which", "luke", "greenfield", "documentary", "made", "in", "1940", "received", "eight", "stars"], "labels": ["O", "B-director", "I-director", "B-genre", "O", "O", "B-year", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, trailer, song, plot, year, rating, review, character, actor, genre, average ratings, director and O.\nSentence: which luke greenfield documentary made in 1940 received eight stars", "prompt_labels": "which(O) luke(B-director) greenfield(I-director) documentary(B-genre) made(O) in(O) 1940(B-year) received(O) eight(B-average ratings) stars(I-average ratings)"}}
{"id": "1129", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "review", "average ratings", "plot", "song", "year", "title", "genre", "actor", "character", "director", "trailer"], "instance": {"id": "1129", "words": ["did", "chad", "martin", "direct", "a", "western", "in", "the", "1990", "s"], "labels": ["O", "B-director", "I-director", "O", "O", "B-genre", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, review, average ratings, plot, song, year, title, genre, actor, character, director, trailer and O.\nSentence: did chad martin direct a western in the 1990 s", "prompt_labels": "did(O) chad(B-director) martin(I-director) direct(O) a(O) western(B-genre) in(O) the(O) 1990(B-year) s(I-year)"}}
{"id": "9", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "plot", "review", "actor", "genre", "rating", "average ratings", "director", "title", "trailer", "character", "year"], "instance": {"id": "9", "words": ["what", "is", "the", "most", "current", "movie", "featuring", "mat", "damon"], "labels": ["O", "O", "O", "O", "B-year", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, plot, review, actor, genre, rating, average ratings, director, title, trailer, character, year and O.\nSentence: what is the most current movie featuring mat damon", "prompt_labels": "what(O) is(O) the(O) most(O) current(B-year) movie(O) featuring(O) mat(B-actor) damon(I-actor)"}}
{"id": "1899", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "rating", "character", "review", "song", "year", "title", "trailer", "director", "plot", "average ratings", "actor"], "instance": {"id": "1899", "words": ["what", "are", "some", "childrens", "films", "directed", "by", "george", "lucas"], "labels": ["O", "O", "O", "B-genre", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, rating, character, review, song, year, title, trailer, director, plot, average ratings, actor and O.\nSentence: what are some childrens films directed by george lucas", "prompt_labels": "what(O) are(O) some(O) childrens(B-genre) films(O) directed(O) by(O) george(B-director) lucas(I-director)"}}
{"id": "52", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "genre", "director", "year", "average ratings", "actor", "song", "review", "trailer", "character", "plot", "title"], "instance": {"id": "52", "words": ["list", "the", "dirty", "harry", "films", "from", "the", "1980s"], "labels": ["O", "O", "B-title", "I-title", "O", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, genre, director, year, average ratings, actor, song, review, trailer, character, plot, title and O.\nSentence: list the dirty harry films from the 1980s", "prompt_labels": "list(O) the(O) dirty(B-title) harry(I-title) films(O) from(O) the(O) 1980s(B-year)"}}
{"id": "1356", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "year", "character", "rating", "trailer", "director", "plot", "actor", "genre", "title", "song", "review"], "instance": {"id": "1356", "words": ["im", "looking", "for", "a", "pg", "13", "thriller", "starring", "damiano", "damiani"], "labels": ["O", "O", "O", "O", "B-rating", "I-rating", "B-genre", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, year, character, rating, trailer, director, plot, actor, genre, title, song, review and O.\nSentence: im looking for a pg 13 thriller starring damiano damiani", "prompt_labels": "im(O) looking(O) for(O) a(O) pg(B-rating) 13(I-rating) thriller(B-genre) starring(O) damiano(B-director) damiani(I-director)"}}
{"id": "584", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "plot", "character", "genre", "director", "rating", "average ratings", "year", "trailer", "actor", "review", "title"], "instance": {"id": "584", "words": ["show", "me", "an", "eartha", "kitt", "film", "about", "a", "torch", "singer"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O", "O", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, plot, character, genre, director, rating, average ratings, year, trailer, actor, review, title and O.\nSentence: show me an eartha kitt film about a torch singer", "prompt_labels": "show(O) me(O) an(O) eartha(B-actor) kitt(I-actor) film(O) about(O) a(O) torch(B-plot) singer(I-plot)"}}
{"id": "2261", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "plot", "title", "year", "trailer", "average ratings", "director", "genre", "actor", "character", "song", "rating"], "instance": {"id": "2261", "words": ["when", "did", "gallants", "come", "out"], "labels": ["O", "O", "B-title", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, title, year, trailer, average ratings, director, genre, actor, character, song, rating and O.\nSentence: when did gallants come out", "prompt_labels": "when(O) did(O) gallants(B-title) come(O) out(O)"}}
{"id": "2011", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "actor", "year", "trailer", "rating", "review", "song", "director", "plot", "character", "title", "average ratings"], "instance": {"id": "2011", "words": ["what", "is", "a", "good", "1990", "s", "romance", "movie", "starring", "kelsy", "grammer"], "labels": ["O", "O", "O", "O", "B-year", "I-year", "B-genre", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, actor, year, trailer, rating, review, song, director, plot, character, title, average ratings and O.\nSentence: what is a good 1990 s romance movie starring kelsy grammer", "prompt_labels": "what(O) is(O) a(O) good(O) 1990(B-year) s(I-year) romance(B-genre) movie(O) starring(O) kelsy(B-actor) grammer(I-actor)"}}
{"id": "2336", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "review", "year", "rating", "song", "plot", "trailer", "director", "average ratings", "title", "character", "genre"], "instance": {"id": "2336", "words": ["who", "stars", "in", "world", "for", "ransom"], "labels": ["O", "O", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, review, year, rating, song, plot, trailer, director, average ratings, title, character, genre and O.\nSentence: who stars in world for ransom", "prompt_labels": "who(O) stars(O) in(O) world(B-title) for(I-title) ransom(I-title)"}}
{"id": "465", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "title", "genre", "actor", "director", "trailer", "song", "review", "plot", "rating", "average ratings", "year"], "instance": {"id": "465", "words": ["find", "me", "the", "review", "of", "black", "swan"], "labels": ["O", "O", "O", "B-average ratings", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, title, genre, actor, director, trailer, song, review, plot, rating, average ratings, year and O.\nSentence: find me the review of black swan", "prompt_labels": "find(O) me(O) the(O) review(B-average ratings) of(O) black(B-title) swan(I-title)"}}
{"id": "1123", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "plot", "year", "character", "average ratings", "review", "rating", "title", "director", "genre", "song", "actor"], "instance": {"id": "1123", "words": ["dating", "1980", "s", "movie", "that", "was", "rated", "pg", "13", "and", "was", "rated", "four", "stars"], "labels": ["B-plot", "B-year", "I-year", "O", "O", "O", "O", "B-rating", "I-rating", "O", "O", "O", "B-average ratings", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, plot, year, character, average ratings, review, rating, title, director, genre, song, actor and O.\nSentence: dating 1980 s movie that was rated pg 13 and was rated four stars", "prompt_labels": "dating(B-plot) 1980(B-year) s(I-year) movie(O) that(O) was(O) rated(O) pg(B-rating) 13(I-rating) and(O) was(O) rated(O) four(B-average ratings) stars(O)"}}
{"id": "1746", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "genre", "title", "director", "average ratings", "rating", "character", "trailer", "song", "year", "plot", "actor"], "instance": {"id": "1746", "words": ["please", "list", "a", "1940", "s", "short", "movie", "rated", "pg", "13", "starring", "jon", "stewart"], "labels": ["O", "O", "O", "B-year", "I-year", "B-genre", "O", "O", "B-rating", "I-rating", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, genre, title, director, average ratings, rating, character, trailer, song, year, plot, actor and O.\nSentence: please list a 1940 s short movie rated pg 13 starring jon stewart", "prompt_labels": "please(O) list(O) a(O) 1940(B-year) s(I-year) short(B-genre) movie(O) rated(O) pg(B-rating) 13(I-rating) starring(O) jon(B-actor) stewart(I-actor)"}}
{"id": "2266", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "song", "plot", "actor", "genre", "average ratings", "review", "trailer", "title", "character", "rating", "year"], "instance": {"id": "2266", "words": ["when", "did", "the", "movie", "titled", "b", "girl", "come", "out"], "labels": ["O", "O", "O", "O", "O", "B-title", "I-title", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, song, plot, actor, genre, average ratings, review, trailer, title, character, rating, year and O.\nSentence: when did the movie titled b girl come out", "prompt_labels": "when(O) did(O) the(O) movie(O) titled(O) b(B-title) girl(I-title) come(O) out(O)"}}
{"id": "1677", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "song", "actor", "genre", "title", "trailer", "year", "character", "average ratings", "plot", "review", "rating"], "instance": {"id": "1677", "words": ["list", "an", "unrated", "adventure", "from", "the", "past", "ten", "decades"], "labels": ["O", "O", "B-rating", "B-genre", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, song, actor, genre, title, trailer, year, character, average ratings, plot, review, rating and O.\nSentence: list an unrated adventure from the past ten decades", "prompt_labels": "list(O) an(O) unrated(B-rating) adventure(B-genre) from(O) the(O) past(B-year) ten(I-year) decades(I-year)"}}
{"id": "985", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "average ratings", "review", "trailer", "title", "plot", "director", "year", "rating", "song", "character", "genre"], "instance": {"id": "985", "words": ["what", "film", "features", "the", "song", "kiss", "from", "a", "rose", "by", "seal"], "labels": ["O", "O", "O", "O", "O", "B-song", "I-song", "I-song", "I-song", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, average ratings, review, trailer, title, plot, director, year, rating, song, character, genre and O.\nSentence: what film features the song kiss from a rose by seal", "prompt_labels": "what(O) film(O) features(O) the(O) song(O) kiss(B-song) from(I-song) a(I-song) rose(I-song) by(O) seal(O)"}}
{"id": "501", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "actor", "plot", "genre", "song", "year", "rating", "review", "trailer", "average ratings", "title", "director"], "instance": {"id": "501", "words": ["what", "year", "was", "disneys", "pocohontas", "released"], "labels": ["O", "O", "O", "O", "B-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, actor, plot, genre, song, year, rating, review, trailer, average ratings, title, director and O.\nSentence: what year was disneys pocohontas released", "prompt_labels": "what(O) year(O) was(O) disneys(O) pocohontas(B-title) released(O)"}}
{"id": "1381", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "review", "rating", "song", "title", "character", "trailer", "director", "average ratings", "year", "genre", "actor"], "instance": {"id": "1381", "words": ["im", "looking", "for", "that", "childrens", "movie", "starring", "sylvester", "stallone", "from", "the", "1980", "s"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O", "B-director", "I-director", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, review, rating, song, title, character, trailer, director, average ratings, year, genre, actor and O.\nSentence: im looking for that childrens movie starring sylvester stallone from the 1980 s", "prompt_labels": "im(O) looking(O) for(O) that(O) childrens(B-genre) movie(O) starring(O) sylvester(B-director) stallone(I-director) from(O) the(O) 1980(B-year) s(I-year)"}}
{"id": "2056", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "actor", "trailer", "average ratings", "character", "year", "review", "title", "plot", "rating", "director", "song"], "instance": {"id": "2056", "words": ["what", "is", "that", "six", "star", "chick", "flick", "with", "andie", "macdowell", "in", "it"], "labels": ["O", "O", "O", "B-average ratings", "O", "B-genre", "O", "O", "B-actor", "I-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, actor, trailer, average ratings, character, year, review, title, plot, rating, director, song and O.\nSentence: what is that six star chick flick with andie macdowell in it", "prompt_labels": "what(O) is(O) that(O) six(B-average ratings) star(O) chick(B-genre) flick(O) with(O) andie(B-actor) macdowell(I-actor) in(O) it(O)"}}
{"id": "810", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "plot", "review", "year", "average ratings", "genre", "title", "actor", "song", "rating", "director", "character"], "instance": {"id": "810", "words": ["find", "me", "the", "action", "movie", "seven", "starring", "brad", "pitt"], "labels": ["O", "O", "O", "B-genre", "O", "B-title", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, plot, review, year, average ratings, genre, title, actor, song, rating, director, character and O.\nSentence: find me the action movie seven starring brad pitt", "prompt_labels": "find(O) me(O) the(O) action(B-genre) movie(O) seven(B-title) starring(O) brad(B-actor) pitt(I-actor)"}}
{"id": "1447", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "average ratings", "plot", "trailer", "review", "character", "actor", "song", "year", "director", "rating", "genre"], "instance": {"id": "1447", "words": ["is", "it", "true", "that", "kate", "mulgrew", "was", "born", "in", "1940"], "labels": ["O", "O", "O", "O", "B-actor", "I-actor", "O", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, average ratings, plot, trailer, review, character, actor, song, year, director, rating, genre and O.\nSentence: is it true that kate mulgrew was born in 1940", "prompt_labels": "is(O) it(O) true(O) that(O) kate(B-actor) mulgrew(I-actor) was(O) born(O) in(O) 1940(B-year)"}}
{"id": "1986", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "review", "average ratings", "year", "character", "genre", "plot", "director", "trailer", "song", "rating", "actor"], "instance": {"id": "1986", "words": ["what", "is", "the", "first", "20", "million", "is", "always", "the", "hardest", "about"], "labels": ["O", "O", "B-title", "I-title", "I-title", "I-title", "I-title", "I-title", "I-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, review, average ratings, year, character, genre, plot, director, trailer, song, rating, actor and O.\nSentence: what is the first 20 million is always the hardest about", "prompt_labels": "what(O) is(O) the(B-title) first(I-title) 20(I-title) million(I-title) is(I-title) always(I-title) the(I-title) hardest(I-title) about(O)"}}
{"id": "804", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "actor", "plot", "title", "rating", "song", "trailer", "director", "year", "character", "review", "genre"], "instance": {"id": "804", "words": ["look", "for", "a", "movie", "filmed", "in", "california"], "labels": ["O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, actor, plot, title, rating, song, trailer, director, year, character, review, genre and O.\nSentence: look for a movie filmed in california", "prompt_labels": "look(O) for(O) a(O) movie(O) filmed(O) in(O) california(O)"}}
{"id": "1393", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "trailer", "song", "year", "actor", "character", "review", "genre", "average ratings", "director", "title", "plot"], "instance": {"id": "1393", "words": ["in", "1970", "was", "deidre", "hall", "in", "an", "unrated", "short", "film"], "labels": ["O", "B-year", "O", "B-actor", "I-actor", "O", "O", "B-rating", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, trailer, song, year, actor, character, review, genre, average ratings, director, title, plot and O.\nSentence: in 1970 was deidre hall in an unrated short film", "prompt_labels": "in(O) 1970(B-year) was(O) deidre(B-actor) hall(I-actor) in(O) an(O) unrated(B-rating) short(B-genre) film(O)"}}
{"id": "2013", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "plot", "review", "title", "song", "year", "trailer", "director", "character", "rating", "actor", "average ratings"], "instance": {"id": "2013", "words": ["what", "is", "a", "good", "pg", "13", "science", "fiction", "film", "that", "michael", "kastenbaum", "directed", "in", "the", "last", "eight", "decades"], "labels": ["O", "O", "O", "O", "B-rating", "I-rating", "B-genre", "I-genre", "O", "O", "B-director", "I-director", "O", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, plot, review, title, song, year, trailer, director, character, rating, actor, average ratings and O.\nSentence: what is a good pg 13 science fiction film that michael kastenbaum directed in the last eight decades", "prompt_labels": "what(O) is(O) a(O) good(O) pg(B-rating) 13(I-rating) science(B-genre) fiction(I-genre) film(O) that(O) michael(B-director) kastenbaum(I-director) directed(O) in(O) the(O) last(B-year) eight(I-year) decades(I-year)"}}
{"id": "46", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "average ratings", "song", "plot", "year", "actor", "title", "trailer", "character", "review", "genre", "rating"], "instance": {"id": "46", "words": ["present", "list", "of", "family", "movies", "that", "chris", "columbus", "directed"], "labels": ["O", "O", "O", "B-genre", "I-genre", "O", "B-director", "I-director", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, average ratings, song, plot, year, actor, title, trailer, character, review, genre, rating and O.\nSentence: present list of family movies that chris columbus directed", "prompt_labels": "present(O) list(O) of(O) family(B-genre) movies(I-genre) that(O) chris(B-director) columbus(I-director) directed(O)"}}
{"id": "867", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "plot", "director", "actor", "character", "title", "genre", "year", "trailer", "review", "rating", "song"], "instance": {"id": "867", "words": ["what", "movie", "featured", "drew", "barrymore", "in", "her", "breakout", "role"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, plot, director, actor, character, title, genre, year, trailer, review, rating, song and O.\nSentence: what movie featured drew barrymore in her breakout role", "prompt_labels": "what(O) movie(O) featured(O) drew(B-actor) barrymore(I-actor) in(O) her(O) breakout(O) role(O)"}}
{"id": "1969", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "review", "actor", "character", "director", "title", "song", "plot", "genre", "trailer", "average ratings", "rating"], "instance": {"id": "1969", "words": ["what", "history", "movie", "starred", "eva", "larue", "and", "got", "good", "reviews"], "labels": ["O", "B-genre", "O", "O", "B-actor", "I-actor", "O", "O", "B-average ratings", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, review, actor, character, director, title, song, plot, genre, trailer, average ratings, rating and O.\nSentence: what history movie starred eva larue and got good reviews", "prompt_labels": "what(O) history(B-genre) movie(O) starred(O) eva(B-actor) larue(I-actor) and(O) got(O) good(B-average ratings) reviews(O)"}}
{"id": "1658", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "director", "average ratings", "song", "year", "actor", "rating", "review", "genre", "plot", "title", "trailer"], "instance": {"id": "1658", "words": ["list", "all", "fantasy", "movies", "that", "have", "been", "released", "in", "the", "past", "decade"], "labels": ["O", "O", "B-genre", "O", "O", "O", "O", "O", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, director, average ratings, song, year, actor, rating, review, genre, plot, title, trailer and O.\nSentence: list all fantasy movies that have been released in the past decade", "prompt_labels": "list(O) all(O) fantasy(B-genre) movies(O) that(O) have(O) been(O) released(O) in(O) the(O) past(B-year) decade(I-year)"}}
{"id": "1718", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "average ratings", "year", "director", "rating", "song", "actor", "character", "title", "plot", "review", "genre"], "instance": {"id": "1718", "words": ["name", "a", "bruce", "boxleitner", "comedy", "that", "is", "rated", "g"], "labels": ["O", "O", "B-actor", "I-actor", "B-genre", "O", "O", "O", "B-rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, average ratings, year, director, rating, song, actor, character, title, plot, review, genre and O.\nSentence: name a bruce boxleitner comedy that is rated g", "prompt_labels": "name(O) a(O) bruce(B-actor) boxleitner(I-actor) comedy(B-genre) that(O) is(O) rated(O) g(B-rating)"}}
{"id": "1417", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "actor", "plot", "trailer", "director", "rating", "genre", "character", "song", "title", "average ratings", "review"], "instance": {"id": "1417", "words": ["in", "the", "last", "six", "years", "has", "there", "been", "a", "film", "about", "artists", "with", "alexandre", "rockwell", "in", "it"], "labels": ["O", "O", "B-year", "I-year", "I-year", "O", "O", "O", "O", "O", "O", "B-plot", "O", "B-director", "I-director", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, actor, plot, trailer, director, rating, genre, character, song, title, average ratings, review and O.\nSentence: in the last six years has there been a film about artists with alexandre rockwell in it", "prompt_labels": "in(O) the(O) last(B-year) six(I-year) years(I-year) has(O) there(O) been(O) a(O) film(O) about(O) artists(B-plot) with(O) alexandre(B-director) rockwell(I-director) in(O) it(O)"}}
{"id": "64", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "rating", "song", "plot", "genre", "character", "year", "trailer", "review", "average ratings", "title", "director"], "instance": {"id": "64", "words": ["whats", "the", "latetest", "foreign", "romantic", "movie", "with", "lots", "of", "sex", "and", "sadness"], "labels": ["O", "O", "B-year", "B-genre", "I-genre", "O", "O", "O", "O", "B-plot", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, rating, song, plot, genre, character, year, trailer, review, average ratings, title, director and O.\nSentence: whats the latetest foreign romantic movie with lots of sex and sadness", "prompt_labels": "whats(O) the(O) latetest(B-year) foreign(B-genre) romantic(I-genre) movie(O) with(O) lots(O) of(O) sex(B-plot) and(O) sadness(B-plot)"}}
{"id": "210", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "director", "rating", "trailer", "character", "average ratings", "actor", "genre", "year", "plot", "title", "song"], "instance": {"id": "210", "words": ["how", "many", "lord", "of", "the", "rings", "films", "are", "there"], "labels": ["O", "O", "B-title", "I-title", "I-title", "I-title", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, director, rating, trailer, character, average ratings, actor, genre, year, plot, title, song and O.\nSentence: how many lord of the rings films are there", "prompt_labels": "how(O) many(O) lord(B-title) of(I-title) the(I-title) rings(I-title) films(O) are(O) there(O)"}}
{"id": "1180", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "rating", "plot", "trailer", "director", "actor", "year", "average ratings", "character", "genre", "song", "review"], "instance": {"id": "1180", "words": ["did", "director", "stefan", "popescu", "make", "a", "movie", "about", "veganism"], "labels": ["O", "O", "B-director", "I-director", "O", "O", "O", "O", "B-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, rating, plot, trailer, director, actor, year, average ratings, character, genre, song, review and O.\nSentence: did director stefan popescu make a movie about veganism", "prompt_labels": "did(O) director(O) stefan(B-director) popescu(I-director) make(O) a(O) movie(O) about(O) veganism(B-plot)"}}
{"id": "1303", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "actor", "review", "song", "year", "director", "trailer", "rating", "plot", "title", "genre", "average ratings"], "instance": {"id": "1303", "words": ["how", "likely", "is", "antony", "sher", "to", "have", "been", "in", "a", "movie", "as", "early", "as", "1950", "that", "was", "rated", "at", "least", "seven", "stars"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-year", "O", "O", "O", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, actor, review, song, year, director, trailer, rating, plot, title, genre, average ratings and O.\nSentence: how likely is antony sher to have been in a movie as early as 1950 that was rated at least seven stars", "prompt_labels": "how(O) likely(O) is(O) antony(B-actor) sher(I-actor) to(O) have(O) been(O) in(O) a(O) movie(O) as(O) early(O) as(O) 1950(B-year) that(O) was(O) rated(O) at(O) least(O) seven(B-average ratings) stars(I-average ratings)"}}
{"id": "388", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "character", "year", "actor", "plot", "genre", "song", "title", "average ratings", "director", "review", "rating"], "instance": {"id": "388", "words": ["who", "played", "leo", "marvin", "in", "what", "about", "bob"], "labels": ["O", "O", "B-character", "I-character", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, character, year, actor, plot, genre, song, title, average ratings, director, review, rating and O.\nSentence: who played leo marvin in what about bob", "prompt_labels": "who(O) played(O) leo(B-character) marvin(I-character) in(O) what(B-title) about(I-title) bob(I-title)"}}
{"id": "1001", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "year", "review", "title", "rating", "average ratings", "trailer", "plot", "genre", "song", "actor", "director"], "instance": {"id": "1001", "words": ["in", "what", "genre", "is", "the", "movie", "par", "6"], "labels": ["O", "O", "B-genre", "O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, year, review, title, rating, average ratings, trailer, plot, genre, song, actor, director and O.\nSentence: in what genre is the movie par 6", "prompt_labels": "in(O) what(O) genre(B-genre) is(O) the(O) movie(O) par(B-title) 6(I-title)"}}
{"id": "1120", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "title", "song", "average ratings", "actor", "genre", "year", "rating", "character", "plot", "director", "review"], "instance": {"id": "1120", "words": ["could", "you", "help", "me", "find", "a", "movie", "starring", "larenz", "tate", "that", "was", "made", "in", "the", "past", "eight", "years", "that", "was", "rated", "well"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-actor", "I-actor", "O", "O", "O", "O", "O", "B-year", "I-year", "I-year", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, title, song, average ratings, actor, genre, year, rating, character, plot, director, review and O.\nSentence: could you help me find a movie starring larenz tate that was made in the past eight years that was rated well", "prompt_labels": "could(O) you(O) help(O) me(O) find(O) a(O) movie(O) starring(O) larenz(B-actor) tate(I-actor) that(O) was(O) made(O) in(O) the(O) past(B-year) eight(I-year) years(I-year) that(O) was(O) rated(B-average ratings) well(I-average ratings)"}}
{"id": "609", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "song", "rating", "average ratings", "character", "trailer", "review", "plot", "director", "genre", "year", "actor"], "instance": {"id": "609", "words": ["show", "me", "a", "bruce", "willis", "movie", "about", "ghosts"], "labels": ["O", "O", "O", "B-actor", "I-actor", "B-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, song, rating, average ratings, character, trailer, review, plot, director, genre, year, actor and O.\nSentence: show me a bruce willis movie about ghosts", "prompt_labels": "show(O) me(O) a(O) bruce(B-actor) willis(I-actor) movie(B-plot) about(I-plot) ghosts(I-plot)"}}
{"id": "431", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "year", "genre", "average ratings", "review", "trailer", "song", "rating", "character", "title", "director", "actor"], "instance": {"id": "431", "words": ["show", "me", "a", "list", "of", "family", "movies", "with", "holiday", "themes"], "labels": ["O", "O", "O", "O", "O", "B-genre", "O", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, year, genre, average ratings, review, trailer, song, rating, character, title, director, actor and O.\nSentence: show me a list of family movies with holiday themes", "prompt_labels": "show(O) me(O) a(O) list(O) of(O) family(B-genre) movies(O) with(O) holiday(B-plot) themes(I-plot)"}}
{"id": "102", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "review", "average ratings", "genre", "plot", "character", "rating", "director", "actor", "year", "trailer", "song"], "instance": {"id": "102", "words": ["avatar", "came", "out", "when", "and", "what", "did", "it", "gross"], "labels": ["B-title", "O", "O", "B-year", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, review, average ratings, genre, plot, character, rating, director, actor, year, trailer, song and O.\nSentence: avatar came out when and what did it gross", "prompt_labels": "avatar(B-title) came(O) out(O) when(B-year) and(O) what(O) did(O) it(O) gross(O)"}}
{"id": "2358", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "title", "song", "average ratings", "trailer", "genre", "year", "rating", "actor", "review", "plot", "director"], "instance": {"id": "2358", "words": ["is", "there", "a", "a", "film", "noir", "movie", "with", "ridley", "scott", "from", "the", "2000", "s"], "labels": ["O", "O", "O", "O", "B-genre", "I-genre", "O", "O", "B-director", "I-director", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, title, song, average ratings, trailer, genre, year, rating, actor, review, plot, director and O.\nSentence: is there a a film noir movie with ridley scott from the 2000 s", "prompt_labels": "is(O) there(O) a(O) a(O) film(B-genre) noir(I-genre) movie(O) with(O) ridley(B-director) scott(I-director) from(O) the(O) 2000(B-year) s(I-year)"}}
{"id": "748", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "rating", "genre", "character", "average ratings", "review", "trailer", "song", "actor", "director", "plot", "title"], "instance": {"id": "748", "words": ["show", "me", "the", "lindsey", "anderson", "movies", "starring", "malcolm", "mcdowell"], "labels": ["O", "O", "O", "B-director", "I-director", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, rating, genre, character, average ratings, review, trailer, song, actor, director, plot, title and O.\nSentence: show me the lindsey anderson movies starring malcolm mcdowell", "prompt_labels": "show(O) me(O) the(O) lindsey(B-director) anderson(I-director) movies(O) starring(O) malcolm(B-actor) mcdowell(I-actor)"}}
{"id": "2376", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "average ratings", "plot", "year", "song", "trailer", "genre", "title", "actor", "character", "rating", "review"], "instance": {"id": "2376", "words": ["name", "an", "adventure", "movie", "directed", "by", "francis", "ford", "coppola"], "labels": ["O", "O", "B-genre", "O", "O", "O", "B-director", "I-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, average ratings, plot, year, song, trailer, genre, title, actor, character, rating, review and O.\nSentence: name an adventure movie directed by francis ford coppola", "prompt_labels": "name(O) an(O) adventure(B-genre) movie(O) directed(O) by(O) francis(B-director) ford(I-director) coppola(I-director)"}}
{"id": "463", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "rating", "character", "review", "trailer", "title", "year", "song", "genre", "actor", "director", "plot"], "instance": {"id": "463", "words": ["who", "was", "the", "cast", "of", "airplane"], "labels": ["O", "O", "O", "O", "O", "B-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, rating, character, review, trailer, title, year, song, genre, actor, director, plot and O.\nSentence: who was the cast of airplane", "prompt_labels": "who(O) was(O) the(O) cast(O) of(O) airplane(B-title)"}}
{"id": "313", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "rating", "genre", "trailer", "director", "review", "title", "character", "plot", "year", "actor", "song"], "instance": {"id": "313", "words": ["can", "you", "find", "me", "a", "romantic", "comedy", "with", "natalie", "portman"], "labels": ["O", "O", "O", "O", "O", "B-plot", "B-genre", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, rating, genre, trailer, director, review, title, character, plot, year, actor, song and O.\nSentence: can you find me a romantic comedy with natalie portman", "prompt_labels": "can(O) you(O) find(O) me(O) a(O) romantic(B-plot) comedy(B-genre) with(O) natalie(B-actor) portman(I-actor)"}}
{"id": "581", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "year", "rating", "actor", "song", "average ratings", "title", "genre", "character", "director", "review", "trailer"], "instance": {"id": "581", "words": ["find", "pg", "13", "movies", "with", "kevin", "costner"], "labels": ["O", "B-rating", "I-rating", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, year, rating, actor, song, average ratings, title, genre, character, director, review, trailer and O.\nSentence: find pg 13 movies with kevin costner", "prompt_labels": "find(O) pg(B-rating) 13(I-rating) movies(O) with(O) kevin(B-actor) costner(I-actor)"}}
{"id": "908", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "director", "genre", "actor", "review", "title", "average ratings", "plot", "character", "rating", "trailer", "year"], "instance": {"id": "908", "words": ["what", "year", "was", "psycho", "released"], "labels": ["O", "O", "O", "B-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, director, genre, actor, review, title, average ratings, plot, character, rating, trailer, year and O.\nSentence: what year was psycho released", "prompt_labels": "what(O) year(O) was(O) psycho(B-title) released(O)"}}
{"id": "1805", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "trailer", "rating", "average ratings", "year", "actor", "plot", "review", "character", "song", "director", "title"], "instance": {"id": "1805", "words": ["was", "samuel", "l", "jackson", "in", "any", "crime", "movies", "in", "the", "1960", "s", "when", "he", "was", "a", "child"], "labels": ["O", "B-actor", "I-actor", "I-actor", "O", "O", "B-genre", "O", "O", "O", "B-year", "I-year", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, trailer, rating, average ratings, year, actor, plot, review, character, song, director, title and O.\nSentence: was samuel l jackson in any crime movies in the 1960 s when he was a child", "prompt_labels": "was(O) samuel(B-actor) l(I-actor) jackson(I-actor) in(O) any(O) crime(B-genre) movies(O) in(O) the(O) 1960(B-year) s(I-year) when(O) he(O) was(O) a(O) child(O)"}}
{"id": "1668", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "character", "genre", "review", "actor", "trailer", "plot", "song", "year", "rating", "director", "title"], "instance": {"id": "1668", "words": ["list", "an", "r", "rated", "highly", "recommended", "political", "film", "from", "the", "past", "year"], "labels": ["O", "O", "B-rating", "O", "B-average ratings", "I-average ratings", "B-genre", "O", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, character, genre, review, actor, trailer, plot, song, year, rating, director, title and O.\nSentence: list an r rated highly recommended political film from the past year", "prompt_labels": "list(O) an(O) r(B-rating) rated(O) highly(B-average ratings) recommended(I-average ratings) political(B-genre) film(O) from(O) the(O) past(B-year) year(I-year)"}}
{"id": "1758", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "genre", "character", "trailer", "plot", "review", "rating", "director", "title", "actor", "year", "average ratings"], "instance": {"id": "1758", "words": ["rose", "jackson", "starred", "in", "this", "g", "rated", "film", "of", "the", "2010", "s"], "labels": ["B-actor", "I-actor", "O", "O", "O", "B-rating", "O", "O", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, genre, character, trailer, plot, review, rating, director, title, actor, year, average ratings and O.\nSentence: rose jackson starred in this g rated film of the 2010 s", "prompt_labels": "rose(B-actor) jackson(I-actor) starred(O) in(O) this(O) g(B-rating) rated(O) film(O) of(O) the(O) 2010(B-year) s(I-year)"}}
{"id": "1819", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "title", "character", "average ratings", "rating", "actor", "year", "review", "genre", "song", "trailer", "director"], "instance": {"id": "1819", "words": ["what", "1940", "hit", "received", "all", "right", "ratings", "and", "was", "based", "on", "a", "culture", "clash", "directed", "by", "mark", "romanek"], "labels": ["O", "B-year", "O", "O", "B-average ratings", "I-average ratings", "O", "O", "O", "O", "O", "O", "B-plot", "I-plot", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, title, character, average ratings, rating, actor, year, review, genre, song, trailer, director and O.\nSentence: what 1940 hit received all right ratings and was based on a culture clash directed by mark romanek", "prompt_labels": "what(O) 1940(B-year) hit(O) received(O) all(B-average ratings) right(I-average ratings) ratings(O) and(O) was(O) based(O) on(O) a(O) culture(B-plot) clash(I-plot) directed(O) by(O) mark(B-director) romanek(I-director)"}}
{"id": "672", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "trailer", "director", "review", "actor", "genre", "plot", "title", "character", "song", "average ratings", "year"], "instance": {"id": "672", "words": ["are", "there", "any", "pg", "disney", "movies"], "labels": ["O", "O", "O", "B-rating", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, trailer, director, review, actor, genre, plot, title, character, song, average ratings, year and O.\nSentence: are there any pg disney movies", "prompt_labels": "are(O) there(O) any(O) pg(B-rating) disney(B-genre) movies(O)"}}
{"id": "1989", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "trailer", "song", "genre", "rating", "title", "plot", "year", "average ratings", "character", "director", "review"], "instance": {"id": "1989", "words": ["what", "is", "a", "1940", "nc", "17", "short", "movie", "with", "a", "average", "rating", "of", "five", "directed", "by", "david", "vonallmen"], "labels": ["O", "O", "O", "B-year", "B-rating", "I-rating", "B-genre", "O", "O", "O", "O", "O", "O", "B-average ratings", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, trailer, song, genre, rating, title, plot, year, average ratings, character, director, review and O.\nSentence: what is a 1940 nc 17 short movie with a average rating of five directed by david vonallmen", "prompt_labels": "what(O) is(O) a(O) 1940(B-year) nc(B-rating) 17(I-rating) short(B-genre) movie(O) with(O) a(O) average(O) rating(O) of(O) five(B-average ratings) directed(O) by(O) david(B-director) vonallmen(I-director)"}}
{"id": "2335", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "actor", "rating", "trailer", "average ratings", "review", "genre", "plot", "character", "director", "year", "song"], "instance": {"id": "2335", "words": ["who", "stars", "in", "west", "is", "west"], "labels": ["O", "O", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, actor, rating, trailer, average ratings, review, genre, plot, character, director, year, song and O.\nSentence: who stars in west is west", "prompt_labels": "who(O) stars(O) in(O) west(B-title) is(I-title) west(I-title)"}}
{"id": "63", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "review", "rating", "plot", "title", "genre", "director", "song", "actor", "average ratings", "trailer", "character"], "instance": {"id": "63", "words": ["how", "many", "times", "has", "matt", "damon", "been", "jason", "bourne"], "labels": ["O", "O", "O", "O", "B-actor", "I-actor", "O", "B-character", "I-character"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, review, rating, plot, title, genre, director, song, actor, average ratings, trailer, character and O.\nSentence: how many times has matt damon been jason bourne", "prompt_labels": "how(O) many(O) times(O) has(O) matt(B-actor) damon(I-actor) been(O) jason(B-character) bourne(I-character)"}}
{"id": "500", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "plot", "trailer", "average ratings", "character", "genre", "song", "title", "director", "rating", "review", "year"], "instance": {"id": "500", "words": ["play", "a", "trailer", "for", "teen", "wolf"], "labels": ["O", "O", "B-trailer", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, plot, trailer, average ratings, character, genre, song, title, director, rating, review, year and O.\nSentence: play a trailer for teen wolf", "prompt_labels": "play(O) a(O) trailer(B-trailer) for(O) teen(B-title) wolf(I-title)"}}
{"id": "1666", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "plot", "rating", "director", "trailer", "average ratings", "song", "year", "genre", "actor", "title", "character"], "instance": {"id": "1666", "words": ["list", "an", "r", "rated", "drama", "starring", "yasmine", "bleeth"], "labels": ["O", "O", "B-rating", "O", "B-genre", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, plot, rating, director, trailer, average ratings, song, year, genre, actor, title, character and O.\nSentence: list an r rated drama starring yasmine bleeth", "prompt_labels": "list(O) an(O) r(B-rating) rated(O) drama(B-genre) starring(O) yasmine(B-actor) bleeth(I-actor)"}}
{"id": "1438", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "actor", "genre", "average ratings", "year", "director", "trailer", "song", "rating", "plot", "review", "character"], "instance": {"id": "1438", "words": ["is", "james", "cagney", "in", "the", "shining"], "labels": ["O", "B-actor", "I-actor", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, actor, genre, average ratings, year, director, trailer, song, rating, plot, review, character and O.\nSentence: is james cagney in the shining", "prompt_labels": "is(O) james(B-actor) cagney(I-actor) in(O) the(B-title) shining(I-title)"}}
{"id": "1020", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "year", "genre", "review", "rating", "plot", "title", "actor", "trailer", "director", "song", "character"], "instance": {"id": "1020", "words": ["show", "me", "the", "peter", "greenaway", "movie", "based", "on", "the", "tempest"], "labels": ["O", "O", "O", "B-director", "I-director", "O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, year, genre, review, rating, plot, title, actor, trailer, director, song, character and O.\nSentence: show me the peter greenaway movie based on the tempest", "prompt_labels": "show(O) me(O) the(O) peter(B-director) greenaway(I-director) movie(O) based(O) on(O) the(B-title) tempest(I-title)"}}
{"id": "1700", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "average ratings", "trailer", "review", "title", "genre", "director", "year", "rating", "song", "character", "actor"], "instance": {"id": "1700", "words": ["list", "some", "really", "good", "mystery", "movies"], "labels": ["O", "O", "B-average ratings", "I-average ratings", "B-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, average ratings, trailer, review, title, genre, director, year, rating, song, character, actor and O.\nSentence: list some really good mystery movies", "prompt_labels": "list(O) some(O) really(B-average ratings) good(I-average ratings) mystery(B-genre) movies(O)"}}
{"id": "319", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "character", "year", "rating", "average ratings", "song", "review", "director", "actor", "title", "genre", "trailer"], "instance": {"id": "319", "words": ["was", "the", "rock", "in", "drive"], "labels": ["O", "O", "B-actor", "O", "B-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, character, year, rating, average ratings, song, review, director, actor, title, genre, trailer and O.\nSentence: was the rock in drive", "prompt_labels": "was(O) the(O) rock(B-actor) in(O) drive(B-title)"}}
{"id": "1084", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "character", "year", "review", "rating", "plot", "title", "average ratings", "director", "song", "trailer", "genre"], "instance": {"id": "1084", "words": ["are", "there", "any", "animation", "films", "from", "2010", "with", "actress", "cybill", "shepherd"], "labels": ["O", "O", "O", "B-genre", "O", "O", "B-year", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, character, year, review, rating, plot, title, average ratings, director, song, trailer, genre and O.\nSentence: are there any animation films from 2010 with actress cybill shepherd", "prompt_labels": "are(O) there(O) any(O) animation(B-genre) films(O) from(O) 2010(B-year) with(O) actress(O) cybill(B-actor) shepherd(I-actor)"}}
{"id": "1496", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "actor", "director", "rating", "average ratings", "review", "genre", "plot", "song", "year", "character", "trailer"], "instance": {"id": "1496", "words": ["is", "there", "a", "good", "sci", "fi", "movie", "that", "could", "be", "recommended"], "labels": ["O", "O", "O", "O", "B-genre", "I-genre", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, actor, director, rating, average ratings, review, genre, plot, song, year, character, trailer and O.\nSentence: is there a good sci fi movie that could be recommended", "prompt_labels": "is(O) there(O) a(O) good(O) sci(B-genre) fi(I-genre) movie(O) that(O) could(O) be(O) recommended(O)"}}
{"id": "204", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "actor", "year", "average ratings", "character", "song", "review", "title", "rating", "trailer", "genre", "plot"], "instance": {"id": "204", "words": ["did", "people", "like", "or", "hate", "the", "last", "twilight", "movie"], "labels": ["O", "O", "B-review", "O", "B-review", "O", "O", "B-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, year, average ratings, character, song, review, title, rating, trailer, genre, plot and O.\nSentence: did people like or hate the last twilight movie", "prompt_labels": "did(O) people(O) like(B-review) or(O) hate(B-review) the(O) last(O) twilight(B-title) movie(O)"}}
{"id": "761", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "rating", "plot", "director", "trailer", "actor", "character", "average ratings", "genre", "song", "title", "review"], "instance": {"id": "761", "words": ["what", "animated", "movies", "were", "nominated", "for", "oscars"], "labels": ["O", "B-genre", "O", "O", "B-average ratings", "I-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, rating, plot, director, trailer, actor, character, average ratings, genre, song, title, review and O.\nSentence: what animated movies were nominated for oscars", "prompt_labels": "what(O) animated(B-genre) movies(O) were(O) nominated(B-average ratings) for(I-average ratings) oscars(I-average ratings)"}}
{"id": "899", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "plot", "average ratings", "review", "year", "director", "genre", "title", "song", "trailer", "character", "rating"], "instance": {"id": "899", "words": ["who", "is", "the", "star", "of", "twilight"], "labels": ["O", "O", "O", "O", "O", "B-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, plot, average ratings, review, year, director, genre, title, song, trailer, character, rating and O.\nSentence: who is the star of twilight", "prompt_labels": "who(O) is(O) the(O) star(O) of(O) twilight(B-title)"}}
{"id": "83", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "plot", "genre", "title", "character", "director", "average ratings", "trailer", "review", "rating", "song", "actor"], "instance": {"id": "83", "words": ["find", "action", "movies", "featuring", "comic", "book", "characters"], "labels": ["O", "B-genre", "O", "O", "B-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, plot, genre, title, character, director, average ratings, trailer, review, rating, song, actor and O.\nSentence: find action movies featuring comic book characters", "prompt_labels": "find(O) action(B-genre) movies(O) featuring(O) comic(B-plot) book(I-plot) characters(I-plot)"}}
{"id": "1453", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "review", "song", "actor", "title", "director", "genre", "plot", "trailer", "average ratings", "character", "year"], "instance": {"id": "1453", "words": ["is", "there", "a", "1970", "s", "avant", "garde", "film"], "labels": ["O", "O", "O", "B-year", "I-year", "B-genre", "I-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, review, song, actor, title, director, genre, plot, trailer, average ratings, character, year and O.\nSentence: is there a 1970 s avant garde film", "prompt_labels": "is(O) there(O) a(O) 1970(B-year) s(I-year) avant(B-genre) garde(I-genre) film(O)"}}
{"id": "1414", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "song", "character", "review", "actor", "genre", "rating", "average ratings", "trailer", "year", "plot", "director"], "instance": {"id": "1414", "words": ["in", "the", "last", "four", "years", "which", "movie", "did", "donnie", "walberg", "appear", "in", "that", "was", "pg", "13"], "labels": ["O", "O", "B-year", "I-year", "I-year", "O", "O", "O", "B-actor", "I-actor", "O", "O", "O", "O", "B-rating", "I-rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, song, character, review, actor, genre, rating, average ratings, trailer, year, plot, director and O.\nSentence: in the last four years which movie did donnie walberg appear in that was pg 13", "prompt_labels": "in(O) the(O) last(B-year) four(I-year) years(I-year) which(O) movie(O) did(O) donnie(B-actor) walberg(I-actor) appear(O) in(O) that(O) was(O) pg(B-rating) 13(I-rating)"}}
{"id": "1394", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "year", "plot", "character", "director", "average ratings", "title", "song", "trailer", "rating", "review", "actor"], "instance": {"id": "1394", "words": ["in", "1970", "was", "kellie", "martin", "in", "a", "pg", "13", "science", "fiction", "movie", "that", "was", "highly", "recommended"], "labels": ["O", "B-year", "O", "B-actor", "I-actor", "O", "O", "B-rating", "I-rating", "B-genre", "I-genre", "O", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, year, plot, character, director, average ratings, title, song, trailer, rating, review, actor and O.\nSentence: in 1970 was kellie martin in a pg 13 science fiction movie that was highly recommended", "prompt_labels": "in(O) 1970(B-year) was(O) kellie(B-actor) martin(I-actor) in(O) a(O) pg(B-rating) 13(I-rating) science(B-genre) fiction(I-genre) movie(O) that(O) was(O) highly(B-average ratings) recommended(I-average ratings)"}}
{"id": "266", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "year", "review", "rating", "song", "character", "director", "actor", "trailer", "plot", "title", "average ratings"], "instance": {"id": "266", "words": ["what", "was", "the", "third", "sequel", "in", "the", "star", "wars", "film", "series"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-title", "I-title", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, year, review, rating, song, character, director, actor, trailer, plot, title, average ratings and O.\nSentence: what was the third sequel in the star wars film series", "prompt_labels": "what(O) was(O) the(O) third(O) sequel(O) in(O) the(O) star(B-title) wars(I-title) film(O) series(O)"}}
{"id": "1606", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "year", "genre", "average ratings", "rating", "review", "director", "character", "actor", "trailer", "song", "plot"], "instance": {"id": "1606", "words": ["list", "a", "comedy", "from", "1950", "with", "a", "ratings", "average", "of", "five", "stars", "starring", "andre", "braugher"], "labels": ["O", "O", "B-genre", "O", "B-year", "O", "O", "O", "O", "O", "B-average ratings", "I-average ratings", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, year, genre, average ratings, rating, review, director, character, actor, trailer, song, plot and O.\nSentence: list a comedy from 1950 with a ratings average of five stars starring andre braugher", "prompt_labels": "list(O) a(O) comedy(B-genre) from(O) 1950(B-year) with(O) a(O) ratings(O) average(O) of(O) five(B-average ratings) stars(I-average ratings) starring(O) andre(B-actor) braugher(I-actor)"}}
{"id": "889", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "actor", "director", "trailer", "year", "plot", "character", "genre", "song", "title", "rating", "review"], "instance": {"id": "889", "words": ["find", "movies", "from", "the", "1990s", "with", "strong", "female", "leads"], "labels": ["O", "O", "O", "O", "B-year", "O", "B-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, actor, director, trailer, year, plot, character, genre, song, title, rating, review and O.\nSentence: find movies from the 1990s with strong female leads", "prompt_labels": "find(O) movies(O) from(O) the(O) 1990s(B-year) with(O) strong(B-plot) female(I-plot) leads(I-plot)"}}
{"id": "306", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "genre", "director", "review", "trailer", "rating", "plot", "year", "character", "average ratings", "actor", "song"], "instance": {"id": "306", "words": ["show", "a", "spy", "movie", "from", "the", "1930s"], "labels": ["O", "O", "B-plot", "O", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, genre, director, review, trailer, rating, plot, year, character, average ratings, actor, song and O.\nSentence: show a spy movie from the 1930s", "prompt_labels": "show(O) a(O) spy(B-plot) movie(O) from(O) the(O) 1930s(B-year)"}}
{"id": "307", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "review", "trailer", "title", "genre", "director", "rating", "actor", "average ratings", "year", "plot", "character"], "instance": {"id": "307", "words": ["look", "for", "the", "movie", "mean", "girls"], "labels": ["O", "O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, review, trailer, title, genre, director, rating, actor, average ratings, year, plot, character and O.\nSentence: look for the movie mean girls", "prompt_labels": "look(O) for(O) the(O) movie(O) mean(B-title) girls(I-title)"}}
{"id": "1043", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "trailer", "plot", "actor", "average ratings", "song", "character", "title", "rating", "year", "review", "director"], "instance": {"id": "1043", "words": ["what", "movies", "has", "jason", "segel", "been", "in"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, trailer, plot, actor, average ratings, song, character, title, rating, year, review, director and O.\nSentence: what movies has jason segel been in", "prompt_labels": "what(O) movies(O) has(O) jason(B-actor) segel(I-actor) been(O) in(O)"}}
{"id": "382", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "average ratings", "title", "actor", "song", "review", "plot", "character", "genre", "trailer", "director", "rating"], "instance": {"id": "382", "words": ["did", "denzel", "washington", "act", "in", "any", "romantic", "comedy", "movies"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "B-genre", "I-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, average ratings, title, actor, song, review, plot, character, genre, trailer, director, rating and O.\nSentence: did denzel washington act in any romantic comedy movies", "prompt_labels": "did(O) denzel(B-actor) washington(I-actor) act(O) in(O) any(O) romantic(B-genre) comedy(I-genre) movies(O)"}}
{"id": "558", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "actor", "trailer", "character", "rating", "review", "average ratings", "director", "song", "year", "genre", "title"], "instance": {"id": "558", "words": ["what", "chucky", "movie", "didnt", "have", "andy", "in", "it"], "labels": ["O", "B-character", "O", "O", "O", "B-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, actor, trailer, character, rating, review, average ratings, director, song, year, genre, title and O.\nSentence: what chucky movie didnt have andy in it", "prompt_labels": "what(O) chucky(B-character) movie(O) didnt(O) have(O) andy(B-actor) in(O) it(O)"}}
{"id": "1271", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "song", "character", "title", "trailer", "year", "genre", "actor", "plot", "review", "director", "average ratings"], "instance": {"id": "1271", "words": ["has", "jennifer", "connelly", "ever", "been", "in", "a", "critically", "acclaimed", "nc", "17", "movie"], "labels": ["O", "B-actor", "I-actor", "O", "O", "O", "O", "B-average ratings", "I-average ratings", "B-rating", "I-rating", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, song, character, title, trailer, year, genre, actor, plot, review, director, average ratings and O.\nSentence: has jennifer connelly ever been in a critically acclaimed nc 17 movie", "prompt_labels": "has(O) jennifer(B-actor) connelly(I-actor) ever(O) been(O) in(O) a(O) critically(B-average ratings) acclaimed(I-average ratings) nc(B-rating) 17(I-rating) movie(O)"}}
{"id": "855", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "year", "trailer", "rating", "song", "character", "actor", "plot", "review", "genre", "average ratings", "director"], "instance": {"id": "855", "words": ["show", "me", "a", "pg", "13", "movie", "about", "aliens"], "labels": ["O", "O", "O", "B-rating", "I-rating", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, year, trailer, rating, song, character, actor, plot, review, genre, average ratings, director and O.\nSentence: show me a pg 13 movie about aliens", "prompt_labels": "show(O) me(O) a(O) pg(B-rating) 13(I-rating) movie(O) about(O) aliens(O)"}}
{"id": "2245", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "trailer", "title", "actor", "genre", "review", "song", "plot", "year", "director", "character", "rating"], "instance": {"id": "2245", "words": ["what", "year", "was", "northwest", "hounded", "police", "film", "was", "released"], "labels": ["O", "O", "O", "B-title", "I-title", "I-title", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, trailer, title, actor, genre, review, song, plot, year, director, character, rating and O.\nSentence: what year was northwest hounded police film was released", "prompt_labels": "what(O) year(O) was(O) northwest(B-title) hounded(I-title) police(I-title) film(O) was(O) released(O)"}}
{"id": "146", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "title", "rating", "actor", "plot", "genre", "year", "review", "character", "trailer", "average ratings", "song"], "instance": {"id": "146", "words": ["are", "there", "any", "scary", "pg", "rated", "movies"], "labels": ["O", "O", "O", "B-genre", "B-rating", "I-rating", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, title, rating, actor, plot, genre, year, review, character, trailer, average ratings, song and O.\nSentence: are there any scary pg rated movies", "prompt_labels": "are(O) there(O) any(O) scary(B-genre) pg(B-rating) rated(I-rating) movies(O)"}}
{"id": "2022", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "actor", "song", "title", "director", "review", "character", "average ratings", "trailer", "genre", "plot", "year"], "instance": {"id": "2022", "words": ["what", "is", "a", "highly", "recommended", "r", "rated", "film", "with", "a", "found", "dead", "plot"], "labels": ["O", "O", "O", "B-average ratings", "I-average ratings", "B-rating", "O", "O", "O", "O", "B-plot", "I-plot", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, actor, song, title, director, review, character, average ratings, trailer, genre, plot, year and O.\nSentence: what is a highly recommended r rated film with a found dead plot", "prompt_labels": "what(O) is(O) a(O) highly(B-average ratings) recommended(I-average ratings) r(B-rating) rated(O) film(O) with(O) a(O) found(B-plot) dead(I-plot) plot(O)"}}
{"id": "363", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "plot", "rating", "genre", "song", "director", "average ratings", "title", "actor", "review", "trailer", "character"], "instance": {"id": "363", "words": ["show", "me", "the", "harry", "potter", "series"], "labels": ["O", "O", "O", "B-character", "I-character", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, plot, rating, genre, song, director, average ratings, title, actor, review, trailer, character and O.\nSentence: show me the harry potter series", "prompt_labels": "show(O) me(O) the(O) harry(B-character) potter(I-character) series(O)"}}
{"id": "331", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "average ratings", "year", "actor", "director", "rating", "genre", "song", "review", "title", "plot", "character"], "instance": {"id": "331", "words": ["find", "movies", "directed", "by", "judd", "apatow"], "labels": ["O", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, average ratings, year, actor, director, rating, genre, song, review, title, plot, character and O.\nSentence: find movies directed by judd apatow", "prompt_labels": "find(O) movies(O) directed(O) by(O) judd(B-director) apatow(I-director)"}}
{"id": "141", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "song", "title", "review", "character", "genre", "actor", "plot", "trailer", "director", "year", "average ratings"], "instance": {"id": "141", "words": ["did", "whitney", "houston", "sing", "in", "the", "preachers", "wife"], "labels": ["O", "B-actor", "I-actor", "O", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, song, title, review, character, genre, actor, plot, trailer, director, year, average ratings and O.\nSentence: did whitney houston sing in the preachers wife", "prompt_labels": "did(O) whitney(B-actor) houston(I-actor) sing(O) in(O) the(B-title) preachers(I-title) wife(I-title)"}}
{"id": "2340", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "actor", "genre", "character", "director", "plot", "title", "review", "trailer", "rating", "year", "average ratings"], "instance": {"id": "2340", "words": ["who", "wrote", "and", "directed", "head", "above", "water"], "labels": ["O", "O", "O", "O", "B-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, actor, genre, character, director, plot, title, review, trailer, rating, year, average ratings and O.\nSentence: who wrote and directed head above water", "prompt_labels": "who(O) wrote(O) and(O) directed(O) head(B-title) above(I-title) water(I-title)"}}
{"id": "519", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "actor", "plot", "rating", "director", "genre", "average ratings", "review", "year", "title", "song", "character"], "instance": {"id": "519", "words": ["what", "was", "the", "last", "terminator", "film", "to", "be", "released"], "labels": ["O", "O", "O", "O", "B-title", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, actor, plot, rating, director, genre, average ratings, review, year, title, song, character and O.\nSentence: what was the last terminator film to be released", "prompt_labels": "what(O) was(O) the(O) last(O) terminator(B-title) film(O) to(O) be(O) released(O)"}}
{"id": "1145", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "trailer", "genre", "year", "plot", "song", "title", "actor", "rating", "review", "character", "average ratings"], "instance": {"id": "1145", "words": ["did", "hayao", "miyazaki", "ever", "direct", "and", "films", "for", "children"], "labels": ["O", "B-director", "I-director", "O", "O", "O", "O", "O", "B-genre"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, trailer, genre, year, plot, song, title, actor, rating, review, character, average ratings and O.\nSentence: did hayao miyazaki ever direct and films for children", "prompt_labels": "did(O) hayao(B-director) miyazaki(I-director) ever(O) direct(O) and(O) films(O) for(O) children(B-genre)"}}
{"id": "1750", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "year", "plot", "actor", "genre", "review", "rating", "trailer", "director", "song", "title", "character"], "instance": {"id": "1750", "words": ["please", "list", "movies", "in", "the", "crime", "genre", "that", "were", "given", "an", "average", "rating", "of", "six"], "labels": ["O", "O", "O", "O", "O", "B-genre", "O", "O", "O", "O", "O", "O", "O", "O", "B-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, year, plot, actor, genre, review, rating, trailer, director, song, title, character and O.\nSentence: please list movies in the crime genre that were given an average rating of six", "prompt_labels": "please(O) list(O) movies(O) in(O) the(O) crime(B-genre) genre(O) that(O) were(O) given(O) an(O) average(O) rating(O) of(O) six(B-average ratings)"}}
{"id": "891", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "actor", "character", "plot", "year", "song", "title", "trailer", "genre", "average ratings", "rating", "director"], "instance": {"id": "891", "words": ["show", "me", "a", "movie", "with", "daniel", "radcliff"], "labels": ["O", "O", "O", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, actor, character, plot, year, song, title, trailer, genre, average ratings, rating, director and O.\nSentence: show me a movie with daniel radcliff", "prompt_labels": "show(O) me(O) a(O) movie(O) with(O) daniel(B-actor) radcliff(I-actor)"}}
{"id": "341", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "genre", "average ratings", "plot", "song", "rating", "character", "title", "trailer", "actor", "review", "year"], "instance": {"id": "341", "words": ["who", "directed", "blazing", "saddles"], "labels": ["O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, genre, average ratings, plot, song, rating, character, title, trailer, actor, review, year and O.\nSentence: who directed blazing saddles", "prompt_labels": "who(O) directed(O) blazing(B-title) saddles(I-title)"}}
{"id": "772", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "actor", "director", "title", "genre", "review", "trailer", "rating", "plot", "song", "average ratings", "character"], "instance": {"id": "772", "words": ["what", "movie", "has", "liev", "shrieber", "and", "sean", "william", "scott", "in", "it", "as", "hockey", "players"], "labels": ["O", "O", "O", "B-actor", "I-actor", "O", "B-actor", "I-actor", "I-actor", "O", "O", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, actor, director, title, genre, review, trailer, rating, plot, song, average ratings, character and O.\nSentence: what movie has liev shrieber and sean william scott in it as hockey players", "prompt_labels": "what(O) movie(O) has(O) liev(B-actor) shrieber(I-actor) and(O) sean(B-actor) william(I-actor) scott(I-actor) in(O) it(O) as(O) hockey(B-plot) players(I-plot)"}}
{"id": "297", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "plot", "review", "trailer", "title", "rating", "song", "actor", "character", "genre", "year", "director"], "instance": {"id": "297", "words": ["what", "was", "the", "third", "harry", "potter", "movie", "called"], "labels": ["O", "O", "O", "B-title", "I-title", "I-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, plot, review, trailer, title, rating, song, actor, character, genre, year, director and O.\nSentence: what was the third harry potter movie called", "prompt_labels": "what(O) was(O) the(O) third(B-title) harry(I-title) potter(I-title) movie(I-title) called(O)"}}
{"id": "1620", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "average ratings", "title", "review", "director", "plot", "actor", "trailer", "genre", "character", "year", "song"], "instance": {"id": "1620", "words": ["list", "a", "highly", "liked", "horror", "film", "that", "is", "rated", "r"], "labels": ["O", "O", "B-average ratings", "I-average ratings", "B-genre", "O", "O", "O", "O", "B-rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, average ratings, title, review, director, plot, actor, trailer, genre, character, year, song and O.\nSentence: list a highly liked horror film that is rated r", "prompt_labels": "list(O) a(O) highly(B-average ratings) liked(I-average ratings) horror(B-genre) film(O) that(O) is(O) rated(O) r(B-rating)"}}
{"id": "1161", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "average ratings", "title", "rating", "actor", "director", "character", "review", "song", "year", "plot", "genre"], "instance": {"id": "1161", "words": ["did", "ron", "howard", "ever", "direct", "a", "musical", "in", "the", "1960", "s"], "labels": ["O", "B-director", "I-director", "O", "O", "O", "B-genre", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, average ratings, title, rating, actor, director, character, review, song, year, plot, genre and O.\nSentence: did ron howard ever direct a musical in the 1960 s", "prompt_labels": "did(O) ron(B-director) howard(I-director) ever(O) direct(O) a(O) musical(B-genre) in(O) the(O) 1960(B-year) s(I-year)"}}
{"id": "128", "dataset": "mit-movie", "split": "dev", "label_list": ["trailer", "title", "review", "rating", "year", "average ratings", "actor", "director", "song", "plot", "character", "genre"], "instance": {"id": "128", "words": ["what", "was", "the", "first", "movie", "patrick", "stewart", "played", "in"], "labels": ["O", "O", "O", "B-year", "O", "B-actor", "I-actor", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, title, review, rating, year, average ratings, actor, director, song, plot, character, genre and O.\nSentence: what was the first movie patrick stewart played in", "prompt_labels": "what(O) was(O) the(O) first(B-year) movie(O) patrick(B-actor) stewart(I-actor) played(O) in(O)"}}
{"id": "916", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "genre", "rating", "character", "average ratings", "actor", "trailer", "song", "plot", "title", "review", "director"], "instance": {"id": "916", "words": ["what", "films", "has", "john", "travolta", "directed"], "labels": ["O", "O", "O", "B-director", "I-director", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, genre, rating, character, average ratings, actor, trailer, song, plot, title, review, director and O.\nSentence: what films has john travolta directed", "prompt_labels": "what(O) films(O) has(O) john(B-director) travolta(I-director) directed(O)"}}
{"id": "287", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "year", "trailer", "plot", "genre", "character", "song", "review", "rating", "title", "director", "average ratings"], "instance": {"id": "287", "words": ["which", "romantic", "comedies", "are", "rated", "pg"], "labels": ["O", "B-genre", "O", "O", "O", "B-rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, year, trailer, plot, genre, character, song, review, rating, title, director, average ratings and O.\nSentence: which romantic comedies are rated pg", "prompt_labels": "which(O) romantic(B-genre) comedies(O) are(O) rated(O) pg(B-rating)"}}
{"id": "1468", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "title", "trailer", "actor", "rating", "song", "genre", "review", "plot", "character", "director", "average ratings"], "instance": {"id": "1468", "words": ["is", "there", "a", "wall", "e", "movie", "starring", "robert", "redford"], "labels": ["O", "O", "O", "B-title", "I-title", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, title, trailer, actor, rating, song, genre, review, plot, character, director, average ratings and O.\nSentence: is there a wall e movie starring robert redford", "prompt_labels": "is(O) there(O) a(O) wall(B-title) e(I-title) movie(O) starring(O) robert(B-actor) redford(I-actor)"}}
{"id": "2005", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "director", "review", "year", "plot", "average ratings", "actor", "character", "rating", "title", "genre", "trailer"], "instance": {"id": "2005", "words": ["what", "is", "a", "four", "star", "revenge", "spaghetti", "western", "film", "directed", "by", "graham", "reznick", "in", "the", "last", "three", "decades"], "labels": ["O", "O", "O", "B-average ratings", "I-average ratings", "B-plot", "B-genre", "I-genre", "O", "O", "O", "B-director", "I-director", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, director, review, year, plot, average ratings, actor, character, rating, title, genre, trailer and O.\nSentence: what is a four star revenge spaghetti western film directed by graham reznick in the last three decades", "prompt_labels": "what(O) is(O) a(O) four(B-average ratings) star(I-average ratings) revenge(B-plot) spaghetti(B-genre) western(I-genre) film(O) directed(O) by(O) graham(B-director) reznick(I-director) in(O) the(O) last(B-year) three(I-year) decades(I-year)"}}
{"id": "1602", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "average ratings", "year", "trailer", "rating", "plot", "actor", "song", "review", "director", "character", "title"], "instance": {"id": "1602", "words": ["list", "a", "wall", "e", "movie", "which", "director", "were", "the", "coen", "brothers"], "labels": ["O", "O", "B-title", "I-title", "O", "O", "O", "O", "B-director", "I-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, average ratings, year, trailer, rating, plot, actor, song, review, director, character, title and O.\nSentence: list a wall e movie which director were the coen brothers", "prompt_labels": "list(O) a(O) wall(B-title) e(I-title) movie(O) which(O) director(O) were(O) the(B-director) coen(I-director) brothers(I-director)"}}
{"id": "1324", "dataset": "mit-movie", "split": "dev", "label_list": ["average ratings", "review", "title", "trailer", "actor", "plot", "song", "character", "rating", "genre", "year", "director"], "instance": {"id": "1324", "words": ["i", "want", "to", "find", "a", "2010", "movie", "that", "was", "directed", "by", "alek", "keshishian", "with", "the", "plot", "being", "about", "striking", "it", "rich"], "labels": ["O", "O", "O", "O", "O", "B-year", "O", "O", "O", "O", "O", "B-director", "I-director", "O", "O", "O", "O", "O", "B-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, review, title, trailer, actor, plot, song, character, rating, genre, year, director and O.\nSentence: i want to find a 2010 movie that was directed by alek keshishian with the plot being about striking it rich", "prompt_labels": "i(O) want(O) to(O) find(O) a(O) 2010(B-year) movie(O) that(O) was(O) directed(O) by(O) alek(B-director) keshishian(I-director) with(O) the(O) plot(O) being(O) about(O) striking(B-plot) it(I-plot) rich(I-plot)"}}
{"id": "192", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "song", "trailer", "rating", "character", "year", "average ratings", "director", "genre", "plot", "actor", "title"], "instance": {"id": "192", "words": ["what", "was", "the", "title", "of", "the", "bio", "pic", "about", "robert", "e", "howard"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-actor", "I-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, trailer, rating, character, year, average ratings, director, genre, plot, actor, title and O.\nSentence: what was the title of the bio pic about robert e howard", "prompt_labels": "what(O) was(O) the(O) title(O) of(O) the(O) bio(O) pic(O) about(O) robert(B-actor) e(I-actor) howard(I-actor)"}}
{"id": "836", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "song", "trailer", "average ratings", "plot", "character", "genre", "rating", "actor", "year", "director", "title"], "instance": {"id": "836", "words": ["find", "me", "the", "movie", "where", "someone", "says", "i", "love", "the", "smell", "of", "napalm", "in", "the", "morning"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, song, trailer, average ratings, plot, character, genre, rating, actor, year, director, title and O.\nSentence: find me the movie where someone says i love the smell of napalm in the morning", "prompt_labels": "find(O) me(O) the(O) movie(O) where(O) someone(O) says(O) i(O) love(O) the(O) smell(O) of(O) napalm(O) in(O) the(O) morning(O)"}}
{"id": "1942", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "director", "title", "review", "actor", "character", "trailer", "year", "average ratings", "rating", "plot", "genre"], "instance": {"id": "1942", "words": ["what", "cowboy", "film", "directed", "by", "iren", "koster", "received", "two", "thumbs", "up"], "labels": ["O", "B-genre", "O", "O", "O", "B-director", "I-director", "O", "B-average ratings", "I-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, director, title, review, actor, character, trailer, year, average ratings, rating, plot, genre and O.\nSentence: what cowboy film directed by iren koster received two thumbs up", "prompt_labels": "what(O) cowboy(B-genre) film(O) directed(O) by(O) iren(B-director) koster(I-director) received(O) two(B-average ratings) thumbs(I-average ratings) up(I-average ratings)"}}
{"id": "432", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "character", "plot", "average ratings", "trailer", "director", "title", "actor", "review", "song", "year", "rating"], "instance": {"id": "432", "words": ["true", "stories", "about", "hispanic", "musicians"], "labels": ["B-genre", "O", "O", "B-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, character, plot, average ratings, trailer, director, title, actor, review, song, year, rating and O.\nSentence: true stories about hispanic musicians", "prompt_labels": "true(B-genre) stories(O) about(O) hispanic(B-plot) musicians(I-plot)"}}
{"id": "604", "dataset": "mit-movie", "split": "dev", "label_list": ["rating", "title", "average ratings", "year", "character", "genre", "actor", "director", "plot", "review", "song", "trailer"], "instance": {"id": "604", "words": ["name", "a", "well", "known", "director", "of", "japanese", "samurai", "movies"], "labels": ["O", "O", "O", "O", "O", "O", "B-genre", "I-genre", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, title, average ratings, year, character, genre, actor, director, plot, review, song, trailer and O.\nSentence: name a well known director of japanese samurai movies", "prompt_labels": "name(O) a(O) well(O) known(O) director(O) of(O) japanese(B-genre) samurai(I-genre) movies(O)"}}
{"id": "1539", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "plot", "character", "trailer", "rating", "review", "director", "song", "genre", "title", "year", "average ratings"], "instance": {"id": "1539", "words": ["is", "there", "a", "spaghetti", "western", "about", "a", "sheriff", "with", "an", "average", "rating"], "labels": ["O", "O", "O", "B-genre", "I-genre", "O", "O", "B-plot", "O", "O", "B-average ratings", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, plot, character, trailer, rating, review, director, song, genre, title, year, average ratings and O.\nSentence: is there a spaghetti western about a sheriff with an average rating", "prompt_labels": "is(O) there(O) a(O) spaghetti(B-genre) western(I-genre) about(O) a(O) sheriff(B-plot) with(O) an(O) average(B-average ratings) rating(O)"}}
{"id": "376", "dataset": "mit-movie", "split": "dev", "label_list": ["review", "genre", "trailer", "year", "song", "average ratings", "director", "rating", "actor", "character", "title", "plot"], "instance": {"id": "376", "words": ["list", "uma", "thurman", "movies"], "labels": ["O", "B-actor", "I-actor", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, genre, trailer, year, song, average ratings, director, rating, actor, character, title, plot and O.\nSentence: list uma thurman movies", "prompt_labels": "list(O) uma(B-actor) thurman(I-actor) movies(O)"}}
{"id": "1054", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "review", "actor", "title", "rating", "genre", "year", "director", "trailer", "song", "average ratings", "character"], "instance": {"id": "1054", "words": ["a", "1980", "well", "rated", "pg", "13", "that", "involves", "fighting", "of", "any", "kind"], "labels": ["O", "B-year", "B-average ratings", "I-average ratings", "B-rating", "I-rating", "O", "O", "B-plot", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, review, actor, title, rating, genre, year, director, trailer, song, average ratings, character and O.\nSentence: a 1980 well rated pg 13 that involves fighting of any kind", "prompt_labels": "a(O) 1980(B-year) well(B-average ratings) rated(I-average ratings) pg(B-rating) 13(I-rating) that(O) involves(O) fighting(B-plot) of(O) any(O) kind(O)"}}
{"id": "1714", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "song", "trailer", "actor", "review", "character", "average ratings", "rating", "director", "title", "genre", "year"], "instance": {"id": "1714", "words": ["movie", "information", "on", "south", "of", "the", "border"], "labels": ["O", "O", "O", "B-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, song, trailer, actor, review, character, average ratings, rating, director, title, genre, year and O.\nSentence: movie information on south of the border", "prompt_labels": "movie(O) information(O) on(O) south(B-title) of(I-title) the(I-title) border(I-title)"}}
{"id": "1047", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "review", "trailer", "plot", "actor", "rating", "title", "director", "average ratings", "year", "song", "genre"], "instance": {"id": "1047", "words": ["in", "what", "year", "was", "kid", "galahad", "released"], "labels": ["O", "O", "O", "O", "B-title", "I-title", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, review, trailer, plot, actor, rating, title, director, average ratings, year, song, genre and O.\nSentence: in what year was kid galahad released", "prompt_labels": "in(O) what(O) year(O) was(O) kid(B-title) galahad(I-title) released(O)"}}
{"id": "1316", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "title", "review", "song", "director", "plot", "genre", "trailer", "average ratings", "rating", "actor", "character"], "instance": {"id": "1316", "words": ["i", "am", "looking", "for", "an", "unrated", "science", "fiction", "movie", "from", "this", "year"], "labels": ["O", "O", "O", "O", "O", "B-rating", "B-genre", "I-genre", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, title, review, song, director, plot, genre, trailer, average ratings, rating, actor, character and O.\nSentence: i am looking for an unrated science fiction movie from this year", "prompt_labels": "i(O) am(O) looking(O) for(O) an(O) unrated(B-rating) science(B-genre) fiction(I-genre) movie(O) from(O) this(B-year) year(I-year)"}}
{"id": "365", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "trailer", "review", "character", "song", "rating", "genre", "actor", "plot", "title", "average ratings", "year"], "instance": {"id": "365", "words": ["play", "a", "song", "from", "blow", "soundtrack"], "labels": ["O", "O", "O", "O", "B-song", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, trailer, review, character, song, rating, genre, actor, plot, title, average ratings, year and O.\nSentence: play a song from blow soundtrack", "prompt_labels": "play(O) a(O) song(O) from(O) blow(B-song) soundtrack(O)"}}
{"id": "625", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "character", "title", "average ratings", "genre", "review", "plot", "director", "trailer", "actor", "song", "rating"], "instance": {"id": "625", "words": ["how", "many", "movies", "were", "released", "in", "the", "year", "2009"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, character, title, average ratings, genre, review, plot, director, trailer, actor, song, rating and O.\nSentence: how many movies were released in the year 2009", "prompt_labels": "how(O) many(O) movies(O) were(O) released(O) in(O) the(O) year(O) 2009(B-year)"}}
{"id": "430", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "review", "title", "rating", "plot", "director", "character", "trailer", "song", "genre", "average ratings", "year"], "instance": {"id": "430", "words": ["what", "film", "genre", "is", "blazing", "saddles"], "labels": ["O", "O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, review, title, rating, plot, director, character, trailer, song, genre, average ratings, year and O.\nSentence: what film genre is blazing saddles", "prompt_labels": "what(O) film(O) genre(O) is(O) blazing(B-title) saddles(I-title)"}}
{"id": "32", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "review", "character", "year", "song", "director", "genre", "average ratings", "plot", "trailer", "title", "rating"], "instance": {"id": "32", "words": ["have", "pg", "13", "movies", "for", "the", "kidz"], "labels": ["O", "B-rating", "I-rating", "O", "O", "O", "B-genre"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, review, character, year, song, director, genre, average ratings, plot, trailer, title, rating and O.\nSentence: have pg 13 movies for the kidz", "prompt_labels": "have(O) pg(B-rating) 13(I-rating) movies(O) for(O) the(O) kidz(B-genre)"}}
{"id": "1575", "dataset": "mit-movie", "split": "dev", "label_list": ["director", "character", "review", "average ratings", "trailer", "title", "year", "actor", "plot", "song", "rating", "genre"], "instance": {"id": "1575", "words": ["jennifer", "lien", "starred", "in", "this", "action", "film", "of", "the", "the", "last", "six", "years", "that", "received", "a", "really", "good", "rating"], "labels": ["B-actor", "I-actor", "O", "O", "O", "B-genre", "O", "O", "O", "O", "B-year", "I-year", "I-year", "O", "O", "O", "B-average ratings", "I-average ratings", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, character, review, average ratings, trailer, title, year, actor, plot, song, rating, genre and O.\nSentence: jennifer lien starred in this action film of the the last six years that received a really good rating", "prompt_labels": "jennifer(B-actor) lien(I-actor) starred(O) in(O) this(O) action(B-genre) film(O) of(O) the(O) the(O) last(B-year) six(I-year) years(I-year) that(O) received(O) a(O) really(B-average ratings) good(I-average ratings) rating(O)"}}
{"id": "997", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "average ratings", "character", "actor", "director", "plot", "genre", "year", "review", "trailer", "title", "rating"], "instance": {"id": "997", "words": ["show", "me", "what", "films", "were", "directed", "by", "the", "wachowski", "brothers"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, average ratings, character, actor, director, plot, genre, year, review, trailer, title, rating and O.\nSentence: show me what films were directed by the wachowski brothers", "prompt_labels": "show(O) me(O) what(O) films(O) were(O) directed(O) by(O) the(O) wachowski(B-director) brothers(I-director)"}}
{"id": "2279", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "plot", "title", "year", "director", "average ratings", "review", "rating", "actor", "song", "character", "trailer"], "instance": {"id": "2279", "words": ["where", "can", "i", "rent", "satellite", "in", "the", "sky"], "labels": ["O", "O", "O", "O", "B-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, plot, title, year, director, average ratings, review, rating, actor, song, character, trailer and O.\nSentence: where can i rent satellite in the sky", "prompt_labels": "where(O) can(O) i(O) rent(O) satellite(B-title) in(I-title) the(I-title) sky(I-title)"}}
{"id": "1386", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "character", "title", "average ratings", "trailer", "song", "review", "genre", "rating", "director", "plot", "year"], "instance": {"id": "1386", "words": ["im", "looking", "for", "the", "unrated", "sports", "film", "directed", "by", "lewis", "milestone", "from", "the", "past", "four", "years", "that", "was", "considered", "all", "right"], "labels": ["O", "O", "O", "O", "B-rating", "B-genre", "O", "O", "O", "B-director", "I-director", "O", "O", "B-year", "I-year", "I-year", "O", "O", "O", "B-average ratings", "I-average ratings"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, character, title, average ratings, trailer, song, review, genre, rating, director, plot, year and O.\nSentence: im looking for the unrated sports film directed by lewis milestone from the past four years that was considered all right", "prompt_labels": "im(O) looking(O) for(O) the(O) unrated(B-rating) sports(B-genre) film(O) directed(O) by(O) lewis(B-director) milestone(I-director) from(O) the(O) past(B-year) four(I-year) years(I-year) that(O) was(O) considered(O) all(B-average ratings) right(I-average ratings)"}}
{"id": "1808", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "plot", "character", "actor", "average ratings", "title", "director", "genre", "review", "trailer", "year", "rating"], "instance": {"id": "1808", "words": ["was", "there", "a", "biography", "about", "jeff", "bridges", "made", "in", "the", "last", "year"], "labels": ["O", "O", "O", "B-genre", "O", "B-actor", "I-actor", "O", "O", "O", "B-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, plot, character, actor, average ratings, title, director, genre, review, trailer, year, rating and O.\nSentence: was there a biography about jeff bridges made in the last year", "prompt_labels": "was(O) there(O) a(O) biography(B-genre) about(O) jeff(B-actor) bridges(I-actor) made(O) in(O) the(O) last(B-year) year(I-year)"}}
{"id": "1663", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "review", "title", "character", "average ratings", "trailer", "genre", "song", "rating", "year", "director", "plot"], "instance": {"id": "1663", "words": ["list", "an", "nc", "17", "rated", "biography", "film", "that", "centers", "on", "composer", "staring", "fisher", "stevens", "in", "the", "last", "nine", "decades"], "labels": ["O", "O", "B-rating", "I-rating", "O", "B-genre", "O", "O", "O", "O", "B-plot", "O", "B-actor", "I-actor", "O", "O", "B-year", "I-year", "I-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, review, title, character, average ratings, trailer, genre, song, rating, year, director, plot and O.\nSentence: list an nc 17 rated biography film that centers on composer staring fisher stevens in the last nine decades", "prompt_labels": "list(O) an(O) nc(B-rating) 17(I-rating) rated(O) biography(B-genre) film(O) that(O) centers(O) on(O) composer(B-plot) staring(O) fisher(B-actor) stevens(I-actor) in(O) the(O) last(B-year) nine(I-year) decades(I-year)"}}
{"id": "607", "dataset": "mit-movie", "split": "dev", "label_list": ["actor", "average ratings", "year", "character", "genre", "review", "director", "title", "trailer", "rating", "song", "plot"], "instance": {"id": "607", "words": ["find", "an", "action", "movie", "starring", "harrison", "ford"], "labels": ["O", "O", "B-genre", "O", "O", "B-actor", "I-actor"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, average ratings, year, character, genre, review, director, title, trailer, rating, song, plot and O.\nSentence: find an action movie starring harrison ford", "prompt_labels": "find(O) an(O) action(B-genre) movie(O) starring(O) harrison(B-actor) ford(I-actor)"}}
{"id": "779", "dataset": "mit-movie", "split": "dev", "label_list": ["character", "director", "trailer", "plot", "genre", "rating", "year", "title", "review", "average ratings", "actor", "song"], "instance": {"id": "779", "words": ["show", "me", "the", "movie", "where", "cary", "grant", "has", "two", "aunts", "that", "poison", "and", "bury", "old", "men"], "labels": ["O", "O", "O", "O", "O", "B-actor", "I-actor", "O", "B-plot", "I-plot", "I-plot", "I-plot", "I-plot", "I-plot", "I-plot", "I-plot"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, director, trailer, plot, genre, rating, year, title, review, average ratings, actor, song and O.\nSentence: show me the movie where cary grant has two aunts that poison and bury old men", "prompt_labels": "show(O) me(O) the(O) movie(O) where(O) cary(B-actor) grant(I-actor) has(O) two(B-plot) aunts(I-plot) that(I-plot) poison(I-plot) and(I-plot) bury(I-plot) old(I-plot) men(I-plot)"}}
{"id": "2306", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "rating", "year", "actor", "average ratings", "song", "director", "character", "genre", "review", "trailer", "plot"], "instance": {"id": "2306", "words": ["who", "directed", "the", "back", "up", "plan"], "labels": ["O", "O", "B-title", "I-title", "I-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, rating, year, actor, average ratings, song, director, character, genre, review, trailer, plot and O.\nSentence: who directed the back up plan", "prompt_labels": "who(O) directed(O) the(B-title) back(I-title) up(I-title) plan(I-title)"}}
{"id": "2239", "dataset": "mit-movie", "split": "dev", "label_list": ["song", "plot", "average ratings", "trailer", "genre", "year", "character", "review", "director", "actor", "rating", "title"], "instance": {"id": "2239", "words": ["what", "year", "did", "ip", "man", "come", "out"], "labels": ["O", "O", "O", "B-title", "I-title", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, plot, average ratings, trailer, genre, year, character, review, director, actor, rating, title and O.\nSentence: what year did ip man come out", "prompt_labels": "what(O) year(O) did(O) ip(B-title) man(I-title) come(O) out(O)"}}
{"id": "940", "dataset": "mit-movie", "split": "dev", "label_list": ["plot", "year", "song", "director", "character", "average ratings", "actor", "trailer", "rating", "title", "genre", "review"], "instance": {"id": "940", "words": ["what", "is", "the", "plot", "of", "mad", "max"], "labels": ["O", "O", "O", "O", "O", "B-title", "I-title"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, year, song, director, character, average ratings, actor, trailer, rating, title, genre, review and O.\nSentence: what is the plot of mad max", "prompt_labels": "what(O) is(O) the(O) plot(O) of(O) mad(B-title) max(I-title)"}}
{"id": "1891", "dataset": "mit-movie", "split": "dev", "label_list": ["title", "song", "year", "plot", "rating", "director", "average ratings", "character", "review", "actor", "trailer", "genre"], "instance": {"id": "1891", "words": ["what", "are", "some", "r", "rated", "drama", "films", "that", "were", "directed", "by", "john", "geddes"], "labels": ["O", "O", "O", "B-rating", "O", "B-genre", "O", "O", "O", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, song, year, plot, rating, director, average ratings, character, review, actor, trailer, genre and O.\nSentence: what are some r rated drama films that were directed by john geddes", "prompt_labels": "what(O) are(O) some(O) r(B-rating) rated(O) drama(B-genre) films(O) that(O) were(O) directed(O) by(O) john(B-director) geddes(I-director)"}}
{"id": "1194", "dataset": "mit-movie", "split": "dev", "label_list": ["year", "review", "average ratings", "song", "director", "plot", "genre", "trailer", "character", "rating", "title", "actor"], "instance": {"id": "1194", "words": ["do", "you", "have", "a", "fantasy", "film", "from", "last", "year", "directed", "by", "randy", "barbato"], "labels": ["O", "O", "O", "O", "B-genre", "O", "O", "B-year", "I-year", "O", "O", "B-director", "I-director"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, review, average ratings, song, director, plot, genre, trailer, character, rating, title, actor and O.\nSentence: do you have a fantasy film from last year directed by randy barbato", "prompt_labels": "do(O) you(O) have(O) a(O) fantasy(B-genre) film(O) from(O) last(B-year) year(I-year) directed(O) by(O) randy(B-director) barbato(I-director)"}}
{"id": "1825", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "song", "year", "plot", "title", "director", "review", "average ratings", "actor", "trailer", "rating", "character"], "instance": {"id": "1825", "words": ["what", "1970", "short", "film", "directed", "by", "james", "de", "frond", "got", "a", "pg", "13", "rating"], "labels": ["O", "B-year", "B-genre", "O", "O", "O", "B-director", "I-director", "I-director", "O", "O", "B-rating", "I-rating", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, song, year, plot, title, director, review, average ratings, actor, trailer, rating, character and O.\nSentence: what 1970 short film directed by james de frond got a pg 13 rating", "prompt_labels": "what(O) 1970(B-year) short(B-genre) film(O) directed(O) by(O) james(B-director) de(I-director) frond(I-director) got(O) a(O) pg(B-rating) 13(I-rating) rating(O)"}}
{"id": "49", "dataset": "mit-movie", "split": "dev", "label_list": ["genre", "director", "title", "character", "actor", "plot", "year", "song", "review", "average ratings", "rating", "trailer"], "instance": {"id": "49", "words": ["show", "me", "morgan", "freeman", "movies", "from", "the", "90s"], "labels": ["O", "O", "B-actor", "I-actor", "O", "O", "O", "B-year"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, director, title, character, actor, plot, year, song, review, average ratings, rating, trailer and O.\nSentence: show me morgan freeman movies from the 90s", "prompt_labels": "show(O) me(O) morgan(B-actor) freeman(I-actor) movies(O) from(O) the(O) 90s(B-year)"}}
{"id": "1459", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Cuisine", "Restaurant Name", "Dish", "Location", "Price", "Rating", "Hours"], "instance": {"id": "1459", "words": ["where", "is", "there", "a", "great", "steak", "near", "here", "that", "has", "a", "buffet"], "labels": ["O", "O", "O", "O", "B-Rating", "B-Dish", "B-Location", "I-Location", "O", "O", "O", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Cuisine, Restaurant Name, Dish, Location, Price, Rating, Hours and O.\nSentence: where is there a great steak near here that has a buffet", "prompt_labels": "where(O) is(O) there(O) a(O) great(B-Rating) steak(B-Dish) near(B-Location) here(I-Location) that(O) has(O) a(O) buffet(B-Amenity)"}}
{"id": "1034", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Hours", "Restaurant Name", "Dish", "Cuisine", "Amenity", "Location", "Rating"], "instance": {"id": "1034", "words": ["please", "locate", "a", "mcdonalds"], "labels": ["O", "O", "O", "B-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Restaurant Name, Dish, Cuisine, Amenity, Location, Rating and O.\nSentence: please locate a mcdonalds", "prompt_labels": "please(O) locate(O) a(O) mcdonalds(B-Restaurant Name)"}}
{"id": "1151", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Price", "Location", "Restaurant Name", "Rating", "Dish", "Cuisine", "Amenity"], "instance": {"id": "1151", "words": ["what", "is", "the", "rating", "on", "arbys"], "labels": ["O", "O", "O", "O", "O", "B-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Location, Restaurant Name, Rating, Dish, Cuisine, Amenity and O.\nSentence: what is the rating on arbys", "prompt_labels": "what(O) is(O) the(O) rating(O) on(O) arbys(B-Restaurant Name)"}}
{"id": "1248", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Rating", "Cuisine", "Restaurant Name", "Dish", "Location", "Hours", "Amenity"], "instance": {"id": "1248", "words": ["when", "does", "dominos", "open"], "labels": ["O", "O", "B-Restaurant Name", "B-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Cuisine, Restaurant Name, Dish, Location, Hours, Amenity and O.\nSentence: when does dominos open", "prompt_labels": "when(O) does(O) dominos(B-Restaurant Name) open(B-Hours)"}}
{"id": "516", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Cuisine", "Restaurant Name", "Rating", "Amenity", "Location", "Price", "Dish"], "instance": {"id": "516", "words": ["how", "far", "is", "the", "english", "pub", "that", "serves", "a", "fry", "up"], "labels": ["O", "O", "O", "O", "B-Amenity", "I-Amenity", "O", "O", "O", "B-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Restaurant Name, Rating, Amenity, Location, Price, Dish and O.\nSentence: how far is the english pub that serves a fry up", "prompt_labels": "how(O) far(O) is(O) the(O) english(B-Amenity) pub(I-Amenity) that(O) serves(O) a(O) fry(B-Dish) up(I-Dish)"}}
{"id": "1078", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Rating", "Location", "Restaurant Name", "Hours", "Dish", "Cuisine", "Price"], "instance": {"id": "1078", "words": ["tell", "me", "the", "hours", "of", "chipotle", "in", "midtown"], "labels": ["O", "O", "O", "O", "O", "B-Restaurant Name", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Location, Restaurant Name, Hours, Dish, Cuisine, Price and O.\nSentence: tell me the hours of chipotle in midtown", "prompt_labels": "tell(O) me(O) the(O) hours(O) of(O) chipotle(B-Restaurant Name) in(O) midtown(B-Location)"}}
{"id": "1189", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Location", "Rating", "Dish", "Price", "Hours", "Amenity", "Cuisine"], "instance": {"id": "1189", "words": ["what", "restaurants", "are", "open", "in", "the", "downtown", "area"], "labels": ["O", "O", "O", "B-Hours", "B-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Rating, Dish, Price, Hours, Amenity, Cuisine and O.\nSentence: what restaurants are open in the downtown area", "prompt_labels": "what(O) restaurants(O) are(O) open(B-Hours) in(B-Location) the(I-Location) downtown(I-Location) area(I-Location)"}}
{"id": "732", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Location", "Dish", "Restaurant Name", "Hours", "Cuisine", "Rating", "Price"], "instance": {"id": "732", "words": ["im", "looking", "for", "a", "5", "star", "restaurant", "whats", "the", "closest", "one"], "labels": ["O", "O", "O", "O", "B-Rating", "I-Rating", "O", "O", "O", "B-Location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Dish, Restaurant Name, Hours, Cuisine, Rating, Price and O.\nSentence: im looking for a 5 star restaurant whats the closest one", "prompt_labels": "im(O) looking(O) for(O) a(O) 5(B-Rating) star(I-Rating) restaurant(O) whats(O) the(O) closest(B-Location) one(O)"}}
{"id": "8", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Cuisine", "Dish", "Amenity", "Location", "Rating", "Restaurant Name", "Hours"], "instance": {"id": "8", "words": ["any", "mexican", "places", "have", "a", "tameles", "special", "today"], "labels": ["O", "B-Cuisine", "O", "O", "O", "B-Dish", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Dish, Amenity, Location, Rating, Restaurant Name, Hours and O.\nSentence: any mexican places have a tameles special today", "prompt_labels": "any(O) mexican(B-Cuisine) places(O) have(O) a(O) tameles(B-Dish) special(B-Amenity) today(I-Amenity)"}}
{"id": "1267", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Amenity", "Dish", "Price", "Rating", "Cuisine", "Hours", "Location"], "instance": {"id": "1267", "words": ["where", "can", "i", "find", "a", "cambodian", "restaurant", "that", "also", "sells", "ingredients"], "labels": ["O", "O", "O", "O", "O", "B-Cuisine", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Dish, Price, Rating, Cuisine, Hours, Location and O.\nSentence: where can i find a cambodian restaurant that also sells ingredients", "prompt_labels": "where(O) can(O) i(O) find(O) a(O) cambodian(B-Cuisine) restaurant(O) that(O) also(O) sells(B-Amenity) ingredients(I-Amenity)"}}
{"id": "889", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Amenity", "Hours", "Rating", "Location", "Dish", "Price", "Restaurant Name"], "instance": {"id": "889", "words": ["is", "there", "an", "environmentally", "friendly", "restaurant", "nearby"], "labels": ["O", "O", "O", "B-Amenity", "I-Amenity", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Amenity, Hours, Rating, Location, Dish, Price, Restaurant Name and O.\nSentence: is there an environmentally friendly restaurant nearby", "prompt_labels": "is(O) there(O) an(O) environmentally(B-Amenity) friendly(I-Amenity) restaurant(O) nearby(B-Location)"}}
{"id": "714", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Price", "Restaurant Name", "Amenity", "Hours", "Rating", "Cuisine", "Dish"], "instance": {"id": "714", "words": ["id", "really", "like", "a", "thai", "restaurant", "that", "has", "carryout", "do", "you", "know", "of", "one"], "labels": ["O", "O", "O", "O", "B-Cuisine", "O", "O", "O", "B-Amenity", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Restaurant Name, Amenity, Hours, Rating, Cuisine, Dish and O.\nSentence: id really like a thai restaurant that has carryout do you know of one", "prompt_labels": "id(O) really(O) like(O) a(O) thai(B-Cuisine) restaurant(O) that(O) has(O) carryout(B-Amenity) do(O) you(O) know(O) of(O) one(O)"}}
{"id": "1520", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Location", "Dish", "Cuisine", "Amenity", "Hours", "Restaurant Name", "Rating"], "instance": {"id": "1520", "words": ["you", "can", "help", "me", "with", "some", "onion", "rings"], "labels": ["O", "O", "O", "O", "O", "O", "B-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Dish, Cuisine, Amenity, Hours, Restaurant Name, Rating and O.\nSentence: you can help me with some onion rings", "prompt_labels": "you(O) can(O) help(O) me(O) with(O) some(O) onion(B-Dish) rings(I-Dish)"}}
{"id": "942", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Amenity", "Dish", "Price", "Rating", "Location", "Cuisine", "Hours"], "instance": {"id": "942", "words": ["list", "the", "nearest", "mcdonalds", "to", "87", "th", "and", "ashland"], "labels": ["O", "O", "B-Location", "B-Restaurant Name", "O", "B-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Dish, Price, Rating, Location, Cuisine, Hours and O.\nSentence: list the nearest mcdonalds to 87 th and ashland", "prompt_labels": "list(O) the(O) nearest(B-Location) mcdonalds(B-Restaurant Name) to(O) 87(B-Location) th(I-Location) and(I-Location) ashland(I-Location)"}}
{"id": "295", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Price", "Amenity", "Cuisine", "Dish", "Location", "Hours", "Rating"], "instance": {"id": "295", "words": ["does", "olive", "garden", "serve", "wine"], "labels": ["O", "B-Restaurant Name", "I-Restaurant Name", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Amenity, Cuisine, Dish, Location, Hours, Rating and O.\nSentence: does olive garden serve wine", "prompt_labels": "does(O) olive(B-Restaurant Name) garden(I-Restaurant Name) serve(O) wine(B-Dish)"}}
{"id": "724", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Rating", "Hours", "Cuisine", "Amenity", "Dish", "Price", "Restaurant Name"], "instance": {"id": "724", "words": ["im", "hungry", "lets", "get", "some", "tacos"], "labels": ["O", "O", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Hours, Cuisine, Amenity, Dish, Price, Restaurant Name and O.\nSentence: im hungry lets get some tacos", "prompt_labels": "im(O) hungry(O) lets(O) get(O) some(O) tacos(B-Dish)"}}
{"id": "187", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Amenity", "Location", "Dish", "Rating", "Hours", "Cuisine", "Price"], "instance": {"id": "187", "words": ["can", "you", "help", "me", "find", "inexpensive", "dining", "within", "a", "five", "mile", "radius", "of", "my", "current", "location"], "labels": ["O", "O", "O", "O", "O", "B-Price", "O", "B-Location", "I-Location", "I-Location", "I-Location", "I-Location", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Location, Dish, Rating, Hours, Cuisine, Price and O.\nSentence: can you help me find inexpensive dining within a five mile radius of my current location", "prompt_labels": "can(O) you(O) help(O) me(O) find(O) inexpensive(B-Price) dining(O) within(B-Location) a(I-Location) five(I-Location) mile(I-Location) radius(I-Location) of(O) my(O) current(O) location(O)"}}
{"id": "918", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Price", "Rating", "Restaurant Name", "Hours", "Location", "Dish", "Cuisine"], "instance": {"id": "918", "words": ["is", "there", "anywhere", "near", "here", "that", "is", "open", "24", "hours", "and", "serves", "breakfast"], "labels": ["O", "O", "O", "B-Location", "I-Location", "O", "O", "O", "B-Hours", "I-Hours", "O", "O", "B-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Rating, Restaurant Name, Hours, Location, Dish, Cuisine and O.\nSentence: is there anywhere near here that is open 24 hours and serves breakfast", "prompt_labels": "is(O) there(O) anywhere(O) near(B-Location) here(I-Location) that(O) is(O) open(O) 24(B-Hours) hours(I-Hours) and(O) serves(O) breakfast(B-Cuisine)"}}
{"id": "1338", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Location", "Price", "Cuisine", "Dish", "Rating", "Restaurant Name", "Hours"], "instance": {"id": "1338", "words": ["where", "can", "i", "get", "some", "macaroni", "and", "cheese"], "labels": ["O", "O", "O", "O", "O", "B-Dish", "I-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Price, Cuisine, Dish, Rating, Restaurant Name, Hours and O.\nSentence: where can i get some macaroni and cheese", "prompt_labels": "where(O) can(O) i(O) get(O) some(O) macaroni(B-Dish) and(I-Dish) cheese(I-Dish)"}}
{"id": "1136", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Amenity", "Rating", "Location", "Hours", "Cuisine", "Restaurant Name", "Price"], "instance": {"id": "1136", "words": ["what", "is", "the", "most", "closest", "eat", "in", "restaurant", "in", "the", "area"], "labels": ["O", "O", "O", "O", "B-Location", "B-Amenity", "I-Amenity", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Rating, Location, Hours, Cuisine, Restaurant Name, Price and O.\nSentence: what is the most closest eat in restaurant in the area", "prompt_labels": "what(O) is(O) the(O) most(O) closest(B-Location) eat(B-Amenity) in(I-Amenity) restaurant(O) in(B-Location) the(I-Location) area(I-Location)"}}
{"id": "446", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Dish", "Location", "Hours", "Restaurant Name", "Price", "Cuisine", "Rating"], "instance": {"id": "446", "words": ["find", "restaurants", "within", "5", "miles", "with", "entrees", "under", "15"], "labels": ["O", "O", "B-Location", "I-Location", "I-Location", "O", "B-Price", "I-Price", "I-Price"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Location, Hours, Restaurant Name, Price, Cuisine, Rating and O.\nSentence: find restaurants within 5 miles with entrees under 15", "prompt_labels": "find(O) restaurants(O) within(B-Location) 5(I-Location) miles(I-Location) with(O) entrees(B-Price) under(I-Price) 15(I-Price)"}}
{"id": "511", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Dish", "Rating", "Hours", "Location", "Cuisine", "Price", "Restaurant Name"], "instance": {"id": "511", "words": ["how", "far", "away", "is", "the", "nearest", "applebees"], "labels": ["O", "O", "O", "O", "O", "B-Location", "B-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Rating, Hours, Location, Cuisine, Price, Restaurant Name and O.\nSentence: how far away is the nearest applebees", "prompt_labels": "how(O) far(O) away(O) is(O) the(O) nearest(B-Location) applebees(B-Restaurant Name)"}}
{"id": "1055", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Dish", "Restaurant Name", "Rating", "Price", "Hours", "Location", "Amenity"], "instance": {"id": "1055", "words": ["show", "me", "all", "of", "the", "local", "restaurants", "with", "a", "smoking", "area"], "labels": ["O", "O", "O", "O", "O", "B-Location", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Dish, Restaurant Name, Rating, Price, Hours, Location, Amenity and O.\nSentence: show me all of the local restaurants with a smoking area", "prompt_labels": "show(O) me(O) all(O) of(O) the(O) local(B-Location) restaurants(O) with(O) a(O) smoking(B-Amenity) area(I-Amenity)"}}
{"id": "211", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Cuisine", "Hours", "Rating", "Location", "Dish", "Price", "Amenity"], "instance": {"id": "211", "words": ["can", "you", "tell", "me", "where", "to", "get", "some", "cheap", "vegetarian", "food"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-Price", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Hours, Rating, Location, Dish, Price, Amenity and O.\nSentence: can you tell me where to get some cheap vegetarian food", "prompt_labels": "can(O) you(O) tell(O) me(O) where(O) to(O) get(O) some(O) cheap(B-Price) vegetarian(B-Cuisine) food(O)"}}
{"id": "976", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Restaurant Name", "Cuisine", "Location", "Rating", "Hours", "Dish", "Amenity"], "instance": {"id": "976", "words": ["looking", "for", "duck", "dish", "with", "great", "pricing", "and", "good", "rating"], "labels": ["O", "O", "B-Dish", "I-Dish", "O", "B-Price", "O", "O", "B-Rating", "I-Rating"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Restaurant Name, Cuisine, Location, Rating, Hours, Dish, Amenity and O.\nSentence: looking for duck dish with great pricing and good rating", "prompt_labels": "looking(O) for(O) duck(B-Dish) dish(I-Dish) with(O) great(B-Price) pricing(O) and(O) good(B-Rating) rating(I-Rating)"}}
{"id": "1105", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Restaurant Name", "Rating", "Amenity", "Location", "Dish", "Cuisine", "Price"], "instance": {"id": "1105", "words": ["what", "chicken", "restaurants", "are", "in", "the", "area"], "labels": ["O", "B-Dish", "O", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Rating, Amenity, Location, Dish, Cuisine, Price and O.\nSentence: what chicken restaurants are in the area", "prompt_labels": "what(O) chicken(B-Dish) restaurants(O) are(O) in(B-Location) the(I-Location) area(I-Location)"}}
{"id": "1141", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Price", "Dish", "Location", "Rating", "Restaurant Name", "Amenity", "Hours"], "instance": {"id": "1141", "words": ["what", "is", "the", "name", "or", "phone", "number", "of", "the", "coffee", "shop", "on", "seminary", "street"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Cuisine", "O", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Dish, Location, Rating, Restaurant Name, Amenity, Hours and O.\nSentence: what is the name or phone number of the coffee shop on seminary street", "prompt_labels": "what(O) is(O) the(O) name(O) or(O) phone(O) number(O) of(O) the(O) coffee(B-Cuisine) shop(O) on(O) seminary(B-Location) street(I-Location)"}}
{"id": "1330", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Cuisine", "Amenity", "Restaurant Name", "Hours", "Rating", "Dish", "Price"], "instance": {"id": "1330", "words": ["where", "can", "i", "get", "nachos", "within", "1", "mile", "of", "me", "that", "is", "open", "late"], "labels": ["O", "O", "O", "O", "B-Dish", "B-Location", "I-Location", "I-Location", "O", "O", "O", "O", "B-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Amenity, Restaurant Name, Hours, Rating, Dish, Price and O.\nSentence: where can i get nachos within 1 mile of me that is open late", "prompt_labels": "where(O) can(O) i(O) get(O) nachos(B-Dish) within(B-Location) 1(I-Location) mile(I-Location) of(O) me(O) that(O) is(O) open(B-Hours) late(I-Hours)"}}
{"id": "1214", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Location", "Dish", "Amenity", "Hours", "Rating", "Restaurant Name", "Cuisine"], "instance": {"id": "1214", "words": ["what", "time", "does", "the", "nearest", "chipotle", "close"], "labels": ["O", "O", "O", "O", "B-Location", "B-Restaurant Name", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Dish, Amenity, Hours, Rating, Restaurant Name, Cuisine and O.\nSentence: what time does the nearest chipotle close", "prompt_labels": "what(O) time(O) does(O) the(O) nearest(B-Location) chipotle(B-Restaurant Name) close(O)"}}
{"id": "1106", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Location", "Price", "Rating", "Restaurant Name", "Cuisine", "Dish", "Hours"], "instance": {"id": "1106", "words": ["what", "do", "you", "know", "about", "restaurants", "that", "have", "impressive", "wine", "lists"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Price, Rating, Restaurant Name, Cuisine, Dish, Hours and O.\nSentence: what do you know about restaurants that have impressive wine lists", "prompt_labels": "what(O) do(O) you(O) know(O) about(O) restaurants(O) that(O) have(O) impressive(B-Amenity) wine(I-Amenity) lists(I-Amenity)"}}
{"id": "1350", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Cuisine", "Hours", "Amenity", "Restaurant Name", "Location", "Price", "Dish"], "instance": {"id": "1350", "words": ["where", "can", "i", "go", "for", "expensive", "gelato", "for", "carry", "out"], "labels": ["O", "O", "O", "O", "O", "B-Price", "B-Dish", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Hours, Amenity, Restaurant Name, Location, Price, Dish and O.\nSentence: where can i go for expensive gelato for carry out", "prompt_labels": "where(O) can(O) i(O) go(O) for(O) expensive(B-Price) gelato(B-Dish) for(O) carry(B-Amenity) out(I-Amenity)"}}
{"id": "1190", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Amenity", "Restaurant Name", "Price", "Hours", "Location", "Rating", "Cuisine"], "instance": {"id": "1190", "words": ["what", "restaurants", "are", "open", "past", "midnight"], "labels": ["O", "O", "O", "B-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Restaurant Name, Price, Hours, Location, Rating, Cuisine and O.\nSentence: what restaurants are open past midnight", "prompt_labels": "what(O) restaurants(O) are(O) open(B-Hours) past(I-Hours) midnight(I-Hours)"}}
{"id": "1369", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Amenity", "Location", "Price", "Rating", "Cuisine", "Hours", "Restaurant Name"], "instance": {"id": "1369", "words": ["where", "i", "can", "get", "seafood", "and", "bring", "my", "own", "drinks"], "labels": ["O", "O", "O", "O", "B-Cuisine", "O", "B-Amenity", "I-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Location, Price, Rating, Cuisine, Hours, Restaurant Name and O.\nSentence: where i can get seafood and bring my own drinks", "prompt_labels": "where(O) i(O) can(O) get(O) seafood(B-Cuisine) and(O) bring(B-Amenity) my(I-Amenity) own(I-Amenity) drinks(I-Amenity)"}}
{"id": "745", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Rating", "Cuisine", "Restaurant Name", "Location", "Dish", "Amenity", "Price"], "instance": {"id": "745", "words": ["im", "looking", "for", "somewhere", "i", "can", "get", "a", "lot", "of", "food", "for", "not", "too", "much", "money"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity", "O", "B-Price", "I-Price", "I-Price", "I-Price"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Rating, Cuisine, Restaurant Name, Location, Dish, Amenity, Price and O.\nSentence: im looking for somewhere i can get a lot of food for not too much money", "prompt_labels": "im(O) looking(O) for(O) somewhere(O) i(O) can(O) get(O) a(O) lot(B-Amenity) of(I-Amenity) food(I-Amenity) for(O) not(B-Price) too(I-Price) much(I-Price) money(I-Price)"}}
{"id": "769", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Cuisine", "Restaurant Name", "Location", "Rating", "Hours", "Price", "Dish"], "instance": {"id": "769", "words": ["is", "golden", "house", "restaurant", "kids", "friendly"], "labels": ["O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Cuisine, Restaurant Name, Location, Rating, Hours, Price, Dish and O.\nSentence: is golden house restaurant kids friendly", "prompt_labels": "is(O) golden(B-Restaurant Name) house(I-Restaurant Name) restaurant(I-Restaurant Name) kids(B-Amenity) friendly(I-Amenity)"}}
{"id": "944", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Hours", "Restaurant Name", "Price", "Rating", "Location", "Dish", "Amenity"], "instance": {"id": "944", "words": ["local", "fish", "tacos"], "labels": ["B-Location", "B-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Restaurant Name, Price, Rating, Location, Dish, Amenity and O.\nSentence: local fish tacos", "prompt_labels": "local(B-Location) fish(B-Dish) tacos(I-Dish)"}}
{"id": "98", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Hours", "Location", "Price", "Dish", "Amenity", "Restaurant Name", "Rating"], "instance": {"id": "98", "words": ["are", "there", "any", "vietnamese", "restaurants", "nearby"], "labels": ["O", "O", "O", "B-Cuisine", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Location, Price, Dish, Amenity, Restaurant Name, Rating and O.\nSentence: are there any vietnamese restaurants nearby", "prompt_labels": "are(O) there(O) any(O) vietnamese(B-Cuisine) restaurants(O) nearby(B-Location)"}}
{"id": "1238", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Amenity", "Price", "Cuisine", "Hours", "Dish", "Rating", "Restaurant Name"], "instance": {"id": "1238", "words": ["whats", "the", "closest", "restaurant", "with", "a", "full", "salad", "bar"], "labels": ["O", "O", "B-Location", "O", "O", "O", "B-Cuisine", "I-Cuisine", "I-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Price, Cuisine, Hours, Dish, Rating, Restaurant Name and O.\nSentence: whats the closest restaurant with a full salad bar", "prompt_labels": "whats(O) the(O) closest(B-Location) restaurant(O) with(O) a(O) full(B-Cuisine) salad(I-Cuisine) bar(I-Cuisine)"}}
{"id": "199", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Restaurant Name", "Location", "Rating", "Amenity", "Dish", "Hours", "Price"], "instance": {"id": "199", "words": ["can", "you", "make", "reservations", "for", "two", "at", "heartland", "restaurant", "for", "tonight", "at", "7", "30"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "O", "B-Hours", "I-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Location, Rating, Amenity, Dish, Hours, Price and O.\nSentence: can you make reservations for two at heartland restaurant for tonight at 7 30", "prompt_labels": "can(O) you(O) make(O) reservations(O) for(O) two(O) at(O) heartland(B-Restaurant Name) restaurant(I-Restaurant Name) for(O) tonight(B-Hours) at(I-Hours) 7(I-Hours) 30(I-Hours)"}}
{"id": "140", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Amenity", "Rating", "Dish", "Location", "Cuisine", "Restaurant Name", "Price"], "instance": {"id": "140", "words": ["can", "you", "find", "a", "restaurant", "that", "serves", "bean", "soup", "in", "the", "morning", "and", "isnt", "too", "expensive"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Dish", "I-Dish", "O", "O", "B-Hours", "O", "B-Price", "I-Price", "I-Price"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Rating, Dish, Location, Cuisine, Restaurant Name, Price and O.\nSentence: can you find a restaurant that serves bean soup in the morning and isnt too expensive", "prompt_labels": "can(O) you(O) find(O) a(O) restaurant(O) that(O) serves(O) bean(B-Dish) soup(I-Dish) in(O) the(O) morning(B-Hours) and(O) isnt(B-Price) too(I-Price) expensive(I-Price)"}}
{"id": "1371", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Restaurant Name", "Cuisine", "Price", "Dish", "Rating", "Location", "Amenity"], "instance": {"id": "1371", "words": ["where", "in", "the", "theater", "district", "is", "there", "a", "restaurant", "with", "portions", "that", "are", "a", "bit", "small"], "labels": ["O", "O", "O", "B-Location", "I-Location", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Cuisine, Price, Dish, Rating, Location, Amenity and O.\nSentence: where in the theater district is there a restaurant with portions that are a bit small", "prompt_labels": "where(O) in(O) the(O) theater(B-Location) district(I-Location) is(O) there(O) a(O) restaurant(O) with(O) portions(O) that(O) are(O) a(O) bit(O) small(O)"}}
{"id": "752", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Hours", "Rating", "Dish", "Price", "Amenity", "Restaurant Name", "Location"], "instance": {"id": "752", "words": ["im", "starving", "help", "me", "find", "a", "fast", "food", "restaurant"], "labels": ["O", "O", "O", "O", "O", "O", "B-Cuisine", "I-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Rating, Dish, Price, Amenity, Restaurant Name, Location and O.\nSentence: im starving help me find a fast food restaurant", "prompt_labels": "im(O) starving(O) help(O) me(O) find(O) a(O) fast(B-Cuisine) food(I-Cuisine) restaurant(O)"}}
{"id": "491", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Dish", "Location", "Hours", "Price", "Rating", "Amenity", "Restaurant Name"], "instance": {"id": "491", "words": ["hi", "please", "find", "me", "a", "sushi", "restaurant", "that", "has", "good", "reviews", "and", "that", "isnt", "too", "expensive"], "labels": ["O", "O", "O", "O", "O", "B-Cuisine", "O", "O", "O", "B-Rating", "I-Rating", "O", "O", "B-Price", "I-Price", "I-Price"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Dish, Location, Hours, Price, Rating, Amenity, Restaurant Name and O.\nSentence: hi please find me a sushi restaurant that has good reviews and that isnt too expensive", "prompt_labels": "hi(O) please(O) find(O) me(O) a(O) sushi(B-Cuisine) restaurant(O) that(O) has(O) good(B-Rating) reviews(I-Rating) and(O) that(O) isnt(B-Price) too(I-Price) expensive(I-Price)"}}
{"id": "0", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Amenity", "Location", "Hours", "Cuisine", "Rating", "Price", "Dish"], "instance": {"id": "0", "words": ["a", "four", "star", "restaurant", "with", "a", "bar"], "labels": ["O", "B-Rating", "I-Rating", "O", "B-Location", "I-Location", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Location, Hours, Cuisine, Rating, Price, Dish and O.\nSentence: a four star restaurant with a bar", "prompt_labels": "a(O) four(B-Rating) star(I-Rating) restaurant(O) with(B-Location) a(I-Location) bar(B-Amenity)"}}
{"id": "886", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Dish", "Price", "Restaurant Name", "Hours", "Cuisine", "Amenity", "Location"], "instance": {"id": "886", "words": ["is", "there", "an", "apple", "bees", "near", "by", "or", "similar", "bar", "and", "grill"], "labels": ["O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "B-Location", "I-Location", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Price, Restaurant Name, Hours, Cuisine, Amenity, Location and O.\nSentence: is there an apple bees near by or similar bar and grill", "prompt_labels": "is(O) there(O) an(O) apple(B-Restaurant Name) bees(I-Restaurant Name) near(B-Location) by(I-Location) or(O) similar(O) bar(O) and(O) grill(O)"}}
{"id": "1365", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Location", "Rating", "Hours", "Cuisine", "Price", "Amenity", "Restaurant Name"], "instance": {"id": "1365", "words": ["where", "could", "i", "get", "some", "cakes", "in", "a", "business", "atmosphere", "thats", "open", "until", "1", "am"], "labels": ["O", "O", "O", "O", "O", "B-Dish", "O", "O", "B-Amenity", "I-Amenity", "O", "B-Hours", "I-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Rating, Hours, Cuisine, Price, Amenity, Restaurant Name and O.\nSentence: where could i get some cakes in a business atmosphere thats open until 1 am", "prompt_labels": "where(O) could(O) i(O) get(O) some(O) cakes(B-Dish) in(O) a(O) business(B-Amenity) atmosphere(I-Amenity) thats(O) open(B-Hours) until(I-Hours) 1(I-Hours) am(I-Hours)"}}
{"id": "867", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Dish", "Restaurant Name", "Price", "Rating", "Amenity", "Cuisine", "Location"], "instance": {"id": "867", "words": ["is", "there", "a", "restaurant", "with", "a", "bar", "scene", "nearby", "that", "serves", "small", "portioned", "meals", "and", "snacks"], "labels": ["O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "B-Location", "O", "O", "B-Cuisine", "I-Cuisine", "I-Cuisine", "I-Cuisine", "I-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Dish, Restaurant Name, Price, Rating, Amenity, Cuisine, Location and O.\nSentence: is there a restaurant with a bar scene nearby that serves small portioned meals and snacks", "prompt_labels": "is(O) there(O) a(O) restaurant(O) with(O) a(O) bar(B-Amenity) scene(I-Amenity) nearby(B-Location) that(O) serves(O) small(B-Cuisine) portioned(I-Cuisine) meals(I-Cuisine) and(I-Cuisine) snacks(I-Cuisine)"}}
{"id": "1204", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Price", "Hours", "Restaurant Name", "Dish", "Rating", "Location", "Amenity"], "instance": {"id": "1204", "words": ["what", "time", "does", "king", "palace", "serve", "dimsum", "until"], "labels": ["O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "B-Amenity", "B-Dish", "B-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Hours, Restaurant Name, Dish, Rating, Location, Amenity and O.\nSentence: what time does king palace serve dimsum until", "prompt_labels": "what(O) time(O) does(O) king(B-Restaurant Name) palace(I-Restaurant Name) serve(B-Amenity) dimsum(B-Dish) until(B-Hours)"}}
{"id": "1263", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Cuisine", "Price", "Location", "Rating", "Hours", "Dish", "Amenity"], "instance": {"id": "1263", "words": ["where", "can", "i", "eat", "that", "delivers", "food"], "labels": ["O", "O", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Price, Location, Rating, Hours, Dish, Amenity and O.\nSentence: where can i eat that delivers food", "prompt_labels": "where(O) can(O) i(O) eat(O) that(O) delivers(B-Amenity) food(I-Amenity)"}}
{"id": "791", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Cuisine", "Hours", "Dish", "Price", "Location", "Rating", "Amenity"], "instance": {"id": "791", "words": ["is", "the", "seven", "seas", "diner", "expensive"], "labels": ["O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "B-Price"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Hours, Dish, Price, Location, Rating, Amenity and O.\nSentence: is the seven seas diner expensive", "prompt_labels": "is(O) the(O) seven(B-Restaurant Name) seas(I-Restaurant Name) diner(I-Restaurant Name) expensive(B-Price)"}}
{"id": "318", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Location", "Dish", "Restaurant Name", "Hours", "Rating", "Amenity", "Price"], "instance": {"id": "318", "words": ["does", "the", "kyotoyo", "japanese", "restaurant", "in", "the", "theater", "district", "deliver"], "labels": ["O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "O", "O", "B-Location", "I-Location", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Dish, Restaurant Name, Hours, Rating, Amenity, Price and O.\nSentence: does the kyotoyo japanese restaurant in the theater district deliver", "prompt_labels": "does(O) the(O) kyotoyo(B-Restaurant Name) japanese(I-Restaurant Name) restaurant(I-Restaurant Name) in(O) the(O) theater(B-Location) district(I-Location) deliver(B-Amenity)"}}
{"id": "917", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Price", "Hours", "Restaurant Name", "Rating", "Dish", "Location", "Amenity"], "instance": {"id": "917", "words": ["is", "there", "anywhere", "in", "kendall", "square", "that", "has", "tea", "and", "delivers"], "labels": ["O", "O", "O", "O", "B-Location", "I-Location", "O", "O", "B-Dish", "O", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Hours, Restaurant Name, Rating, Dish, Location, Amenity and O.\nSentence: is there anywhere in kendall square that has tea and delivers", "prompt_labels": "is(O) there(O) anywhere(O) in(O) kendall(B-Location) square(I-Location) that(O) has(O) tea(B-Dish) and(O) delivers(B-Amenity)"}}
{"id": "802", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Price", "Restaurant Name", "Cuisine", "Amenity", "Location", "Dish", "Rating"], "instance": {"id": "802", "words": ["is", "there", "a", "business", "dining", "restaurant", "where", "i", "can", "get", "cakes", "until", "1", "am"], "labels": ["O", "O", "O", "B-Amenity", "O", "O", "O", "O", "O", "O", "B-Dish", "B-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Restaurant Name, Cuisine, Amenity, Location, Dish, Rating and O.\nSentence: is there a business dining restaurant where i can get cakes until 1 am", "prompt_labels": "is(O) there(O) a(O) business(B-Amenity) dining(O) restaurant(O) where(O) i(O) can(O) get(O) cakes(B-Dish) until(B-Hours) 1(I-Hours) am(I-Hours)"}}
{"id": "1268", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Restaurant Name", "Hours", "Dish", "Price", "Cuisine", "Location", "Amenity"], "instance": {"id": "1268", "words": ["where", "can", "i", "find", "a", "cheap", "place", "to", "eat", "in", "peabody"], "labels": ["O", "O", "O", "O", "O", "B-Price", "O", "O", "O", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Hours, Dish, Price, Cuisine, Location, Amenity and O.\nSentence: where can i find a cheap place to eat in peabody", "prompt_labels": "where(O) can(O) i(O) find(O) a(O) cheap(B-Price) place(O) to(O) eat(O) in(O) peabody(B-Location)"}}
{"id": "110", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Rating", "Location", "Amenity", "Cuisine", "Dish", "Hours", "Price"], "instance": {"id": "110", "words": ["cafes", "on", "ashlannd", "street"], "labels": ["B-Cuisine", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Location, Amenity, Cuisine, Dish, Hours, Price and O.\nSentence: cafes on ashlannd street", "prompt_labels": "cafes(B-Cuisine) on(O) ashlannd(B-Location) street(I-Location)"}}
{"id": "47", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Price", "Rating", "Hours", "Amenity", "Cuisine", "Dish", "Location"], "instance": {"id": "47", "words": ["are", "there", "any", "hamburger", "restaurants", "close", "by"], "labels": ["O", "O", "O", "B-Cuisine", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Rating, Hours, Amenity, Cuisine, Dish, Location and O.\nSentence: are there any hamburger restaurants close by", "prompt_labels": "are(O) there(O) any(O) hamburger(B-Cuisine) restaurants(O) close(B-Location) by(I-Location)"}}
{"id": "93", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Dish", "Hours", "Location", "Cuisine", "Price", "Restaurant Name", "Amenity"], "instance": {"id": "93", "words": ["are", "there", "any", "tapas", "restaurants", "with", "good", "reviews", "nearby"], "labels": ["O", "O", "O", "B-Dish", "O", "O", "B-Rating", "I-Rating", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Hours, Location, Cuisine, Price, Restaurant Name, Amenity and O.\nSentence: are there any tapas restaurants with good reviews nearby", "prompt_labels": "are(O) there(O) any(O) tapas(B-Dish) restaurants(O) with(O) good(B-Rating) reviews(I-Rating) nearby(B-Location)"}}
{"id": "1233", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Cuisine", "Amenity", "Price", "Hours", "Location", "Rating", "Restaurant Name"], "instance": {"id": "1233", "words": ["whats", "the", "best", "restaurant", "within", "10", "blocks", "from", "us"], "labels": ["O", "O", "B-Rating", "O", "B-Location", "I-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Amenity, Price, Hours, Location, Rating, Restaurant Name and O.\nSentence: whats the best restaurant within 10 blocks from us", "prompt_labels": "whats(O) the(O) best(B-Rating) restaurant(O) within(B-Location) 10(I-Location) blocks(I-Location) from(I-Location) us(I-Location)"}}
{"id": "1368", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Restaurant Name", "Price", "Dish", "Hours", "Location", "Amenity", "Cuisine"], "instance": {"id": "1368", "words": ["where", "do", "i", "park", "to", "go", "to", "the", "butcher", "and", "the", "boar"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Price, Dish, Hours, Location, Amenity, Cuisine and O.\nSentence: where do i park to go to the butcher and the boar", "prompt_labels": "where(O) do(O) i(O) park(O) to(O) go(O) to(O) the(O) butcher(B-Restaurant Name) and(I-Restaurant Name) the(I-Restaurant Name) boar(I-Restaurant Name)"}}
{"id": "914", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Dish", "Location", "Cuisine", "Rating", "Restaurant Name", "Price", "Amenity"], "instance": {"id": "914", "words": ["is", "there", "any", "restaurant", "near", "kendall", "square", "that", "delivers", "tea"], "labels": ["O", "O", "O", "O", "B-Location", "I-Location", "I-Location", "O", "B-Amenity", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Dish, Location, Cuisine, Rating, Restaurant Name, Price, Amenity and O.\nSentence: is there any restaurant near kendall square that delivers tea", "prompt_labels": "is(O) there(O) any(O) restaurant(O) near(B-Location) kendall(I-Location) square(I-Location) that(O) delivers(B-Amenity) tea(B-Dish)"}}
{"id": "827", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Hours", "Location", "Cuisine", "Price", "Restaurant Name", "Amenity", "Rating"], "instance": {"id": "827", "words": ["is", "there", "a", "late", "night", "place", "in", "west", "newbury", "where", "i", "can", "smoke"], "labels": ["O", "O", "O", "B-Hours", "I-Hours", "O", "O", "B-Location", "I-Location", "O", "O", "O", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Hours, Location, Cuisine, Price, Restaurant Name, Amenity, Rating and O.\nSentence: is there a late night place in west newbury where i can smoke", "prompt_labels": "is(O) there(O) a(O) late(B-Hours) night(I-Hours) place(O) in(O) west(B-Location) newbury(I-Location) where(O) i(O) can(O) smoke(B-Amenity)"}}
{"id": "1283", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Rating", "Cuisine", "Price", "Location", "Dish", "Amenity", "Hours"], "instance": {"id": "1283", "words": ["where", "can", "i", "find", "a", "place", "within", "5", "minutes", "with", "great", "prices"], "labels": ["O", "O", "O", "O", "O", "O", "B-Location", "I-Location", "I-Location", "O", "B-Price", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Cuisine, Price, Location, Dish, Amenity, Hours and O.\nSentence: where can i find a place within 5 minutes with great prices", "prompt_labels": "where(O) can(O) i(O) find(O) a(O) place(O) within(B-Location) 5(I-Location) minutes(I-Location) with(O) great(B-Price) prices(O)"}}
{"id": "1457", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Hours", "Dish", "Rating", "Price", "Cuisine", "Restaurant Name", "Location"], "instance": {"id": "1457", "words": ["where", "is", "there", "a", "fatz", "with", "interesting", "people", "around"], "labels": ["O", "O", "O", "O", "B-Restaurant Name", "O", "B-Amenity", "I-Amenity", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Dish, Rating, Price, Cuisine, Restaurant Name, Location and O.\nSentence: where is there a fatz with interesting people around", "prompt_labels": "where(O) is(O) there(O) a(O) fatz(B-Restaurant Name) with(O) interesting(B-Amenity) people(I-Amenity) around(O)"}}
{"id": "1119", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Cuisine", "Rating", "Dish", "Amenity", "Price", "Restaurant Name", "Location"], "instance": {"id": "1119", "words": ["what", "is", "the", "best", "ice", "cream", "parlor"], "labels": ["O", "O", "O", "B-Rating", "B-Cuisine", "I-Cuisine", "I-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Rating, Dish, Amenity, Price, Restaurant Name, Location and O.\nSentence: what is the best ice cream parlor", "prompt_labels": "what(O) is(O) the(O) best(B-Rating) ice(B-Cuisine) cream(I-Cuisine) parlor(I-Cuisine)"}}
{"id": "296", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Restaurant Name", "Location", "Dish", "Amenity", "Hours", "Price", "Rating"], "instance": {"id": "296", "words": ["does", "osaka", "sushi", "express", "have", "great", "portions"], "labels": ["O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Location, Dish, Amenity, Hours, Price, Rating and O.\nSentence: does osaka sushi express have great portions", "prompt_labels": "does(O) osaka(B-Restaurant Name) sushi(I-Restaurant Name) express(I-Restaurant Name) have(O) great(B-Amenity) portions(I-Amenity)"}}
{"id": "530", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Cuisine", "Rating", "Hours", "Price", "Restaurant Name", "Location", "Dish"], "instance": {"id": "530", "words": ["how", "many", "pizza", "restaurants", "are", "nearby"], "labels": ["O", "O", "B-Dish", "O", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Cuisine, Rating, Hours, Price, Restaurant Name, Location, Dish and O.\nSentence: how many pizza restaurants are nearby", "prompt_labels": "how(O) many(O) pizza(B-Dish) restaurants(O) are(O) nearby(B-Location)"}}
{"id": "489", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Rating", "Dish", "Location", "Restaurant Name", "Hours", "Cuisine", "Amenity"], "instance": {"id": "489", "words": ["hi", "hershel", "is", "there", "any", "place", "around", "here", "with", "something", "good", "to", "eat"], "labels": ["O", "O", "O", "O", "O", "O", "B-Location", "I-Location", "O", "O", "B-Rating", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Dish, Location, Restaurant Name, Hours, Cuisine, Amenity and O.\nSentence: hi hershel is there any place around here with something good to eat", "prompt_labels": "hi(O) hershel(O) is(O) there(O) any(O) place(O) around(B-Location) here(I-Location) with(O) something(O) good(B-Rating) to(O) eat(O)"}}
{"id": "405", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Restaurant Name", "Amenity", "Location", "Rating", "Cuisine", "Hours", "Price"], "instance": {"id": "405", "words": ["find", "me", "a", "southwestern", "restaurant", "that", "serves", "breakfast", "and", "is", "located", "nearby"], "labels": ["O", "O", "O", "B-Cuisine", "O", "O", "O", "B-Hours", "O", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Restaurant Name, Amenity, Location, Rating, Cuisine, Hours, Price and O.\nSentence: find me a southwestern restaurant that serves breakfast and is located nearby", "prompt_labels": "find(O) me(O) a(O) southwestern(B-Cuisine) restaurant(O) that(O) serves(O) breakfast(B-Hours) and(O) is(O) located(B-Location) nearby(I-Location)"}}
{"id": "916", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Location", "Restaurant Name", "Amenity", "Price", "Dish", "Cuisine", "Rating"], "instance": {"id": "916", "words": ["is", "there", "any", "sandwich", "shop", "by", "the", "high", "school"], "labels": ["O", "O", "O", "B-Cuisine", "O", "B-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Restaurant Name, Amenity, Price, Dish, Cuisine, Rating and O.\nSentence: is there any sandwich shop by the high school", "prompt_labels": "is(O) there(O) any(O) sandwich(B-Cuisine) shop(O) by(B-Location) the(I-Location) high(I-Location) school(I-Location)"}}
{"id": "579", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Rating", "Amenity", "Dish", "Cuisine", "Location", "Hours", "Price"], "instance": {"id": "579", "words": ["i", "need", "a", "list", "of", "restaurants", "that", "take", "the", "diners", "card", "in", "a", "5", "mile", "radius"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "O", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Amenity, Dish, Cuisine, Location, Hours, Price and O.\nSentence: i need a list of restaurants that take the diners card in a 5 mile radius", "prompt_labels": "i(O) need(O) a(O) list(O) of(O) restaurants(O) that(O) take(O) the(O) diners(B-Amenity) card(I-Amenity) in(O) a(O) 5(B-Location) mile(I-Location) radius(I-Location)"}}
{"id": "595", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Hours", "Amenity", "Restaurant Name", "Price", "Dish", "Location", "Cuisine"], "instance": {"id": "595", "words": ["i", "need", "directions", "to", "the", "closest", "pancake", "place"], "labels": ["O", "O", "O", "O", "O", "B-Location", "B-Dish", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Amenity, Restaurant Name, Price, Dish, Location, Cuisine and O.\nSentence: i need directions to the closest pancake place", "prompt_labels": "i(O) need(O) directions(O) to(O) the(O) closest(B-Location) pancake(B-Dish) place(O)"}}
{"id": "1505", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Price", "Rating", "Location", "Cuisine", "Restaurant Name", "Hours", "Dish"], "instance": {"id": "1505", "words": ["who", "has", "happy", "hour", "right", "now"], "labels": ["O", "O", "B-Amenity", "I-Amenity", "B-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Rating, Location, Cuisine, Restaurant Name, Hours, Dish and O.\nSentence: who has happy hour right now", "prompt_labels": "who(O) has(O) happy(B-Amenity) hour(I-Amenity) right(B-Hours) now(I-Hours)"}}
{"id": "1454", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Dish", "Hours", "Amenity", "Restaurant Name", "Price", "Location", "Rating"], "instance": {"id": "1454", "words": ["where", "is", "the", "place", "with", "byob", "2", "miles", "from", "here"], "labels": ["O", "O", "O", "O", "O", "B-Amenity", "B-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Dish, Hours, Amenity, Restaurant Name, Price, Location, Rating and O.\nSentence: where is the place with byob 2 miles from here", "prompt_labels": "where(O) is(O) the(O) place(O) with(O) byob(B-Amenity) 2(B-Location) miles(I-Location) from(I-Location) here(I-Location)"}}
{"id": "487", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Rating", "Cuisine", "Hours", "Restaurant Name", "Location", "Amenity", "Price"], "instance": {"id": "487", "words": ["hey", "take", "me", "to", "variety", "food", "court"], "labels": ["O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Rating, Cuisine, Hours, Restaurant Name, Location, Amenity, Price and O.\nSentence: hey take me to variety food court", "prompt_labels": "hey(O) take(O) me(O) to(O) variety(B-Restaurant Name) food(I-Restaurant Name) court(I-Restaurant Name)"}}
{"id": "1455", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Cuisine", "Rating", "Dish", "Price", "Hours", "Location", "Amenity"], "instance": {"id": "1455", "words": ["where", "is", "the", "reading", "station", "coffee", "depot", "on", "norfolk", "ave", "do", "they", "serve", "breakfast"], "labels": ["O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "O", "B-Location", "I-Location", "O", "O", "O", "B-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Rating, Dish, Price, Hours, Location, Amenity and O.\nSentence: where is the reading station coffee depot on norfolk ave do they serve breakfast", "prompt_labels": "where(O) is(O) the(O) reading(B-Restaurant Name) station(I-Restaurant Name) coffee(I-Restaurant Name) depot(I-Restaurant Name) on(O) norfolk(B-Location) ave(I-Location) do(O) they(O) serve(O) breakfast(B-Cuisine)"}}
{"id": "14", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Hours", "Location", "Cuisine", "Dish", "Rating", "Price", "Restaurant Name"], "instance": {"id": "14", "words": ["any", "stores", "around", "where", "i", "could", "buy", "a", "pasta", "dish", "where", "the", "prices", "are", "not", "too", "high"], "labels": ["O", "O", "B-Location", "O", "O", "O", "O", "O", "B-Dish", "O", "O", "O", "B-Price", "I-Price", "I-Price", "I-Price", "I-Price"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Location, Cuisine, Dish, Rating, Price, Restaurant Name and O.\nSentence: any stores around where i could buy a pasta dish where the prices are not too high", "prompt_labels": "any(O) stores(O) around(B-Location) where(O) i(O) could(O) buy(O) a(O) pasta(B-Dish) dish(O) where(O) the(O) prices(B-Price) are(I-Price) not(I-Price) too(I-Price) high(I-Price)"}}
{"id": "280", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Dish", "Cuisine", "Price", "Restaurant Name", "Location", "Amenity", "Rating"], "instance": {"id": "280", "words": ["does", "chuck", "e", "cheeses", "have", "drive", "thru"], "labels": ["O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Dish, Cuisine, Price, Restaurant Name, Location, Amenity, Rating and O.\nSentence: does chuck e cheeses have drive thru", "prompt_labels": "does(O) chuck(B-Restaurant Name) e(I-Restaurant Name) cheeses(I-Restaurant Name) have(O) drive(B-Amenity) thru(I-Amenity)"}}
{"id": "532", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Amenity", "Restaurant Name", "Dish", "Hours", "Rating", "Location", "Price"], "instance": {"id": "532", "words": ["how", "many", "restaurants", "near", "me", "have", "bathrooms"], "labels": ["O", "O", "O", "B-Location", "I-Location", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Amenity, Restaurant Name, Dish, Hours, Rating, Location, Price and O.\nSentence: how many restaurants near me have bathrooms", "prompt_labels": "how(O) many(O) restaurants(O) near(B-Location) me(I-Location) have(B-Amenity) bathrooms(I-Amenity)"}}
{"id": "168", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Cuisine", "Amenity", "Rating", "Restaurant Name", "Location", "Dish", "Price"], "instance": {"id": "168", "words": ["can", "you", "find", "me", "hotel", "dining", "with", "comfort", "food"], "labels": ["O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "O", "B-Cuisine", "I-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Amenity, Rating, Restaurant Name, Location, Dish, Price and O.\nSentence: can you find me hotel dining with comfort food", "prompt_labels": "can(O) you(O) find(O) me(O) hotel(B-Restaurant Name) dining(I-Restaurant Name) with(O) comfort(B-Cuisine) food(I-Cuisine)"}}
{"id": "924", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Dish", "Location", "Hours", "Amenity", "Restaurant Name", "Cuisine", "Rating"], "instance": {"id": "924", "words": ["is", "there", "moderately", "priced", "food", "near"], "labels": ["O", "O", "B-Price", "O", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Location, Hours, Amenity, Restaurant Name, Cuisine, Rating and O.\nSentence: is there moderately priced food near", "prompt_labels": "is(O) there(O) moderately(B-Price) priced(O) food(O) near(B-Location)"}}
{"id": "286", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Amenity", "Location", "Hours", "Rating", "Price", "Restaurant Name", "Cuisine"], "instance": {"id": "286", "words": ["does", "jaimes", "bakery", "have", "a", "great", "decor"], "labels": ["O", "B-Restaurant Name", "I-Restaurant Name", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Location, Hours, Rating, Price, Restaurant Name, Cuisine and O.\nSentence: does jaimes bakery have a great decor", "prompt_labels": "does(O) jaimes(B-Restaurant Name) bakery(I-Restaurant Name) have(O) a(O) great(B-Amenity) decor(I-Amenity)"}}
{"id": "235", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Price", "Hours", "Amenity", "Cuisine", "Location", "Rating", "Restaurant Name"], "instance": {"id": "235", "words": ["do", "any", "of", "the", "authentic", "italian", "restaurants", "sell", "bottles", "of", "imported", "extra", "virgin", "olive", "oil"], "labels": ["O", "O", "O", "O", "B-Cuisine", "I-Cuisine", "O", "B-Amenity", "I-Amenity", "I-Amenity", "I-Amenity", "I-Amenity", "I-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Hours, Amenity, Cuisine, Location, Rating, Restaurant Name and O.\nSentence: do any of the authentic italian restaurants sell bottles of imported extra virgin olive oil", "prompt_labels": "do(O) any(O) of(O) the(O) authentic(B-Cuisine) italian(I-Cuisine) restaurants(O) sell(B-Amenity) bottles(I-Amenity) of(I-Amenity) imported(I-Amenity) extra(I-Amenity) virgin(I-Amenity) olive(I-Amenity) oil(I-Amenity)"}}
{"id": "1022", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Location", "Dish", "Amenity", "Hours", "Restaurant Name", "Price", "Rating"], "instance": {"id": "1022", "words": ["please", "find", "me", "a", "cheap", "chinese", "restaurant", "nearby"], "labels": ["O", "O", "O", "O", "B-Price", "B-Cuisine", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Dish, Amenity, Hours, Restaurant Name, Price, Rating and O.\nSentence: please find me a cheap chinese restaurant nearby", "prompt_labels": "please(O) find(O) me(O) a(O) cheap(B-Price) chinese(B-Cuisine) restaurant(O) nearby(B-Location)"}}
{"id": "1112", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Price", "Dish", "Location", "Hours", "Amenity", "Rating", "Restaurant Name"], "instance": {"id": "1112", "words": ["what", "is", "dominos", "numbers"], "labels": ["O", "O", "B-Restaurant Name", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Dish, Location, Hours, Amenity, Rating, Restaurant Name and O.\nSentence: what is dominos numbers", "prompt_labels": "what(O) is(O) dominos(B-Restaurant Name) numbers(O)"}}
{"id": "278", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Amenity", "Price", "Rating", "Cuisine", "Location", "Dish", "Restaurant Name"], "instance": {"id": "278", "words": ["does", "burger", "king", "accept", "credit", "cards"], "labels": ["O", "B-Restaurant Name", "I-Restaurant Name", "B-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Price, Rating, Cuisine, Location, Dish, Restaurant Name and O.\nSentence: does burger king accept credit cards", "prompt_labels": "does(O) burger(B-Restaurant Name) king(I-Restaurant Name) accept(B-Amenity) credit(I-Amenity) cards(I-Amenity)"}}
{"id": "1121", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Cuisine", "Amenity", "Rating", "Price", "Dish", "Hours", "Restaurant Name"], "instance": {"id": "1121", "words": ["what", "is", "the", "best", "pizza", "place", "in", "the", "theater", "district"], "labels": ["O", "O", "O", "B-Rating", "B-Cuisine", "O", "O", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Amenity, Rating, Price, Dish, Hours, Restaurant Name and O.\nSentence: what is the best pizza place in the theater district", "prompt_labels": "what(O) is(O) the(O) best(B-Rating) pizza(B-Cuisine) place(O) in(O) the(O) theater(B-Location) district(I-Location)"}}
{"id": "935", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Restaurant Name", "Amenity", "Price", "Cuisine", "Dish", "Location", "Rating"], "instance": {"id": "935", "words": ["kid", "friendly", "restaurant", "never", "design", "center", "place", "with", "uper", "nice", "service"], "labels": ["B-Amenity", "I-Amenity", "O", "O", "B-Location", "I-Location", "I-Location", "O", "B-Rating", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Amenity, Price, Cuisine, Dish, Location, Rating and O.\nSentence: kid friendly restaurant never design center place with uper nice service", "prompt_labels": "kid(B-Amenity) friendly(I-Amenity) restaurant(O) never(O) design(B-Location) center(I-Location) place(I-Location) with(O) uper(B-Rating) nice(B-Amenity) service(I-Amenity)"}}
{"id": "1402", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Rating", "Hours", "Location", "Amenity", "Restaurant Name", "Price", "Dish"], "instance": {"id": "1402", "words": ["where", "is", "the", "best", "italian", "food", "in", "the", "city"], "labels": ["O", "O", "O", "B-Rating", "B-Cuisine", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Hours, Location, Amenity, Restaurant Name, Price, Dish and O.\nSentence: where is the best italian food in the city", "prompt_labels": "where(O) is(O) the(O) best(B-Rating) italian(B-Cuisine) food(O) in(B-Location) the(I-Location) city(I-Location)"}}
{"id": "1352", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Hours", "Dish", "Amenity", "Restaurant Name", "Price", "Rating", "Location"], "instance": {"id": "1352", "words": ["where", "can", "i", "go", "to", "get", "a", "sandwich", "around", "here"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Dish", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Dish, Amenity, Restaurant Name, Price, Rating, Location and O.\nSentence: where can i go to get a sandwich around here", "prompt_labels": "where(O) can(O) i(O) go(O) to(O) get(O) a(O) sandwich(B-Dish) around(B-Location) here(I-Location)"}}
{"id": "529", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Amenity", "Dish", "Price", "Restaurant Name", "Cuisine", "Rating", "Location"], "instance": {"id": "529", "words": ["how", "many", "miles", "will", "it", "take", "me", "to", "get", "to", "dominoes"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Dish, Price, Restaurant Name, Cuisine, Rating, Location and O.\nSentence: how many miles will it take me to get to dominoes", "prompt_labels": "how(O) many(O) miles(O) will(O) it(O) take(O) me(O) to(O) get(O) to(O) dominoes(B-Restaurant Name)"}}
{"id": "1245", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Location", "Restaurant Name", "Cuisine", "Rating", "Dish", "Amenity", "Hours"], "instance": {"id": "1245", "words": ["whats", "the", "shortest", "route", "to", "mcdonalds"], "labels": ["O", "O", "O", "O", "O", "B-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Restaurant Name, Cuisine, Rating, Dish, Amenity, Hours and O.\nSentence: whats the shortest route to mcdonalds", "prompt_labels": "whats(O) the(O) shortest(O) route(O) to(O) mcdonalds(B-Restaurant Name)"}}
{"id": "1065", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Dish", "Rating", "Restaurant Name", "Hours", "Amenity", "Location", "Cuisine"], "instance": {"id": "1065", "words": ["smoke", "friendly", "restaurants"], "labels": ["B-Amenity", "I-Amenity", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Rating, Restaurant Name, Hours, Amenity, Location, Cuisine and O.\nSentence: smoke friendly restaurants", "prompt_labels": "smoke(B-Amenity) friendly(I-Amenity) restaurants(O)"}}
{"id": "71", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Dish", "Location", "Cuisine", "Hours", "Rating", "Price", "Restaurant Name"], "instance": {"id": "71", "words": ["are", "there", "any", "restaurant", "nearby", "that", "serve", "thai", "food"], "labels": ["O", "O", "O", "O", "B-Location", "O", "O", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Location, Cuisine, Hours, Rating, Price, Restaurant Name and O.\nSentence: are there any restaurant nearby that serve thai food", "prompt_labels": "are(O) there(O) any(O) restaurant(O) nearby(B-Location) that(O) serve(O) thai(B-Cuisine) food(O)"}}
{"id": "939", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Cuisine", "Price", "Amenity", "Rating", "Hours", "Dish", "Location"], "instance": {"id": "939", "words": ["lets", "go", "get", "a", "taco"], "labels": ["O", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Price, Amenity, Rating, Hours, Dish, Location and O.\nSentence: lets go get a taco", "prompt_labels": "lets(O) go(O) get(O) a(O) taco(B-Dish)"}}
{"id": "893", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Price", "Location", "Hours", "Amenity", "Rating", "Cuisine", "Restaurant Name"], "instance": {"id": "893", "words": ["is", "there", "an", "indian", "restaurant", "within", "5", "miles", "that", "has", "a", "dinner", "buffet"], "labels": ["O", "O", "O", "B-Cuisine", "O", "B-Location", "I-Location", "I-Location", "O", "O", "O", "B-Hours", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Location, Hours, Amenity, Rating, Cuisine, Restaurant Name and O.\nSentence: is there an indian restaurant within 5 miles that has a dinner buffet", "prompt_labels": "is(O) there(O) an(O) indian(B-Cuisine) restaurant(O) within(B-Location) 5(I-Location) miles(I-Location) that(O) has(O) a(O) dinner(B-Hours) buffet(B-Amenity)"}}
{"id": "67", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Cuisine", "Amenity", "Rating", "Location", "Dish", "Price", "Restaurant Name"], "instance": {"id": "67", "words": ["are", "there", "any", "places", "near", "by", "that", "serve", "lunch", "all", "day"], "labels": ["O", "O", "O", "O", "B-Location", "I-Location", "O", "O", "B-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Amenity, Rating, Location, Dish, Price, Restaurant Name and O.\nSentence: are there any places near by that serve lunch all day", "prompt_labels": "are(O) there(O) any(O) places(O) near(B-Location) by(I-Location) that(O) serve(O) lunch(B-Hours) all(I-Hours) day(I-Hours)"}}
{"id": "157", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Restaurant Name", "Location", "Cuisine", "Dish", "Price", "Hours", "Rating"], "instance": {"id": "157", "words": ["can", "you", "find", "me", "a", "moderately", "priced", "italian", "restaurant"], "labels": ["O", "O", "O", "O", "O", "B-Price", "O", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Restaurant Name, Location, Cuisine, Dish, Price, Hours, Rating and O.\nSentence: can you find me a moderately priced italian restaurant", "prompt_labels": "can(O) you(O) find(O) me(O) a(O) moderately(B-Price) priced(O) italian(B-Cuisine) restaurant(O)"}}
{"id": "718", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Location", "Dish", "Rating", "Price", "Amenity", "Hours", "Restaurant Name"], "instance": {"id": "718", "words": ["im", "feeling", "a", "little", "down", "so", "id", "like", "go", "somewhere", "thats", "really", "bright", "and", "fun", "for", "breakfast"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity", "O", "B-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Dish, Rating, Price, Amenity, Hours, Restaurant Name and O.\nSentence: im feeling a little down so id like go somewhere thats really bright and fun for breakfast", "prompt_labels": "im(O) feeling(O) a(O) little(O) down(O) so(O) id(O) like(O) go(O) somewhere(O) thats(O) really(O) bright(B-Amenity) and(I-Amenity) fun(I-Amenity) for(O) breakfast(B-Hours)"}}
{"id": "385", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Restaurant Name", "Hours", "Cuisine", "Location", "Price", "Amenity", "Rating"], "instance": {"id": "385", "words": ["find", "me", "a", "place", "that", "serves", "chinese", "takeout"], "labels": ["O", "O", "O", "O", "O", "O", "B-Cuisine", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Restaurant Name, Hours, Cuisine, Location, Price, Amenity, Rating and O.\nSentence: find me a place that serves chinese takeout", "prompt_labels": "find(O) me(O) a(O) place(O) that(O) serves(O) chinese(B-Cuisine) takeout(B-Amenity)"}}
{"id": "688", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Rating", "Price", "Restaurant Name", "Amenity", "Dish", "Hours", "Cuisine"], "instance": {"id": "688", "words": ["i", "would", "like", "some", "food", "what", "restaurants", "arent", "closed", "in", "the", "area"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-Hours", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Price, Restaurant Name, Amenity, Dish, Hours, Cuisine and O.\nSentence: i would like some food what restaurants arent closed in the area", "prompt_labels": "i(O) would(O) like(O) some(O) food(O) what(O) restaurants(O) arent(O) closed(B-Hours) in(B-Location) the(I-Location) area(I-Location)"}}
{"id": "1440", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Restaurant Name", "Amenity", "Cuisine", "Price", "Rating", "Location", "Hours"], "instance": {"id": "1440", "words": ["where", "is", "the", "nearest", "place", "with", "a", "dining", "patio"], "labels": ["O", "O", "O", "B-Location", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Restaurant Name, Amenity, Cuisine, Price, Rating, Location, Hours and O.\nSentence: where is the nearest place with a dining patio", "prompt_labels": "where(O) is(O) the(O) nearest(B-Location) place(O) with(O) a(O) dining(B-Amenity) patio(I-Amenity)"}}
{"id": "1487", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Rating", "Price", "Amenity", "Hours", "Restaurant Name", "Cuisine", "Location"], "instance": {"id": "1487", "words": ["which", "pizza", "places", "will", "deliver", "to", "me"], "labels": ["O", "B-Cuisine", "O", "O", "B-Amenity", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Rating, Price, Amenity, Hours, Restaurant Name, Cuisine, Location and O.\nSentence: which pizza places will deliver to me", "prompt_labels": "which(O) pizza(B-Cuisine) places(O) will(O) deliver(B-Amenity) to(O) me(O)"}}
{"id": "774", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Dish", "Cuisine", "Location", "Price", "Hours", "Amenity", "Restaurant Name"], "instance": {"id": "774", "words": ["is", "papa", "johns", "on", "cradle", "way", "still", "open"], "labels": ["O", "B-Restaurant Name", "I-Restaurant Name", "O", "B-Location", "I-Location", "B-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Cuisine, Location, Price, Hours, Amenity, Restaurant Name and O.\nSentence: is papa johns on cradle way still open", "prompt_labels": "is(O) papa(B-Restaurant Name) johns(I-Restaurant Name) on(O) cradle(B-Location) way(I-Location) still(B-Hours) open(I-Hours)"}}
{"id": "824", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Dish", "Restaurant Name", "Cuisine", "Location", "Rating", "Price", "Hours"], "instance": {"id": "824", "words": ["is", "there", "a", "japanese", "restraunt", "near", "by"], "labels": ["O", "O", "O", "B-Cuisine", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Restaurant Name, Cuisine, Location, Rating, Price, Hours and O.\nSentence: is there a japanese restraunt near by", "prompt_labels": "is(O) there(O) a(O) japanese(B-Cuisine) restraunt(O) near(B-Location) by(I-Location)"}}
{"id": "1171", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Rating", "Restaurant Name", "Price", "Amenity", "Cuisine", "Location", "Hours"], "instance": {"id": "1171", "words": ["what", "places", "have", "great", "selections", "of", "beer"], "labels": ["O", "O", "O", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Rating, Restaurant Name, Price, Amenity, Cuisine, Location, Hours and O.\nSentence: what places have great selections of beer", "prompt_labels": "what(O) places(O) have(O) great(O) selections(O) of(O) beer(B-Dish)"}}
{"id": "754", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Rating", "Location", "Price", "Restaurant Name", "Hours", "Cuisine", "Dish"], "instance": {"id": "754", "words": ["im", "starving", "so", "fast", "food", "will", "do"], "labels": ["O", "O", "O", "B-Cuisine", "I-Cuisine", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Location, Price, Restaurant Name, Hours, Cuisine, Dish and O.\nSentence: im starving so fast food will do", "prompt_labels": "im(O) starving(O) so(O) fast(B-Cuisine) food(I-Cuisine) will(O) do(O)"}}
{"id": "61", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Location", "Restaurant Name", "Price", "Cuisine", "Amenity", "Rating", "Dish"], "instance": {"id": "61", "words": ["are", "there", "any", "new", "restaurants", "nearby", "that", "i", "can", "try"], "labels": ["O", "O", "O", "B-Amenity", "O", "B-Location", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Restaurant Name, Price, Cuisine, Amenity, Rating, Dish and O.\nSentence: are there any new restaurants nearby that i can try", "prompt_labels": "are(O) there(O) any(O) new(B-Amenity) restaurants(O) nearby(B-Location) that(O) i(O) can(O) try(O)"}}
{"id": "758", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Location", "Cuisine", "Dish", "Restaurant Name", "Amenity", "Price", "Rating"], "instance": {"id": "758", "words": ["in", "which", "restaurants", "can", "one", "smoke"], "labels": ["O", "O", "O", "O", "O", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Cuisine, Dish, Restaurant Name, Amenity, Price, Rating and O.\nSentence: in which restaurants can one smoke", "prompt_labels": "in(O) which(O) restaurants(O) can(O) one(O) smoke(B-Amenity)"}}
{"id": "877", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Hours", "Restaurant Name", "Rating", "Cuisine", "Location", "Amenity", "Price"], "instance": {"id": "877", "words": ["is", "there", "a", "taco", "joint", "near", "the", "college"], "labels": ["O", "O", "O", "B-Cuisine", "I-Cuisine", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Hours, Restaurant Name, Rating, Cuisine, Location, Amenity, Price and O.\nSentence: is there a taco joint near the college", "prompt_labels": "is(O) there(O) a(O) taco(B-Cuisine) joint(I-Cuisine) near(B-Location) the(I-Location) college(I-Location)"}}
{"id": "404", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Amenity", "Hours", "Location", "Cuisine", "Rating", "Restaurant Name", "Dish"], "instance": {"id": "404", "words": ["find", "me", "a", "sitdown", "place", "with", "a", "buy", "one", "get", "one", "free", "offer"], "labels": ["O", "O", "O", "B-Amenity", "O", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity", "I-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Hours, Location, Cuisine, Rating, Restaurant Name, Dish and O.\nSentence: find me a sitdown place with a buy one get one free offer", "prompt_labels": "find(O) me(O) a(O) sitdown(B-Amenity) place(O) with(O) a(O) buy(B-Amenity) one(I-Amenity) get(I-Amenity) one(I-Amenity) free(I-Amenity) offer(I-Amenity)"}}
{"id": "117", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Cuisine", "Location", "Dish", "Price", "Amenity", "Restaurant Name", "Rating"], "instance": {"id": "117", "words": ["can", "i", "dine", "at", "the", "barat", "a", "nossa", "casa"], "labels": ["O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Location, Dish, Price, Amenity, Restaurant Name, Rating and O.\nSentence: can i dine at the barat a nossa casa", "prompt_labels": "can(O) i(O) dine(O) at(O) the(O) barat(B-Restaurant Name) a(I-Restaurant Name) nossa(I-Restaurant Name) casa(I-Restaurant Name)"}}
{"id": "1149", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Restaurant Name", "Price", "Cuisine", "Amenity", "Location", "Dish", "Hours"], "instance": {"id": "1149", "words": ["what", "is", "the", "price", "range", "for", "dinner", "at", "clarkes", "family", "restaurant"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Price, Cuisine, Amenity, Location, Dish, Hours and O.\nSentence: what is the price range for dinner at clarkes family restaurant", "prompt_labels": "what(O) is(O) the(O) price(O) range(O) for(O) dinner(O) at(O) clarkes(B-Restaurant Name) family(I-Restaurant Name) restaurant(I-Restaurant Name)"}}
{"id": "1490", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Hours", "Amenity", "Location", "Restaurant Name", "Dish", "Cuisine", "Price"], "instance": {"id": "1490", "words": ["which", "restaurant", "has", "a", "better", "rating", "on", "yelp", "sakura", "or", "fuji", "ya"], "labels": ["O", "O", "O", "O", "B-Rating", "I-Rating", "O", "O", "B-Restaurant Name", "O", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Amenity, Location, Restaurant Name, Dish, Cuisine, Price and O.\nSentence: which restaurant has a better rating on yelp sakura or fuji ya", "prompt_labels": "which(O) restaurant(O) has(O) a(O) better(B-Rating) rating(I-Rating) on(O) yelp(O) sakura(B-Restaurant Name) or(O) fuji(B-Restaurant Name) ya(I-Restaurant Name)"}}
{"id": "831", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Location", "Rating", "Cuisine", "Dish", "Price", "Restaurant Name", "Amenity"], "instance": {"id": "831", "words": ["is", "there", "a", "mcdonalds", "between", "here", "and", "my", "destination"], "labels": ["O", "O", "O", "B-Restaurant Name", "B-Location", "I-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Rating, Cuisine, Dish, Price, Restaurant Name, Amenity and O.\nSentence: is there a mcdonalds between here and my destination", "prompt_labels": "is(O) there(O) a(O) mcdonalds(B-Restaurant Name) between(B-Location) here(I-Location) and(I-Location) my(I-Location) destination(I-Location)"}}
{"id": "75", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Amenity", "Cuisine", "Price", "Dish", "Restaurant Name", "Rating", "Location"], "instance": {"id": "75", "words": ["are", "there", "any", "restaurants", "nearby", "that", "have", "outdoor", "dining"], "labels": ["O", "O", "O", "O", "B-Location", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Cuisine, Price, Dish, Restaurant Name, Rating, Location and O.\nSentence: are there any restaurants nearby that have outdoor dining", "prompt_labels": "are(O) there(O) any(O) restaurants(O) nearby(B-Location) that(O) have(O) outdoor(B-Amenity) dining(I-Amenity)"}}
{"id": "109", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Restaurant Name", "Location", "Price", "Dish", "Rating", "Hours", "Amenity"], "instance": {"id": "109", "words": ["burgers"], "labels": ["B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Location, Price, Dish, Rating, Hours, Amenity and O.\nSentence: burgers", "prompt_labels": "burgers(B-Dish)"}}
{"id": "510", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Rating", "Dish", "Location", "Hours", "Amenity", "Price", "Restaurant Name"], "instance": {"id": "510", "words": ["how", "far", "away", "is", "the", "closest", "burger", "king"], "labels": ["O", "O", "O", "O", "O", "B-Location", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Dish, Location, Hours, Amenity, Price, Restaurant Name and O.\nSentence: how far away is the closest burger king", "prompt_labels": "how(O) far(O) away(O) is(O) the(O) closest(B-Location) burger(B-Restaurant Name) king(I-Restaurant Name)"}}
{"id": "555", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Location", "Hours", "Price", "Restaurant Name", "Rating", "Dish", "Cuisine"], "instance": {"id": "555", "words": ["i", "am", "looking", "for", "sandwhiches"], "labels": ["O", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Hours, Price, Restaurant Name, Rating, Dish, Cuisine and O.\nSentence: i am looking for sandwhiches", "prompt_labels": "i(O) am(O) looking(O) for(O) sandwhiches(B-Dish)"}}
{"id": "1382", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Hours", "Dish", "Amenity", "Cuisine", "Restaurant Name", "Rating", "Price"], "instance": {"id": "1382", "words": ["where", "is", "a", "restaurant", "that", "opens", "24", "hours"], "labels": ["O", "O", "O", "O", "O", "B-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Hours, Dish, Amenity, Cuisine, Restaurant Name, Rating, Price and O.\nSentence: where is a restaurant that opens 24 hours", "prompt_labels": "where(O) is(O) a(O) restaurant(O) that(O) opens(B-Hours) 24(I-Hours) hours(I-Hours)"}}
{"id": "1363", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Hours", "Rating", "Price", "Cuisine", "Dish", "Amenity", "Location"], "instance": {"id": "1363", "words": ["where", "could", "i", "eat", "sushi", "on", "the", "waterfront"], "labels": ["O", "O", "O", "O", "B-Dish", "O", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Rating, Price, Cuisine, Dish, Amenity, Location and O.\nSentence: where could i eat sushi on the waterfront", "prompt_labels": "where(O) could(O) i(O) eat(O) sushi(B-Dish) on(O) the(O) waterfront(B-Location)"}}
{"id": "1042", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Rating", "Restaurant Name", "Location", "Cuisine", "Hours", "Dish", "Price"], "instance": {"id": "1042", "words": ["reasonably", "priced", "byob", "irish", "restaurant"], "labels": ["B-Price", "O", "B-Amenity", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Restaurant Name, Location, Cuisine, Hours, Dish, Price and O.\nSentence: reasonably priced byob irish restaurant", "prompt_labels": "reasonably(B-Price) priced(O) byob(B-Amenity) irish(B-Cuisine) restaurant(O)"}}
{"id": "923", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Hours", "Cuisine", "Dish", "Rating", "Location", "Amenity", "Restaurant Name"], "instance": {"id": "923", "words": ["is", "there", "food", "near", "by"], "labels": ["O", "O", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Cuisine, Dish, Rating, Location, Amenity, Restaurant Name and O.\nSentence: is there food near by", "prompt_labels": "is(O) there(O) food(O) near(B-Location) by(I-Location)"}}
{"id": "1515", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Cuisine", "Location", "Price", "Hours", "Rating", "Amenity", "Dish"], "instance": {"id": "1515", "words": ["will", "i", "be", "able", "to", "find", "a", "romantic", "restaurant", "for", "my", "date", "tonight"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Amenity", "O", "O", "O", "O", "B-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Location, Price, Hours, Rating, Amenity, Dish and O.\nSentence: will i be able to find a romantic restaurant for my date tonight", "prompt_labels": "will(O) i(O) be(O) able(O) to(O) find(O) a(O) romantic(B-Amenity) restaurant(O) for(O) my(O) date(O) tonight(B-Hours)"}}
{"id": "1005", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Location", "Hours", "Cuisine", "Price", "Rating", "Dish", "Restaurant Name"], "instance": {"id": "1005", "words": ["my", "kids", "want", "a", "happy", "meal"], "labels": ["O", "O", "O", "O", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Hours, Cuisine, Price, Rating, Dish, Restaurant Name and O.\nSentence: my kids want a happy meal", "prompt_labels": "my(O) kids(O) want(O) a(O) happy(B-Cuisine) meal(O)"}}
{"id": "175", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Dish", "Amenity", "Location", "Rating", "Restaurant Name", "Hours", "Cuisine"], "instance": {"id": "175", "words": ["can", "you", "find", "the", "waterfront", "restaurant", "albertos", "deli", "of", "course", "thats", "open", "until", "11", "pm"], "labels": ["O", "O", "O", "O", "B-Location", "O", "B-Restaurant Name", "I-Restaurant Name", "O", "O", "O", "B-Hours", "I-Hours", "I-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Amenity, Location, Rating, Restaurant Name, Hours, Cuisine and O.\nSentence: can you find the waterfront restaurant albertos deli of course thats open until 11 pm", "prompt_labels": "can(O) you(O) find(O) the(O) waterfront(B-Location) restaurant(O) albertos(B-Restaurant Name) deli(I-Restaurant Name) of(O) course(O) thats(O) open(B-Hours) until(I-Hours) 11(I-Hours) pm(I-Hours)"}}
{"id": "712", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Price", "Dish", "Amenity", "Restaurant Name", "Cuisine", "Hours", "Rating"], "instance": {"id": "712", "words": ["id", "like", "to", "go", "somewhere", "off", "the", "beaten", "path", "to", "get", "some", "middle", "eastern", "food"], "labels": ["O", "O", "O", "O", "O", "B-Location", "I-Location", "I-Location", "I-Location", "O", "O", "O", "B-Cuisine", "I-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Dish, Amenity, Restaurant Name, Cuisine, Hours, Rating and O.\nSentence: id like to go somewhere off the beaten path to get some middle eastern food", "prompt_labels": "id(O) like(O) to(O) go(O) somewhere(O) off(B-Location) the(I-Location) beaten(I-Location) path(I-Location) to(O) get(O) some(O) middle(B-Cuisine) eastern(I-Cuisine) food(O)"}}
{"id": "1158", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Hours", "Cuisine", "Rating", "Restaurant Name", "Location", "Dish", "Amenity"], "instance": {"id": "1158", "words": ["what", "kind", "of", "food", "does", "abc", "cafe", "serve"], "labels": ["O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Cuisine, Rating, Restaurant Name, Location, Dish, Amenity and O.\nSentence: what kind of food does abc cafe serve", "prompt_labels": "what(O) kind(O) of(O) food(O) does(O) abc(B-Restaurant Name) cafe(I-Restaurant Name) serve(O)"}}
{"id": "1087", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Price", "Amenity", "Cuisine", "Restaurant Name", "Hours", "Rating", "Dish"], "instance": {"id": "1087", "words": ["want", "to", "find", "a", "restaurant", "that", "specilizes", "in", "spagetti"], "labels": ["O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Amenity, Cuisine, Restaurant Name, Hours, Rating, Dish and O.\nSentence: want to find a restaurant that specilizes in spagetti", "prompt_labels": "want(O) to(O) find(O) a(O) restaurant(O) that(O) specilizes(B-Amenity) in(I-Amenity) spagetti(B-Dish)"}}
{"id": "1415", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Dish", "Rating", "Hours", "Cuisine", "Price", "Location", "Amenity"], "instance": {"id": "1415", "words": ["where", "is", "the", "closest", "non", "smoking", "restaurant"], "labels": ["O", "O", "O", "B-Location", "B-Amenity", "I-Amenity", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Rating, Hours, Cuisine, Price, Location, Amenity and O.\nSentence: where is the closest non smoking restaurant", "prompt_labels": "where(O) is(O) the(O) closest(B-Location) non(B-Amenity) smoking(I-Amenity) restaurant(O)"}}
{"id": "200", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Hours", "Price", "Cuisine", "Dish", "Location", "Restaurant Name", "Amenity"], "instance": {"id": "200", "words": ["can", "you", "order", "me", "a", "pizza"], "labels": ["O", "O", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Price, Cuisine, Dish, Location, Restaurant Name, Amenity and O.\nSentence: can you order me a pizza", "prompt_labels": "can(O) you(O) order(O) me(O) a(O) pizza(B-Dish)"}}
{"id": "575", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Dish", "Restaurant Name", "Hours", "Amenity", "Location", "Rating", "Price"], "instance": {"id": "575", "words": ["i", "need", "a", "kid", "friendly", "lunch", "place"], "labels": ["O", "O", "O", "B-Amenity", "I-Amenity", "B-Hours", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Dish, Restaurant Name, Hours, Amenity, Location, Rating, Price and O.\nSentence: i need a kid friendly lunch place", "prompt_labels": "i(O) need(O) a(O) kid(B-Amenity) friendly(I-Amenity) lunch(B-Hours) place(O)"}}
{"id": "1307", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Price", "Dish", "Amenity", "Location", "Hours", "Cuisine", "Restaurant Name"], "instance": {"id": "1307", "words": ["where", "can", "i", "get", "a", "cheap", "dinner"], "labels": ["O", "O", "O", "O", "O", "B-Price", "B-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Dish, Amenity, Location, Hours, Cuisine, Restaurant Name and O.\nSentence: where can i get a cheap dinner", "prompt_labels": "where(O) can(O) i(O) get(O) a(O) cheap(B-Price) dinner(B-Hours)"}}
{"id": "1183", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Rating", "Dish", "Price", "Hours", "Restaurant Name", "Location", "Cuisine"], "instance": {"id": "1183", "words": ["what", "restaurant", "sells", "crab", "close", "by"], "labels": ["O", "O", "O", "B-Dish", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Dish, Price, Hours, Restaurant Name, Location, Cuisine and O.\nSentence: what restaurant sells crab close by", "prompt_labels": "what(O) restaurant(O) sells(O) crab(B-Dish) close(B-Location) by(I-Location)"}}
{"id": "114", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Amenity", "Restaurant Name", "Cuisine", "Rating", "Price", "Location", "Dish"], "instance": {"id": "114", "words": ["call", "the", "closest", "korean", "restaurant"], "labels": ["O", "O", "B-Location", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Restaurant Name, Cuisine, Rating, Price, Location, Dish and O.\nSentence: call the closest korean restaurant", "prompt_labels": "call(O) the(O) closest(B-Location) korean(B-Cuisine) restaurant(O)"}}
{"id": "895", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Hours", "Cuisine", "Amenity", "Restaurant Name", "Location", "Price", "Dish"], "instance": {"id": "895", "words": ["is", "there", "an", "italian", "place", "nearby"], "labels": ["O", "O", "O", "B-Cuisine", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Cuisine, Amenity, Restaurant Name, Location, Price, Dish and O.\nSentence: is there an italian place nearby", "prompt_labels": "is(O) there(O) an(O) italian(B-Cuisine) place(O) nearby(B-Location)"}}
{"id": "41", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Price", "Amenity", "Rating", "Cuisine", "Dish", "Hours", "Restaurant Name"], "instance": {"id": "41", "words": ["are", "there", "any", "four", "star", "restaurants", "in", "this", "town"], "labels": ["O", "O", "O", "B-Rating", "I-Rating", "O", "O", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Amenity, Rating, Cuisine, Dish, Hours, Restaurant Name and O.\nSentence: are there any four star restaurants in this town", "prompt_labels": "are(O) there(O) any(O) four(B-Rating) star(I-Rating) restaurants(O) in(O) this(O) town(B-Location)"}}
{"id": "647", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Dish", "Price", "Cuisine", "Location", "Hours", "Restaurant Name", "Rating"], "instance": {"id": "647", "words": ["i", "want", "to", "eat", "hamburgers"], "labels": ["O", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Price, Cuisine, Location, Hours, Restaurant Name, Rating and O.\nSentence: i want to eat hamburgers", "prompt_labels": "i(O) want(O) to(O) eat(O) hamburgers(B-Dish)"}}
{"id": "156", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Location", "Restaurant Name", "Dish", "Amenity", "Hours", "Cuisine", "Price"], "instance": {"id": "156", "words": ["can", "you", "find", "me", "a", "kid", "friendly", "sushi", "restaurant"], "labels": ["O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Location, Restaurant Name, Dish, Amenity, Hours, Cuisine, Price and O.\nSentence: can you find me a kid friendly sushi restaurant", "prompt_labels": "can(O) you(O) find(O) me(O) a(O) kid(B-Amenity) friendly(I-Amenity) sushi(B-Cuisine) restaurant(O)"}}
{"id": "779", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Amenity", "Dish", "Cuisine", "Restaurant Name", "Rating", "Hours", "Location"], "instance": {"id": "779", "words": ["is", "sidney", "and", "hampton", "an", "expensive", "hotel", "restaurant"], "labels": ["O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "O", "B-Price", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Dish, Cuisine, Restaurant Name, Rating, Hours, Location and O.\nSentence: is sidney and hampton an expensive hotel restaurant", "prompt_labels": "is(O) sidney(B-Restaurant Name) and(I-Restaurant Name) hampton(I-Restaurant Name) an(O) expensive(B-Price) hotel(O) restaurant(O)"}}
{"id": "544", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Dish", "Restaurant Name", "Cuisine", "Rating", "Location", "Price", "Amenity"], "instance": {"id": "544", "words": ["i", "am", "in", "the", "mood", "for", "some", "chinese", "food", "can", "you", "find", "me", "a", "place"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Cuisine", "O", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Dish, Restaurant Name, Cuisine, Rating, Location, Price, Amenity and O.\nSentence: i am in the mood for some chinese food can you find me a place", "prompt_labels": "i(O) am(O) in(O) the(O) mood(O) for(O) some(O) chinese(B-Cuisine) food(O) can(O) you(O) find(O) me(O) a(O) place(O)"}}
{"id": "1006", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Rating", "Hours", "Restaurant Name", "Location", "Dish", "Amenity", "Cuisine"], "instance": {"id": "1006", "words": ["my", "kids", "want", "to", "sit", "outside", "and", "eat", "cheeseburgers"], "labels": ["O", "O", "O", "O", "B-Amenity", "I-Amenity", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Hours, Restaurant Name, Location, Dish, Amenity, Cuisine and O.\nSentence: my kids want to sit outside and eat cheeseburgers", "prompt_labels": "my(O) kids(O) want(O) to(O) sit(B-Amenity) outside(I-Amenity) and(O) eat(O) cheeseburgers(B-Dish)"}}
{"id": "604", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Hours", "Rating", "Restaurant Name", "Dish", "Price", "Location", "Cuisine"], "instance": {"id": "604", "words": ["i", "need", "something", "hot", "to", "eat", "on", "the", "way", "to", "work"], "labels": ["O", "O", "O", "B-Cuisine", "O", "O", "O", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Rating, Restaurant Name, Dish, Price, Location, Cuisine and O.\nSentence: i need something hot to eat on the way to work", "prompt_labels": "i(O) need(O) something(O) hot(B-Cuisine) to(O) eat(O) on(O) the(O) way(B-Location) to(I-Location) work(I-Location)"}}
{"id": "927", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Cuisine", "Location", "Hours", "Rating", "Amenity", "Restaurant Name", "Price"], "instance": {"id": "927", "words": ["is", "there", "somewhere", "to", "eat", "that", "is", "kid", "friendly"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Location, Hours, Rating, Amenity, Restaurant Name, Price and O.\nSentence: is there somewhere to eat that is kid friendly", "prompt_labels": "is(O) there(O) somewhere(O) to(O) eat(O) that(O) is(O) kid(B-Amenity) friendly(I-Amenity)"}}
{"id": "941", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Price", "Dish", "Location", "Rating", "Restaurant Name", "Hours", "Cuisine"], "instance": {"id": "941", "words": ["list", "close", "restaurants"], "labels": ["O", "B-Location", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Dish, Location, Rating, Restaurant Name, Hours, Cuisine and O.\nSentence: list close restaurants", "prompt_labels": "list(O) close(B-Location) restaurants(O)"}}
{"id": "493", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Rating", "Restaurant Name", "Price", "Dish", "Hours", "Amenity", "Location"], "instance": {"id": "493", "words": ["how", "are", "the", "prices", "at", "donatellas"], "labels": ["O", "O", "O", "O", "O", "B-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Restaurant Name, Price, Dish, Hours, Amenity, Location and O.\nSentence: how are the prices at donatellas", "prompt_labels": "how(O) are(O) the(O) prices(O) at(O) donatellas(B-Restaurant Name)"}}
{"id": "423", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Rating", "Restaurant Name", "Hours", "Price", "Dish", "Cuisine", "Amenity"], "instance": {"id": "423", "words": ["find", "me", "chicken", "places", "that", "accept", "discover", "card"], "labels": ["O", "O", "B-Dish", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Restaurant Name, Hours, Price, Dish, Cuisine, Amenity and O.\nSentence: find me chicken places that accept discover card", "prompt_labels": "find(O) me(O) chicken(B-Dish) places(O) that(O) accept(B-Amenity) discover(I-Amenity) card(I-Amenity)"}}
{"id": "1260", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Rating", "Amenity", "Dish", "Location", "Restaurant Name", "Price", "Hours"], "instance": {"id": "1260", "words": ["where", "can", "i", "eat", "around", "here"], "labels": ["O", "O", "O", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Amenity, Dish, Location, Restaurant Name, Price, Hours and O.\nSentence: where can i eat around here", "prompt_labels": "where(O) can(O) i(O) eat(O) around(B-Location) here(I-Location)"}}
{"id": "275", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Cuisine", "Hours", "Price", "Dish", "Restaurant Name", "Location", "Amenity"], "instance": {"id": "275", "words": ["does", "anyone", "in", "town", "deliver", "tasty", "vegan", "pizza"], "labels": ["O", "O", "B-Location", "I-Location", "B-Amenity", "B-Rating", "B-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Hours, Price, Dish, Restaurant Name, Location, Amenity and O.\nSentence: does anyone in town deliver tasty vegan pizza", "prompt_labels": "does(O) anyone(O) in(B-Location) town(I-Location) deliver(B-Amenity) tasty(B-Rating) vegan(B-Dish) pizza(I-Dish)"}}
{"id": "1108", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Rating", "Hours", "Amenity", "Restaurant Name", "Dish", "Location", "Cuisine"], "instance": {"id": "1108", "words": ["what", "has", "the", "best", "dumplings", "in", "town"], "labels": ["O", "O", "O", "B-Rating", "B-Dish", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Hours, Amenity, Restaurant Name, Dish, Location, Cuisine and O.\nSentence: what has the best dumplings in town", "prompt_labels": "what(O) has(O) the(O) best(B-Rating) dumplings(B-Dish) in(B-Location) town(I-Location)"}}
{"id": "508", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Location", "Hours", "Restaurant Name", "Price", "Rating", "Dish", "Cuisine"], "instance": {"id": "508", "words": ["how", "far", "am", "i", "from", "true", "thai", "right", "now"], "labels": ["O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Hours, Restaurant Name, Price, Rating, Dish, Cuisine and O.\nSentence: how far am i from true thai right now", "prompt_labels": "how(O) far(O) am(O) i(O) from(O) true(B-Restaurant Name) thai(I-Restaurant Name) right(O) now(O)"}}
{"id": "1447", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Amenity", "Hours", "Restaurant Name", "Rating", "Price", "Location", "Cuisine"], "instance": {"id": "1447", "words": ["where", "is", "the", "nearest", "sushi", "bar"], "labels": ["O", "O", "O", "B-Location", "B-Cuisine", "I-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Amenity, Hours, Restaurant Name, Rating, Price, Location, Cuisine and O.\nSentence: where is the nearest sushi bar", "prompt_labels": "where(O) is(O) the(O) nearest(B-Location) sushi(B-Cuisine) bar(I-Cuisine)"}}
{"id": "468", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Hours", "Restaurant Name", "Dish", "Cuisine", "Rating", "Location", "Amenity"], "instance": {"id": "468", "words": ["give", "me", "the", "closest", "place", "that", "does", "sushi"], "labels": ["O", "O", "O", "B-Location", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Restaurant Name, Dish, Cuisine, Rating, Location, Amenity and O.\nSentence: give me the closest place that does sushi", "prompt_labels": "give(O) me(O) the(O) closest(B-Location) place(O) that(O) does(O) sushi(B-Dish)"}}
{"id": "165", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Location", "Hours", "Price", "Rating", "Amenity", "Dish", "Restaurant Name"], "instance": {"id": "165", "words": ["can", "you", "find", "me", "a", "restaurant", "that", "has", "entrees", "priced", "between", "15", "and", "20", "dollars"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Price", "I-Price", "I-Price", "I-Price", "I-Price"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Hours, Price, Rating, Amenity, Dish, Restaurant Name and O.\nSentence: can you find me a restaurant that has entrees priced between 15 and 20 dollars", "prompt_labels": "can(O) you(O) find(O) me(O) a(O) restaurant(O) that(O) has(O) entrees(O) priced(O) between(B-Price) 15(I-Price) and(I-Price) 20(I-Price) dollars(I-Price)"}}
{"id": "13", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Rating", "Price", "Location", "Restaurant Name", "Hours", "Amenity", "Dish"], "instance": {"id": "13", "words": ["any", "restaurants", "that", "still", "allow", "smoking"], "labels": ["O", "O", "O", "O", "O", "B-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Rating, Price, Location, Restaurant Name, Hours, Amenity, Dish and O.\nSentence: any restaurants that still allow smoking", "prompt_labels": "any(O) restaurants(O) that(O) still(O) allow(O) smoking(B-Amenity)"}}
{"id": "131", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Rating", "Amenity", "Hours", "Dish", "Restaurant Name", "Price", "Cuisine"], "instance": {"id": "131", "words": ["can", "i", "wear", "shorts"], "labels": ["O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Amenity, Hours, Dish, Restaurant Name, Price, Cuisine and O.\nSentence: can i wear shorts", "prompt_labels": "can(O) i(O) wear(B-Amenity) shorts(I-Amenity)"}}
{"id": "1504", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Cuisine", "Location", "Price", "Hours", "Restaurant Name", "Amenity", "Dish"], "instance": {"id": "1504", "words": ["who", "has", "good", "burgers", "around", "here"], "labels": ["O", "O", "B-Rating", "B-Dish", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Location, Price, Hours, Restaurant Name, Amenity, Dish and O.\nSentence: who has good burgers around here", "prompt_labels": "who(O) has(O) good(B-Rating) burgers(B-Dish) around(B-Location) here(I-Location)"}}
{"id": "1471", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Hours", "Rating", "Cuisine", "Dish", "Restaurant Name", "Location", "Price"], "instance": {"id": "1471", "words": ["wheres", "the", "closest", "pizza", "place"], "labels": ["O", "O", "B-Location", "B-Dish", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Rating, Cuisine, Dish, Restaurant Name, Location, Price and O.\nSentence: wheres the closest pizza place", "prompt_labels": "wheres(O) the(O) closest(B-Location) pizza(B-Dish) place(O)"}}
{"id": "1412", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Cuisine", "Hours", "Rating", "Restaurant Name", "Location", "Dish", "Price"], "instance": {"id": "1412", "words": ["where", "is", "the", "closest", "happy", "hour"], "labels": ["O", "O", "O", "B-Location", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Cuisine, Hours, Rating, Restaurant Name, Location, Dish, Price and O.\nSentence: where is the closest happy hour", "prompt_labels": "where(O) is(O) the(O) closest(B-Location) happy(B-Amenity) hour(I-Amenity)"}}
{"id": "1476", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Rating", "Restaurant Name", "Hours", "Cuisine", "Dish", "Price", "Location"], "instance": {"id": "1476", "words": ["wheres", "the", "nearest", "italian", "restaurant"], "labels": ["O", "O", "B-Location", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Restaurant Name, Hours, Cuisine, Dish, Price, Location and O.\nSentence: wheres the nearest italian restaurant", "prompt_labels": "wheres(O) the(O) nearest(B-Location) italian(B-Cuisine) restaurant(O)"}}
{"id": "859", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Dish", "Rating", "Restaurant Name", "Hours", "Amenity", "Price", "Cuisine"], "instance": {"id": "859", "words": ["is", "there", "a", "restaurant", "close", "by", "that", "has", "dancing", "and", "serves", "scallops"], "labels": ["O", "O", "O", "O", "B-Location", "I-Location", "O", "O", "B-Amenity", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Dish, Rating, Restaurant Name, Hours, Amenity, Price, Cuisine and O.\nSentence: is there a restaurant close by that has dancing and serves scallops", "prompt_labels": "is(O) there(O) a(O) restaurant(O) close(B-Location) by(I-Location) that(O) has(O) dancing(B-Amenity) and(O) serves(O) scallops(B-Dish)"}}
{"id": "566", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Price", "Location", "Cuisine", "Amenity", "Restaurant Name", "Dish", "Hours"], "instance": {"id": "566", "words": ["i", "have", "to", "be", "back", "at", "the", "office", "before", "3", "pm", "which", "restaurant", "is", "located", "within", "1", "mile", "from", "here"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Location", "I-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Location, Cuisine, Amenity, Restaurant Name, Dish, Hours and O.\nSentence: i have to be back at the office before 3 pm which restaurant is located within 1 mile from here", "prompt_labels": "i(O) have(O) to(O) be(O) back(O) at(O) the(O) office(O) before(O) 3(O) pm(O) which(O) restaurant(O) is(O) located(O) within(B-Location) 1(I-Location) mile(I-Location) from(I-Location) here(I-Location)"}}
{"id": "922", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Dish", "Location", "Hours", "Rating", "Restaurant Name", "Price", "Cuisine"], "instance": {"id": "922", "words": ["is", "there", "caribbean", "food", "around", "here"], "labels": ["O", "O", "B-Cuisine", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Location, Hours, Rating, Restaurant Name, Price, Cuisine and O.\nSentence: is there caribbean food around here", "prompt_labels": "is(O) there(O) caribbean(B-Cuisine) food(O) around(B-Location) here(I-Location)"}}
{"id": "129", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Price", "Amenity", "Rating", "Hours", "Cuisine", "Dish", "Restaurant Name"], "instance": {"id": "129", "words": ["can", "i", "see", "hamburger", "restaurants", "nearby"], "labels": ["O", "O", "O", "B-Dish", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Amenity, Rating, Hours, Cuisine, Dish, Restaurant Name and O.\nSentence: can i see hamburger restaurants nearby", "prompt_labels": "can(O) i(O) see(O) hamburger(B-Dish) restaurants(O) nearby(B-Location)"}}
{"id": "66", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Location", "Rating", "Dish", "Amenity", "Cuisine", "Hours", "Restaurant Name"], "instance": {"id": "66", "words": ["are", "there", "any", "places", "near", "by", "that", "sell", "hamburgers", "and", "pizza"], "labels": ["O", "O", "O", "O", "B-Location", "I-Location", "O", "O", "B-Dish", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Rating, Dish, Amenity, Cuisine, Hours, Restaurant Name and O.\nSentence: are there any places near by that sell hamburgers and pizza", "prompt_labels": "are(O) there(O) any(O) places(O) near(B-Location) by(I-Location) that(O) sell(O) hamburgers(B-Dish) and(O) pizza(B-Dish)"}}
{"id": "1130", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Location", "Amenity", "Cuisine", "Price", "Dish", "Hours", "Rating"], "instance": {"id": "1130", "words": ["what", "is", "the", "dress", "code", "for", "eating", "at", "planet", "hollywood"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Amenity, Cuisine, Price, Dish, Hours, Rating and O.\nSentence: what is the dress code for eating at planet hollywood", "prompt_labels": "what(O) is(O) the(O) dress(O) code(O) for(O) eating(O) at(O) planet(B-Restaurant Name) hollywood(I-Restaurant Name)"}}
{"id": "305", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Rating", "Location", "Amenity", "Restaurant Name", "Cuisine", "Hours", "Dish"], "instance": {"id": "305", "words": ["does", "target", "have", "their", "own", "parking", "spot", "is", "it", "good", "for", "bringing", "my", "date", "there"], "labels": ["O", "O", "O", "O", "O", "B-Amenity", "I-Amenity", "O", "O", "B-Rating", "O", "B-Location", "O", "B-Amenity", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Location, Amenity, Restaurant Name, Cuisine, Hours, Dish and O.\nSentence: does target have their own parking spot is it good for bringing my date there", "prompt_labels": "does(O) target(O) have(O) their(O) own(O) parking(B-Amenity) spot(I-Amenity) is(O) it(O) good(B-Rating) for(O) bringing(B-Location) my(O) date(B-Amenity) there(O)"}}
{"id": "527", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Hours", "Cuisine", "Rating", "Location", "Price", "Dish", "Restaurant Name"], "instance": {"id": "527", "words": ["how", "many", "burger", "kings", "are", "nearby"], "labels": ["O", "O", "B-Restaurant Name", "I-Restaurant Name", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Cuisine, Rating, Location, Price, Dish, Restaurant Name and O.\nSentence: how many burger kings are nearby", "prompt_labels": "how(O) many(O) burger(B-Restaurant Name) kings(I-Restaurant Name) are(O) nearby(B-Location)"}}
{"id": "1134", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Rating", "Amenity", "Dish", "Restaurant Name", "Cuisine", "Location", "Hours"], "instance": {"id": "1134", "words": ["what", "is", "the", "favorite", "type", "of", "food", "people", "eat", "out", "here", "and", "where", "can", "i", "get", "it"], "labels": ["O", "O", "O", "B-Rating", "O", "O", "O", "O", "O", "B-Location", "I-Location", "O", "O", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Amenity, Dish, Restaurant Name, Cuisine, Location, Hours and O.\nSentence: what is the favorite type of food people eat out here and where can i get it", "prompt_labels": "what(O) is(O) the(O) favorite(B-Rating) type(O) of(O) food(O) people(O) eat(O) out(B-Location) here(I-Location) and(O) where(O) can(O) i(O) get(O) it(O)"}}
{"id": "481", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Dish", "Restaurant Name", "Cuisine", "Location", "Price", "Rating", "Amenity"], "instance": {"id": "481", "words": ["help", "me", "find", "a", "place", "for", "fast", "food"], "labels": ["O", "O", "O", "O", "O", "O", "B-Cuisine", "I-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Dish, Restaurant Name, Cuisine, Location, Price, Rating, Amenity and O.\nSentence: help me find a place for fast food", "prompt_labels": "help(O) me(O) find(O) a(O) place(O) for(O) fast(B-Cuisine) food(I-Cuisine)"}}
{"id": "222", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Dish", "Hours", "Rating", "Price", "Cuisine", "Restaurant Name", "Location"], "instance": {"id": "222", "words": ["could", "you", "locate", "the", "closest", "seafood", "restaurant"], "labels": ["O", "O", "O", "O", "B-Location", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Hours, Rating, Price, Cuisine, Restaurant Name, Location and O.\nSentence: could you locate the closest seafood restaurant", "prompt_labels": "could(O) you(O) locate(O) the(O) closest(B-Location) seafood(B-Cuisine) restaurant(O)"}}
{"id": "1090", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Rating", "Restaurant Name", "Location", "Dish", "Cuisine", "Price", "Amenity"], "instance": {"id": "1090", "words": ["what", "are", "some", "fine", "dining", "restaurants", "in", "the", "area"], "labels": ["O", "O", "O", "B-Amenity", "I-Amenity", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Rating, Restaurant Name, Location, Dish, Cuisine, Price, Amenity and O.\nSentence: what are some fine dining restaurants in the area", "prompt_labels": "what(O) are(O) some(O) fine(B-Amenity) dining(I-Amenity) restaurants(O) in(B-Location) the(I-Location) area(I-Location)"}}
{"id": "1100", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Price", "Rating", "Cuisine", "Amenity", "Dish", "Restaurant Name", "Hours"], "instance": {"id": "1100", "words": ["what", "are", "the", "prices", "like", "at", "gregorys", "restaurant"], "labels": ["O", "O", "O", "O", "O", "O", "B-Restaurant Name", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Rating, Cuisine, Amenity, Dish, Restaurant Name, Hours and O.\nSentence: what are the prices like at gregorys restaurant", "prompt_labels": "what(O) are(O) the(O) prices(O) like(O) at(O) gregorys(B-Restaurant Name) restaurant(O)"}}
{"id": "456", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Rating", "Restaurant Name", "Dish", "Cuisine", "Price", "Location", "Amenity"], "instance": {"id": "456", "words": ["find", "us", "a", "deli", "near", "central", "park"], "labels": ["O", "O", "O", "B-Cuisine", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Rating, Restaurant Name, Dish, Cuisine, Price, Location, Amenity and O.\nSentence: find us a deli near central park", "prompt_labels": "find(O) us(O) a(O) deli(B-Cuisine) near(B-Location) central(I-Location) park(I-Location)"}}
{"id": "833", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Location", "Amenity", "Cuisine", "Price", "Rating", "Dish", "Hours"], "instance": {"id": "833", "words": ["is", "there", "a", "mcdonalds", "within", "two", "miles", "of", "here"], "labels": ["O", "O", "O", "B-Restaurant Name", "B-Location", "I-Location", "I-Location", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Amenity, Cuisine, Price, Rating, Dish, Hours and O.\nSentence: is there a mcdonalds within two miles of here", "prompt_labels": "is(O) there(O) a(O) mcdonalds(B-Restaurant Name) within(B-Location) two(I-Location) miles(I-Location) of(O) here(O)"}}
{"id": "798", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Location", "Dish", "Hours", "Cuisine", "Price", "Amenity", "Rating"], "instance": {"id": "798", "words": ["is", "there", "a", "bar", "that", "stays", "open", "after", "2", "am", "that", "is", "within", "a", "5", "minute", "distance"], "labels": ["O", "O", "O", "B-Cuisine", "O", "O", "B-Hours", "I-Hours", "I-Hours", "I-Hours", "O", "O", "B-Location", "I-Location", "I-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Dish, Hours, Cuisine, Price, Amenity, Rating and O.\nSentence: is there a bar that stays open after 2 am that is within a 5 minute distance", "prompt_labels": "is(O) there(O) a(O) bar(B-Cuisine) that(O) stays(O) open(B-Hours) after(I-Hours) 2(I-Hours) am(I-Hours) that(O) is(O) within(B-Location) a(I-Location) 5(I-Location) minute(I-Location) distance(I-Location)"}}
{"id": "879", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Restaurant Name", "Location", "Cuisine", "Amenity", "Price", "Dish", "Rating"], "instance": {"id": "879", "words": ["is", "there", "a", "thai", "restaurant", "with", "a", "great", "wine", "list"], "labels": ["O", "O", "O", "B-Cuisine", "O", "O", "O", "B-Rating", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Location, Cuisine, Amenity, Price, Dish, Rating and O.\nSentence: is there a thai restaurant with a great wine list", "prompt_labels": "is(O) there(O) a(O) thai(B-Cuisine) restaurant(O) with(O) a(O) great(B-Rating) wine(B-Amenity) list(I-Amenity)"}}
{"id": "1322", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Price", "Location", "Rating", "Restaurant Name", "Hours", "Amenity", "Cuisine"], "instance": {"id": "1322", "words": ["where", "can", "i", "get", "barbecue", "no", "farther", "than", "5", "miles"], "labels": ["O", "O", "O", "O", "B-Cuisine", "O", "O", "O", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Location, Rating, Restaurant Name, Hours, Amenity, Cuisine and O.\nSentence: where can i get barbecue no farther than 5 miles", "prompt_labels": "where(O) can(O) i(O) get(O) barbecue(B-Cuisine) no(O) farther(O) than(O) 5(B-Location) miles(I-Location)"}}
{"id": "336", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Amenity", "Dish", "Location", "Price", "Rating", "Hours", "Cuisine"], "instance": {"id": "336", "words": ["find", "a", "chinese", "restaurant", "that", "will", "take", "american", "express"], "labels": ["O", "O", "B-Cuisine", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Dish, Location, Price, Rating, Hours, Cuisine and O.\nSentence: find a chinese restaurant that will take american express", "prompt_labels": "find(O) a(O) chinese(B-Cuisine) restaurant(O) that(O) will(B-Amenity) take(I-Amenity) american(I-Amenity) express(I-Amenity)"}}
{"id": "58", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Location", "Price", "Rating", "Amenity", "Cuisine", "Hours", "Dish"], "instance": {"id": "58", "words": ["are", "there", "any", "mcdonalds", "that", "i", "can", "drive", "too", "in", "3", "minutes"], "labels": ["O", "O", "O", "B-Restaurant Name", "O", "O", "O", "O", "O", "B-Location", "I-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Price, Rating, Amenity, Cuisine, Hours, Dish and O.\nSentence: are there any mcdonalds that i can drive too in 3 minutes", "prompt_labels": "are(O) there(O) any(O) mcdonalds(B-Restaurant Name) that(O) i(O) can(O) drive(O) too(O) in(B-Location) 3(I-Location) minutes(I-Location)"}}
{"id": "522", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Restaurant Name", "Price", "Location", "Hours", "Rating", "Cuisine", "Dish"], "instance": {"id": "522", "words": ["how", "late", "does", "mcdonalds", "serve", "breakfast"], "labels": ["O", "O", "O", "B-Restaurant Name", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Restaurant Name, Price, Location, Hours, Rating, Cuisine, Dish and O.\nSentence: how late does mcdonalds serve breakfast", "prompt_labels": "how(O) late(O) does(O) mcdonalds(B-Restaurant Name) serve(O) breakfast(O)"}}
{"id": "543", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Restaurant Name", "Location", "Price", "Cuisine", "Rating", "Dish", "Amenity"], "instance": {"id": "543", "words": ["i", "am", "in", "the", "mood", "for", "shrimp", "where", "is", "the", "closet", "place", "i", "can", "go"], "labels": ["O", "O", "O", "O", "O", "O", "B-Dish", "O", "O", "O", "B-Location", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Location, Price, Cuisine, Rating, Dish, Amenity and O.\nSentence: i am in the mood for shrimp where is the closet place i can go", "prompt_labels": "i(O) am(O) in(O) the(O) mood(O) for(O) shrimp(B-Dish) where(O) is(O) the(O) closet(B-Location) place(O) i(O) can(O) go(O)"}}
{"id": "800", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Hours", "Location", "Rating", "Restaurant Name", "Price", "Cuisine", "Dish"], "instance": {"id": "800", "words": ["is", "there", "a", "bojangles", "in", "laurel"], "labels": ["O", "O", "O", "B-Restaurant Name", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Location, Rating, Restaurant Name, Price, Cuisine, Dish and O.\nSentence: is there a bojangles in laurel", "prompt_labels": "is(O) there(O) a(O) bojangles(B-Restaurant Name) in(O) laurel(B-Location)"}}
{"id": "289", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Price", "Location", "Cuisine", "Hours", "Restaurant Name", "Amenity", "Dish"], "instance": {"id": "289", "words": ["does", "mcdonalds", "serve", "ice", "cream", "during", "breakfast", "hours"], "labels": ["O", "B-Restaurant Name", "O", "B-Dish", "I-Dish", "I-Dish", "B-Hours", "I-Hours"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Location, Cuisine, Hours, Restaurant Name, Amenity, Dish and O.\nSentence: does mcdonalds serve ice cream during breakfast hours", "prompt_labels": "does(O) mcdonalds(B-Restaurant Name) serve(O) ice(B-Dish) cream(I-Dish) during(I-Dish) breakfast(B-Hours) hours(I-Hours)"}}
{"id": "885", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Price", "Hours", "Location", "Dish", "Cuisine", "Rating", "Restaurant Name"], "instance": {"id": "885", "words": ["is", "there", "an", "all", "asia", "cafe", "nearby"], "labels": ["O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Hours, Location, Dish, Cuisine, Rating, Restaurant Name and O.\nSentence: is there an all asia cafe nearby", "prompt_labels": "is(O) there(O) an(O) all(B-Restaurant Name) asia(I-Restaurant Name) cafe(I-Restaurant Name) nearby(B-Location)"}}
{"id": "180", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Location", "Rating", "Dish", "Amenity", "Cuisine", "Hours", "Price"], "instance": {"id": "180", "words": ["can", "you", "give", "me", "the", "phone", "number", "for", "the", "nearest", "mexican", "restaurant", "that", "is", "both", "affordable", "and", "has", "good", "quality", "food"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Location", "B-Cuisine", "O", "O", "O", "O", "B-Price", "O", "O", "B-Rating", "I-Rating", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Rating, Dish, Amenity, Cuisine, Hours, Price and O.\nSentence: can you give me the phone number for the nearest mexican restaurant that is both affordable and has good quality food", "prompt_labels": "can(O) you(O) give(O) me(O) the(O) phone(O) number(O) for(O) the(O) nearest(B-Location) mexican(B-Cuisine) restaurant(O) that(O) is(O) both(O) affordable(B-Price) and(O) has(O) good(B-Rating) quality(I-Rating) food(O)"}}
{"id": "263", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Price", "Amenity", "Restaurant Name", "Cuisine", "Location", "Hours", "Rating"], "instance": {"id": "263", "words": ["do", "you", "know", "if", "there", "are", "any", "reviews", "on", "monacos"], "labels": ["O", "O", "O", "O", "O", "O", "O", "O", "O", "B-Restaurant Name"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Amenity, Restaurant Name, Cuisine, Location, Hours, Rating and O.\nSentence: do you know if there are any reviews on monacos", "prompt_labels": "do(O) you(O) know(O) if(O) there(O) are(O) any(O) reviews(O) on(O) monacos(B-Restaurant Name)"}}
{"id": "145", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Rating", "Price", "Dish", "Hours", "Amenity", "Location", "Cuisine"], "instance": {"id": "145", "words": ["can", "you", "find", "a", "steak", "house", "that", "serves", "wine"], "labels": ["O", "O", "O", "O", "B-Cuisine", "I-Cuisine", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Price, Dish, Hours, Amenity, Location, Cuisine and O.\nSentence: can you find a steak house that serves wine", "prompt_labels": "can(O) you(O) find(O) a(O) steak(B-Cuisine) house(I-Cuisine) that(O) serves(O) wine(B-Dish)"}}
{"id": "805", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Cuisine", "Restaurant Name", "Amenity", "Dish", "Price", "Rating", "Location"], "instance": {"id": "805", "words": ["is", "there", "a", "cheap", "vegetarian", "restaurant", "nearby"], "labels": ["O", "O", "O", "B-Price", "B-Cuisine", "O", "B-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Restaurant Name, Amenity, Dish, Price, Rating, Location and O.\nSentence: is there a cheap vegetarian restaurant nearby", "prompt_labels": "is(O) there(O) a(O) cheap(B-Price) vegetarian(B-Cuisine) restaurant(O) nearby(B-Location)"}}
{"id": "17", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Restaurant Name", "Price", "Rating", "Location", "Cuisine", "Amenity", "Dish", "Hours"], "instance": {"id": "17", "words": ["are", "children", "allowed", "in", "this", "particular", "sitting", "area"], "labels": ["O", "B-Amenity", "O", "O", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Price, Rating, Location, Cuisine, Amenity, Dish, Hours and O.\nSentence: are children allowed in this particular sitting area", "prompt_labels": "are(O) children(B-Amenity) allowed(O) in(O) this(O) particular(O) sitting(B-Amenity) area(I-Amenity)"}}
{"id": "1127", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Restaurant Name", "Location", "Hours", "Price", "Rating", "Cuisine", "Amenity"], "instance": {"id": "1127", "words": ["what", "is", "the", "closest", "wendys", "to", "me"], "labels": ["O", "O", "O", "B-Location", "B-Restaurant Name", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Restaurant Name, Location, Hours, Price, Rating, Cuisine, Amenity and O.\nSentence: what is the closest wendys to me", "prompt_labels": "what(O) is(O) the(O) closest(B-Location) wendys(B-Restaurant Name) to(O) me(O)"}}
{"id": "1346", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Hours", "Restaurant Name", "Dish", "Rating", "Amenity", "Price", "Location"], "instance": {"id": "1346", "words": ["where", "can", "i", "get", "starbucks", "around", "me"], "labels": ["O", "O", "O", "O", "B-Restaurant Name", "B-Location", "I-Location"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Restaurant Name, Dish, Rating, Amenity, Price, Location and O.\nSentence: where can i get starbucks around me", "prompt_labels": "where(O) can(O) i(O) get(O) starbucks(B-Restaurant Name) around(B-Location) me(I-Location)"}}
{"id": "19", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Location", "Amenity", "Rating", "Dish", "Restaurant Name", "Hours", "Price", "Cuisine"], "instance": {"id": "19", "words": ["are", "the", "portion", "at", "le", "bec", "fin", "large", "or", "very", "small"], "labels": ["O", "O", "O", "O", "B-Restaurant Name", "I-Restaurant Name", "I-Restaurant Name", "O", "O", "O", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Rating, Dish, Restaurant Name, Hours, Price, Cuisine and O.\nSentence: are the portion at le bec fin large or very small", "prompt_labels": "are(O) the(O) portion(O) at(O) le(B-Restaurant Name) bec(I-Restaurant Name) fin(I-Restaurant Name) large(O) or(O) very(O) small(O)"}}
{"id": "1343", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Cuisine", "Hours", "Dish", "Location", "Restaurant Name", "Price", "Amenity"], "instance": {"id": "1343", "words": ["where", "can", "i", "get", "some", "sushi"], "labels": ["O", "O", "O", "O", "O", "B-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Hours, Dish, Location, Restaurant Name, Price, Amenity and O.\nSentence: where can i get some sushi", "prompt_labels": "where(O) can(O) i(O) get(O) some(O) sushi(B-Dish)"}}
{"id": "1484", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Price", "Restaurant Name", "Amenity", "Location", "Cuisine", "Hours", "Rating", "Dish"], "instance": {"id": "1484", "words": ["which", "local", "restaurant", "serves", "only", "seafood"], "labels": ["O", "B-Location", "O", "O", "B-Amenity", "B-Cuisine"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Restaurant Name, Amenity, Location, Cuisine, Hours, Rating, Dish and O.\nSentence: which local restaurant serves only seafood", "prompt_labels": "which(O) local(B-Location) restaurant(O) serves(O) only(B-Amenity) seafood(B-Cuisine)"}}
{"id": "585", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Amenity", "Hours", "Rating", "Restaurant Name", "Dish", "Price", "Cuisine", "Location"], "instance": {"id": "585", "words": ["i", "need", "a", "reservation", "for", "12", "at", "6", "pm", "at", "the", "nearest", "asian", "restaurant"], "labels": ["O", "O", "O", "O", "O", "O", "O", "B-Hours", "I-Hours", "O", "O", "B-Location", "B-Cuisine", "O"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Rating, Restaurant Name, Dish, Price, Cuisine, Location and O.\nSentence: i need a reservation for 12 at 6 pm at the nearest asian restaurant", "prompt_labels": "i(O) need(O) a(O) reservation(O) for(O) 12(O) at(O) 6(B-Hours) pm(I-Hours) at(O) the(O) nearest(B-Location) asian(B-Cuisine) restaurant(O)"}}
{"id": "762", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Amenity", "Location", "Restaurant Name", "Dish", "Price", "Cuisine", "Hours"], "instance": {"id": "762", "words": ["is", "azita", "restaurant", "a", "date", "spot"], "labels": ["O", "B-Restaurant Name", "I-Restaurant Name", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Location, Restaurant Name, Dish, Price, Cuisine, Hours and O.\nSentence: is azita restaurant a date spot", "prompt_labels": "is(O) azita(B-Restaurant Name) restaurant(I-Restaurant Name) a(O) date(B-Amenity) spot(I-Amenity)"}}
{"id": "1353", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Hours", "Amenity", "Dish", "Location", "Price", "Rating", "Cuisine", "Restaurant Name"], "instance": {"id": "1353", "words": ["where", "can", "i", "have", "a", "glass", "of", "wine", "in", "jamaica", "plain", "that", "also", "does", "not", "allow", "smoking"], "labels": ["O", "O", "O", "O", "O", "B-Rating", "O", "B-Cuisine", "O", "B-Location", "I-Location", "O", "O", "B-Amenity", "I-Amenity", "I-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Dish, Location, Price, Rating, Cuisine, Restaurant Name and O.\nSentence: where can i have a glass of wine in jamaica plain that also does not allow smoking", "prompt_labels": "where(O) can(O) i(O) have(O) a(O) glass(B-Rating) of(O) wine(B-Cuisine) in(O) jamaica(B-Location) plain(I-Location) that(O) also(O) does(B-Amenity) not(I-Amenity) allow(I-Amenity) smoking(I-Amenity)"}}
{"id": "1443", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Dish", "Location", "Hours", "Price", "Amenity", "Rating", "Restaurant Name", "Cuisine"], "instance": {"id": "1443", "words": ["where", "is", "the", "nearest", "restaurant", "that", "serves", "hush", "puppies"], "labels": ["O", "O", "O", "B-Location", "O", "O", "O", "B-Dish", "I-Dish"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Hours, Price, Amenity, Rating, Restaurant Name, Cuisine and O.\nSentence: where is the nearest restaurant that serves hush puppies", "prompt_labels": "where(O) is(O) the(O) nearest(B-Location) restaurant(O) that(O) serves(O) hush(B-Dish) puppies(I-Dish)"}}
{"id": "115", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Cuisine", "Amenity", "Location", "Price", "Hours", "Restaurant Name", "Rating", "Dish"], "instance": {"id": "115", "words": ["can", "i", "bring", "my", "kid", "to", "any", "of", "the", "restaurants", "down", "town", "that", "have", "bars", "attached"], "labels": ["O", "O", "O", "B-Amenity", "I-Amenity", "O", "O", "O", "O", "O", "B-Location", "I-Location", "O", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Amenity, Location, Price, Hours, Restaurant Name, Rating, Dish and O.\nSentence: can i bring my kid to any of the restaurants down town that have bars attached", "prompt_labels": "can(O) i(O) bring(O) my(B-Amenity) kid(I-Amenity) to(O) any(O) of(O) the(O) restaurants(O) down(B-Location) town(I-Location) that(O) have(O) bars(B-Amenity) attached(I-Amenity)"}}
{"id": "307", "dataset": "mit-restaurant", "split": "dev", "label_list": ["Rating", "Cuisine", "Hours", "Amenity", "Price", "Restaurant Name", "Dish", "Location"], "instance": {"id": "307", "words": ["does", "tgi", "fridays", "have", "senior", "discounts"], "labels": ["O", "B-Restaurant Name", "I-Restaurant Name", "O", "B-Amenity", "I-Amenity"], "instruction_inputs": "Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Hours, Amenity, Price, Restaurant Name, Dish, Location and O.\nSentence: does tgi fridays have senior discounts", "prompt_labels": "does(O) tgi(B-Restaurant Name) fridays(I-Restaurant Name) have(O) senior(B-Amenity) discounts(I-Amenity)"}}
