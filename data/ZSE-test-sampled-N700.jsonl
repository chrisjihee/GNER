{"id":"16","dataset":"mit-movie","split":"test","instance":{"id":"16","prompt_labels":"what(O) was(O) the(O) best(B-review) rated(O) stanley(B-director) kubrick(I-director) film(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, character, song, rating, actor, director, year, title, review, genre, trailer, plot and O.\nSentence: what was the best rated stanley kubrick film","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","best","rated","stanley","kubrick","film"],"labels":["O","O","O","B-review","O","B-director","I-director","O"],"target_index":null,"target_label":null},"label_list":["average_ratings","character","song","rating","actor","director","year","title","review","genre","trailer","plot"]}
{"id":"18","dataset":"mit-movie","split":"test","instance":{"id":"18","prompt_labels":"list(O) pg(B-rating) rated(O) movies(O) about(O) cars(B-plot) released(O) in(O) the(O) 1990s(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, character, genre, rating, plot, actor, director, average ratings, year, review, title, trailer and O.\nSentence: list pg rated movies about cars released in the 1990s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","pg","rated","movies","about","cars","released","in","the","1990s"],"labels":["O","B-rating","O","O","O","B-plot","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["song","character","genre","rating","plot","actor","director","average_ratings","year","review","title","trailer"]}
{"id":"30","dataset":"mit-movie","split":"test","instance":{"id":"30","prompt_labels":"what(O) movies(O) made(O) in(O) 2004(B-year) were(O) pg(B-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, song, average ratings, year, director, actor, title, plot, trailer, review, character, genre and O.\nSentence: what movies made in 2004 were pg","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movies","made","in","2004","were","pg"],"labels":["O","O","O","O","B-year","O","B-rating"],"target_index":null,"target_label":null},"label_list":["rating","song","average_ratings","year","director","actor","title","plot","trailer","review","character","genre"]}
{"id":"33","dataset":"mit-movie","split":"test","instance":{"id":"33","prompt_labels":"list(O) the(O) science(B-genre) fiction(I-genre) movies(O) directed(O) by(O) shawn(B-director) levy(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, title, year, plot, actor, song, average ratings, trailer, director, rating, genre, character and O.\nSentence: list the science fiction movies directed by shawn levy","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","the","science","fiction","movies","directed","by","shawn","levy"],"labels":["O","O","B-genre","I-genre","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["review","title","year","plot","actor","song","average_ratings","trailer","director","rating","genre","character"]}
{"id":"48","dataset":"mit-movie","split":"test","instance":{"id":"48","prompt_labels":"show(O) me(O) movies(O) about(O) horse(B-plot) racing(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, genre, actor, rating, year, average ratings, review, title, song, trailer, plot, character and O.\nSentence: show me movies about horse racing","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","movies","about","horse","racing"],"labels":["O","O","O","O","B-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["director","genre","actor","rating","year","average_ratings","review","title","song","trailer","plot","character"]}
{"id":"56","dataset":"mit-movie","split":"test","instance":{"id":"56","prompt_labels":"what(O) are(O) top(B-average ratings) 50(I-average ratings) movies(O) of(O) all(B-average ratings) time(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, genre, character, trailer, rating, plot, review, actor, song, year, average ratings, director and O.\nSentence: what are top 50 movies of all time","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","top","50","movies","of","all","time"],"labels":["O","O","B-average ratings","I-average ratings","O","O","B-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["title","genre","character","trailer","rating","plot","review","actor","song","year","average_ratings","director"]}
{"id":"66","dataset":"mit-movie","split":"test","instance":{"id":"66","prompt_labels":"show(O) me(O) the(O) collection(O) of(O) action(B-genre) movies(I-genre) of(O) arnold(B-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, director, average ratings, trailer, year, genre, title, song, rating, review, actor, plot and O.\nSentence: show me the collection of action movies of arnold","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","the","collection","of","action","movies","of","arnold"],"labels":["O","O","O","O","O","B-genre","I-genre","O","B-actor"],"target_index":null,"target_label":null},"label_list":["character","director","average_ratings","trailer","year","genre","title","song","rating","review","actor","plot"]}
{"id":"86","dataset":"mit-movie","split":"test","instance":{"id":"86","prompt_labels":"what(O) movie(O) did(O) rod(B-director) serling(I-director) write(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, genre, director, trailer, title, review, average ratings, year, character, song, actor, plot and O.\nSentence: what movie did rod serling write","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movie","did","rod","serling","write"],"labels":["O","O","O","B-director","I-director","O"],"target_index":null,"target_label":null},"label_list":["rating","genre","director","trailer","title","review","average_ratings","year","character","song","actor","plot"]}
{"id":"90","dataset":"mit-movie","split":"test","instance":{"id":"90","prompt_labels":"find(O) me(O) the(O) name(O) of(O) the(O) actor(O) that(O) played(O) v(B-character) in(O) v(B-title) for(I-title) vendetta(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, plot, actor, year, title, character, genre, song, rating, review, trailer, average ratings and O.\nSentence: find me the name of the actor that played v in v for vendetta","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","the","name","of","the","actor","that","played","v","in","v","for","vendetta"],"labels":["O","O","O","O","O","O","O","O","O","B-character","O","B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["director","plot","actor","year","title","character","genre","song","rating","review","trailer","average_ratings"]}
{"id":"174","dataset":"mit-movie","split":"test","instance":{"id":"174","prompt_labels":"is(O) there(O) a(O) comedy(B-genre) crime(I-genre) drama(I-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, trailer, year, average ratings, song, director, review, actor, genre, title, plot, character and O.\nSentence: is there a comedy crime drama","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","comedy","crime","drama"],"labels":["O","O","O","B-genre","I-genre","I-genre"],"target_index":null,"target_label":null},"label_list":["rating","trailer","year","average_ratings","song","director","review","actor","genre","title","plot","character"]}
{"id":"177","dataset":"mit-movie","split":"test","instance":{"id":"177","prompt_labels":"what(O) are(O) the(O) best(B-review) werewolf(B-plot) movies(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, song, character, genre, plot, rating, review, director, trailer, average ratings, actor, title and O.\nSentence: what are the best werewolf movies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","the","best","werewolf","movies"],"labels":["O","O","O","B-review","B-plot","O"],"target_index":null,"target_label":null},"label_list":["year","song","character","genre","plot","rating","review","director","trailer","average_ratings","actor","title"]}
{"id":"184","dataset":"mit-movie","split":"test","instance":{"id":"184","prompt_labels":"whats(O) a(O) john(B-director) huston(I-director) flick(O) from(O) the(O) 1970s(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, trailer, character, song, genre, rating, actor, plot, review, title, year, director and O.\nSentence: whats a john huston flick from the 1970s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["whats","a","john","huston","flick","from","the","1970s"],"labels":["O","O","B-director","I-director","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["average_ratings","trailer","character","song","genre","rating","actor","plot","review","title","year","director"]}
{"id":"209","dataset":"mit-movie","split":"test","instance":{"id":"209","prompt_labels":"i(B-genre) want(I-genre) a(I-genre) 1960s(I-genre) zombie(I-genre) flick(I-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, year, average ratings, character, song, review, actor, rating, plot, genre, title, trailer and O.\nSentence: i want a 1960s zombie flick","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","a","1960s","zombie","flick"],"labels":["B-genre","I-genre","I-genre","I-genre","I-genre","I-genre"],"target_index":null,"target_label":null},"label_list":["director","year","average_ratings","character","song","review","actor","rating","plot","genre","title","trailer"]}
{"id":"219","dataset":"mit-movie","split":"test","instance":{"id":"219","prompt_labels":"what(O) was(O) the(O) film(O) about(O) brandon(B-character) teena(I-character)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, trailer, plot, year, character, review, rating, genre, song, actor, title, director and O.\nSentence: what was the film about brandon teena","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","film","about","brandon","teena"],"labels":["O","O","O","O","O","B-character","I-character"],"target_index":null,"target_label":null},"label_list":["average_ratings","trailer","plot","year","character","review","rating","genre","song","actor","title","director"]}
{"id":"261","dataset":"mit-movie","split":"test","instance":{"id":"261","prompt_labels":"which(O) harry(B-title) potter(I-title) movies(O) have(O) robert(B-actor) pattinson(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, review, title, actor, genre, trailer, plot, character, song, year, average ratings, rating and O.\nSentence: which harry potter movies have robert pattinson","prediction_output":null,"prediction_outputs":null,"group":null,"words":["which","harry","potter","movies","have","robert","pattinson"],"labels":["O","B-title","I-title","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["director","review","title","actor","genre","trailer","plot","character","song","year","average_ratings","rating"]}
{"id":"274","dataset":"mit-movie","split":"test","instance":{"id":"274","prompt_labels":"show(O) me(O) the(O) half(B-title) baked(I-title) cover(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, title, director, plot, average ratings, song, actor, trailer, review, character, genre, year and O.\nSentence: show me the half baked cover","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","the","half","baked","cover"],"labels":["O","O","O","B-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["rating","title","director","plot","average_ratings","song","actor","trailer","review","character","genre","year"]}
{"id":"301","dataset":"mit-movie","split":"test","instance":{"id":"301","prompt_labels":"what(O) city(O) was(O) the(O) famous(O) car(B-plot) chase(I-plot) from(O) the(O) french(B-title) connection(I-title) in(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, average ratings, song, review, plot, director, title, year, character, genre, trailer, actor and O.\nSentence: what city was the famous car chase from the french connection in","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","city","was","the","famous","car","chase","from","the","french","connection","in"],"labels":["O","O","O","O","O","B-plot","I-plot","O","O","B-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["rating","average_ratings","song","review","plot","director","title","year","character","genre","trailer","actor"]}
{"id":"320","dataset":"mit-movie","split":"test","instance":{"id":"320","prompt_labels":"show(O) me(O) a(O) list(O) of(O) comedies(B-genre) from(O) the(O) 1970s(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, character, title, actor, trailer, year, plot, song, director, genre, rating, review and O.\nSentence: show me a list of comedies from the 1970s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","a","list","of","comedies","from","the","1970s"],"labels":["O","O","O","O","O","B-genre","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["average_ratings","character","title","actor","trailer","year","plot","song","director","genre","rating","review"]}
{"id":"376","dataset":"mit-movie","split":"test","instance":{"id":"376","prompt_labels":"list(O) uma(B-actor) thurman(I-actor) movies(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, average ratings, year, plot, song, actor, trailer, title, character, director, rating, review and O.\nSentence: list uma thurman movies","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","uma","thurman","movies"],"labels":["O","B-actor","I-actor","O"],"target_index":null,"target_label":null},"label_list":["genre","average_ratings","year","plot","song","actor","trailer","title","character","director","rating","review"]}
{"id":"379","dataset":"mit-movie","split":"test","instance":{"id":"379","prompt_labels":"list(O) all(O) science(B-genre) fiction(I-genre) movies(O) playing(O) the(O) next(O) 12(O) hours(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, review, song, year, character, plot, trailer, actor, genre, average ratings, director, rating and O.\nSentence: list all science fiction movies playing the next 12 hours","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","all","science","fiction","movies","playing","the","next","12","hours"],"labels":["O","O","B-genre","I-genre","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["title","review","song","year","character","plot","trailer","actor","genre","average_ratings","director","rating"]}
{"id":"397","dataset":"mit-movie","split":"test","instance":{"id":"397","prompt_labels":"what(O) family(B-genre) movie(I-genre) got(O) the(O) best(B-average ratings) rating(I-average ratings) from(O) kids(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, trailer, title, rating, year, song, genre, plot, director, character, review, actor and O.\nSentence: what family movie got the best rating from kids","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","family","movie","got","the","best","rating","from","kids"],"labels":["O","B-genre","I-genre","O","O","B-average ratings","I-average ratings","O","O"],"target_index":null,"target_label":null},"label_list":["average_ratings","trailer","title","rating","year","song","genre","plot","director","character","review","actor"]}
{"id":"422","dataset":"mit-movie","split":"test","instance":{"id":"422","prompt_labels":"what(O) character(O) did(O) michael(B-actor) j(I-actor) fox(I-actor) voice(O) in(O) homeward(B-title) bound(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, title, year, review, song, plot, character, genre, average ratings, actor, trailer, rating and O.\nSentence: what character did michael j fox voice in homeward bound","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","character","did","michael","j","fox","voice","in","homeward","bound"],"labels":["O","O","O","B-actor","I-actor","I-actor","O","O","B-title","I-title"],"target_index":null,"target_label":null},"label_list":["director","title","year","review","song","plot","character","genre","average_ratings","actor","trailer","rating"]}
{"id":"426","dataset":"mit-movie","split":"test","instance":{"id":"426","prompt_labels":"i(O) want(O) a(O) jamie(B-actor) lee(I-actor) curtis(I-actor) horror(B-genre) film(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, director, year, average ratings, plot, character, actor, rating, review, title, song, genre and O.\nSentence: i want a jamie lee curtis horror film","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","a","jamie","lee","curtis","horror","film"],"labels":["O","O","O","B-actor","I-actor","I-actor","B-genre","O"],"target_index":null,"target_label":null},"label_list":["trailer","director","year","average_ratings","plot","character","actor","rating","review","title","song","genre"]}
{"id":"446","dataset":"mit-movie","split":"test","instance":{"id":"446","prompt_labels":"who(O) directed(O) blow(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, character, rating, trailer, plot, actor, genre, title, review, song, average ratings and O.\nSentence: who directed blow","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","directed","blow"],"labels":["O","O","B-title"],"target_index":null,"target_label":null},"label_list":["year","director","character","rating","trailer","plot","actor","genre","title","review","song","average_ratings"]}
{"id":"449","dataset":"mit-movie","split":"test","instance":{"id":"449","prompt_labels":"what(O) horror(B-genre) movies(O) came(O) out(O) in(O) the(O) 80s(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, review, plot, average ratings, rating, actor, year, genre, song, character, title, director and O.\nSentence: what horror movies came out in the 80s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","horror","movies","came","out","in","the","80s"],"labels":["O","B-genre","O","O","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["trailer","review","plot","average_ratings","rating","actor","year","genre","song","character","title","director"]}
{"id":"583","dataset":"mit-movie","split":"test","instance":{"id":"583","prompt_labels":"what(O) movie(O) has(O) a(O) great(B-plot) car(I-plot) chase(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, plot, actor, song, year, average ratings, character, rating, review, director, genre, trailer and O.\nSentence: what movie has a great car chase","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movie","has","a","great","car","chase"],"labels":["O","O","O","O","B-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["title","plot","actor","song","year","average_ratings","character","rating","review","director","genre","trailer"]}
{"id":"589","dataset":"mit-movie","split":"test","instance":{"id":"589","prompt_labels":"which(O) films(O) have(O) sherlock(B-character) holmes(I-character) as(O) a(O) character(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, plot, actor, character, title, genre, rating, director, song, average ratings, year, review and O.\nSentence: which films have sherlock holmes as a character","prediction_output":null,"prediction_outputs":null,"group":null,"words":["which","films","have","sherlock","holmes","as","a","character"],"labels":["O","O","O","B-character","I-character","O","O","O"],"target_index":null,"target_label":null},"label_list":["trailer","plot","actor","character","title","genre","rating","director","song","average_ratings","year","review"]}
{"id":"628","dataset":"mit-movie","split":"test","instance":{"id":"628","prompt_labels":"is(O) there(O) a(O) color(B-genre) slapstick(I-genre) comedy(I-genre) movie(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, song, genre, average ratings, plot, rating, actor, character, trailer, year, review, title and O.\nSentence: is there a color slapstick comedy movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","color","slapstick","comedy","movie"],"labels":["O","O","O","B-genre","I-genre","I-genre","O"],"target_index":null,"target_label":null},"label_list":["director","song","genre","average_ratings","plot","rating","actor","character","trailer","year","review","title"]}
{"id":"633","dataset":"mit-movie","split":"test","instance":{"id":"633","prompt_labels":"show(O) me(O) all(O) the(O) movies(O) zac(B-actor) efron(I-actor) has(O) been(O) in(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, average ratings, review, year, director, genre, rating, plot, title, character, trailer, actor and O.\nSentence: show me all the movies zac efron has been in","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","all","the","movies","zac","efron","has","been","in"],"labels":["O","O","O","O","O","B-actor","I-actor","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","average_ratings","review","year","director","genre","rating","plot","title","character","trailer","actor"]}
{"id":"636","dataset":"mit-movie","split":"test","instance":{"id":"636","prompt_labels":"name(O) a(O) paul(B-director) williams(I-director) musical(B-genre) film(I-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, title, character, rating, song, genre, plot, average ratings, actor, year, review, trailer and O.\nSentence: name a paul williams musical film","prediction_output":null,"prediction_outputs":null,"group":null,"words":["name","a","paul","williams","musical","film"],"labels":["O","O","B-director","I-director","B-genre","I-genre"],"target_index":null,"target_label":null},"label_list":["director","title","character","rating","song","genre","plot","average_ratings","actor","year","review","trailer"]}
{"id":"650","dataset":"mit-movie","split":"test","instance":{"id":"650","prompt_labels":"find(O) a(O) movie(O) with(O) jude(B-actor) law(I-actor) and(O) julia(B-actor) roberts(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, review, director, character, genre, trailer, year, song, rating, average ratings, plot, title and O.\nSentence: find a movie with jude law and julia roberts","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","a","movie","with","jude","law","and","julia","roberts"],"labels":["O","O","O","O","B-actor","I-actor","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["actor","review","director","character","genre","trailer","year","song","rating","average_ratings","plot","title"]}
{"id":"674","dataset":"mit-movie","split":"test","instance":{"id":"674","prompt_labels":"list(O) movies(O) directed(O) by(O) peter(B-director) jackson(I-director) in(O) the(O) 2000s(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, rating, trailer, year, average ratings, genre, plot, actor, title, song, review, director and O.\nSentence: list movies directed by peter jackson in the 2000s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","movies","directed","by","peter","jackson","in","the","2000s"],"labels":["O","O","O","O","B-director","I-director","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["character","rating","trailer","year","average_ratings","genre","plot","actor","title","song","review","director"]}
{"id":"687","dataset":"mit-movie","split":"test","instance":{"id":"687","prompt_labels":"did(O) steven(B-director) soderbergh(I-director) direct(O) any(O) movies(O) released(O) in(O) 2010(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, genre, trailer, plot, average ratings, review, rating, director, character, title, song, year and O.\nSentence: did steven soderbergh direct any movies released in 2010","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","steven","soderbergh","direct","any","movies","released","in","2010"],"labels":["O","B-director","I-director","O","O","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["actor","genre","trailer","plot","average_ratings","review","rating","director","character","title","song","year"]}
{"id":"729","dataset":"mit-movie","split":"test","instance":{"id":"729","prompt_labels":"are(O) there(O) any(O) movies(B-plot) based(I-plot) on(I-plot) british(I-plot) versions(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, song, review, plot, genre, title, average ratings, year, character, rating, trailer, director and O.\nSentence: are there any movies based on british versions","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","movies","based","on","british","versions"],"labels":["O","O","O","B-plot","I-plot","I-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["actor","song","review","plot","genre","title","average_ratings","year","character","rating","trailer","director"]}
{"id":"756","dataset":"mit-movie","split":"test","instance":{"id":"756","prompt_labels":"show(O) me(O) movies(B-plot) about(I-plot) the(I-plot) military(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, average ratings, character, trailer, plot, year, title, director, review, song, rating, genre and O.\nSentence: show me movies about the military","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","movies","about","the","military"],"labels":["O","O","B-plot","I-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["actor","average_ratings","character","trailer","plot","year","title","director","review","song","rating","genre"]}
{"id":"810","dataset":"mit-movie","split":"test","instance":{"id":"810","prompt_labels":"find(O) me(O) the(O) action(B-genre) movie(O) seven(B-title) starring(O) brad(B-actor) pitt(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, director, character, review, rating, plot, average ratings, song, genre, actor, trailer, year and O.\nSentence: find me the action movie seven starring brad pitt","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","the","action","movie","seven","starring","brad","pitt"],"labels":["O","O","O","B-genre","O","B-title","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["title","director","character","review","rating","plot","average_ratings","song","genre","actor","trailer","year"]}
{"id":"820","dataset":"mit-movie","split":"test","instance":{"id":"820","prompt_labels":"show(B-plot) me(I-plot) the(I-plot) movie(I-plot) about(I-plot) strange(I-plot) men(I-plot) who(I-plot) repossess(I-plot) cars(I-plot) starring(O) emilio(B-actor) estevez(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, rating, year, actor, review, plot, song, trailer, title, director, average ratings, genre and O.\nSentence: show me the movie about strange men who repossess cars starring emilio estevez","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","the","movie","about","strange","men","who","repossess","cars","starring","emilio","estevez"],"labels":["B-plot","I-plot","I-plot","I-plot","I-plot","I-plot","I-plot","I-plot","I-plot","I-plot","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["character","rating","year","actor","review","plot","song","trailer","title","director","average_ratings","genre"]}
{"id":"912","dataset":"mit-movie","split":"test","instance":{"id":"912","prompt_labels":"show(O) me(O) arnold(B-actor) schwartzenegger(I-actor) movies(O) with(O) robots(B-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, title, song, year, director, genre, rating, plot, average ratings, character, trailer, review and O.\nSentence: show me arnold schwartzenegger movies with robots","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","arnold","schwartzenegger","movies","with","robots"],"labels":["O","O","B-actor","I-actor","O","O","B-plot"],"target_index":null,"target_label":null},"label_list":["actor","title","song","year","director","genre","rating","plot","average_ratings","character","trailer","review"]}
{"id":"974","dataset":"mit-movie","split":"test","instance":{"id":"974","prompt_labels":"did(O) russel(B-actor) crowe(I-actor) appear(O) in(O) a(O) movie(O) about(B-plot) a(I-plot) mathematician(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, review, plot, title, character, song, trailer, average ratings, genre, year, actor, rating and O.\nSentence: did russel crowe appear in a movie about a mathematician","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","russel","crowe","appear","in","a","movie","about","a","mathematician"],"labels":["O","B-actor","I-actor","O","O","O","O","B-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["director","review","plot","title","character","song","trailer","average_ratings","genre","year","actor","rating"]}
{"id":"989","dataset":"mit-movie","split":"test","instance":{"id":"989","prompt_labels":"find(O) me(O) a(O) well(B-average ratings) reviewed(I-average ratings) comedy(B-genre) starring(O) john(B-actor) candy(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, review, trailer, rating, director, character, average ratings, year, actor, title, genre, song and O.\nSentence: find me a well reviewed comedy starring john candy","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","well","reviewed","comedy","starring","john","candy"],"labels":["O","O","O","B-average ratings","I-average ratings","B-genre","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["plot","review","trailer","rating","director","character","average_ratings","year","actor","title","genre","song"]}
{"id":"998","dataset":"mit-movie","split":"test","instance":{"id":"998","prompt_labels":"show(O) me(O) a(O) childrens(B-genre) movie(I-genre) by(O) disney(O) with(O) a(O) character(O) named(O) simba(B-character)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, year, director, genre, song, plot, review, character, average ratings, title, rating, actor and O.\nSentence: show me a childrens movie by disney with a character named simba","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","a","childrens","movie","by","disney","with","a","character","named","simba"],"labels":["O","O","O","B-genre","I-genre","O","O","O","O","O","O","B-character"],"target_index":null,"target_label":null},"label_list":["trailer","year","director","genre","song","plot","review","character","average_ratings","title","rating","actor"]}
{"id":"1012","dataset":"mit-movie","split":"test","instance":{"id":"1012","prompt_labels":"what(B-plot) is(I-plot) the(I-plot) plot(I-plot) of(I-plot) happy(I-plot) gilmore(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, year, trailer, song, actor, rating, character, title, plot, genre, review, average ratings and O.\nSentence: what is the plot of happy gilmore","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","plot","of","happy","gilmore"],"labels":["B-plot","I-plot","I-plot","I-plot","I-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["director","year","trailer","song","actor","rating","character","title","plot","genre","review","average_ratings"]}
{"id":"1042","dataset":"mit-movie","split":"test","instance":{"id":"1042","prompt_labels":"which(O) movie(O) features(O) jack(B-actor) blacks(I-actor) voice(B-plot) as(I-plot) a(I-plot) panda(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, trailer, average ratings, character, plot, rating, review, year, director, title, genre, actor and O.\nSentence: which movie features jack blacks voice as a panda","prediction_output":null,"prediction_outputs":null,"group":null,"words":["which","movie","features","jack","blacks","voice","as","a","panda"],"labels":["O","O","O","B-actor","I-actor","B-plot","I-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["song","trailer","average_ratings","character","plot","rating","review","year","director","title","genre","actor"]}
{"id":"1059","dataset":"mit-movie","split":"test","instance":{"id":"1059","prompt_labels":"a(O) list(O) of(O) sci(B-genre) fi(I-genre) movies(O) in(O) 2010(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, plot, genre, song, review, rating, director, average ratings, year, trailer, character, actor and O.\nSentence: a list of sci fi movies in 2010","prediction_output":null,"prediction_outputs":null,"group":null,"words":["a","list","of","sci","fi","movies","in","2010"],"labels":["O","O","O","B-genre","I-genre","O","O","O"],"target_index":null,"target_label":null},"label_list":["title","plot","genre","song","review","rating","director","average_ratings","year","trailer","character","actor"]}
{"id":"1071","dataset":"mit-movie","split":"test","instance":{"id":"1071","prompt_labels":"any(O) good(O) harrison(B-actor) ford(I-actor) thrillers(B-genre) released(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, character, actor, rating, title, average ratings, song, genre, plot, director, year, trailer and O.\nSentence: any good harrison ford thrillers released","prediction_output":null,"prediction_outputs":null,"group":null,"words":["any","good","harrison","ford","thrillers","released"],"labels":["O","O","B-actor","I-actor","B-genre","O"],"target_index":null,"target_label":null},"label_list":["review","character","actor","rating","title","average_ratings","song","genre","plot","director","year","trailer"]}
{"id":"1123","dataset":"mit-movie","split":"test","instance":{"id":"1123","prompt_labels":"dating(B-plot) 1980(B-year) s(I-year) movie(O) that(O) was(O) rated(O) pg(B-rating) 13(I-rating) and(O) was(O) rated(O) four(B-average ratings) stars(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, review, average ratings, trailer, genre, year, actor, rating, plot, title, director, character and O.\nSentence: dating 1980 s movie that was rated pg 13 and was rated four stars","prediction_output":null,"prediction_outputs":null,"group":null,"words":["dating","1980","s","movie","that","was","rated","pg","13","and","was","rated","four","stars"],"labels":["B-plot","B-year","I-year","O","O","O","O","B-rating","I-rating","O","O","O","B-average ratings","O"],"target_index":null,"target_label":null},"label_list":["song","review","average_ratings","trailer","genre","year","actor","rating","plot","title","director","character"]}
{"id":"1126","dataset":"mit-movie","split":"test","instance":{"id":"1126","prompt_labels":"did(O) alfred(B-director) hitchcock(I-director) ever(O) direct(O) a(O) documentary(B-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, title, genre, rating, song, year, director, review, plot, average ratings, trailer, actor and O.\nSentence: did alfred hitchcock ever direct a documentary","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","alfred","hitchcock","ever","direct","a","documentary"],"labels":["O","B-director","I-director","O","O","O","B-genre"],"target_index":null,"target_label":null},"label_list":["character","title","genre","rating","song","year","director","review","plot","average_ratings","trailer","actor"]}
{"id":"1130","dataset":"mit-movie","split":"test","instance":{"id":"1130","prompt_labels":"did(O) clint(B-director) eastwood(I-director) direct(O) inception(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, genre, average ratings, character, title, plot, song, trailer, year, rating, review, director and O.\nSentence: did clint eastwood direct inception","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","clint","eastwood","direct","inception"],"labels":["O","B-director","I-director","O","B-title"],"target_index":null,"target_label":null},"label_list":["actor","genre","average_ratings","character","title","plot","song","trailer","year","rating","review","director"]}
{"id":"1144","dataset":"mit-movie","split":"test","instance":{"id":"1144","prompt_labels":"did(O) hal(B-director) mohr(I-director) direct(O) a(O) movie(O) about(O) gags(B-genre) in(O) the(O) 1950(B-year) s(I-year) that(O) was(O) given(O) six(B-average ratings) stars(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, genre, song, director, average ratings, actor, review, plot, title, trailer, character, year and O.\nSentence: did hal mohr direct a movie about gags in the 1950 s that was given six stars","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","hal","mohr","direct","a","movie","about","gags","in","the","1950","s","that","was","given","six","stars"],"labels":["O","B-director","I-director","O","O","O","O","B-genre","O","O","B-year","I-year","O","O","O","B-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["rating","genre","song","director","average_ratings","actor","review","plot","title","trailer","character","year"]}
{"id":"1154","dataset":"mit-movie","split":"test","instance":{"id":"1154","prompt_labels":"did(O) nick(B-director) stagliano(I-director) direct(O) any(O) pg(B-rating) 13(I-rating) mystery(B-genre) movies(O) in(O) the(O) 1980(B-year) s(I-year) rated(O) around(O) eight(B-average ratings) stars(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, trailer, director, plot, average ratings, genre, review, song, character, title, year, actor and O.\nSentence: did nick stagliano direct any pg 13 mystery movies in the 1980 s rated around eight stars","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","nick","stagliano","direct","any","pg","13","mystery","movies","in","the","1980","s","rated","around","eight","stars"],"labels":["O","B-director","I-director","O","O","B-rating","I-rating","B-genre","O","O","O","B-year","I-year","O","O","B-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["rating","trailer","director","plot","average_ratings","genre","review","song","character","title","year","actor"]}
{"id":"1168","dataset":"mit-movie","split":"test","instance":{"id":"1168","prompt_labels":"did(O) stanley(B-director) kubrick(I-director) ever(O) direct(O) any(O) romantic(B-genre) comedy(I-genre) films(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, rating, plot, title, actor, review, average ratings, year, genre, song, character, director and O.\nSentence: did stanley kubrick ever direct any romantic comedy films","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","stanley","kubrick","ever","direct","any","romantic","comedy","films"],"labels":["O","B-director","I-director","O","O","O","B-genre","I-genre","O"],"target_index":null,"target_label":null},"label_list":["trailer","rating","plot","title","actor","review","average_ratings","year","genre","song","character","director"]}
{"id":"1178","dataset":"mit-movie","split":"test","instance":{"id":"1178","prompt_labels":"did(O) tom(B-actor) jones(I-actor) starring(O) in(O) pg(B-rating) 13(I-rating) film(O) centers(O) on(O) human(B-plot) in(O) the(O) past(B-year) eight(I-year) years(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, average ratings, title, trailer, song, genre, review, plot, year, character, rating, actor and O.\nSentence: did tom jones starring in pg 13 film centers on human in the past eight years","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","tom","jones","starring","in","pg","13","film","centers","on","human","in","the","past","eight","years"],"labels":["O","B-actor","I-actor","O","O","B-rating","I-rating","O","O","O","B-plot","O","O","B-year","I-year","I-year"],"target_index":null,"target_label":null},"label_list":["director","average_ratings","title","trailer","song","genre","review","plot","year","character","rating","actor"]}
{"id":"1191","dataset":"mit-movie","split":"test","instance":{"id":"1191","prompt_labels":"do(O) you(O) have(O) a(O) pg(B-rating) 13(I-rating) history(B-genre) film(O) about(O) a(O) political(B-plot) state(I-plot) that(O) stars(O) elizabeth(B-actor) taylor(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, actor, plot, year, song, trailer, character, rating, review, genre, title, director and O.\nSentence: do you have a pg 13 history film about a political state that stars elizabeth taylor","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","have","a","pg","13","history","film","about","a","political","state","that","stars","elizabeth","taylor"],"labels":["O","O","O","O","B-rating","I-rating","B-genre","O","O","O","B-plot","I-plot","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["average_ratings","actor","plot","year","song","trailer","character","rating","review","genre","title","director"]}
{"id":"1207","dataset":"mit-movie","split":"test","instance":{"id":"1207","prompt_labels":"do(O) you(O) have(O) the(O) film(O) the(B-title) loss(I-title) of(I-title) sexual(I-title) innocence(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, average ratings, director, genre, year, review, plot, character, title, rating, trailer, song and O.\nSentence: do you have the film the loss of sexual innocence","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","have","the","film","the","loss","of","sexual","innocence"],"labels":["O","O","O","O","O","B-title","I-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["actor","average_ratings","director","genre","year","review","plot","character","title","rating","trailer","song"]}
{"id":"1225","dataset":"mit-movie","split":"test","instance":{"id":"1225","prompt_labels":"do(O) you(O) think(O) youd(O) be(O) able(O) to(O) help(O) me(O) find(O) the(B-title) penguins(I-title) of(I-title) madagascar(I-title) operation(I-title) dvd(I-title) premier(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, review, song, rating, actor, trailer, genre, director, average ratings, year, title, plot and O.\nSentence: do you think youd be able to help me find the penguins of madagascar operation dvd premier","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","think","youd","be","able","to","help","me","find","the","penguins","of","madagascar","operation","dvd","premier"],"labels":["O","O","O","O","O","O","O","O","O","O","B-title","I-title","I-title","I-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["character","review","song","rating","actor","trailer","genre","director","average_ratings","year","title","plot"]}
{"id":"1239","dataset":"mit-movie","split":"test","instance":{"id":"1239","prompt_labels":"find(O) a(O) film(O) titled(O) shut(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, plot, genre, character, rating, actor, director, song, year, review, trailer, title and O.\nSentence: find a film titled shut","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","a","film","titled","shut"],"labels":["O","O","O","O","B-title"],"target_index":null,"target_label":null},"label_list":["average_ratings","plot","genre","character","rating","actor","director","song","year","review","trailer","title"]}
{"id":"1241","dataset":"mit-movie","split":"test","instance":{"id":"1241","prompt_labels":"find(O) a(O) military(B-genre) film(O) from(O) the(O) last(B-year) six(I-year) years(I-year) starring(O) stacey(B-actor) dash(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, actor, rating, year, character, average ratings, song, genre, review, plot, title, trailer and O.\nSentence: find a military film from the last six years starring stacey dash","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","a","military","film","from","the","last","six","years","starring","stacey","dash"],"labels":["O","O","B-genre","O","O","O","B-year","I-year","I-year","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["director","actor","rating","year","character","average_ratings","song","genre","review","plot","title","trailer"]}
{"id":"1254","dataset":"mit-movie","split":"test","instance":{"id":"1254","prompt_labels":"had(O) david(B-director) lynch(I-director) ever(O) directed(O) an(O) avant(B-genre) garde(I-genre) movie(O) that(O) is(O) rated(O) r(B-rating)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, plot, average ratings, year, title, trailer, genre, actor, director, review, rating, character and O.\nSentence: had david lynch ever directed an avant garde movie that is rated r","prediction_output":null,"prediction_outputs":null,"group":null,"words":["had","david","lynch","ever","directed","an","avant","garde","movie","that","is","rated","r"],"labels":["O","B-director","I-director","O","O","O","B-genre","I-genre","O","O","O","O","B-rating"],"target_index":null,"target_label":null},"label_list":["song","plot","average_ratings","year","title","trailer","genre","actor","director","review","rating","character"]}
{"id":"1260","dataset":"mit-movie","split":"test","instance":{"id":"1260","prompt_labels":"has(O) david(B-director) denneen(I-director) directed(O) a(O) rated(O) r(B-rating) sci(B-genre) fi(I-genre) on(O) the(O) past(B-year) decade(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, genre, review, character, actor, director, plot, rating, title, trailer, year, average ratings and O.\nSentence: has david denneen directed a rated r sci fi on the past decade","prediction_output":null,"prediction_outputs":null,"group":null,"words":["has","david","denneen","directed","a","rated","r","sci","fi","on","the","past","decade"],"labels":["O","B-director","I-director","O","O","O","B-rating","B-genre","I-genre","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["song","genre","review","character","actor","director","plot","rating","title","trailer","year","average_ratings"]}
{"id":"1320","dataset":"mit-movie","split":"test","instance":{"id":"1320","prompt_labels":"i(O) want(O) a(O) pg(B-rating) 2010(B-year) documentary(B-genre) that(O) was(O) directed(O) by(O) hiromichi(B-director) matano(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, actor, title, song, review, year, character, director, rating, plot, average ratings, genre and O.\nSentence: i want a pg 2010 documentary that was directed by hiromichi matano","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","a","pg","2010","documentary","that","was","directed","by","hiromichi","matano"],"labels":["O","O","O","B-rating","B-year","B-genre","O","O","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["trailer","actor","title","song","review","year","character","director","rating","plot","average_ratings","genre"]}
{"id":"1385","dataset":"mit-movie","split":"test","instance":{"id":"1385","prompt_labels":"im(O) looking(O) for(O) the(O) unrated(B-rating) 1940(B-year) s(I-year) violent(B-genre) movie(O) that(O) was(O) liked(B-average ratings) by(I-average ratings) many(I-average ratings) and(O) starred(O) tom(B-actor) selleck(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, director, title, review, song, average ratings, character, plot, actor, rating, year, genre and O.\nSentence: im looking for the unrated 1940 s violent movie that was liked by many and starred tom selleck","prediction_output":null,"prediction_outputs":null,"group":null,"words":["im","looking","for","the","unrated","1940","s","violent","movie","that","was","liked","by","many","and","starred","tom","selleck"],"labels":["O","O","O","O","B-rating","B-year","I-year","B-genre","O","O","O","B-average ratings","I-average ratings","I-average ratings","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["trailer","director","title","review","song","average_ratings","character","plot","actor","rating","year","genre"]}
{"id":"1402","dataset":"mit-movie","split":"test","instance":{"id":"1402","prompt_labels":"in(O) the(O) 1960(B-year) s(I-year) did(O) cory(B-director) edwards(I-director) direct(O) an(O) unrated(B-rating) sci(B-genre) fi(I-genre) that(O) received(O) an(O) average(O) rating(O) of(O) six(B-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, genre, rating, review, plot, song, title, average ratings, character, actor, trailer and O.\nSentence: in the 1960 s did cory edwards direct an unrated sci fi that received an average rating of six","prediction_output":null,"prediction_outputs":null,"group":null,"words":["in","the","1960","s","did","cory","edwards","direct","an","unrated","sci","fi","that","received","an","average","rating","of","six"],"labels":["O","O","B-year","I-year","O","B-director","I-director","O","O","B-rating","B-genre","I-genre","O","O","O","O","O","O","B-average ratings"],"target_index":null,"target_label":null},"label_list":["year","director","genre","rating","review","plot","song","title","average_ratings","character","actor","trailer"]}
{"id":"1429","dataset":"mit-movie","split":"test","instance":{"id":"1429","prompt_labels":"is(O) billy(B-director) wilder(I-director) the(O) director(O) of(O) inception(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, song, character, year, director, review, genre, trailer, actor, rating, plot, average ratings and O.\nSentence: is billy wilder the director of inception","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","billy","wilder","the","director","of","inception"],"labels":["O","B-director","I-director","O","O","O","B-title"],"target_index":null,"target_label":null},"label_list":["title","song","character","year","director","review","genre","trailer","actor","rating","plot","average_ratings"]}
{"id":"1450","dataset":"mit-movie","split":"test","instance":{"id":"1450","prompt_labels":"is(O) the(O) movie(O) open(B-title) season(I-title) animated(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, review, year, character, actor, director, average ratings, rating, song, genre, trailer, title and O.\nSentence: is the movie open season animated","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","the","movie","open","season","animated"],"labels":["O","O","O","B-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["plot","review","year","character","actor","director","average_ratings","rating","song","genre","trailer","title"]}
{"id":"1468","dataset":"mit-movie","split":"test","instance":{"id":"1468","prompt_labels":"is(O) there(O) a(O) wall(B-title) e(I-title) movie(O) starring(O) robert(B-actor) redford(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, director, average ratings, title, trailer, year, rating, actor, character, song, genre, plot and O.\nSentence: is there a wall e movie starring robert redford","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","wall","e","movie","starring","robert","redford"],"labels":["O","O","O","B-title","I-title","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["review","director","average_ratings","title","trailer","year","rating","actor","character","song","genre","plot"]}
{"id":"1525","dataset":"mit-movie","split":"test","instance":{"id":"1525","prompt_labels":"is(O) there(O) a(O) movie(O) titled(O) supergirl(B-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, average ratings, rating, year, genre, trailer, review, actor, song, title, plot, director and O.\nSentence: is there a movie titled supergirl","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","movie","titled","supergirl"],"labels":["O","O","O","O","O","B-title"],"target_index":null,"target_label":null},"label_list":["character","average_ratings","rating","year","genre","trailer","review","actor","song","title","plot","director"]}
{"id":"1559","dataset":"mit-movie","split":"test","instance":{"id":"1559","prompt_labels":"is(O) there(O) any(O) documentary(B-genre) films(O) about(O) religion(B-plot) that(O) received(O) two(B-average ratings) thumbs(I-average ratings) up(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, year, character, genre, review, actor, title, average ratings, director, trailer, plot, rating and O.\nSentence: is there any documentary films about religion that received two thumbs up","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","any","documentary","films","about","religion","that","received","two","thumbs","up"],"labels":["O","O","O","B-genre","O","O","B-plot","O","O","B-average ratings","I-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["song","year","character","genre","review","actor","title","average_ratings","director","trailer","plot","rating"]}
{"id":"1563","dataset":"mit-movie","split":"test","instance":{"id":"1563","prompt_labels":"is(O) there(O) any(O) good(O) rated(O) r(B-rating) crime(B-genre) movies(O) coming(O) out(O) next(O) month(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, rating, plot, genre, actor, song, year, average ratings, title, review, director, trailer and O.\nSentence: is there any good rated r crime movies coming out next month","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","any","good","rated","r","crime","movies","coming","out","next","month"],"labels":["O","O","O","O","O","B-rating","B-genre","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["character","rating","plot","genre","actor","song","year","average_ratings","title","review","director","trailer"]}
{"id":"1591","dataset":"mit-movie","split":"test","instance":{"id":"1591","prompt_labels":"list(O) a(O) david(B-director) lean(I-director) musical(B-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, director, trailer, character, song, actor, year, rating, plot, genre, review, average ratings and O.\nSentence: list a david lean musical","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","a","david","lean","musical"],"labels":["O","O","B-director","I-director","B-genre"],"target_index":null,"target_label":null},"label_list":["title","director","trailer","character","song","actor","year","rating","plot","genre","review","average_ratings"]}
{"id":"1596","dataset":"mit-movie","split":"test","instance":{"id":"1596","prompt_labels":"list(O) a(O) pg(B-rating) rated(O) fantasy(B-genre) movie(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, director, character, actor, plot, trailer, rating, year, genre, average ratings, review, title and O.\nSentence: list a pg rated fantasy movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","a","pg","rated","fantasy","movie"],"labels":["O","O","B-rating","O","B-genre","O"],"target_index":null,"target_label":null},"label_list":["song","director","character","actor","plot","trailer","rating","year","genre","average_ratings","review","title"]}
{"id":"1602","dataset":"mit-movie","split":"test","instance":{"id":"1602","prompt_labels":"list(O) a(O) wall(B-title) e(I-title) movie(O) which(O) director(O) were(O) the(B-director) coen(I-director) brothers(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: character, genre, review, director, year, plot, average ratings, rating, song, actor, title, trailer and O.\nSentence: list a wall e movie which director were the coen brothers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","a","wall","e","movie","which","director","were","the","coen","brothers"],"labels":["O","O","B-title","I-title","O","O","O","O","B-director","I-director","I-director"],"target_index":null,"target_label":null},"label_list":["character","genre","review","director","year","plot","average_ratings","rating","song","actor","title","trailer"]}
{"id":"1622","dataset":"mit-movie","split":"test","instance":{"id":"1622","prompt_labels":"list(O) a(O) kid(B-genre) film(O) about(O) growing(B-plot) up(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, year, character, review, director, rating, genre, song, trailer, actor, title, plot and O.\nSentence: list a kid film about growing up","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","a","kid","film","about","growing","up"],"labels":["O","O","B-genre","O","O","B-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["average_ratings","year","character","review","director","rating","genre","song","trailer","actor","title","plot"]}
{"id":"1628","dataset":"mit-movie","split":"test","instance":{"id":"1628","prompt_labels":"list(O) a(O) movie(O) starring(O) tom(B-actor) hanks(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, average ratings, song, character, plot, rating, title, actor, trailer, director, genre, year and O.\nSentence: list a movie starring tom hanks","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","a","movie","starring","tom","hanks"],"labels":["O","O","O","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["review","average_ratings","song","character","plot","rating","title","actor","trailer","director","genre","year"]}
{"id":"1641","dataset":"mit-movie","split":"test","instance":{"id":"1641","prompt_labels":"list(O) a(O) short(B-genre) film(O) last(B-year) year(I-year) with(O) sissy(B-actor) spacek(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, song, genre, character, average ratings, actor, director, year, rating, plot, review, title and O.\nSentence: list a short film last year with sissy spacek","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","a","short","film","last","year","with","sissy","spacek"],"labels":["O","O","B-genre","O","B-year","I-year","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["trailer","song","genre","character","average_ratings","actor","director","year","rating","plot","review","title"]}
{"id":"1659","dataset":"mit-movie","split":"test","instance":{"id":"1659","prompt_labels":"list(O) all(O) horror(B-genre) movies(O) directed(O) by(O) amos(B-director) kollek(I-director) in(O) the(O) past(B-year) five(I-year) decades(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, actor, plot, director, genre, rating, average ratings, character, review, song, year, trailer and O.\nSentence: list all horror movies directed by amos kollek in the past five decades","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","all","horror","movies","directed","by","amos","kollek","in","the","past","five","decades"],"labels":["O","O","B-genre","O","O","O","B-director","I-director","O","O","B-year","I-year","I-year"],"target_index":null,"target_label":null},"label_list":["title","actor","plot","director","genre","rating","average_ratings","character","review","song","year","trailer"]}
{"id":"1677","dataset":"mit-movie","split":"test","instance":{"id":"1677","prompt_labels":"list(O) an(O) unrated(B-rating) adventure(B-genre) from(O) the(O) past(B-year) ten(I-year) decades(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, average ratings, genre, plot, trailer, song, review, actor, year, director, character, rating and O.\nSentence: list an unrated adventure from the past ten decades","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","an","unrated","adventure","from","the","past","ten","decades"],"labels":["O","O","B-rating","B-genre","O","O","B-year","I-year","I-year"],"target_index":null,"target_label":null},"label_list":["title","average_ratings","genre","plot","trailer","song","review","actor","year","director","character","rating"]}
{"id":"1699","dataset":"mit-movie","split":"test","instance":{"id":"1699","prompt_labels":"list(O) some(O) movies(O) featuring(O) graphic(B-title) sexual(I-title) horror(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, character, genre, rating, review, average ratings, director, trailer, title, plot, actor, year and O.\nSentence: list some movies featuring graphic sexual horror","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","some","movies","featuring","graphic","sexual","horror"],"labels":["O","O","O","O","B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["song","character","genre","rating","review","average_ratings","director","trailer","title","plot","actor","year"]}
{"id":"1701","dataset":"mit-movie","split":"test","instance":{"id":"1701","prompt_labels":"list(O) the(O) g(B-rating) rated(O) films(O) with(O) susan(B-actor) hargrove(I-actor) from(O) the(O) 1960(B-year) s(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, year, average ratings, trailer, director, song, review, rating, actor, genre, title, character and O.\nSentence: list the g rated films with susan hargrove from the 1960 s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","the","g","rated","films","with","susan","hargrove","from","the","1960","s"],"labels":["O","O","B-rating","O","O","O","B-actor","I-actor","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["plot","year","average_ratings","trailer","director","song","review","rating","actor","genre","title","character"]}
{"id":"1708","dataset":"mit-movie","split":"test","instance":{"id":"1708","prompt_labels":"look(O) up(O) the(O) movie(O) title(O) i(B-title) love(I-title) hong(I-title) kong(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: rating, trailer, plot, character, average ratings, genre, review, actor, song, title, year, director and O.\nSentence: look up the movie title i love hong kong","prediction_output":null,"prediction_outputs":null,"group":null,"words":["look","up","the","movie","title","i","love","hong","kong"],"labels":["O","O","O","O","O","B-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["rating","trailer","plot","character","average_ratings","genre","review","actor","song","title","year","director"]}
{"id":"1731","dataset":"mit-movie","split":"test","instance":{"id":"1731","prompt_labels":"name(O) a(O) mystery(B-genre) with(O) a(O) serial(B-plot) killer(I-plot) that(O) was(O) rated(O) seven(B-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, rating, year, song, trailer, review, plot, character, director, actor, title, genre and O.\nSentence: name a mystery with a serial killer that was rated seven","prediction_output":null,"prediction_outputs":null,"group":null,"words":["name","a","mystery","with","a","serial","killer","that","was","rated","seven"],"labels":["O","O","B-genre","O","O","B-plot","I-plot","O","O","O","B-average ratings"],"target_index":null,"target_label":null},"label_list":["average_ratings","rating","year","song","trailer","review","plot","character","director","actor","title","genre"]}
{"id":"1760","dataset":"mit-movie","split":"test","instance":{"id":"1760","prompt_labels":"show(O) me(O) all(O) action(B-genre) films(O) that(O) were(O) rated(O) very(B-average ratings) good(I-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, trailer, character, director, genre, actor, title, average ratings, plot, year, review, rating and O.\nSentence: show me all action films that were rated very good","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","all","action","films","that","were","rated","very","good"],"labels":["O","O","O","B-genre","O","O","O","O","B-average ratings","I-average ratings"],"target_index":null,"target_label":null},"label_list":["song","trailer","character","director","genre","actor","title","average_ratings","plot","year","review","rating"]}
{"id":"1774","dataset":"mit-movie","split":"test","instance":{"id":"1774","prompt_labels":"the(B-title) tree(I-title) came(O) out(O) when(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: average ratings, director, genre, plot, year, rating, trailer, character, song, title, review, actor and O.\nSentence: the tree came out when","prediction_output":null,"prediction_outputs":null,"group":null,"words":["the","tree","came","out","when"],"labels":["B-title","I-title","O","O","O"],"target_index":null,"target_label":null},"label_list":["average_ratings","director","genre","plot","year","rating","trailer","character","song","title","review","actor"]}
{"id":"1867","dataset":"mit-movie","split":"test","instance":{"id":"1867","prompt_labels":"what(O) r(B-rating) rated(O) suspense(B-genre) movies(O) came(O) out(O) in(O) 1990(B-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, title, plot, year, average ratings, actor, genre, song, character, director, rating, review and O.\nSentence: what r rated suspense movies came out in 1990","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","r","rated","suspense","movies","came","out","in","1990"],"labels":["O","B-rating","O","B-genre","O","O","O","O","B-year"],"target_index":null,"target_label":null},"label_list":["trailer","title","plot","year","average_ratings","actor","genre","song","character","director","rating","review"]}
{"id":"1924","dataset":"mit-movie","split":"test","instance":{"id":"1924","prompt_labels":"what(O) are(O) the(O) movies(O) released(O) the(O) last(B-year) eight(I-year) years(I-year) with(O) james(B-actor) dean(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, genre, average ratings, song, year, character, rating, plot, trailer, title, review, director and O.\nSentence: what are the movies released the last eight years with james dean","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","the","movies","released","the","last","eight","years","with","james","dean"],"labels":["O","O","O","O","O","O","B-year","I-year","I-year","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["actor","genre","average_ratings","song","year","character","rating","plot","trailer","title","review","director"]}
{"id":"1932","dataset":"mit-movie","split":"test","instance":{"id":"1932","prompt_labels":"what(O) are(O) the(O) titles(O) of(O) some(O) science(B-genre) fiction(I-genre) movies(O) that(O) were(O) produced(O) in(O) the(O) 2000(B-year) s(I-year)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, director, rating, song, average ratings, plot, actor, title, character, genre, review, trailer and O.\nSentence: what are the titles of some science fiction movies that were produced in the 2000 s","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","are","the","titles","of","some","science","fiction","movies","that","were","produced","in","the","2000","s"],"labels":["O","O","O","O","O","O","B-genre","I-genre","O","O","O","O","O","O","B-year","I-year"],"target_index":null,"target_label":null},"label_list":["year","director","rating","song","average_ratings","plot","actor","title","character","genre","review","trailer"]}
{"id":"1949","dataset":"mit-movie","split":"test","instance":{"id":"1949","prompt_labels":"what(O) exactly(O) is(O) the(O) ubaldo(B-title) terzani(I-title) horror(I-title) show(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, song, character, year, trailer, plot, title, actor, review, rating, genre, average ratings and O.\nSentence: what exactly is the ubaldo terzani horror show","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","exactly","is","the","ubaldo","terzani","horror","show"],"labels":["O","O","O","O","B-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["director","song","character","year","trailer","plot","title","actor","review","rating","genre","average_ratings"]}
{"id":"1953","dataset":"mit-movie","split":"test","instance":{"id":"1953","prompt_labels":"what(O) fantasy(B-genre) movies(O) were(O) released(O) in(O) 2011(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, character, actor, director, title, genre, review, trailer, average ratings, plot, rating, song and O.\nSentence: what fantasy movies were released in 2011","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","fantasy","movies","were","released","in","2011"],"labels":["O","B-genre","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["year","character","actor","director","title","genre","review","trailer","average_ratings","plot","rating","song"]}
{"id":"1965","dataset":"mit-movie","split":"test","instance":{"id":"1965","prompt_labels":"what(O) good(O) romantic(B-genre) comedy(I-genre) can(O) i(O) watch(O) that(O) is(O) about(O) star(B-plot) crossed(I-plot) lovers(I-plot)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: plot, director, title, song, genre, actor, review, rating, year, trailer, average ratings, character and O.\nSentence: what good romantic comedy can i watch that is about star crossed lovers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","good","romantic","comedy","can","i","watch","that","is","about","star","crossed","lovers"],"labels":["O","O","B-genre","I-genre","O","O","O","O","O","O","B-plot","I-plot","I-plot"],"target_index":null,"target_label":null},"label_list":["plot","director","title","song","genre","actor","review","rating","year","trailer","average_ratings","character"]}
{"id":"1983","dataset":"mit-movie","split":"test","instance":{"id":"1983","prompt_labels":"what(O) is(O) the(B-title) governess(I-title) about(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, trailer, song, director, actor, character, review, average ratings, rating, plot, year, title and O.\nSentence: what is the governess about","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","governess","about"],"labels":["O","O","B-title","I-title","O"],"target_index":null,"target_label":null},"label_list":["genre","trailer","song","director","actor","character","review","average_ratings","rating","plot","year","title"]}
{"id":"2060","dataset":"mit-movie","split":"test","instance":{"id":"2060","prompt_labels":"what(O) is(O) the(O) fists(B-title) of(I-title) fury(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, rating, actor, review, trailer, plot, average ratings, song, character, title, genre, year and O.\nSentence: what is the fists of fury","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","fists","of","fury"],"labels":["O","O","O","B-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["director","rating","actor","review","trailer","plot","average_ratings","song","character","title","genre","year"]}
{"id":"2088","dataset":"mit-movie","split":"test","instance":{"id":"2088","prompt_labels":"what(O) is(O) the(O) movie(O) with(O) a(O) seven(B-average ratings) star(I-average ratings) rating(O) from(O) 2000(B-year) starring(O) jennifer(B-actor) connelly(I-actor) with(O) a(O) pg(B-rating) 13(I-rating) rating(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, character, director, review, year, trailer, average ratings, rating, song, actor, plot, title and O.\nSentence: what is the movie with a seven star rating from 2000 starring jennifer connelly with a pg 13 rating","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","movie","with","a","seven","star","rating","from","2000","starring","jennifer","connelly","with","a","pg","13","rating"],"labels":["O","O","O","O","O","O","B-average ratings","I-average ratings","O","O","B-year","O","B-actor","I-actor","O","O","B-rating","I-rating","O"],"target_index":null,"target_label":null},"label_list":["genre","character","director","review","year","trailer","average_ratings","rating","song","actor","plot","title"]}
{"id":"2134","dataset":"mit-movie","split":"test","instance":{"id":"2134","prompt_labels":"what(O) movie(O) could(O) be(O) classified(O) as(O) a(O) mystery(B-genre)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: trailer, plot, year, rating, character, average ratings, actor, review, song, title, genre, director and O.\nSentence: what movie could be classified as a mystery","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movie","could","be","classified","as","a","mystery"],"labels":["O","O","O","O","O","O","O","B-genre"],"target_index":null,"target_label":null},"label_list":["trailer","plot","year","rating","character","average_ratings","actor","review","song","title","genre","director"]}
{"id":"2140","dataset":"mit-movie","split":"test","instance":{"id":"2140","prompt_labels":"what(O) movie(O) is(O) known(O) as(O) oliver(B-director) stones(I-director) best(O) work(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, review, average ratings, character, song, trailer, genre, director, title, actor, plot, rating and O.\nSentence: what movie is known as oliver stones best work","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","movie","is","known","as","oliver","stones","best","work"],"labels":["O","O","O","O","O","B-director","I-director","O","O"],"target_index":null,"target_label":null},"label_list":["year","review","average_ratings","character","song","trailer","genre","director","title","actor","plot","rating"]}
{"id":"2205","dataset":"mit-movie","split":"test","instance":{"id":"2205","prompt_labels":"what(O) was(O) a(O) nc(B-rating) 17(I-rating) rated(O) documentary(B-genre) directed(O) by(O) michael(B-director) spierig(I-director)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, average ratings, director, plot, actor, review, character, year, trailer, rating, title, song and O.\nSentence: what was a nc 17 rated documentary directed by michael spierig","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","a","nc","17","rated","documentary","directed","by","michael","spierig"],"labels":["O","O","O","B-rating","I-rating","O","B-genre","O","O","B-director","I-director"],"target_index":null,"target_label":null},"label_list":["genre","average_ratings","director","plot","actor","review","character","year","trailer","rating","title","song"]}
{"id":"2228","dataset":"mit-movie","split":"test","instance":{"id":"2228","prompt_labels":"what(O) was(O) the(O) name(O) of(O) the(O) nc(B-rating) 17(I-rating) kathrine(B-director) windfeld(I-director) movie(O) about(O) companionship(B-plot) that(O) has(O) a(O) ratings(O) average(O) of(O) nine(B-average ratings)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: actor, trailer, rating, title, plot, director, average ratings, song, review, year, character, genre and O.\nSentence: what was the name of the nc 17 kathrine windfeld movie about companionship that has a ratings average of nine","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","was","the","name","of","the","nc","17","kathrine","windfeld","movie","about","companionship","that","has","a","ratings","average","of","nine"],"labels":["O","O","O","O","O","O","B-rating","I-rating","B-director","I-director","O","O","B-plot","O","O","O","O","O","O","B-average ratings"],"target_index":null,"target_label":null},"label_list":["actor","trailer","rating","title","plot","director","average_ratings","song","review","year","character","genre"]}
{"id":"2264","dataset":"mit-movie","split":"test","instance":{"id":"2264","prompt_labels":"when(O) did(O) war(B-title) gods(I-title) of(I-title) the(I-title) deep(I-title) come(O) out(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: director, rating, trailer, actor, character, year, genre, average ratings, plot, title, review, song and O.\nSentence: when did war gods of the deep come out","prediction_output":null,"prediction_outputs":null,"group":null,"words":["when","did","war","gods","of","the","deep","come","out"],"labels":["O","O","B-title","I-title","I-title","I-title","I-title","O","O"],"target_index":null,"target_label":null},"label_list":["director","rating","trailer","actor","character","year","genre","average_ratings","plot","title","review","song"]}
{"id":"2280","dataset":"mit-movie","split":"test","instance":{"id":"2280","prompt_labels":"where(O) can(O) i(O) watch(O) big(B-title) deal(I-title) on(I-title) madonna(I-title) street(I-title) subbed(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: review, average ratings, rating, trailer, year, genre, actor, title, plot, character, song, director and O.\nSentence: where can i watch big deal on madonna street subbed","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","watch","big","deal","on","madonna","street","subbed"],"labels":["O","O","O","O","B-title","I-title","I-title","I-title","I-title","I-title"],"target_index":null,"target_label":null},"label_list":["review","average_ratings","rating","trailer","year","genre","actor","title","plot","character","song","director"]}
{"id":"2293","dataset":"mit-movie","split":"test","instance":{"id":"2293","prompt_labels":"which(O) drama(B-genre) movie(O) released(O) in(O) the(O) past(B-year) decade(I-year) starred(O) phil(B-actor) hartman(I-actor)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: year, trailer, plot, title, character, actor, rating, director, review, genre, song, average ratings and O.\nSentence: which drama movie released in the past decade starred phil hartman","prediction_output":null,"prediction_outputs":null,"group":null,"words":["which","drama","movie","released","in","the","past","decade","starred","phil","hartman"],"labels":["O","B-genre","O","O","O","O","B-year","I-year","O","B-actor","I-actor"],"target_index":null,"target_label":null},"label_list":["year","trailer","plot","title","character","actor","rating","director","review","genre","song","average_ratings"]}
{"id":"2348","dataset":"mit-movie","split":"test","instance":{"id":"2348","prompt_labels":"did(O) steven(B-director) soderbergh(I-director) direct(O) a(O) film(B-genre) noir(I-genre) movie(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: title, director, rating, review, actor, year, song, genre, trailer, average ratings, character, plot and O.\nSentence: did steven soderbergh direct a film noir movie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["did","steven","soderbergh","direct","a","film","noir","movie"],"labels":["O","B-director","I-director","O","O","B-genre","I-genre","O"],"target_index":null,"target_label":null},"label_list":["title","director","rating","review","actor","year","song","genre","trailer","average_ratings","character","plot"]}
{"id":"2426","dataset":"mit-movie","split":"test","instance":{"id":"2426","prompt_labels":"broken(B-title) blossoms(I-title)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: genre, actor, trailer, year, character, song, review, rating, plot, title, average ratings, director and O.\nSentence: broken blossoms","prediction_output":null,"prediction_outputs":null,"group":null,"words":["broken","blossoms"],"labels":["B-title","I-title"],"target_index":null,"target_label":null},"label_list":["genre","actor","trailer","year","character","song","review","rating","plot","title","average_ratings","director"]}
{"id":"20","dataset":"mit-restaurant","split":"test","instance":{"id":"20","prompt_labels":"are(O) there(O) any(O) 24(B-Hours) hour(I-Hours) breakfast(B-Cuisine) places(O) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Dish, Hours, Location, Cuisine, Restaurant Name, Price and O.\nSentence: are there any 24 hour breakfast places nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","24","hour","breakfast","places","nearby"],"labels":["O","O","O","B-Hours","I-Hours","B-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["rating","amenity","dish","hours","location","cuisine","restaurant_name","price"]}
{"id":"40","dataset":"mit-restaurant","split":"test","instance":{"id":"40","prompt_labels":"are(O) there(O) any(O) five(B-Rating) star(I-Rating) restaurants(O) around(B-Location) here(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Price, Dish, Cuisine, Restaurant Name, Amenity, Rating, Location and O.\nSentence: are there any five star restaurants around here","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","five","star","restaurants","around","here"],"labels":["O","O","O","B-Rating","I-Rating","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["hours","price","dish","cuisine","restaurant_name","amenity","rating","location"]}
{"id":"46","dataset":"mit-restaurant","split":"test","instance":{"id":"46","prompt_labels":"are(O) there(O) any(O) greek(B-Cuisine) restaurants(O) in(O) the(O) theater(B-Location) district(I-Location) of(O) the(O) back(B-Location) bay(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Restaurant Name, Price, Rating, Amenity, Hours, Cuisine and O.\nSentence: are there any greek restaurants in the theater district of the back bay","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","greek","restaurants","in","the","theater","district","of","the","back","bay"],"labels":["O","O","O","B-Cuisine","O","O","O","B-Location","I-Location","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["dish","location","restaurant_name","price","rating","amenity","hours","cuisine"]}
{"id":"64","dataset":"mit-restaurant","split":"test","instance":{"id":"64","prompt_labels":"are(O) there(O) any(O) places(O) around(B-Location) here(I-Location) that(O) has(O) tomato(B-Dish) sauce(I-Dish) based(I-Dish) dishes(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Price, Dish, Amenity, Restaurant Name, Hours, Cuisine, Rating and O.\nSentence: are there any places around here that has tomato sauce based dishes","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","places","around","here","that","has","tomato","sauce","based","dishes"],"labels":["O","O","O","O","B-Location","I-Location","O","O","B-Dish","I-Dish","I-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["location","price","dish","amenity","restaurant_name","hours","cuisine","rating"]}
{"id":"80","dataset":"mit-restaurant","split":"test","instance":{"id":"80","prompt_labels":"are(O) there(O) any(O) restaurants(O) open(B-Hours) after(I-Hours) 2(I-Hours) am(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Hours, Restaurant Name, Location, Dish, Cuisine, Rating and O.\nSentence: are there any restaurants open after 2 am","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","restaurants","open","after","2","am"],"labels":["O","O","O","O","B-Hours","I-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["price","amenity","hours","restaurant_name","location","dish","cuisine","rating"]}
{"id":"97","dataset":"mit-restaurant","split":"test","instance":{"id":"97","prompt_labels":"are(O) there(O) any(O) vegetarian(B-Cuisine) restaurants(O) that(O) allow(O) you(O) to(O) order(B-Amenity) online(I-Amenity) ahead(I-Amenity) of(I-Amenity) time(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Amenity, Rating, Location, Restaurant Name, Cuisine, Price, Dish and O.\nSentence: are there any vegetarian restaurants that allow you to order online ahead of time","prediction_output":null,"prediction_outputs":null,"group":null,"words":["are","there","any","vegetarian","restaurants","that","allow","you","to","order","online","ahead","of","time"],"labels":["O","O","O","B-Cuisine","O","O","O","O","O","B-Amenity","I-Amenity","I-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["hours","amenity","rating","location","restaurant_name","cuisine","price","dish"]}
{"id":"106","dataset":"mit-restaurant","split":"test","instance":{"id":"106","prompt_labels":"borscht(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Price, Amenity, Hours, Dish, Rating, Location and O.\nSentence: borscht","prediction_output":null,"prediction_outputs":null,"group":null,"words":["borscht"],"labels":["B-Dish"],"target_index":null,"target_label":null},"label_list":["restaurant_name","cuisine","price","amenity","hours","dish","rating","location"]}
{"id":"118","dataset":"mit-restaurant","split":"test","instance":{"id":"118","prompt_labels":"can(O) i(O) find(O) a(O) bar(B-Cuisine) and(I-Cuisine) grill(I-Cuisine) within(O) short(B-Location) walking(I-Location) distance(I-Location) of(I-Location) the(I-Location) shopping(I-Location) district(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Location, Cuisine, Restaurant Name, Price, Amenity, Hours and O.\nSentence: can i find a bar and grill within short walking distance of the shopping district","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","i","find","a","bar","and","grill","within","short","walking","distance","of","the","shopping","district"],"labels":["O","O","O","O","B-Cuisine","I-Cuisine","I-Cuisine","O","B-Location","I-Location","I-Location","I-Location","I-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["rating","dish","location","cuisine","restaurant_name","price","amenity","hours"]}
{"id":"179","dataset":"mit-restaurant","split":"test","instance":{"id":"179","prompt_labels":"can(O) you(O) give(O) me(O) the(O) name(B-Restaurant Name) of(O) the(O) restaurant(O) on(O) green(B-Location) st(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Dish, Restaurant Name, Amenity, Location, Rating, Cuisine, Hours and O.\nSentence: can you give me the name of the restaurant on green st","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","give","me","the","name","of","the","restaurant","on","green","st"],"labels":["O","O","O","O","O","B-Restaurant Name","O","O","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["price","dish","restaurant_name","amenity","location","rating","cuisine","hours"]}
{"id":"195","dataset":"mit-restaurant","split":"test","instance":{"id":"195","prompt_labels":"can(O) you(O) make(O) me(O) a(O) reservation(B-Amenity) at(O) the(O) cheapest(B-Price) restaurant(O) with(O) valet(B-Amenity) parking(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Restaurant Name, Location, Cuisine, Dish, Rating, Price, Hours and O.\nSentence: can you make me a reservation at the cheapest restaurant with valet parking","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","make","me","a","reservation","at","the","cheapest","restaurant","with","valet","parking"],"labels":["O","O","O","O","O","B-Amenity","O","O","B-Price","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["amenity","restaurant_name","location","cuisine","dish","rating","price","hours"]}
{"id":"203","dataset":"mit-restaurant","split":"test","instance":{"id":"203","prompt_labels":"can(O) you(O) search(O) for(O) the(O) most(B-Price) expensive(I-Price) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Dish, Restaurant Name, Cuisine, Hours, Amenity, Price and O.\nSentence: can you search for the most expensive restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","search","for","the","most","expensive","restaurant"],"labels":["O","O","O","O","O","B-Price","I-Price","O"],"target_index":null,"target_label":null},"label_list":["location","rating","dish","restaurant_name","cuisine","hours","amenity","price"]}
{"id":"206","dataset":"mit-restaurant","split":"test","instance":{"id":"206","prompt_labels":"can(O) you(O) tell(O) me(O) what(O) italian(B-Cuisine) restaurants(O) north(B-Location) of(I-Location) heat(I-Location) rd(I-Location) in(I-Location) fairfax(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Dish, Restaurant Name, Rating, Price, Amenity, Cuisine, Hours and O.\nSentence: can you tell me what italian restaurants north of heat rd in fairfax","prediction_output":null,"prediction_outputs":null,"group":null,"words":["can","you","tell","me","what","italian","restaurants","north","of","heat","rd","in","fairfax"],"labels":["O","O","O","O","O","B-Cuisine","O","B-Location","I-Location","I-Location","I-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["location","dish","restaurant_name","rating","price","amenity","cuisine","hours"]}
{"id":"241","dataset":"mit-restaurant","split":"test","instance":{"id":"241","prompt_labels":"do(O) any(O) restaurants(O) nearby(B-Location) offer(O) a(O) few(O) gluten(B-Amenity) free(I-Amenity) options(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Rating, Hours, Cuisine, Amenity, Price, Restaurant Name and O.\nSentence: do any restaurants nearby offer a few gluten free options","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","any","restaurants","nearby","offer","a","few","gluten","free","options"],"labels":["O","O","O","B-Location","O","O","O","B-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["dish","location","rating","hours","cuisine","amenity","price","restaurant_name"]}
{"id":"252","dataset":"mit-restaurant","split":"test","instance":{"id":"252","prompt_labels":"do(O) you(O) know(O) any(O) good(B-Rating) pizza(B-Dish) places(O) open(B-Hours) until(I-Hours) midnight(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Hours, Rating, Cuisine, Restaurant Name, Amenity, Price, Location and O.\nSentence: do you know any good pizza places open until midnight","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","know","any","good","pizza","places","open","until","midnight"],"labels":["O","O","O","O","B-Rating","B-Dish","O","B-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["dish","hours","rating","cuisine","restaurant_name","amenity","price","location"]}
{"id":"255","dataset":"mit-restaurant","split":"test","instance":{"id":"255","prompt_labels":"do(O) you(O) know(O) if(O) reggianos(B-Restaurant Name) serve(O) breakfast(B-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Cuisine, Location, Rating, Hours, Amenity, Dish, Restaurant Name and O.\nSentence: do you know if reggianos serve breakfast","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","know","if","reggianos","serve","breakfast"],"labels":["O","O","O","O","B-Restaurant Name","O","B-Hours"],"target_index":null,"target_label":null},"label_list":["price","cuisine","location","rating","hours","amenity","dish","restaurant_name"]}
{"id":"264","dataset":"mit-restaurant","split":"test","instance":{"id":"264","prompt_labels":"do(O) you(O) know(O) of(O) a(O) bakery(B-Cuisine) that(O) is(O) still(B-Hours) open(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Dish, Price, Amenity, Hours, Rating, Location, Restaurant Name and O.\nSentence: do you know of a bakery that is still open","prediction_output":null,"prediction_outputs":null,"group":null,"words":["do","you","know","of","a","bakery","that","is","still","open"],"labels":["O","O","O","O","O","B-Cuisine","O","O","B-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["cuisine","dish","price","amenity","hours","rating","location","restaurant_name"]}
{"id":"279","dataset":"mit-restaurant","split":"test","instance":{"id":"279","prompt_labels":"does(O) caribe(B-Restaurant Name) have(O) a(O) smoking(B-Amenity) area(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Hours, Amenity, Dish, Restaurant Name, Rating, Price and O.\nSentence: does caribe have a smoking area","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","caribe","have","a","smoking","area"],"labels":["O","B-Restaurant Name","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["location","cuisine","hours","amenity","dish","restaurant_name","rating","price"]}
{"id":"286","dataset":"mit-restaurant","split":"test","instance":{"id":"286","prompt_labels":"does(O) jaimes(B-Restaurant Name) bakery(I-Restaurant Name) have(O) a(O) great(B-Amenity) decor(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Dish, Price, Location, Amenity, Hours, Cuisine and O.\nSentence: does jaimes bakery have a great decor","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","jaimes","bakery","have","a","great","decor"],"labels":["O","B-Restaurant Name","I-Restaurant Name","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["rating","restaurant_name","dish","price","location","amenity","hours","cuisine"]}
{"id":"291","dataset":"mit-restaurant","split":"test","instance":{"id":"291","prompt_labels":"does(O) mikes(B-Restaurant Name) cafe(I-Restaurant Name) have(O) a(O) smoking(B-Amenity) section(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Location, Hours, Price, Amenity, Restaurant Name, Rating, Cuisine and O.\nSentence: does mikes cafe have a smoking section","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","mikes","cafe","have","a","smoking","section"],"labels":["O","B-Restaurant Name","I-Restaurant Name","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["dish","location","hours","price","amenity","restaurant_name","rating","cuisine"]}
{"id":"294","dataset":"mit-restaurant","split":"test","instance":{"id":"294","prompt_labels":"does(O) mortons(B-Restaurant Name) have(O) a(O) dress(B-Amenity) code(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Amenity, Rating, Hours, Dish, Restaurant Name, Cuisine and O.\nSentence: does mortons have a dress code","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","mortons","have","a","dress","code"],"labels":["O","B-Restaurant Name","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["price","location","amenity","rating","hours","dish","restaurant_name","cuisine"]}
{"id":"301","dataset":"mit-restaurant","split":"test","instance":{"id":"301","prompt_labels":"does(O) ricatonis(B-Restaurant Name) offer(O) a(O) lunch(B-Amenity) portion(I-Amenity) option(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Restaurant Name, Cuisine, Dish, Location, Rating, Hours, Amenity and O.\nSentence: does ricatonis offer a lunch portion option","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","ricatonis","offer","a","lunch","portion","option"],"labels":["O","B-Restaurant Name","O","O","B-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["price","restaurant_name","cuisine","dish","location","rating","hours","amenity"]}
{"id":"307","dataset":"mit-restaurant","split":"test","instance":{"id":"307","prompt_labels":"does(O) tgi(B-Restaurant Name) fridays(I-Restaurant Name) have(O) senior(B-Amenity) discounts(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Restaurant Name, Dish, Price, Hours, Amenity, Cuisine, Rating and O.\nSentence: does tgi fridays have senior discounts","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","tgi","fridays","have","senior","discounts"],"labels":["O","B-Restaurant Name","I-Restaurant Name","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["location","restaurant_name","dish","price","hours","amenity","cuisine","rating"]}
{"id":"311","dataset":"mit-restaurant","split":"test","instance":{"id":"311","prompt_labels":"does(O) the(O) chinese(B-Cuisine) buffet(I-Cuisine) on(O) 6(B-Location) th(I-Location) avenue(I-Location) have(O) a(O) smoking(B-Amenity) section(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Restaurant Name, Price, Hours, Location, Amenity, Rating and O.\nSentence: does the chinese buffet on 6 th avenue have a smoking section","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","the","chinese","buffet","on","6","th","avenue","have","a","smoking","section"],"labels":["O","O","B-Cuisine","I-Cuisine","O","B-Location","I-Location","I-Location","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["dish","cuisine","restaurant_name","price","hours","location","amenity","rating"]}
{"id":"314","dataset":"mit-restaurant","split":"test","instance":{"id":"314","prompt_labels":"does(O) the(O) italian(B-Cuisine) restaurant(O) in(B-Location) the(I-Location) town(I-Location) center(I-Location) offer(O) carry(B-Amenity) out(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Cuisine, Price, Amenity, Hours, Location, Dish and O.\nSentence: does the italian restaurant in the town center offer carry out","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","the","italian","restaurant","in","the","town","center","offer","carry","out"],"labels":["O","O","B-Cuisine","O","B-Location","I-Location","I-Location","I-Location","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","rating","cuisine","price","amenity","hours","location","dish"]}
{"id":"323","dataset":"mit-restaurant","split":"test","instance":{"id":"323","prompt_labels":"does(O) the(O) pizza(B-Restaurant Name) shop(I-Restaurant Name) on(O) florida(B-Location) have(O) outdoor(B-Amenity) parking(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Hours, Location, Dish, Cuisine, Restaurant Name, Rating and O.\nSentence: does the pizza shop on florida have outdoor parking","prediction_output":null,"prediction_outputs":null,"group":null,"words":["does","the","pizza","shop","on","florida","have","outdoor","parking"],"labels":["O","O","B-Restaurant Name","I-Restaurant Name","O","B-Location","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["price","amenity","hours","location","dish","cuisine","restaurant_name","rating"]}
{"id":"334","dataset":"mit-restaurant","split":"test","instance":{"id":"334","prompt_labels":"find(O) a(O) cheap(B-Price) brewpub(B-Cuisine) that(O) serves(O) beef(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Restaurant Name, Hours, Cuisine, Amenity, Price, Rating, Location and O.\nSentence: find a cheap brewpub that serves beef","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","a","cheap","brewpub","that","serves","beef"],"labels":["O","O","B-Price","B-Cuisine","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["dish","restaurant_name","hours","cuisine","amenity","price","rating","location"]}
{"id":"350","dataset":"mit-restaurant","split":"test","instance":{"id":"350","prompt_labels":"find(O) a(O) wright(B-Restaurant Name) wright(I-Restaurant Name) take(B-Amenity) out(I-Amenity) for(O) a(O) special(B-Amenity) occasion(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Rating, Price, Cuisine, Hours, Location, Dish and O.\nSentence: find a wright wright take out for a special occasion","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","a","wright","wright","take","out","for","a","special","occasion"],"labels":["O","O","B-Restaurant Name","I-Restaurant Name","B-Amenity","I-Amenity","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","amenity","rating","price","cuisine","hours","location","dish"]}
{"id":"376","dataset":"mit-restaurant","split":"test","instance":{"id":"376","prompt_labels":"find(O) me(O) a(O) good(B-Rating) vegetarian(B-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Dish, Restaurant Name, Location, Amenity, Hours, Cuisine and O.\nSentence: find me a good vegetarian restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","good","vegetarian","restaurant"],"labels":["O","O","O","B-Rating","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["rating","price","dish","restaurant_name","location","amenity","hours","cuisine"]}
{"id":"382","dataset":"mit-restaurant","split":"test","instance":{"id":"382","prompt_labels":"find(O) me(O) a(O) pizza(B-Cuisine) parlour(I-Cuisine) please(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Price, Location, Cuisine, Amenity, Restaurant Name, Hours, Rating and O.\nSentence: find me a pizza parlour please","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","pizza","parlour","please"],"labels":["O","O","O","B-Cuisine","I-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["dish","price","location","cuisine","amenity","restaurant_name","hours","rating"]}
{"id":"410","dataset":"mit-restaurant","split":"test","instance":{"id":"410","prompt_labels":"find(O) me(O) a(O) take(B-Amenity) out(I-Amenity) chinese(B-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Restaurant Name, Price, Dish, Cuisine, Rating, Hours, Location and O.\nSentence: find me a take out chinese restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","a","take","out","chinese","restaurant"],"labels":["O","O","O","B-Amenity","I-Amenity","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["amenity","restaurant_name","price","dish","cuisine","rating","hours","location"]}
{"id":"419","dataset":"mit-restaurant","split":"test","instance":{"id":"419","prompt_labels":"find(O) me(O) an(O) expensive(B-Price) american(B-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Restaurant Name, Dish, Rating, Price, Amenity, Hours and O.\nSentence: find me an expensive american restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","an","expensive","american","restaurant"],"labels":["O","O","O","B-Price","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["cuisine","location","restaurant_name","dish","rating","price","amenity","hours"]}
{"id":"423","dataset":"mit-restaurant","split":"test","instance":{"id":"423","prompt_labels":"find(O) me(O) chicken(B-Dish) places(O) that(O) accept(B-Amenity) discover(I-Amenity) card(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Restaurant Name, Hours, Dish, Price, Rating, Cuisine and O.\nSentence: find me chicken places that accept discover card","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","chicken","places","that","accept","discover","card"],"labels":["O","O","B-Dish","O","O","B-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["location","amenity","restaurant_name","hours","dish","price","rating","cuisine"]}
{"id":"426","dataset":"mit-restaurant","split":"test","instance":{"id":"426","prompt_labels":"find(O) me(O) italian(B-Cuisine) restaurants(O) with(O) cheesecake(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Restaurant Name, Amenity, Hours, Rating, Price, Dish, Cuisine and O.\nSentence: find me italian restaurants with cheesecake","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","me","italian","restaurants","with","cheesecake"],"labels":["O","O","B-Cuisine","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["location","restaurant_name","amenity","hours","rating","price","dish","cuisine"]}
{"id":"440","dataset":"mit-restaurant","split":"test","instance":{"id":"440","prompt_labels":"find(O) my(O) someplace(B-Location) around(O) here(O) where(O) i(O) can(O) rock(O) out(O) with(O) a(O) smoothie(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Dish, Location, Hours, Restaurant Name, Price, Amenity, Rating and O.\nSentence: find my someplace around here where i can rock out with a smoothie","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","my","someplace","around","here","where","i","can","rock","out","with","a","smoothie"],"labels":["O","O","B-Location","O","O","O","O","O","O","O","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["cuisine","dish","location","hours","restaurant_name","price","amenity","rating"]}
{"id":"446","dataset":"mit-restaurant","split":"test","instance":{"id":"446","prompt_labels":"find(O) restaurants(O) within(B-Location) 5(I-Location) miles(I-Location) with(O) entrees(B-Price) under(I-Price) 15(I-Price)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Location, Restaurant Name, Rating, Price, Dish, Amenity and O.\nSentence: find restaurants within 5 miles with entrees under 15","prediction_output":null,"prediction_outputs":null,"group":null,"words":["find","restaurants","within","5","miles","with","entrees","under","15"],"labels":["O","O","B-Location","I-Location","I-Location","O","B-Price","I-Price","I-Price"],"target_index":null,"target_label":null},"label_list":["cuisine","hours","location","restaurant_name","rating","price","dish","amenity"]}
{"id":"460","dataset":"mit-restaurant","split":"test","instance":{"id":"460","prompt_labels":"get(O) me(O) to(O) a(O) good(B-Rating) pho(B-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Rating, Dish, Price, Location, Amenity, Restaurant Name, Cuisine and O.\nSentence: get me to a good pho restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["get","me","to","a","good","pho","restaurant"],"labels":["O","O","O","O","B-Rating","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["hours","rating","dish","price","location","amenity","restaurant_name","cuisine"]}
{"id":"482","dataset":"mit-restaurant","split":"test","instance":{"id":"482","prompt_labels":"help(O) me(O) find(O) a(O) place(O) my(O) kids(B-Amenity) would(I-Amenity) like(I-Amenity) to(I-Amenity) eat(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Dish, Amenity, Rating, Restaurant Name, Location, Price and O.\nSentence: help me find a place my kids would like to eat","prediction_output":null,"prediction_outputs":null,"group":null,"words":["help","me","find","a","place","my","kids","would","like","to","eat"],"labels":["O","O","O","O","O","O","B-Amenity","I-Amenity","I-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["cuisine","hours","dish","amenity","rating","restaurant_name","location","price"]}
{"id":"514","dataset":"mit-restaurant","split":"test","instance":{"id":"514","prompt_labels":"how(O) far(O) is(O) evergreen(B-Restaurant Name) taiwanese(I-Restaurant Name) restaurant(O) from(O) the(O) himalayan(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Restaurant Name, Amenity, Rating, Price, Dish, Location, Cuisine and O.\nSentence: how far is evergreen taiwanese restaurant from the himalayan","prediction_output":null,"prediction_outputs":null,"group":null,"words":["how","far","is","evergreen","taiwanese","restaurant","from","the","himalayan"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","O","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["hours","restaurant_name","amenity","rating","price","dish","location","cuisine"]}
{"id":"540","dataset":"mit-restaurant","split":"test","instance":{"id":"540","prompt_labels":"i(O) am(O) diabetic(O) and(O) need(O) to(O) know(O) if(O) there(O) are(O) any(O) health(B-Cuisine) stores(O) in(B-Location) the(I-Location) area(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Rating, Cuisine, Amenity, Hours, Restaurant Name, Dish, Location and O.\nSentence: i am diabetic and need to know if there are any health stores in the area","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","am","diabetic","and","need","to","know","if","there","are","any","health","stores","in","the","area"],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-Cuisine","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["price","rating","cuisine","amenity","hours","restaurant_name","dish","location"]}
{"id":"543","dataset":"mit-restaurant","split":"test","instance":{"id":"543","prompt_labels":"i(O) am(O) in(O) the(O) mood(O) for(O) shrimp(B-Dish) where(O) is(O) the(O) closet(B-Location) place(O) i(O) can(O) go(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Cuisine, Dish, Hours, Restaurant Name, Price, Location and O.\nSentence: i am in the mood for shrimp where is the closet place i can go","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","am","in","the","mood","for","shrimp","where","is","the","closet","place","i","can","go"],"labels":["O","O","O","O","O","O","B-Dish","O","O","O","B-Location","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["rating","amenity","cuisine","dish","hours","restaurant_name","price","location"]}
{"id":"553","dataset":"mit-restaurant","split":"test","instance":{"id":"553","prompt_labels":"i(O) am(O) looking(O) for(O) an(O) olive(B-Restaurant Name) garden(I-Restaurant Name) are(O) there(O) any(O) close(B-Location) by(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Hours, Dish, Amenity, Price, Cuisine, Rating, Location and O.\nSentence: i am looking for an olive garden are there any close by","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","am","looking","for","an","olive","garden","are","there","any","close","by"],"labels":["O","O","O","O","O","B-Restaurant Name","I-Restaurant Name","O","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","hours","dish","amenity","price","cuisine","rating","location"]}
{"id":"562","dataset":"mit-restaurant","split":"test","instance":{"id":"562","prompt_labels":"i(O) have(O) a(O) craving(O) for(O) fish(B-Cuisine) and(O) chips(B-Cuisine) but(O) i(O) am(O) on(O) a(O) budget(O) so(O) i(O) need(O) them(O) cheap(B-Price)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Rating, Dish, Location, Cuisine, Price, Hours and O.\nSentence: i have a craving for fish and chips but i am on a budget so i need them cheap","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","have","a","craving","for","fish","and","chips","but","i","am","on","a","budget","so","i","need","them","cheap"],"labels":["O","O","O","O","O","B-Cuisine","O","B-Cuisine","O","O","O","O","O","O","O","O","O","O","B-Price"],"target_index":null,"target_label":null},"label_list":["restaurant_name","amenity","rating","dish","location","cuisine","price","hours"]}
{"id":"579","dataset":"mit-restaurant","split":"test","instance":{"id":"579","prompt_labels":"i(O) need(O) a(O) list(O) of(O) restaurants(O) that(O) take(O) the(O) diners(B-Amenity) card(I-Amenity) in(O) a(O) 5(B-Location) mile(I-Location) radius(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Dish, Amenity, Hours, Price, Cuisine, Restaurant Name and O.\nSentence: i need a list of restaurants that take the diners card in a 5 mile radius","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","need","a","list","of","restaurants","that","take","the","diners","card","in","a","5","mile","radius"],"labels":["O","O","O","O","O","O","O","O","O","B-Amenity","I-Amenity","O","O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["location","rating","dish","amenity","hours","price","cuisine","restaurant_name"]}
{"id":"618","dataset":"mit-restaurant","split":"test","instance":{"id":"618","prompt_labels":"i(O) want(O) a(O) beer(B-Dish) from(O) the(O) cambridge(B-Restaurant Name) brewing(I-Restaurant Name) company(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Location, Cuisine, Amenity, Dish, Restaurant Name, Price, Hours and O.\nSentence: i want a beer from the cambridge brewing company","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","a","beer","from","the","cambridge","brewing","company"],"labels":["O","O","O","B-Dish","O","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["rating","location","cuisine","amenity","dish","restaurant_name","price","hours"]}
{"id":"620","dataset":"mit-restaurant","split":"test","instance":{"id":"620","prompt_labels":"i(O) want(O) a(O) good(B-Rating) milkshake(B-Dish) where(O) can(O) i(O) find(O) it(O) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Hours, Cuisine, Dish, Amenity, Restaurant Name, Price and O.\nSentence: i want a good milkshake where can i find it nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","a","good","milkshake","where","can","i","find","it","nearby"],"labels":["O","O","O","B-Rating","B-Dish","O","O","O","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["location","rating","hours","cuisine","dish","amenity","restaurant_name","price"]}
{"id":"627","dataset":"mit-restaurant","split":"test","instance":{"id":"627","prompt_labels":"i(O) want(O) a(O) restaurant(O) on(O) smith(B-Location) st(I-Location) that(O) serves(O) toast(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Cuisine, Dish, Price, Location, Restaurant Name, Hours, Amenity and O.\nSentence: i want a restaurant on smith st that serves toast","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","a","restaurant","on","smith","st","that","serves","toast"],"labels":["O","O","O","O","O","B-Location","I-Location","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["rating","cuisine","dish","price","location","restaurant_name","hours","amenity"]}
{"id":"638","dataset":"mit-restaurant","split":"test","instance":{"id":"638","prompt_labels":"i(O) want(O) some(O) taco(B-Restaurant Name) bell(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Price, Location, Dish, Hours, Cuisine, Amenity, Restaurant Name and O.\nSentence: i want some taco bell","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","some","taco","bell"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["rating","price","location","dish","hours","cuisine","amenity","restaurant_name"]}
{"id":"640","dataset":"mit-restaurant","split":"test","instance":{"id":"640","prompt_labels":"i(O) want(O) something(O) to(O) eat(O) close(B-Location) by(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Location, Dish, Cuisine, Restaurant Name, Hours, Amenity, Rating and O.\nSentence: i want something to eat close by","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","something","to","eat","close","by"],"labels":["O","O","O","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["price","location","dish","cuisine","restaurant_name","hours","amenity","rating"]}
{"id":"680","dataset":"mit-restaurant","split":"test","instance":{"id":"680","prompt_labels":"i(O) want(O) to(O) try(O) a(O) blow(B-Dish) fish(I-Dish) could(O) you(O) find(O) a(O) good(B-Rating) place(O) to(O) try(O) it(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Amenity, Price, Rating, Cuisine, Location, Hours, Dish and O.\nSentence: i want to try a blow fish could you find a good place to try it","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","to","try","a","blow","fish","could","you","find","a","good","place","to","try","it"],"labels":["O","O","O","O","O","B-Dish","I-Dish","O","O","O","O","B-Rating","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["restaurant_name","amenity","price","rating","cuisine","location","hours","dish"]}
{"id":"682","dataset":"mit-restaurant","split":"test","instance":{"id":"682","prompt_labels":"i(O) want(O) to(O) try(O) something(O) new(B-Cuisine) for(O) dinner(B-Cuisine) tonight(B-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Dish, Amenity, Location, Price, Rating, Restaurant Name, Hours and O.\nSentence: i want to try something new for dinner tonight","prediction_output":null,"prediction_outputs":null,"group":null,"words":["i","want","to","try","something","new","for","dinner","tonight"],"labels":["O","O","O","O","O","B-Cuisine","O","B-Cuisine","B-Hours"],"target_index":null,"target_label":null},"label_list":["cuisine","dish","amenity","location","price","rating","restaurant_name","hours"]}
{"id":"724","dataset":"mit-restaurant","split":"test","instance":{"id":"724","prompt_labels":"im(O) hungry(O) lets(O) get(O) some(O) tacos(B-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Price, Amenity, Dish, Rating, Restaurant Name, Location and O.\nSentence: im hungry lets get some tacos","prediction_output":null,"prediction_outputs":null,"group":null,"words":["im","hungry","lets","get","some","tacos"],"labels":["O","O","O","O","O","B-Dish"],"target_index":null,"target_label":null},"label_list":["hours","cuisine","price","amenity","dish","rating","restaurant_name","location"]}
{"id":"729","dataset":"mit-restaurant","split":"test","instance":{"id":"729","prompt_labels":"im(O) in(O) the(O) mood(O) for(O) some(O) texas(B-Dish) chili(I-Dish)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Rating, Amenity, Price, Cuisine, Dish, Restaurant Name and O.\nSentence: im in the mood for some texas chili","prediction_output":null,"prediction_outputs":null,"group":null,"words":["im","in","the","mood","for","some","texas","chili"],"labels":["O","O","O","O","O","O","B-Dish","I-Dish"],"target_index":null,"target_label":null},"label_list":["hours","location","rating","amenity","price","cuisine","dish","restaurant_name"]}
{"id":"753","dataset":"mit-restaurant","split":"test","instance":{"id":"753","prompt_labels":"im(O) starving(O) is(O) there(O) a(O) restaurant(O) that(O) sells(O) shawarma(B-Dish) here(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Restaurant Name, Price, Rating, Location, Dish, Cuisine, Hours and O.\nSentence: im starving is there a restaurant that sells shawarma here","prediction_output":null,"prediction_outputs":null,"group":null,"words":["im","starving","is","there","a","restaurant","that","sells","shawarma","here"],"labels":["O","O","O","O","O","O","O","O","B-Dish","B-Location"],"target_index":null,"target_label":null},"label_list":["amenity","restaurant_name","price","rating","location","dish","cuisine","hours"]}
{"id":"775","dataset":"mit-restaurant","split":"test","instance":{"id":"775","prompt_labels":"is(O) pasquales(B-Restaurant Name) still(O) located(O) on(O) dayton(B-Location) street(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Amenity, Restaurant Name, Dish, Location, Hours, Price, Rating and O.\nSentence: is pasquales still located on dayton street","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","pasquales","still","located","on","dayton","street"],"labels":["O","B-Restaurant Name","O","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","amenity","restaurant_name","dish","location","hours","price","rating"]}
{"id":"793","dataset":"mit-restaurant","split":"test","instance":{"id":"793","prompt_labels":"is(O) the(O) tea(B-Restaurant Name) garden(I-Restaurant Name) restaurant(I-Restaurant Name) in(O) medford(B-Location) good(O) for(O) a(O) date(B-Amenity) night(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Hours, Price, Location, Amenity, Cuisine, Restaurant Name and O.\nSentence: is the tea garden restaurant in medford good for a date night","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","the","tea","garden","restaurant","in","medford","good","for","a","date","night"],"labels":["O","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","O","B-Location","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["rating","dish","hours","price","location","amenity","cuisine","restaurant_name"]}
{"id":"794","dataset":"mit-restaurant","split":"test","instance":{"id":"794","prompt_labels":"is(O) the(O) twirl(B-Restaurant Name) pasta(I-Restaurant Name) in(O) north(B-Location) end(I-Location) a(O) good(O) place(O) for(O) a(O) business(B-Amenity) lunch(B-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Price, Rating, Dish, Location, Cuisine, Restaurant Name, Hours and O.\nSentence: is the twirl pasta in north end a good place for a business lunch","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","the","twirl","pasta","in","north","end","a","good","place","for","a","business","lunch"],"labels":["O","O","B-Restaurant Name","I-Restaurant Name","O","B-Location","I-Location","O","O","O","O","O","B-Amenity","B-Hours"],"target_index":null,"target_label":null},"label_list":["amenity","price","rating","dish","location","cuisine","restaurant_name","hours"]}
{"id":"805","dataset":"mit-restaurant","split":"test","instance":{"id":"805","prompt_labels":"is(O) there(O) a(O) cheap(B-Price) vegetarian(B-Cuisine) restaurant(O) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Price, Amenity, Hours, Location, Dish, Rating, Restaurant Name and O.\nSentence: is there a cheap vegetarian restaurant nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","cheap","vegetarian","restaurant","nearby"],"labels":["O","O","O","B-Price","B-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","price","amenity","hours","location","dish","rating","restaurant_name"]}
{"id":"838","dataset":"mit-restaurant","split":"test","instance":{"id":"838","prompt_labels":"is(O) there(O) a(O) olive(B-Restaurant Name) garden(I-Restaurant Name) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Cuisine, Restaurant Name, Price, Hours, Amenity, Rating, Location and O.\nSentence: is there a olive garden nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","olive","garden","nearby"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","B-Location"],"target_index":null,"target_label":null},"label_list":["dish","cuisine","restaurant_name","price","hours","amenity","rating","location"]}
{"id":"849","dataset":"mit-restaurant","split":"test","instance":{"id":"849","prompt_labels":"is(O) there(O) a(O) place(O) within(B-Location) 10(I-Location) minutes(I-Location) that(O) has(O) great(B-Amenity) atmosphere(I-Amenity) for(O) a(O) special(O) meal(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Price, Rating, Amenity, Hours, Location, Dish and O.\nSentence: is there a place within 10 minutes that has great atmosphere for a special meal","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","place","within","10","minutes","that","has","great","atmosphere","for","a","special","meal"],"labels":["O","O","O","O","B-Location","I-Location","I-Location","O","O","B-Amenity","I-Amenity","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["cuisine","restaurant_name","price","rating","amenity","hours","location","dish"]}
{"id":"856","dataset":"mit-restaurant","split":"test","instance":{"id":"856","prompt_labels":"is(O) there(O) a(O) red(B-Restaurant Name) lobster(I-Restaurant Name) in(O) the(O) area(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Rating, Cuisine, Restaurant Name, Amenity, Hours, Price, Location and O.\nSentence: is there a red lobster in the area","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","red","lobster","in","the","area"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["dish","rating","cuisine","restaurant_name","amenity","hours","price","location"]}
{"id":"868","dataset":"mit-restaurant","split":"test","instance":{"id":"868","prompt_labels":"is(O) there(O) a(O) sbarro(B-Restaurant Name) in(O) the(O) galleria(B-Location) mall(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Dish, Rating, Hours, Amenity, Price, Location, Cuisine and O.\nSentence: is there a sbarro in the galleria mall","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","sbarro","in","the","galleria","mall"],"labels":["O","O","O","B-Restaurant Name","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","dish","rating","hours","amenity","price","location","cuisine"]}
{"id":"878","dataset":"mit-restaurant","split":"test","instance":{"id":"878","prompt_labels":"is(O) there(O) a(O) taqueria(B-Restaurant Name) el(I-Restaurant Name) rancho(I-Restaurant Name) grandes(I-Restaurant Name) around(B-Location) here(I-Location) for(O) taking(B-Amenity) a(I-Amenity) date(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Hours, Restaurant Name, Dish, Amenity, Price, Rating, Cuisine and O.\nSentence: is there a taqueria el rancho grandes around here for taking a date","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","taqueria","el","rancho","grandes","around","here","for","taking","a","date"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","I-Restaurant Name","I-Restaurant Name","B-Location","I-Location","O","B-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["location","hours","restaurant_name","dish","amenity","price","rating","cuisine"]}
{"id":"879","dataset":"mit-restaurant","split":"test","instance":{"id":"879","prompt_labels":"is(O) there(O) a(O) thai(B-Cuisine) restaurant(O) with(O) a(O) great(B-Rating) wine(B-Amenity) list(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Hours, Rating, Restaurant Name, Dish, Price, Amenity and O.\nSentence: is there a thai restaurant with a great wine list","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","thai","restaurant","with","a","great","wine","list"],"labels":["O","O","O","B-Cuisine","O","O","O","B-Rating","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["location","cuisine","hours","rating","restaurant_name","dish","price","amenity"]}
{"id":"884","dataset":"mit-restaurant","split":"test","instance":{"id":"884","prompt_labels":"is(O) there(O) a(O) white(B-Restaurant Name) castle(I-Restaurant Name) on(O) berkeley(B-Location) avenue(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Restaurant Name, Rating, Hours, Dish, Amenity, Location, Price and O.\nSentence: is there a white castle on berkeley avenue","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","a","white","castle","on","berkeley","avenue"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["cuisine","restaurant_name","rating","hours","dish","amenity","location","price"]}
{"id":"886","dataset":"mit-restaurant","split":"test","instance":{"id":"886","prompt_labels":"is(O) there(O) an(O) apple(B-Restaurant Name) bees(I-Restaurant Name) near(B-Location) by(I-Location) or(O) similar(O) bar(O) and(O) grill(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Restaurant Name, Hours, Price, Dish, Cuisine, Amenity, Location and O.\nSentence: is there an apple bees near by or similar bar and grill","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","an","apple","bees","near","by","or","similar","bar","and","grill"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","B-Location","I-Location","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["rating","restaurant_name","hours","price","dish","cuisine","amenity","location"]}
{"id":"895","dataset":"mit-restaurant","split":"test","instance":{"id":"895","prompt_labels":"is(O) there(O) an(O) italian(B-Cuisine) place(O) nearby(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Dish, Restaurant Name, Cuisine, Location, Price, Hours and O.\nSentence: is there an italian place nearby","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","an","italian","place","nearby"],"labels":["O","O","O","B-Cuisine","O","B-Location"],"target_index":null,"target_label":null},"label_list":["rating","amenity","dish","restaurant_name","cuisine","location","price","hours"]}
{"id":"909","dataset":"mit-restaurant","split":"test","instance":{"id":"909","prompt_labels":"is(O) there(O) any(O) place(O) that(O) i(O) can(O) get(O) a(O) quick(O) meal(B-Cuisine) that(I-Cuisine) isnt(O) fast(B-Cuisine) food(I-Cuisine)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Cuisine, Restaurant Name, Hours, Dish, Price, Amenity, Rating and O.\nSentence: is there any place that i can get a quick meal that isnt fast food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","any","place","that","i","can","get","a","quick","meal","that","isnt","fast","food"],"labels":["O","O","O","O","O","O","O","O","O","O","B-Cuisine","I-Cuisine","O","B-Cuisine","I-Cuisine"],"target_index":null,"target_label":null},"label_list":["location","cuisine","restaurant_name","hours","dish","price","amenity","rating"]}
{"id":"911","dataset":"mit-restaurant","split":"test","instance":{"id":"911","prompt_labels":"is(O) there(O) any(O) red(B-Restaurant Name) lobster(I-Restaurant Name) by(B-Location) the(I-Location) mall(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Rating, Amenity, Restaurant Name, Location, Cuisine, Hours, Price and O.\nSentence: is there any red lobster by the mall","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","any","red","lobster","by","the","mall"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["dish","rating","amenity","restaurant_name","location","cuisine","hours","price"]}
{"id":"923","dataset":"mit-restaurant","split":"test","instance":{"id":"923","prompt_labels":"is(O) there(O) food(O) near(B-Location) by(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Cuisine, Location, Restaurant Name, Hours, Price, Rating and O.\nSentence: is there food near by","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","there","food","near","by"],"labels":["O","O","O","B-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["amenity","dish","cuisine","location","restaurant_name","hours","price","rating"]}
{"id":"928","dataset":"mit-restaurant","split":"test","instance":{"id":"928","prompt_labels":"is(O) this(O) restaurant(O) a(O) hidden(B-Amenity) find(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Dish, Rating, Cuisine, Price, Hours, Restaurant Name, Amenity and O.\nSentence: is this restaurant a hidden find","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","this","restaurant","a","hidden","find"],"labels":["O","O","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["location","dish","rating","cuisine","price","hours","restaurant_name","amenity"]}
{"id":"930","dataset":"mit-restaurant","split":"test","instance":{"id":"930","prompt_labels":"is(O) this(O) restuarant(O) open(B-Hours) 7(I-Hours) days(I-Hours) a(O) week(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Dish, Cuisine, Price, Hours, Amenity, Rating, Restaurant Name and O.\nSentence: is this restuarant open 7 days a week","prediction_output":null,"prediction_outputs":null,"group":null,"words":["is","this","restuarant","open","7","days","a","week"],"labels":["O","O","O","B-Hours","I-Hours","I-Hours","O","O"],"target_index":null,"target_label":null},"label_list":["location","dish","cuisine","price","hours","amenity","rating","restaurant_name"]}
{"id":"941","dataset":"mit-restaurant","split":"test","instance":{"id":"941","prompt_labels":"list(O) close(B-Location) restaurants(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Amenity, Hours, Cuisine, Location, Restaurant Name, Dish, Price and O.\nSentence: list close restaurants","prediction_output":null,"prediction_outputs":null,"group":null,"words":["list","close","restaurants"],"labels":["O","B-Location","O"],"target_index":null,"target_label":null},"label_list":["rating","amenity","hours","cuisine","location","restaurant_name","dish","price"]}
{"id":"960","dataset":"mit-restaurant","split":"test","instance":{"id":"960","prompt_labels":"looking(O) for(O) a(O) diner(B-Cuisine) with(O) comfortable(B-Amenity) atmosphere(I-Amenity) and(O) a(O) rustic(B-Amenity) setting(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Restaurant Name, Price, Cuisine, Hours, Rating, Dish and O.\nSentence: looking for a diner with comfortable atmosphere and a rustic setting","prediction_output":null,"prediction_outputs":null,"group":null,"words":["looking","for","a","diner","with","comfortable","atmosphere","and","a","rustic","setting"],"labels":["O","O","O","B-Cuisine","O","B-Amenity","I-Amenity","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["location","amenity","restaurant_name","price","cuisine","hours","rating","dish"]}
{"id":"991","dataset":"mit-restaurant","split":"test","instance":{"id":"991","prompt_labels":"make(O) a(O) 5(O) 00(O) p(O) m(O) reservation(O) for(O) black(B-Restaurant Name) angus(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Amenity, Cuisine, Hours, Dish, Price, Restaurant Name, Rating and O.\nSentence: make a 5 00 p m reservation for black angus","prediction_output":null,"prediction_outputs":null,"group":null,"words":["make","a","5","00","p","m","reservation","for","black","angus"],"labels":["O","O","O","O","O","O","O","O","B-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["location","amenity","cuisine","hours","dish","price","restaurant_name","rating"]}
{"id":"1046","dataset":"mit-restaurant","split":"test","instance":{"id":"1046","prompt_labels":"restaurants(O) in(B-Location) the(I-Location) area(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Dish, Cuisine, Hours, Rating, Restaurant Name, Price, Location and O.\nSentence: restaurants in the area","prediction_output":null,"prediction_outputs":null,"group":null,"words":["restaurants","in","the","area"],"labels":["O","B-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["amenity","dish","cuisine","hours","rating","restaurant_name","price","location"]}
{"id":"1064","dataset":"mit-restaurant","split":"test","instance":{"id":"1064","prompt_labels":"show(O) me(O) where(O) the(O) nearest(B-Location) sports(B-Cuisine) bar(I-Cuisine) is(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Amenity, Hours, Location, Cuisine, Price, Dish and O.\nSentence: show me where the nearest sports bar is","prediction_output":null,"prediction_outputs":null,"group":null,"words":["show","me","where","the","nearest","sports","bar","is"],"labels":["O","O","O","O","B-Location","B-Cuisine","I-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["restaurant_name","rating","amenity","hours","location","cuisine","price","dish"]}
{"id":"1080","dataset":"mit-restaurant","split":"test","instance":{"id":"1080","prompt_labels":"tell(O) me(O) where(O) i(O) can(O) find(O) a(O) good(B-Rating) meal(O) in(O) this(O) town(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Dish, Rating, Restaurant Name, Hours, Cuisine, Price and O.\nSentence: tell me where i can find a good meal in this town","prediction_output":null,"prediction_outputs":null,"group":null,"words":["tell","me","where","i","can","find","a","good","meal","in","this","town"],"labels":["O","O","O","O","O","O","O","B-Rating","O","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["amenity","location","dish","rating","restaurant_name","hours","cuisine","price"]}
{"id":"1117","dataset":"mit-restaurant","split":"test","instance":{"id":"1117","prompt_labels":"what(O) is(O) the(O) average(B-Price) price(O) range(O) at(O) ruby(B-Restaurant Name) tuesday(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Restaurant Name, Cuisine, Dish, Location, Rating, Amenity and O.\nSentence: what is the average price range at ruby tuesday","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","average","price","range","at","ruby","tuesday"],"labels":["O","O","O","B-Price","O","O","O","B-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["price","hours","restaurant_name","cuisine","dish","location","rating","amenity"]}
{"id":"1130","dataset":"mit-restaurant","split":"test","instance":{"id":"1130","prompt_labels":"what(O) is(O) the(O) dress(O) code(O) for(O) eating(O) at(O) planet(B-Restaurant Name) hollywood(I-Restaurant Name)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Location, Amenity, Cuisine, Price, Rating, Dish, Restaurant Name and O.\nSentence: what is the dress code for eating at planet hollywood","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","dress","code","for","eating","at","planet","hollywood"],"labels":["O","O","O","O","O","O","O","O","B-Restaurant Name","I-Restaurant Name"],"target_index":null,"target_label":null},"label_list":["hours","location","amenity","cuisine","price","rating","dish","restaurant_name"]}
{"id":"1146","dataset":"mit-restaurant","split":"test","instance":{"id":"1146","prompt_labels":"what(O) is(O) the(O) phone(O) number(O) of(O) the(O) mcdonalds(B-Restaurant Name) on(O) the(O) east(B-Location) side(I-Location) of(I-Location) town(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Dish, Cuisine, Restaurant Name, Hours, Rating, Location and O.\nSentence: what is the phone number of the mcdonalds on the east side of town","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","is","the","phone","number","of","the","mcdonalds","on","the","east","side","of","town"],"labels":["O","O","O","O","O","O","O","B-Restaurant Name","O","O","B-Location","I-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["price","amenity","dish","cuisine","restaurant_name","hours","rating","location"]}
{"id":"1160","dataset":"mit-restaurant","split":"test","instance":{"id":"1160","prompt_labels":"what(O) kind(O) of(O) food(O) does(O) ubice(B-Restaurant Name) sell(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Location, Restaurant Name, Price, Cuisine, Dish, Hours, Rating and O.\nSentence: what kind of food does ubice sell","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","kind","of","food","does","ubice","sell"],"labels":["O","O","O","O","O","B-Restaurant Name","O"],"target_index":null,"target_label":null},"label_list":["amenity","location","restaurant_name","price","cuisine","dish","hours","rating"]}
{"id":"1209","dataset":"mit-restaurant","split":"test","instance":{"id":"1209","prompt_labels":"what(O) time(O) does(O) red(B-Restaurant Name) robin(I-Restaurant Name) close(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Hours, Cuisine, Rating, Location, Restaurant Name, Dish, Price and O.\nSentence: what time does red robin close","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","time","does","red","robin","close"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","O"],"target_index":null,"target_label":null},"label_list":["amenity","hours","cuisine","rating","location","restaurant_name","dish","price"]}
{"id":"1216","dataset":"mit-restaurant","split":"test","instance":{"id":"1216","prompt_labels":"what(O) time(O) does(O) the(O) sushi(B-Cuisine) place(O) near(B-Location) me(O) open(B-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Location, Price, Dish, Hours, Rating, Amenity, Restaurant Name and O.\nSentence: what time does the sushi place near me open","prediction_output":null,"prediction_outputs":null,"group":null,"words":["what","time","does","the","sushi","place","near","me","open"],"labels":["O","O","O","O","B-Cuisine","O","B-Location","O","B-Hours"],"target_index":null,"target_label":null},"label_list":["cuisine","location","price","dish","hours","rating","amenity","restaurant_name"]}
{"id":"1233","dataset":"mit-restaurant","split":"test","instance":{"id":"1233","prompt_labels":"whats(O) the(O) best(B-Rating) restaurant(O) within(B-Location) 10(I-Location) blocks(I-Location) from(I-Location) us(I-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Restaurant Name, Cuisine, Dish, Rating, Location, Amenity, Hours and O.\nSentence: whats the best restaurant within 10 blocks from us","prediction_output":null,"prediction_outputs":null,"group":null,"words":["whats","the","best","restaurant","within","10","blocks","from","us"],"labels":["O","O","B-Rating","O","B-Location","I-Location","I-Location","I-Location","I-Location"],"target_index":null,"target_label":null},"label_list":["price","restaurant_name","cuisine","dish","rating","location","amenity","hours"]}
{"id":"1251","dataset":"mit-restaurant","split":"test","instance":{"id":"1251","prompt_labels":"when(O) does(O) white(B-Restaurant Name) castle(I-Restaurant Name) close(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Price, Location, Dish, Cuisine, Restaurant Name, Amenity and O.\nSentence: when does white castle close","prediction_output":null,"prediction_outputs":null,"group":null,"words":["when","does","white","castle","close"],"labels":["O","O","B-Restaurant Name","I-Restaurant Name","B-Location"],"target_index":null,"target_label":null},"label_list":["rating","hours","price","location","dish","cuisine","restaurant_name","amenity"]}
{"id":"1276","dataset":"mit-restaurant","split":"test","instance":{"id":"1276","prompt_labels":"where(O) can(O) i(O) find(O) a(O) high(B-Price) end(I-Price) restaurant(O) on(O) chestnut(B-Location) street(I-Location) open(B-Hours) after(I-Hours) 11(I-Hours) pm(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Amenity, Location, Restaurant Name, Dish, Rating, Price and O.\nSentence: where can i find a high end restaurant on chestnut street open after 11 pm","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","find","a","high","end","restaurant","on","chestnut","street","open","after","11","pm"],"labels":["O","O","O","O","O","B-Price","I-Price","O","O","B-Location","I-Location","B-Hours","I-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["hours","cuisine","amenity","location","restaurant_name","dish","rating","price"]}
{"id":"1341","dataset":"mit-restaurant","split":"test","instance":{"id":"1341","prompt_labels":"where(O) can(O) i(O) get(O) some(O) southern(B-Cuisine) food(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Rating, Cuisine, Price, Amenity, Hours, Dish, Restaurant Name and O.\nSentence: where can i get some southern food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","get","some","southern","food"],"labels":["O","O","O","O","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["location","rating","cuisine","price","amenity","hours","dish","restaurant_name"]}
{"id":"1348","dataset":"mit-restaurant","split":"test","instance":{"id":"1348","prompt_labels":"where(O) can(O) i(O) get(O) the(O) top(B-Rating) rated(I-Rating) hamburger(B-Dish) in(O) baltimore(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Hours, Restaurant Name, Price, Location, Amenity, Cuisine, Dish and O.\nSentence: where can i get the top rated hamburger in baltimore","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","get","the","top","rated","hamburger","in","baltimore"],"labels":["O","O","O","O","O","B-Rating","I-Rating","B-Dish","O","B-Location"],"target_index":null,"target_label":null},"label_list":["rating","hours","restaurant_name","price","location","amenity","cuisine","dish"]}
{"id":"1356","dataset":"mit-restaurant","split":"test","instance":{"id":"1356","prompt_labels":"where(O) can(O) i(O) take(O) coworkers(B-Amenity) for(O) excellent(B-Price) priced(O) food(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Dish, Hours, Rating, Location, Cuisine, Price, Amenity, Restaurant Name and O.\nSentence: where can i take coworkers for excellent priced food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","can","i","take","coworkers","for","excellent","priced","food"],"labels":["O","O","O","O","B-Amenity","O","B-Price","O","O"],"target_index":null,"target_label":null},"label_list":["dish","hours","rating","location","cuisine","price","amenity","restaurant_name"]}
{"id":"1369","dataset":"mit-restaurant","split":"test","instance":{"id":"1369","prompt_labels":"where(O) i(O) can(O) get(O) seafood(B-Cuisine) and(O) bring(B-Amenity) my(I-Amenity) own(I-Amenity) drinks(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Hours, Cuisine, Rating, Restaurant Name, Price, Location, Dish, Amenity and O.\nSentence: where i can get seafood and bring my own drinks","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","i","can","get","seafood","and","bring","my","own","drinks"],"labels":["O","O","O","O","B-Cuisine","O","B-Amenity","I-Amenity","I-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["hours","cuisine","rating","restaurant_name","price","location","dish","amenity"]}
{"id":"1371","dataset":"mit-restaurant","split":"test","instance":{"id":"1371","prompt_labels":"where(O) in(O) the(O) theater(B-Location) district(I-Location) is(O) there(O) a(O) restaurant(O) with(O) portions(O) that(O) are(O) a(O) bit(O) small(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Rating, Cuisine, Dish, Price, Location, Hours, Amenity and O.\nSentence: where in the theater district is there a restaurant with portions that are a bit small","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","in","the","theater","district","is","there","a","restaurant","with","portions","that","are","a","bit","small"],"labels":["O","O","O","B-Location","I-Location","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["restaurant_name","rating","cuisine","dish","price","location","hours","amenity"]}
{"id":"1372","dataset":"mit-restaurant","split":"test","instance":{"id":"1372","prompt_labels":"where(O) is(O) a(O) bob(B-Restaurant Name) evans(I-Restaurant Name) on(O) diauto(B-Location) drive(I-Location) with(O) moderate(B-Amenity) portions(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Rating, Dish, Price, Hours, Restaurant Name, Cuisine, Amenity, Location and O.\nSentence: where is a bob evans on diauto drive with moderate portions","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","a","bob","evans","on","diauto","drive","with","moderate","portions"],"labels":["O","O","O","B-Restaurant Name","I-Restaurant Name","O","B-Location","I-Location","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["rating","dish","price","hours","restaurant_name","cuisine","amenity","location"]}
{"id":"1376","dataset":"mit-restaurant","split":"test","instance":{"id":"1376","prompt_labels":"where(O) is(O) a(O) good(B-Rating) restaurant(O) on(O) the(O) water(B-Location)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Amenity, Hours, Price, Rating, Dish, Cuisine and O.\nSentence: where is a good restaurant on the water","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","a","good","restaurant","on","the","water"],"labels":["O","O","O","B-Rating","O","O","O","B-Location"],"target_index":null,"target_label":null},"label_list":["restaurant_name","location","amenity","hours","price","rating","dish","cuisine"]}
{"id":"1382","dataset":"mit-restaurant","split":"test","instance":{"id":"1382","prompt_labels":"where(O) is(O) a(O) restaurant(O) that(O) opens(B-Hours) 24(I-Hours) hours(I-Hours)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Cuisine, Dish, Location, Amenity, Price, Hours, Rating and O.\nSentence: where is a restaurant that opens 24 hours","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","a","restaurant","that","opens","24","hours"],"labels":["O","O","O","O","O","B-Hours","I-Hours","I-Hours"],"target_index":null,"target_label":null},"label_list":["restaurant_name","cuisine","dish","location","amenity","price","hours","rating"]}
{"id":"1403","dataset":"mit-restaurant","split":"test","instance":{"id":"1403","prompt_labels":"where(O) is(O) the(O) best(B-Rating) korean(B-Cuisine) bbq(I-Cuisine)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Amenity, Rating, Restaurant Name, Dish, Hours, Cuisine, Location, Price and O.\nSentence: where is the best korean bbq","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","the","best","korean","bbq"],"labels":["O","O","O","B-Rating","B-Cuisine","I-Cuisine"],"target_index":null,"target_label":null},"label_list":["amenity","rating","restaurant_name","dish","hours","cuisine","location","price"]}
{"id":"1435","dataset":"mit-restaurant","split":"test","instance":{"id":"1435","prompt_labels":"where(O) is(O) the(O) nearest(B-Location) kosher(B-Cuisine) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Hours, Cuisine, Restaurant Name, Rating, Location, Amenity, Dish and O.\nSentence: where is the nearest kosher restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","the","nearest","kosher","restaurant"],"labels":["O","O","O","B-Location","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["price","hours","cuisine","restaurant_name","rating","location","amenity","dish"]}
{"id":"1458","dataset":"mit-restaurant","split":"test","instance":{"id":"1458","prompt_labels":"where(O) is(O) there(O) a(O) good(B-Rating) mid(B-Price) priced(O) restaurant(O) near(B-Location) 4(I-Location) th(I-Location) street(I-Location) that(O) serves(O) appetizers(B-Cuisine)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Price, Amenity, Restaurant Name, Location, Dish, Cuisine, Hours, Rating and O.\nSentence: where is there a good mid priced restaurant near 4 th street that serves appetizers","prediction_output":null,"prediction_outputs":null,"group":null,"words":["where","is","there","a","good","mid","priced","restaurant","near","4","th","street","that","serves","appetizers"],"labels":["O","O","O","O","B-Rating","B-Price","O","O","B-Location","I-Location","I-Location","I-Location","O","O","B-Cuisine"],"target_index":null,"target_label":null},"label_list":["price","amenity","restaurant_name","location","dish","cuisine","hours","rating"]}
{"id":"1478","dataset":"mit-restaurant","split":"test","instance":{"id":"1478","prompt_labels":"wheres(O) the(O) nearest(B-Location) restaurant(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Cuisine, Hours, Restaurant Name, Price, Dish, Location, Amenity, Rating and O.\nSentence: wheres the nearest restaurant","prediction_output":null,"prediction_outputs":null,"group":null,"words":["wheres","the","nearest","restaurant"],"labels":["O","O","B-Location","O"],"target_index":null,"target_label":null},"label_list":["cuisine","hours","restaurant_name","price","dish","location","amenity","rating"]}
{"id":"1507","dataset":"mit-restaurant","split":"test","instance":{"id":"1507","prompt_labels":"who(O) has(O) the(O) best(B-Rating) pizza(B-Dish) that(O) takes(O) credit(B-Amenity) cards(I-Amenity)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Restaurant Name, Location, Dish, Amenity, Cuisine, Hours, Price, Rating and O.\nSentence: who has the best pizza that takes credit cards","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","has","the","best","pizza","that","takes","credit","cards"],"labels":["O","O","O","B-Rating","B-Dish","O","O","B-Amenity","I-Amenity"],"target_index":null,"target_label":null},"label_list":["restaurant_name","location","dish","amenity","cuisine","hours","price","rating"]}
{"id":"1512","dataset":"mit-restaurant","split":"test","instance":{"id":"1512","prompt_labels":"who(O) serves(O) cali(B-Cuisine) food(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: Location, Hours, Cuisine, Price, Rating, Dish, Amenity, Restaurant Name and O.\nSentence: who serves cali food","prediction_output":null,"prediction_outputs":null,"group":null,"words":["who","serves","cali","food"],"labels":["O","O","B-Cuisine","O"],"target_index":null,"target_label":null},"label_list":["location","hours","cuisine","price","rating","dish","amenity","restaurant_name"]}
{"id":"1","dataset":"crossner_ai","split":"test","instance":{"id":"1","prompt_labels":"Finally(O) ,(O) every(O) other(O) year(O) ,(O) ELRA(B-conference) organizes(O) a(O) major(O) conference(O) LREC(B-conference) ,(O) the(O) International(B-conference) Language(I-conference) Resources(I-conference) and(I-conference) Evaluation(I-conference) Conference(I-conference) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, algorithm, metric, person, conference, researcher, university, task, country, programming language, product, organization, field and O.\nSentence: Finally , every other year , ELRA organizes a major conference LREC , the International Language Resources and Evaluation Conference .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Finally",",","every","other","year",",","ELRA","organizes","a","major","conference","LREC",",","the","International","Language","Resources","and","Evaluation","Conference","."],"labels":["O","O","O","O","O","O","B-conference","O","O","O","O","B-conference","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O"],"target_index":null,"target_label":null},"label_list":["location","algorithm","metric","person","conference","researcher","university","task","country","programming_language","product","organization","field"]}
{"id":"2","dataset":"crossner_ai","split":"test","instance":{"id":"2","prompt_labels":"The(O) task(O) is(O) usually(O) to(O) derive(O) the(O) maximum(B-algorithm) likelihood(I-algorithm) estimate(I-algorithm) of(O) the(O) parameters(O) of(O) the(O) HMM(B-algorithm) given(O) the(O) of(O) output(O) sequences(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, university, researcher, country, metric, product, person, programming language, organization, conference, task, location, algorithm and O.\nSentence: The task is usually to derive the maximum likelihood estimate of the parameters of the HMM given the of output sequences .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","task","is","usually","to","derive","the","maximum","likelihood","estimate","of","the","parameters","of","the","HMM","given","the","of","output","sequences","."],"labels":["O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","B-algorithm","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","university","researcher","country","metric","product","person","programming_language","organization","conference","task","location","algorithm"]}
{"id":"3","dataset":"crossner_ai","split":"test","instance":{"id":"3","prompt_labels":"Unlike(O) neural(B-algorithm) network(I-algorithm) s(O) and(O) Support(B-algorithm) vector(I-algorithm) machine(I-algorithm) ,(O) the(O) AdaBoost(B-algorithm) training(O) process(O) selects(O) only(O) those(O) features(O) known(O) to(O) improve(O) the(O) predictive(O) power(O) of(O) the(O) model(O) ,(O) reducing(O) dimensionality(O) and(O) potentially(O) improving(O) execution(O) time(O) as(O) irrelevant(O) features(O) need(O) not(O) be(O) computed(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, person, organization, programming language, algorithm, product, conference, field, country, researcher, task, university, location and O.\nSentence: Unlike neural network s and Support vector machine , the AdaBoost training process selects only those features known to improve the predictive power of the model , reducing dimensionality and potentially improving execution time as irrelevant features need not be computed .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Unlike","neural","network","s","and","Support","vector","machine",",","the","AdaBoost","training","process","selects","only","those","features","known","to","improve","the","predictive","power","of","the","model",",","reducing","dimensionality","and","potentially","improving","execution","time","as","irrelevant","features","need","not","be","computed","."],"labels":["O","B-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","B-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","person","organization","programming_language","algorithm","product","conference","field","country","researcher","task","university","location"]}
{"id":"6","dataset":"crossner_ai","split":"test","instance":{"id":"6","prompt_labels":"NIST(B-metric) also(O) differs(O) from(O) Bilingual(B-metric) evaluation(I-metric) understudy(I-metric) in(O) its(O) calculation(O) of(O) the(O) brevity(O) penalty(O) insofar(O) as(O) small(O) variations(O) in(O) translation(O) length(O) do(O) not(O) impact(O) the(O) overall(O) score(O) as(O) much(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, algorithm, product, organization, programming language, field, location, conference, country, metric, university, person, researcher and O.\nSentence: NIST also differs from Bilingual evaluation understudy in its calculation of the brevity penalty insofar as small variations in translation length do not impact the overall score as much .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["NIST","also","differs","from","Bilingual","evaluation","understudy","in","its","calculation","of","the","brevity","penalty","insofar","as","small","variations","in","translation","length","do","not","impact","the","overall","score","as","much","."],"labels":["B-metric","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","algorithm","product","organization","programming_language","field","location","conference","country","metric","university","person","researcher"]}
{"id":"7","dataset":"crossner_ai","split":"test","instance":{"id":"7","prompt_labels":"The(O) model(O) is(O) initially(O) fit(O) on(O) a(O) training(O) dataset(O) ,(O) The(O) model(O) ((O) e.g.(O) a(O) neural(B-algorithm) net(I-algorithm) or(O) a(O) naive(B-algorithm) Bayes(I-algorithm) classifier(I-algorithm) )(O) is(O) trained(O) on(O) the(O) training(O) dataset(O) using(O) a(O) supervised(B-field) learning(I-field) method(O) ,(O) for(O) example(O) using(O) optimization(O) methods(O) such(O) as(O) gradient(B-algorithm) descent(I-algorithm) or(O) stochastic(B-algorithm) gradient(I-algorithm) descent(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, conference, metric, programming language, task, country, algorithm, product, field, organization, location, university, researcher and O.\nSentence: The model is initially fit on a training dataset , The model ( e.g. a neural net or a naive Bayes classifier ) is trained on the training dataset using a supervised learning method , for example using optimization methods such as gradient descent or stochastic gradient descent .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","model","is","initially","fit","on","a","training","dataset",",","The","model","(","e.g.","a","neural","net","or","a","naive","Bayes","classifier",")","is","trained","on","the","training","dataset","using","a","supervised","learning","method",",","for","example","using","optimization","methods","such","as","gradient","descent","or","stochastic","gradient","descent","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","B-algorithm","I-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["person","conference","metric","programming_language","task","country","algorithm","product","field","organization","location","university","researcher"]}
{"id":"8","dataset":"crossner_ai","split":"test","instance":{"id":"8","prompt_labels":"FrameNet(B-product) has(O) been(O) used(O) in(O) applications(O) like(O) question(B-task) answering(I-task) ,(O) paraphrasing(B-task) ,(O) recognizing(B-task) textual(I-task) entailment(I-task) ,(O) and(O) information(B-task) extraction(I-task) ,(O) either(O) directly(O) or(O) by(O) means(O) of(O) Semantic(B-task) Role(I-task) Labeling(I-task) tools(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, university, field, country, algorithm, researcher, product, location, programming language, metric, task, person, organization and O.\nSentence: FrameNet has been used in applications like question answering , paraphrasing , recognizing textual entailment , and information extraction , either directly or by means of Semantic Role Labeling tools .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["FrameNet","has","been","used","in","applications","like","question","answering",",","paraphrasing",",","recognizing","textual","entailment",",","and","information","extraction",",","either","directly","or","by","means","of","Semantic","Role","Labeling","tools","."],"labels":["B-product","O","O","O","O","O","O","B-task","I-task","O","B-task","O","B-task","I-task","I-task","O","O","B-task","I-task","O","O","O","O","O","O","O","B-task","I-task","I-task","O","O"],"target_index":null,"target_label":null},"label_list":["conference","university","field","country","algorithm","researcher","product","location","programming_language","metric","task","person","organization"]}
{"id":"20","dataset":"crossner_ai","split":"test","instance":{"id":"20","prompt_labels":"Decision(B-algorithm) tree(I-algorithm) learning(O) is(O) one(O) of(O) the(O) predictive(O) modeling(O) approaches(O) used(O) in(O) statistics(B-field) ,(O) data(B-field) mining(I-field) and(O) machine(B-field) learning(I-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, researcher, location, task, programming language, conference, algorithm, product, country, person, metric, field, organization and O.\nSentence: Decision tree learning is one of the predictive modeling approaches used in statistics , data mining and machine learning .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Decision","tree","learning","is","one","of","the","predictive","modeling","approaches","used","in","statistics",",","data","mining","and","machine","learning","."],"labels":["B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","B-field","O","B-field","I-field","O","B-field","I-field","O"],"target_index":null,"target_label":null},"label_list":["university","researcher","location","task","programming_language","conference","algorithm","product","country","person","metric","field","organization"]}
{"id":"25","dataset":"crossner_ai","split":"test","instance":{"id":"25","prompt_labels":"The(O) majority(O) are(O) results(O) of(O) the(O) word2vec(B-product) model(I-product) developed(O) by(O) Mikolov(B-researcher) et(O) al(O) or(O) variants(O) of(O) word2vec(B-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, task, algorithm, researcher, university, country, product, field, conference, location, organization, person, metric and O.\nSentence: The majority are results of the word2vec model developed by Mikolov et al or variants of word2vec .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","majority","are","results","of","the","word2vec","model","developed","by","Mikolov","et","al","or","variants","of","word2vec","."],"labels":["O","O","O","O","O","O","B-product","I-product","O","O","B-researcher","O","O","O","O","O","B-product","O"],"target_index":null,"target_label":null},"label_list":["programming_language","task","algorithm","researcher","university","country","product","field","conference","location","organization","person","metric"]}
{"id":"30","dataset":"crossner_ai","split":"test","instance":{"id":"30","prompt_labels":"An(O) overview(O) of(O) calibration(O) methods(O) for(O) binary(B-task) classification(I-task) and(O) multiclass(B-task) classification(I-task) classification(I-task) tasks(I-task) is(O) given(O) by(O) Gebel(B-researcher) ((O) 2009(O) )(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, metric, location, conference, task, programming language, person, university, researcher, algorithm, organization, country, product and O.\nSentence: An overview of calibration methods for binary classification and multiclass classification classification tasks is given by Gebel ( 2009 )","prediction_output":null,"prediction_outputs":null,"group":null,"words":["An","overview","of","calibration","methods","for","binary","classification","and","multiclass","classification","classification","tasks","is","given","by","Gebel","(","2009",")"],"labels":["O","O","O","O","O","O","B-task","I-task","O","B-task","I-task","I-task","I-task","O","O","O","B-researcher","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","metric","location","conference","task","programming_language","person","university","researcher","algorithm","organization","country","product"]}
{"id":"33","dataset":"crossner_ai","split":"test","instance":{"id":"33","prompt_labels":"Johnson-Laird(B-researcher) is(O) a(O) Fellow(O) of(O) the(O) American(B-organization) Philosophical(I-organization) Society(I-organization) ,(O) a(O) Fellow(O) of(O) the(O) Royal(B-organization) Society(I-organization) ,(O) a(O) Fellow(O) of(O) the(O) British(B-organization) Academy(I-organization) ,(O) a(O) William(B-researcher) James(I-researcher) Fellow(O) of(O) the(O) Association(B-organization) for(I-organization) Psychological(I-organization) Science(I-organization) ,(O) and(O) a(O) Fellow(O) of(O) the(O) Cognitive(B-organization) Science(I-organization) Society(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, programming language, person, country, algorithm, location, field, researcher, organization, university, conference, metric, task and O.\nSentence: Johnson-Laird is a Fellow of the American Philosophical Society , a Fellow of the Royal Society , a Fellow of the British Academy , a William James Fellow of the Association for Psychological Science , and a Fellow of the Cognitive Science Society .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Johnson-Laird","is","a","Fellow","of","the","American","Philosophical","Society",",","a","Fellow","of","the","Royal","Society",",","a","Fellow","of","the","British","Academy",",","a","William","James","Fellow","of","the","Association","for","Psychological","Science",",","and","a","Fellow","of","the","Cognitive","Science","Society","."],"labels":["B-researcher","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","B-organization","I-organization","O","O","B-researcher","I-researcher","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["product","programming_language","person","country","algorithm","location","field","researcher","organization","university","conference","metric","task"]}
{"id":"35","dataset":"crossner_ai","split":"test","instance":{"id":"35","prompt_labels":"BLEU(B-metric) uses(O) a(O) modified(O) form(O) of(O) precision(B-metric) to(O) compare(O) a(O) candidate(O) translation(O) against(O) multiple(O) reference(O) translations(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, country, algorithm, metric, location, person, programming language, organization, conference, product, university, task, field and O.\nSentence: BLEU uses a modified form of precision to compare a candidate translation against multiple reference translations .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["BLEU","uses","a","modified","form","of","precision","to","compare","a","candidate","translation","against","multiple","reference","translations","."],"labels":["B-metric","O","O","O","O","O","B-metric","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","country","algorithm","metric","location","person","programming_language","organization","conference","product","university","task","field"]}
{"id":"36","dataset":"crossner_ai","split":"test","instance":{"id":"36","prompt_labels":"For(O) the(O) case(O) of(O) a(O) general(O) base(O) space(O) math(O) ((O) Y(O) ,(O) \\(O) mathcal(O) {(O) B(O) }(O) ,(O) \\(O) nu(O) )(O) /(O) math(O) ((O) i.e.(O) a(O) base(O) space(O) which(O) is(O) not(O) countable(O) )(O) ,(O) one(O) typically(O) considers(O) the(O) relative(B-metric) entropy(I-metric) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, programming language, university, organization, algorithm, person, location, field, product, metric, country, researcher, conference and O.\nSentence: For the case of a general base space math ( Y , \\ mathcal { B } , \\ nu ) / math ( i.e. a base space which is not countable ) , one typically considers the relative entropy .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","the","case","of","a","general","base","space","math","(","Y",",","\\","mathcal","{","B","}",",","\\","nu",")","/","math","(","i.e.","a","base","space","which","is","not","countable",")",",","one","typically","considers","the","relative","entropy","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","O"],"target_index":null,"target_label":null},"label_list":["task","programming_language","university","organization","algorithm","person","location","field","product","metric","country","researcher","conference"]}
{"id":"44","dataset":"crossner_ai","split":"test","instance":{"id":"44","prompt_labels":"The(O) ImageNet(B-conference) Large(I-conference) Scale(I-conference) Visual(I-conference) Recognition(I-conference) Challenge(I-conference) is(O) a(O) benchmark(O) in(O) object(B-task) classification(I-task) and(I-task) detection(I-task) ,(O) with(O) millions(O) of(O) images(O) and(O) hundreds(O) of(O) object(O) classes(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, metric, product, field, organization, programming language, researcher, country, conference, person, algorithm, location, task and O.\nSentence: The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object classification and detection , with millions of images and hundreds of object classes .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","ImageNet","Large","Scale","Visual","Recognition","Challenge","is","a","benchmark","in","object","classification","and","detection",",","with","millions","of","images","and","hundreds","of","object","classes","."],"labels":["O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","O","O","O","B-task","I-task","I-task","I-task","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","metric","product","field","organization","programming_language","researcher","country","conference","person","algorithm","location","task"]}
{"id":"45","dataset":"crossner_ai","split":"test","instance":{"id":"45","prompt_labels":"Bengio(B-researcher) ,(O) together(O) with(O) Geoffrey(B-researcher) Hinton(I-researcher) and(O) Yann(B-researcher) LeCun(I-researcher) ,(O) are(O) referred(O) to(O) by(O) some(O) as(O) the(O) Godfathers(O) of(O) AI(O) and(O) Godfathers(O) of(O) Deep(O) Learning(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, field, country, person, metric, task, location, conference, researcher, programming language, product, organization, algorithm and O.\nSentence: Bengio , together with Geoffrey Hinton and Yann LeCun , are referred to by some as the Godfathers of AI and Godfathers of Deep Learning .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Bengio",",","together","with","Geoffrey","Hinton","and","Yann","LeCun",",","are","referred","to","by","some","as","the","Godfathers","of","AI","and","Godfathers","of","Deep","Learning","."],"labels":["B-researcher","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","field","country","person","metric","task","location","conference","researcher","programming_language","product","organization","algorithm"]}
{"id":"47","dataset":"crossner_ai","split":"test","instance":{"id":"47","prompt_labels":"NSA(B-organization) Bethesda(I-organization) is(O) responsible(O) for(O) base(O) operational(O) support(O) for(O) its(O) major(O) tenant(O) ,(O) the(O) Walter(B-organization) Reed(I-organization) National(I-organization) Military(I-organization) Medical(I-organization) Center(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, conference, country, university, metric, product, programming language, person, researcher, location, field, task, algorithm and O.\nSentence: NSA Bethesda is responsible for base operational support for its major tenant , the Walter Reed National Military Medical Center .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["NSA","Bethesda","is","responsible","for","base","operational","support","for","its","major","tenant",",","the","Walter","Reed","National","Military","Medical","Center","."],"labels":["B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["organization","conference","country","university","metric","product","programming_language","person","researcher","location","field","task","algorithm"]}
{"id":"51","dataset":"crossner_ai","split":"test","instance":{"id":"51","prompt_labels":"However(O) ,(O) by(O) formulating(O) the(O) problem(O) as(O) the(O) solution(O) of(O) a(O) Toeplitz(O) matrix(O) and(O) using(O) Levinson(B-algorithm) recursion(I-algorithm) ,(O) we(O) can(O) relatively(O) quickly(O) estimate(O) a(O) filter(O) with(O) the(O) smallest(O) mean(B-metric) squared(I-metric) error(I-metric) possible(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, task, country, product, location, university, person, field, organization, algorithm, programming language, metric, researcher and O.\nSentence: However , by formulating the problem as the solution of a Toeplitz matrix and using Levinson recursion , we can relatively quickly estimate a filter with the smallest mean squared error possible .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["However",",","by","formulating","the","problem","as","the","solution","of","a","Toeplitz","matrix","and","using","Levinson","recursion",",","we","can","relatively","quickly","estimate","a","filter","with","the","smallest","mean","squared","error","possible","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O"],"target_index":null,"target_label":null},"label_list":["conference","task","country","product","location","university","person","field","organization","algorithm","programming_language","metric","researcher"]}
{"id":"52","dataset":"crossner_ai","split":"test","instance":{"id":"52","prompt_labels":"In(O) July(O) 2011(O) the(O) 15th(B-conference) edition(I-conference) of(I-conference) Campus(I-conference) Party(I-conference) Spain(I-conference) will(O) be(O) held(O) at(O) the(O) City(B-location) of(I-location) Arts(I-location) and(I-location) Sciences(I-location) in(O) Valencia(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, location, product, metric, country, conference, field, person, organization, researcher, task, algorithm, university and O.\nSentence: In July 2011 the 15th edition of Campus Party Spain will be held at the City of Arts and Sciences in Valencia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","July","2011","the","15th","edition","of","Campus","Party","Spain","will","be","held","at","the","City","of","Arts","and","Sciences","in","Valencia","."],"labels":["O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","O","O","O","O","B-location","I-location","I-location","I-location","I-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["programming_language","location","product","metric","country","conference","field","person","organization","researcher","task","algorithm","university"]}
{"id":"63","dataset":"crossner_ai","split":"test","instance":{"id":"63","prompt_labels":"Notable(O) former(O) PhD(O) students(O) and(O) postdoctoral(O) researchers(O) from(O) his(O) group(O) include(O) Richard(B-researcher) Zemel(I-researcher) ,(O) and(O) Zoubin(B-researcher) Ghahramani(I-researcher) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, product, field, conference, country, metric, location, university, task, algorithm, organization, programming language, person and O.\nSentence: Notable former PhD students and postdoctoral researchers from his group include Richard Zemel , and Zoubin Ghahramani .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Notable","former","PhD","students","and","postdoctoral","researchers","from","his","group","include","Richard","Zemel",",","and","Zoubin","Ghahramani","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-researcher","I-researcher","O","O","B-researcher","I-researcher","O"],"target_index":null,"target_label":null},"label_list":["researcher","product","field","conference","country","metric","location","university","task","algorithm","organization","programming_language","person"]}
{"id":"76","dataset":"crossner_ai","split":"test","instance":{"id":"76","prompt_labels":"Scientific(O) conferences(O) where(O) vision(B-task) based(I-task) activity(I-task) recognition(I-task) work(O) often(O) appears(O) are(O) ICCV(B-conference) and(O) CVPR(B-conference) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, person, conference, researcher, organization, algorithm, university, country, field, product, location, task, metric and O.\nSentence: Scientific conferences where vision based activity recognition work often appears are ICCV and CVPR .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Scientific","conferences","where","vision","based","activity","recognition","work","often","appears","are","ICCV","and","CVPR","."],"labels":["O","O","O","B-task","I-task","I-task","I-task","O","O","O","O","B-conference","O","B-conference","O"],"target_index":null,"target_label":null},"label_list":["programming_language","person","conference","researcher","organization","algorithm","university","country","field","product","location","task","metric"]}
{"id":"77","dataset":"crossner_ai","split":"test","instance":{"id":"77","prompt_labels":"In(O) statistics(B-field) ,(O) an(O) expectation-maximization(B-algorithm) ((O) EM(B-algorithm) )(O) algorithm(O) is(O) an(O) iterative(O) method(O) to(O) find(O) maximum(B-metric) likelihood(I-metric) or(O) maximum(B-metric) a(I-metric) posteriori(I-metric) ((O) MAP(B-metric) )(O) estimates(O) of(O) parameter(O) s(O) in(O) statistical(O) model(O) s(O) ,(O) where(O) the(O) model(O) depends(O) on(O) unobserved(O) latent(O) variable(O) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, conference, field, task, country, university, location, algorithm, programming language, metric, researcher, person, product and O.\nSentence: In statistics , an expectation-maximization ( EM ) algorithm is an iterative method to find maximum likelihood or maximum a posteriori ( MAP ) estimates of parameter s in statistical model s , where the model depends on unobserved latent variable s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","statistics",",","an","expectation-maximization","(","EM",")","algorithm","is","an","iterative","method","to","find","maximum","likelihood","or","maximum","a","posteriori","(","MAP",")","estimates","of","parameter","s","in","statistical","model","s",",","where","the","model","depends","on","unobserved","latent","variable","s","."],"labels":["O","B-field","O","O","B-algorithm","O","B-algorithm","O","O","O","O","O","O","O","O","B-metric","I-metric","O","B-metric","I-metric","I-metric","O","B-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","conference","field","task","country","university","location","algorithm","programming_language","metric","researcher","person","product"]}
{"id":"79","dataset":"crossner_ai","split":"test","instance":{"id":"79","prompt_labels":"The(O) concept(O) is(O) similar(O) to(O) the(O) signal(B-metric) to(I-metric) noise(I-metric) ratio(I-metric) used(O) in(O) the(O) sciences(B-field) and(O) confusion(B-metric) matrix(I-metric) used(O) in(O) artificial(B-field) intelligence(I-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, university, metric, organization, country, person, algorithm, product, field, task, conference, location, programming language and O.\nSentence: The concept is similar to the signal to noise ratio used in the sciences and confusion matrix used in artificial intelligence .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","concept","is","similar","to","the","signal","to","noise","ratio","used","in","the","sciences","and","confusion","matrix","used","in","artificial","intelligence","."],"labels":["O","O","O","O","O","O","B-metric","I-metric","I-metric","I-metric","O","O","O","B-field","O","B-metric","I-metric","O","O","B-field","I-field","O"],"target_index":null,"target_label":null},"label_list":["researcher","university","metric","organization","country","person","algorithm","product","field","task","conference","location","programming_language"]}
{"id":"94","dataset":"crossner_ai","split":"test","instance":{"id":"94","prompt_labels":"In(O) 2009(O) ,(O) experts(O) attended(O) a(O) conference(O) hosted(O) by(O) the(O) Association(B-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Artificial(I-conference) Intelligence(I-conference) ((O) AAAI(B-conference) )(O) to(O) discuss(O) whether(O) computers(O) and(O) robots(O) might(O) be(O) able(O) to(O) acquire(O) any(O) autonomy(O) ,(O) and(O) how(O) much(O) these(O) abilities(O) might(O) pose(O) a(O) threat(O) or(O) hazard(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, organization, person, product, metric, task, field, university, researcher, location, conference, programming language, country and O.\nSentence: In 2009 , experts attended a conference hosted by the Association for the Advancement of Artificial Intelligence ( AAAI ) to discuss whether computers and robots might be able to acquire any autonomy , and how much these abilities might pose a threat or hazard .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2009",",","experts","attended","a","conference","hosted","by","the","Association","for","the","Advancement","of","Artificial","Intelligence","(","AAAI",")","to","discuss","whether","computers","and","robots","might","be","able","to","acquire","any","autonomy",",","and","how","much","these","abilities","might","pose","a","threat","or","hazard","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","organization","person","product","metric","task","field","university","researcher","location","conference","programming_language","country"]}
{"id":"101","dataset":"crossner_ai","split":"test","instance":{"id":"101","prompt_labels":"This(O) tends(O) to(O) yield(O) very(O) large(O) performance(O) gains(O) when(O) working(O) with(O) large(O) corpora(O) such(O) as(O) WordNet(B-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, programming language, product, location, field, country, task, conference, person, organization, algorithm, university, metric and O.\nSentence: This tends to yield very large performance gains when working with large corpora such as WordNet .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","tends","to","yield","very","large","performance","gains","when","working","with","large","corpora","such","as","WordNet","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","O"],"target_index":null,"target_label":null},"label_list":["researcher","programming_language","product","location","field","country","task","conference","person","organization","algorithm","university","metric"]}
{"id":"104","dataset":"crossner_ai","split":"test","instance":{"id":"104","prompt_labels":",(O) Ltd.(O) in(O) Thailand(B-country) ;(O) Komatsu(B-organization) ((I-organization) Shanghai(I-organization) )(I-organization) Ltd.(I-organization) in(O) 1996(O) in(O) Shanghai(B-location) ,(O) China(B-country) ;(O) Industrial(B-organization) Power(I-organization) Alliance(I-organization) Ltd.(I-organization) in(O) Japan(B-country) ,(O) a(O) joint(O) venture(O) with(O) Cummins(B-organization) ,(O) in(O) 1998(O) ;(O) L(B-organization) &(I-organization) T-Komatsu(I-organization) Limited(I-organization) in(O) India(B-country) in(O) 1998(O) ((O) shares(O) sold(O) in(O) 2013(O) )(O) ;(O) and(O) Komatsu(B-organization) Brasil(I-organization) International(I-organization) Ltda.(I-organization) in(O) Brazil(B-country) in(O) 1998(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, university, country, organization, person, product, task, field, researcher, location, conference, metric, programming language and O.\nSentence: , Ltd. in Thailand ; Komatsu ( Shanghai ) Ltd. in 1996 in Shanghai , China ; Industrial Power Alliance Ltd. in Japan , a joint venture with Cummins , in 1998 ; L & T-Komatsu Limited in India in 1998 ( shares sold in 2013 ) ; and Komatsu Brasil International Ltda. in Brazil in 1998 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":[",","Ltd.","in","Thailand",";","Komatsu","(","Shanghai",")","Ltd.","in","1996","in","Shanghai",",","China",";","Industrial","Power","Alliance","Ltd.","in","Japan",",","a","joint","venture","with","Cummins",",","in","1998",";","L","&","T-Komatsu","Limited","in","India","in","1998","(","shares","sold","in","2013",")",";","and","Komatsu","Brasil","International","Ltda.","in","Brazil","in","1998","."],"labels":["O","O","O","B-country","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-location","O","B-country","O","B-organization","I-organization","I-organization","I-organization","O","B-country","O","O","O","O","O","B-organization","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-country","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-country","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","university","country","organization","person","product","task","field","researcher","location","conference","metric","programming_language"]}
{"id":"110","dataset":"crossner_ai","split":"test","instance":{"id":"110","prompt_labels":"An(O) articulated(B-product) robot(I-product) is(O) a(O) robot(O) with(O) rotary(O) joint(O) s(O) ((O) e.g.(O) a(O) legged(O) robot(O) or(O) an(O) industrial(B-product) robot(I-product) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, field, university, researcher, product, metric, organization, conference, task, person, algorithm, programming language and O.\nSentence: An articulated robot is a robot with rotary joint s ( e.g. a legged robot or an industrial robot ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["An","articulated","robot","is","a","robot","with","rotary","joint","s","(","e.g.","a","legged","robot","or","an","industrial","robot",")","."],"labels":["O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","I-product","O","O"],"target_index":null,"target_label":null},"label_list":["country","location","field","university","researcher","product","metric","organization","conference","task","person","algorithm","programming_language"]}
{"id":"115","dataset":"crossner_ai","split":"test","instance":{"id":"115","prompt_labels":"KUKA(B-organization) is(O) a(O) German(O) manufacturer(O) of(O) industrial(B-product) robot(I-product) s(O) and(O) solution(O) s(O) for(O) factory(O) automation(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, person, field, conference, algorithm, location, researcher, organization, task, programming language, university, product, country and O.\nSentence: KUKA is a German manufacturer of industrial robot s and solution s for factory automation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["KUKA","is","a","German","manufacturer","of","industrial","robot","s","and","solution","s","for","factory","automation","."],"labels":["B-organization","O","O","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","person","field","conference","algorithm","location","researcher","organization","task","programming_language","university","product","country"]}
{"id":"116","dataset":"crossner_ai","split":"test","instance":{"id":"116","prompt_labels":"Other(O) films(O) between(O) 2016(O) to(O) 2020(O) that(O) captured(O) with(O) IMAX(O) camera(O) 's(O) were(O) Zack(B-person) Snyder(I-person) '(O) s(O) Batman(O) v(O) Superman(O) :(O) Dawn(O) of(O) Justice(O) ,(O) Clint(B-person) Eastwood(I-person) '(O) s(O) Sully(O) ,(O) Damien(B-person) Chazelle(I-person) '(O) s(O) First(O) Man(O) ,(O) Patty(B-person) Jenkins(I-person) '(O) Wonder(O) Woman(O) 1984(O) ,(O) Cary(B-person) Joji(I-person) Fukunaga(I-person) '(O) s(O) No(O) Time(O) to(O) Die(O) and(O) Joseph(B-person) Kosinski(I-person) '(O) s(O) Top(O) Gun(O) :(O) Maverick(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, algorithm, task, metric, country, product, researcher, organization, person, university, programming language, location, field and O.\nSentence: Other films between 2016 to 2020 that captured with IMAX camera 's were Zack Snyder ' s Batman v Superman : Dawn of Justice , Clint Eastwood ' s Sully , Damien Chazelle ' s First Man , Patty Jenkins ' Wonder Woman 1984 , Cary Joji Fukunaga ' s No Time to Die and Joseph Kosinski ' s Top Gun : Maverick .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","films","between","2016","to","2020","that","captured","with","IMAX","camera","'s","were","Zack","Snyder","'","s","Batman","v","Superman",":","Dawn","of","Justice",",","Clint","Eastwood","'","s","Sully",",","Damien","Chazelle","'","s","First","Man",",","Patty","Jenkins","'","Wonder","Woman","1984",",","Cary","Joji","Fukunaga","'","s","No","Time","to","Die","and","Joseph","Kosinski","'","s","Top","Gun",":","Maverick","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","B-person","I-person","O","O","O","O","O","B-person","I-person","O","O","O","O","O","B-person","I-person","I-person","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["conference","algorithm","task","metric","country","product","researcher","organization","person","university","programming_language","location","field"]}
{"id":"128","dataset":"crossner_ai","split":"test","instance":{"id":"128","prompt_labels":"The(O) NeuralExpert(O) centers(O) the(O) design(O) specifications(O) around(O) the(O) type(O) of(O) problem(O) the(O) user(O) would(O) like(O) the(O) neural(B-algorithm) network(I-algorithm) to(O) solve(O) ((O) Classification(B-task) ,(O) Prediction(B-task) ,(O) Function(B-task) approximation(I-task) or(O) Cluster(B-task) analysis(I-task) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, field, person, researcher, location, programming language, algorithm, organization, country, conference, product, university, task and O.\nSentence: The NeuralExpert centers the design specifications around the type of problem the user would like the neural network to solve ( Classification , Prediction , Function approximation or Cluster analysis ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","NeuralExpert","centers","the","design","specifications","around","the","type","of","problem","the","user","would","like","the","neural","network","to","solve","(","Classification",",","Prediction",",","Function","approximation","or","Cluster","analysis",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","B-task","O","B-task","O","B-task","I-task","O","B-task","I-task","O","O"],"target_index":null,"target_label":null},"label_list":["metric","field","person","researcher","location","programming_language","algorithm","organization","country","conference","product","university","task"]}
{"id":"137","dataset":"crossner_ai","split":"test","instance":{"id":"137","prompt_labels":"For(O) example(O) ,(O) speech(B-task) synthesis(I-task) ,(O) combined(O) with(O) speech(B-task) recognition(I-task) ,(O) allows(O) for(O) interaction(O) with(O) mobile(O) devices(O) via(O) language(B-field) processing(I-field) interfaces(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, programming language, location, task, university, person, algorithm, metric, country, conference, researcher, field, product and O.\nSentence: For example , speech synthesis , combined with speech recognition , allows for interaction with mobile devices via language processing interfaces .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","example",",","speech","synthesis",",","combined","with","speech","recognition",",","allows","for","interaction","with","mobile","devices","via","language","processing","interfaces","."],"labels":["O","O","O","B-task","I-task","O","O","O","B-task","I-task","O","O","O","O","O","O","O","O","B-field","I-field","O","O"],"target_index":null,"target_label":null},"label_list":["organization","programming_language","location","task","university","person","algorithm","metric","country","conference","researcher","field","product"]}
{"id":"138","dataset":"crossner_ai","split":"test","instance":{"id":"138","prompt_labels":"Phidgets(B-product) can(O) be(O) programmed(O) using(O) a(O) variety(O) of(O) software(O) and(O) programming(O) languages(O) ,(O) ranging(O) from(O) Java(B-programming language) to(O) Microsoft(B-product) Excel(I-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, product, university, location, country, task, researcher, field, person, programming language, conference, metric, organization and O.\nSentence: Phidgets can be programmed using a variety of software and programming languages , ranging from Java to Microsoft Excel .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Phidgets","can","be","programmed","using","a","variety","of","software","and","programming","languages",",","ranging","from","Java","to","Microsoft","Excel","."],"labels":["B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-programming language","O","B-product","I-product","O"],"target_index":null,"target_label":null},"label_list":["algorithm","product","university","location","country","task","researcher","field","person","programming_language","conference","metric","organization"]}
{"id":"144","dataset":"crossner_ai","split":"test","instance":{"id":"144","prompt_labels":"To(O) be(O) specific(O) ,(O) the(O) BIBO(B-metric) stability(I-metric) criterion(I-metric) requires(O) that(O) the(O) ROC(B-metric) of(O) the(O) system(O) includes(O) the(O) unit(O) circle(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, programming language, researcher, conference, field, organization, person, university, metric, task, product, location, algorithm and O.\nSentence: To be specific , the BIBO stability criterion requires that the ROC of the system includes the unit circle .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["To","be","specific",",","the","BIBO","stability","criterion","requires","that","the","ROC","of","the","system","includes","the","unit","circle","."],"labels":["O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","B-metric","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","programming_language","researcher","conference","field","organization","person","university","metric","task","product","location","algorithm"]}
{"id":"150","dataset":"crossner_ai","split":"test","instance":{"id":"150","prompt_labels":"The(O) Viterbi(B-algorithm) algorithm(I-algorithm) is(O) a(O) dynamic(B-algorithm) programming(I-algorithm) algorithm(I-algorithm) for(O) finding(O) the(O) most(O) likely(O) sequence(O) of(O) hidden(O) states(O) called(O) the(O) Viterbi(O) path(O) that(O) results(O) in(O) a(O) sequence(O) of(O) observed(O) events(O) ,(O) especially(O) in(O) the(O) context(O) of(O) Markov(O) information(O) source(O) s(O) and(O) hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) s(O) ((O) HMM(B-algorithm) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, algorithm, metric, university, country, person, programming language, task, conference, field, location, organization, researcher and O.\nSentence: The Viterbi algorithm is a dynamic programming algorithm for finding the most likely sequence of hidden states called the Viterbi path that results in a sequence of observed events , especially in the context of Markov information source s and hidden Markov model s ( HMM ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Viterbi","algorithm","is","a","dynamic","programming","algorithm","for","finding","the","most","likely","sequence","of","hidden","states","called","the","Viterbi","path","that","results","in","a","sequence","of","observed","events",",","especially","in","the","context","of","Markov","information","source","s","and","hidden","Markov","model","s","(","HMM",")","."],"labels":["O","B-algorithm","I-algorithm","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","B-algorithm","O","O"],"target_index":null,"target_label":null},"label_list":["product","algorithm","metric","university","country","person","programming_language","task","conference","field","location","organization","researcher"]}
{"id":"160","dataset":"crossner_ai","split":"test","instance":{"id":"160","prompt_labels":"Important(O) journals(O) include(O) the(O) IEEE(B-conference) Transactions(I-conference) on(I-conference) Speech(I-conference) and(I-conference) Audio(I-conference) Processing(I-conference) ((O) later(O) renamed(O) IEEE(B-conference) Transactions(I-conference) on(I-conference) Audio(I-conference) ,(I-conference) Speech(I-conference) and(I-conference) Language(I-conference) Processing(I-conference) and(O) since(O) Sept(O) 2014(O) renamed(O) IEEE(B-conference) /(I-conference) ACM(I-conference) Transactions(I-conference) on(I-conference) Audio(I-conference) ,(I-conference) Speech(I-conference) and(I-conference) Language(I-conference) Processing(I-conference) -(O) after(O) merging(O) with(O) an(O) ACM(B-conference) publication(O) )(O) ,(O) Computer(B-conference) Speech(I-conference) and(I-conference) Language(I-conference) ,(O) and(O) Speech(B-conference) Communication(I-conference) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, researcher, programming language, field, task, organization, country, university, conference, person, algorithm, metric, location and O.\nSentence: Important journals include the IEEE Transactions on Speech and Audio Processing ( later renamed IEEE Transactions on Audio , Speech and Language Processing and since Sept 2014 renamed IEEE / ACM Transactions on Audio , Speech and Language Processing - after merging with an ACM publication ) , Computer Speech and Language , and Speech Communication .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Important","journals","include","the","IEEE","Transactions","on","Speech","and","Audio","Processing","(","later","renamed","IEEE","Transactions","on","Audio",",","Speech","and","Language","Processing","and","since","Sept","2014","renamed","IEEE","/","ACM","Transactions","on","Audio",",","Speech","and","Language","Processing","-","after","merging","with","an","ACM","publication",")",",","Computer","Speech","and","Language",",","and","Speech","Communication","."],"labels":["O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","O","O","O","O","B-conference","O","O","O","B-conference","I-conference","I-conference","I-conference","O","O","B-conference","I-conference","O"],"target_index":null,"target_label":null},"label_list":["product","researcher","programming_language","field","task","organization","country","university","conference","person","algorithm","metric","location"]}
{"id":"161","dataset":"crossner_ai","split":"test","instance":{"id":"161","prompt_labels":"EM(B-algorithm) is(O) frequently(O) used(O) for(O) data(B-task) clustering(I-task) in(O) machine(B-field) learning(I-field) and(O) computer(B-field) vision(I-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, product, organization, programming language, university, researcher, field, location, person, conference, algorithm, country, metric and O.\nSentence: EM is frequently used for data clustering in machine learning and computer vision .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["EM","is","frequently","used","for","data","clustering","in","machine","learning","and","computer","vision","."],"labels":["B-algorithm","O","O","O","O","B-task","I-task","O","B-field","I-field","O","B-field","I-field","O"],"target_index":null,"target_label":null},"label_list":["task","product","organization","programming_language","university","researcher","field","location","person","conference","algorithm","country","metric"]}
{"id":"173","dataset":"crossner_ai","split":"test","instance":{"id":"173","prompt_labels":"The(O) Association(B-conference) for(I-conference) Computational(I-conference) Linguistics(I-conference) defines(O) computational(B-field) linguistics(I-field) as(O) :(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, product, researcher, organization, field, algorithm, university, country, metric, person, conference, task, programming language and O.\nSentence: The Association for Computational Linguistics defines computational linguistics as :","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Association","for","Computational","Linguistics","defines","computational","linguistics","as",":"],"labels":["O","B-conference","I-conference","I-conference","I-conference","O","B-field","I-field","O","O"],"target_index":null,"target_label":null},"label_list":["location","product","researcher","organization","field","algorithm","university","country","metric","person","conference","task","programming_language"]}
{"id":"174","dataset":"crossner_ai","split":"test","instance":{"id":"174","prompt_labels":"Expectation-maximization(B-algorithm) algorithm(I-algorithm) s(O) may(O) be(O) employed(O) to(O) calculate(O) approximate(O) maximum(B-algorithm) likelihood(I-algorithm) estimates(I-algorithm) of(O) unknown(O) state-space(O) parameters(O) within(O) minimum-variance(O) filters(O) and(O) smoothers(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, university, researcher, algorithm, task, conference, product, metric, country, organization, programming language, person, location and O.\nSentence: Expectation-maximization algorithm s may be employed to calculate approximate maximum likelihood estimates of unknown state-space parameters within minimum-variance filters and smoothers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Expectation-maximization","algorithm","s","may","be","employed","to","calculate","approximate","maximum","likelihood","estimates","of","unknown","state-space","parameters","within","minimum-variance","filters","and","smoothers","."],"labels":["B-algorithm","I-algorithm","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","university","researcher","algorithm","task","conference","product","metric","country","organization","programming_language","person","location"]}
{"id":"175","dataset":"crossner_ai","split":"test","instance":{"id":"175","prompt_labels":"Correspondents(O) included(O) former(O) Baywatch(O) actresses(O) Donna(B-person) D(I-person) 'Errico(I-person) ,(O) Carmen(B-person) Electra(I-person) ,(O) and(O) Traci(B-person) Bingham(I-person) ,(O) former(O) Playboy(O) Playmate(O) Heidi(B-person) Mark(I-person) ,(O) comedian(O) Arj(B-person) Barker(I-person) and(O) identical(O) twins(O) Randy(B-person) and(O) Jason(B-person) Sklar(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, task, programming language, field, organization, algorithm, country, product, conference, metric, person, researcher, location and O.\nSentence: Correspondents included former Baywatch actresses Donna D 'Errico , Carmen Electra , and Traci Bingham , former Playboy Playmate Heidi Mark , comedian Arj Barker and identical twins Randy and Jason Sklar .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Correspondents","included","former","Baywatch","actresses","Donna","D","'Errico",",","Carmen","Electra",",","and","Traci","Bingham",",","former","Playboy","Playmate","Heidi","Mark",",","comedian","Arj","Barker","and","identical","twins","Randy","and","Jason","Sklar","."],"labels":["O","O","O","O","O","B-person","I-person","I-person","O","B-person","I-person","O","O","B-person","I-person","O","O","O","O","B-person","I-person","O","O","B-person","I-person","O","O","O","B-person","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["university","task","programming_language","field","organization","algorithm","country","product","conference","metric","person","researcher","location"]}
{"id":"182","dataset":"crossner_ai","split":"test","instance":{"id":"182","prompt_labels":"Such(O) a(O) sequence(O) ((O) which(O) depends(O) on(O) the(O) outcome(O) of(O) the(O) investigation(O) of(O) previous(O) attributes(O) at(O) each(O) stage(O) )(O) is(O) called(O) a(O) decision(B-algorithm) tree(I-algorithm) and(O) applied(O) in(O) the(O) area(O) of(O) machine(B-field) learning(I-field) known(O) as(O) decision(B-algorithm) tree(I-algorithm) learning(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, algorithm, country, location, field, product, programming language, organization, person, university, task, metric, researcher and O.\nSentence: Such a sequence ( which depends on the outcome of the investigation of previous attributes at each stage ) is called a decision tree and applied in the area of machine learning known as decision tree learning .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Such","a","sequence","(","which","depends","on","the","outcome","of","the","investigation","of","previous","attributes","at","each","stage",")","is","called","a","decision","tree","and","applied","in","the","area","of","machine","learning","known","as","decision","tree","learning","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","O","O","O","B-field","I-field","O","O","B-algorithm","I-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["conference","algorithm","country","location","field","product","programming_language","organization","person","university","task","metric","researcher"]}
{"id":"183","dataset":"crossner_ai","split":"test","instance":{"id":"183","prompt_labels":"As(O) in(O) factor(B-task) analysis(I-task) ,(O) the(O) LCA(B-algorithm) can(O) also(O) be(O) used(O) to(O) classify(O) case(O) according(O) to(O) their(O) maximum(B-algorithm) likelihood(I-algorithm) class(O) membership(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, researcher, algorithm, task, person, university, product, conference, country, organization, location, field, metric and O.\nSentence: As in factor analysis , the LCA can also be used to classify case according to their maximum likelihood class membership .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["As","in","factor","analysis",",","the","LCA","can","also","be","used","to","classify","case","according","to","their","maximum","likelihood","class","membership","."],"labels":["O","O","B-task","I-task","O","O","B-algorithm","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","researcher","algorithm","task","person","university","product","conference","country","organization","location","field","metric"]}
{"id":"187","dataset":"crossner_ai","split":"test","instance":{"id":"187","prompt_labels":"Traditional(O) image(O) quality(O) measures(O) ,(O) such(O) as(O) PSNR(B-metric) ,(O) are(O) typically(O) performed(O) on(O) fixed(O) resolution(O) images(O) and(O) do(O) not(O) take(O) into(O) account(O) some(O) aspects(O) of(O) the(O) human(O) visual(O) system(O) ,(O) like(O) the(O) change(O) in(O) spatial(O) resolution(O) across(O) the(O) retina(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, metric, product, programming language, task, university, field, organization, conference, algorithm, person, country, researcher and O.\nSentence: Traditional image quality measures , such as PSNR , are typically performed on fixed resolution images and do not take into account some aspects of the human visual system , like the change in spatial resolution across the retina .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Traditional","image","quality","measures",",","such","as","PSNR",",","are","typically","performed","on","fixed","resolution","images","and","do","not","take","into","account","some","aspects","of","the","human","visual","system",",","like","the","change","in","spatial","resolution","across","the","retina","."],"labels":["O","O","O","O","O","O","O","B-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","metric","product","programming_language","task","university","field","organization","conference","algorithm","person","country","researcher"]}
{"id":"190","dataset":"crossner_ai","split":"test","instance":{"id":"190","prompt_labels":"Now(O) let(O) us(O) start(O) explaining(O) the(O) different(O) possible(O) relations(O) between(O) predicted(O) and(O) actual(O) outcome(O) :(O) Confusion(B-metric) matrix(I-metric)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, university, algorithm, programming language, country, metric, researcher, organization, product, conference, person, task, location and O.\nSentence: Now let us start explaining the different possible relations between predicted and actual outcome : Confusion matrix","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Now","let","us","start","explaining","the","different","possible","relations","between","predicted","and","actual","outcome",":","Confusion","matrix"],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric"],"target_index":null,"target_label":null},"label_list":["field","university","algorithm","programming_language","country","metric","researcher","organization","product","conference","person","task","location"]}
{"id":"197","dataset":"crossner_ai","split":"test","instance":{"id":"197","prompt_labels":"It(O) is(O) an(O) alternative(O) to(O) the(O) Word(B-metric) error(I-metric) rate(I-metric) ((O) Word(B-metric) Error(I-metric) Rate(I-metric) )(O) used(O) in(O) several(O) countries(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, country, field, product, location, person, university, task, programming language, organization, algorithm, conference, metric and O.\nSentence: It is an alternative to the Word error rate ( Word Error Rate ) used in several countries .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","is","an","alternative","to","the","Word","error","rate","(","Word","Error","Rate",")","used","in","several","countries","."],"labels":["O","O","O","O","O","O","B-metric","I-metric","I-metric","O","B-metric","I-metric","I-metric","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","country","field","product","location","person","university","task","programming_language","organization","algorithm","conference","metric"]}
{"id":"202","dataset":"crossner_ai","split":"test","instance":{"id":"202","prompt_labels":"Time-inhomogeneous(B-algorithm) hidden(I-algorithm) Bernoulli(I-algorithm) model(I-algorithm) ((O) TI-HBM(B-algorithm) )(O) is(O) an(O) alternative(O) to(O) hidden(B-algorithm) Markov(I-algorithm) model(I-algorithm) ((O) HMM(B-algorithm) )(O) for(O) automatic(B-task) speech(I-task) recognition(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, programming language, conference, algorithm, metric, task, location, country, field, university, person, organization, researcher and O.\nSentence: Time-inhomogeneous hidden Bernoulli model ( TI-HBM ) is an alternative to hidden Markov model ( HMM ) for automatic speech recognition .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Time-inhomogeneous","hidden","Bernoulli","model","(","TI-HBM",")","is","an","alternative","to","hidden","Markov","model","(","HMM",")","for","automatic","speech","recognition","."],"labels":["B-algorithm","I-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","B-algorithm","O","O","B-task","I-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["product","programming_language","conference","algorithm","metric","task","location","country","field","university","person","organization","researcher"]}
{"id":"205","dataset":"crossner_ai","split":"test","instance":{"id":"205","prompt_labels":"Neural(B-algorithm) network(I-algorithm) models(I-algorithm) of(O) concept(O) formation(O) and(O) the(O) structure(O) of(O) knowledge(O) have(O) opened(O) powerful(O) hierarchical(O) models(O) of(O) knowledge(O) organization(O) such(O) as(O) George(B-researcher) Miller(I-researcher) '(O) s(O) Wordnet(B-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, field, algorithm, conference, programming language, country, task, researcher, university, organization, metric, person, location and O.\nSentence: Neural network models of concept formation and the structure of knowledge have opened powerful hierarchical models of knowledge organization such as George Miller ' s Wordnet .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Neural","network","models","of","concept","formation","and","the","structure","of","knowledge","have","opened","powerful","hierarchical","models","of","knowledge","organization","such","as","George","Miller","'","s","Wordnet","."],"labels":["B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-researcher","I-researcher","O","O","B-product","O"],"target_index":null,"target_label":null},"label_list":["product","field","algorithm","conference","programming_language","country","task","researcher","university","organization","metric","person","location"]}
{"id":"207","dataset":"crossner_ai","split":"test","instance":{"id":"207","prompt_labels":"However(O) ,(O) usage(O) only(O) became(O) widespread(O) in(O) 2005(O) when(O) Navneet(B-researcher) Dalal(I-researcher) and(O) Bill(B-researcher) Triggs(I-researcher) ,(O) researchers(O) for(O) the(O) French(B-organization) National(I-organization) Institute(I-organization) for(I-organization) Research(I-organization) in(I-organization) Computer(I-organization) Science(I-organization) and(I-organization) Automation(I-organization) ((O) INRIA(B-organization) )(O) ,(O) presented(O) their(O) supplementary(O) work(O) on(O) HOG(B-algorithm) descriptors(I-algorithm) at(O) the(O) Conference(B-conference) on(I-conference) Computer(I-conference) Vision(I-conference) and(I-conference) Pattern(I-conference) Recognition(I-conference) ((O) CVPR(B-conference) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, location, conference, country, researcher, metric, field, product, algorithm, person, programming language, task, organization and O.\nSentence: However , usage only became widespread in 2005 when Navneet Dalal and Bill Triggs , researchers for the French National Institute for Research in Computer Science and Automation ( INRIA ) , presented their supplementary work on HOG descriptors at the Conference on Computer Vision and Pattern Recognition ( CVPR ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["However",",","usage","only","became","widespread","in","2005","when","Navneet","Dalal","and","Bill","Triggs",",","researchers","for","the","French","National","Institute","for","Research","in","Computer","Science","and","Automation","(","INRIA",")",",","presented","their","supplementary","work","on","HOG","descriptors","at","the","Conference","on","Computer","Vision","and","Pattern","Recognition","(","CVPR",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O"],"target_index":null,"target_label":null},"label_list":["university","location","conference","country","researcher","metric","field","product","algorithm","person","programming_language","task","organization"]}
{"id":"209","dataset":"crossner_ai","split":"test","instance":{"id":"209","prompt_labels":"When(O) data(O) are(O) unlabelled(O) ,(O) supervised(B-field) learning(I-field) is(O) not(O) possible(O) ,(O) and(O) an(O) unsupervised(B-field) learning(I-field) approach(O) is(O) required(O) which(O) attempts(O) to(O) find(O) natural(O) Cluster(B-task) analysis(I-task) to(O) groups(O) ,(O) and(O) then(O) map(O) new(O) data(O) to(O) these(O) formed(O) groups(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, country, task, conference, location, field, product, person, organization, metric, university, programming language, algorithm and O.\nSentence: When data are unlabelled , supervised learning is not possible , and an unsupervised learning approach is required which attempts to find natural Cluster analysis to groups , and then map new data to these formed groups .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["When","data","are","unlabelled",",","supervised","learning","is","not","possible",",","and","an","unsupervised","learning","approach","is","required","which","attempts","to","find","natural","Cluster","analysis","to","groups",",","and","then","map","new","data","to","these","formed","groups","."],"labels":["O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","B-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","country","task","conference","location","field","product","person","organization","metric","university","programming_language","algorithm"]}
{"id":"227","dataset":"crossner_ai","split":"test","instance":{"id":"227","prompt_labels":"Another(O) resource(O) ((O) free(O) but(O) copyrighted(O) )(O) is(O) the(O) HTK(B-product) book(I-product) ((O) and(O) the(O) accompanying(O) HTK(B-product) toolkit(I-product) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, person, researcher, location, country, metric, programming language, algorithm, task, product, conference, organization, field and O.\nSentence: Another resource ( free but copyrighted ) is the HTK book ( and the accompanying HTK toolkit ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Another","resource","(","free","but","copyrighted",")","is","the","HTK","book","(","and","the","accompanying","HTK","toolkit",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-product","I-product","O","O","O","O","B-product","I-product","O","O"],"target_index":null,"target_label":null},"label_list":["university","person","researcher","location","country","metric","programming_language","algorithm","task","product","conference","organization","field"]}
{"id":"228","dataset":"crossner_ai","split":"test","instance":{"id":"228","prompt_labels":"-(O) were(O) taken(O) in(O) the(O) 2004(B-conference) AAAI(I-conference) Spring(O) Symposium(O) where(O) linguists(O) ,(O) computer(O) scientists(O) ,(O) and(O) other(O) interested(O) researchers(O) first(O) aligned(O) interests(O) and(O) proposed(O) shared(O) tasks(O) and(O) benchmark(O) data(O) sets(O) for(O) the(O) systematic(O) computational(O) research(O) on(O) affect(O) ,(O) appeal(O) ,(O) subjectivity(O) ,(O) and(O) sentiment(O) in(O) text(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, algorithm, task, programming language, product, metric, person, field, university, country, organization, conference, location and O.\nSentence: - were taken in the 2004 AAAI Spring Symposium where linguists , computer scientists , and other interested researchers first aligned interests and proposed shared tasks and benchmark data sets for the systematic computational research on affect , appeal , subjectivity , and sentiment in text .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["-","were","taken","in","the","2004","AAAI","Spring","Symposium","where","linguists",",","computer","scientists",",","and","other","interested","researchers","first","aligned","interests","and","proposed","shared","tasks","and","benchmark","data","sets","for","the","systematic","computational","research","on","affect",",","appeal",",","subjectivity",",","and","sentiment","in","text","."],"labels":["O","O","O","O","O","B-conference","I-conference","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","algorithm","task","programming_language","product","metric","person","field","university","country","organization","conference","location"]}
{"id":"230","dataset":"crossner_ai","split":"test","instance":{"id":"230","prompt_labels":"In(O) 2018(O) Toyota(B-organization) was(O) regarded(O) as(O) being(O) behind(O) in(O) Self-driving(B-product) car(I-product) and(O) in(O) need(O) of(O) innovation(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, metric, country, organization, person, algorithm, conference, university, researcher, programming language, product, task, field and O.\nSentence: In 2018 Toyota was regarded as being behind in Self-driving car and in need of innovation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2018","Toyota","was","regarded","as","being","behind","in","Self-driving","car","and","in","need","of","innovation","."],"labels":["O","O","B-organization","O","O","O","O","O","O","B-product","I-product","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","metric","country","organization","person","algorithm","conference","university","researcher","programming_language","product","task","field"]}
{"id":"231","dataset":"crossner_ai","split":"test","instance":{"id":"231","prompt_labels":"Such(O) targets(O) include(O) natural(O) objects(O) such(O) as(O) ground(O) ,(O) sea(O) ,(O) precipitation(O) ((O) such(O) as(O) rain(O) ,(O) snow(O) or(O) hail(O) )(O) ,(O) sand(O) storm(O) s(O) ,(O) animals(O) ((O) especially(O) birds(O) )(O) ,(O) atmospheric(O) turbulence(O) ,(O) and(O) other(O) atmospheric(O) effects(O) ,(O) such(O) as(O) ionosphere(O) reflections(O) ,(O) meteor(O) trails(O) ,(O) and(O) three(O) body(O) scatter(O) spike(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, university, country, person, field, researcher, product, location, conference, organization, task, programming language, algorithm and O.\nSentence: Such targets include natural objects such as ground , sea , precipitation ( such as rain , snow or hail ) , sand storm s , animals ( especially birds ) , atmospheric turbulence , and other atmospheric effects , such as ionosphere reflections , meteor trails , and three body scatter spike .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Such","targets","include","natural","objects","such","as","ground",",","sea",",","precipitation","(","such","as","rain",",","snow","or","hail",")",",","sand","storm","s",",","animals","(","especially","birds",")",",","atmospheric","turbulence",",","and","other","atmospheric","effects",",","such","as","ionosphere","reflections",",","meteor","trails",",","and","three","body","scatter","spike","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","university","country","person","field","researcher","product","location","conference","organization","task","programming_language","algorithm"]}
{"id":"234","dataset":"crossner_ai","split":"test","instance":{"id":"234","prompt_labels":"The(O) 1997(O) RoboCup(O) 2D(O) Soccer(O) Simulation(O) League(O) was(O) the(O) first(O) RoboCup(O) competition(O) promoted(O) in(O) conjunction(O) with(O) International(B-conference) Joint(I-conference) Conference(I-conference) on(I-conference) Artificial(I-conference) Intelligence(I-conference) held(O) in(O) Nagoya(B-location) ,(O) Japan(B-country) ,(O) from(O) 23(O) to(O) 29(O) August(O) 1997(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, university, researcher, programming language, task, product, organization, field, country, conference, metric, person, location and O.\nSentence: The 1997 RoboCup 2D Soccer Simulation League was the first RoboCup competition promoted in conjunction with International Joint Conference on Artificial Intelligence held in Nagoya , Japan , from 23 to 29 August 1997 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","1997","RoboCup","2D","Soccer","Simulation","League","was","the","first","RoboCup","competition","promoted","in","conjunction","with","International","Joint","Conference","on","Artificial","Intelligence","held","in","Nagoya",",","Japan",",","from","23","to","29","August","1997","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","O","B-location","O","B-country","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["algorithm","university","researcher","programming_language","task","product","organization","field","country","conference","metric","person","location"]}
{"id":"241","dataset":"crossner_ai","split":"test","instance":{"id":"241","prompt_labels":"In(O) 1969(O) a(O) famous(O) book(O) entitled(O) Perceptrons(O) by(O) Marvin(B-researcher) Minsky(I-researcher) and(O) Seymour(B-researcher) Papert(I-researcher) showed(O) that(O) it(O) was(O) impossible(O) for(O) these(O) classes(O) of(O) network(O) to(O) learn(O) an(O) XOR(O) function(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, algorithm, location, programming language, metric, country, field, university, task, conference, researcher, product and O.\nSentence: In 1969 a famous book entitled Perceptrons by Marvin Minsky and Seymour Papert showed that it was impossible for these classes of network to learn an XOR function .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1969","a","famous","book","entitled","Perceptrons","by","Marvin","Minsky","and","Seymour","Papert","showed","that","it","was","impossible","for","these","classes","of","network","to","learn","an","XOR","function","."],"labels":["O","O","O","O","O","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","organization","algorithm","location","programming_language","metric","country","field","university","task","conference","researcher","product"]}
{"id":"254","dataset":"crossner_ai","split":"test","instance":{"id":"254","prompt_labels":"In(O) 1971(O) Terry(B-researcher) Winograd(I-researcher) developed(O) an(O) early(O) natural(B-field) language(I-field) processing(I-field) engine(O) capable(O) of(O) interpreting(O) naturally(O) written(O) commands(O) within(O) a(O) simple(O) rule-governed(O) environment(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, person, task, organization, university, metric, researcher, programming language, product, conference, algorithm, field and O.\nSentence: In 1971 Terry Winograd developed an early natural language processing engine capable of interpreting naturally written commands within a simple rule-governed environment .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1971","Terry","Winograd","developed","an","early","natural","language","processing","engine","capable","of","interpreting","naturally","written","commands","within","a","simple","rule-governed","environment","."],"labels":["O","O","B-researcher","I-researcher","O","O","O","B-field","I-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","country","person","task","organization","university","metric","researcher","programming_language","product","conference","algorithm","field"]}
{"id":"255","dataset":"crossner_ai","split":"test","instance":{"id":"255","prompt_labels":"In(O) artificial(B-field) intelligence(I-field) ,(O) Marvin(B-researcher) Minsky(I-researcher) ,(O) Herbert(B-researcher) A.(I-researcher) Simon(I-researcher) ,(O) and(O) Allen(B-researcher) Newell(I-researcher) are(O) prominent(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, product, programming language, field, person, algorithm, university, organization, researcher, country, task, location, conference and O.\nSentence: In artificial intelligence , Marvin Minsky , Herbert A. Simon , and Allen Newell are prominent .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","artificial","intelligence",",","Marvin","Minsky",",","Herbert","A.","Simon",",","and","Allen","Newell","are","prominent","."],"labels":["O","B-field","I-field","O","B-researcher","I-researcher","O","B-researcher","I-researcher","I-researcher","O","O","B-researcher","I-researcher","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","product","programming_language","field","person","algorithm","university","organization","researcher","country","task","location","conference"]}
{"id":"256","dataset":"crossner_ai","split":"test","instance":{"id":"256","prompt_labels":"In(O) the(O) latter(O) half(O) of(O) the(O) 20th(O) century(O) ,(O) electrical(B-field) engineering(I-field) itself(O) separated(O) into(O) several(O) disciplines(O) ,(O) specialising(O) in(O) the(O) design(O) and(O) analysis(O) of(O) systems(O) that(O) manipulate(O) physical(O) signals(O) ;(O) electronic(B-field) engineering(I-field) and(O) computer(B-field) engineering(I-field) as(O) examples(O) ;(O) while(O) design(B-field) engineering(I-field) developed(O) to(O) deal(O) with(O) functional(O) design(O) of(O) user-machine(O) interfaces(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, university, country, product, researcher, location, person, task, organization, metric, algorithm, conference, field and O.\nSentence: In the latter half of the 20th century , electrical engineering itself separated into several disciplines , specialising in the design and analysis of systems that manipulate physical signals ; electronic engineering and computer engineering as examples ; while design engineering developed to deal with functional design of user-machine interfaces .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","latter","half","of","the","20th","century",",","electrical","engineering","itself","separated","into","several","disciplines",",","specialising","in","the","design","and","analysis","of","systems","that","manipulate","physical","signals",";","electronic","engineering","and","computer","engineering","as","examples",";","while","design","engineering","developed","to","deal","with","functional","design","of","user-machine","interfaces","."],"labels":["O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","B-field","I-field","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","university","country","product","researcher","location","person","task","organization","metric","algorithm","conference","field"]}
{"id":"257","dataset":"crossner_ai","split":"test","instance":{"id":"257","prompt_labels":"Perhaps(O) the(O) simplest(O) statistic(O) is(O) accuracy(B-metric) or(O) Fraction(B-metric) Correct(I-metric) ((O) FC(B-metric) )(O) ,(O) which(O) measures(O) the(O) fraction(O) of(O) all(O) instances(O) that(O) are(O) correctly(O) categorized(O) ;(O) it(O) is(O) the(O) ratio(O) of(O) the(O) number(O) of(O) correct(O) classifications(O) to(O) the(O) total(O) number(O) of(O) correct(O) or(O) incorrect(O) classifications(O) :(O) ((O) TP(B-metric) +(I-metric) TN(I-metric) )(O) /(O) Total(O) Population(O) =(O) ((O) TP(B-metric) +(I-metric) TN(I-metric) )(O) /(O) ((O) TP(B-metric) +(I-metric) TN(I-metric) +(I-metric) FP(I-metric) +(I-metric) FN(I-metric) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, researcher, conference, field, product, person, university, algorithm, organization, task, country, programming language, metric and O.\nSentence: Perhaps the simplest statistic is accuracy or Fraction Correct ( FC ) , which measures the fraction of all instances that are correctly categorized ; it is the ratio of the number of correct classifications to the total number of correct or incorrect classifications : ( TP + TN ) / Total Population = ( TP + TN ) / ( TP + TN + FP + FN ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Perhaps","the","simplest","statistic","is","accuracy","or","Fraction","Correct","(","FC",")",",","which","measures","the","fraction","of","all","instances","that","are","correctly","categorized",";","it","is","the","ratio","of","the","number","of","correct","classifications","to","the","total","number","of","correct","or","incorrect","classifications",":","(","TP","+","TN",")","/","Total","Population","=","(","TP","+","TN",")","/","(","TP","+","TN","+","FP","+","FN",")","."],"labels":["O","O","O","O","O","B-metric","O","B-metric","I-metric","O","B-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","B-metric","I-metric","I-metric","I-metric","I-metric","I-metric","I-metric","O","O"],"target_index":null,"target_label":null},"label_list":["location","researcher","conference","field","product","person","university","algorithm","organization","task","country","programming_language","metric"]}
{"id":"263","dataset":"crossner_ai","split":"test","instance":{"id":"263","prompt_labels":"WordNet(B-product) has(O) been(O) used(O) for(O) a(O) number(O) of(O) purposes(O) in(O) information(B-product) systems(I-product) ,(O) including(O) word-sense(B-task) disambiguation(I-task) ,(O) information(B-task) retrieval(I-task) ,(O) automatic(B-task) text(I-task) classification(I-task) ,(O) Automatic(B-task) summarization(I-task) ,(O) machine(B-task) translation(I-task) and(O) even(O) automatic(B-task) crossword(I-task) puzzle(I-task) generation(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, task, metric, country, researcher, conference, product, algorithm, organization, field, location, university, programming language and O.\nSentence: WordNet has been used for a number of purposes in information systems , including word-sense disambiguation , information retrieval , automatic text classification , Automatic summarization , machine translation and even automatic crossword puzzle generation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["WordNet","has","been","used","for","a","number","of","purposes","in","information","systems",",","including","word-sense","disambiguation",",","information","retrieval",",","automatic","text","classification",",","Automatic","summarization",",","machine","translation","and","even","automatic","crossword","puzzle","generation","."],"labels":["B-product","O","O","O","O","O","O","O","O","O","B-product","I-product","O","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","I-task","O","B-task","I-task","O","B-task","I-task","O","O","B-task","I-task","I-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["person","task","metric","country","researcher","conference","product","algorithm","organization","field","location","university","programming_language"]}
{"id":"266","dataset":"crossner_ai","split":"test","instance":{"id":"266","prompt_labels":"In(O) the(O) film(O) Westworld(O) ,(O) female(O) robots(O) actually(O) engaged(O) in(O) intercourse(O) with(O) human(O) men(O) as(O) part(O) of(O) the(O) make-believe(O) vacation(O) world(O) human(O) customers(O) paid(O) to(O) attend(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, conference, field, metric, organization, product, location, algorithm, task, university, person, country, researcher and O.\nSentence: In the film Westworld , female robots actually engaged in intercourse with human men as part of the make-believe vacation world human customers paid to attend .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","film","Westworld",",","female","robots","actually","engaged","in","intercourse","with","human","men","as","part","of","the","make-believe","vacation","world","human","customers","paid","to","attend","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","conference","field","metric","organization","product","location","algorithm","task","university","person","country","researcher"]}
{"id":"269","dataset":"crossner_ai","split":"test","instance":{"id":"269","prompt_labels":"While(O) studying(O) at(O) Stanford(B-university) ,(O) Scheinman(B-researcher) was(O) awarded(O) a(O) fellowship(O) sponsored(O) by(O) George(B-researcher) Devol(I-researcher) ,(O) the(O) inventor(O) of(O) the(O) Unimate(B-product) ,(O) the(O) first(O) industrial(B-product) robot(I-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, university, country, researcher, programming language, field, person, location, organization, conference, algorithm, task, metric and O.\nSentence: While studying at Stanford , Scheinman was awarded a fellowship sponsored by George Devol , the inventor of the Unimate , the first industrial robot .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["While","studying","at","Stanford",",","Scheinman","was","awarded","a","fellowship","sponsored","by","George","Devol",",","the","inventor","of","the","Unimate",",","the","first","industrial","robot","."],"labels":["O","O","O","B-university","O","B-researcher","O","O","O","O","O","O","B-researcher","I-researcher","O","O","O","O","O","B-product","O","O","O","B-product","I-product","O"],"target_index":null,"target_label":null},"label_list":["product","university","country","researcher","programming_language","field","person","location","organization","conference","algorithm","task","metric"]}
{"id":"272","dataset":"crossner_ai","split":"test","instance":{"id":"272","prompt_labels":"Much(O) of(O) the(O) confusion(O) between(O) these(O) two(O) research(O) communities(O) ((O) which(O) do(O) often(O) have(O) separate(O) conferences(O) and(O) separate(O) journals(O) ,(O) ECML(B-conference) PKDD(I-conference) being(O) a(O) major(O) exception(O) )(O) comes(O) from(O) the(O) basic(O) assumptions(O) they(O) work(O) with(O) :(O) in(O) machine(B-field) learning(I-field) ,(O) performance(O) is(O) usually(O) evaluated(O) with(O) respect(O) to(O) the(O) ability(O) to(O) reproduce(O) known(O) knowledge(O) ,(O) while(O) in(O) knowledge(B-conference) discovery(I-conference) and(I-conference) data(I-conference) mining(I-conference) ((O) KDD(B-conference) )(O) the(O) key(O) task(O) is(O) the(O) discovery(O) of(O) previously(O) unknown(O) knowledge(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, researcher, programming language, person, location, metric, task, university, country, product, organization, algorithm, field and O.\nSentence: Much of the confusion between these two research communities ( which do often have separate conferences and separate journals , ECML PKDD being a major exception ) comes from the basic assumptions they work with : in machine learning , performance is usually evaluated with respect to the ability to reproduce known knowledge , while in knowledge discovery and data mining ( KDD ) the key task is the discovery of previously unknown knowledge .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Much","of","the","confusion","between","these","two","research","communities","(","which","do","often","have","separate","conferences","and","separate","journals",",","ECML","PKDD","being","a","major","exception",")","comes","from","the","basic","assumptions","they","work","with",":","in","machine","learning",",","performance","is","usually","evaluated","with","respect","to","the","ability","to","reproduce","known","knowledge",",","while","in","knowledge","discovery","and","data","mining","(","KDD",")","the","key","task","is","the","discovery","of","previously","unknown","knowledge","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["conference","researcher","programming_language","person","location","metric","task","university","country","product","organization","algorithm","field"]}
{"id":"274","dataset":"crossner_ai","split":"test","instance":{"id":"274","prompt_labels":",(O) a(O) company(O) in(O) Bangalore(B-location) ,(O) India(B-country) specializing(O) in(O) online(O) handwriting(B-task) recognition(I-task) software(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, researcher, person, programming language, university, task, field, algorithm, metric, product, country, conference, organization and O.\nSentence: , a company in Bangalore , India specializing in online handwriting recognition software .","prediction_output":null,"prediction_outputs":null,"group":null,"words":[",","a","company","in","Bangalore",",","India","specializing","in","online","handwriting","recognition","software","."],"labels":["O","O","O","O","B-location","O","B-country","O","O","O","B-task","I-task","O","O"],"target_index":null,"target_label":null},"label_list":["location","researcher","person","programming_language","university","task","field","algorithm","metric","product","country","conference","organization"]}
{"id":"276","dataset":"crossner_ai","split":"test","instance":{"id":"276","prompt_labels":"He(O) holds(O) fellowships(O) in(O) the(O) American(B-conference) Association(I-conference) for(I-conference) Artificial(I-conference) Intelligence(I-conference) ,(O) the(O) Center(B-organization) for(I-organization) Advanced(I-organization) Study(I-organization) in(I-organization) the(I-organization) Behavioral(I-organization) Sciences(I-organization) at(O) Stanford(B-university) University(I-university) ,(O) the(O) MIT(B-university) Center(O) for(O) Cognitive(B-field) Science(I-field) ,(O) the(O) Canadian(B-organization) Institute(I-organization) for(I-organization) Advanced(I-organization) Research(I-organization) ,(O) the(O) Canadian(B-organization) Psychological(I-organization) Association(I-organization) ,(O) and(O) was(O) elected(O) Fellow(O) of(O) the(O) Royal(B-organization) Society(I-organization) of(I-organization) Canada(I-organization) in(O) 1998(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, metric, algorithm, programming language, country, field, product, university, conference, researcher, location, task and O.\nSentence: He holds fellowships in the American Association for Artificial Intelligence , the Center for Advanced Study in the Behavioral Sciences at Stanford University , the MIT Center for Cognitive Science , the Canadian Institute for Advanced Research , the Canadian Psychological Association , and was elected Fellow of the Royal Society of Canada in 1998 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","holds","fellowships","in","the","American","Association","for","Artificial","Intelligence",",","the","Center","for","Advanced","Study","in","the","Behavioral","Sciences","at","Stanford","University",",","the","MIT","Center","for","Cognitive","Science",",","the","Canadian","Institute","for","Advanced","Research",",","the","Canadian","Psychological","Association",",","and","was","elected","Fellow","of","the","Royal","Society","of","Canada","in","1998","."],"labels":["O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-university","I-university","O","O","B-university","O","O","B-field","I-field","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","person","metric","algorithm","programming_language","country","field","product","university","conference","researcher","location","task"]}
{"id":"281","dataset":"crossner_ai","split":"test","instance":{"id":"281","prompt_labels":"Edsinger(B-person) and(O) Weber(B-organization) collaborated(O) on(O) many(O) other(O) robots(O) as(O) well(O) ,(O) and(O) their(O) experience(O) working(O) with(O) the(O) Kismet(B-product)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, university, algorithm, location, conference, metric, researcher, organization, field, product, country, person, task and O.\nSentence: Edsinger and Weber collaborated on many other robots as well , and their experience working with the Kismet","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Edsinger","and","Weber","collaborated","on","many","other","robots","as","well",",","and","their","experience","working","with","the","Kismet"],"labels":["B-person","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product"],"target_index":null,"target_label":null},"label_list":["programming_language","university","algorithm","location","conference","metric","researcher","organization","field","product","country","person","task"]}
{"id":"288","dataset":"crossner_ai","split":"test","instance":{"id":"288","prompt_labels":"Also(O) known(O) as(O) parallel(O) robots(O) ,(O) or(O) generalized(O) Stewart(B-product) platforms(I-product) ((O) in(O) the(O) Stewart(B-product) platform(I-product) ,(O) the(O) actuators(O) are(O) paired(O) together(O) on(O) both(O) the(O) basis(O) and(O) the(O) platform(O) )(O) ,(O) these(O) systems(O) are(O) articulated(B-product) robot(I-product) s(O) that(O) use(O) similar(O) mechanisms(O) for(O) the(O) movement(O) of(O) either(O) the(O) robot(O) on(O) its(O) base(O) ,(O) or(O) one(O) or(O) more(O) manipulator(O) arms(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, university, person, country, metric, researcher, organization, location, programming language, conference, field, algorithm, product and O.\nSentence: Also known as parallel robots , or generalized Stewart platforms ( in the Stewart platform , the actuators are paired together on both the basis and the platform ) , these systems are articulated robot s that use similar mechanisms for the movement of either the robot on its base , or one or more manipulator arms .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Also","known","as","parallel","robots",",","or","generalized","Stewart","platforms","(","in","the","Stewart","platform",",","the","actuators","are","paired","together","on","both","the","basis","and","the","platform",")",",","these","systems","are","articulated","robot","s","that","use","similar","mechanisms","for","the","movement","of","either","the","robot","on","its","base",",","or","one","or","more","manipulator","arms","."],"labels":["O","O","O","O","O","O","O","O","B-product","I-product","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","university","person","country","metric","researcher","organization","location","programming_language","conference","field","algorithm","product"]}
{"id":"290","dataset":"crossner_ai","split":"test","instance":{"id":"290","prompt_labels":"The(O) activation(O) function(O) of(O) the(O) LSTM(B-algorithm) gates(I-algorithm) is(O) often(O) the(O) logistic(B-algorithm) sigmoid(I-algorithm) function(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, organization, task, university, country, conference, product, location, metric, researcher, field, person, algorithm and O.\nSentence: The activation function of the LSTM gates is often the logistic sigmoid function .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","activation","function","of","the","LSTM","gates","is","often","the","logistic","sigmoid","function","."],"labels":["O","O","O","O","O","B-algorithm","I-algorithm","O","O","O","B-algorithm","I-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["programming_language","organization","task","university","country","conference","product","location","metric","researcher","field","person","algorithm"]}
{"id":"291","dataset":"crossner_ai","split":"test","instance":{"id":"291","prompt_labels":"In(O) other(O) words(O) ,(O) the(O) sample(B-metric) mean(I-metric) is(O) the(O) ((O) necessarily(O) unique(O) )(O) efficient(O) estimator(O) ,(O) and(O) thus(O) also(O) the(O) minimum(B-metric) variance(I-metric) unbiased(I-metric) estimator(I-metric) ((O) MVUE(B-metric) )(O) ,(O) in(O) addition(O) to(O) being(O) the(O) maximum(B-metric) likelihood(I-metric) estimator(I-metric) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, product, programming language, country, field, conference, task, organization, algorithm, metric, person, researcher, university and O.\nSentence: In other words , the sample mean is the ( necessarily unique ) efficient estimator , and thus also the minimum variance unbiased estimator ( MVUE ) , in addition to being the maximum likelihood estimator .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","other","words",",","the","sample","mean","is","the","(","necessarily","unique",")","efficient","estimator",",","and","thus","also","the","minimum","variance","unbiased","estimator","(","MVUE",")",",","in","addition","to","being","the","maximum","likelihood","estimator","."],"labels":["O","O","O","O","O","B-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","I-metric","O","B-metric","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O"],"target_index":null,"target_label":null},"label_list":["location","product","programming_language","country","field","conference","task","organization","algorithm","metric","person","researcher","university"]}
{"id":"294","dataset":"crossner_ai","split":"test","instance":{"id":"294","prompt_labels":"Gerry(B-researcher) Sussman(I-researcher) ,(O) Eugene(B-researcher) Charniak(I-researcher) ,(O) Seymour(B-researcher) Papert(I-researcher) and(O) Terry(B-researcher) Winograd(I-researcher) visited(O) the(O) University(B-university) of(I-university) Edinburgh(I-university) in(O) 1971(O) spreading(O) the(O) news(O) about(O) Micro-Planner(B-product) and(O) SHRDLU(B-product) and(O) casting(O) doubt(O) on(O) the(O) resolution(O) uniform(O) proof(O) procedure(O) approach(O) that(O) had(O) been(O) the(O) mainstay(O) of(O) the(O) Edinburgh(B-location) Logicists(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, product, researcher, algorithm, person, task, conference, programming language, organization, country, location, field, metric and O.\nSentence: Gerry Sussman , Eugene Charniak , Seymour Papert and Terry Winograd visited the University of Edinburgh in 1971 spreading the news about Micro-Planner and SHRDLU and casting doubt on the resolution uniform proof procedure approach that had been the mainstay of the Edinburgh Logicists .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Gerry","Sussman",",","Eugene","Charniak",",","Seymour","Papert","and","Terry","Winograd","visited","the","University","of","Edinburgh","in","1971","spreading","the","news","about","Micro-Planner","and","SHRDLU","and","casting","doubt","on","the","resolution","uniform","proof","procedure","approach","that","had","been","the","mainstay","of","the","Edinburgh","Logicists","."],"labels":["B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","O","B-university","I-university","I-university","O","O","O","O","O","O","B-product","O","B-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O"],"target_index":null,"target_label":null},"label_list":["university","product","researcher","algorithm","person","task","conference","programming_language","organization","country","location","field","metric"]}
{"id":"300","dataset":"crossner_ai","split":"test","instance":{"id":"300","prompt_labels":"During(O) 2015(O) ,(O) many(O) of(O) SenseTime(B-organization) 's(O) papers(O) were(O) accepted(O) into(O) the(O) Conference(B-conference) on(I-conference) Computer(I-conference) Vision(I-conference) and(I-conference) Pattern(I-conference) Recognition(I-conference) ((O) CVPR(B-conference) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, location, programming language, metric, university, person, researcher, task, product, field, algorithm, conference and O.\nSentence: During 2015 , many of SenseTime 's papers were accepted into the Conference on Computer Vision and Pattern Recognition ( CVPR ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","2015",",","many","of","SenseTime","'s","papers","were","accepted","into","the","Conference","on","Computer","Vision","and","Pattern","Recognition","(","CVPR",")","."],"labels":["O","O","O","O","O","B-organization","O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","O"],"target_index":null,"target_label":null},"label_list":["organization","country","location","programming_language","metric","university","person","researcher","task","product","field","algorithm","conference"]}
{"id":"319","dataset":"crossner_ai","split":"test","instance":{"id":"319","prompt_labels":"Also(O) ,(O) text(O) produced(O) by(O) processing(O) spontaneous(O) speech(O) using(O) automatic(B-task) speech(I-task) recognition(I-task) and(O) printed(O) or(O) handwritten(O) text(O) using(O) optical(B-task) character(I-task) recognition(I-task) contains(O) processing(O) noise(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, programming language, algorithm, country, conference, product, university, person, location, researcher, task, metric, field and O.\nSentence: Also , text produced by processing spontaneous speech using automatic speech recognition and printed or handwritten text using optical character recognition contains processing noise .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Also",",","text","produced","by","processing","spontaneous","speech","using","automatic","speech","recognition","and","printed","or","handwritten","text","using","optical","character","recognition","contains","processing","noise","."],"labels":["O","O","O","O","O","O","O","O","O","B-task","I-task","I-task","O","O","O","O","O","O","B-task","I-task","I-task","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","programming_language","algorithm","country","conference","product","university","person","location","researcher","task","metric","field"]}
{"id":"323","dataset":"crossner_ai","split":"test","instance":{"id":"323","prompt_labels":"Pausch(B-researcher) received(O) two(O) awards(O) from(O) Association(B-conference) for(I-conference) Computing(I-conference) Machinery(I-conference) in(O) 2007(O) for(O) his(O) achievements(O) in(O) computing(B-field) education(I-field) :(O) the(O) Karl(O) V.(O) Karlstrom(O) Outstanding(O) Educator(O) Award(O) and(O) the(O) ACM(O) SIGCSE(O) Award(O) for(O) Outstanding(O) Contributions(O) to(O) Computer(O) Science(O) Education(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, location, person, country, task, field, product, conference, algorithm, researcher, programming language, organization, university and O.\nSentence: Pausch received two awards from Association for Computing Machinery in 2007 for his achievements in computing education : the Karl V. Karlstrom Outstanding Educator Award and the ACM SIGCSE Award for Outstanding Contributions to Computer Science Education .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Pausch","received","two","awards","from","Association","for","Computing","Machinery","in","2007","for","his","achievements","in","computing","education",":","the","Karl","V.","Karlstrom","Outstanding","Educator","Award","and","the","ACM","SIGCSE","Award","for","Outstanding","Contributions","to","Computer","Science","Education","."],"labels":["B-researcher","O","O","O","O","B-conference","I-conference","I-conference","I-conference","O","O","O","O","O","O","B-field","I-field","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","location","person","country","task","field","product","conference","algorithm","researcher","programming_language","organization","university"]}
{"id":"327","dataset":"crossner_ai","split":"test","instance":{"id":"327","prompt_labels":"In(O) addition(O) to(O) maintaining(O) the(O) Discovery(B-product) One(I-product) spacecraft(I-product) systems(I-product) during(O) the(O) interplanetary(O) mission(O) to(O) Jupiter(O) ((O) or(O) Saturn(O) in(O) the(O) novel(O) )(O) ,(O) HAL(B-product) is(O) capable(O) of(O) speech(B-task) synthesis(I-task) ,(O) speech(B-task) recognition(I-task) ,(O) facial(B-task) recognition(I-task) ,(O) natural(B-field) language(I-field) processing(I-field) ,(O) lip(B-task) reading(I-task) ,(O) art(B-field) appreciation(I-field) ,(O) Affective(B-task) computing(I-task) ,(O) automated(B-task) reasoning(I-task) ,(O) spacecraft(B-task) piloting(I-task) and(O) playing(B-task) chess(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, algorithm, organization, location, person, university, metric, country, researcher, product, field, task, programming language and O.\nSentence: In addition to maintaining the Discovery One spacecraft systems during the interplanetary mission to Jupiter ( or Saturn in the novel ) , HAL is capable of speech synthesis , speech recognition , facial recognition , natural language processing , lip reading , art appreciation , Affective computing , automated reasoning , spacecraft piloting and playing chess .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition","to","maintaining","the","Discovery","One","spacecraft","systems","during","the","interplanetary","mission","to","Jupiter","(","or","Saturn","in","the","novel",")",",","HAL","is","capable","of","speech","synthesis",",","speech","recognition",",","facial","recognition",",","natural","language","processing",",","lip","reading",",","art","appreciation",",","Affective","computing",",","automated","reasoning",",","spacecraft","piloting","and","playing","chess","."],"labels":["O","O","O","O","O","B-product","I-product","I-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-product","O","O","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-field","I-field","I-field","O","B-task","I-task","O","B-field","I-field","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["conference","algorithm","organization","location","person","university","metric","country","researcher","product","field","task","programming_language"]}
{"id":"328","dataset":"crossner_ai","split":"test","instance":{"id":"328","prompt_labels":"Dr.(B-researcher) Julesz(I-researcher) emigrated(O) from(O) Hungary(B-country) to(O) the(B-country) United(I-country) States(I-country) following(O) the(O) 1956(O) Soviet(B-country) invasion(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, university, conference, product, location, person, researcher, programming language, task, country, organization, metric, algorithm and O.\nSentence: Dr. Julesz emigrated from Hungary to the United States following the 1956 Soviet invasion .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Dr.","Julesz","emigrated","from","Hungary","to","the","United","States","following","the","1956","Soviet","invasion","."],"labels":["B-researcher","I-researcher","O","O","B-country","O","B-country","I-country","I-country","O","O","O","B-country","O","O"],"target_index":null,"target_label":null},"label_list":["field","university","conference","product","location","person","researcher","programming_language","task","country","organization","metric","algorithm"]}
{"id":"330","dataset":"crossner_ai","split":"test","instance":{"id":"330","prompt_labels":"These(O) probabilities(O) are(O) used(O) to(O) determine(O) what(O) the(O) target(O) is(O) using(O) a(O) maximum(B-algorithm) likelihood(I-algorithm) decision(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, algorithm, organization, conference, field, location, country, metric, task, person, product, programming language, researcher and O.\nSentence: These probabilities are used to determine what the target is using a maximum likelihood decision .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["These","probabilities","are","used","to","determine","what","the","target","is","using","a","maximum","likelihood","decision","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["university","algorithm","organization","conference","field","location","country","metric","task","person","product","programming_language","researcher"]}
{"id":"332","dataset":"crossner_ai","split":"test","instance":{"id":"332","prompt_labels":"Some(O) popular(O) fitness(O) functions(O) based(O) on(O) the(O) confusion(B-metric) matrix(I-metric) include(O) sensitivity(B-metric) /(I-metric) specificity(I-metric) ,(O) recall(B-metric) /(I-metric) precision(I-metric) ,(O) F-measure(B-metric) ,(O) Jaccard(B-metric) similarity(I-metric) ,(O) Matthews(B-metric) correlation(I-metric) coefficient(I-metric) ,(O) and(O) cost(B-metric) /(I-metric) gain(I-metric) matrix(I-metric) which(O) combines(O) the(O) costs(O) and(O) gains(O) assigned(O) to(O) the(O) 4(O) different(O) types(O) of(O) classifications(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, researcher, university, metric, country, task, field, location, algorithm, conference, programming language, product, organization and O.\nSentence: Some popular fitness functions based on the confusion matrix include sensitivity / specificity , recall / precision , F-measure , Jaccard similarity , Matthews correlation coefficient , and cost / gain matrix which combines the costs and gains assigned to the 4 different types of classifications .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","popular","fitness","functions","based","on","the","confusion","matrix","include","sensitivity","/","specificity",",","recall","/","precision",",","F-measure",",","Jaccard","similarity",",","Matthews","correlation","coefficient",",","and","cost","/","gain","matrix","which","combines","the","costs","and","gains","assigned","to","the","4","different","types","of","classifications","."],"labels":["O","O","O","O","O","O","O","B-metric","I-metric","O","B-metric","I-metric","I-metric","O","B-metric","I-metric","I-metric","O","B-metric","O","B-metric","I-metric","O","B-metric","I-metric","I-metric","O","O","B-metric","I-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","researcher","university","metric","country","task","field","location","algorithm","conference","programming_language","product","organization"]}
{"id":"334","dataset":"crossner_ai","split":"test","instance":{"id":"334","prompt_labels":"Industrial(B-product) robots(I-product) have(O) been(O) implemented(O) to(O) collaborate(O) with(O) humans(O) to(O) perform(O) industrial(O) manufacturing(O) tasks(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, country, task, university, programming language, algorithm, organization, person, location, researcher, field, product, metric and O.\nSentence: Industrial robots have been implemented to collaborate with humans to perform industrial manufacturing tasks .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Industrial","robots","have","been","implemented","to","collaborate","with","humans","to","perform","industrial","manufacturing","tasks","."],"labels":["B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["conference","country","task","university","programming_language","algorithm","organization","person","location","researcher","field","product","metric"]}
{"id":"344","dataset":"crossner_ai","split":"test","instance":{"id":"344","prompt_labels":"Cognitive(B-algorithm) maps(I-algorithm) serve(O) the(O) construction(O) and(O) accumulation(O) of(O) spatial(O) knowledge(O) ,(O) allowing(O) the(O) mind(O) 's(O) eye(O) to(O) visualize(O) images(O) in(O) order(O) to(O) reduce(O) cognitive(O) load(O) ,(O) enhance(O) recall(B-metric) and(O) learning(O) of(O) information(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, algorithm, conference, programming language, metric, researcher, country, organization, task, field, product, university, person and O.\nSentence: Cognitive maps serve the construction and accumulation of spatial knowledge , allowing the mind 's eye to visualize images in order to reduce cognitive load , enhance recall and learning of information .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Cognitive","maps","serve","the","construction","and","accumulation","of","spatial","knowledge",",","allowing","the","mind","'s","eye","to","visualize","images","in","order","to","reduce","cognitive","load",",","enhance","recall","and","learning","of","information","."],"labels":["B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","algorithm","conference","programming_language","metric","researcher","country","organization","task","field","product","university","person"]}
{"id":"345","dataset":"crossner_ai","split":"test","instance":{"id":"345","prompt_labels":",(O) typically(O) providing(O) bindings(O) to(O) languages(O) such(O) as(O) Python(B-programming language) ,(O) C(B-programming language) +(I-programming language) +(I-programming language) ,(O) Java(B-programming language) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, product, country, conference, metric, organization, person, university, programming language, algorithm, location, task, researcher and O.\nSentence: , typically providing bindings to languages such as Python , C + + , Java ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":[",","typically","providing","bindings","to","languages","such","as","Python",",","C","+","+",",","Java",")","."],"labels":["O","O","O","O","O","O","O","O","B-programming language","O","B-programming language","I-programming language","I-programming language","O","B-programming language","O","O"],"target_index":null,"target_label":null},"label_list":["field","product","country","conference","metric","organization","person","university","programming_language","algorithm","location","task","researcher"]}
{"id":"346","dataset":"crossner_ai","split":"test","instance":{"id":"346","prompt_labels":"A(O) voice-user(B-product) interface(I-product) ((O) VUI(B-product) )(O) makes(O) spoken(O) human(O) interaction(O) with(O) computers(O) possible(O) ,(O) using(O) speech(B-task) recognition(I-task) to(O) understand(O) spoken(O) commands(O) and(O) Question(B-task) answering(I-task) ,(O) and(O) typically(O) text(B-task) to(I-task) speech(I-task) to(O) play(O) a(O) reply(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, conference, product, organization, person, metric, programming language, algorithm, task, field, university, researcher, country and O.\nSentence: A voice-user interface ( VUI ) makes spoken human interaction with computers possible , using speech recognition to understand spoken commands and Question answering , and typically text to speech to play a reply .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","voice-user","interface","(","VUI",")","makes","spoken","human","interaction","with","computers","possible",",","using","speech","recognition","to","understand","spoken","commands","and","Question","answering",",","and","typically","text","to","speech","to","play","a","reply","."],"labels":["O","B-product","I-product","O","B-product","O","O","O","O","O","O","O","O","O","O","B-task","I-task","O","O","O","O","O","B-task","I-task","O","O","O","B-task","I-task","I-task","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","conference","product","organization","person","metric","programming_language","algorithm","task","field","university","researcher","country"]}
{"id":"348","dataset":"crossner_ai","split":"test","instance":{"id":"348","prompt_labels":"For(O) multilayer(B-algorithm) perceptron(I-algorithm) s(O) ,(O) where(O) a(O) hidden(O) layer(O) exists(O) ,(O) more(O) sophisticated(O) algorithms(O) such(O) as(O) backpropagation(B-algorithm) must(O) be(O) used(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, researcher, algorithm, country, university, location, organization, person, field, programming language, conference, metric, task and O.\nSentence: For multilayer perceptron s , where a hidden layer exists , more sophisticated algorithms such as backpropagation must be used .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","multilayer","perceptron","s",",","where","a","hidden","layer","exists",",","more","sophisticated","algorithms","such","as","backpropagation","must","be","used","."],"labels":["O","B-algorithm","I-algorithm","O","O","O","O","O","O","O","O","O","O","O","O","O","B-algorithm","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","researcher","algorithm","country","university","location","organization","person","field","programming_language","conference","metric","task"]}
{"id":"349","dataset":"crossner_ai","split":"test","instance":{"id":"349","prompt_labels":"Google(B-product) Translate(I-product) 's(O) neural(B-product) machine(I-product) translation(I-product) system(I-product) uses(O) a(O) large(O) end-to-end(B-algorithm) artificial(I-algorithm) neural(I-algorithm) network(I-algorithm) that(O) attempts(O) to(O) perform(O) deep(B-field) learning(I-field) ,(O) in(O) particular(O) ,(O) long(B-algorithm) short-term(I-algorithm) memory(I-algorithm) networks(I-algorithm) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, researcher, product, person, programming language, field, algorithm, country, metric, university, conference, organization, location and O.\nSentence: Google Translate 's neural machine translation system uses a large end-to-end artificial neural network that attempts to perform deep learning , in particular , long short-term memory networks .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Google","Translate","'s","neural","machine","translation","system","uses","a","large","end-to-end","artificial","neural","network","that","attempts","to","perform","deep","learning",",","in","particular",",","long","short-term","memory","networks","."],"labels":["B-product","I-product","O","B-product","I-product","I-product","I-product","O","O","O","B-algorithm","I-algorithm","I-algorithm","I-algorithm","O","O","O","O","B-field","I-field","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","I-algorithm","O"],"target_index":null,"target_label":null},"label_list":["task","researcher","product","person","programming_language","field","algorithm","country","metric","university","conference","organization","location"]}
{"id":"351","dataset":"crossner_ai","split":"test","instance":{"id":"351","prompt_labels":"|(O) Apple(B-organization) Apple(B-organization) Inc(I-organization) originally(O) licensed(O) software(O) from(O) Nuance(B-organization) to(O) provide(O) speech(B-task) recognition(I-task) capability(O) to(O) its(O) digital(O) assistant(O) Siri(B-product) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, conference, task, organization, metric, field, researcher, programming language, person, location, algorithm, product, country and O.\nSentence: | Apple Apple Inc originally licensed software from Nuance to provide speech recognition capability to its digital assistant Siri .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["|","Apple","Apple","Inc","originally","licensed","software","from","Nuance","to","provide","speech","recognition","capability","to","its","digital","assistant","Siri","."],"labels":["O","B-organization","B-organization","I-organization","O","O","O","O","B-organization","O","O","B-task","I-task","O","O","O","O","O","B-product","O"],"target_index":null,"target_label":null},"label_list":["university","conference","task","organization","metric","field","researcher","programming_language","person","location","algorithm","product","country"]}
{"id":"352","dataset":"crossner_ai","split":"test","instance":{"id":"352","prompt_labels":"Columbia(B-organization) released(O) several(O) 3D(O) westerns(O) produced(O) by(O) Sam(B-person) Katzman(I-person) and(O) directed(O) by(O) William(B-person) Castle(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, location, university, country, task, field, researcher, organization, algorithm, product, conference, person, programming language and O.\nSentence: Columbia released several 3D westerns produced by Sam Katzman and directed by William Castle .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Columbia","released","several","3D","westerns","produced","by","Sam","Katzman","and","directed","by","William","Castle","."],"labels":["B-organization","O","O","O","O","O","O","B-person","I-person","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["metric","location","university","country","task","field","researcher","organization","algorithm","product","conference","person","programming_language"]}
{"id":"354","dataset":"crossner_ai","split":"test","instance":{"id":"354","prompt_labels":"Here(O) is(O) an(O) example(O) of(O) R(B-programming language) code(O) :(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, researcher, country, product, field, university, programming language, location, algorithm, metric, organization, person, task and O.\nSentence: Here is an example of R code :","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Here","is","an","example","of","R","code",":"],"labels":["O","O","O","O","O","B-programming language","O","O"],"target_index":null,"target_label":null},"label_list":["conference","researcher","country","product","field","university","programming_language","location","algorithm","metric","organization","person","task"]}
{"id":"358","dataset":"crossner_ai","split":"test","instance":{"id":"358","prompt_labels":"The(O) metric(O) was(O) designed(O) to(O) fix(O) some(O) of(O) the(O) problems(O) found(O) in(O) the(O) more(O) popular(O) BLEU(B-metric) metric(I-metric) ,(O) and(O) also(O) produce(O) good(O) correlation(O) with(O) human(O) judgement(O) at(O) the(O) sentence(O) or(O) segment(O) level(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, country, conference, university, algorithm, metric, product, organization, field, programming language, task, location, person and O.\nSentence: The metric was designed to fix some of the problems found in the more popular BLEU metric , and also produce good correlation with human judgement at the sentence or segment level .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","metric","was","designed","to","fix","some","of","the","problems","found","in","the","more","popular","BLEU","metric",",","and","also","produce","good","correlation","with","human","judgement","at","the","sentence","or","segment","level","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["researcher","country","conference","university","algorithm","metric","product","organization","field","programming_language","task","location","person"]}
{"id":"362","dataset":"crossner_ai","split":"test","instance":{"id":"362","prompt_labels":"The(O) measured(O) performance(O) on(O) test(O) data(O) of(O) eight(O) naive(O) WSI(B-task) across(O) various(O) tauopathies(O) resulted(O) in(O) the(O) recall(B-metric) ,(O) precision(B-metric) ,(O) and(O) an(O) F1(B-metric) score(I-metric) of(O) 0.92(O) ,(O) 0.72(O) ,(O) and(O) 0.81(O) ,(O) respectively(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, person, metric, organization, conference, university, researcher, algorithm, task, country, field, product, location and O.\nSentence: The measured performance on test data of eight naive WSI across various tauopathies resulted in the recall , precision , and an F1 score of 0.92 , 0.72 , and 0.81 , respectively .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","measured","performance","on","test","data","of","eight","naive","WSI","across","various","tauopathies","resulted","in","the","recall",",","precision",",","and","an","F1","score","of","0.92",",","0.72",",","and","0.81",",","respectively","."],"labels":["O","O","O","O","O","O","O","O","O","B-task","O","O","O","O","O","O","B-metric","O","B-metric","O","O","O","B-metric","I-metric","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","person","metric","organization","conference","university","researcher","algorithm","task","country","field","product","location"]}
{"id":"363","dataset":"crossner_ai","split":"test","instance":{"id":"363","prompt_labels":"With(O) the(O) help(O) of(O) advanced(O) AR(B-field) technologies(O) ((O) e.g.(O) adding(O) computer(B-field) vision(I-field) ,(O) incorporating(O) AR(B-field) cameras(O) into(O) smartphone(O) and(O) object(B-task) recognition(I-task) )(O) the(O) information(O) about(O) the(O) surrounding(O) real(O) world(O) of(O) the(O) user(O) becomes(O) interactive(O) and(O) digitally(O) manipulated(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: metric, organization, algorithm, university, researcher, task, conference, product, field, programming language, location, person, country and O.\nSentence: With the help of advanced AR technologies ( e.g. adding computer vision , incorporating AR cameras into smartphone and object recognition ) the information about the surrounding real world of the user becomes interactive and digitally manipulated .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["With","the","help","of","advanced","AR","technologies","(","e.g.","adding","computer","vision",",","incorporating","AR","cameras","into","smartphone","and","object","recognition",")","the","information","about","the","surrounding","real","world","of","the","user","becomes","interactive","and","digitally","manipulated","."],"labels":["O","O","O","O","O","B-field","O","O","O","O","B-field","I-field","O","O","B-field","O","O","O","O","B-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["metric","organization","algorithm","university","researcher","task","conference","product","field","programming_language","location","person","country"]}
{"id":"365","dataset":"crossner_ai","split":"test","instance":{"id":"365","prompt_labels":"Not(O) only(O) does(O) this(O) alter(O) the(O) performance(O) of(O) all(O) subsequent(O) tests(O) on(O) the(O) retained(O) explanatory(O) model(O) ,(O) it(O) may(O) introduce(O) bias(O) and(O) alter(O) mean(B-metric) square(I-metric) error(I-metric) in(O) estimation(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, university, location, conference, algorithm, programming language, product, organization, researcher, task, metric, field, country and O.\nSentence: Not only does this alter the performance of all subsequent tests on the retained explanatory model , it may introduce bias and alter mean square error in estimation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Not","only","does","this","alter","the","performance","of","all","subsequent","tests","on","the","retained","explanatory","model",",","it","may","introduce","bias","and","alter","mean","square","error","in","estimation","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","university","location","conference","algorithm","programming_language","product","organization","researcher","task","metric","field","country"]}
{"id":"366","dataset":"crossner_ai","split":"test","instance":{"id":"366","prompt_labels":"Bigrams(O) are(O) used(O) in(O) most(O) successful(O) language(B-algorithm) model(I-algorithm) s(O) for(O) speech(B-task) recognition(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, metric, programming language, organization, country, algorithm, product, university, researcher, task, location, field, person and O.\nSentence: Bigrams are used in most successful language model s for speech recognition .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Bigrams","are","used","in","most","successful","language","model","s","for","speech","recognition","."],"labels":["O","O","O","O","O","O","B-algorithm","I-algorithm","O","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["conference","metric","programming_language","organization","country","algorithm","product","university","researcher","task","location","field","person"]}
{"id":"368","dataset":"crossner_ai","split":"test","instance":{"id":"368","prompt_labels":"An(O) eigenface(O) ((O) The(O) approach(O) of(O) using(O) eigenfaces(O) for(O) Facial(B-product) recognition(I-product) system(I-product) was(O) developed(O) by(O) Sirovich(B-researcher) and(O) Kirby(B-researcher) ((O) 1987(O) )(O) and(O) used(O) by(O) Matthew(B-researcher) Turk(I-researcher) and(O) Alex(B-researcher) Pentland(I-researcher) in(O) face(B-task) classification(I-task) .(O) Turk(B-researcher) ,(I-researcher) Matthew(I-researcher) A(I-researcher) and(O) Pentland(B-researcher) ,(I-researcher) Alex(I-researcher) P.(I-researcher) Face(B-task) recognition(I-task) using(O) eigenfaces(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: conference, task, country, programming language, person, researcher, product, university, algorithm, location, organization, field, metric and O.\nSentence: An eigenface ( The approach of using eigenfaces for Facial recognition system was developed by Sirovich and Kirby ( 1987 ) and used by Matthew Turk and Alex Pentland in face classification . Turk , Matthew A and Pentland , Alex P. Face recognition using eigenfaces .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["An","eigenface","(","The","approach","of","using","eigenfaces","for","Facial","recognition","system","was","developed","by","Sirovich","and","Kirby","(","1987",")","and","used","by","Matthew","Turk","and","Alex","Pentland","in","face","classification",".","Turk",",","Matthew","A","and","Pentland",",","Alex","P.","Face","recognition","using","eigenfaces","."],"labels":["O","O","O","O","O","O","O","O","O","B-product","I-product","I-product","O","O","O","B-researcher","O","B-researcher","O","O","O","O","O","O","B-researcher","I-researcher","O","B-researcher","I-researcher","O","B-task","I-task","O","B-researcher","I-researcher","I-researcher","I-researcher","O","B-researcher","I-researcher","I-researcher","I-researcher","B-task","I-task","O","O","O"],"target_index":null,"target_label":null},"label_list":["conference","task","country","programming_language","person","researcher","product","university","algorithm","location","organization","field","metric"]}
{"id":"374","dataset":"crossner_ai","split":"test","instance":{"id":"374","prompt_labels":"He(O) is(O) a(O) Fellow(O) of(O) the(O) Association(B-conference) for(I-conference) Computing(I-conference) Machinery(I-conference) ,(O) IEEE(B-organization) ,(O) American(B-conference) Association(I-conference) for(I-conference) the(I-conference) Advancement(I-conference) of(I-conference) Science(I-conference) ,(O) IAPR(B-conference) and(O) SPIE(B-conference) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: algorithm, organization, product, task, metric, conference, field, person, location, programming language, country, researcher, university and O.\nSentence: He is a Fellow of the Association for Computing Machinery , IEEE , American Association for the Advancement of Science , IAPR and SPIE .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","a","Fellow","of","the","Association","for","Computing","Machinery",",","IEEE",",","American","Association","for","the","Advancement","of","Science",",","IAPR","and","SPIE","."],"labels":["O","O","O","O","O","O","B-conference","I-conference","I-conference","I-conference","O","B-organization","O","B-conference","I-conference","I-conference","I-conference","I-conference","I-conference","I-conference","O","B-conference","O","B-conference","O"],"target_index":null,"target_label":null},"label_list":["algorithm","organization","product","task","metric","conference","field","person","location","programming_language","country","researcher","university"]}
{"id":"379","dataset":"crossner_ai","split":"test","instance":{"id":"379","prompt_labels":"If(O) it(O) is(O) assumed(O) that(O) distortion(O) is(O) measured(O) by(O) mean(B-metric) squared(I-metric) error(I-metric) ,(O) the(O) distortion(O) D(O) ,(O) is(O) given(O) by(O) :(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: field, person, researcher, programming language, task, country, location, product, metric, algorithm, organization, university, conference and O.\nSentence: If it is assumed that distortion is measured by mean squared error , the distortion D , is given by :","prediction_output":null,"prediction_outputs":null,"group":null,"words":["If","it","is","assumed","that","distortion","is","measured","by","mean","squared","error",",","the","distortion","D",",","is","given","by",":"],"labels":["O","O","O","O","O","O","O","O","O","B-metric","I-metric","I-metric","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["field","person","researcher","programming_language","task","country","location","product","metric","algorithm","organization","university","conference"]}
{"id":"385","dataset":"crossner_ai","split":"test","instance":{"id":"385","prompt_labels":"Increasingly(O) ,(O) however(O) ,(O) work(O) at(O) Cycorp(B-organization) involves(O) giving(O) the(O) Cyc(B-product) system(I-product) the(O) ability(O) to(O) communicate(O) with(O) end(O) users(O) in(O) natural(O) language(O) ,(O) and(O) to(O) assist(O) with(O) the(O) ongoing(O) knowledge(O) formation(O) process(O) via(O) machine(B-field) learning(I-field) and(O) natural(B-task) language(I-task) understanding(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, metric, university, task, programming language, organization, field, product, conference, country, algorithm, researcher and O.\nSentence: Increasingly , however , work at Cycorp involves giving the Cyc system the ability to communicate with end users in natural language , and to assist with the ongoing knowledge formation process via machine learning and natural language understanding .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Increasingly",",","however",",","work","at","Cycorp","involves","giving","the","Cyc","system","the","ability","to","communicate","with","end","users","in","natural","language",",","and","to","assist","with","the","ongoing","knowledge","formation","process","via","machine","learning","and","natural","language","understanding","."],"labels":["O","O","O","O","O","O","B-organization","O","O","O","B-product","I-product","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","B-task","I-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["location","person","metric","university","task","programming_language","organization","field","product","conference","country","algorithm","researcher"]}
{"id":"391","dataset":"crossner_ai","split":"test","instance":{"id":"391","prompt_labels":"In(O) recent(O) research(O) ,(O) kernel-based(O) methods(O) such(O) as(O) support(B-algorithm) vector(I-algorithm) machine(I-algorithm) s(O) have(O) shown(O) superior(O) performance(O) in(O) supervised(B-field) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, field, metric, organization, location, conference, researcher, product, task, programming language, algorithm, country, person and O.\nSentence: In recent research , kernel-based methods such as support vector machine s have shown superior performance in supervised .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","recent","research",",","kernel-based","methods","such","as","support","vector","machine","s","have","shown","superior","performance","in","supervised","."],"labels":["O","O","O","O","O","O","O","O","B-algorithm","I-algorithm","I-algorithm","O","O","O","O","O","O","B-field","O"],"target_index":null,"target_label":null},"label_list":["university","field","metric","organization","location","conference","researcher","product","task","programming_language","algorithm","country","person"]}
{"id":"394","dataset":"crossner_ai","split":"test","instance":{"id":"394","prompt_labels":"Where(O) Bilingual(B-metric) evaluation(I-metric) understudy(I-metric) simply(O) calculates(O) n-gram(B-metric) precision(I-metric) adding(O) equal(O) weight(O) to(O) each(O) one(O) ,(O) NIST(B-metric) also(O) calculates(O) how(O) informative(O) a(O) particular(O) n-gram(O) is(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, conference, task, organization, university, country, researcher, metric, programming language, location, field, algorithm, person and O.\nSentence: Where Bilingual evaluation understudy simply calculates n-gram precision adding equal weight to each one , NIST also calculates how informative a particular n-gram is .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Where","Bilingual","evaluation","understudy","simply","calculates","n-gram","precision","adding","equal","weight","to","each","one",",","NIST","also","calculates","how","informative","a","particular","n-gram","is","."],"labels":["O","B-metric","I-metric","I-metric","O","O","B-metric","I-metric","O","O","O","O","O","O","O","B-metric","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","conference","task","organization","university","country","researcher","metric","programming_language","location","field","algorithm","person"]}
{"id":"401","dataset":"crossner_ai","split":"test","instance":{"id":"401","prompt_labels":"It(O) is(O) covered(O) by(O) American(O) National(O) Standards(O) Institute(O) /(O) NISO(O) standard(O) Z39.50(O) ,(O) and(O) International(O) Organization(O) for(O) Standardization(O) standard(O) 23950(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: task, metric, location, person, university, field, product, organization, researcher, algorithm, programming language, country, conference and O.\nSentence: It is covered by American National Standards Institute / NISO standard Z39.50 , and International Organization for Standardization standard 23950 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","is","covered","by","American","National","Standards","Institute","/","NISO","standard","Z39.50",",","and","International","Organization","for","Standardization","standard","23950","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["task","metric","location","person","university","field","product","organization","researcher","algorithm","programming_language","country","conference"]}
{"id":"412","dataset":"crossner_ai","split":"test","instance":{"id":"412","prompt_labels":"In(O) particular(O) ,(O) his(O) research(O) focused(O) on(O) areas(O) such(O) as(O) text(B-field) mining(I-field) ((O) extraction(B-task) ,(O) categorization(B-task) ,(O) novelty(B-task) detection(I-task) )(O) and(O) in(O) new(O) theoretical(O) frameworks(O) such(O) as(O) a(O) unified(O) utility-based(O) theory(O) bridging(O) information(B-task) retrieval(I-task) ,(O) Automatic(B-task) summarization(I-task) ,(O) free-text(B-task) Question(I-task) Answering(I-task) and(O) related(O) tasks(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, conference, researcher, organization, field, location, university, metric, product, programming language, country, task, algorithm and O.\nSentence: In particular , his research focused on areas such as text mining ( extraction , categorization , novelty detection ) and in new theoretical frameworks such as a unified utility-based theory bridging information retrieval , Automatic summarization , free-text Question Answering and related tasks .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","particular",",","his","research","focused","on","areas","such","as","text","mining","(","extraction",",","categorization",",","novelty","detection",")","and","in","new","theoretical","frameworks","such","as","a","unified","utility-based","theory","bridging","information","retrieval",",","Automatic","summarization",",","free-text","Question","Answering","and","related","tasks","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-field","I-field","O","B-task","O","B-task","O","B-task","I-task","O","O","O","O","O","O","O","O","O","O","O","O","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","I-task","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","conference","researcher","organization","field","location","university","metric","product","programming_language","country","task","algorithm"]}
{"id":"416","dataset":"crossner_ai","split":"test","instance":{"id":"416","prompt_labels":"For(O) a(O) recommender(B-product) system(I-product) ,(O) sentiment(B-task) analysis(I-task) has(O) been(O) proven(O) to(O) be(O) a(O) valuable(O) technique(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: programming language, metric, researcher, person, conference, location, country, field, product, task, organization, university, algorithm and O.\nSentence: For a recommender system , sentiment analysis has been proven to be a valuable technique .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","a","recommender","system",",","sentiment","analysis","has","been","proven","to","be","a","valuable","technique","."],"labels":["O","O","B-product","I-product","O","B-task","I-task","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["programming_language","metric","researcher","person","conference","location","country","field","product","task","organization","university","algorithm"]}
{"id":"422","dataset":"crossner_ai","split":"test","instance":{"id":"422","prompt_labels":"Other(O) areas(O) of(O) usage(O) for(O) ontologies(O) within(O) NLP(B-field) include(O) information(B-task) retrieval(I-task) ,(O) information(B-task) extraction(I-task) and(O) automatic(B-task) summarization(I-task) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: researcher, person, organization, task, algorithm, metric, university, programming language, field, country, conference, product, location and O.\nSentence: Other areas of usage for ontologies within NLP include information retrieval , information extraction and automatic summarization .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","areas","of","usage","for","ontologies","within","NLP","include","information","retrieval",",","information","extraction","and","automatic","summarization","."],"labels":["O","O","O","O","O","O","O","B-field","O","B-task","I-task","O","B-task","I-task","O","B-task","I-task","O"],"target_index":null,"target_label":null},"label_list":["researcher","person","organization","task","algorithm","metric","university","programming_language","field","country","conference","product","location"]}
{"id":"423","dataset":"crossner_ai","split":"test","instance":{"id":"423","prompt_labels":"The(O) Institute(O) has(O) collaborated(O) closely(O) with(O) the(O) Janelia(B-organization) Farm(I-organization) Campus(I-organization) of(I-organization) Howard(I-organization) Hughes(I-organization) Medical(I-organization) Institute(I-organization) ,(O) the(O) Allen(B-organization) Institute(I-organization) for(I-organization) Brain(I-organization) Science(I-organization) and(O) the(O) National(B-organization) Institutes(I-organization) of(I-organization) Health(I-organization) to(O) develop(O) better(O) methods(O) of(O) reconstructing(O) neuronal(O) architectures(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: product, field, conference, country, researcher, location, organization, programming language, university, algorithm, metric, person, task and O.\nSentence: The Institute has collaborated closely with the Janelia Farm Campus of Howard Hughes Medical Institute , the Allen Institute for Brain Science and the National Institutes of Health to develop better methods of reconstructing neuronal architectures .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Institute","has","collaborated","closely","with","the","Janelia","Farm","Campus","of","Howard","Hughes","Medical","Institute",",","the","Allen","Institute","for","Brain","Science","and","the","National","Institutes","of","Health","to","develop","better","methods","of","reconstructing","neuronal","architectures","."],"labels":["O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["product","field","conference","country","researcher","location","organization","programming_language","university","algorithm","metric","person","task"]}
{"id":"425","dataset":"crossner_ai","split":"test","instance":{"id":"425","prompt_labels":"Events(O) are(O) held(O) worldwide(O) ,(O) and(O) are(O) most(O) popular(O) in(O) the(O) United(B-country) Kingdom(I-country) ,(O) United(B-country) States(I-country) ,(O) Japan(B-country) ,(O) Singapore(B-country) ,(O) India(B-country) ,(O) South(B-country) Korea(I-country) and(O) becoming(O) popular(O) in(O) subcontinent(O) countries(O) such(O) as(O) Sri(B-country) Lanka(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, researcher, organization, conference, location, programming language, university, field, metric, task, person, algorithm, product and O.\nSentence: Events are held worldwide , and are most popular in the United Kingdom , United States , Japan , Singapore , India , South Korea and becoming popular in subcontinent countries such as Sri Lanka .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Events","are","held","worldwide",",","and","are","most","popular","in","the","United","Kingdom",",","United","States",",","Japan",",","Singapore",",","India",",","South","Korea","and","becoming","popular","in","subcontinent","countries","such","as","Sri","Lanka","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","B-country","I-country","O","B-country","O","B-country","O","B-country","O","B-country","I-country","O","O","O","O","O","O","O","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["country","researcher","organization","conference","location","programming_language","university","field","metric","task","person","algorithm","product"]}
{"id":"0","dataset":"crossner_literature","split":"test","instance":{"id":"0","prompt_labels":"Two(O) decades(O) after(O) Frank(B-writer) Herbert(I-writer) 's(O) death(O) ,(O) his(O) son(O) Brian(B-writer) Herbert(I-writer) ,(O) along(O) with(O) Kevin(B-writer) J.(I-writer) Anderson(I-writer) ,(O) published(O) two(O) sequel(O) s(O) -(O) Hunters(B-book) of(I-book) Dune(I-book) ((O) 2006(O) )(O) and(O) Sandworms(B-book) of(I-book) Dune(I-book) ((O) 2007(O) )(O) -(O) based(O) on(O) notes(O) left(O) behind(O) by(O) Frank(B-writer) Herbert(I-writer) for(O) what(O) he(O) referred(O) to(O) as(O) Dune(B-book) 7(I-book) ,(O) his(O) own(O) planned(O) seventh(O) novel(B-literary genre) in(O) the(O) Dune(O) series(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, poem, book, country, location, award, event, person, organization, literary genre, magazine and O.\nSentence: Two decades after Frank Herbert 's death , his son Brian Herbert , along with Kevin J. Anderson , published two sequel s - Hunters of Dune ( 2006 ) and Sandworms of Dune ( 2007 ) - based on notes left behind by Frank Herbert for what he referred to as Dune 7 , his own planned seventh novel in the Dune series .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Two","decades","after","Frank","Herbert","'s","death",",","his","son","Brian","Herbert",",","along","with","Kevin","J.","Anderson",",","published","two","sequel","s","-","Hunters","of","Dune","(","2006",")","and","Sandworms","of","Dune","(","2007",")","-","based","on","notes","left","behind","by","Frank","Herbert","for","what","he","referred","to","as","Dune","7",",","his","own","planned","seventh","novel","in","the","Dune","series","."],"labels":["O","O","O","B-writer","I-writer","O","O","O","O","O","B-writer","I-writer","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","B-book","I-book","O","O","O","O","O","B-literary genre","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","poem","book","country","location","award","event","person","organization","literary_genre","magazine"]}
{"id":"3","dataset":"crossner_literature","split":"test","instance":{"id":"3","prompt_labels":"Stoker(B-writer) 's(O) inspirations(O) for(O) the(O) story(O) ,(O) in(O) addition(O) to(O) Whitby(B-location) ,(O) may(O) have(O) included(O) a(O) visit(O) to(O) Slains(B-location) Castle(I-location) in(O) Aberdeenshire(B-location) ,(O) a(O) visit(O) to(O) the(O) crypts(O) of(O) St.(B-location) Michan(I-location) 's(I-location) Church(I-location) in(O) Dublin(B-location) ,(O) and(O) the(O) novella(B-literary genre) Carmilla(B-book) by(O) Sheridan(B-writer) Le(I-writer) Fanu(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, event, person, award, poem, magazine, organization, country, location, writer, book and O.\nSentence: Stoker 's inspirations for the story , in addition to Whitby , may have included a visit to Slains Castle in Aberdeenshire , a visit to the crypts of St. Michan 's Church in Dublin , and the novella Carmilla by Sheridan Le Fanu .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Stoker","'s","inspirations","for","the","story",",","in","addition","to","Whitby",",","may","have","included","a","visit","to","Slains","Castle","in","Aberdeenshire",",","a","visit","to","the","crypts","of","St.","Michan","'s","Church","in","Dublin",",","and","the","novella","Carmilla","by","Sheridan","Le","Fanu","."],"labels":["B-writer","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","B-location","O","O","O","B-literary genre","B-book","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","event","person","award","poem","magazine","organization","country","location","writer","book"]}
{"id":"13","dataset":"crossner_literature","split":"test","instance":{"id":"13","prompt_labels":"Among(O) his(O) awards(O) for(O) particular(O) works(O) were(O) :(O) Hugo(B-award) Award(I-award) s(O) ,(O) in(O) 1963(O) for(O) The(B-book) Dragon(I-book) Masters(I-book) ,(O) in(O) 1967(O) for(O) The(B-book) Last(I-book) Castle(I-book) ,(O) and(O) in(O) 2010(O) for(O) his(O) memoir(O) This(B-book) is(I-book) Me(I-book) ,(I-book) Jack(I-book) Vance(I-book) !(I-book) ;(O) a(O) Nebula(B-award) Award(I-award) in(O) 1966(O) ,(O) also(O) for(O) The(B-book) Last(I-book) Castle(I-book) ;(O) the(O) Jupiter(B-award) Award(I-award) in(O) 1975(O) ;(O) the(O) World(B-award) Fantasy(I-award) Award(I-award) in(O) 1990(O) for(O) Lyonesse(B-book) :(I-book) Madouc(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, event, organization, award, literary genre, location, country, magazine, book, poem, person and O.\nSentence: Among his awards for particular works were : Hugo Award s , in 1963 for The Dragon Masters , in 1967 for The Last Castle , and in 2010 for his memoir This is Me , Jack Vance ! ; a Nebula Award in 1966 , also for The Last Castle ; the Jupiter Award in 1975 ; the World Fantasy Award in 1990 for Lyonesse : Madouc .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Among","his","awards","for","particular","works","were",":","Hugo","Award","s",",","in","1963","for","The","Dragon","Masters",",","in","1967","for","The","Last","Castle",",","and","in","2010","for","his","memoir","This","is","Me",",","Jack","Vance","!",";","a","Nebula","Award","in","1966",",","also","for","The","Last","Castle",";","the","Jupiter","Award","in","1975",";","the","World","Fantasy","Award","in","1990","for","Lyonesse",":","Madouc","."],"labels":["O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","I-book","I-book","I-book","O","O","B-award","I-award","O","O","O","O","O","B-book","I-book","I-book","O","O","B-award","I-award","O","O","O","O","B-award","I-award","I-award","O","O","O","B-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["writer","event","organization","award","literary_genre","location","country","magazine","book","poem","person"]}
{"id":"15","dataset":"crossner_literature","split":"test","instance":{"id":"15","prompt_labels":"Better(O) parts(O) followed(O) ,(O) including(O) The(B-book) Cider(I-book) House(I-book) Rules(I-book) ((O) 1999(O) )(O) ,(O) for(O) which(O) he(O) won(O) his(O) second(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Supporting(I-award) Actor(I-award) ..(O) BBC(B-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, magazine, person, writer, book, country, event, literary genre, organization, location, poem and O.\nSentence: Better parts followed , including The Cider House Rules ( 1999 ) , for which he won his second Academy Award for Best Supporting Actor .. BBC .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Better","parts","followed",",","including","The","Cider","House","Rules","(","1999",")",",","for","which","he","won","his","second","Academy","Award","for","Best","Supporting","Actor","..","BBC","."],"labels":["O","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-organization","O"],"target_index":null,"target_label":null},"label_list":["award","magazine","person","writer","book","country","event","literary_genre","organization","location","poem"]}
{"id":"16","dataset":"crossner_literature","split":"test","instance":{"id":"16","prompt_labels":"Her(O) novel(B-literary genre) Quartet(B-book) in(I-book) Autumn(I-book) ((O) 1977(O) )(O) was(O) nominated(O) for(O) the(O) Booker(B-award) Prize(I-award) that(O) year(O) ,(O) and(O) she(O) was(O) elected(O) as(O) a(O) Fellow(O) of(O) the(O) Royal(B-organization) Society(I-organization) of(I-organization) Literature(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, poem, event, magazine, organization, location, award, person, country, literary genre, book and O.\nSentence: Her novel Quartet in Autumn ( 1977 ) was nominated for the Booker Prize that year , and she was elected as a Fellow of the Royal Society of Literature .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Her","novel","Quartet","in","Autumn","(","1977",")","was","nominated","for","the","Booker","Prize","that","year",",","and","she","was","elected","as","a","Fellow","of","the","Royal","Society","of","Literature","."],"labels":["O","B-literary genre","B-book","I-book","I-book","O","O","O","O","O","O","O","B-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["writer","poem","event","magazine","organization","location","award","person","country","literary_genre","book"]}
{"id":"17","dataset":"crossner_literature","split":"test","instance":{"id":"17","prompt_labels":"Gide(B-person) went(O) successively(O) to(O) Middle(B-country) Congo(I-country) ((O) now(O) the(O) Republic(B-country) of(I-country) the(I-country) Congo(I-country) )(O) ,(O) Ubangi-Shari(B-country) ((O) now(O) the(O) Central(B-country) African(I-country) Republic(I-country) )(O) ,(O) briefly(O) to(O) Chad(B-country) and(O) then(O) to(O) Cameroon(B-country) before(O) returning(O) to(O) France(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, poem, writer, event, person, literary genre, magazine, book, organization, country and O.\nSentence: Gide went successively to Middle Congo ( now the Republic of the Congo ) , Ubangi-Shari ( now the Central African Republic ) , briefly to Chad and then to Cameroon before returning to France .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Gide","went","successively","to","Middle","Congo","(","now","the","Republic","of","the","Congo",")",",","Ubangi-Shari","(","now","the","Central","African","Republic",")",",","briefly","to","Chad","and","then","to","Cameroon","before","returning","to","France","."],"labels":["B-person","O","O","O","B-country","I-country","O","O","O","B-country","I-country","I-country","I-country","O","O","B-country","O","O","O","B-country","I-country","I-country","O","O","O","O","B-country","O","O","O","B-country","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["location","award","poem","writer","event","person","literary_genre","magazine","book","organization","country"]}
{"id":"19","dataset":"crossner_literature","split":"test","instance":{"id":"19","prompt_labels":"Elwyn(B-writer) Brooks(I-writer) White(I-writer) ((O) July(O) 11(O) ,(O) 1899(O) -(O) October(O) 1(O) ,(O) 1985(O) )(O) In(O) addition(O) ,(O) he(O) was(O) a(O) contributor(O) to(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) magazine(O) ,(O) and(O) also(O) a(O) co-author(O) of(O) the(O) English(O) language(O) style(O) guide(O) The(B-book) Elements(I-book) of(I-book) Style(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, literary genre, poem, organization, book, writer, magazine, country, person, event, location and O.\nSentence: Elwyn Brooks White ( July 11 , 1899 - October 1 , 1985 ) In addition , he was a contributor to The New Yorker magazine , and also a co-author of the English language style guide The Elements of Style .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Elwyn","Brooks","White","(","July","11",",","1899","-","October","1",",","1985",")","In","addition",",","he","was","a","contributor","to","The","New","Yorker","magazine",",","and","also","a","co-author","of","the","English","language","style","guide","The","Elements","of","Style","."],"labels":["B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["award","literary_genre","poem","organization","book","writer","magazine","country","person","event","location"]}
{"id":"20","dataset":"crossner_literature","split":"test","instance":{"id":"20","prompt_labels":"Among(O) his(O) best-known(O) works(O) -(O) most(O) of(O) which(O) were(O) published(O) posthumously(O) -(O) are(O) Dulce(B-poem) et(I-poem) Decorum(I-poem) est(I-poem) ,(O) Insensibility(B-poem) ,(O) Anthem(B-poem) for(I-poem) Doomed(I-poem) Youth(I-poem) ,(O) Futility(B-poem) ,(O) Spring(B-poem) Offensive(I-poem) and(O) Strange(B-poem) Meeting(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, book, event, organization, poem, literary genre, location, writer, person, country, award and O.\nSentence: Among his best-known works - most of which were published posthumously - are Dulce et Decorum est , Insensibility , Anthem for Doomed Youth , Futility , Spring Offensive and Strange Meeting .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Among","his","best-known","works","-","most","of","which","were","published","posthumously","-","are","Dulce","et","Decorum","est",",","Insensibility",",","Anthem","for","Doomed","Youth",",","Futility",",","Spring","Offensive","and","Strange","Meeting","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-poem","I-poem","I-poem","I-poem","O","B-poem","O","B-poem","I-poem","I-poem","I-poem","O","B-poem","O","B-poem","I-poem","O","B-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["magazine","book","event","organization","poem","literary_genre","location","writer","person","country","award"]}
{"id":"23","dataset":"crossner_literature","split":"test","instance":{"id":"23","prompt_labels":"The(B-book) Saga(I-book) of(I-book) Eric(I-book) Brighteyes(I-book) is(O) an(O) epic(B-literary genre) viking(I-literary genre) novel(I-literary genre) by(O) H.(B-writer) Rider(I-writer) Haggard(I-writer) that(O) concerns(O) the(O) adventures(O) of(O) its(O) eponymous(O) principal(O) character(O) in(O) 10th-century(O) Iceland(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, writer, poem, person, location, literary genre, country, event, book, organization, award and O.\nSentence: The Saga of Eric Brighteyes is an epic viking novel by H. Rider Haggard that concerns the adventures of its eponymous principal character in 10th-century Iceland .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Saga","of","Eric","Brighteyes","is","an","epic","viking","novel","by","H.","Rider","Haggard","that","concerns","the","adventures","of","its","eponymous","principal","character","in","10th-century","Iceland","."],"labels":["B-book","I-book","I-book","I-book","I-book","O","O","B-literary genre","I-literary genre","I-literary genre","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["magazine","writer","poem","person","location","literary_genre","country","event","book","organization","award"]}
{"id":"26","dataset":"crossner_literature","split":"test","instance":{"id":"26","prompt_labels":"Kim(B-writer) Stanley(I-writer) Robinson(I-writer) '(O) s(O) novel(B-literary genre) ,(O) The(B-book) Years(I-book) of(I-book) Rice(I-book) and(I-book) Salt(I-book) ((O) 2002(O) )(O) ,(O) starts(O) at(O) the(O) point(O) of(O) divergence(O) with(O) Timur(B-person) turning(O) his(O) army(O) away(O) from(O) Europe(B-location) ,(O) and(O) the(O) Black(O) Death(O) has(O) killed(O) 99(O) %(O) of(O) Europe(B-location) 's(O) population(O) ,(O) instead(O) of(O) only(O) a(O) third(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, person, book, magazine, country, writer, award, organization, literary genre, event, location and O.\nSentence: Kim Stanley Robinson ' s novel , The Years of Rice and Salt ( 2002 ) , starts at the point of divergence with Timur turning his army away from Europe , and the Black Death has killed 99 % of Europe 's population , instead of only a third .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Kim","Stanley","Robinson","'","s","novel",",","The","Years","of","Rice","and","Salt","(","2002",")",",","starts","at","the","point","of","divergence","with","Timur","turning","his","army","away","from","Europe",",","and","the","Black","Death","has","killed","99","%","of","Europe","'s","population",",","instead","of","only","a","third","."],"labels":["B-writer","I-writer","I-writer","O","O","B-literary genre","O","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","person","book","magazine","country","writer","award","organization","literary_genre","event","location"]}
{"id":"29","dataset":"crossner_literature","split":"test","instance":{"id":"29","prompt_labels":"Later(O) ,(O) he(O) was(O) best(O) known(O) for(O) his(O) setting(O) the(O) Draumkvedet(B-poem) ((O) 1905(O) )(O) and(O) the(O) Poetic(B-book) Edda(I-book) ((O) 1908(O) )(O) into(O) modern(O) Norwegian(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, person, organization, event, country, writer, poem, magazine, book, literary genre and O.\nSentence: Later , he was best known for his setting the Draumkvedet ( 1905 ) and the Poetic Edda ( 1908 ) into modern Norwegian .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Later",",","he","was","best","known","for","his","setting","the","Draumkvedet","(","1905",")","and","the","Poetic","Edda","(","1908",")","into","modern","Norwegian","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-poem","O","O","O","O","O","B-book","I-book","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","location","person","organization","event","country","writer","poem","magazine","book","literary_genre"]}
{"id":"31","dataset":"crossner_literature","split":"test","instance":{"id":"31","prompt_labels":"The(B-book) Faerie(I-book) Queene(I-book) by(O) Edmund(B-writer) Spenser(I-writer) ,(O) with(O) its(O) stanzas(O) of(O) eight(O) iambic(O) pentameter(O) lines(O) followed(O) by(O) one(O) alexandrine(O) ,(O) exemplifies(O) what(O) came(O) to(O) be(O) its(O) chief(O) role(O) :(O) as(O) a(O) somewhat(O) infrequent(O) variant(O) line(O) in(O) an(O) otherwise(O) iambic(O) pentameter(O) context(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, writer, person, organization, event, award, magazine, location, poem, literary genre, book and O.\nSentence: The Faerie Queene by Edmund Spenser , with its stanzas of eight iambic pentameter lines followed by one alexandrine , exemplifies what came to be its chief role : as a somewhat infrequent variant line in an otherwise iambic pentameter context .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Faerie","Queene","by","Edmund","Spenser",",","with","its","stanzas","of","eight","iambic","pentameter","lines","followed","by","one","alexandrine",",","exemplifies","what","came","to","be","its","chief","role",":","as","a","somewhat","infrequent","variant","line","in","an","otherwise","iambic","pentameter","context","."],"labels":["B-book","I-book","I-book","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","writer","person","organization","event","award","magazine","location","poem","literary_genre","book"]}
{"id":"37","dataset":"crossner_literature","split":"test","instance":{"id":"37","prompt_labels":"Writing(O) for(O) The(B-magazine) Spectator(I-magazine) in(O) the(O) UK(B-country) ,(O) Graham(B-writer) Greene(I-writer) gave(O) the(O) film(O) a(O) mixed(O) good(O) review(O) ,(O) characterizing(O) the(O) first(O) half(O) of(O) the(O) film(O) as(O) simple(O) and(O) direct(O) and(O) TRUE(O) ,(O) but(O) complaining(O) that(O) the(O) second(O) half(O) displays(O) a(O) little(O) less(O) than(O) life(O) and(O) that(O) the(O) last(O) hour(O) was(O) permeated(O) by(O) banality(O) and(O) ennui(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, award, book, poem, person, country, event, magazine, location, literary genre, organization and O.\nSentence: Writing for The Spectator in the UK , Graham Greene gave the film a mixed good review , characterizing the first half of the film as simple and direct and TRUE , but complaining that the second half displays a little less than life and that the last hour was permeated by banality and ennui .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Writing","for","The","Spectator","in","the","UK",",","Graham","Greene","gave","the","film","a","mixed","good","review",",","characterizing","the","first","half","of","the","film","as","simple","and","direct","and","TRUE",",","but","complaining","that","the","second","half","displays","a","little","less","than","life","and","that","the","last","hour","was","permeated","by","banality","and","ennui","."],"labels":["O","O","B-magazine","I-magazine","O","O","B-country","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","award","book","poem","person","country","event","magazine","location","literary_genre","organization"]}
{"id":"40","dataset":"crossner_literature","split":"test","instance":{"id":"40","prompt_labels":"Carmilla(B-book) is(O) an(O) 1872(O) Gothic(B-literary genre) novel(I-literary genre) la(O) by(O) Irish(O) author(O) Sheridan(B-writer) Le(I-writer) Fanu(I-writer) and(O) one(O) of(O) the(O) early(O) works(O) of(O) vampire(B-literary genre) fiction(I-literary genre) ,(O) predating(O) Bram(B-writer) Stoker(I-writer) '(O) s(O) Dracula(B-book) ((O) 1897(O) )(O) by(O) 26(O) years(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, book, person, literary genre, organization, poem, writer, magazine, location, event, award and O.\nSentence: Carmilla is an 1872 Gothic novel la by Irish author Sheridan Le Fanu and one of the early works of vampire fiction , predating Bram Stoker ' s Dracula ( 1897 ) by 26 years .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Carmilla","is","an","1872","Gothic","novel","la","by","Irish","author","Sheridan","Le","Fanu","and","one","of","the","early","works","of","vampire","fiction",",","predating","Bram","Stoker","'","s","Dracula","(","1897",")","by","26","years","."],"labels":["B-book","O","O","O","B-literary genre","I-literary genre","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","B-writer","I-writer","O","O","B-book","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","book","person","literary_genre","organization","poem","writer","magazine","location","event","award"]}
{"id":"41","dataset":"crossner_literature","split":"test","instance":{"id":"41","prompt_labels":"She(O) appeared(O) in(O) Catherine(B-writer) Cookson(I-writer) '(O) s(O) The(B-book) Fifteen(I-book) Streets(I-book) ,(O) alongside(O) Sean(B-person) Bean(I-person) and(O) Owen(B-person) Teale(I-person) in(O) 1989(O) ;(O) Our(B-book) Own(I-book) Kind(I-book) ((O) Bush(B-person) ,(O) 1991(O) )(O) ;(O) Deadly(B-book) Advice(I-book) ((O) Fletcher(B-person) ,(O) 1993(O) )(O) ;(O) Cabaret(B-book) ((O) Donmar(B-location) Warehouse(I-location) 1994(O) )(O) ;(O) Macbeth(B-book) ((O) Greenwich(B-location) Theatre(I-location) ,(O) 1995(O) )(O) ;(O) and(O) Absurd(B-book) Person(I-book) Singular(I-book) ((O) Garrick(B-location) Theatre(I-location) ,(O) 2007(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, poem, writer, organization, magazine, book, person, location, award, country, literary genre and O.\nSentence: She appeared in Catherine Cookson ' s The Fifteen Streets , alongside Sean Bean and Owen Teale in 1989 ; Our Own Kind ( Bush , 1991 ) ; Deadly Advice ( Fletcher , 1993 ) ; Cabaret ( Donmar Warehouse 1994 ) ; Macbeth ( Greenwich Theatre , 1995 ) ; and Absurd Person Singular ( Garrick Theatre , 2007 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","appeared","in","Catherine","Cookson","'","s","The","Fifteen","Streets",",","alongside","Sean","Bean","and","Owen","Teale","in","1989",";","Our","Own","Kind","(","Bush",",","1991",")",";","Deadly","Advice","(","Fletcher",",","1993",")",";","Cabaret","(","Donmar","Warehouse","1994",")",";","Macbeth","(","Greenwich","Theatre",",","1995",")",";","and","Absurd","Person","Singular","(","Garrick","Theatre",",","2007",")","."],"labels":["O","O","O","B-writer","I-writer","O","O","B-book","I-book","I-book","O","O","B-person","I-person","O","B-person","I-person","O","O","O","B-book","I-book","I-book","O","B-person","O","O","O","O","B-book","I-book","O","B-person","O","O","O","O","B-book","O","B-location","I-location","O","O","O","B-book","O","B-location","I-location","O","O","O","O","O","B-book","I-book","I-book","O","B-location","I-location","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","poem","writer","organization","magazine","book","person","location","award","country","literary_genre"]}
{"id":"46","dataset":"crossner_literature","split":"test","instance":{"id":"46","prompt_labels":"His(O) best(O) known(O) works(O) include(O) The(B-book) Day(I-book) of(I-book) the(I-book) Triffids(I-book) ((O) 1951(O) )(O) and(O) The(B-book) Midwich(I-book) Cuckoos(I-book) ((O) 1957(O) )(O) ,(O) the(O) latter(O) filmed(O) twice(O) as(O) Village(O) of(O) the(O) Damned(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, book, literary genre, country, organization, event, award, magazine, writer, poem, person and O.\nSentence: His best known works include The Day of the Triffids ( 1951 ) and The Midwich Cuckoos ( 1957 ) , the latter filmed twice as Village of the Damned .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","best","known","works","include","The","Day","of","the","Triffids","(","1951",")","and","The","Midwich","Cuckoos","(","1957",")",",","the","latter","filmed","twice","as","Village","of","the","Damned","."],"labels":["O","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","book","literary_genre","country","organization","event","award","magazine","writer","poem","person"]}
{"id":"49","dataset":"crossner_literature","split":"test","instance":{"id":"49","prompt_labels":"In(O) addition(O) to(O) his(O) Best(B-award) Picture(I-award) Awards(I-award) ,(O) he(O) received(O) an(O) Academy(B-award) Honorary(I-award) Award(I-award) for(O) his(O) film(O) contributions(O) ,(O) the(O) Palme(B-award) d(I-award) 'Or(I-award) ((O) posthumously(O) )(O) for(O) Union(B-organization) Pacific(I-organization) ((O) 1939(O) )(O) ,(O) a(O) Directors(B-award) Guild(I-award) of(I-award) America(I-award) Award(I-award) for(I-award) Lifetime(I-award) Achievement(I-award) ,(O) and(O) the(O) Irving(B-award) G.(I-award) Thalberg(I-award) Memorial(I-award) Award(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, person, event, location, poem, organization, magazine, literary genre, writer, book and O.\nSentence: In addition to his Best Picture Awards , he received an Academy Honorary Award for his film contributions , the Palme d 'Or ( posthumously ) for Union Pacific ( 1939 ) , a Directors Guild of America Award for Lifetime Achievement , and the Irving G. Thalberg Memorial Award .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition","to","his","Best","Picture","Awards",",","he","received","an","Academy","Honorary","Award","for","his","film","contributions",",","the","Palme","d","'Or","(","posthumously",")","for","Union","Pacific","(","1939",")",",","a","Directors","Guild","of","America","Award","for","Lifetime","Achievement",",","and","the","Irving","G.","Thalberg","Memorial","Award","."],"labels":["O","O","O","O","B-award","I-award","I-award","O","O","O","O","B-award","I-award","I-award","O","O","O","O","O","O","B-award","I-award","I-award","O","O","O","O","B-organization","I-organization","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["country","award","person","event","location","poem","organization","magazine","literary_genre","writer","book"]}
{"id":"62","dataset":"crossner_literature","split":"test","instance":{"id":"62","prompt_labels":"The(O) Reference(O) Library(O) ,(O) Astounding(B-magazine) Science(I-magazine) Fiction(I-magazine) ,(O) March(O) 1952(O) ,(O) pp.159(O) In(O) his(O) Books(O) column(O) for(O) The(B-magazine) Magazine(I-magazine) of(I-magazine) Fantasy(I-magazine) &(I-magazine) Science(I-magazine) Fiction(I-magazine) ,(O) Damon(B-writer) Knight(I-writer) selected(O) the(O) novel(B-literary genre) as(O) one(O) of(O) the(O) 10(O) best(O) SF(B-literary genre) books(O) of(O) the(O) 1950s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, event, award, writer, magazine, person, country, literary genre, organization, location, book and O.\nSentence: The Reference Library , Astounding Science Fiction , March 1952 , pp.159 In his Books column for The Magazine of Fantasy & Science Fiction , Damon Knight selected the novel as one of the 10 best SF books of the 1950s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Reference","Library",",","Astounding","Science","Fiction",",","March","1952",",","pp.159","In","his","Books","column","for","The","Magazine","of","Fantasy","&","Science","Fiction",",","Damon","Knight","selected","the","novel","as","one","of","the","10","best","SF","books","of","the","1950s","."],"labels":["O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","I-magazine","I-magazine","I-magazine","I-magazine","I-magazine","O","B-writer","I-writer","O","O","B-literary genre","O","O","O","O","O","O","B-literary genre","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","event","award","writer","magazine","person","country","literary_genre","organization","location","book"]}
{"id":"77","dataset":"crossner_literature","split":"test","instance":{"id":"77","prompt_labels":"It(O) is(O) influenced(O) by(O) the(O) tragedies(O) of(O) Sophocles(B-writer) and(O) Seneca(B-writer) ,(O) and(O) tells(O) the(O) story(O) of(O) princess(O) Alvida(B-person) of(O) Norway(B-country) ,(O) who(O) is(O) forcibly(O) married(O) off(O) to(O) the(O) Goth(O) king(O) Torrismondo(B-person) ,(O) when(O) she(O) is(O) devoted(O) to(O) her(O) childhood(O) friend(O) ,(O) king(O) Germondo(B-person) of(O) Sweden(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, book, person, location, event, writer, country, literary genre, magazine, poem and O.\nSentence: It is influenced by the tragedies of Sophocles and Seneca , and tells the story of princess Alvida of Norway , who is forcibly married off to the Goth king Torrismondo , when she is devoted to her childhood friend , king Germondo of Sweden .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","is","influenced","by","the","tragedies","of","Sophocles","and","Seneca",",","and","tells","the","story","of","princess","Alvida","of","Norway",",","who","is","forcibly","married","off","to","the","Goth","king","Torrismondo",",","when","she","is","devoted","to","her","childhood","friend",",","king","Germondo","of","Sweden","."],"labels":["O","O","O","O","O","O","O","B-writer","O","B-writer","O","O","O","O","O","O","O","B-person","O","B-country","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","B-person","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["organization","award","book","person","location","event","writer","country","literary_genre","magazine","poem"]}
{"id":"80","dataset":"crossner_literature","split":"test","instance":{"id":"80","prompt_labels":"The(B-poem) Gentle(I-poem) Shepherd(I-poem) ,(O) first(O) published(O) in(O) 1725(O) ,(O) was(O) dedicated(O) to(O) her(O) by(O) Allan(B-writer) Ramsay(I-writer) and(O) Hamilton(B-writer) of(I-writer) Bangour(I-writer) wrote(O) flattering(O) verse(B-literary genre) to(O) Susanna(B-person) ,(I-person) Lady(I-person) Eglinton(I-person) and(O) her(O) daughters(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, book, location, country, magazine, award, poem, organization, person, event, writer and O.\nSentence: The Gentle Shepherd , first published in 1725 , was dedicated to her by Allan Ramsay and Hamilton of Bangour wrote flattering verse to Susanna , Lady Eglinton and her daughters .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Gentle","Shepherd",",","first","published","in","1725",",","was","dedicated","to","her","by","Allan","Ramsay","and","Hamilton","of","Bangour","wrote","flattering","verse","to","Susanna",",","Lady","Eglinton","and","her","daughters","."],"labels":["B-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","B-literary genre","O","B-person","I-person","I-person","I-person","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","book","location","country","magazine","award","poem","organization","person","event","writer"]}
{"id":"84","dataset":"crossner_literature","split":"test","instance":{"id":"84","prompt_labels":"Some(O) of(O) his(O) best(O) known(O) poems(O) are(O) about(O) love(B-literary genre) ,(O) such(O) as(O) Funeral(B-poem) Blues(I-poem) ;(O) on(O) political(B-literary genre) and(I-literary genre) social(I-literary genre) themes(I-literary genre) ,(O) such(O) as(O) September(B-poem) 1(I-poem) ,(I-poem) 1939(I-poem) and(O) The(B-poem) Shield(I-poem) of(I-poem) Achilles(I-poem) ;(O) on(O) cultural(B-literary genre) and(I-literary genre) psychological(I-literary genre) themes(I-literary genre) ,(O) such(O) as(O) The(B-poem) Age(I-poem) of(I-poem) Anxiety(I-poem) ;(O) and(O) on(O) religious(B-literary genre) themes(I-literary genre) such(O) as(O) For(B-poem) the(I-poem) Time(I-poem) Being(I-poem) and(O) Horae(B-poem) Canonicae(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, location, writer, person, literary genre, magazine, book, award, organization, poem and O.\nSentence: Some of his best known poems are about love , such as Funeral Blues ; on political and social themes , such as September 1 , 1939 and The Shield of Achilles ; on cultural and psychological themes , such as The Age of Anxiety ; and on religious themes such as For the Time Being and Horae Canonicae .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","of","his","best","known","poems","are","about","love",",","such","as","Funeral","Blues",";","on","political","and","social","themes",",","such","as","September","1",",","1939","and","The","Shield","of","Achilles",";","on","cultural","and","psychological","themes",",","such","as","The","Age","of","Anxiety",";","and","on","religious","themes","such","as","For","the","Time","Being","and","Horae","Canonicae","."],"labels":["O","O","O","O","O","O","O","O","B-literary genre","O","O","O","B-poem","I-poem","O","O","B-literary genre","I-literary genre","I-literary genre","I-literary genre","O","O","O","B-poem","I-poem","I-poem","I-poem","O","B-poem","I-poem","I-poem","I-poem","O","O","B-literary genre","I-literary genre","I-literary genre","I-literary genre","O","O","O","B-poem","I-poem","I-poem","I-poem","O","O","O","B-literary genre","I-literary genre","O","O","B-poem","I-poem","I-poem","I-poem","O","B-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["event","country","location","writer","person","literary_genre","magazine","book","award","organization","poem"]}
{"id":"87","dataset":"crossner_literature","split":"test","instance":{"id":"87","prompt_labels":"Soon(O) after(O) his(O) return(O) to(O) England(B-country) ,(O) Dickens(B-writer) began(O) work(O) on(O) the(O) first(O) of(O) his(O) Christmas(B-event) stories(O) ,(O) A(B-book) Christmas(I-book) Carol(I-book) ,(O) written(O) in(O) 1843(O) ,(O) which(O) was(O) followed(O) by(O) The(B-book) Chimes(I-book) in(O) 1844(O) and(O) The(B-book) Cricket(I-book) on(I-book) the(I-book) Hearth(I-book) in(O) 1845(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, poem, book, location, literary genre, writer, event, organization, award, magazine, country and O.\nSentence: Soon after his return to England , Dickens began work on the first of his Christmas stories , A Christmas Carol , written in 1843 , which was followed by The Chimes in 1844 and The Cricket on the Hearth in 1845 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Soon","after","his","return","to","England",",","Dickens","began","work","on","the","first","of","his","Christmas","stories",",","A","Christmas","Carol",",","written","in","1843",",","which","was","followed","by","The","Chimes","in","1844","and","The","Cricket","on","the","Hearth","in","1845","."],"labels":["O","O","O","O","O","B-country","O","B-writer","O","O","O","O","O","O","O","B-event","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","B-book","I-book","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","poem","book","location","literary_genre","writer","event","organization","award","magazine","country"]}
{"id":"93","dataset":"crossner_literature","split":"test","instance":{"id":"93","prompt_labels":"Bernardo(B-writer) Bertolucci(I-writer) ((O) ;(O) 16(O) March(O) 1941(O) -(O) 26(O) November(O) 2018(O) )(O) was(O) an(O) Italian(O) director(O) and(O) screenwriter(O) ,(O) whose(O) films(O) include(O) The(O) Conformist(O) ,(O) Last(O) Tango(O) in(O) Paris(O) ,(O) 1900(O) ,(O) The(O) Last(O) Emperor(O) ((O) for(O) which(O) he(O) won(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) and(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) )(O) ,(O) The(O) Sheltering(O) Sky(O) ,(O) Little(O) Buddha(O) ,(O) Stealing(O) Beauty(O) and(O) The(O) Dreamers(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, book, country, magazine, event, location, writer, literary genre, person, award, organization and O.\nSentence: Bernardo Bertolucci ( ; 16 March 1941 - 26 November 2018 ) was an Italian director and screenwriter , whose films include The Conformist , Last Tango in Paris , 1900 , The Last Emperor ( for which he won the Academy Award for Best Director and the Academy Award for Best Adapted Screenplay ) , The Sheltering Sky , Little Buddha , Stealing Beauty and The Dreamers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Bernardo","Bertolucci","(",";","16","March","1941","-","26","November","2018",")","was","an","Italian","director","and","screenwriter",",","whose","films","include","The","Conformist",",","Last","Tango","in","Paris",",","1900",",","The","Last","Emperor","(","for","which","he","won","the","Academy","Award","for","Best","Director","and","the","Academy","Award","for","Best","Adapted","Screenplay",")",",","The","Sheltering","Sky",",","Little","Buddha",",","Stealing","Beauty","and","The","Dreamers","."],"labels":["B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","book","country","magazine","event","location","writer","literary_genre","person","award","organization"]}
{"id":"96","dataset":"crossner_literature","split":"test","instance":{"id":"96","prompt_labels":"The(B-book) Color(I-book) Purple(I-book) is(O) a(O) 1982(O) epistolary(B-literary genre) novel(I-literary genre) by(O) American(O) author(O) Alice(B-writer) Walker(I-writer) which(O) won(O) the(O) 1983(O) Pulitzer(B-award) Prize(I-award) for(I-award) Fiction(I-award) and(O) the(O) National(B-award) Book(I-award) Award(I-award) for(I-award) Fiction(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, event, magazine, literary genre, country, award, writer, person, organization, location, book and O.\nSentence: The Color Purple is a 1982 epistolary novel by American author Alice Walker which won the 1983 Pulitzer Prize for Fiction and the National Book Award for Fiction .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Color","Purple","is","a","1982","epistolary","novel","by","American","author","Alice","Walker","which","won","the","1983","Pulitzer","Prize","for","Fiction","and","the","National","Book","Award","for","Fiction","."],"labels":["B-book","I-book","I-book","O","O","O","B-literary genre","I-literary genre","O","O","O","B-writer","I-writer","O","O","O","O","B-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["poem","event","magazine","literary_genre","country","award","writer","person","organization","location","book"]}
{"id":"98","dataset":"crossner_literature","split":"test","instance":{"id":"98","prompt_labels":"Humorist(O) Henry(B-person) Morgan(I-person) had(O) a(O) recurring(O) role(O) as(O) a(O) humor(O) writer(O) for(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) ,(O) which(O) was(O) said(O) to(O) be(O) based(O) on(O) real-life(O) humorist(O) /(O) actor(O) Robert(B-writer) Benchley(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, literary genre, organization, writer, award, magazine, book, country, person, location, event and O.\nSentence: Humorist Henry Morgan had a recurring role as a humor writer for The New Yorker , which was said to be based on real-life humorist / actor Robert Benchley .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Humorist","Henry","Morgan","had","a","recurring","role","as","a","humor","writer","for","The","New","Yorker",",","which","was","said","to","be","based","on","real-life","humorist","/","actor","Robert","Benchley","."],"labels":["O","B-person","I-person","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["poem","literary_genre","organization","writer","award","magazine","book","country","person","location","event"]}
{"id":"101","dataset":"crossner_literature","split":"test","instance":{"id":"101","prompt_labels":"In(O) his(O) later(O) writings(O) ,(O) some(O) believe(O) that(O) he(O) proposed(O) ideas(O) such(O) as(O) purification(O) of(O) venial(O) sin(O) s(O) after(O) death(O) in(O) purgatory(O) ((O) The(B-book) Great(I-book) Divorce(I-book) and(O) Letters(B-book) to(I-book) Malcolm(I-book) )(O) and(O) mortal(O) sin(O) ((O) The(B-book) Screwtape(I-book) Letters(I-book) )(O) ,(O) which(O) are(O) generally(O) considered(O) to(O) be(O) Roman(O) Catholic(O) teachings(O) ,(O) although(O) they(O) are(O) also(O) widely(O) held(O) in(O) Anglicanism(O) ((O) particularly(O) in(O) high(O) church(O) Anglo-Catholic(O) circles(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, literary genre, poem, location, award, event, organization, writer, book, country, magazine and O.\nSentence: In his later writings , some believe that he proposed ideas such as purification of venial sin s after death in purgatory ( The Great Divorce and Letters to Malcolm ) and mortal sin ( The Screwtape Letters ) , which are generally considered to be Roman Catholic teachings , although they are also widely held in Anglicanism ( particularly in high church Anglo-Catholic circles ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","his","later","writings",",","some","believe","that","he","proposed","ideas","such","as","purification","of","venial","sin","s","after","death","in","purgatory","(","The","Great","Divorce","and","Letters","to","Malcolm",")","and","mortal","sin","(","The","Screwtape","Letters",")",",","which","are","generally","considered","to","be","Roman","Catholic","teachings",",","although","they","are","also","widely","held","in","Anglicanism","(","particularly","in","high","church","Anglo-Catholic","circles",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","B-book","I-book","I-book","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","literary_genre","poem","location","award","event","organization","writer","book","country","magazine"]}
{"id":"103","dataset":"crossner_literature","split":"test","instance":{"id":"103","prompt_labels":"Xuanxue(O) was(O) a(O) philosophical(O) school(O) that(O) combined(O) elements(O) of(O) Confucianism(O) and(O) Taoism(O) to(O) reinterpret(O) the(O) I(B-book) Ching(I-book) ,(O) Tao(B-book) Te(I-book) Ching(I-book) ,(O) and(O) Zhuangzi(B-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, magazine, person, award, event, literary genre, poem, location, country, organization and O.\nSentence: Xuanxue was a philosophical school that combined elements of Confucianism and Taoism to reinterpret the I Ching , Tao Te Ching , and Zhuangzi .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Xuanxue","was","a","philosophical","school","that","combined","elements","of","Confucianism","and","Taoism","to","reinterpret","the","I","Ching",",","Tao","Te","Ching",",","and","Zhuangzi","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","O","B-book","I-book","I-book","O","O","B-book","O"],"target_index":null,"target_label":null},"label_list":["writer","book","magazine","person","award","event","literary_genre","poem","location","country","organization"]}
{"id":"111","dataset":"crossner_literature","split":"test","instance":{"id":"111","prompt_labels":"Landing(O) in(O) Boston(B-location) ,(O) he(O) devoted(O) the(O) rest(O) of(O) the(O) month(O) to(O) a(O) round(O) of(O) dinners(O) with(O) such(O) notables(O) as(O) Ralph(B-writer) Waldo(I-writer) Emerson(I-writer) ,(O) Henry(B-writer) Wadsworth(I-writer) Longfellow(I-writer) ,(O) and(O) his(O) American(O) publisher(O) ,(O) James(B-writer) Thomas(I-writer) Fields(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, literary genre, book, event, organization, person, country, writer, award, magazine, poem and O.\nSentence: Landing in Boston , he devoted the rest of the month to a round of dinners with such notables as Ralph Waldo Emerson , Henry Wadsworth Longfellow , and his American publisher , James Thomas Fields .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Landing","in","Boston",",","he","devoted","the","rest","of","the","month","to","a","round","of","dinners","with","such","notables","as","Ralph","Waldo","Emerson",",","Henry","Wadsworth","Longfellow",",","and","his","American","publisher",",","James","Thomas","Fields","."],"labels":["O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["location","literary_genre","book","event","organization","person","country","writer","award","magazine","poem"]}
{"id":"117","dataset":"crossner_literature","split":"test","instance":{"id":"117","prompt_labels":"His(O) most(O) famous(O) works(O) were(O) his(O) longer(O) and(O) more(O) moralistic(O) Troy(B-poem) Book(I-poem) ((O) 1412-20(O) )(O) ,(O) a(O) 30,000(O) line(O) translation(O) of(O) the(O) Latin(B-literary genre) prose(I-literary genre) narrative(O) by(O) Guido(B-writer) delle(I-writer) Colonne(I-writer) ,(O) Historia(B-poem) destructionis(I-poem) Troiae(I-poem) ,(O) the(O) Siege(B-poem) of(I-poem) Thebes(I-poem) which(O) was(O) translated(O) from(O) a(O) French(B-literary genre) prose(I-literary genre) redaction(O) of(O) the(O) Roman(B-poem) de(I-poem) Thebes(I-poem) and(O) the(O) Fall(B-poem) of(I-poem) Princes(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, organization, book, literary genre, location, writer, person, award, poem, magazine and O.\nSentence: His most famous works were his longer and more moralistic Troy Book ( 1412-20 ) , a 30,000 line translation of the Latin prose narrative by Guido delle Colonne , Historia destructionis Troiae , the Siege of Thebes which was translated from a French prose redaction of the Roman de Thebes and the Fall of Princes .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","most","famous","works","were","his","longer","and","more","moralistic","Troy","Book","(","1412-20",")",",","a","30,000","line","translation","of","the","Latin","prose","narrative","by","Guido","delle","Colonne",",","Historia","destructionis","Troiae",",","the","Siege","of","Thebes","which","was","translated","from","a","French","prose","redaction","of","the","Roman","de","Thebes","and","the","Fall","of","Princes","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-poem","I-poem","O","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","B-writer","I-writer","I-writer","O","B-poem","I-poem","I-poem","O","O","B-poem","I-poem","I-poem","O","O","O","O","O","B-literary genre","I-literary genre","O","O","O","B-poem","I-poem","I-poem","O","O","B-poem","I-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["event","country","organization","book","literary_genre","location","writer","person","award","poem","magazine"]}
{"id":"124","dataset":"crossner_literature","split":"test","instance":{"id":"124","prompt_labels":"Hunter(B-writer) S.(I-writer) Thompson(I-writer) wrote(O) a(O) scathing(O) piece(O) denouncing(O) Nixon(B-person) for(O) Rolling(B-magazine) Stone(I-magazine) ,(O) entitled(O) He(O) Was(O) a(O) Crook(O) ((O) which(O) also(O) appeared(O) a(O) month(O) later(O) in(O) The(B-magazine) Atlantic(I-magazine) )(O) .ref(O) name(O) =(O) atlantic(O) In(O) his(O) article(O) ,(O) Thompson(B-writer) described(O) Nixon(B-person) as(O) a(O) political(O) monster(O) straight(O) out(O) of(O) Grendel(O) and(O) a(O) very(O) dangerous(O) enemy(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, magazine, organization, poem, literary genre, country, location, book, writer, award and O.\nSentence: Hunter S. Thompson wrote a scathing piece denouncing Nixon for Rolling Stone , entitled He Was a Crook ( which also appeared a month later in The Atlantic ) .ref name = atlantic In his article , Thompson described Nixon as a political monster straight out of Grendel and a very dangerous enemy .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Hunter","S.","Thompson","wrote","a","scathing","piece","denouncing","Nixon","for","Rolling","Stone",",","entitled","He","Was","a","Crook","(","which","also","appeared","a","month","later","in","The","Atlantic",")",".ref","name","=","atlantic","In","his","article",",","Thompson","described","Nixon","as","a","political","monster","straight","out","of","Grendel","and","a","very","dangerous","enemy","."],"labels":["B-writer","I-writer","I-writer","O","O","O","O","O","B-person","O","B-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","O","O","O","O","O","O","O","O","O","B-writer","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","person","magazine","organization","poem","literary_genre","country","location","book","writer","award"]}
{"id":"127","dataset":"crossner_literature","split":"test","instance":{"id":"127","prompt_labels":"Susan(B-writer) Cooper(I-writer) related(O) that(O) The(O) power(O) and(O) range(O) of(O) Alan(B-writer) Garner(I-writer) 's(O) astounding(O) talent(O) has(O) grown(O) with(O) every(O) book(O) he(O) 's(O) written(O) ,(O) whilst(O) David(B-writer) Almond(I-writer) called(O) him(O) one(O) of(O) Britain(B-country) 's(O) greatest(O) writers(O) whose(O) works(O) really(O) matter(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, poem, literary genre, award, country, organization, person, magazine, event, book, writer and O.\nSentence: Susan Cooper related that The power and range of Alan Garner 's astounding talent has grown with every book he 's written , whilst David Almond called him one of Britain 's greatest writers whose works really matter .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Susan","Cooper","related","that","The","power","and","range","of","Alan","Garner","'s","astounding","talent","has","grown","with","every","book","he","'s","written",",","whilst","David","Almond","called","him","one","of","Britain","'s","greatest","writers","whose","works","really","matter","."],"labels":["B-writer","I-writer","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","B-country","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","poem","literary_genre","award","country","organization","person","magazine","event","book","writer"]}
{"id":"140","dataset":"crossner_literature","split":"test","instance":{"id":"140","prompt_labels":"Akhundzade(B-writer) 's(O) first(O) published(O) work(O) was(O) Eastern(B-poem) poem(I-poem) on(I-poem) the(I-poem) death(I-poem) of(I-poem) Pushkin(I-poem) ((O) 1837(O) )(O) ,(O) written(O) to(O) lament(O) the(O) death(O) of(O) the(O) great(O) Russian(O) poet(O) Alexander(B-writer) Pushkin(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, writer, organization, magazine, poem, book, country, literary genre, award, event and O.\nSentence: Akhundzade 's first published work was Eastern poem on the death of Pushkin ( 1837 ) , written to lament the death of the great Russian poet Alexander Pushkin .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Akhundzade","'s","first","published","work","was","Eastern","poem","on","the","death","of","Pushkin","(","1837",")",",","written","to","lament","the","death","of","the","great","Russian","poet","Alexander","Pushkin","."],"labels":["B-writer","O","O","O","O","O","B-poem","I-poem","I-poem","I-poem","I-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["person","location","writer","organization","magazine","poem","book","country","literary_genre","award","event"]}
{"id":"147","dataset":"crossner_literature","split":"test","instance":{"id":"147","prompt_labels":"After(O) his(O) death(O) ,(O) some(O) of(O) his(O) poems(O) ,(O) notably(O) Funeral(B-poem) Blues(I-poem) ,(O) Musée(B-poem) des(I-poem) Beaux(I-poem) Arts(I-poem) ,(O) Refugee(B-poem) Blues(I-poem) ,(O) The(B-poem) Unknown(I-poem) Citizen(I-poem) ,(O) and(O) September(B-poem) 1(I-poem) ,(I-poem) 1939(I-poem) ,(O) became(O) known(O) to(O) a(O) much(O) wider(O) public(O) than(O) during(O) his(O) lifetime(O) through(O) films(O) ,(O) broadcasts(O) ,(O) and(O) popular(O) media(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, country, event, book, literary genre, location, organization, person, writer, award, poem and O.\nSentence: After his death , some of his poems , notably Funeral Blues , Musée des Beaux Arts , Refugee Blues , The Unknown Citizen , and September 1 , 1939 , became known to a much wider public than during his lifetime through films , broadcasts , and popular media .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","his","death",",","some","of","his","poems",",","notably","Funeral","Blues",",","Musée","des","Beaux","Arts",",","Refugee","Blues",",","The","Unknown","Citizen",",","and","September","1",",","1939",",","became","known","to","a","much","wider","public","than","during","his","lifetime","through","films",",","broadcasts",",","and","popular","media","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-poem","I-poem","O","B-poem","I-poem","I-poem","I-poem","O","B-poem","I-poem","O","B-poem","I-poem","I-poem","O","O","B-poem","I-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","country","event","book","literary_genre","location","organization","person","writer","award","poem"]}
{"id":"152","dataset":"crossner_literature","split":"test","instance":{"id":"152","prompt_labels":"During(O) this(O) time(O) ,(O) he(O) made(O) the(O) short(O) film(O) I(B-poem) Am(I-poem) Joaquin(I-poem) based(O) on(O) the(O) legendary(O) poem(B-literary genre) by(O) Rodolfo(B-writer) Corky(I-writer) Gonzáles(I-writer) ((O) it(O) was(O) later(O) inducted(O) into(O) the(O) National(B-organization) Film(I-organization) Registry(I-organization) in(O) 2010(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, award, location, magazine, event, poem, country, person, book, literary genre, organization and O.\nSentence: During this time , he made the short film I Am Joaquin based on the legendary poem by Rodolfo Corky Gonzáles ( it was later inducted into the National Film Registry in 2010 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","this","time",",","he","made","the","short","film","I","Am","Joaquin","based","on","the","legendary","poem","by","Rodolfo","Corky","Gonzáles","(","it","was","later","inducted","into","the","National","Film","Registry","in","2010",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-poem","I-poem","I-poem","O","O","O","O","B-literary genre","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","award","location","magazine","event","poem","country","person","book","literary_genre","organization"]}
{"id":"153","dataset":"crossner_literature","split":"test","instance":{"id":"153","prompt_labels":"Some(O) of(O) his(O) early(O) novels(B-literary genre) ,(O) called(O) scientific(B-literary genre) romance(I-literary genre) s(O) ,(O) invented(O) several(O) themes(O) now(O) classic(O) in(O) science(B-literary genre) fiction(I-literary genre) in(O) such(O) works(O) as(O) The(B-book) Time(I-book) Machine(I-book) ,(O) The(B-book) Island(I-book) of(I-book) Doctor(I-book) Moreau(I-book) ,(O) The(B-book) Invisible(I-book) Man(I-book) ,(O) The(B-book) War(I-book) of(I-book) the(I-book) Worlds(I-book) ,(O) The(B-book) Sleeper(I-book) Awakes(I-book) ,(O) and(O) The(B-book) First(I-book) Men(I-book) in(I-book) the(I-book) Moon(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, award, country, organization, book, person, magazine, event, location, writer, poem and O.\nSentence: Some of his early novels , called scientific romance s , invented several themes now classic in science fiction in such works as The Time Machine , The Island of Doctor Moreau , The Invisible Man , The War of the Worlds , The Sleeper Awakes , and The First Men in the Moon .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","of","his","early","novels",",","called","scientific","romance","s",",","invented","several","themes","now","classic","in","science","fiction","in","such","works","as","The","Time","Machine",",","The","Island","of","Doctor","Moreau",",","The","Invisible","Man",",","The","War","of","the","Worlds",",","The","Sleeper","Awakes",",","and","The","First","Men","in","the","Moon","."],"labels":["O","O","O","O","B-literary genre","O","O","B-literary genre","I-literary genre","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","O","O","B-book","I-book","I-book","O","B-book","I-book","I-book","I-book","I-book","O","B-book","I-book","I-book","O","B-book","I-book","I-book","I-book","I-book","O","B-book","I-book","I-book","O","O","B-book","I-book","I-book","I-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","award","country","organization","book","person","magazine","event","location","writer","poem"]}
{"id":"154","dataset":"crossner_literature","split":"test","instance":{"id":"154","prompt_labels":"When(O) Handke(B-writer) was(O) awarded(O) the(O) International(B-award) Ibsen(I-award) Award(I-award) in(O) 2014(O) ,(O) it(O) caused(O) some(O) calls(O) On(O) the(O) other(O) hand(O) ,(O) Jon(B-writer) Fosse(I-writer) ,(O) former(O) recipient(O) of(O) the(O) prize(O) ,(O) welcomed(O) the(O) decision(O) ,(O) saying(O) that(O) Handke(B-writer) was(O) a(O) worthy(O) recipient(O) and(O) deserved(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, literary genre, location, country, book, writer, magazine, person, organization, poem, award and O.\nSentence: When Handke was awarded the International Ibsen Award in 2014 , it caused some calls On the other hand , Jon Fosse , former recipient of the prize , welcomed the decision , saying that Handke was a worthy recipient and deserved the Nobel Prize in Literature .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["When","Handke","was","awarded","the","International","Ibsen","Award","in","2014",",","it","caused","some","calls","On","the","other","hand",",","Jon","Fosse",",","former","recipient","of","the","prize",",","welcomed","the","decision",",","saying","that","Handke","was","a","worthy","recipient","and","deserved","the","Nobel","Prize","in","Literature","."],"labels":["O","B-writer","O","O","O","B-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["event","literary_genre","location","country","book","writer","magazine","person","organization","poem","award"]}
{"id":"163","dataset":"crossner_literature","split":"test","instance":{"id":"163","prompt_labels":"After(O) the(O) KGB(O) had(O) confiscated(O) Solzhenitsyn(B-writer) 's(O) materials(O) in(O) Moscow(B-location) ,(O) during(O) 1965-67(O) ,(O) the(O) preparatory(O) drafts(O) of(O) The(B-book) Gulag(I-book) Archipelago(I-book) were(O) turned(O) into(O) finished(O) typescript(O) in(O) hiding(O) at(O) his(O) friends(O) '(O) homes(O) in(O) Estonia(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, award, book, organization, poem, location, magazine, literary genre, writer, event and O.\nSentence: After the KGB had confiscated Solzhenitsyn 's materials in Moscow , during 1965-67 , the preparatory drafts of The Gulag Archipelago were turned into finished typescript in hiding at his friends ' homes in Estonia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","the","KGB","had","confiscated","Solzhenitsyn","'s","materials","in","Moscow",",","during","1965-67",",","the","preparatory","drafts","of","The","Gulag","Archipelago","were","turned","into","finished","typescript","in","hiding","at","his","friends","'","homes","in","Estonia","."],"labels":["O","O","O","O","O","B-writer","O","O","O","B-location","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["person","country","award","book","organization","poem","location","magazine","literary_genre","writer","event"]}
{"id":"168","dataset":"crossner_literature","split":"test","instance":{"id":"168","prompt_labels":"He(O) has(O) directed(O) film(O) versions(O) of(O) his(O) plays(O) The(B-book) Grey(I-book) Zone(I-book) and(O) Eye(B-book) of(I-book) God(I-book) ((O) for(O) which(O) he(O) received(O) an(O) Independent(B-award) Spirit(I-award) Awards(I-award) nomination(O) for(O) the(O) Someone(B-award) to(I-award) Watch(I-award) Award(I-award) )(O) ,(O) as(O) well(O) as(O) writing(O) and(O) directing(O) two(O) original(O) screenplays(O) :(O) 1998(B-book) 's(I-book) Kansas(I-book) and(O) Leaves(B-book) of(I-book) Grass(I-book) ,(O) which(O) was(O) released(O) in(O) 2009(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, magazine, literary genre, book, person, location, organization, writer, poem, event and O.\nSentence: He has directed film versions of his plays The Grey Zone and Eye of God ( for which he received an Independent Spirit Awards nomination for the Someone to Watch Award ) , as well as writing and directing two original screenplays : 1998 's Kansas and Leaves of Grass , which was released in 2009 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","directed","film","versions","of","his","plays","The","Grey","Zone","and","Eye","of","God","(","for","which","he","received","an","Independent","Spirit","Awards","nomination","for","the","Someone","to","Watch","Award",")",",","as","well","as","writing","and","directing","two","original","screenplays",":","1998","'s","Kansas","and","Leaves","of","Grass",",","which","was","released","in","2009","."],"labels":["O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","B-book","I-book","I-book","O","O","O","O","O","O","B-award","I-award","I-award","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","B-book","I-book","I-book","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","award","magazine","literary_genre","book","person","location","organization","writer","poem","event"]}
{"id":"170","dataset":"crossner_literature","split":"test","instance":{"id":"170","prompt_labels":"The(B-book) Rape(I-book) of(I-book) Nanking(I-book) :(I-book) The(I-book) Forgotten(I-book) Holocaust(I-book) of(I-book) World(I-book) War(I-book) II(I-book) is(O) a(O) bestselling(O) 1997(O) non-fiction(O) book(O) written(O) by(O) Iris(B-writer) Chang(I-writer) about(O) the(O) 1937-1938(B-event) Nanking(I-event) Massacre(I-event) ,(O) the(O) massacre(O) and(O) atrocities(O) committed(O) by(O) the(O) Imperial(O) Japanese(O) Army(O) after(O) it(O) captured(O) Nanjing(B-location) ,(O) then(O) capital(O) of(O) China(B-country) ,(O) during(O) the(O) Second(B-event) Sino-Japanese(I-event) War(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, literary genre, writer, magazine, person, location, country, organization, book, award, event and O.\nSentence: The Rape of Nanking : The Forgotten Holocaust of World War II is a bestselling 1997 non-fiction book written by Iris Chang about the 1937-1938 Nanking Massacre , the massacre and atrocities committed by the Imperial Japanese Army after it captured Nanjing , then capital of China , during the Second Sino-Japanese War .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Rape","of","Nanking",":","The","Forgotten","Holocaust","of","World","War","II","is","a","bestselling","1997","non-fiction","book","written","by","Iris","Chang","about","the","1937-1938","Nanking","Massacre",",","the","massacre","and","atrocities","committed","by","the","Imperial","Japanese","Army","after","it","captured","Nanjing",",","then","capital","of","China",",","during","the","Second","Sino-Japanese","War","."],"labels":["B-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","O","B-country","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["poem","literary_genre","writer","magazine","person","location","country","organization","book","award","event"]}
{"id":"172","dataset":"crossner_literature","split":"test","instance":{"id":"172","prompt_labels":"However(O) ,(O) in(O) the(O) parallel(B-literary genre) poem(I-literary genre) The(B-poem) Greene(I-poem) Knight(I-poem) ,(O) the(O) lace(O) is(O) white(O) ,(O) not(O) green(O) ,(O) and(O) is(O) considered(O) the(O) origin(O) of(O) the(O) collar(O) worn(O) by(O) the(O) knights(O) of(O) the(O) Bath(O) ,(O) not(O) the(O) Order(O) of(O) the(O) Garter(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, poem, person, literary genre, organization, magazine, book, writer, country, event and O.\nSentence: However , in the parallel poem The Greene Knight , the lace is white , not green , and is considered the origin of the collar worn by the knights of the Bath , not the Order of the Garter .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["However",",","in","the","parallel","poem","The","Greene","Knight",",","the","lace","is","white",",","not","green",",","and","is","considered","the","origin","of","the","collar","worn","by","the","knights","of","the","Bath",",","not","the","Order","of","the","Garter","."],"labels":["O","O","O","O","B-literary genre","I-literary genre","B-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","award","poem","person","literary_genre","organization","magazine","book","writer","country","event"]}
{"id":"176","dataset":"crossner_literature","split":"test","instance":{"id":"176","prompt_labels":"In(O) 1967(O) Algis(B-writer) Budrys(I-writer) listed(O) Aldiss(O) ,(O) J.(B-writer) G.(I-writer) Ballard(I-writer) ,(O) Roger(B-writer) Zelazny(I-writer) ,(O) and(O) Samuel(B-writer) R.(I-writer) Delany(I-writer) as(O) an(O) earthshaking(O) new(O) kind(O) of(O) writers(O) ,(O) and(O) leaders(O) of(O) the(O) New(O) Wave(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, person, location, award, poem, event, writer, magazine, organization, book, country and O.\nSentence: In 1967 Algis Budrys listed Aldiss , J. G. Ballard , Roger Zelazny , and Samuel R. Delany as an earthshaking new kind of writers , and leaders of the New Wave .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1967","Algis","Budrys","listed","Aldiss",",","J.","G.","Ballard",",","Roger","Zelazny",",","and","Samuel","R.","Delany","as","an","earthshaking","new","kind","of","writers",",","and","leaders","of","the","New","Wave","."],"labels":["O","O","B-writer","I-writer","O","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","person","location","award","poem","event","writer","magazine","organization","book","country"]}
{"id":"182","dataset":"crossner_literature","split":"test","instance":{"id":"182","prompt_labels":"In(O) Patricia(B-writer) Wrede(I-writer) '(O) s(O) Regency(B-book) fantasies(I-book) ,(O) Great(O) Britain(O) has(O) a(O) Royal(B-organization) Society(I-organization) of(I-organization) Wizards(I-organization) ,(O) and(O) in(O) Poul(B-writer) Anderson(I-writer) 's(O) A(B-book) Midsummer(I-book) Tempest(I-book) William(B-writer) Shakespeare(I-writer) is(O) remembered(O) as(O) the(O) Great(O) Historian(O) ,(O) with(O) the(O) novel(B-literary genre) itself(O) taking(O) place(O) in(O) the(O) era(O) of(O) Oliver(B-person) Cromwell(I-person) and(O) Charles(B-person) I(I-person) ,(O) with(O) an(O) alternate(O) outcome(O) for(O) the(O) English(B-event) Civil(I-event) War(I-event) and(O) an(O) earlier(O) Industrial(B-event) Revolution(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, poem, person, writer, organization, country, location, event, award, book, magazine and O.\nSentence: In Patricia Wrede ' s Regency fantasies , Great Britain has a Royal Society of Wizards , and in Poul Anderson 's A Midsummer Tempest William Shakespeare is remembered as the Great Historian , with the novel itself taking place in the era of Oliver Cromwell and Charles I , with an alternate outcome for the English Civil War and an earlier Industrial Revolution .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","Patricia","Wrede","'","s","Regency","fantasies",",","Great","Britain","has","a","Royal","Society","of","Wizards",",","and","in","Poul","Anderson","'s","A","Midsummer","Tempest","William","Shakespeare","is","remembered","as","the","Great","Historian",",","with","the","novel","itself","taking","place","in","the","era","of","Oliver","Cromwell","and","Charles","I",",","with","an","alternate","outcome","for","the","English","Civil","War","and","an","earlier","Industrial","Revolution","."],"labels":["O","B-writer","I-writer","O","O","B-book","I-book","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","B-writer","I-writer","O","B-book","I-book","I-book","B-writer","I-writer","O","O","O","O","O","O","O","O","O","B-literary genre","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","B-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","poem","person","writer","organization","country","location","event","award","book","magazine"]}
{"id":"188","dataset":"crossner_literature","split":"test","instance":{"id":"188","prompt_labels":"In(O) 1962(O) ,(O) two(O) major(O) anthologies(O) of(O) Borges(B-writer) 's(O) writings(O) were(O) published(O) in(O) English(O) by(O) New(B-organization) York(I-organization) presses(I-organization) :(O) Ficciones(B-book) and(O) Labyrinths(B-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, organization, person, poem, event, award, literary genre, country, magazine, location and O.\nSentence: In 1962 , two major anthologies of Borges 's writings were published in English by New York presses : Ficciones and Labyrinths .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1962",",","two","major","anthologies","of","Borges","'s","writings","were","published","in","English","by","New","York","presses",":","Ficciones","and","Labyrinths","."],"labels":["O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-book","O","B-book","O"],"target_index":null,"target_label":null},"label_list":["writer","book","organization","person","poem","event","award","literary_genre","country","magazine","location"]}
{"id":"192","dataset":"crossner_literature","split":"test","instance":{"id":"192","prompt_labels":"Ti-Grace(B-writer) Atkinson(I-writer) ,(O) the(O) New(B-location) York(I-location) chapter(O) president(O) of(O) the(O) National(B-organization) Organization(I-organization) for(I-organization) Women(I-organization) ((O) NOW(B-organization) )(O) ,(O) described(O) Solanas(B-writer) as(O) the(O) first(O) outstanding(O) champion(O) of(O) women(O) 's(O) rights(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, magazine, person, literary genre, country, location, event, poem, award, book, writer and O.\nSentence: Ti-Grace Atkinson , the New York chapter president of the National Organization for Women ( NOW ) , described Solanas as the first outstanding champion of women 's rights","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ti-Grace","Atkinson",",","the","New","York","chapter","president","of","the","National","Organization","for","Women","(","NOW",")",",","described","Solanas","as","the","first","outstanding","champion","of","women","'s","rights"],"labels":["B-writer","I-writer","O","O","B-location","I-location","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-writer","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","magazine","person","literary_genre","country","location","event","poem","award","book","writer"]}
{"id":"195","dataset":"crossner_literature","split":"test","instance":{"id":"195","prompt_labels":"In(O) the(O) 1970s(O) ,(O) Pohl(B-writer) re-emerged(O) as(O) a(O) novel(B-literary genre) writer(O) in(O) his(O) own(O) right(O) ,(O) with(O) books(O) such(O) as(O) Man(B-book) Plus(I-book) and(O) the(O) Heechee(B-book) series(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, book, poem, literary genre, award, location, organization, magazine, writer, person and O.\nSentence: In the 1970s , Pohl re-emerged as a novel writer in his own right , with books such as Man Plus and the Heechee series .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","1970s",",","Pohl","re-emerged","as","a","novel","writer","in","his","own","right",",","with","books","such","as","Man","Plus","and","the","Heechee","series","."],"labels":["O","O","O","O","B-writer","O","O","O","B-literary genre","O","O","O","O","O","O","O","O","O","O","B-book","I-book","O","O","B-book","O","O"],"target_index":null,"target_label":null},"label_list":["event","country","book","poem","literary_genre","award","location","organization","magazine","writer","person"]}
{"id":"196","dataset":"crossner_literature","split":"test","instance":{"id":"196","prompt_labels":"After(O) the(O) war(O) ,(O) Geisel(B-writer) returned(O) to(O) writing(O) children(O) 's(O) books(O) ,(O) writing(O) classics(B-literary genre) like(O) If(B-book) I(I-book) Ran(I-book) the(I-book) Zoo(I-book) ((O) 1950(O) )(O) ,(O) Horton(B-book) Hears(I-book) a(I-book) Who(I-book) !(I-book) ((O) 1955(O) )(O) ,(O) If(B-book) I(I-book) Ran(I-book) the(I-book) Circus(I-book) ((O) 1956(O) )(O) ,(O) The(B-book) Cat(I-book) in(I-book) the(I-book) Hat(I-book) ((O) 1957(O) )(O) ,(O) How(B-book) the(I-book) Grinch(I-book) Stole(I-book) Christmas(I-book) !(I-book) ((O) 1957(O) )(O) ,(O) and(O) Green(B-book) Eggs(I-book) and(I-book) Ham(I-book) ((O) 1960(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, book, organization, writer, event, literary genre, country, poem, magazine, location and O.\nSentence: After the war , Geisel returned to writing children 's books , writing classics like If I Ran the Zoo ( 1950 ) , Horton Hears a Who ! ( 1955 ) , If I Ran the Circus ( 1956 ) , The Cat in the Hat ( 1957 ) , How the Grinch Stole Christmas ! ( 1957 ) , and Green Eggs and Ham ( 1960 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","the","war",",","Geisel","returned","to","writing","children","'s","books",",","writing","classics","like","If","I","Ran","the","Zoo","(","1950",")",",","Horton","Hears","a","Who","!","(","1955",")",",","If","I","Ran","the","Circus","(","1956",")",",","The","Cat","in","the","Hat","(","1957",")",",","How","the","Grinch","Stole","Christmas","!","(","1957",")",",","and","Green","Eggs","and","Ham","(","1960",")","."],"labels":["O","O","O","O","B-writer","O","O","O","O","O","O","O","O","B-literary genre","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","award","book","organization","writer","event","literary_genre","country","poem","magazine","location"]}
{"id":"200","dataset":"crossner_literature","split":"test","instance":{"id":"200","prompt_labels":"In(O) 1930(O) ,(O) an(O) American(O) film(O) of(O) the(O) novel(B-literary genre) was(O) made(O) ,(O) directed(O) by(O) Lewis(B-person) Milestone(I-person) ;(O) with(O) a(O) screenplay(O) by(O) Maxwell(B-writer) Anderson(I-writer) ,(O) George(B-writer) Abbott(I-writer) ,(O) Del(B-writer) Andrews(I-writer) ,(O) C.(B-writer) Gardner(I-writer) Sullivan(I-writer) ;(O) and(O) with(O) uncredited(O) work(O) by(O) Walter(B-writer) Anthony(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, person, book, writer, poem, award, magazine, organization, literary genre, country and O.\nSentence: In 1930 , an American film of the novel was made , directed by Lewis Milestone ; with a screenplay by Maxwell Anderson , George Abbott , Del Andrews , C. Gardner Sullivan ; and with uncredited work by Walter Anthony .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1930",",","an","American","film","of","the","novel","was","made",",","directed","by","Lewis","Milestone",";","with","a","screenplay","by","Maxwell","Anderson",",","George","Abbott",",","Del","Andrews",",","C.","Gardner","Sullivan",";","and","with","uncredited","work","by","Walter","Anthony","."],"labels":["O","O","O","O","O","O","O","O","B-literary genre","O","O","O","O","O","B-person","I-person","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["location","event","person","book","writer","poem","award","magazine","organization","literary_genre","country"]}
{"id":"205","dataset":"crossner_literature","split":"test","instance":{"id":"205","prompt_labels":"Owen(B-writer) 's(O) experiences(O) with(O) religion(O) also(O) heavily(O) influenced(O) his(O) poetry(B-literary genre) ,(O) notably(O) in(O) poems(B-literary genre) such(O) as(O) Anthem(B-poem) for(I-poem) Doomed(I-poem) Youth(I-poem) ,(O) in(O) which(O) the(O) ceremony(O) of(O) a(O) funeral(O) is(O) re-enacted(O) not(O) in(O) a(O) church(O) ,(O) but(O) on(O) the(O) battlefield(O) itself(O) ,(O) and(O) At(B-poem) a(I-poem) Calvary(I-poem) near(I-poem) the(I-poem) Ancre(I-poem) ,(O) which(O) comments(O) on(O) the(O) Crucifixion(O) of(O) Christ(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, magazine, writer, literary genre, poem, person, organization, award, book, event, country and O.\nSentence: Owen 's experiences with religion also heavily influenced his poetry , notably in poems such as Anthem for Doomed Youth , in which the ceremony of a funeral is re-enacted not in a church , but on the battlefield itself , and At a Calvary near the Ancre , which comments on the Crucifixion of Christ .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Owen","'s","experiences","with","religion","also","heavily","influenced","his","poetry",",","notably","in","poems","such","as","Anthem","for","Doomed","Youth",",","in","which","the","ceremony","of","a","funeral","is","re-enacted","not","in","a","church",",","but","on","the","battlefield","itself",",","and","At","a","Calvary","near","the","Ancre",",","which","comments","on","the","Crucifixion","of","Christ","."],"labels":["B-writer","O","O","O","O","O","O","O","O","B-literary genre","O","O","O","B-literary genre","O","O","B-poem","I-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-poem","I-poem","I-poem","I-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","magazine","writer","literary_genre","poem","person","organization","award","book","event","country"]}
{"id":"210","dataset":"crossner_literature","split":"test","instance":{"id":"210","prompt_labels":"He(O) was(O) also(O) a(O) prolific(O) author(O) who(O) wrote(O) on(O) both(O) scientific(B-literary genre) and(I-literary genre) social(I-literary genre) issues(I-literary genre) ;(O) his(O) account(O) of(O) his(O) adventures(O) and(O) observations(O) during(O) his(O) explorations(O) in(O) Singapore(B-country) ,(O) Indonesia(B-country) and(O) Malaysia(B-country) ,(O) The(B-book) Malay(I-book) Archipelago(I-book) ,(O) was(O) both(O) popular(O) and(O) highly(O) regarded(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, writer, country, person, poem, literary genre, award, book, location, organization, magazine and O.\nSentence: He was also a prolific author who wrote on both scientific and social issues ; his account of his adventures and observations during his explorations in Singapore , Indonesia and Malaysia , The Malay Archipelago , was both popular and highly regarded .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","also","a","prolific","author","who","wrote","on","both","scientific","and","social","issues",";","his","account","of","his","adventures","and","observations","during","his","explorations","in","Singapore",",","Indonesia","and","Malaysia",",","The","Malay","Archipelago",",","was","both","popular","and","highly","regarded","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","I-literary genre","I-literary genre","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","B-country","O","B-country","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","writer","country","person","poem","literary_genre","award","book","location","organization","magazine"]}
{"id":"212","dataset":"crossner_literature","split":"test","instance":{"id":"212","prompt_labels":"Live-action(O) film(O) adaptations(O) have(O) been(O) made(O) of(O) three(O) of(O) The(B-book) Chronicles(I-book) of(I-book) Narnia(I-book) :(O) The(B-book) Lion(I-book) ,(O) the(B-book) Witch(I-book) ,(O) and(O) the(B-book) Wardrobe(I-book) ((O) 2005(O) )(O) ,(O) Prince(B-book) Caspian(I-book) ((O) 2008(O) )(O) and(O) The(B-book) Voyage(I-book) of(I-book) the(I-book) Dawn(I-book) Treader(I-book) ((O) 2010(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, literary genre, person, writer, organization, country, location, event, book, award, magazine and O.\nSentence: Live-action film adaptations have been made of three of The Chronicles of Narnia : The Lion , the Witch , and the Wardrobe ( 2005 ) , Prince Caspian ( 2008 ) and The Voyage of the Dawn Treader ( 2010 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Live-action","film","adaptations","have","been","made","of","three","of","The","Chronicles","of","Narnia",":","The","Lion",",","the","Witch",",","and","the","Wardrobe","(","2005",")",",","Prince","Caspian","(","2008",")","and","The","Voyage","of","the","Dawn","Treader","(","2010",")","."],"labels":["O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","I-book","O","B-book","I-book","O","B-book","I-book","O","O","B-book","I-book","O","O","O","O","B-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["poem","literary_genre","person","writer","organization","country","location","event","book","award","magazine"]}
{"id":"216","dataset":"crossner_literature","split":"test","instance":{"id":"216","prompt_labels":"The(O) first(O) professional(O) critic(O) to(O) comment(O) on(O) Howard(B-writer) 's(O) work(O) was(O) Hoffman(B-person) Reynolds(I-person) Hays(I-person) ,(O) reviewing(O) the(O) Arkham(B-organization) House(I-organization) collection(O) Skull-Face(B-book) and(I-book) Others(I-book) in(O) The(B-magazine) New(I-magazine) York(I-magazine) Times(I-magazine) Book(I-magazine) Review(I-magazine) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, award, literary genre, poem, person, book, event, organization, magazine, location, writer and O.\nSentence: The first professional critic to comment on Howard 's work was Hoffman Reynolds Hays , reviewing the Arkham House collection Skull-Face and Others in The New York Times Book Review .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","first","professional","critic","to","comment","on","Howard","'s","work","was","Hoffman","Reynolds","Hays",",","reviewing","the","Arkham","House","collection","Skull-Face","and","Others","in","The","New","York","Times","Book","Review","."],"labels":["O","O","O","O","O","O","O","B-writer","O","O","O","B-person","I-person","I-person","O","O","O","B-organization","I-organization","O","B-book","I-book","I-book","O","B-magazine","I-magazine","I-magazine","I-magazine","I-magazine","I-magazine","O"],"target_index":null,"target_label":null},"label_list":["country","award","literary_genre","poem","person","book","event","organization","magazine","location","writer"]}
{"id":"218","dataset":"crossner_literature","split":"test","instance":{"id":"218","prompt_labels":"Slaughterhouse-Five(B-book) received(O) generally(O) positive(O) reviews(O) ,(O) with(O) Michael(B-writer) Crichton(I-writer) writing(O) in(O) The(B-magazine) New(I-magazine) Republic(I-magazine) ,(O) he(O) writes(O) about(O) the(O) most(O) excruciatingly(O) painful(O) things(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, book, magazine, award, person, country, writer, literary genre, poem, organization, event and O.\nSentence: Slaughterhouse-Five received generally positive reviews , with Michael Crichton writing in The New Republic , he writes about the most excruciatingly painful things .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Slaughterhouse-Five","received","generally","positive","reviews",",","with","Michael","Crichton","writing","in","The","New","Republic",",","he","writes","about","the","most","excruciatingly","painful","things","."],"labels":["B-book","O","O","O","O","O","O","B-writer","I-writer","O","O","B-magazine","I-magazine","I-magazine","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","book","magazine","award","person","country","writer","literary_genre","poem","organization","event"]}
{"id":"219","dataset":"crossner_literature","split":"test","instance":{"id":"219","prompt_labels":"He(O) continued(O) his(O) Dune(B-book) saga(I-book) ,(O) following(O) it(O) with(O) Dune(B-book) Messiah(I-book) ,(O) Children(B-book) of(I-book) Dune(I-book) ,(O) and(O) God(B-book) Emperor(I-book) of(I-book) Dune(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, organization, writer, magazine, person, country, location, event, literary genre, award, book and O.\nSentence: He continued his Dune saga , following it with Dune Messiah , Children of Dune , and God Emperor of Dune .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","continued","his","Dune","saga",",","following","it","with","Dune","Messiah",",","Children","of","Dune",",","and","God","Emperor","of","Dune","."],"labels":["O","O","O","B-book","I-book","O","O","O","O","B-book","I-book","O","B-book","I-book","I-book","O","O","B-book","I-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["poem","organization","writer","magazine","person","country","location","event","literary_genre","award","book"]}
{"id":"222","dataset":"crossner_literature","split":"test","instance":{"id":"222","prompt_labels":"Barry(O) Lyndon(O) is(O) a(O) 1975(O) period(O) drama(O) film(O) written(O) ,(O) directed(O) and(O) produced(O) by(O) Stanley(B-writer) Kubrick(I-writer) ,(O) based(O) on(O) the(O) 1844(O) novel(B-literary genre) The(B-book) Luck(I-book) of(I-book) Barry(I-book) Lyndon(I-book) by(O) William(B-writer) Makepeace(I-writer) Thackeray(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, writer, literary genre, event, magazine, organization, person, country, book, location, award and O.\nSentence: Barry Lyndon is a 1975 period drama film written , directed and produced by Stanley Kubrick , based on the 1844 novel The Luck of Barry Lyndon by William Makepeace Thackeray .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Barry","Lyndon","is","a","1975","period","drama","film","written",",","directed","and","produced","by","Stanley","Kubrick",",","based","on","the","1844","novel","The","Luck","of","Barry","Lyndon","by","William","Makepeace","Thackeray","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","B-literary genre","B-book","I-book","I-book","I-book","I-book","O","B-writer","I-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["poem","writer","literary_genre","event","magazine","organization","person","country","book","location","award"]}
{"id":"224","dataset":"crossner_literature","split":"test","instance":{"id":"224","prompt_labels":"In(O) 1998(O) ,(O) he(O) was(O) a(O) member(O) of(O) the(O) jury(O) at(O) the(O) 48th(B-event) Berlin(I-event) International(I-event) Film(I-event) Festival(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, award, country, magazine, event, organization, book, literary genre, writer, location, person and O.\nSentence: In 1998 , he was a member of the jury at the 48th Berlin International Film Festival .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1998",",","he","was","a","member","of","the","jury","at","the","48th","Berlin","International","Film","Festival","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["poem","award","country","magazine","event","organization","book","literary_genre","writer","location","person"]}
{"id":"230","dataset":"crossner_literature","split":"test","instance":{"id":"230","prompt_labels":"He(O) was(O) a(O) high-ranking(O) Song(B-country) dynasty(I-country) scholar-official(O) and(O) historian(O) who(O) authored(O) the(O) monumental(B-literary genre) history(I-literary genre) book(O) Zizhi(B-book) Tongjian(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, location, award, book, organization, person, event, literary genre, magazine, country, poem and O.\nSentence: He was a high-ranking Song dynasty scholar-official and historian who authored the monumental history book Zizhi Tongjian .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","a","high-ranking","Song","dynasty","scholar-official","and","historian","who","authored","the","monumental","history","book","Zizhi","Tongjian","."],"labels":["O","O","O","O","B-country","I-country","O","O","O","O","O","O","B-literary genre","I-literary genre","O","B-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["writer","location","award","book","organization","person","event","literary_genre","magazine","country","poem"]}
{"id":"247","dataset":"crossner_literature","split":"test","instance":{"id":"247","prompt_labels":"Another(O) well-known(O) version(O) of(O) the(O) play(O) is(O) Jedermann(B-book) by(O) the(O) Austrian(O) playwright(O) Hugo(B-writer) von(I-writer) Hofmannsthal(I-writer) ,(O) which(O) has(O) been(O) performed(O) annually(O) at(O) the(O) Salzburg(B-event) Festival(I-event) since(O) 1920(O) ,(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, poem, literary genre, country, writer, location, person, book, magazine, organization and O.\nSentence: Another well-known version of the play is Jedermann by the Austrian playwright Hugo von Hofmannsthal , which has been performed annually at the Salzburg Festival since 1920 , .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Another","well-known","version","of","the","play","is","Jedermann","by","the","Austrian","playwright","Hugo","von","Hofmannsthal",",","which","has","been","performed","annually","at","the","Salzburg","Festival","since","1920",",","."],"labels":["O","O","O","O","O","O","O","B-book","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","B-event","I-event","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","award","poem","literary_genre","country","writer","location","person","book","magazine","organization"]}
{"id":"253","dataset":"crossner_literature","split":"test","instance":{"id":"253","prompt_labels":"Nurturing(O) an(O) image(O) of(O) her(O) own(O) country(O) as(O) a(O) haven(O) of(O) social(O) peace(O) and(O) prosperity(O) ,(O) she(O) threatened(O) to(O) boycott(O) the(O) 1955(O) Venice(B-event) Film(I-event) Festival(I-event) if(O) the(O) American(O) juvenile(O) delinquent(O) film(O) Blackboard(O) Jungle(O) was(O) shown(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, magazine, location, poem, organization, country, person, literary genre, event, award, writer and O.\nSentence: Nurturing an image of her own country as a haven of social peace and prosperity , she threatened to boycott the 1955 Venice Film Festival if the American juvenile delinquent film Blackboard Jungle was shown .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Nurturing","an","image","of","her","own","country","as","a","haven","of","social","peace","and","prosperity",",","she","threatened","to","boycott","the","1955","Venice","Film","Festival","if","the","American","juvenile","delinquent","film","Blackboard","Jungle","was","shown","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","magazine","location","poem","organization","country","person","literary_genre","event","award","writer"]}
{"id":"254","dataset":"crossner_literature","split":"test","instance":{"id":"254","prompt_labels":"Le(B-writer) Guin(I-writer) refused(O) a(O) Nebula(B-award) Award(I-award) for(I-award) Best(I-award) Novelette(I-award) for(O) her(O) story(O) The(B-book) Diary(I-book) of(I-book) the(I-book) Rose(I-book) in(O) 1977(O) ,(O) in(O) protest(O) at(O) the(O) Science(B-organization) Fiction(I-organization) Writers(I-organization) of(I-organization) America(I-organization) '(O) s(O) revocation(O) of(O) Stanisław(B-writer) Lem(I-writer) '(O) s(O) membership(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, literary genre, person, writer, award, poem, magazine, location, book, country, event and O.\nSentence: Le Guin refused a Nebula Award for Best Novelette for her story The Diary of the Rose in 1977 , in protest at the Science Fiction Writers of America ' s revocation of Stanisław Lem ' s membership .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Le","Guin","refused","a","Nebula","Award","for","Best","Novelette","for","her","story","The","Diary","of","the","Rose","in","1977",",","in","protest","at","the","Science","Fiction","Writers","of","America","'","s","revocation","of","Stanisław","Lem","'","s","membership","."],"labels":["B-writer","I-writer","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","O","B-writer","I-writer","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","literary_genre","person","writer","award","poem","magazine","location","book","country","event"]}
{"id":"260","dataset":"crossner_literature","split":"test","instance":{"id":"260","prompt_labels":"Writing(O) for(O) The(B-magazine) Spectator(I-magazine) in(O) 1936(O) ,(O) Graham(B-writer) Greene(I-writer) gave(O) the(O) film(O) a(O) tepid(O) review(O) ,(O) describing(O) it(O) on(O) the(O) one(O) hand(O) as(O) his(O) favorite(O) of(O) the(O) films(O) he(O) reviewed(O) that(O) week(O) ,(O) but(O) describing(O) it(O) as(O) a(O) fine(O) spirited(O) mix-up(O) and(O) making(O) pointed(O) note(O) of(O) the(O) magnificently(O) wrong(O) characterization(O) of(O) bad(O) King(B-person) James(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, writer, location, poem, event, book, magazine, country, literary genre, organization, person and O.\nSentence: Writing for The Spectator in 1936 , Graham Greene gave the film a tepid review , describing it on the one hand as his favorite of the films he reviewed that week , but describing it as a fine spirited mix-up and making pointed note of the magnificently wrong characterization of bad King James .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Writing","for","The","Spectator","in","1936",",","Graham","Greene","gave","the","film","a","tepid","review",",","describing","it","on","the","one","hand","as","his","favorite","of","the","films","he","reviewed","that","week",",","but","describing","it","as","a","fine","spirited","mix-up","and","making","pointed","note","of","the","magnificently","wrong","characterization","of","bad","King","James","."],"labels":["O","O","B-magazine","I-magazine","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["award","writer","location","poem","event","book","magazine","country","literary_genre","organization","person"]}
{"id":"277","dataset":"crossner_literature","split":"test","instance":{"id":"277","prompt_labels":"Pauline(B-writer) Kael(I-writer) of(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) wrote(O) that(O) Kubrick(B-writer) has(O) taken(O) a(O) quick-witted(O) story(O) and(O) controlled(O) it(O) so(O) meticulously(O) that(O) he(O) 's(O) drained(O) the(O) blood(O) out(O) of(O) it(O) ,(O) adding(O) ,(O) It(O) 's(O) a(O) coffee-table(O) movie(O) ;(O) we(O) might(O) as(O) well(O) be(O) at(O) a(O) three-hour(O) slide(O) show(O) for(O) art-history(O) majors(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, poem, event, book, country, person, magazine, writer, location, literary genre and O.\nSentence: Pauline Kael of The New Yorker wrote that Kubrick has taken a quick-witted story and controlled it so meticulously that he 's drained the blood out of it , adding , It 's a coffee-table movie ; we might as well be at a three-hour slide show for art-history majors .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Pauline","Kael","of","The","New","Yorker","wrote","that","Kubrick","has","taken","a","quick-witted","story","and","controlled","it","so","meticulously","that","he","'s","drained","the","blood","out","of","it",",","adding",",","It","'s","a","coffee-table","movie",";","we","might","as","well","be","at","a","three-hour","slide","show","for","art-history","majors","."],"labels":["B-writer","I-writer","O","B-magazine","I-magazine","I-magazine","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","award","poem","event","book","country","person","magazine","writer","location","literary_genre"]}
{"id":"281","dataset":"crossner_literature","split":"test","instance":{"id":"281","prompt_labels":"In(O) 1988(O) ,(O) Berkoff(B-person) directed(O) an(O) interpretation(O) of(O) Salome(O) by(O) Oscar(B-writer) Wilde(I-writer) ,(O) performed(O) in(O) slow(O) motion(O) ,(O) at(O) the(O) Gate(B-location) Theatre(I-location) ,(O) Dublin(B-location) ..(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, poem, location, writer, literary genre, organization, person, award, magazine, book, event and O.\nSentence: In 1988 , Berkoff directed an interpretation of Salome by Oscar Wilde , performed in slow motion , at the Gate Theatre , Dublin ..","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1988",",","Berkoff","directed","an","interpretation","of","Salome","by","Oscar","Wilde",",","performed","in","slow","motion",",","at","the","Gate","Theatre",",","Dublin",".."],"labels":["O","O","O","B-person","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["country","poem","location","writer","literary_genre","organization","person","award","magazine","book","event"]}
{"id":"291","dataset":"crossner_literature","split":"test","instance":{"id":"291","prompt_labels":"St.(B-organization) Columba(I-organization) 's(I-organization) School(I-organization) one(O) of(O) the(O) most(O) prominent(O) English-Medium(O) schools(O) in(O) India(B-country) run(O) by(O) the(O) Congregation(B-organization) of(I-organization) Christian(I-organization) Brothers(I-organization) is(O) also(O) named(O) after(O) the(O) saint(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, poem, book, literary genre, location, magazine, writer, organization, country, person, award and O.\nSentence: St. Columba 's School one of the most prominent English-Medium schools in India run by the Congregation of Christian Brothers is also named after the saint .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["St.","Columba","'s","School","one","of","the","most","prominent","English-Medium","schools","in","India","run","by","the","Congregation","of","Christian","Brothers","is","also","named","after","the","saint","."],"labels":["B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","B-country","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","poem","book","literary_genre","location","magazine","writer","organization","country","person","award"]}
{"id":"295","dataset":"crossner_literature","split":"test","instance":{"id":"295","prompt_labels":"During(O) China(B-country) '(O) s(O) Warring(B-event) States(I-event) period(I-event) ,(O) the(O) Songs(B-book) of(I-book) Chu(I-book) collected(O) by(O) Qu(B-writer) Yuan(I-writer) and(O) Song(B-writer) Yu(I-writer) defined(O) a(O) new(O) form(O) of(O) poetry(B-literary genre) that(O) came(O) from(O) the(O) exotic(O) Yangtze(B-location) Valley(I-location) ,(O) far(O) from(O) the(O) Wei(B-location) and(O) Yellow(B-location) River(I-location) homeland(O) of(O) the(O) traditional(O) four-character(O) verses(B-literary genre) collected(O) in(O) the(O) Classic(B-book) of(I-book) Poetry(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, location, literary genre, person, country, organization, magazine, book, poem, award, event and O.\nSentence: During China ' s Warring States period , the Songs of Chu collected by Qu Yuan and Song Yu defined a new form of poetry that came from the exotic Yangtze Valley , far from the Wei and Yellow River homeland of the traditional four-character verses collected in the Classic of Poetry .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","China","'","s","Warring","States","period",",","the","Songs","of","Chu","collected","by","Qu","Yuan","and","Song","Yu","defined","a","new","form","of","poetry","that","came","from","the","exotic","Yangtze","Valley",",","far","from","the","Wei","and","Yellow","River","homeland","of","the","traditional","four-character","verses","collected","in","the","Classic","of","Poetry","."],"labels":["O","B-country","O","O","B-event","I-event","I-event","O","O","B-book","I-book","I-book","O","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O","O","B-literary genre","O","O","O","O","O","B-location","I-location","O","O","O","O","B-location","O","B-location","I-location","O","O","O","O","O","B-literary genre","O","O","O","B-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["writer","location","literary_genre","person","country","organization","magazine","book","poem","award","event"]}
{"id":"296","dataset":"crossner_literature","split":"test","instance":{"id":"296","prompt_labels":"Frances(B-writer) Yates(I-writer) in(O) her(O) 1966(O) study(O) The(B-book) Art(I-book) of(I-book) Memory(I-book) argues(O) that(O) a(O) brief(O) passage(O) of(O) the(O) Confessions(B-book) ,(O) 10.8.12(O) ,(O) in(O) which(O) Augustine(B-writer) writes(O) of(O) walking(O) up(O) a(O) flight(O) of(O) stairs(O) and(O) entering(O) the(O) vast(O) fields(O) of(O) memory(O) technique(O) for(O) organizing(O) large(O) amounts(O) of(O) information(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, country, poem, person, event, award, organization, magazine, writer, location, literary genre and O.\nSentence: Frances Yates in her 1966 study The Art of Memory argues that a brief passage of the Confessions , 10.8.12 , in which Augustine writes of walking up a flight of stairs and entering the vast fields of memory technique for organizing large amounts of information .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Frances","Yates","in","her","1966","study","The","Art","of","Memory","argues","that","a","brief","passage","of","the","Confessions",",","10.8.12",",","in","which","Augustine","writes","of","walking","up","a","flight","of","stairs","and","entering","the","vast","fields","of","memory","technique","for","organizing","large","amounts","of","information","."],"labels":["B-writer","I-writer","O","O","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","B-book","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["book","country","poem","person","event","award","organization","magazine","writer","location","literary_genre"]}
{"id":"299","dataset":"crossner_literature","split":"test","instance":{"id":"299","prompt_labels":"His(O) poem(B-literary genre) ,(O) Jai(B-poem) Jai(I-poem) Garavi(I-poem) Gujarat(I-poem) ((O) 1873(O) )(O) ,(O) is(O) used(O) as(O) a(O) de(O) facto(O) state(O) song(O) for(O) Gujarat(B-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, literary genre, location, magazine, person, writer, country, book, poem, event, organization and O.\nSentence: His poem , Jai Jai Garavi Gujarat ( 1873 ) , is used as a de facto state song for Gujarat .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","poem",",","Jai","Jai","Garavi","Gujarat","(","1873",")",",","is","used","as","a","de","facto","state","song","for","Gujarat","."],"labels":["O","B-literary genre","O","B-poem","I-poem","I-poem","I-poem","O","O","O","O","O","O","O","O","O","O","O","O","O","B-poem","O"],"target_index":null,"target_label":null},"label_list":["award","literary_genre","location","magazine","person","writer","country","book","poem","event","organization"]}
{"id":"303","dataset":"crossner_literature","split":"test","instance":{"id":"303","prompt_labels":"Chaucer(B-writer) 's(O) sources(O) for(O) the(O) legends(O) include(O) :(O) Virgil(B-writer) '(O) s(O) Aeneid(B-poem) ,(O) Vincent(B-writer) of(I-writer) Beauvais(I-writer) ,(O) Guido(B-writer) delle(I-writer) Colonne(I-writer) '(O) s(O) Historia(B-poem) destructionis(I-poem) Troiae(I-poem) ,(O) Gaius(B-writer) Julius(I-writer) Hyginus(I-writer) '(O) Fabulae(B-poem) and(O) Ovid(B-writer) '(O) s(O) Metamorphoses(B-poem) and(O) Heroides(B-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, organization, person, poem, event, award, country, literary genre, location, writer, book and O.\nSentence: Chaucer 's sources for the legends include : Virgil ' s Aeneid , Vincent of Beauvais , Guido delle Colonne ' s Historia destructionis Troiae , Gaius Julius Hyginus ' Fabulae and Ovid ' s Metamorphoses and Heroides .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Chaucer","'s","sources","for","the","legends","include",":","Virgil","'","s","Aeneid",",","Vincent","of","Beauvais",",","Guido","delle","Colonne","'","s","Historia","destructionis","Troiae",",","Gaius","Julius","Hyginus","'","Fabulae","and","Ovid","'","s","Metamorphoses","and","Heroides","."],"labels":["B-writer","O","O","O","O","O","O","O","B-writer","O","O","B-poem","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","B-poem","I-poem","I-poem","O","B-writer","I-writer","I-writer","O","B-poem","O","B-writer","O","O","B-poem","O","B-poem","O"],"target_index":null,"target_label":null},"label_list":["magazine","organization","person","poem","event","award","country","literary_genre","location","writer","book"]}
{"id":"305","dataset":"crossner_literature","split":"test","instance":{"id":"305","prompt_labels":"According(O) to(O) Plutarch(B-writer) '(O) s(O) Life(B-book) of(I-book) Theseus(I-book) ,(O) the(O) ship(O) Theseus(B-person) used(O) on(O) his(O) return(O) from(O) Crete(B-location) to(O) Classical(B-country) Athens(I-country) was(O) kept(O) in(O) the(O) Athenian(O) harbour(O) as(O) a(O) memorial(O) for(O) several(O) centuries(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, poem, country, location, award, magazine, book, organization, writer, literary genre and O.\nSentence: According to Plutarch ' s Life of Theseus , the ship Theseus used on his return from Crete to Classical Athens was kept in the Athenian harbour as a memorial for several centuries .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["According","to","Plutarch","'","s","Life","of","Theseus",",","the","ship","Theseus","used","on","his","return","from","Crete","to","Classical","Athens","was","kept","in","the","Athenian","harbour","as","a","memorial","for","several","centuries","."],"labels":["O","O","B-writer","O","O","B-book","I-book","I-book","O","O","O","B-person","O","O","O","O","O","B-location","O","B-country","I-country","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","person","poem","country","location","award","magazine","book","organization","writer","literary_genre"]}
{"id":"307","dataset":"crossner_literature","split":"test","instance":{"id":"307","prompt_labels":"Wilhelm(B-book) Meister(I-book) 's(I-book) Apprenticeship(I-book) provided(O) the(O) text(O) for(O) many(O) lied(O) er(O) ,(O) among(O) others(O) by(O) Beethoven(B-person) ,(O) for(O) example(O) Sehnsucht(B-poem) :(I-poem) Gedicht(I-poem) von(I-poem) Goethe(I-poem) viermal(O) in(O) Musik(O) gesetzt(O) von(O) L.(B-person) van(I-person) Beethoven(I-person) ,(O) four(O) settings(O) of(O) Nur(B-poem) wer(I-poem) die(I-poem) Sehnsucht(I-poem) kennt(I-poem) ,(O) WoO(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, magazine, organization, literary genre, event, person, book, writer, award, poem and O.\nSentence: Wilhelm Meister 's Apprenticeship provided the text for many lied er , among others by Beethoven , for example Sehnsucht : Gedicht von Goethe viermal in Musik gesetzt von L. van Beethoven , four settings of Nur wer die Sehnsucht kennt , WoO .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Wilhelm","Meister","'s","Apprenticeship","provided","the","text","for","many","lied","er",",","among","others","by","Beethoven",",","for","example","Sehnsucht",":","Gedicht","von","Goethe","viermal","in","Musik","gesetzt","von","L.","van","Beethoven",",","four","settings","of","Nur","wer","die","Sehnsucht","kennt",",","WoO","."],"labels":["B-book","I-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","B-poem","I-poem","I-poem","I-poem","I-poem","O","O","O","O","O","B-person","I-person","I-person","O","O","O","O","B-poem","I-poem","I-poem","I-poem","I-poem","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","country","magazine","organization","literary_genre","event","person","book","writer","award","poem"]}
{"id":"310","dataset":"crossner_literature","split":"test","instance":{"id":"310","prompt_labels":"Al(B-writer) Capp(I-writer) Admits(O) One(O) Morals(O) Count(O) ;(O) Pays(O) $(O) 500(O) Fine(O) ;(O) The(B-organization) Capital(I-organization) Times(I-organization) ,(O) February(O) 12(O) ,(O) 1972(O) In(O) a(O) December(O) 1992(O) article(O) for(O) The(B-magazine) New(I-magazine) Yorker(I-magazine) ,(O) Seymour(B-writer) Hersh(I-writer) reported(O) that(O) President(O) Richard(B-person) Nixon(I-person) and(O) Charles(B-person) Colson(I-person) had(O) repeatedly(O) discussed(O) the(O) Capp(B-writer) case(O) in(O) Oval(B-location) Office(I-location) recordings(O) that(O) had(O) recently(O) been(O) made(O) available(O) by(O) the(B-organization) National(I-organization) Archives(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, organization, literary genre, award, poem, book, writer, person, country, magazine and O.\nSentence: Al Capp Admits One Morals Count ; Pays $ 500 Fine ; The Capital Times , February 12 , 1972 In a December 1992 article for The New Yorker , Seymour Hersh reported that President Richard Nixon and Charles Colson had repeatedly discussed the Capp case in Oval Office recordings that had recently been made available by the National Archives .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Al","Capp","Admits","One","Morals","Count",";","Pays","$","500","Fine",";","The","Capital","Times",",","February","12",",","1972","In","a","December","1992","article","for","The","New","Yorker",",","Seymour","Hersh","reported","that","President","Richard","Nixon","and","Charles","Colson","had","repeatedly","discussed","the","Capp","case","in","Oval","Office","recordings","that","had","recently","been","made","available","by","the","National","Archives","."],"labels":["B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","B-magazine","I-magazine","I-magazine","O","B-writer","I-writer","O","O","O","B-person","I-person","O","B-person","I-person","O","O","O","O","B-writer","O","O","B-location","I-location","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["location","event","organization","literary_genre","award","poem","book","writer","person","country","magazine"]}
{"id":"312","dataset":"crossner_literature","split":"test","instance":{"id":"312","prompt_labels":"Pierre(B-person) Teilhard(I-person) de(I-person) Chardin(I-person) was(O) born(O) in(O) the(O) Château(B-location) of(I-location) Sarcenat(I-location) ,(O) Orcines(B-location) commune(I-location) ,(O) some(O) 4(O) km(O) north-west(O) of(O) Clermont-Ferrand(B-location) ,(O) Auvergne(B-location) ,(O) French(B-country) Third(I-country) Republic(I-country) ,(O) on(O) 1(O) May(O) 1881(O) ,(O) as(O) the(O) fourth(O) of(O) eleven(O) children(O) of(O) librarian(O) Emmanuel(B-person) Teilhard(I-person) de(I-person) Chardin(I-person) ((O) 1844-1932(O) )(O) and(O) of(O) Berthe-Adèle(B-person) ,(I-person) née(I-person) de(I-person) Dompierre(I-person) d(I-person) 'Hornoys(I-person) of(I-person) Picardy(I-person) ,(O) a(O) great-grandniece(O) of(O) Voltaire(B-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, writer, magazine, literary genre, poem, country, organization, book, award, person and O.\nSentence: Pierre Teilhard de Chardin was born in the Château of Sarcenat , Orcines commune , some 4 km north-west of Clermont-Ferrand , Auvergne , French Third Republic , on 1 May 1881 , as the fourth of eleven children of librarian Emmanuel Teilhard de Chardin ( 1844-1932 ) and of Berthe-Adèle , née de Dompierre d 'Hornoys of Picardy , a great-grandniece of Voltaire .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Pierre","Teilhard","de","Chardin","was","born","in","the","Château","of","Sarcenat",",","Orcines","commune",",","some","4","km","north-west","of","Clermont-Ferrand",",","Auvergne",",","French","Third","Republic",",","on","1","May","1881",",","as","the","fourth","of","eleven","children","of","librarian","Emmanuel","Teilhard","de","Chardin","(","1844-1932",")","and","of","Berthe-Adèle",",","née","de","Dompierre","d","'Hornoys","of","Picardy",",","a","great-grandniece","of","Voltaire","."],"labels":["B-person","I-person","I-person","I-person","O","O","O","O","B-location","I-location","I-location","O","B-location","I-location","O","O","O","O","O","O","B-location","O","B-location","O","B-country","I-country","I-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","I-person","O","O","O","O","O","B-person","I-person","I-person","I-person","I-person","I-person","I-person","I-person","I-person","O","O","O","O","B-writer","O"],"target_index":null,"target_label":null},"label_list":["location","event","writer","magazine","literary_genre","poem","country","organization","book","award","person"]}
{"id":"318","dataset":"crossner_literature","split":"test","instance":{"id":"318","prompt_labels":"He(O) began(O) acting(O) on-screen(O) in(O) the(O) early(O) 1980s(O) ,(O) with(O) his(O) mainstream(O) breakthrough(O) coming(O) with(O) Lee(B-person) 's(O) Do(O) the(O) Right(O) Thing(O) ((O) 1989(O) )(O) and(O) the(O) Coens(B-person) '(O) Miller(O) 's(O) Crossing(O) ((O) 1990(O) )(O) and(O) Barton(O) Fink(O) ((O) 1991(O) )(O) ,(O) for(O) which(O) he(O) won(O) the(O) Cannes(B-award) Film(I-award) Festival(I-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) Award(O) at(O) the(O) Cannes(B-event) Film(I-event) Festival(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, organization, book, event, country, magazine, person, location, literary genre, writer, award and O.\nSentence: He began acting on-screen in the early 1980s , with his mainstream breakthrough coming with Lee 's Do the Right Thing ( 1989 ) and the Coens ' Miller 's Crossing ( 1990 ) and Barton Fink ( 1991 ) , for which he won the Cannes Film Festival Award for Best Actor Award at the Cannes Film Festival .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","began","acting","on-screen","in","the","early","1980s",",","with","his","mainstream","breakthrough","coming","with","Lee","'s","Do","the","Right","Thing","(","1989",")","and","the","Coens","'","Miller","'s","Crossing","(","1990",")","and","Barton","Fink","(","1991",")",",","for","which","he","won","the","Cannes","Film","Festival","Award","for","Best","Actor","Award","at","the","Cannes","Film","Festival","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["poem","organization","book","event","country","magazine","person","location","literary_genre","writer","award"]}
{"id":"320","dataset":"crossner_literature","split":"test","instance":{"id":"320","prompt_labels":"Too(O) old(O) to(O) enlist(O) when(O) the(O) First(B-event) World(I-event) War(I-event) broke(O) out(O) ,(O) he(O) served(O) in(O) France(B-country) as(O) a(O) member(O) of(O) the(O) British(B-organization) Red(I-organization) Cross(I-organization) '(O) s(O) so-called(O) Literary(B-organization) Ambulance(I-organization) Drivers(I-organization) ,(O) a(O) group(O) of(O) some(O) 24(O) well-known(O) writers(O) ,(O) including(O) the(O) Americans(O) John(B-writer) Dos(I-writer) Passos(I-writer) ,(O) E.(B-writer) E.(I-writer) Cummings(I-writer) ,(O) and(O) Ernest(B-writer) Hemingway(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, writer, poem, organization, event, literary genre, award, book, magazine, person, location and O.\nSentence: Too old to enlist when the First World War broke out , he served in France as a member of the British Red Cross ' s so-called Literary Ambulance Drivers , a group of some 24 well-known writers , including the Americans John Dos Passos , E. E. Cummings , and Ernest Hemingway .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Too","old","to","enlist","when","the","First","World","War","broke","out",",","he","served","in","France","as","a","member","of","the","British","Red","Cross","'","s","so-called","Literary","Ambulance","Drivers",",","a","group","of","some","24","well-known","writers",",","including","the","Americans","John","Dos","Passos",",","E.","E.","Cummings",",","and","Ernest","Hemingway","."],"labels":["O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","B-country","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["country","writer","poem","organization","event","literary_genre","award","book","magazine","person","location"]}
{"id":"322","dataset":"crossner_literature","split":"test","instance":{"id":"322","prompt_labels":"Sterling(B-writer) is(O) one(O) of(O) the(O) founders(O) of(O) the(O) cyberpunk(O) movement(O) in(O) science(B-literary genre) fiction(I-literary genre) ,(O) along(O) with(O) William(B-writer) Gibson(I-writer) ,(O) Rudy(B-writer) Rucker(I-writer) ,(O) John(B-writer) Shirley(I-writer) ,(O) Lewis(B-writer) Shiner(I-writer) ,(O) and(O) Pat(B-writer) Cadigan(I-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, writer, country, award, person, magazine, poem, location, book, literary genre and O.\nSentence: Sterling is one of the founders of the cyberpunk movement in science fiction , along with William Gibson , Rudy Rucker , John Shirley , Lewis Shiner , and Pat Cadigan .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Sterling","is","one","of","the","founders","of","the","cyberpunk","movement","in","science","fiction",",","along","with","William","Gibson",",","Rudy","Rucker",",","John","Shirley",",","Lewis","Shiner",",","and","Pat","Cadigan","."],"labels":["B-writer","O","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","O","B-writer","I-writer","O"],"target_index":null,"target_label":null},"label_list":["event","organization","writer","country","award","person","magazine","poem","location","book","literary_genre"]}
{"id":"327","dataset":"crossner_literature","split":"test","instance":{"id":"327","prompt_labels":"The(O) title(O) is(O) taken(O) from(O) his(O) poem(B-literary genre) The(B-poem) Negro(I-poem) Speaks(I-poem) of(I-poem) Rivers(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, poem, event, organization, award, location, person, writer, magazine, book, literary genre and O.\nSentence: The title is taken from his poem The Negro Speaks of Rivers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","title","is","taken","from","his","poem","The","Negro","Speaks","of","Rivers","."],"labels":["O","O","O","O","O","O","B-literary genre","B-poem","I-poem","I-poem","I-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["country","poem","event","organization","award","location","person","writer","magazine","book","literary_genre"]}
{"id":"334","dataset":"crossner_literature","split":"test","instance":{"id":"334","prompt_labels":"247(O) Many(O) lyricists(O) worked(O) on(O) the(O) show(O) ,(O) including(O) James(B-writer) Agee(I-writer) ,(O) Dorothy(B-writer) Parker(I-writer) ,(O) John(B-writer) Latouche(I-writer) ,(O) Richard(B-writer) Wilbur(I-writer) ,(O) Leonard(B-writer) and(O) Felicia(B-writer) Bernstein(I-writer) ,(O) and(O) Hellman(B-writer) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: poem, award, country, book, literary genre, organization, event, magazine, person, writer, location and O.\nSentence: 247 Many lyricists worked on the show , including James Agee , Dorothy Parker , John Latouche , Richard Wilbur , Leonard and Felicia Bernstein , and Hellman .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["247","Many","lyricists","worked","on","the","show",",","including","James","Agee",",","Dorothy","Parker",",","John","Latouche",",","Richard","Wilbur",",","Leonard","and","Felicia","Bernstein",",","and","Hellman","."],"labels":["O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","B-writer","O","B-writer","I-writer","O","O","B-writer","O"],"target_index":null,"target_label":null},"label_list":["poem","award","country","book","literary_genre","organization","event","magazine","person","writer","location"]}
{"id":"335","dataset":"crossner_literature","split":"test","instance":{"id":"335","prompt_labels":"In(O) 1993(O) ,(O) Thornton(B-person) married(O) Playboy(B-magazine) model(O) Pietra(B-person) Dawn(I-person) Cherniak(I-person) ,(O) with(O) whom(O) he(O) had(O) two(O) sons(O) ,(O) Harry(B-person) James(I-person) and(O) William(B-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, person, writer, organization, country, literary genre, poem, magazine, award, book and O.\nSentence: In 1993 , Thornton married Playboy model Pietra Dawn Cherniak , with whom he had two sons , Harry James and William .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1993",",","Thornton","married","Playboy","model","Pietra","Dawn","Cherniak",",","with","whom","he","had","two","sons",",","Harry","James","and","William","."],"labels":["O","O","O","B-person","O","B-magazine","O","B-person","I-person","I-person","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","O"],"target_index":null,"target_label":null},"label_list":["event","location","person","writer","organization","country","literary_genre","poem","magazine","award","book"]}
{"id":"341","dataset":"crossner_literature","split":"test","instance":{"id":"341","prompt_labels":"Much(O) of(O) her(O) work(O) ,(O) including(O) the(O) collections(O) of(O) erotica(O) Delta(B-book) of(I-book) Venus(I-book) and(O) Little(B-book) Birds(I-book) ,(O) was(O) published(O) posthumously(O) amid(O) renewed(O) critical(O) interest(O) in(O) her(O) life(O) and(O) work(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, award, event, writer, country, location, person, literary genre, organization, poem, book and O.\nSentence: Much of her work , including the collections of erotica Delta of Venus and Little Birds , was published posthumously amid renewed critical interest in her life and work .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Much","of","her","work",",","including","the","collections","of","erotica","Delta","of","Venus","and","Little","Birds",",","was","published","posthumously","amid","renewed","critical","interest","in","her","life","and","work","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-book","I-book","I-book","O","B-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","award","event","writer","country","location","person","literary_genre","organization","poem","book"]}
{"id":"345","dataset":"crossner_literature","split":"test","instance":{"id":"345","prompt_labels":"On(O) June(O) 3(O) ,(O) 1968(O) ,(O) radical(O) feminist(O) writer(O) Valerie(B-writer) Solanas(I-writer) shot(O) Warhol(B-writer) and(O) Mario(B-writer) Amaya(I-writer) ,(O) art(O) critic(O) and(O) curator(O) ,(O) at(O) Warhol(B-writer) 's(O) studio.(O) a(O) separatist(O) feminist(O) tract(O) that(O) advocated(O) the(O) elimination(O) of(O) men(O) ;(O) and(O) appeared(O) in(O) the(O) 1968(O) Warhol(B-writer) film(O) I(O) ,(O) a(O) Man(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, person, country, organization, poem, award, location, book, literary genre, writer, event and O.\nSentence: On June 3 , 1968 , radical feminist writer Valerie Solanas shot Warhol and Mario Amaya , art critic and curator , at Warhol 's studio. a separatist feminist tract that advocated the elimination of men ; and appeared in the 1968 Warhol film I , a Man .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","June","3",",","1968",",","radical","feminist","writer","Valerie","Solanas","shot","Warhol","and","Mario","Amaya",",","art","critic","and","curator",",","at","Warhol","'s","studio.","a","separatist","feminist","tract","that","advocated","the","elimination","of","men",";","and","appeared","in","the","1968","Warhol","film","I",",","a","Man","."],"labels":["O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","B-writer","O","B-writer","I-writer","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","person","country","organization","poem","award","location","book","literary_genre","writer","event"]}
{"id":"346","dataset":"crossner_literature","split":"test","instance":{"id":"346","prompt_labels":"He(O) was(O) also(O) a(O) lifetime(O) member(O) of(O) the(O) Veterans(B-organization) of(I-organization) Foreign(I-organization) Wars(I-organization) ,(O) the(O) American(B-organization) Legion(I-organization) ,(O) and(O) Sigma(B-organization) Chi(I-organization) fraternity(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, country, location, book, person, literary genre, event, writer, award, organization, poem and O.\nSentence: He was also a lifetime member of the Veterans of Foreign Wars , the American Legion , and Sigma Chi fraternity .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","also","a","lifetime","member","of","the","Veterans","of","Foreign","Wars",",","the","American","Legion",",","and","Sigma","Chi","fraternity","."],"labels":["O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["magazine","country","location","book","person","literary_genre","event","writer","award","organization","poem"]}
{"id":"350","dataset":"crossner_literature","split":"test","instance":{"id":"350","prompt_labels":"Two(O) decades(O) later(O) ,(O) Herbert(B-writer) 's(O) son(O) Brian(B-writer) Herbert(I-writer) ,(O) along(O) with(O) Kevin(B-writer) J.(I-writer) Anderson(I-writer) ,(O) published(O) two(O) sequel(O) s(O) -(O) Hunters(B-book) of(I-book) Dune(I-book) ((O) 2006(O) )(O) and(O) Sandworms(B-book) of(I-book) Dune(I-book) ((O) 2007(O) )(O) -(O) based(O) in(O) part(O) on(O) notes(O) left(O) behind(O) by(O) Frank(B-writer) Herbert(I-writer) for(O) what(O) he(O) referred(O) to(O) as(O) Dune(B-book) 7(I-book) ,(O) his(O) own(O) planned(O) seventh(O) novel(B-literary genre) in(O) the(O) Dune(O) series(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, organization, country, location, award, magazine, writer, literary genre, poem, book and O.\nSentence: Two decades later , Herbert 's son Brian Herbert , along with Kevin J. Anderson , published two sequel s - Hunters of Dune ( 2006 ) and Sandworms of Dune ( 2007 ) - based in part on notes left behind by Frank Herbert for what he referred to as Dune 7 , his own planned seventh novel in the Dune series .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Two","decades","later",",","Herbert","'s","son","Brian","Herbert",",","along","with","Kevin","J.","Anderson",",","published","two","sequel","s","-","Hunters","of","Dune","(","2006",")","and","Sandworms","of","Dune","(","2007",")","-","based","in","part","on","notes","left","behind","by","Frank","Herbert","for","what","he","referred","to","as","Dune","7",",","his","own","planned","seventh","novel","in","the","Dune","series","."],"labels":["O","O","O","O","B-writer","O","O","B-writer","I-writer","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","O","O","O","O","O","O","B-book","I-book","O","O","O","O","O","B-literary genre","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","person","organization","country","location","award","magazine","writer","literary_genre","poem","book"]}
{"id":"352","dataset":"crossner_literature","split":"test","instance":{"id":"352","prompt_labels":"She(O) memorized(O) entire(O) books(O) ,(O) including(O) The(O) Shipwreck(B-book) ,(O) The(O) Lady(B-book) of(I-book) the(I-book) Lake(I-book) ,(O) Lalla-Rookh(B-poem) ,(O) The(B-poem) Bride(I-poem) of(I-poem) Abydos(I-poem) ,(O) and(O) The(B-poem) Corsair(I-poem) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, poem, location, organization, event, book, literary genre, award, person, writer, country and O.\nSentence: She memorized entire books , including The Shipwreck , The Lady of the Lake , Lalla-Rookh , The Bride of Abydos , and The Corsair .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","memorized","entire","books",",","including","The","Shipwreck",",","The","Lady","of","the","Lake",",","Lalla-Rookh",",","The","Bride","of","Abydos",",","and","The","Corsair","."],"labels":["O","O","O","O","O","O","O","B-book","O","O","B-book","I-book","I-book","I-book","O","B-poem","O","B-poem","I-poem","I-poem","I-poem","O","O","B-poem","I-poem","O"],"target_index":null,"target_label":null},"label_list":["magazine","poem","location","organization","event","book","literary_genre","award","person","writer","country"]}
{"id":"355","dataset":"crossner_literature","split":"test","instance":{"id":"355","prompt_labels":"She(O) wrote(O) seven(O) novels(B-literary genre) ,(O) Adam(B-book) Bede(I-book) ((O) 1859(O) )(O) ,(O) The(B-book) Mill(I-book) on(I-book) the(I-book) Floss(I-book) ((O) 1860(O) )(O) ,(O) Silas(B-book) Marner(I-book) ((O) 1861(O) )(O) ,(O) Romola(B-book) ((O) 1862-63(O) )(O) ,(O) Felix(B-book) Holt(I-book) ,(I-book) the(I-book) Radical(I-book) ((O) 1866(O) )(O) ,(O) Middlemarch(B-book) ((O) 1871-72(O) )(O) and(O) Daniel(B-book) Deronda(I-book) ((O) 1876(O) )(O) ,(O) most(O) of(O) which(O) are(O) set(O) in(O) provincial(O) England(B-country) and(O) known(O) for(O) their(O) realism(O) and(O) psychological(O) insight(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, organization, country, person, book, location, writer, award, literary genre, poem, event and O.\nSentence: She wrote seven novels , Adam Bede ( 1859 ) , The Mill on the Floss ( 1860 ) , Silas Marner ( 1861 ) , Romola ( 1862-63 ) , Felix Holt , the Radical ( 1866 ) , Middlemarch ( 1871-72 ) and Daniel Deronda ( 1876 ) , most of which are set in provincial England and known for their realism and psychological insight .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","wrote","seven","novels",",","Adam","Bede","(","1859",")",",","The","Mill","on","the","Floss","(","1860",")",",","Silas","Marner","(","1861",")",",","Romola","(","1862-63",")",",","Felix","Holt",",","the","Radical","(","1866",")",",","Middlemarch","(","1871-72",")","and","Daniel","Deronda","(","1876",")",",","most","of","which","are","set","in","provincial","England","and","known","for","their","realism","and","psychological","insight","."],"labels":["O","O","O","B-literary genre","O","B-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","O","O","O","O","B-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","O","O","O","O","B-book","I-book","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","organization","country","person","book","location","writer","award","literary_genre","poem","event"]}
{"id":"357","dataset":"crossner_literature","split":"test","instance":{"id":"357","prompt_labels":"For(O) example(O) ,(O) Holmes(B-person) falls(O) in(O) love(O) and(O) marries(O) in(O) Laurie(B-writer) R.(I-writer) King(I-writer) '(O) s(O) Mary(O) Russell(O) series(O) ,(O) is(O) re-animated(O) after(O) his(O) death(O) to(O) fight(O) future(O) crime(O) in(O) the(O) animated(O) series(O) Sherlock(O) Holmes(O) in(O) the(O) 22nd(O) Century(O) ,(O) and(O) is(O) meshed(O) with(O) the(O) setting(O) of(O) H.(B-writer) P.(I-writer) Lovecraft(I-writer) '(O) s(O) Cthulhu(B-book) Mythos(I-book) in(O) Neil(B-writer) Gaiman(I-writer) '(O) s(O) A(B-book) Study(I-book) in(I-book) Emerald(I-book) ((O) which(O) won(O) the(O) 2004(O) Hugo(B-award) Award(I-award) for(I-award) Best(I-award) Short(I-award) Story(I-award) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, award, poem, person, event, organization, country, location, book, writer, magazine and O.\nSentence: For example , Holmes falls in love and marries in Laurie R. King ' s Mary Russell series , is re-animated after his death to fight future crime in the animated series Sherlock Holmes in the 22nd Century , and is meshed with the setting of H. P. Lovecraft ' s Cthulhu Mythos in Neil Gaiman ' s A Study in Emerald ( which won the 2004 Hugo Award for Best Short Story ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","example",",","Holmes","falls","in","love","and","marries","in","Laurie","R.","King","'","s","Mary","Russell","series",",","is","re-animated","after","his","death","to","fight","future","crime","in","the","animated","series","Sherlock","Holmes","in","the","22nd","Century",",","and","is","meshed","with","the","setting","of","H.","P.","Lovecraft","'","s","Cthulhu","Mythos","in","Neil","Gaiman","'","s","A","Study","in","Emerald","(","which","won","the","2004","Hugo","Award","for","Best","Short","Story",")","."],"labels":["O","O","O","B-person","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","O","O","B-book","I-book","O","B-writer","I-writer","O","O","B-book","I-book","I-book","I-book","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","award","poem","person","event","organization","country","location","book","writer","magazine"]}
{"id":"365","dataset":"crossner_literature","split":"test","instance":{"id":"365","prompt_labels":"Dune(B-book) was(O) parodied(O) in(O) 1984(O) 's(O) National(B-book) Lampoon(I-book) 's(I-book) Doon(I-book) by(O) Ellis(B-writer) Weiner(I-writer) ,(O) which(O) William(B-writer) F.(I-writer) Touponce(I-writer) called(O) something(O) of(O) a(O) tribute(O) to(O) Herbert(B-writer) 's(O) success(O) on(O) college(O) campuses(O) ,(O) noting(O) that(O) the(O) only(O) other(O) book(O) to(O) have(O) been(O) so(O) honored(O) is(O) J.(B-writer) R.(I-writer) R.(I-writer) Tolkien(I-writer) '(O) s(O) Lord(B-book) of(I-book) the(I-book) Rings(I-book) ,(O) which(O) Bored(B-book) of(I-book) the(I-book) Rings(I-book) by(O) The(B-magazine) Harvard(I-magazine) Lampoon(I-magazine) in(O) 1969(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, literary genre, poem, location, person, award, event, book, writer, organization, magazine and O.\nSentence: Dune was parodied in 1984 's National Lampoon 's Doon by Ellis Weiner , which William F. Touponce called something of a tribute to Herbert 's success on college campuses , noting that the only other book to have been so honored is J. R. R. Tolkien ' s Lord of the Rings , which Bored of the Rings by The Harvard Lampoon in 1969 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Dune","was","parodied","in","1984","'s","National","Lampoon","'s","Doon","by","Ellis","Weiner",",","which","William","F.","Touponce","called","something","of","a","tribute","to","Herbert","'s","success","on","college","campuses",",","noting","that","the","only","other","book","to","have","been","so","honored","is","J.","R.","R.","Tolkien","'","s","Lord","of","the","Rings",",","which","Bored","of","the","Rings","by","The","Harvard","Lampoon","in","1969","."],"labels":["B-book","O","O","O","O","O","B-book","I-book","I-book","I-book","O","B-writer","I-writer","O","O","B-writer","I-writer","I-writer","O","O","O","O","O","O","B-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-writer","I-writer","I-writer","I-writer","O","O","B-book","I-book","I-book","I-book","O","O","B-book","I-book","I-book","I-book","O","B-magazine","I-magazine","I-magazine","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","literary_genre","poem","location","person","award","event","book","writer","organization","magazine"]}
{"id":"370","dataset":"crossner_literature","split":"test","instance":{"id":"370","prompt_labels":"He(O) received(O) his(O) first(O) nomination(O) for(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) for(O) his(O) circus(O) drama(O) The(O) Greatest(O) Show(O) on(O) Earth(O) ((O) 1952(O) )(O) ,(O) which(O) won(O) both(O) the(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Picture(I-award) and(O) the(O) Golden(B-award) Globe(I-award) Award(I-award) for(I-award) Best(I-award) Motion(I-award) Picture(I-award) -(I-award) Drama(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: literary genre, book, country, award, magazine, writer, event, person, organization, poem, location and O.\nSentence: He received his first nomination for the Academy Award for Best Director for his circus drama The Greatest Show on Earth ( 1952 ) , which won both the Academy Award for Best Picture and the Golden Globe Award for Best Motion Picture - Drama .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","received","his","first","nomination","for","the","Academy","Award","for","Best","Director","for","his","circus","drama","The","Greatest","Show","on","Earth","(","1952",")",",","which","won","both","the","Academy","Award","for","Best","Picture","and","the","Golden","Globe","Award","for","Best","Motion","Picture","-","Drama","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["literary_genre","book","country","award","magazine","writer","event","person","organization","poem","location"]}
{"id":"373","dataset":"crossner_literature","split":"test","instance":{"id":"373","prompt_labels":"His(O) fictional(B-literary genre) work(I-literary genre) ,(O) Redburn(B-book) ((O) 1849(O) )(O) ,(O) and(O) his(O) non-fiction(O) White-Jacket(B-book) ((O) 1850(O) )(O) were(O) given(O) better(O) reviews(O) but(O) did(O) not(O) provide(O) financial(O) security(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, country, event, award, book, poem, person, literary genre, organization, writer, location and O.\nSentence: His fictional work , Redburn ( 1849 ) , and his non-fiction White-Jacket ( 1850 ) were given better reviews but did not provide financial security .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","fictional","work",",","Redburn","(","1849",")",",","and","his","non-fiction","White-Jacket","(","1850",")","were","given","better","reviews","but","did","not","provide","financial","security","."],"labels":["O","B-literary genre","I-literary genre","O","B-book","O","O","O","O","O","O","O","B-book","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","country","event","award","book","poem","person","literary_genre","organization","writer","location"]}
{"id":"383","dataset":"crossner_literature","split":"test","instance":{"id":"383","prompt_labels":"Vernor(B-writer) Steffen(I-writer) Vinge(I-writer) ((O) He(O) has(O) won(O) the(O) Hugo(B-award) Award(I-award) for(O) his(O) novels(B-literary genre) and(O) novellas(B-literary genre) A(B-book) Fire(I-book) Upon(I-book) the(I-book) Deep(I-book) ((O) 1992(O) )(O) ,(O) A(B-book) Deepness(I-book) in(I-book) the(I-book) Sky(I-book) ((O) 1999(O) )(O) ,(O) Rainbows(B-book) End(I-book) ((O) 2006(O) )(O) ,(O) Fast(B-book) Times(I-book) at(I-book) Fairmont(I-book) High(I-book) ((O) 2002(O) )(O) ,(O) and(O) The(B-book) Cookie(I-book) Monster(I-book) ((O) 2004(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, country, magazine, literary genre, award, event, poem, person, organization, book, location and O.\nSentence: Vernor Steffen Vinge ( He has won the Hugo Award for his novels and novellas A Fire Upon the Deep ( 1992 ) , A Deepness in the Sky ( 1999 ) , Rainbows End ( 2006 ) , Fast Times at Fairmont High ( 2002 ) , and The Cookie Monster ( 2004 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Vernor","Steffen","Vinge","(","He","has","won","the","Hugo","Award","for","his","novels","and","novellas","A","Fire","Upon","the","Deep","(","1992",")",",","A","Deepness","in","the","Sky","(","1999",")",",","Rainbows","End","(","2006",")",",","Fast","Times","at","Fairmont","High","(","2002",")",",","and","The","Cookie","Monster","(","2004",")","."],"labels":["B-writer","I-writer","I-writer","O","O","O","O","O","B-award","I-award","O","O","B-literary genre","O","B-literary genre","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","B-book","I-book","O","O","O","O","B-book","I-book","I-book","I-book","I-book","O","O","O","O","O","B-book","I-book","I-book","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","country","magazine","literary_genre","award","event","poem","person","organization","book","location"]}
{"id":"384","dataset":"crossner_literature","split":"test","instance":{"id":"384","prompt_labels":"This(O) period(O) also(O) saw(O) alternate(B-literary genre) history(I-literary genre) works(O) by(O) S.(B-writer) M.(I-writer) Stirling(I-writer) ,(O) Kim(B-writer) Stanley(I-writer) Robinson(I-writer) ,(O) Harry(B-writer) Harrison(I-writer) ,(O) Howard(B-writer) Waldrop(I-writer) ,(O) and(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, book, poem, country, location, literary genre, event, person, magazine, award, organization and O.\nSentence: This period also saw alternate history works by S. M. Stirling , Kim Stanley Robinson , Harry Harrison , Howard Waldrop , and others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","period","also","saw","alternate","history","works","by","S.","M.","Stirling",",","Kim","Stanley","Robinson",",","Harry","Harrison",",","Howard","Waldrop",",","and","others","."],"labels":["O","O","O","O","B-literary genre","I-literary genre","O","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","I-writer","O","B-writer","I-writer","O","B-writer","I-writer","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","book","poem","country","location","literary_genre","event","person","magazine","award","organization"]}
{"id":"389","dataset":"crossner_literature","split":"test","instance":{"id":"389","prompt_labels":"Likewise(O) ,(O) in(O) 1963(O) ,(O) J.(B-writer) M.(I-writer) G.(I-writer) Le(I-writer) Clézio(I-writer) ,(O) winner(O) of(O) the(O) 2008(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) ,(O) published(O) the(O) novel(B-literary genre) Le(B-book) Proces-Verbal(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, literary genre, book, writer, event, award, magazine, poem, country, location, organization and O.\nSentence: Likewise , in 1963 , J. M. G. Le Clézio , winner of the 2008 Nobel Prize in Literature , published the novel Le Proces-Verbal .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Likewise",",","in","1963",",","J.","M.","G.","Le","Clézio",",","winner","of","the","2008","Nobel","Prize","in","Literature",",","published","the","novel","Le","Proces-Verbal","."],"labels":["O","O","O","O","O","B-writer","I-writer","I-writer","I-writer","I-writer","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","B-literary genre","B-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["person","literary_genre","book","writer","event","award","magazine","poem","country","location","organization"]}
{"id":"393","dataset":"crossner_literature","split":"test","instance":{"id":"393","prompt_labels":"The(O) Knighthood(O) ((O) i.e.(O) the(O) title(O) of(O) '(O) Sir(O) '(O) )(O) was(O) conferred(O) on(O) him(O) by(O) the(O) same(O) King(B-person) George(I-person) V(I-person) after(O) receiving(O) the(O) Nobel(B-award) Prize(I-award) in(I-award) Literature(I-award) for(O) Gitanjali(B-book) from(O) the(O) government(O) of(O) Sweden(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, poem, person, location, country, literary genre, book, magazine, award, writer and O.\nSentence: The Knighthood ( i.e. the title of ' Sir ' ) was conferred on him by the same King George V after receiving the Nobel Prize in Literature for Gitanjali from the government of Sweden .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Knighthood","(","i.e.","the","title","of","'","Sir","'",")","was","conferred","on","him","by","the","same","King","George","V","after","receiving","the","Nobel","Prize","in","Literature","for","Gitanjali","from","the","government","of","Sweden","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","O","O","O","B-award","I-award","I-award","I-award","O","B-book","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["organization","event","poem","person","location","country","literary_genre","book","magazine","award","writer"]}
{"id":"396","dataset":"crossner_literature","split":"test","instance":{"id":"396","prompt_labels":"In(O) November(O) 1813(O) Johann(B-writer) Wolfgang(I-writer) von(I-writer) Goethe(I-writer) invited(O) Schopenhauer(B-writer) for(O) research(O) on(O) his(O) Theory(B-book) of(I-book) Colours(I-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: book, location, organization, award, country, event, person, poem, magazine, writer, literary genre and O.\nSentence: In November 1813 Johann Wolfgang von Goethe invited Schopenhauer for research on his Theory of Colours .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","November","1813","Johann","Wolfgang","von","Goethe","invited","Schopenhauer","for","research","on","his","Theory","of","Colours","."],"labels":["O","O","O","B-writer","I-writer","I-writer","I-writer","O","B-writer","O","O","O","O","B-book","I-book","I-book","O"],"target_index":null,"target_label":null},"label_list":["book","location","organization","award","country","event","person","poem","magazine","writer","literary_genre"]}
{"id":"398","dataset":"crossner_literature","split":"test","instance":{"id":"398","prompt_labels":"Other(O) ancient(B-literary genre) epic(I-literary genre) poetry(I-literary genre) includes(O) the(O) Greek(B-literary genre) epics(I-literary genre) ,(O) the(O) Iliad(B-book) and(O) the(O) Odyssey(B-book) ;(O) the(O) Avestan(O) books(O) ,(O) the(O) Gathic(B-book) Avesta(I-book) and(O) the(O) Yasna(B-book) ;(O) the(O) Ancient(B-country) Rome(I-country) national(O) epic(O) ,(O) Virgil(B-writer) '(O) s(O) Aeneid(B-book) ((O) written(O) between(O) 29(O) and(O) 19(O) BCE(O) )(O) ;(O) and(O) the(O) Indian(B-literary genre) epics(I-literary genre) ,(O) the(O) Ramayana(B-book) and(O) the(O) Mahabharata(B-book) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, writer, literary genre, poem, location, magazine, book, organization, country, person and O.\nSentence: Other ancient epic poetry includes the Greek epics , the Iliad and the Odyssey ; the Avestan books , the Gathic Avesta and the Yasna ; the Ancient Rome national epic , Virgil ' s Aeneid ( written between 29 and 19 BCE ) ; and the Indian epics , the Ramayana and the Mahabharata .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","ancient","epic","poetry","includes","the","Greek","epics",",","the","Iliad","and","the","Odyssey",";","the","Avestan","books",",","the","Gathic","Avesta","and","the","Yasna",";","the","Ancient","Rome","national","epic",",","Virgil","'","s","Aeneid","(","written","between","29","and","19","BCE",")",";","and","the","Indian","epics",",","the","Ramayana","and","the","Mahabharata","."],"labels":["O","B-literary genre","I-literary genre","I-literary genre","O","O","B-literary genre","I-literary genre","O","O","B-book","O","O","B-book","O","O","O","O","O","O","B-book","I-book","O","O","B-book","O","O","B-country","I-country","O","O","O","B-writer","O","O","B-book","O","O","O","O","O","O","O","O","O","O","O","B-literary genre","I-literary genre","O","O","B-book","O","O","B-book","O"],"target_index":null,"target_label":null},"label_list":["event","award","writer","literary_genre","poem","location","magazine","book","organization","country","person"]}
{"id":"402","dataset":"crossner_literature","split":"test","instance":{"id":"402","prompt_labels":"Charles(B-person) appeared(O) in(O) the(O) John(B-writer) Godber(I-writer) comedy(O) play(O) Teechers(O) ,(O) in(O) which(O) he(O) swapped(O) in(O) and(O) out(O) of(O) various(O) roles(O) ,(O) at(O) the(O) Arts(B-location) Theatre(I-location) ,(O) London(B-location) ,(O) and(O) at(O) the(O) Edinburgh(B-event) Festival(I-event) ((O) 1989(O) )(O) ,(O) and(O) he(O) played(O) Idle(B-person) Jack(I-person) in(O) the(O) pantomime(O) Dick(O) Whittington(O) ,(O) at(O) the(O) Hull(B-location) New(I-location) Theatre(I-location) ((O) 1997(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, magazine, location, poem, person, award, book, country, organization, event, literary genre and O.\nSentence: Charles appeared in the John Godber comedy play Teechers , in which he swapped in and out of various roles , at the Arts Theatre , London , and at the Edinburgh Festival ( 1989 ) , and he played Idle Jack in the pantomime Dick Whittington , at the Hull New Theatre ( 1997 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Charles","appeared","in","the","John","Godber","comedy","play","Teechers",",","in","which","he","swapped","in","and","out","of","various","roles",",","at","the","Arts","Theatre",",","London",",","and","at","the","Edinburgh","Festival","(","1989",")",",","and","he","played","Idle","Jack","in","the","pantomime","Dick","Whittington",",","at","the","Hull","New","Theatre","(","1997",")","."],"labels":["B-person","O","O","O","B-writer","I-writer","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","O","O","O","B-event","I-event","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","magazine","location","poem","person","award","book","country","organization","event","literary_genre"]}
{"id":"407","dataset":"crossner_literature","split":"test","instance":{"id":"407","prompt_labels":"Germany(B-country) '(O) s(O) national(O) poet(O) ,(O) Johann(B-writer) Wolfgang(I-writer) von(I-writer) Goethe(I-writer) ,(O) also(O) wrote(O) many(O) sonnets(B-literary genre) ,(O) using(O) a(O) rhyme(O) scheme(O) derived(O) from(O) Italian(O) poetry(B-literary genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, writer, country, book, event, award, poem, organization, magazine, literary genre, person and O.\nSentence: Germany ' s national poet , Johann Wolfgang von Goethe , also wrote many sonnets , using a rhyme scheme derived from Italian poetry .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Germany","'","s","national","poet",",","Johann","Wolfgang","von","Goethe",",","also","wrote","many","sonnets",",","using","a","rhyme","scheme","derived","from","Italian","poetry","."],"labels":["B-country","O","O","O","O","O","B-writer","I-writer","I-writer","I-writer","O","O","O","O","B-literary genre","O","O","O","O","O","O","O","O","B-literary genre","O"],"target_index":null,"target_label":null},"label_list":["location","writer","country","book","event","award","poem","organization","magazine","literary_genre","person"]}
{"id":"408","dataset":"crossner_literature","split":"test","instance":{"id":"408","prompt_labels":"Ride(O) with(O) the(O) Devil(O) received(O) its(O) world(O) premiere(O) at(O) the(O) 25th(O) Deauville(B-event) American(I-event) Film(I-event) Festival(I-event) in(O) France(B-country) on(O) September(O) 9(O) ,(O) 1999(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, poem, person, location, country, writer, literary genre, award, organization, event, book and O.\nSentence: Ride with the Devil received its world premiere at the 25th Deauville American Film Festival in France on September 9 , 1999 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ride","with","the","Devil","received","its","world","premiere","at","the","25th","Deauville","American","Film","Festival","in","France","on","September","9",",","1999","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O","B-country","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","poem","person","location","country","writer","literary_genre","award","organization","event","book"]}
{"id":"411","dataset":"crossner_literature","split":"test","instance":{"id":"411","prompt_labels":"The(O) several(O) characters(O) live(O) within(O) a(O) divided(O) United(B-country) States(I-country) ,(O) in(O) which(O) the(O) Empire(B-country) of(I-country) Japan(I-country) takes(O) the(O) Pacific(B-location) states(I-location) ,(O) governing(O) them(O) as(O) a(O) puppet(O) ,(O) Nazi(B-country) Germany(I-country) takes(O) the(O) East(B-location) Coast(I-location) of(I-location) the(I-location) United(I-location) States(I-location) and(O) parts(O) of(O) the(O) Midwest(B-location) ,(O) with(O) the(O) remnants(O) of(O) the(O) old(O) United(B-country) States(I-country) '(O) government(O) as(O) the(O) Neutral(B-location) Zone(I-location) ,(O) a(O) buffer(O) state(O) between(O) the(O) two(O) superpowers(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: writer, organization, literary genre, book, poem, award, person, location, event, magazine, country and O.\nSentence: The several characters live within a divided United States , in which the Empire of Japan takes the Pacific states , governing them as a puppet , Nazi Germany takes the East Coast of the United States and parts of the Midwest , with the remnants of the old United States ' government as the Neutral Zone , a buffer state between the two superpowers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","several","characters","live","within","a","divided","United","States",",","in","which","the","Empire","of","Japan","takes","the","Pacific","states",",","governing","them","as","a","puppet",",","Nazi","Germany","takes","the","East","Coast","of","the","United","States","and","parts","of","the","Midwest",",","with","the","remnants","of","the","old","United","States","'","government","as","the","Neutral","Zone",",","a","buffer","state","between","the","two","superpowers","."],"labels":["O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","B-country","I-country","I-country","O","O","B-location","I-location","O","O","O","O","O","O","O","B-country","I-country","O","O","B-location","I-location","I-location","I-location","I-location","I-location","O","O","O","O","B-location","O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["writer","organization","literary_genre","book","poem","award","person","location","event","magazine","country"]}
{"id":"412","dataset":"crossner_literature","split":"test","instance":{"id":"412","prompt_labels":"Oliver(B-book) Twist(I-book) and(O) Great(B-book) Expectations(I-book) are(O) also(O) frequently(O) adapted(O) and(O) ,(O) like(O) many(O) of(O) his(O) novels(B-literary genre) ,(O) evoke(O) images(O) of(O) early(O) Victorian(B-location) London(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, writer, literary genre, country, award, poem, book, location, magazine, organization, event and O.\nSentence: Oliver Twist and Great Expectations are also frequently adapted and , like many of his novels , evoke images of early Victorian London .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Oliver","Twist","and","Great","Expectations","are","also","frequently","adapted","and",",","like","many","of","his","novels",",","evoke","images","of","early","Victorian","London","."],"labels":["B-book","I-book","O","B-book","I-book","O","O","O","O","O","O","O","O","O","O","B-literary genre","O","O","O","O","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["person","writer","literary_genre","country","award","poem","book","location","magazine","organization","event"]}
{"id":"413","dataset":"crossner_literature","split":"test","instance":{"id":"413","prompt_labels":"Eisenhower(B-person) 's(O) stint(O) as(O) the(O) president(O) of(O) Columbia(B-organization) University(I-organization) was(O) punctuated(O) by(O) his(O) activity(O) within(O) the(O) Council(B-organization) on(I-organization) Foreign(I-organization) Relations(I-organization) ,(O) a(O) study(O) group(O) he(O) led(O) as(O) president(O) concerning(O) the(O) political(O) and(O) military(O) implications(O) of(O) the(O) Marshall(O) Plan(O) ,(O) and(O) The(B-organization) American(I-organization) Assembly(I-organization) ,(O) Eisenhower(B-person) 's(O) vision(O) of(O) a(O) great(O) cultural(O) center(O) where(O) business(O) ,(O) professional(O) and(O) governmental(O) leaders(O) could(O) meet(O) from(O) time(O) to(O) time(O) to(O) discuss(O) and(O) reach(O) conclusions(O) concerning(O) problems(O) of(O) a(O) social(O) and(O) political(O) nature(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, literary genre, poem, event, location, magazine, writer, country, award, book and O.\nSentence: Eisenhower 's stint as the president of Columbia University was punctuated by his activity within the Council on Foreign Relations , a study group he led as president concerning the political and military implications of the Marshall Plan , and The American Assembly , Eisenhower 's vision of a great cultural center where business , professional and governmental leaders could meet from time to time to discuss and reach conclusions concerning problems of a social and political nature .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Eisenhower","'s","stint","as","the","president","of","Columbia","University","was","punctuated","by","his","activity","within","the","Council","on","Foreign","Relations",",","a","study","group","he","led","as","president","concerning","the","political","and","military","implications","of","the","Marshall","Plan",",","and","The","American","Assembly",",","Eisenhower","'s","vision","of","a","great","cultural","center","where","business",",","professional","and","governmental","leaders","could","meet","from","time","to","time","to","discuss","and","reach","conclusions","concerning","problems","of","a","social","and","political","nature","."],"labels":["B-person","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","person","literary_genre","poem","event","location","magazine","writer","country","award","book"]}
{"id":"415","dataset":"crossner_literature","split":"test","instance":{"id":"415","prompt_labels":"Shortly(O) thereafter(O) ,(O) Bloch(B-writer) created(O) the(O) Damon(B-writer) Runyon(I-writer) -esque(O) humorous(O) series(O) character(O) Lefty(B-person) Feep(I-person) in(O) the(O) story(O) Time(B-book) Wounds(I-book) All(I-book) Heels(I-book) Fantastic(B-magazine) Adventures(I-magazine) ((O) April(O) 1942(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: magazine, event, location, book, poem, country, award, person, writer, literary genre, organization and O.\nSentence: Shortly thereafter , Bloch created the Damon Runyon -esque humorous series character Lefty Feep in the story Time Wounds All Heels Fantastic Adventures ( April 1942 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Shortly","thereafter",",","Bloch","created","the","Damon","Runyon","-esque","humorous","series","character","Lefty","Feep","in","the","story","Time","Wounds","All","Heels","Fantastic","Adventures","(","April","1942",")","."],"labels":["O","O","O","B-writer","O","O","B-writer","I-writer","O","O","O","O","B-person","I-person","O","O","O","B-book","I-book","I-book","I-book","B-magazine","I-magazine","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["magazine","event","location","book","poem","country","award","person","writer","literary_genre","organization"]}
{"id":"4","dataset":"crossner_music","split":"test","instance":{"id":"4","prompt_labels":"None(O) of(O) the(O) singles(O) from(O) the(O) album(O) reached(O) number(O) one(O) on(O) the(O) UK(B-country) charts(O) ,(O) but(O) Chiquitita(B-song) ,(O) Does(B-song) Your(I-song) Mother(I-song) Know(I-song) ,(O) Angeleyes(B-song) ((O) with(O) Voulez-Vous(B-album) ,(O) released(O) as(O) a(O) double(O) A-side(O) )(O) and(O) I(B-song) Have(I-song) a(I-song) Dream(I-song) were(O) all(O) UK(B-country) Top(O) 5(O) hits(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, musical artist, country, song, band, organization, music genre, location, musical instrument, event, album and O.\nSentence: None of the singles from the album reached number one on the UK charts , but Chiquitita , Does Your Mother Know , Angeleyes ( with Voulez-Vous , released as a double A-side ) and I Have a Dream were all UK Top 5 hits .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["None","of","the","singles","from","the","album","reached","number","one","on","the","UK","charts",",","but","Chiquitita",",","Does","Your","Mother","Know",",","Angeleyes","(","with","Voulez-Vous",",","released","as","a","double","A-side",")","and","I","Have","a","Dream","were","all","UK","Top","5","hits","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","B-song","O","B-song","I-song","I-song","I-song","O","B-song","O","O","B-album","O","O","O","O","O","O","O","O","B-song","I-song","I-song","I-song","O","O","B-country","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","award","musical_artist","country","song","band","organization","music_genre","location","musical_instrument","event","album"]}
{"id":"14","dataset":"crossner_music","split":"test","instance":{"id":"14","prompt_labels":"Often(O) cited(O) as(O) a(O) pop(O) icon(O) ,(O) Timberlake(B-musical artist) is(O) the(O) recipient(O) of(O) numerous(O) awards(O) and(O) accolades(O) ,(O) including(O) ten(O) Grammy(B-award) Award(I-award) s(O) ,(O) four(O) Emmy(B-award) Award(I-award) s(O) ,(O) three(O) Brit(B-award) Awards(I-award) ,(O) nine(O) Billboard(B-award) Music(I-award) Awards(I-award) ,(O) the(O) Contemporary(B-award) Icon(I-award) Award(I-award) by(O) the(O) Songwriters(B-organization) Hall(I-organization) of(I-organization) Fame(I-organization) ,(O) an(O) honorary(B-award) Doctor(I-award) of(I-award) Music(I-award) degree(I-award) from(O) the(O) Berklee(B-organization) College(I-organization) of(I-organization) Music(I-organization) ,(O) and(O) the(O) Michael(B-award) Jackson(I-award) Video(I-award) Vanguard(I-award) Award(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, musical artist, album, music genre, location, country, musical instrument, band, organization, person, award and O.\nSentence: Often cited as a pop icon , Timberlake is the recipient of numerous awards and accolades , including ten Grammy Award s , four Emmy Award s , three Brit Awards , nine Billboard Music Awards , the Contemporary Icon Award by the Songwriters Hall of Fame , an honorary Doctor of Music degree from the Berklee College of Music , and the Michael Jackson Video Vanguard Award .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Often","cited","as","a","pop","icon",",","Timberlake","is","the","recipient","of","numerous","awards","and","accolades",",","including","ten","Grammy","Award","s",",","four","Emmy","Award","s",",","three","Brit","Awards",",","nine","Billboard","Music","Awards",",","the","Contemporary","Icon","Award","by","the","Songwriters","Hall","of","Fame",",","an","honorary","Doctor","of","Music","degree","from","the","Berklee","College","of","Music",",","and","the","Michael","Jackson","Video","Vanguard","Award","."],"labels":["O","O","O","O","O","O","O","B-musical artist","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","O","O","B-award","I-award","I-award","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-award","I-award","I-award","I-award","I-award","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","B-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["song","event","musical_artist","album","music_genre","location","country","musical_instrument","band","organization","person","award"]}
{"id":"18","dataset":"crossner_music","split":"test","instance":{"id":"18","prompt_labels":"The(O) first(O) wave(O) of(O) black(B-music genre) metal(I-music genre) emerged(O) in(O) Europe(B-location) in(O) the(O) early(O) and(O) mid-1980s(O) ,(O) led(O) by(O) the(O) United(B-country) Kingdom(I-country) 's(O) Venom(B-band) ,(O) Denmark(B-country) 's(O) Mercyful(B-band) Fate(I-band) ,(O) Switzerland(B-country) 's(O) Hellhammer(B-band) and(O) Celtic(B-band) Frost(I-band) ,(O) and(O) Sweden(B-country) 's(O) Bathory(B-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, organization, country, music genre, musical instrument, album, band, award, location, musical artist, person, event and O.\nSentence: The first wave of black metal emerged in Europe in the early and mid-1980s , led by the United Kingdom 's Venom , Denmark 's Mercyful Fate , Switzerland 's Hellhammer and Celtic Frost , and Sweden 's Bathory .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","first","wave","of","black","metal","emerged","in","Europe","in","the","early","and","mid-1980s",",","led","by","the","United","Kingdom","'s","Venom",",","Denmark","'s","Mercyful","Fate",",","Switzerland","'s","Hellhammer","and","Celtic","Frost",",","and","Sweden","'s","Bathory","."],"labels":["O","O","O","O","B-music genre","I-music genre","O","O","B-location","O","O","O","O","O","O","O","O","O","B-country","I-country","O","B-band","O","B-country","O","B-band","I-band","O","B-country","O","B-band","O","B-band","I-band","O","O","B-country","O","B-band","O"],"target_index":null,"target_label":null},"label_list":["song","organization","country","music_genre","musical_instrument","album","band","award","location","musical_artist","person","event"]}
{"id":"20","dataset":"crossner_music","split":"test","instance":{"id":"20","prompt_labels":"Starting(O) in(O) the(O) early(O) 1950s(O) ,(O) and(O) during(O) the(O) mid-1960s(O) ,(O) Western(O) singer-songwriters(O) such(O) as(O) Michael(B-musical artist) Martin(I-musical artist) Murphey(I-musical artist) and(O) Marty(B-musical artist) Robbins(I-musical artist) rose(O) in(O) prominence(O) as(O) did(O) others(O) ,(O) throughout(O) Western(B-music genre) music(I-music genre) traditions(O) ,(O) like(O) New(B-music genre) Mexico(I-music genre) music(I-music genre) '(O) s(O) Al(B-musical artist) Hurricane(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, music genre, organization, musical artist, award, band, musical instrument, event, person, location, album, country and O.\nSentence: Starting in the early 1950s , and during the mid-1960s , Western singer-songwriters such as Michael Martin Murphey and Marty Robbins rose in prominence as did others , throughout Western music traditions , like New Mexico music ' s Al Hurricane .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Starting","in","the","early","1950s",",","and","during","the","mid-1960s",",","Western","singer-songwriters","such","as","Michael","Martin","Murphey","and","Marty","Robbins","rose","in","prominence","as","did","others",",","throughout","Western","music","traditions",",","like","New","Mexico","music","'","s","Al","Hurricane","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","B-music genre","I-music genre","I-music genre","O","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["song","music_genre","organization","musical_artist","award","band","musical_instrument","event","person","location","album","country"]}
{"id":"26","dataset":"crossner_music","split":"test","instance":{"id":"26","prompt_labels":"After(O) Gulf(B-album) Winds(I-album) ((O) 1976(O) )(O) ,(O) an(O) album(O) of(O) entirely(O) self-composed(O) songs(O) and(O) From(B-album) Every(I-album) Stage(I-album) ((O) 1976(O) )(O) ,(O) a(O) live(O) album(O) that(O) had(O) Baez(B-musical artist) performing(O) songs(O) from(O) every(O) stage(O) of(O) her(O) career(O) ,(O) Baez(B-musical artist) again(O) parted(O) ways(O) with(O) a(O) record(O) label(O) when(O) she(O) moved(O) to(O) CBS(B-organization) Records(I-organization) for(O) Blowin(B-album) '(I-album) Away(I-album) ((O) 1977(O) )(O) and(O) Honest(B-album) Lullaby(I-album) ((O) 1979(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, album, band, location, award, music genre, musical artist, song, country, organization, musical instrument, event and O.\nSentence: After Gulf Winds ( 1976 ) , an album of entirely self-composed songs and From Every Stage ( 1976 ) , a live album that had Baez performing songs from every stage of her career , Baez again parted ways with a record label when she moved to CBS Records for Blowin ' Away ( 1977 ) and Honest Lullaby ( 1979 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","Gulf","Winds","(","1976",")",",","an","album","of","entirely","self-composed","songs","and","From","Every","Stage","(","1976",")",",","a","live","album","that","had","Baez","performing","songs","from","every","stage","of","her","career",",","Baez","again","parted","ways","with","a","record","label","when","she","moved","to","CBS","Records","for","Blowin","'","Away","(","1977",")","and","Honest","Lullaby","(","1979",")","."],"labels":["O","B-album","I-album","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O","O","O","B-musical artist","O","O","O","O","O","O","O","O","O","B-musical artist","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","album","band","location","award","music_genre","musical_artist","song","country","organization","musical_instrument","event"]}
{"id":"31","dataset":"crossner_music","split":"test","instance":{"id":"31","prompt_labels":"The(O) style(O) of(O) British(B-music genre) blues(I-music genre) developed(O) in(O) the(O) UK(B-country) ,(O) when(O) bands(O) such(O) as(O) the(O) The(B-band) Animals(I-band) ,(O) Fleetwood(B-band) Mac(I-band) ,(O) John(B-band) Mayall(I-band) &(I-band) the(I-band) Bluesbreakers(I-band) ,(O) the(O) The(B-band) Rolling(I-band) Stones(I-band) ,(O) the(O) The(B-band) Yardbirds(I-band) ,(O) the(O) supergroup(B-band) Cream(I-band) and(O) the(O) Irish(O) musician(O) Rory(B-musical artist) Gallagher(I-musical artist) performed(O) classic(O) blues(B-music genre) songs(O) from(O) the(O) Delta(B-music genre) blues(I-music genre) or(O) Chicago(B-music genre) blues(I-music genre) traditions(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, organization, song, music genre, award, musical instrument, musical artist, album, band, country, location and O.\nSentence: The style of British blues developed in the UK , when bands such as the The Animals , Fleetwood Mac , John Mayall & the Bluesbreakers , the The Rolling Stones , the The Yardbirds , the supergroup Cream and the Irish musician Rory Gallagher performed classic blues songs from the Delta blues or Chicago blues traditions .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","style","of","British","blues","developed","in","the","UK",",","when","bands","such","as","the","The","Animals",",","Fleetwood","Mac",",","John","Mayall","&","the","Bluesbreakers",",","the","The","Rolling","Stones",",","the","The","Yardbirds",",","the","supergroup","Cream","and","the","Irish","musician","Rory","Gallagher","performed","classic","blues","songs","from","the","Delta","blues","or","Chicago","blues","traditions","."],"labels":["O","O","O","B-music genre","I-music genre","O","O","O","B-country","O","O","O","O","O","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","I-band","I-band","I-band","O","O","B-band","I-band","I-band","O","O","B-band","I-band","O","O","B-band","I-band","O","O","O","O","B-musical artist","I-musical artist","O","O","B-music genre","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["person","event","organization","song","music_genre","award","musical_instrument","musical_artist","album","band","country","location"]}
{"id":"36","dataset":"crossner_music","split":"test","instance":{"id":"36","prompt_labels":"Duke(B-musical artist) turned(O) 65(O) in(O) the(O) spring(O) of(O) 1964(O) but(O) showed(O) no(O) signs(O) of(O) slowing(O) down(O) as(O) he(O) continued(O) to(O) make(O) vital(O) and(O) innovative(O) recordings(O) ,(O) including(O) The(B-album) Far(I-album) East(I-album) Suite(I-album) ((O) 1966(O) )(O) ,(O) New(B-album) Orleans(I-album) Suite(I-album) ((O) 1970(O) )(O) ,(O) Latin(B-album) American(I-album) Suite(I-album) ((O) 1972(O) )(O) and(O) The(B-album) Afro-Eurasian(I-album) Eclipse(I-album) ((O) 1971(O) )(O) ,(O) much(O) of(O) it(O) inspired(O) by(O) his(O) world(O) tours(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, band, song, musical instrument, country, music genre, organization, album, musical artist, person, award and O.\nSentence: Duke turned 65 in the spring of 1964 but showed no signs of slowing down as he continued to make vital and innovative recordings , including The Far East Suite ( 1966 ) , New Orleans Suite ( 1970 ) , Latin American Suite ( 1972 ) and The Afro-Eurasian Eclipse ( 1971 ) , much of it inspired by his world tours .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Duke","turned","65","in","the","spring","of","1964","but","showed","no","signs","of","slowing","down","as","he","continued","to","make","vital","and","innovative","recordings",",","including","The","Far","East","Suite","(","1966",")",",","New","Orleans","Suite","(","1970",")",",","Latin","American","Suite","(","1972",")","and","The","Afro-Eurasian","Eclipse","(","1971",")",",","much","of","it","inspired","by","his","world","tours","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","location","band","song","musical_instrument","country","music_genre","organization","album","musical_artist","person","award"]}
{"id":"38","dataset":"crossner_music","split":"test","instance":{"id":"38","prompt_labels":"Some(O) managers(O) in(O) Europe(B-location) soon(O) created(O) their(O) own(O) acts(O) after(O) being(O) inspired(O) by(O) New(B-band) Kids(I-band) on(I-band) the(I-band) Block(I-band) ,(O) beginning(O) with(O) Nigel(B-musical artist) Martin-Smith(I-musical artist) '(O) s(O) Take(B-band) That(I-band) in(O) the(O) UK(B-country) ((O) formed(O) in(O) 1990(O) )(O) and(O) followed(O) by(O) Tom(B-musical artist) Watkins(I-musical artist) ,(O) who(O) had(O) success(O) with(O) Bros(B-band) in(O) the(O) late(O) 1980s(O) and(O) formed(O) East(B-band) 17(I-band) in(O) 1991(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical artist, album, band, organization, country, event, award, song, musical instrument, location, person and O.\nSentence: Some managers in Europe soon created their own acts after being inspired by New Kids on the Block , beginning with Nigel Martin-Smith ' s Take That in the UK ( formed in 1990 ) and followed by Tom Watkins , who had success with Bros in the late 1980s and formed East 17 in 1991 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","managers","in","Europe","soon","created","their","own","acts","after","being","inspired","by","New","Kids","on","the","Block",",","beginning","with","Nigel","Martin-Smith","'","s","Take","That","in","the","UK","(","formed","in","1990",")","and","followed","by","Tom","Watkins",",","who","had","success","with","Bros","in","the","late","1980s","and","formed","East","17","in","1991","."],"labels":["O","O","O","B-location","O","O","O","O","O","O","O","O","O","B-band","I-band","I-band","I-band","I-band","O","O","O","B-musical artist","I-musical artist","O","O","B-band","I-band","O","O","B-country","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","B-band","O","O","O","O","O","O","B-band","I-band","O","O","O"],"target_index":null,"target_label":null},"label_list":["music_genre","musical_artist","album","band","organization","country","event","award","song","musical_instrument","location","person"]}
{"id":"40","dataset":"crossner_music","split":"test","instance":{"id":"40","prompt_labels":"On(O) 26(O) March(O) 2001(O) ,(O) their(O) first(O) full-length(O) album(O) ,(O) the(O) self-titled(O) Gorillaz(B-album) ,(O) was(O) released(O) ,(O) producing(O) four(O) singles(O) :(O) Clint(B-song) Eastwood(I-song) ,(O) 19-2000(B-song) ,(O) Rock(B-song) the(I-song) House(I-song) ,(O) and(O) Tomorrow(B-song) Comes(I-song) Today(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, musical artist, song, organization, location, album, musical instrument, country, band, music genre, event and O.\nSentence: On 26 March 2001 , their first full-length album , the self-titled Gorillaz , was released , producing four singles : Clint Eastwood , 19-2000 , Rock the House , and Tomorrow Comes Today .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","26","March","2001",",","their","first","full-length","album",",","the","self-titled","Gorillaz",",","was","released",",","producing","four","singles",":","Clint","Eastwood",",","19-2000",",","Rock","the","House",",","and","Tomorrow","Comes","Today","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-album","O","O","O","O","O","O","O","O","B-song","I-song","O","B-song","O","B-song","I-song","I-song","O","O","B-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["person","award","musical_artist","song","organization","location","album","musical_instrument","country","band","music_genre","event"]}
{"id":"49","dataset":"crossner_music","split":"test","instance":{"id":"49","prompt_labels":"In(O) 1985(O) ,(O) the(O) trio(B-band) joined(O) up(O) with(O) Marc(B-musical artist) Almond(I-musical artist) to(O) record(O) a(O) version(O) of(O) Donna(B-musical artist) Summer(I-musical artist) '(O) s(O) I(B-song) Feel(I-song) Love(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, music genre, organization, band, song, award, musical artist, album, musical instrument, country, event and O.\nSentence: In 1985 , the trio joined up with Marc Almond to record a version of Donna Summer ' s I Feel Love .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","1985",",","the","trio","joined","up","with","Marc","Almond","to","record","a","version","of","Donna","Summer","'","s","I","Feel","Love","."],"labels":["O","O","O","O","B-band","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","B-musical artist","I-musical artist","O","O","B-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["location","person","music_genre","organization","band","song","award","musical_artist","album","musical_instrument","country","event"]}
{"id":"53","dataset":"crossner_music","split":"test","instance":{"id":"53","prompt_labels":"Funk(O) drumming(O) creates(O) a(O) groove(O) by(O) emphasizing(O) the(O) drummer(O) 's(O) feel(O) and(O) emotion(O) ,(O) which(O) including(O) occasional(O) tempo(O) fluctuations(O) ,(O) the(O) use(O) of(O) Swing(B-music genre) music(I-music genre) feel(O) in(O) some(O) songs(O) ((O) e.g.(O) ,(O) Cissy(B-song) Strut(I-song) by(O) The(B-band) Meters(I-band) and(O) I(B-song) 'll(I-song) Take(I-song) You(I-song) There(I-song) by(O) The(B-band) Staple(I-band) Singers(I-band) ,(O) which(O) have(O) a(O) half-swung(O) feel(O) )(O) ,(O) and(O) less(O) use(O) of(O) fills(O) ((O) as(O) they(O) can(O) lessen(O) the(O) groove(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, country, organization, location, musical instrument, music genre, musical artist, person, song, band, event, award and O.\nSentence: Funk drumming creates a groove by emphasizing the drummer 's feel and emotion , which including occasional tempo fluctuations , the use of Swing music feel in some songs ( e.g. , Cissy Strut by The Meters and I 'll Take You There by The Staple Singers , which have a half-swung feel ) , and less use of fills ( as they can lessen the groove ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Funk","drumming","creates","a","groove","by","emphasizing","the","drummer","'s","feel","and","emotion",",","which","including","occasional","tempo","fluctuations",",","the","use","of","Swing","music","feel","in","some","songs","(","e.g.",",","Cissy","Strut","by","The","Meters","and","I","'ll","Take","You","There","by","The","Staple","Singers",",","which","have","a","half-swung","feel",")",",","and","less","use","of","fills","(","as","they","can","lessen","the","groove",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","B-song","I-song","O","B-band","I-band","O","B-song","I-song","I-song","I-song","I-song","O","B-band","I-band","I-band","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["album","country","organization","location","musical_instrument","music_genre","musical_artist","person","song","band","event","award"]}
{"id":"55","dataset":"crossner_music","split":"test","instance":{"id":"55","prompt_labels":"The(O) album(O) also(O) won(O) Best(B-award) Album(I-award) at(I-award) the(I-award) 2002(I-award) MTV(I-award) Europe(I-award) Music(I-award) Awards(I-award) ,(O) Billboard(B-award) Music(I-award) Award(I-award) for(I-award) Top(I-award) Billboard(I-award) 200(I-award) Album(I-award) at(O) the(O) 2002(O) Billboard(B-award) Music(I-award) Awards(I-award) and(O) Best(B-award) International(I-award) Album(I-award) and(O) International(B-award) Album(I-award) of(I-award) the(I-award) Year(I-award) at(O) the(O) Brit(B-award) Awards(I-award) and(O) the(O) Juno(B-award) Awards(I-award) respectively(O) in(O) 2003(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, music genre, country, band, musical artist, award, person, song, organization, album, musical instrument and O.\nSentence: The album also won Best Album at the 2002 MTV Europe Music Awards , Billboard Music Award for Top Billboard 200 Album at the 2002 Billboard Music Awards and Best International Album and International Album of the Year at the Brit Awards and the Juno Awards respectively in 2003 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","album","also","won","Best","Album","at","the","2002","MTV","Europe","Music","Awards",",","Billboard","Music","Award","for","Top","Billboard","200","Album","at","the","2002","Billboard","Music","Awards","and","Best","International","Album","and","International","Album","of","the","Year","at","the","Brit","Awards","and","the","Juno","Awards","respectively","in","2003","."],"labels":["O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","B-award","I-award","I-award","O","B-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","O","O","B-award","I-award","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","location","music_genre","country","band","musical_artist","award","person","song","organization","album","musical_instrument"]}
{"id":"65","dataset":"crossner_music","split":"test","instance":{"id":"65","prompt_labels":"Alt-country(B-music genre) ,(O) in(O) various(O) iterations(O) overlapped(O) with(O) other(O) genres(O) ,(O) including(O) Red(B-music genre) Dirt(I-music genre) country(I-music genre) music(I-music genre) ((O) Cross(B-band) Canadian(I-band) Ragweed(I-band) )(O) ,(O) jam(O) band(O) s(O) ((O) My(B-band) Morning(I-band) Jacket(I-band) and(O) The(B-band) String(I-band) Cheese(I-band) Incident(I-band) )(O) ,(O) and(O) indie(B-music genre) folk(I-music genre) ((O) The(B-band) Avett(I-band) Brothers(I-band) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, band, song, location, person, event, country, organization, music genre, musical artist, musical instrument, award and O.\nSentence: Alt-country , in various iterations overlapped with other genres , including Red Dirt country music ( Cross Canadian Ragweed ) , jam band s ( My Morning Jacket and The String Cheese Incident ) , and indie folk ( The Avett Brothers ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Alt-country",",","in","various","iterations","overlapped","with","other","genres",",","including","Red","Dirt","country","music","(","Cross","Canadian","Ragweed",")",",","jam","band","s","(","My","Morning","Jacket","and","The","String","Cheese","Incident",")",",","and","indie","folk","(","The","Avett","Brothers",")","."],"labels":["B-music genre","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","I-music genre","O","B-band","I-band","I-band","O","O","O","O","O","O","B-band","I-band","I-band","O","B-band","I-band","I-band","I-band","O","O","O","B-music genre","I-music genre","O","B-band","I-band","I-band","O","O"],"target_index":null,"target_label":null},"label_list":["album","band","song","location","person","event","country","organization","music_genre","musical_artist","musical_instrument","award"]}
{"id":"69","dataset":"crossner_music","split":"test","instance":{"id":"69","prompt_labels":"Today(O) ,(O) zydeco(O) integrates(O) genres(O) such(O) as(O) Rhythm(B-music genre) and(I-music genre) blues(I-music genre) ,(O) Soul(B-music genre) music(I-music genre) ,(O) brass(B-music genre) band(I-music genre) ,(O) reggae(B-music genre) ,(O) Hip(B-music genre) hop(I-music genre) music(I-music genre) ,(O) ska(B-music genre) ,(O) Rock(B-music genre) music(I-music genre) ,(O) Afro-Caribbean(B-music genre) music(I-music genre) and(O) other(O) styles(O) ,(O) in(O) addition(O) to(O) the(O) traditional(O) forms(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, location, band, music genre, country, album, event, song, musical artist, organization, person, award and O.\nSentence: Today , zydeco integrates genres such as Rhythm and blues , Soul music , brass band , reggae , Hip hop music , ska , Rock music , Afro-Caribbean music and other styles , in addition to the traditional forms .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Today",",","zydeco","integrates","genres","such","as","Rhythm","and","blues",",","Soul","music",",","brass","band",",","reggae",",","Hip","hop","music",",","ska",",","Rock","music",",","Afro-Caribbean","music","and","other","styles",",","in","addition","to","the","traditional","forms","."],"labels":["O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","location","band","music_genre","country","album","event","song","musical_artist","organization","person","award"]}
{"id":"74","dataset":"crossner_music","split":"test","instance":{"id":"74","prompt_labels":"The(O) territory(O) band(O) s(O) operating(O) out(O) of(O) Kansas(B-location) City(I-location) ,(O) the(O) Bennie(B-band) Moten(I-band) orchestra(I-band) ,(O) Jay(B-musical artist) McShann(I-musical artist) ,(O) and(O) the(O) Count(B-band) Basie(I-band) Orchestra(I-band) were(O) also(O) concentrating(O) on(O) the(O) blues(B-music genre) ,(O) with(O) 12-bar(O) blues(O) instrumentals(O) such(O) as(O) Basie(B-musical artist) 's(O) One(B-song) O(I-song) 'Clock(I-song) Jump(I-song) and(O) Jumpin(B-song) '(I-song) at(I-song) the(I-song) Woodside(I-song) and(O) boisterous(O) blues(B-music genre) shouting(O) by(O) Jimmy(B-musical artist) Rushing(I-musical artist) on(O) songs(O) such(O) as(O) Going(B-song) to(I-song) Chicago(I-song) and(O) Sent(B-song) for(I-song) You(I-song) Yesterday(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, event, person, musical instrument, band, song, award, organization, music genre, country, musical artist, location and O.\nSentence: The territory band s operating out of Kansas City , the Bennie Moten orchestra , Jay McShann , and the Count Basie Orchestra were also concentrating on the blues , with 12-bar blues instrumentals such as Basie 's One O 'Clock Jump and Jumpin ' at the Woodside and boisterous blues shouting by Jimmy Rushing on songs such as Going to Chicago and Sent for You Yesterday .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","territory","band","s","operating","out","of","Kansas","City",",","the","Bennie","Moten","orchestra",",","Jay","McShann",",","and","the","Count","Basie","Orchestra","were","also","concentrating","on","the","blues",",","with","12-bar","blues","instrumentals","such","as","Basie","'s","One","O","'Clock","Jump","and","Jumpin","'","at","the","Woodside","and","boisterous","blues","shouting","by","Jimmy","Rushing","on","songs","such","as","Going","to","Chicago","and","Sent","for","You","Yesterday","."],"labels":["O","O","O","O","O","O","O","B-location","I-location","O","O","B-band","I-band","I-band","O","B-musical artist","I-musical artist","O","O","O","B-band","I-band","I-band","O","O","O","O","O","B-music genre","O","O","O","O","O","O","O","B-musical artist","O","B-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","I-song","O","O","B-music genre","O","O","B-musical artist","I-musical artist","O","O","O","O","B-song","I-song","I-song","O","B-song","I-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["album","event","person","musical_instrument","band","song","award","organization","music_genre","country","musical_artist","location"]}
{"id":"79","dataset":"crossner_music","split":"test","instance":{"id":"79","prompt_labels":"They(O) play(O) Avant-garde(B-music genre) jazz(I-music genre) versions(O) of(O) tradition(O) American(B-music genre) Folk(I-music genre) music(I-music genre) &(O) amp(O) ;(O) Blues(B-music genre) songs(O) with(O) Ritchie(B-musical artist) 's(O) shakuhachi(B-musical instrument) playing(O) as(O) the(O) focal(O) point(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, award, musical instrument, event, song, band, musical artist, organization, music genre, album, country and O.\nSentence: They play Avant-garde jazz versions of tradition American Folk music & amp ; Blues songs with Ritchie 's shakuhachi playing as the focal point .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","play","Avant-garde","jazz","versions","of","tradition","American","Folk","music","&","amp",";","Blues","songs","with","Ritchie","'s","shakuhachi","playing","as","the","focal","point","."],"labels":["O","O","B-music genre","I-music genre","O","O","O","B-music genre","I-music genre","I-music genre","O","O","O","B-music genre","O","O","B-musical artist","O","B-musical instrument","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","person","award","musical_instrument","event","song","band","musical_artist","organization","music_genre","album","country"]}
{"id":"82","dataset":"crossner_music","split":"test","instance":{"id":"82","prompt_labels":"By(O) the(O) time(O) she(O) released(O) her(O) fourth(O) album(O) in(O) the(O) early(O) 1990s(O) ,(O) she(O) had(O) amassed(O) several(O) top(O) ten(O) singles(O) in(O) the(O) UK(B-country) and(O) Australia(B-country) ,(O) including(O) I(B-song) Should(I-song) Be(I-song) So(I-song) Lucky(I-song) ,(O) The(B-song) Loco-Motion(I-song) ,(O) Hand(B-song) on(I-song) Your(I-song) Heart(I-song) ,(O) Better(B-song) the(I-song) Devil(I-song) You(I-song) Know(I-song) and(O) Step(B-song) Back(I-song) in(I-song) Time(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, album, person, song, country, band, location, musical artist, musical instrument, event, music genre, organization and O.\nSentence: By the time she released her fourth album in the early 1990s , she had amassed several top ten singles in the UK and Australia , including I Should Be So Lucky , The Loco-Motion , Hand on Your Heart , Better the Devil You Know and Step Back in Time .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["By","the","time","she","released","her","fourth","album","in","the","early","1990s",",","she","had","amassed","several","top","ten","singles","in","the","UK","and","Australia",",","including","I","Should","Be","So","Lucky",",","The","Loco-Motion",",","Hand","on","Your","Heart",",","Better","the","Devil","You","Know","and","Step","Back","in","Time","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","B-country","O","O","B-song","I-song","I-song","I-song","I-song","O","B-song","I-song","O","B-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["award","album","person","song","country","band","location","musical_artist","musical_instrument","event","music_genre","organization"]}
{"id":"84","dataset":"crossner_music","split":"test","instance":{"id":"84","prompt_labels":"She(O) had(O) her(O) first(O) experience(O) in(O) show(O) business(O) when(O) she(O) was(O) crowned(O) Miss(B-award) Teenager(I-award) Universal(I-award) in(O) 1971(O) ,(O) and(O) was(O) Miss(B-award) World(I-award) /(O) Venezuela(B-country) in(O) 1975(O) where(O) she(O) became(O) sixth(O) runner-up(O) in(O) the(O) Miss(B-event) World(I-event) pageant(I-event) won(O) by(O) Puerto(B-location) Rico(I-location) 's(O) Wilnelia(B-person) Merced(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, album, location, event, song, organization, musical artist, person, music genre, musical instrument, band, award and O.\nSentence: She had her first experience in show business when she was crowned Miss Teenager Universal in 1971 , and was Miss World / Venezuela in 1975 where she became sixth runner-up in the Miss World pageant won by Puerto Rico 's Wilnelia Merced .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","had","her","first","experience","in","show","business","when","she","was","crowned","Miss","Teenager","Universal","in","1971",",","and","was","Miss","World","/","Venezuela","in","1975","where","she","became","sixth","runner-up","in","the","Miss","World","pageant","won","by","Puerto","Rico","'s","Wilnelia","Merced","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","I-award","O","O","O","O","O","B-award","I-award","O","B-country","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","B-location","I-location","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["country","album","location","event","song","organization","musical_artist","person","music_genre","musical_instrument","band","award"]}
{"id":"90","dataset":"crossner_music","split":"test","instance":{"id":"90","prompt_labels":"He(O) also(O) co-wrote(O) Posible(B-song) ,(O) which(O) has(O) been(O) used(O) as(O) a(O) theme(O) song(O) for(O) the(O) 2005(B-event) Southeast(I-event) Asian(I-event) Games(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, award, person, country, event, musical artist, musical instrument, song, album, band, organization, location and O.\nSentence: He also co-wrote Posible , which has been used as a theme song for the 2005 Southeast Asian Games .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","also","co-wrote","Posible",",","which","has","been","used","as","a","theme","song","for","the","2005","Southeast","Asian","Games","."],"labels":["O","O","O","B-song","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["music_genre","award","person","country","event","musical_artist","musical_instrument","song","album","band","organization","location"]}
{"id":"93","dataset":"crossner_music","split":"test","instance":{"id":"93","prompt_labels":"It(O) is(O) recognized(O) as(O) a(O) primary(O) instrument(O) in(O) genres(O) such(O) as(O) blues(B-music genre) ,(O) Bluegrass(B-music genre) music(I-music genre) ,(O) country(B-music genre) ,(O) flamenco(B-music genre) ,(O) Folk(B-music genre) music(I-music genre) ,(O) jazz(B-music genre) ,(O) jota(B-music genre) ,(O) mariachi(B-music genre) ,(O) metal(B-music genre) ,(O) punk(B-music genre) ,(O) reggae(B-music genre) ,(O) rock(B-music genre) ,(O) Soul(B-music genre) music(I-music genre) ,(O) and(O) pop(B-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, musical instrument, person, album, song, musical artist, band, award, music genre, event, country and O.\nSentence: It is recognized as a primary instrument in genres such as blues , Bluegrass music , country , flamenco , Folk music , jazz , jota , mariachi , metal , punk , reggae , rock , Soul music , and pop .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","is","recognized","as","a","primary","instrument","in","genres","such","as","blues",",","Bluegrass","music",",","country",",","flamenco",",","Folk","music",",","jazz",",","jota",",","mariachi",",","metal",",","punk",",","reggae",",","rock",",","Soul","music",",","and","pop","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","O","B-music genre","O","B-music genre","O","B-music genre","O","B-music genre","O","B-music genre","O","B-music genre","I-music genre","O","O","B-music genre","O"],"target_index":null,"target_label":null},"label_list":["location","organization","musical_instrument","person","album","song","musical_artist","band","award","music_genre","event","country"]}
{"id":"96","dataset":"crossner_music","split":"test","instance":{"id":"96","prompt_labels":"On(O) February(O) 17(O) ,(O) 2020(O) ,(O) a(O) 50th(O) anniversary(O) concert(O) production(O) of(O) Joseph(B-organization) was(O) staged(O) at(O) Lincoln(B-location) Center(I-location) '(O) s(O) David(B-location) Geffen(I-location) Hall(I-location) in(O) New(B-location) York(I-location) City(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, person, country, song, band, organization, event, musical artist, album, location, award, music genre and O.\nSentence: On February 17 , 2020 , a 50th anniversary concert production of Joseph was staged at Lincoln Center ' s David Geffen Hall in New York City .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","February","17",",","2020",",","a","50th","anniversary","concert","production","of","Joseph","was","staged","at","Lincoln","Center","'","s","David","Geffen","Hall","in","New","York","City","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","O","B-location","I-location","O","O","B-location","I-location","I-location","O","B-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","person","country","song","band","organization","event","musical_artist","album","location","award","music_genre"]}
{"id":"99","dataset":"crossner_music","split":"test","instance":{"id":"99","prompt_labels":"Examples(O) include(O) Portuguese(O) Fado(B-music genre) ,(O) Spanish-speaking(O) Mexican(B-music genre) Regional(I-music genre) ,(O) Reggaeton(B-music genre) and(O) Tejano(B-music genre) music(I-music genre) ,(O) French(O) Cajun(B-music genre) music(I-music genre) ((O) especially(O) in(O) French(B-location) Louisiana(I-location) )(O) ,(O) Russian(O) Shanson(B-music genre) ,(O) and(O) ((O) since(O) the(O) late(O) 2000s(O) )(O) Korean(O) K-pop(B-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, country, event, musical instrument, person, music genre, album, award, band, song, organization and O.\nSentence: Examples include Portuguese Fado , Spanish-speaking Mexican Regional , Reggaeton and Tejano music , French Cajun music ( especially in French Louisiana ) , Russian Shanson , and ( since the late 2000s ) Korean K-pop .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Examples","include","Portuguese","Fado",",","Spanish-speaking","Mexican","Regional",",","Reggaeton","and","Tejano","music",",","French","Cajun","music","(","especially","in","French","Louisiana",")",",","Russian","Shanson",",","and","(","since","the","late","2000s",")","Korean","K-pop","."],"labels":["O","O","O","B-music genre","O","O","B-music genre","I-music genre","O","B-music genre","O","B-music genre","I-music genre","O","O","B-music genre","I-music genre","O","O","O","B-location","I-location","O","O","O","B-music genre","O","O","O","O","O","O","O","O","O","B-music genre","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","location","country","event","musical_instrument","person","music_genre","album","award","band","song","organization"]}
{"id":"102","dataset":"crossner_music","split":"test","instance":{"id":"102","prompt_labels":"The(O) banned(O) monastic(O) orders(O) :(O) Jesuits(B-organization) ,(O) Camaldolese(B-organization) ,(O) Order(B-organization) of(I-organization) Friars(I-organization) Minor(I-organization) Capuchin(I-organization) ,(O) Carmelites(B-organization) ,(O) Carthusians(B-organization) ,(O) Poor(B-organization) Clares(I-organization) ,(O) Order(B-organization) of(I-organization) Saint(I-organization) Benedict(I-organization) ,(O) Cistercians(B-organization) ,(O) Dominican(B-organization) Order(I-organization) ((O) Order(B-organization) of(I-organization) Preachers(I-organization) )(O) ,(O) Franciscans(B-organization) ,(O) Order(B-organization) of(I-organization) Saint(I-organization) Paul(I-organization) the(I-organization) First(I-organization) Hermit(I-organization) and(O) Premonstratensians(B-organization) ,(O) and(O) their(O) wealth(O) was(O) taken(O) over(O) by(O) the(O) Religious(B-organization) Fund(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, event, music genre, song, location, country, band, album, musical instrument, organization, musical artist, person and O.\nSentence: The banned monastic orders : Jesuits , Camaldolese , Order of Friars Minor Capuchin , Carmelites , Carthusians , Poor Clares , Order of Saint Benedict , Cistercians , Dominican Order ( Order of Preachers ) , Franciscans , Order of Saint Paul the First Hermit and Premonstratensians , and their wealth was taken over by the Religious Fund .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","banned","monastic","orders",":","Jesuits",",","Camaldolese",",","Order","of","Friars","Minor","Capuchin",",","Carmelites",",","Carthusians",",","Poor","Clares",",","Order","of","Saint","Benedict",",","Cistercians",",","Dominican","Order","(","Order","of","Preachers",")",",","Franciscans",",","Order","of","Saint","Paul","the","First","Hermit","and","Premonstratensians",",","and","their","wealth","was","taken","over","by","the","Religious","Fund","."],"labels":["O","O","O","O","O","B-organization","O","B-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","B-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","B-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["award","event","music_genre","song","location","country","band","album","musical_instrument","organization","musical_artist","person"]}
{"id":"103","dataset":"crossner_music","split":"test","instance":{"id":"103","prompt_labels":"Black(B-music genre) metal(I-music genre) album(O) covers(O) are(O) typically(O) dark(O) and(O) tend(O) to(O) be(O) atmospheric(O) or(O) provocative(O) ;(O) some(O) feature(O) natural(O) or(O) fantasy(O) landscapes(O) ((O) for(O) example(O) Burzum(B-band) '(O) s(O) Filosofem(B-album) and(O) Emperor(B-band) 's(O) In(B-album) the(I-album) Nightside(I-album) Eclipse(I-album) )(O) while(O) others(O) are(O) violent(O) ,(O) sexually(O) transgressive(O) ,(O) sacrilegious(O) ,(O) or(O) iconoclastic(O) ((O) for(O) example(O) Marduk(B-band) '(O) s(O) Fuck(B-album) Me(I-album) Jesus(I-album) and(O) Dimmu(B-band) Borgir(I-band) '(O) s(O) In(B-album) Sorte(I-album) Diaboli(I-album) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, band, music genre, award, organization, song, location, person, album, country, musical artist, musical instrument and O.\nSentence: Black metal album covers are typically dark and tend to be atmospheric or provocative ; some feature natural or fantasy landscapes ( for example Burzum ' s Filosofem and Emperor 's In the Nightside Eclipse ) while others are violent , sexually transgressive , sacrilegious , or iconoclastic ( for example Marduk ' s Fuck Me Jesus and Dimmu Borgir ' s In Sorte Diaboli ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Black","metal","album","covers","are","typically","dark","and","tend","to","be","atmospheric","or","provocative",";","some","feature","natural","or","fantasy","landscapes","(","for","example","Burzum","'","s","Filosofem","and","Emperor","'s","In","the","Nightside","Eclipse",")","while","others","are","violent",",","sexually","transgressive",",","sacrilegious",",","or","iconoclastic","(","for","example","Marduk","'","s","Fuck","Me","Jesus","and","Dimmu","Borgir","'","s","In","Sorte","Diaboli",")","."],"labels":["B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","O","O","B-album","O","B-band","O","B-album","I-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","O","O","B-album","I-album","I-album","O","B-band","I-band","O","O","B-album","I-album","I-album","O","O"],"target_index":null,"target_label":null},"label_list":["event","band","music_genre","award","organization","song","location","person","album","country","musical_artist","musical_instrument"]}
{"id":"105","dataset":"crossner_music","split":"test","instance":{"id":"105","prompt_labels":"The(O) Dark(B-song) Star(I-song) St.(I-song) Stephen(I-song) pairing(O) was(O) taken(O) from(O) the(O) February(O) 27(O) ,(O) 1969(O) show(O) at(O) the(O) The(B-location) Fillmore(I-location) ;(O) The(B-song) Eleven(I-song) and(O) Turn(B-song) On(I-song) Your(I-song) Love(I-song) Light(I-song) were(O) from(O) the(O) January(O) 26(O) ,(O) 1969(O) show(O) at(O) the(O) Avalon(B-location) Ballroom(I-location) ;(O) Death(B-song) Don(I-song) 't(I-song) Have(I-song) No(I-song) Mercy(I-song) ,(O) Feedback(B-song) ,(O) and(O) And(O) We(B-song) Bid(I-song) You(I-song) Goodnight(I-song) were(O) recorded(O) March(O) 2(O) ,(O) 1969(O) ,(O) at(O) the(O) The(B-location) Fillmore(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, music genre, band, musical artist, song, album, country, event, person, organization, location, musical instrument and O.\nSentence: The Dark Star St. Stephen pairing was taken from the February 27 , 1969 show at the The Fillmore ; The Eleven and Turn On Your Love Light were from the January 26 , 1969 show at the Avalon Ballroom ; Death Don 't Have No Mercy , Feedback , and And We Bid You Goodnight were recorded March 2 , 1969 , at the The Fillmore .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Dark","Star","St.","Stephen","pairing","was","taken","from","the","February","27",",","1969","show","at","the","The","Fillmore",";","The","Eleven","and","Turn","On","Your","Love","Light","were","from","the","January","26",",","1969","show","at","the","Avalon","Ballroom",";","Death","Don","'t","Have","No","Mercy",",","Feedback",",","and","And","We","Bid","You","Goodnight","were","recorded","March","2",",","1969",",","at","the","The","Fillmore","."],"labels":["O","B-song","I-song","I-song","I-song","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-song","I-song","O","B-song","I-song","I-song","I-song","I-song","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-song","I-song","I-song","I-song","I-song","I-song","O","B-song","O","O","O","B-song","I-song","I-song","I-song","O","O","O","O","O","O","O","O","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["award","music_genre","band","musical_artist","song","album","country","event","person","organization","location","musical_instrument"]}
{"id":"108","dataset":"crossner_music","split":"test","instance":{"id":"108","prompt_labels":"Alt-country(B-music genre) ,(O) in(O) various(O) iterations(O) overlapped(O) with(O) other(O) genres(O) ,(O) including(O) Red(B-music genre) Dirt(I-music genre) country(I-music genre) music(I-music genre) ((O) Cross(B-band) Canadian(I-band) Ragweed(I-band) )(O) ,(O) jam(B-band) band(I-band) s(O) ((O) My(B-band) Morning(I-band) Jacket(I-band) and(O) The(B-band) String(I-band) Cheese(I-band) Incident(I-band) )(O) ,(O) and(O) indie(B-music genre) folk(I-music genre) ((O) The(B-band) Avett(I-band) Brothers(I-band) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, person, event, album, band, musical artist, location, song, organization, award, musical instrument, country and O.\nSentence: Alt-country , in various iterations overlapped with other genres , including Red Dirt country music ( Cross Canadian Ragweed ) , jam band s ( My Morning Jacket and The String Cheese Incident ) , and indie folk ( The Avett Brothers ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Alt-country",",","in","various","iterations","overlapped","with","other","genres",",","including","Red","Dirt","country","music","(","Cross","Canadian","Ragweed",")",",","jam","band","s","(","My","Morning","Jacket","and","The","String","Cheese","Incident",")",",","and","indie","folk","(","The","Avett","Brothers",")","."],"labels":["B-music genre","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","I-music genre","I-music genre","O","B-band","I-band","I-band","O","O","B-band","I-band","O","O","B-band","I-band","I-band","O","B-band","I-band","I-band","I-band","O","O","O","B-music genre","I-music genre","O","B-band","I-band","I-band","O","O"],"target_index":null,"target_label":null},"label_list":["music_genre","person","event","album","band","musical_artist","location","song","organization","award","musical_instrument","country"]}
{"id":"109","dataset":"crossner_music","split":"test","instance":{"id":"109","prompt_labels":"Paul(B-musical artist) Gilbert(I-musical artist) composes(O) music(O) in(O) a(O) wide(O) variety(O) of(O) styles(O) ,(O) including(O) Pop(B-music genre) music(I-music genre) ,(O) Rock(B-music genre) and(I-music genre) roll(I-music genre) ,(O) metal(B-music genre) ,(O) blues(B-music genre) ,(O) and(O) funk(B-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, event, band, person, musical artist, musical instrument, music genre, award, song, album, location and O.\nSentence: Paul Gilbert composes music in a wide variety of styles , including Pop music , Rock and roll , metal , blues , and funk .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Paul","Gilbert","composes","music","in","a","wide","variety","of","styles",",","including","Pop","music",",","Rock","and","roll",",","metal",",","blues",",","and","funk","."],"labels":["B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","O","B-music genre","O","O","B-music genre","O"],"target_index":null,"target_label":null},"label_list":["country","organization","event","band","person","musical_artist","musical_instrument","music_genre","award","song","album","location"]}
{"id":"123","dataset":"crossner_music","split":"test","instance":{"id":"123","prompt_labels":"For(O) the(O) 2000(B-event) World(I-event) Junior(I-event) Championships(I-event) in(I-event) Athletics(I-event) ,(O) the(O) installation(O) of(O) individual(O) seats(O) was(O) required(O) ,(O) which(O) reduced(O) capacity(O) to(O) 66,000(O) spectators(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, award, band, event, person, music genre, album, musical instrument, location, organization, musical artist, country and O.\nSentence: For the 2000 World Junior Championships in Athletics , the installation of individual seats was required , which reduced capacity to 66,000 spectators .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["For","the","2000","World","Junior","Championships","in","Athletics",",","the","installation","of","individual","seats","was","required",",","which","reduced","capacity","to","66,000","spectators","."],"labels":["O","O","B-event","I-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["song","award","band","event","person","music_genre","album","musical_instrument","location","organization","musical_artist","country"]}
{"id":"125","dataset":"crossner_music","split":"test","instance":{"id":"125","prompt_labels":"His(O) complex(O) and(O) intricate(O) fingerpicking(O) inspired(O) Reverend(B-musical artist) Gary(I-musical artist) Davis(I-musical artist) ,(O) Jorma(B-musical artist) Kaukonen(I-musical artist) ,(O) Ry(B-musical artist) Cooder(I-musical artist) ,(O) Arlen(B-musical artist) Roth(I-musical artist) ,(O) John(B-musical artist) Fahey(I-musical artist) ,(O) Ralph(B-musical artist) McTell(I-musical artist) ,(O) David(B-musical artist) Bromberg(I-musical artist) ,(O) Leon(B-musical artist) Redbone(I-musical artist) and(O) many(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, country, album, music genre, organization, song, location, musical artist, person, event, band, award and O.\nSentence: His complex and intricate fingerpicking inspired Reverend Gary Davis , Jorma Kaukonen , Ry Cooder , Arlen Roth , John Fahey , Ralph McTell , David Bromberg , Leon Redbone and many others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","complex","and","intricate","fingerpicking","inspired","Reverend","Gary","Davis",",","Jorma","Kaukonen",",","Ry","Cooder",",","Arlen","Roth",",","John","Fahey",",","Ralph","McTell",",","David","Bromberg",",","Leon","Redbone","and","many","others","."],"labels":["O","O","O","O","O","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","country","album","music_genre","organization","song","location","musical_artist","person","event","band","award"]}
{"id":"126","dataset":"crossner_music","split":"test","instance":{"id":"126","prompt_labels":"Due(O) to(O) shared(O) cultural(O) heritage(O) and(O) language(O) ,(O) Indian(O) music(O) and(O) Bollywood(O) films(O) are(O) also(O) popular(O) in(O) Afghanistan(B-country) ,(O) Pakistan(B-country) ,(O) Bangladesh(B-country) ,(O) and(O) Nepal(B-country) ,(O) where(O) Hindustani(O) is(O) widely(O) understood(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, album, musical instrument, music genre, song, band, musical artist, location, award, person, country and O.\nSentence: Due to shared cultural heritage and language , Indian music and Bollywood films are also popular in Afghanistan , Pakistan , Bangladesh , and Nepal , where Hindustani is widely understood .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Due","to","shared","cultural","heritage","and","language",",","Indian","music","and","Bollywood","films","are","also","popular","in","Afghanistan",",","Pakistan",",","Bangladesh",",","and","Nepal",",","where","Hindustani","is","widely","understood","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","B-country","O","B-country","O","O","B-country","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","event","album","musical_instrument","music_genre","song","band","musical_artist","location","award","person","country"]}
{"id":"128","dataset":"crossner_music","split":"test","instance":{"id":"128","prompt_labels":"The(O) 2017(O) production(O) was(O) nominated(O) for(O) 10(O) Laurence(B-award) Olivier(I-award) Award(I-award) s(O) and(O) won(O) 2(O) for(O) Laurence(B-award) Olivier(I-award) Award(I-award) for(I-award) Best(I-award) Musical(I-award) Revival(I-award) and(O) Laurence(B-award) Olivier(I-award) Award(I-award) for(I-award) Best(I-award) Costume(I-award) Design(I-award) ((O) by(O) Vicki(B-person) Mortimer(I-person) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, album, band, location, musical artist, award, organization, musical instrument, song, event, music genre, country and O.\nSentence: The 2017 production was nominated for 10 Laurence Olivier Award s and won 2 for Laurence Olivier Award for Best Musical Revival and Laurence Olivier Award for Best Costume Design ( by Vicki Mortimer ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","2017","production","was","nominated","for","10","Laurence","Olivier","Award","s","and","won","2","for","Laurence","Olivier","Award","for","Best","Musical","Revival","and","Laurence","Olivier","Award","for","Best","Costume","Design","(","by","Vicki","Mortimer",")","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","I-award","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","B-person","I-person","O","O"],"target_index":null,"target_label":null},"label_list":["person","album","band","location","musical_artist","award","organization","musical_instrument","song","event","music_genre","country"]}
{"id":"131","dataset":"crossner_music","split":"test","instance":{"id":"131","prompt_labels":"Artists(O) who(O) typified(O) this(O) sound(O) included(O) Travis(B-musical artist) Tritt(I-musical artist) ,(O) Reba(B-musical artist) McEntire(I-musical artist) ,(O) George(B-musical artist) Strait(I-musical artist) ,(O) Keith(B-musical artist) Whitley(I-musical artist) ,(O) Alan(B-musical artist) Jackson(I-musical artist) ,(O) Ricky(B-musical artist) Skaggs(I-musical artist) ,(O) Patty(B-musical artist) Loveless(I-musical artist) ,(O) Kathy(B-musical artist) Mattea(I-musical artist) ,(O) Randy(B-musical artist) Travis(I-musical artist) ,(O) Dwight(B-musical artist) Yoakam(I-musical artist) ,(O) and(O) The(B-band) Judds(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, song, organization, event, band, location, award, music genre, musical instrument, album, person, country and O.\nSentence: Artists who typified this sound included Travis Tritt , Reba McEntire , George Strait , Keith Whitley , Alan Jackson , Ricky Skaggs , Patty Loveless , Kathy Mattea , Randy Travis , Dwight Yoakam , and The Judds .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Artists","who","typified","this","sound","included","Travis","Tritt",",","Reba","McEntire",",","George","Strait",",","Keith","Whitley",",","Alan","Jackson",",","Ricky","Skaggs",",","Patty","Loveless",",","Kathy","Mattea",",","Randy","Travis",",","Dwight","Yoakam",",","and","The","Judds","."],"labels":["O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","song","organization","event","band","location","award","music_genre","musical_instrument","album","person","country"]}
{"id":"136","dataset":"crossner_music","split":"test","instance":{"id":"136","prompt_labels":"|(O) Invisible(O) Children(O) Invisible(B-organization) Children(I-organization) ,(I-organization) Inc(I-organization) ,(O) To(B-organization) Write(I-organization) Love(I-organization) On(I-organization) Her(I-organization) Arms(I-organization) ,(O) Shirts(B-organization) for(I-organization) a(I-organization) Cure(I-organization) ,(O) Keep(B-organization) A(I-organization) Breast(I-organization) Foundation(I-organization) ,(O) and(O) Hope(B-organization) For(I-organization) The(I-organization) Day(I-organization) to(O) advocate(O) about(O) their(O) cause(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, song, band, location, award, country, musical artist, person, music genre, album, organization, musical instrument and O.\nSentence: | Invisible Children Invisible Children , Inc , To Write Love On Her Arms , Shirts for a Cure , Keep A Breast Foundation , and Hope For The Day to advocate about their cause .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["|","Invisible","Children","Invisible","Children",",","Inc",",","To","Write","Love","On","Her","Arms",",","Shirts","for","a","Cure",",","Keep","A","Breast","Foundation",",","and","Hope","For","The","Day","to","advocate","about","their","cause","."],"labels":["O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","song","band","location","award","country","musical_artist","person","music_genre","album","organization","musical_instrument"]}
{"id":"139","dataset":"crossner_music","split":"test","instance":{"id":"139","prompt_labels":"Alan(B-musical artist) Lomax(I-musical artist) '(O) s(O) recordings(O) of(O) Mississippi(B-musical artist) Fred(I-musical artist) McDowell(I-musical artist) would(O) eventually(O) bring(O) him(O) wider(O) attention(O) on(O) both(O) the(O) blues(B-music genre) and(O) Folk(B-music genre) music(I-music genre) circuit(O) ,(O) with(O) McDowell(B-musical artist) 's(O) droning(B-music genre) style(I-music genre) influencing(O) North(B-location) Mississippi(I-location) hill(B-music genre) country(I-music genre) blues(I-music genre) musicians(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, event, album, musical artist, person, band, musical instrument, award, organization, location, music genre, country and O.\nSentence: Alan Lomax ' s recordings of Mississippi Fred McDowell would eventually bring him wider attention on both the blues and Folk music circuit , with McDowell 's droning style influencing North Mississippi hill country blues musicians .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Alan","Lomax","'","s","recordings","of","Mississippi","Fred","McDowell","would","eventually","bring","him","wider","attention","on","both","the","blues","and","Folk","music","circuit",",","with","McDowell","'s","droning","style","influencing","North","Mississippi","hill","country","blues","musicians","."],"labels":["B-musical artist","I-musical artist","O","O","O","O","B-musical artist","I-musical artist","I-musical artist","O","O","O","O","O","O","O","O","O","B-music genre","O","B-music genre","I-music genre","O","O","O","B-musical artist","O","B-music genre","I-music genre","O","B-location","I-location","B-music genre","I-music genre","I-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["song","event","album","musical_artist","person","band","musical_instrument","award","organization","location","music_genre","country"]}
{"id":"153","dataset":"crossner_music","split":"test","instance":{"id":"153","prompt_labels":"Some(O) of(O) Mattacks(B-musical artist) '(O) most(O) notable(O) participation(O) in(O) studio(O) recordings(O) in(O) the(O) late(O) 1970s(O) are(O) the(O) work(O) on(O) art(O) rock(O) studio(O) albums(O) by(O) Brian(B-musical artist) Eno(I-musical artist) ((O) Before(B-album) and(I-album) After(I-album) Science(I-album) )(O) and(O) 801(B-band) '(O) s(O) Listen(B-album) Now(I-album) ,(O) as(O) well(O) as(O) several(O) Ashley(B-musical artist) Hutchings(I-musical artist) -related(O) folk(B-music genre) rock(I-music genre) projects(O) ((O) The(B-album) Compleat(I-album) Dancing(I-album) Master(I-album) ,(O) Son(B-album) of(I-album) Morris(I-album) On(I-album) etc(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, event, album, award, song, organization, music genre, country, location, band, person, musical instrument and O.\nSentence: Some of Mattacks ' most notable participation in studio recordings in the late 1970s are the work on art rock studio albums by Brian Eno ( Before and After Science ) and 801 ' s Listen Now , as well as several Ashley Hutchings -related folk rock projects ( The Compleat Dancing Master , Son of Morris On etc .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","of","Mattacks","'","most","notable","participation","in","studio","recordings","in","the","late","1970s","are","the","work","on","art","rock","studio","albums","by","Brian","Eno","(","Before","and","After","Science",")","and","801","'","s","Listen","Now",",","as","well","as","several","Ashley","Hutchings","-related","folk","rock","projects","(","The","Compleat","Dancing","Master",",","Son","of","Morris","On","etc","."],"labels":["O","O","B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-album","I-album","I-album","I-album","O","O","B-band","O","O","B-album","I-album","O","O","O","O","O","B-musical artist","I-musical artist","O","B-music genre","I-music genre","O","O","B-album","I-album","I-album","I-album","O","B-album","I-album","I-album","I-album","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","event","album","award","song","organization","music_genre","country","location","band","person","musical_instrument"]}
{"id":"157","dataset":"crossner_music","split":"test","instance":{"id":"157","prompt_labels":"After(O) Deftones(B-band) '(O) third(O) album(O) White(B-album) Pony(I-album) ,(O) subsequent(O) releases(O) would(O) be(O) written(O) with(O) seven-strings(B-musical instrument) ,(O) until(O) 2010(O) 's(O) Diamond(B-album) Eyes(I-album) and(O) 2012(O) 's(O) Koi(B-album) No(I-album) Yokan(I-album) ,(O) which(O) were(O) written(O) with(O) an(O) eight-string(B-musical instrument) guitar(I-musical instrument) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, musical artist, country, location, musical instrument, music genre, album, song, award, person, event, organization and O.\nSentence: After Deftones ' third album White Pony , subsequent releases would be written with seven-strings , until 2010 's Diamond Eyes and 2012 's Koi No Yokan , which were written with an eight-string guitar .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","Deftones","'","third","album","White","Pony",",","subsequent","releases","would","be","written","with","seven-strings",",","until","2010","'s","Diamond","Eyes","and","2012","'s","Koi","No","Yokan",",","which","were","written","with","an","eight-string","guitar","."],"labels":["O","B-band","O","O","O","B-album","I-album","O","O","O","O","O","O","O","B-musical instrument","O","O","O","O","B-album","I-album","O","O","O","B-album","I-album","I-album","O","O","O","O","O","O","B-musical instrument","I-musical instrument","O"],"target_index":null,"target_label":null},"label_list":["band","musical_artist","country","location","musical_instrument","music_genre","album","song","award","person","event","organization"]}
{"id":"162","dataset":"crossner_music","split":"test","instance":{"id":"162","prompt_labels":"The(O) Nordic(B-band) Choir(I-band) has(O) also(O) appeared(O) throughout(O) the(O) United(B-country) States(I-country) ,(O) performing(O) in(O) well-known(O) concert(O) halls(O) as(O) Lincoln(B-location) Center(I-location) in(O) New(B-location) York(I-location) and(O) the(O) John(B-location) F.(I-location) Kennedy(I-location) Center(I-location) for(I-location) the(I-location) Performing(I-location) Arts(I-location) in(O) Washington(B-location) ,(I-location) D.C.(I-location) Additionally(O) ,(O) Luther(B-location) College(I-location) has(O) the(O) largest(O) collegiate(O) choral(O) program(O) in(O) the(O) United(B-country) States(I-country) with(O) almost(O) 600(O) student(O) singers(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, song, organization, location, award, band, musical instrument, person, country, event, music genre, album and O.\nSentence: The Nordic Choir has also appeared throughout the United States , performing in well-known concert halls as Lincoln Center in New York and the John F. Kennedy Center for the Performing Arts in Washington , D.C. Additionally , Luther College has the largest collegiate choral program in the United States with almost 600 student singers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Nordic","Choir","has","also","appeared","throughout","the","United","States",",","performing","in","well-known","concert","halls","as","Lincoln","Center","in","New","York","and","the","John","F.","Kennedy","Center","for","the","Performing","Arts","in","Washington",",","D.C.","Additionally",",","Luther","College","has","the","largest","collegiate","choral","program","in","the","United","States","with","almost","600","student","singers","."],"labels":["O","B-band","I-band","O","O","O","O","O","B-country","I-country","O","O","O","O","O","O","O","B-location","I-location","O","B-location","I-location","O","O","B-location","I-location","I-location","I-location","I-location","I-location","I-location","I-location","O","B-location","I-location","I-location","O","O","B-location","I-location","O","O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","song","organization","location","award","band","musical_instrument","person","country","event","music_genre","album"]}
{"id":"171","dataset":"crossner_music","split":"test","instance":{"id":"171","prompt_labels":"Many(O) early(O) rock(B-music genre) and(I-music genre) roll(I-music genre) songs(O) are(O) based(O) on(O) blues(B-music genre) :(O) That(B-song) 's(I-song) All(I-song) Right(I-song) Mama(I-song) ,(O) Johnny(B-song) B.(I-song) Goode(I-song) ,(O) Blue(B-song) Suede(I-song) Shoes(I-song) ,(O) Whole(B-song) Lotta(I-song) Shakin(I-song) '(I-song) Goin(I-song) On(I-song) ,(O) Shake(B-song) ,(I-song) Rattle(I-song) ,(I-song) and(I-song) Roll(I-song) ,(O) and(O) Long(B-song) Tall(I-song) Sally(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, country, organization, event, album, band, music genre, musical instrument, person, musical artist, location, award and O.\nSentence: Many early rock and roll songs are based on blues : That 's All Right Mama , Johnny B. Goode , Blue Suede Shoes , Whole Lotta Shakin ' Goin On , Shake , Rattle , and Roll , and Long Tall Sally .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Many","early","rock","and","roll","songs","are","based","on","blues",":","That","'s","All","Right","Mama",",","Johnny","B.","Goode",",","Blue","Suede","Shoes",",","Whole","Lotta","Shakin","'","Goin","On",",","Shake",",","Rattle",",","and","Roll",",","and","Long","Tall","Sally","."],"labels":["O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","B-music genre","O","B-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","O","B-song","I-song","I-song","O","B-song","I-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","I-song","I-song","O","O","B-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["song","country","organization","event","album","band","music_genre","musical_instrument","person","musical_artist","location","award"]}
{"id":"179","dataset":"crossner_music","split":"test","instance":{"id":"179","prompt_labels":"Particularly(O) noted(O) in(O) the(O) UK(B-country) are(O) the(O) Music(B-location) of(I-location) Newport(I-location) ,(O) once(O) labelled(O) '(O) the(O) new(B-location) Seattle(I-location) '(O) ,(O) and(O) the(O) Music(B-location) of(I-location) Cardiff(I-location) ,(O) for(O) which(O) the(O) city(O) has(O) recently(O) been(O) labelled(O) '(O) Music(B-location) City(I-location) '(O) ,(O) for(O) having(O) the(O) second(O) highest(O) number(O) of(O) independent(O) music(O) venues(O) in(O) the(O) UK(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, event, song, music genre, country, musical artist, organization, award, person, location, band, album and O.\nSentence: Particularly noted in the UK are the Music of Newport , once labelled ' the new Seattle ' , and the Music of Cardiff , for which the city has recently been labelled ' Music City ' , for having the second highest number of independent music venues in the UK .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Particularly","noted","in","the","UK","are","the","Music","of","Newport",",","once","labelled","'","the","new","Seattle","'",",","and","the","Music","of","Cardiff",",","for","which","the","city","has","recently","been","labelled","'","Music","City","'",",","for","having","the","second","highest","number","of","independent","music","venues","in","the","UK","."],"labels":["O","O","O","O","B-country","O","O","B-location","I-location","I-location","O","O","O","O","O","B-location","I-location","O","O","O","O","B-location","I-location","I-location","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","event","song","music_genre","country","musical_artist","organization","award","person","location","band","album"]}
{"id":"181","dataset":"crossner_music","split":"test","instance":{"id":"181","prompt_labels":"In(O) 2001(O) ,(O) having(O) previously(O) won(O) an(O) Emmy(B-award) Award(I-award) ,(O) a(O) Grammy(B-award) Award(I-award) and(O) an(O) Academy(B-award) Awards(I-award) ,(O) he(O) joined(O) a(O) small(O) list(O) of(O) EGOT(B-award) winners(O) with(O) his(O) Tony(B-award) Award(I-award) wins(O) for(O) The(O) Producers(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, musical instrument, album, organization, award, band, country, location, music genre, song, event, musical artist and O.\nSentence: In 2001 , having previously won an Emmy Award , a Grammy Award and an Academy Awards , he joined a small list of EGOT winners with his Tony Award wins for The Producers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2001",",","having","previously","won","an","Emmy","Award",",","a","Grammy","Award","and","an","Academy","Awards",",","he","joined","a","small","list","of","EGOT","winners","with","his","Tony","Award","wins","for","The","Producers","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","O","O","B-award","I-award","O","O","O","O","O","O","O","B-award","O","O","O","B-award","I-award","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","musical_instrument","album","organization","award","band","country","location","music_genre","song","event","musical_artist"]}
{"id":"187","dataset":"crossner_music","split":"test","instance":{"id":"187","prompt_labels":"Four(O) demos(O) for(O) the(O) album(O) were(O) recorded(O) on(O) August(O) 13(O) ,(O) 1990(O) ;(O) Enter(B-song) Sandman(I-song) ,(O) The(B-song) Unforgiven(I-song) ,(O) Nothing(B-song) Else(I-song) Matters(I-song) and(O) Wherever(B-song) I(I-song) May(I-song) Roam(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical instrument, person, music genre, band, country, event, location, album, musical artist, song, organization and O.\nSentence: Four demos for the album were recorded on August 13 , 1990 ; Enter Sandman , The Unforgiven , Nothing Else Matters and Wherever I May Roam .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Four","demos","for","the","album","were","recorded","on","August","13",",","1990",";","Enter","Sandman",",","The","Unforgiven",",","Nothing","Else","Matters","and","Wherever","I","May","Roam","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-song","I-song","O","B-song","I-song","O","B-song","I-song","I-song","O","B-song","I-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["award","musical_instrument","person","music_genre","band","country","event","location","album","musical_artist","song","organization"]}
{"id":"188","dataset":"crossner_music","split":"test","instance":{"id":"188","prompt_labels":"In(O) 2006(O) ,(O) along(O) with(O) Sonu(B-musical artist) Nigam(I-musical artist) ,(O) Sunidhi(B-musical artist) Chauhan(I-musical artist) and(O) Shiamak(B-musical artist) Davar(I-musical artist) ,(O) Ghoshal(B-musical artist) performed(O) the(O) theme(O) song(O) of(O) 2010(B-event) Commonwealth(I-event) Games(I-event) at(O) its(O) closing(O) ceremony(O) ,(O) as(O) an(O) invitation(O) to(O) everyone(O) to(O) the(O) following(O) Commonwealth(B-event) Games(I-event) in(O) Delhi(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, album, location, award, event, musical instrument, country, organization, music genre, person, band, song and O.\nSentence: In 2006 , along with Sonu Nigam , Sunidhi Chauhan and Shiamak Davar , Ghoshal performed the theme song of 2010 Commonwealth Games at its closing ceremony , as an invitation to everyone to the following Commonwealth Games in Delhi .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2006",",","along","with","Sonu","Nigam",",","Sunidhi","Chauhan","and","Shiamak","Davar",",","Ghoshal","performed","the","theme","song","of","2010","Commonwealth","Games","at","its","closing","ceremony",",","as","an","invitation","to","everyone","to","the","following","Commonwealth","Games","in","Delhi","."],"labels":["O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","album","location","award","event","musical_instrument","country","organization","music_genre","person","band","song"]}
{"id":"191","dataset":"crossner_music","split":"test","instance":{"id":"191","prompt_labels":"He(O) is(O) also(O) a(O) recipient(O) of(O) the(O) AFI(B-award) Life(I-award) Achievement(I-award) Award(I-award) for(O) his(O) contributions(O) to(O) the(O) cinema(O) ,(O) and(O) has(O) won(O) an(O) Academy(B-award) Awards(I-award) ,(O) a(O) Palme(B-award) d(I-award) 'Or(I-award) ,(O) Cannes(B-award) Film(I-award) Festival(I-award) Best(I-award) Director(I-award) Award(I-award) ,(O) Silver(B-award) Lion(I-award) ,(O) Grammy(B-award) Award(I-award) ,(O) Emmy(B-award) Award(I-award) ,(O) Golden(B-award) Globes(I-award) ,(O) British(B-award) Academy(I-award) Film(I-award) Awards(I-award) ,(O) and(O) Directors(B-award) Guild(I-award) of(I-award) America(I-award) Award(I-award) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, award, album, event, organization, musical instrument, song, music genre, musical artist, band, country, person and O.\nSentence: He is also a recipient of the AFI Life Achievement Award for his contributions to the cinema , and has won an Academy Awards , a Palme d 'Or , Cannes Film Festival Best Director Award , Silver Lion , Grammy Award , Emmy Award , Golden Globes , British Academy Film Awards , and Directors Guild of America Award s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","is","also","a","recipient","of","the","AFI","Life","Achievement","Award","for","his","contributions","to","the","cinema",",","and","has","won","an","Academy","Awards",",","a","Palme","d","'Or",",","Cannes","Film","Festival","Best","Director","Award",",","Silver","Lion",",","Grammy","Award",",","Emmy","Award",",","Golden","Globes",",","British","Academy","Film","Awards",",","and","Directors","Guild","of","America","Award","s","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","O","B-award","I-award","O","B-award","I-award","O","B-award","I-award","O","B-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","O","O"],"target_index":null,"target_label":null},"label_list":["location","award","album","event","organization","musical_instrument","song","music_genre","musical_artist","band","country","person"]}
{"id":"192","dataset":"crossner_music","split":"test","instance":{"id":"192","prompt_labels":"With(O) the(O) advent(O) of(O) the(O) radio(O) ,(O) bugle(O) signaling(O) units(O) became(O) obsolete(O) and(O) surplus(O) equipment(O) was(O) sold(O) to(O) veteran(O) organizations(O) ((O) such(O) as(O) the(O) Veterans(B-organization) of(I-organization) Foreign(I-organization) Wars(I-organization) and(O) American(B-organization) Legion(I-organization) ,(O) two(O) major(O) organizers(O) for(O) classic(O) drum(O) corps(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, award, music genre, country, musical instrument, location, album, musical artist, event, organization, band, song and O.\nSentence: With the advent of the radio , bugle signaling units became obsolete and surplus equipment was sold to veteran organizations ( such as the Veterans of Foreign Wars and American Legion , two major organizers for classic drum corps ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["With","the","advent","of","the","radio",",","bugle","signaling","units","became","obsolete","and","surplus","equipment","was","sold","to","veteran","organizations","(","such","as","the","Veterans","of","Foreign","Wars","and","American","Legion",",","two","major","organizers","for","classic","drum","corps",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","award","music_genre","country","musical_instrument","location","album","musical_artist","event","organization","band","song"]}
{"id":"194","dataset":"crossner_music","split":"test","instance":{"id":"194","prompt_labels":"Bubbles(B-person) created(O) sleeves(O) for(O) bands(O) including(O) the(B-band) Damned(I-band) ,(O) Elvis(B-musical artist) Costello(I-musical artist) ,(O) Ian(B-musical artist) Dury(I-musical artist) and(O) Wreckless(B-musical artist) Eric(I-musical artist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical instrument, music genre, location, musical artist, album, song, band, country, award, person, organization and O.\nSentence: Bubbles created sleeves for bands including the Damned , Elvis Costello , Ian Dury and Wreckless Eric .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Bubbles","created","sleeves","for","bands","including","the","Damned",",","Elvis","Costello",",","Ian","Dury","and","Wreckless","Eric","."],"labels":["B-person","O","O","O","O","O","B-band","I-band","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O"],"target_index":null,"target_label":null},"label_list":["event","musical_instrument","music_genre","location","musical_artist","album","song","band","country","award","person","organization"]}
{"id":"195","dataset":"crossner_music","split":"test","instance":{"id":"195","prompt_labels":"Notable(O) post-punk(B-music genre) groups(O) that(O) presaged(O) that(O) genre(O) and(O) helped(O) develop(O) and(O) shape(O) the(O) subculture(O) ,(O) include(O) Siouxsie(B-band) and(I-band) the(I-band) Banshees(I-band) ,(O) Joy(B-band) Division(I-band) ,(O) Bauhaus(B-band) and(O) The(B-band) Cure(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, organization, location, award, album, event, person, band, musical instrument, musical artist, country, song and O.\nSentence: Notable post-punk groups that presaged that genre and helped develop and shape the subculture , include Siouxsie and the Banshees , Joy Division , Bauhaus and The Cure .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Notable","post-punk","groups","that","presaged","that","genre","and","helped","develop","and","shape","the","subculture",",","include","Siouxsie","and","the","Banshees",",","Joy","Division",",","Bauhaus","and","The","Cure","."],"labels":["O","B-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","I-band","I-band","O","B-band","I-band","O","B-band","O","B-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["music_genre","organization","location","award","album","event","person","band","musical_instrument","musical_artist","country","song"]}
{"id":"196","dataset":"crossner_music","split":"test","instance":{"id":"196","prompt_labels":"Jagger(B-musical artist) ,(O) in(O) the(O) role(O) of(O) Turner(B-person) in(O) the(O) 1970(O) film(O) Performance(O) ,(O) performed(O) excerpts(O) from(O) Come(B-song) On(I-song) in(I-song) My(I-song) Kitchen(I-song) and(O) Me(B-song) and(I-song) the(I-song) Devil(I-song) Blues(I-song) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, organization, location, album, musical instrument, person, song, musical artist, event, country, music genre, award and O.\nSentence: Jagger , in the role of Turner in the 1970 film Performance , performed excerpts from Come On in My Kitchen and Me and the Devil Blues .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Jagger",",","in","the","role","of","Turner","in","the","1970","film","Performance",",","performed","excerpts","from","Come","On","in","My","Kitchen","and","Me","and","the","Devil","Blues","."],"labels":["B-musical artist","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","B-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","I-song","I-song","O"],"target_index":null,"target_label":null},"label_list":["band","organization","location","album","musical_instrument","person","song","musical_artist","event","country","music_genre","award"]}
{"id":"200","dataset":"crossner_music","split":"test","instance":{"id":"200","prompt_labels":"Additionally(O) ,(O) Grant(B-person) won(O) the(O) Golden(B-award) Globe(I-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) -(I-award) Motion(I-award) Picture(I-award) Musical(I-award) or(I-award) Comedy(I-award) and(O) the(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Actor(I-award) in(I-award) a(I-award) Leading(I-award) Role(I-award) ,(O) and(O) the(O) film(O) won(O) the(O) British(B-award) Academy(I-award) Film(I-award) Awards(I-award) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Film(I-award) ,(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Direction(I-award) ,(O) and(O) BAFTA(B-award) Award(I-award) for(I-award) Best(I-award) Actress(I-award) in(I-award) a(I-award) Supporting(I-award) Role(I-award) for(O) Scott(B-person) Thomas(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, event, band, person, musical artist, country, musical instrument, song, organization, album, award, location and O.\nSentence: Additionally , Grant won the Golden Globe Award for Best Actor - Motion Picture Musical or Comedy and the BAFTA Award for Best Actor in a Leading Role , and the film won the British Academy Film Awards BAFTA Award for Best Film , BAFTA Award for Best Direction , and BAFTA Award for Best Actress in a Supporting Role for Scott Thomas .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Additionally",",","Grant","won","the","Golden","Globe","Award","for","Best","Actor","-","Motion","Picture","Musical","or","Comedy","and","the","BAFTA","Award","for","Best","Actor","in","a","Leading","Role",",","and","the","film","won","the","British","Academy","Film","Awards","BAFTA","Award","for","Best","Film",",","BAFTA","Award","for","Best","Direction",",","and","BAFTA","Award","for","Best","Actress","in","a","Supporting","Role","for","Scott","Thomas","."],"labels":["O","O","B-person","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","B-award","I-award","I-award","I-award","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","I-award","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["music_genre","event","band","person","musical_artist","country","musical_instrument","song","organization","album","award","location"]}
{"id":"206","dataset":"crossner_music","split":"test","instance":{"id":"206","prompt_labels":"Douglas(B-musical artist) also(O) began(O) to(O) appear(O) at(O) the(O) mainstream(O) jazz(B-music genre) clubs(O) around(O) New(B-location) York(I-location) such(O) as(O) Iridium(B-location) Jazz(I-location) Club(I-location) ,(O) Village(B-location) Vanguard(I-location) and(O) Jazz(B-location) Standard(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, song, musical artist, musical instrument, album, event, award, person, band, organization, music genre, country and O.\nSentence: Douglas also began to appear at the mainstream jazz clubs around New York such as Iridium Jazz Club , Village Vanguard and Jazz Standard .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Douglas","also","began","to","appear","at","the","mainstream","jazz","clubs","around","New","York","such","as","Iridium","Jazz","Club",",","Village","Vanguard","and","Jazz","Standard","."],"labels":["B-musical artist","O","O","O","O","O","O","O","B-music genre","O","O","B-location","I-location","O","O","B-location","I-location","I-location","O","B-location","I-location","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["location","song","musical_artist","musical_instrument","album","event","award","person","band","organization","music_genre","country"]}
{"id":"208","dataset":"crossner_music","split":"test","instance":{"id":"208","prompt_labels":"He(O) has(O) also(O) released(O) three(O) solo(O) albums(O) :(O) Stranger(B-album) in(I-album) This(I-album) Town(I-album) in(O) 1991(O) ,(O) Undiscovered(B-album) Soul(I-album) in(O) 1998(O) ,(O) and(O) Aftermath(B-album) of(I-album) the(I-album) Lowdown(I-album) released(O) in(O) September(O) 2012(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, album, event, band, music genre, award, person, organization, musical artist, musical instrument, song and O.\nSentence: He has also released three solo albums : Stranger in This Town in 1991 , Undiscovered Soul in 1998 , and Aftermath of the Lowdown released in September 2012 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","also","released","three","solo","albums",":","Stranger","in","This","Town","in","1991",",","Undiscovered","Soul","in","1998",",","and","Aftermath","of","the","Lowdown","released","in","September","2012","."],"labels":["O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","B-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","country","album","event","band","music_genre","award","person","organization","musical_artist","musical_instrument","song"]}
{"id":"210","dataset":"crossner_music","split":"test","instance":{"id":"210","prompt_labels":"Follow-up(O) albums(O) You(B-album) 've(I-album) Come(I-album) a(I-album) Long(I-album) Way(I-album) ,(I-album) Baby(I-album) ,(O) Halfway(B-album) Between(I-album) the(I-album) Gutter(I-album) and(I-album) the(I-album) Stars(I-album) ,(O) and(O) Palookaville(B-album) ,(O) as(O) well(O) as(O) singles(O) such(O) as(O) The(B-song) Rockafeller(I-song) Skank(I-song) ,(O) Praise(B-song) You(I-song) ,(O) Right(B-song) Here(I-song) ,(I-song) Right(I-song) Now(I-song) ,(O) Weapon(B-song) of(I-song) Choice(I-song) ,(O) and(O) Wonderful(B-song) Night(I-song) ,(O) achieved(O) commercial(O) and(O) critical(O) success(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, band, location, music genre, album, organization, country, person, award, musical instrument, musical artist, song and O.\nSentence: Follow-up albums You 've Come a Long Way , Baby , Halfway Between the Gutter and the Stars , and Palookaville , as well as singles such as The Rockafeller Skank , Praise You , Right Here , Right Now , Weapon of Choice , and Wonderful Night , achieved commercial and critical success .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Follow-up","albums","You","'ve","Come","a","Long","Way",",","Baby",",","Halfway","Between","the","Gutter","and","the","Stars",",","and","Palookaville",",","as","well","as","singles","such","as","The","Rockafeller","Skank",",","Praise","You",",","Right","Here",",","Right","Now",",","Weapon","of","Choice",",","and","Wonderful","Night",",","achieved","commercial","and","critical","success","."],"labels":["O","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","I-album","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","O","O","B-album","O","O","O","O","O","O","O","B-song","I-song","I-song","O","B-song","I-song","O","B-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","O","O","B-song","I-song","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","band","location","music_genre","album","organization","country","person","award","musical_instrument","musical_artist","song"]}
{"id":"211","dataset":"crossner_music","split":"test","instance":{"id":"211","prompt_labels":"In(O) 2002(O) ,(O) Grohl(B-musical artist) helped(O) Chan(B-musical artist) Marshall(I-musical artist) of(O) Cat(B-musical artist) Power(I-musical artist) on(O) the(O) album(O) You(B-album) Are(I-album) Free(I-album) and(O) played(O) with(O) Queens(B-band) of(I-band) the(I-band) Stone(I-band) Age(I-band) on(O) their(O) album(O) Songs(B-album) for(I-album) the(I-album) Deaf(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, music genre, band, song, album, person, award, country, musical artist, musical instrument, location and O.\nSentence: In 2002 , Grohl helped Chan Marshall of Cat Power on the album You Are Free and played with Queens of the Stone Age on their album Songs for the Deaf .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2002",",","Grohl","helped","Chan","Marshall","of","Cat","Power","on","the","album","You","Are","Free","and","played","with","Queens","of","the","Stone","Age","on","their","album","Songs","for","the","Deaf","."],"labels":["O","O","O","B-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","O","B-album","I-album","I-album","O","O","O","B-band","I-band","I-band","I-band","I-band","O","O","O","B-album","I-album","I-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["organization","event","music_genre","band","song","album","person","award","country","musical_artist","musical_instrument","location"]}
{"id":"214","dataset":"crossner_music","split":"test","instance":{"id":"214","prompt_labels":"It(O) received(O) a(O) major(O) expansion(O) ahead(O) of(O) the(O) 1983(B-event) Summer(I-event) Universiade(I-event) ,(O) when(O) it(O) reached(O) a(O) capacity(O) of(O) 60,081(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, song, award, event, musical artist, album, country, musical instrument, music genre, band, location, person and O.\nSentence: It received a major expansion ahead of the 1983 Summer Universiade , when it reached a capacity of 60,081 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","received","a","major","expansion","ahead","of","the","1983","Summer","Universiade",",","when","it","reached","a","capacity","of","60,081","."],"labels":["O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","song","award","event","musical_artist","album","country","musical_instrument","music_genre","band","location","person"]}
{"id":"216","dataset":"crossner_music","split":"test","instance":{"id":"216","prompt_labels":"Reggae(B-music genre) en(I-music genre) Español(I-music genre) spread(O) from(O) the(O) Spanish-speaking(O) Central(B-location) American(I-location) country(O) of(O) Panama(B-country) to(O) the(O) mainland(O) South(B-location) American(I-location) countries(O) of(O) Venezuela(B-country) and(O) Guyana(B-country) then(O) to(O) the(O) rest(O) of(O) South(B-location) America(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical instrument, music genre, location, person, award, organization, album, band, song, musical artist, country and O.\nSentence: Reggae en Español spread from the Spanish-speaking Central American country of Panama to the mainland South American countries of Venezuela and Guyana then to the rest of South America .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Reggae","en","Español","spread","from","the","Spanish-speaking","Central","American","country","of","Panama","to","the","mainland","South","American","countries","of","Venezuela","and","Guyana","then","to","the","rest","of","South","America","."],"labels":["B-music genre","I-music genre","I-music genre","O","O","O","O","B-location","I-location","O","O","B-country","O","O","O","B-location","I-location","O","O","B-country","O","B-country","O","O","O","O","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["event","musical_instrument","music_genre","location","person","award","organization","album","band","song","musical_artist","country"]}
{"id":"218","dataset":"crossner_music","split":"test","instance":{"id":"218","prompt_labels":"In(O) 2003(O) ,(O) Afshin-Jam(B-person) became(O) Miss(B-organization) World(I-organization) Canada(I-organization) and(O) joined(O) in(O) the(O) Miss(B-organization) World(I-organization) contest(O) in(O) Sanya(B-location) ,(O) China(B-country) ,(O) where(O) she(O) placed(O) second(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical instrument, award, music genre, musical artist, event, organization, person, band, location, song, country and O.\nSentence: In 2003 , Afshin-Jam became Miss World Canada and joined in the Miss World contest in Sanya , China , where she placed second .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2003",",","Afshin-Jam","became","Miss","World","Canada","and","joined","in","the","Miss","World","contest","in","Sanya",",","China",",","where","she","placed","second","."],"labels":["O","O","O","B-person","O","B-organization","I-organization","I-organization","O","O","O","O","B-organization","I-organization","O","O","B-location","O","B-country","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["album","musical_instrument","award","music_genre","musical_artist","event","organization","person","band","location","song","country"]}
{"id":"223","dataset":"crossner_music","split":"test","instance":{"id":"223","prompt_labels":"The(O) album(O) included(O) three(O) songs(O) written(O) or(O) co-written(O) by(O) Dylan(B-musical artist) ((O) This(B-song) Wheel(I-song) 's(I-song) on(I-song) Fire(I-song) ,(O) Tears(B-song) of(I-song) Rage(I-song) ,(O) and(O) I(B-song) Shall(I-song) Be(I-song) Released(I-song) )(O) as(O) well(O) as(O) The(B-song) Weight(I-song) ,(O) the(O) use(O) of(O) which(O) in(O) the(O) film(O) Easy(O) Rider(O) would(O) make(O) it(O) one(O) of(O) their(O) best-known(O) songs(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, band, music genre, album, musical instrument, musical artist, award, location, person, song, event, country and O.\nSentence: The album included three songs written or co-written by Dylan ( This Wheel 's on Fire , Tears of Rage , and I Shall Be Released ) as well as The Weight , the use of which in the film Easy Rider would make it one of their best-known songs .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","album","included","three","songs","written","or","co-written","by","Dylan","(","This","Wheel","'s","on","Fire",",","Tears","of","Rage",",","and","I","Shall","Be","Released",")","as","well","as","The","Weight",",","the","use","of","which","in","the","film","Easy","Rider","would","make","it","one","of","their","best-known","songs","."],"labels":["O","O","O","O","O","O","O","O","O","B-musical artist","O","B-song","I-song","I-song","I-song","I-song","O","B-song","I-song","I-song","O","O","B-song","I-song","I-song","I-song","O","O","O","O","B-song","I-song","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","band","music_genre","album","musical_instrument","musical_artist","award","location","person","song","event","country"]}
{"id":"235","dataset":"crossner_music","split":"test","instance":{"id":"235","prompt_labels":"It(O) includes(O) folklore(B-music genre) music(I-music genre) of(O) parts(O) of(O) Bolivia(B-country) ,(O) Ecuador(B-country) ,(O) Chile(B-country) ,(O) Colombia(B-country) ,(O) Peru(B-country) and(O) Venezuela(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, person, album, country, music genre, organization, band, musical instrument, song, event, award and O.\nSentence: It includes folklore music of parts of Bolivia , Ecuador , Chile , Colombia , Peru and Venezuela .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","includes","folklore","music","of","parts","of","Bolivia",",","Ecuador",",","Chile",",","Colombia",",","Peru","and","Venezuela","."],"labels":["O","O","B-music genre","I-music genre","O","O","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","location","person","album","country","music_genre","organization","band","musical_instrument","song","event","award"]}
{"id":"241","dataset":"crossner_music","split":"test","instance":{"id":"241","prompt_labels":"The(O) focus(O) of(O) their(O) activities(O) in(O) Europe(B-location) became(O) annual(O) concert(O) cycles(O) at(O) the(O) Wiener(B-location) Konzerthaus(I-location) ,(O) at(O) London(B-location) 's(O) Queen(B-location) Elizabeth(I-location) Hall(I-location) ,(O) the(O) Frankfurt(B-location) Alte(I-location) Oper(I-location) ,(O) the(O) Théâtre(B-location) des(I-location) Champs(I-location) Elysées(I-location) in(O) Paris(B-location) ,(O) the(O) Philharmonic(B-location) Hall(I-location) in(O) Cologne(B-location) ,(O) the(O) Zurich(B-location) Opera(I-location) ,(O) as(O) well(O) as(O) regular(O) concerts(O) at(O) most(O) major(O) halls(O) and(O) venues(O) around(O) the(O) world(O) ((O) among(O) them(O) La(B-location) Scala(I-location) ,(O) Concertgebouw(B-location) ,(O) Berliner(B-location) Philharmonie(I-location) ,(O) Carnegie(B-location) Hall(I-location) ,(O) Teatro(B-location) Colón(I-location) ,(O) Suntory(B-location) Hall(I-location) ,(O) etc(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, location, musical artist, organization, person, song, music genre, country, award, album, band, event and O.\nSentence: The focus of their activities in Europe became annual concert cycles at the Wiener Konzerthaus , at London 's Queen Elizabeth Hall , the Frankfurt Alte Oper , the Théâtre des Champs Elysées in Paris , the Philharmonic Hall in Cologne , the Zurich Opera , as well as regular concerts at most major halls and venues around the world ( among them La Scala , Concertgebouw , Berliner Philharmonie , Carnegie Hall , Teatro Colón , Suntory Hall , etc .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","focus","of","their","activities","in","Europe","became","annual","concert","cycles","at","the","Wiener","Konzerthaus",",","at","London","'s","Queen","Elizabeth","Hall",",","the","Frankfurt","Alte","Oper",",","the","Théâtre","des","Champs","Elysées","in","Paris",",","the","Philharmonic","Hall","in","Cologne",",","the","Zurich","Opera",",","as","well","as","regular","concerts","at","most","major","halls","and","venues","around","the","world","(","among","them","La","Scala",",","Concertgebouw",",","Berliner","Philharmonie",",","Carnegie","Hall",",","Teatro","Colón",",","Suntory","Hall",",","etc","."],"labels":["O","O","O","O","O","O","B-location","O","O","O","O","O","O","B-location","I-location","O","O","B-location","O","B-location","I-location","I-location","O","O","B-location","I-location","I-location","O","O","B-location","I-location","I-location","I-location","O","B-location","O","O","B-location","I-location","O","B-location","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-location","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","O","B-location","I-location","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","location","musical_artist","organization","person","song","music_genre","country","award","album","band","event"]}
{"id":"243","dataset":"crossner_music","split":"test","instance":{"id":"243","prompt_labels":"Country(B-music genre) music(I-music genre) ,(O) also(O) known(O) as(O) country(B-music genre) and(O) western(B-music genre) ((O) or(O) simply(O) country(B-music genre) )(O) ,(O) and(O) hillbilly(B-music genre) music(I-music genre) ,(O) is(O) a(O) genre(O) of(O) popular(B-music genre) music(I-music genre) that(O) takes(O) its(O) roots(O) from(O) genres(O) such(O) as(O) blues(B-music genre) and(O) old-time(B-music genre) music(I-music genre) ,(O) and(O) various(O) types(O) of(O) American(B-music genre) folk(I-music genre) music(I-music genre) including(O) Appalachian(O) music(O) ,(O) Cajun(O) music(O) ,(O) and(O) the(O) cowboy(O) Western(O) music(O) styles(O) of(O) Red(B-music genre) Dirt(I-music genre) ,(O) New(B-music genre) Mexico(I-music genre) music(I-music genre) ,(O) Texas(B-music genre) country(I-music genre) music(I-music genre) ,(O) and(O) Tejano(B-music genre) music(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, song, album, music genre, person, award, country, band, musical instrument, organization, location, event and O.\nSentence: Country music , also known as country and western ( or simply country ) , and hillbilly music , is a genre of popular music that takes its roots from genres such as blues and old-time music , and various types of American folk music including Appalachian music , Cajun music , and the cowboy Western music styles of Red Dirt , New Mexico music , Texas country music , and Tejano music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Country","music",",","also","known","as","country","and","western","(","or","simply","country",")",",","and","hillbilly","music",",","is","a","genre","of","popular","music","that","takes","its","roots","from","genres","such","as","blues","and","old-time","music",",","and","various","types","of","American","folk","music","including","Appalachian","music",",","Cajun","music",",","and","the","cowboy","Western","music","styles","of","Red","Dirt",",","New","Mexico","music",",","Texas","country","music",",","and","Tejano","music","."],"labels":["B-music genre","I-music genre","O","O","O","O","B-music genre","O","B-music genre","O","O","O","B-music genre","O","O","O","B-music genre","I-music genre","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","B-music genre","O","B-music genre","I-music genre","O","O","O","O","O","B-music genre","I-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","O","B-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","song","album","music_genre","person","award","country","band","musical_instrument","organization","location","event"]}
{"id":"246","dataset":"crossner_music","split":"test","instance":{"id":"246","prompt_labels":"Love(B-musical artist) has(O) been(O) candid(O) about(O) her(O) diverse(O) musical(O) influences(O) ,(O) the(O) earliest(O) being(O) Patti(B-musical artist) Smith(I-musical artist) ,(O) The(B-band) Runaways(I-band) ,(O) and(O) The(B-band) Pretenders(I-band) ,(O) artists(O) she(O) discovered(O) while(O) in(O) juvenile(O) hall(O) at(O) age(O) fifteen(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, song, person, album, music genre, band, event, country, organization, location, musical artist, award and O.\nSentence: Love has been candid about her diverse musical influences , the earliest being Patti Smith , The Runaways , and The Pretenders , artists she discovered while in juvenile hall at age fifteen .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Love","has","been","candid","about","her","diverse","musical","influences",",","the","earliest","being","Patti","Smith",",","The","Runaways",",","and","The","Pretenders",",","artists","she","discovered","while","in","juvenile","hall","at","age","fifteen","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-band","I-band","O","O","B-band","I-band","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","song","person","album","music_genre","band","event","country","organization","location","musical_artist","award"]}
{"id":"248","dataset":"crossner_music","split":"test","instance":{"id":"248","prompt_labels":"Every(O) style(O) of(O) music(O) was(O) offered(O) ,(O) from(O) Rock(B-music genre) music(I-music genre) and(O) Pop(B-music genre) music(I-music genre) to(O) Spanish-language(O) programming(O) ((O) for(O) Mexican(O) restaurants(O) )(O) ,(O) jazz(B-music genre) ,(O) blues(B-music genre) ,(O) classical(B-music genre) and(O) even(O) easy(B-music genre) listening(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, music genre, award, album, country, musical artist, location, organization, musical instrument, song, person, band and O.\nSentence: Every style of music was offered , from Rock music and Pop music to Spanish-language programming ( for Mexican restaurants ) , jazz , blues , classical and even easy listening .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Every","style","of","music","was","offered",",","from","Rock","music","and","Pop","music","to","Spanish-language","programming","(","for","Mexican","restaurants",")",",","jazz",",","blues",",","classical","and","even","easy","listening","."],"labels":["O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","B-music genre","O","B-music genre","O","B-music genre","O","O","B-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["event","music_genre","award","album","country","musical_artist","location","organization","musical_instrument","song","person","band"]}
{"id":"254","dataset":"crossner_music","split":"test","instance":{"id":"254","prompt_labels":"The(O) movie(O) also(O) featured(O) a(O) soundtrack(O) of(O) popular(O) songs(O) ,(O) including(O) a(O) cover(O) version(O) of(O) The(B-band) Troggs(I-band) '(O) Love(B-song) Is(I-song) All(I-song) Around(I-song) performed(O) by(O) Wet(B-band) Wet(I-band) Wet(I-band) that(O) remained(O) at(O) number(O) 1(O) on(O) the(O) UK(O) Singles(O) Chart(O) for(O) fifteen(O) weeks(O) and(O) was(O) then(O) the(O) ninth(O) ((O) now(O) twelfth(O) )(O) biggest(O) selling(O) single(O) of(O) all(O) time(O) in(O) Britain(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, award, country, event, album, organization, musical artist, song, music genre, band, musical instrument and O.\nSentence: The movie also featured a soundtrack of popular songs , including a cover version of The Troggs ' Love Is All Around performed by Wet Wet Wet that remained at number 1 on the UK Singles Chart for fifteen weeks and was then the ninth ( now twelfth ) biggest selling single of all time in Britain .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","movie","also","featured","a","soundtrack","of","popular","songs",",","including","a","cover","version","of","The","Troggs","'","Love","Is","All","Around","performed","by","Wet","Wet","Wet","that","remained","at","number","1","on","the","UK","Singles","Chart","for","fifteen","weeks","and","was","then","the","ninth","(","now","twelfth",")","biggest","selling","single","of","all","time","in","Britain","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","O","B-song","I-song","I-song","I-song","O","O","B-band","I-band","I-band","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["person","location","award","country","event","album","organization","musical_artist","song","music_genre","band","musical_instrument"]}
{"id":"256","dataset":"crossner_music","split":"test","instance":{"id":"256","prompt_labels":"During(O) the(O) course(O) of(O) her(O) career(O) ,(O) Saariaho(B-musical artist) has(O) received(O) commissions(O) from(O) the(O) Lincoln(B-organization) Center(I-organization) for(O) the(O) Kronos(B-band) Quartet(I-band) and(O) from(O) IRCAM(B-organization) for(O) the(O) Ensemble(B-band) Intercontemporain(I-band) ,(O) the(O) BBC(B-organization) ,(O) the(O) New(B-band) York(I-band) Philharmonic(I-band) ,(O) the(O) Salzburg(B-event) Music(I-event) Festival(I-event) ,(O) the(O) Théâtre(B-location) du(I-location) Châtelet(I-location) in(O) Paris(B-location) ,(O) and(O) the(O) Finnish(B-location) National(I-location) Opera(I-location) ,(O) among(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, band, event, person, song, musical artist, album, award, musical instrument, music genre, country and O.\nSentence: During the course of her career , Saariaho has received commissions from the Lincoln Center for the Kronos Quartet and from IRCAM for the Ensemble Intercontemporain , the BBC , the New York Philharmonic , the Salzburg Music Festival , the Théâtre du Châtelet in Paris , and the Finnish National Opera , among others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["During","the","course","of","her","career",",","Saariaho","has","received","commissions","from","the","Lincoln","Center","for","the","Kronos","Quartet","and","from","IRCAM","for","the","Ensemble","Intercontemporain",",","the","BBC",",","the","New","York","Philharmonic",",","the","Salzburg","Music","Festival",",","the","Théâtre","du","Châtelet","in","Paris",",","and","the","Finnish","National","Opera",",","among","others","."],"labels":["O","O","O","O","O","O","O","B-musical artist","O","O","O","O","O","B-organization","I-organization","O","O","B-band","I-band","O","O","B-organization","O","O","B-band","I-band","O","O","B-organization","O","O","B-band","I-band","I-band","O","O","B-event","I-event","I-event","O","O","B-location","I-location","I-location","O","B-location","O","O","O","B-location","I-location","I-location","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","location","band","event","person","song","musical_artist","album","award","musical_instrument","music_genre","country"]}
{"id":"263","dataset":"crossner_music","split":"test","instance":{"id":"263","prompt_labels":"The(O) Elephant(O) Man(O) was(O) a(O) huge(O) critical(O) and(O) commercial(O) success(O) ,(O) and(O) earned(O) eight(O) Academy(B-award) Awards(I-award) nominations(O) ,(O) including(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Director(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Adapted(I-award) Screenplay(I-award) for(O) Lynch(B-musical artist) personally(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, album, person, musical artist, award, song, location, country, music genre, band, musical instrument, event and O.\nSentence: The Elephant Man was a huge critical and commercial success , and earned eight Academy Awards nominations , including Academy Award for Best Director and Academy Award for Best Adapted Screenplay for Lynch personally .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Elephant","Man","was","a","huge","critical","and","commercial","success",",","and","earned","eight","Academy","Awards","nominations",",","including","Academy","Award","for","Best","Director","and","Academy","Award","for","Best","Adapted","Screenplay","for","Lynch","personally","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-musical artist","O","O"],"target_index":null,"target_label":null},"label_list":["organization","album","person","musical_artist","award","song","location","country","music_genre","band","musical_instrument","event"]}
{"id":"291","dataset":"crossner_music","split":"test","instance":{"id":"291","prompt_labels":"Highlights(O) in(O) 2002(O) included(O) the(O) premieres(O) of(O) several(O) major(O) works(O) ,(O) including(O) View(B-album) From(I-album) Olympus(I-album) a(O) double(O) concerto(O) for(O) piano(B-musical instrument) ,(O) percussion(O) and(O) orchestra(O) performed(O) by(O) Evelyn(B-musical artist) Glennie(I-musical artist) ,(O) Philip(B-musical artist) Smith(I-musical artist) and(O) the(O) Halle(B-band) Orchestra(I-band) conducted(O) by(O) Mark(B-person) Elder(I-person) at(O) the(O) Royal(B-event) Gala(I-event) finale(I-event) of(O) the(O) 2002(B-event) Commonwealth(I-event) Games(I-event) '(O) Pulse(B-event) '(I-event) music(I-event) festival(I-event) in(O) Manchester(B-location) ,(O) UK(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, location, song, country, album, musical instrument, event, band, person, musical artist, music genre and O.\nSentence: Highlights in 2002 included the premieres of several major works , including View From Olympus a double concerto for piano , percussion and orchestra performed by Evelyn Glennie , Philip Smith and the Halle Orchestra conducted by Mark Elder at the Royal Gala finale of the 2002 Commonwealth Games ' Pulse ' music festival in Manchester , UK .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Highlights","in","2002","included","the","premieres","of","several","major","works",",","including","View","From","Olympus","a","double","concerto","for","piano",",","percussion","and","orchestra","performed","by","Evelyn","Glennie",",","Philip","Smith","and","the","Halle","Orchestra","conducted","by","Mark","Elder","at","the","Royal","Gala","finale","of","the","2002","Commonwealth","Games","'","Pulse","'","music","festival","in","Manchester",",","UK","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O","B-musical instrument","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-band","I-band","O","O","B-person","I-person","O","O","B-event","I-event","I-event","O","O","B-event","I-event","I-event","O","B-event","I-event","I-event","I-event","O","B-location","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["organization","award","location","song","country","album","musical_instrument","event","band","person","musical_artist","music_genre"]}
{"id":"293","dataset":"crossner_music","split":"test","instance":{"id":"293","prompt_labels":"These(O) have(O) been(O) used(O) in(O) clay(O) shooting(O) and(O) were(O) suggested(O) for(O) use(O) in(O) the(O) Modern(B-event) pentathlon(I-event) at(I-event) the(I-event) 2012(I-event) Summer(I-event) Olympics(I-event) after(O) a(O) successful(O) trial(O) at(O) the(O) 2010(B-event) Summer(I-event) Youth(I-event) Olympics(I-event) in(O) Singapore(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, album, location, country, music genre, event, band, organization, musical instrument, person, award, musical artist and O.\nSentence: These have been used in clay shooting and were suggested for use in the Modern pentathlon at the 2012 Summer Olympics after a successful trial at the 2010 Summer Youth Olympics in Singapore .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["These","have","been","used","in","clay","shooting","and","were","suggested","for","use","in","the","Modern","pentathlon","at","the","2012","Summer","Olympics","after","a","successful","trial","at","the","2010","Summer","Youth","Olympics","in","Singapore","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["song","album","location","country","music_genre","event","band","organization","musical_instrument","person","award","musical_artist"]}
{"id":"294","dataset":"crossner_music","split":"test","instance":{"id":"294","prompt_labels":"On(O) September(O) 12(O) ,(O) 2010(O) ,(O) during(O) the(O) Chilean(B-event) bicentennial(I-event) festivities(I-event) ,(O) President(O) Sebastián(B-person) Piñera(I-person) announced(O) that(O) the(O) capacity(O) of(O) the(O) stadium(O) will(O) be(O) increased(O) so(O) as(O) to(O) reach(O) 70,000(O) seats(O) for(O) the(O) 2014(B-event) South(I-event) American(I-event) Games(I-event) that(O) will(O) take(O) place(O) in(O) Santiago(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, award, band, location, person, musical instrument, song, musical artist, event, music genre, organization, country and O.\nSentence: On September 12 , 2010 , during the Chilean bicentennial festivities , President Sebastián Piñera announced that the capacity of the stadium will be increased so as to reach 70,000 seats for the 2014 South American Games that will take place in Santiago .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","September","12",",","2010",",","during","the","Chilean","bicentennial","festivities",",","President","Sebastián","Piñera","announced","that","the","capacity","of","the","stadium","will","be","increased","so","as","to","reach","70,000","seats","for","the","2014","South","American","Games","that","will","take","place","in","Santiago","."],"labels":["O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O","O","O","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["album","award","band","location","person","musical_instrument","song","musical_artist","event","music_genre","organization","country"]}
{"id":"296","dataset":"crossner_music","split":"test","instance":{"id":"296","prompt_labels":"Deep(B-band) Purple(I-band) and(O) Whitesnake(B-band) '(O) s(O) David(B-musical artist) Coverdale(I-musical artist) ,(O) Samson(B-band) '(O) s(O) Nicky(B-musical artist) Moore(I-musical artist) and(O) Lone(B-band) Star(I-band) '(O) s(O) John(B-musical artist) Sloman(I-musical artist) were(O) all(O) considered(O) and(O) Iommi(B-musical artist) states(O) in(O) his(O) autobiography(O) that(O) Michael(B-musical artist) Bolton(I-musical artist) auditioned(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, event, country, organization, award, song, album, location, musical artist, musical instrument, person, music genre and O.\nSentence: Deep Purple and Whitesnake ' s David Coverdale , Samson ' s Nicky Moore and Lone Star ' s John Sloman were all considered and Iommi states in his autobiography that Michael Bolton auditioned .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Deep","Purple","and","Whitesnake","'","s","David","Coverdale",",","Samson","'","s","Nicky","Moore","and","Lone","Star","'","s","John","Sloman","were","all","considered","and","Iommi","states","in","his","autobiography","that","Michael","Bolton","auditioned","."],"labels":["B-band","I-band","O","B-band","O","O","B-musical artist","I-musical artist","O","B-band","O","O","B-musical artist","I-musical artist","O","B-band","I-band","O","O","B-musical artist","I-musical artist","O","O","O","O","B-musical artist","O","O","O","O","O","B-musical artist","I-musical artist","O","O"],"target_index":null,"target_label":null},"label_list":["band","event","country","organization","award","song","album","location","musical_artist","musical_instrument","person","music_genre"]}
{"id":"301","dataset":"crossner_music","split":"test","instance":{"id":"301","prompt_labels":"The(O) picture(O) received(O) Academy(B-award) Awards(I-award) nominations(O) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Sound(I-award) Mixing(I-award) ((O) Loren(B-musical artist) L.(I-musical artist) Ryder(I-musical artist) )(O) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Screenplay(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, event, location, music genre, musical instrument, song, band, person, album, award, country, organization and O.\nSentence: The picture received Academy Awards nominations for Academy Award for Best Sound Mixing ( Loren L. Ryder ) and Academy Award for Best Original Screenplay .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","picture","received","Academy","Awards","nominations","for","Academy","Award","for","Best","Sound","Mixing","(","Loren","L.","Ryder",")","and","Academy","Award","for","Best","Original","Screenplay","."],"labels":["O","O","O","B-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-musical artist","I-musical artist","I-musical artist","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","event","location","music_genre","musical_instrument","song","band","person","album","award","country","organization"]}
{"id":"302","dataset":"crossner_music","split":"test","instance":{"id":"302","prompt_labels":"By(O) the(O) mid-1980s(O) ,(O) bands(O) began(O) proliferating(O) and(O) became(O) increasingly(O) popular(O) ,(O) including(O) the(B-band) Sisters(I-band) of(I-band) Mercy(I-band) ,(O) the(B-band) Mission(I-band) ,(O) Alien(B-band) Sex(I-band) Fiend(I-band) ,(O) the(B-band) March(I-band) Violets(I-band) ,(O) Xmal(B-band) Deutschland(I-band) ,(O) the(B-band) Membranes(I-band) ,(O) and(O) Fields(B-band) of(I-band) the(I-band) Nephilim(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, musical instrument, event, country, music genre, album, person, song, musical artist, award, band, organization and O.\nSentence: By the mid-1980s , bands began proliferating and became increasingly popular , including the Sisters of Mercy , the Mission , Alien Sex Fiend , the March Violets , Xmal Deutschland , the Membranes , and Fields of the Nephilim .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["By","the","mid-1980s",",","bands","began","proliferating","and","became","increasingly","popular",",","including","the","Sisters","of","Mercy",",","the","Mission",",","Alien","Sex","Fiend",",","the","March","Violets",",","Xmal","Deutschland",",","the","Membranes",",","and","Fields","of","the","Nephilim","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","I-band","I-band","O","B-band","I-band","O","B-band","I-band","I-band","O","B-band","I-band","I-band","O","B-band","I-band","O","B-band","I-band","O","O","B-band","I-band","I-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["location","musical_instrument","event","country","music_genre","album","person","song","musical_artist","award","band","organization"]}
{"id":"327","dataset":"crossner_music","split":"test","instance":{"id":"327","prompt_labels":"The(O) World(B-organization) DanceSport(I-organization) Federation(I-organization) ((O) WDSF(B-organization) )(O) ,(O) formerly(O) the(O) International(B-organization) DanceSport(I-organization) Federation(I-organization) ((O) IDSF(B-organization) )(O) ,(O) is(O) the(O) international(O) governing(O) body(O) of(O) DanceSport(O) and(O) Wheelchair(O) DanceSport(O) ,(O) as(O) recognised(O) by(O) the(O) International(B-organization) Olympic(I-organization) Committee(I-organization) ((O) IOC(B-organization) )(O) and(O) the(O) International(B-organization) Paralympic(I-organization) Committee(I-organization) ((O) IPC(B-organization) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, award, band, album, musical instrument, music genre, organization, country, song, location, musical artist and O.\nSentence: The World DanceSport Federation ( WDSF ) , formerly the International DanceSport Federation ( IDSF ) , is the international governing body of DanceSport and Wheelchair DanceSport , as recognised by the International Olympic Committee ( IOC ) and the International Paralympic Committee ( IPC ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","World","DanceSport","Federation","(","WDSF",")",",","formerly","the","International","DanceSport","Federation","(","IDSF",")",",","is","the","international","governing","body","of","DanceSport","and","Wheelchair","DanceSport",",","as","recognised","by","the","International","Olympic","Committee","(","IOC",")","and","the","International","Paralympic","Committee","(","IPC",")","."],"labels":["O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O"],"target_index":null,"target_label":null},"label_list":["person","event","award","band","album","musical_instrument","music_genre","organization","country","song","location","musical_artist"]}
{"id":"329","dataset":"crossner_music","split":"test","instance":{"id":"329","prompt_labels":"In(O) the(O) 1990s(O) ,(O) artists(O) like(O) Me(B-musical artist) 'shell(I-musical artist) Ndegeocello(I-musical artist) ,(O) Brooklyn(B-band) Funk(I-band) Essentials(I-band) and(O) the(O) ((O) predominantly(O) UK-based(O) )(O) acid(B-music genre) jazz(I-music genre) movement(O) including(O) artists(O) and(O) bands(O) such(O) as(O) Jamiroquai(B-band) ,(O) Incognito(B-band) ,(O) Galliano(B-band) ,(O) Omar(B-musical artist) Lye-Fook(I-musical artist) ,(O) Los(B-band) Tetas(I-band) and(O) the(O) Brand(B-band) New(I-band) Heavies(I-band) carried(O) on(O) with(O) strong(O) elements(O) of(O) funk(B-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, music genre, song, band, location, award, musical artist, album, event, country, musical instrument and O.\nSentence: In the 1990s , artists like Me 'shell Ndegeocello , Brooklyn Funk Essentials and the ( predominantly UK-based ) acid jazz movement including artists and bands such as Jamiroquai , Incognito , Galliano , Omar Lye-Fook , Los Tetas and the Brand New Heavies carried on with strong elements of funk .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","1990s",",","artists","like","Me","'shell","Ndegeocello",",","Brooklyn","Funk","Essentials","and","the","(","predominantly","UK-based",")","acid","jazz","movement","including","artists","and","bands","such","as","Jamiroquai",",","Incognito",",","Galliano",",","Omar","Lye-Fook",",","Los","Tetas","and","the","Brand","New","Heavies","carried","on","with","strong","elements","of","funk","."],"labels":["O","O","O","O","O","O","B-musical artist","I-musical artist","I-musical artist","O","B-band","I-band","I-band","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","B-band","O","B-band","O","B-band","O","B-musical artist","I-musical artist","O","B-band","I-band","O","O","B-band","I-band","I-band","O","O","O","O","O","O","B-music genre","O"],"target_index":null,"target_label":null},"label_list":["organization","person","music_genre","song","band","location","award","musical_artist","album","event","country","musical_instrument"]}
{"id":"330","dataset":"crossner_music","split":"test","instance":{"id":"330","prompt_labels":"The(O) Simpsons(O) episode(O) Homerpalooza(B-person) features(O) Homer(B-person) explaining(O) 1970s(O) rock(B-music genre) music(I-music genre) to(O) Lisa(B-person) ,(O) Bart(B-person) ,(O) and(O) Milhouse(B-person) :(O) Grand(B-band) Funk(I-band) Railroad(I-band) paved(O) the(O) way(O) for(O) Jefferson(B-band) Airplane(I-band) ,(O) clearing(O) the(O) way(O) for(O) Jefferson(B-band) Starship(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, album, country, person, award, organization, musical artist, event, location, band, musical instrument, music genre and O.\nSentence: The Simpsons episode Homerpalooza features Homer explaining 1970s rock music to Lisa , Bart , and Milhouse : Grand Funk Railroad paved the way for Jefferson Airplane , clearing the way for Jefferson Starship .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Simpsons","episode","Homerpalooza","features","Homer","explaining","1970s","rock","music","to","Lisa",",","Bart",",","and","Milhouse",":","Grand","Funk","Railroad","paved","the","way","for","Jefferson","Airplane",",","clearing","the","way","for","Jefferson","Starship","."],"labels":["O","O","O","B-person","O","B-person","O","O","B-music genre","I-music genre","O","B-person","O","B-person","O","O","B-person","O","B-band","I-band","I-band","O","O","O","O","B-band","I-band","O","O","O","O","O","B-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["song","album","country","person","award","organization","musical_artist","event","location","band","musical_instrument","music_genre"]}
{"id":"334","dataset":"crossner_music","split":"test","instance":{"id":"334","prompt_labels":"Muse(B-band) released(O) another(O) live(O) DVD(O) on(O) 12(O) December(O) 2005(O) ,(O) Absolution(B-album) Tour(I-album) ,(O) containing(O) edited(O) and(O) remastered(O) highlights(O) from(O) their(O) Glastonbury(B-event) performance(O) unseen(O) footage(O) from(O) their(O) performances(O) at(O) London(B-location) Earls(B-location) Court(I-location) Exhibition(I-location) Centre(I-location) ,(O) Wembley(B-location) Arena(I-location) ,(O) and(O) the(O) Wiltern(B-location) Theatre(I-location) in(O) Los(B-location) Angeles(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, award, band, location, event, musical instrument, country, music genre, musical artist, album, song and O.\nSentence: Muse released another live DVD on 12 December 2005 , Absolution Tour , containing edited and remastered highlights from their Glastonbury performance unseen footage from their performances at London Earls Court Exhibition Centre , Wembley Arena , and the Wiltern Theatre in Los Angeles .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Muse","released","another","live","DVD","on","12","December","2005",",","Absolution","Tour",",","containing","edited","and","remastered","highlights","from","their","Glastonbury","performance","unseen","footage","from","their","performances","at","London","Earls","Court","Exhibition","Centre",",","Wembley","Arena",",","and","the","Wiltern","Theatre","in","Los","Angeles","."],"labels":["B-band","O","O","O","O","O","O","O","O","O","B-album","I-album","O","O","O","O","O","O","O","O","B-event","O","O","O","O","O","O","O","B-location","B-location","I-location","I-location","I-location","O","B-location","I-location","O","O","O","B-location","I-location","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["person","organization","award","band","location","event","musical_instrument","country","music_genre","musical_artist","album","song"]}
{"id":"335","dataset":"crossner_music","split":"test","instance":{"id":"335","prompt_labels":"The(O) band(O) released(O) four(O) albums(O) :(O) Styx(B-album) I(I-album) ((O) 1972(O) )(O) ,(O) Styx(B-album) II(I-album) ((O) 1973(O) )(O) ,(O) The(B-album) Serpent(I-album) Is(I-album) Rising(I-album) ((O) 1973(O) )(O) ,(O) and(O) Man(B-album) of(I-album) Miracles(I-album) ((O) 1974(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, music genre, location, person, album, musical instrument, event, song, country, musical artist, band, award and O.\nSentence: The band released four albums : Styx I ( 1972 ) , Styx II ( 1973 ) , The Serpent Is Rising ( 1973 ) , and Man of Miracles ( 1974 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","band","released","four","albums",":","Styx","I","(","1972",")",",","Styx","II","(","1973",")",",","The","Serpent","Is","Rising","(","1973",")",",","and","Man","of","Miracles","(","1974",")","."],"labels":["O","O","O","O","O","O","B-album","I-album","O","O","O","O","B-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","B-album","I-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","music_genre","location","person","album","musical_instrument","event","song","country","musical_artist","band","award"]}
{"id":"336","dataset":"crossner_music","split":"test","instance":{"id":"336","prompt_labels":"Nirvana(B-musical artist) occasionally(O) played(O) cover(O) songs(O) by(O) these(O) bands(O) ,(O) including(O) Led(B-band) Zeppelin(I-band) 's(O) Heartbreaker(B-song) ,(O) Moby(B-song) Dick(I-song) and(O) Immigrant(B-song) Song(I-song) ,(O) Black(B-band) Sabbath(I-band) 's(O) Hand(B-song) of(I-song) Doom(I-song) ,(O) and(O) Kiss(B-song) '(I-song) Do(I-song) You(I-song) Love(I-song) Me(I-song) ?(I-song) and(O) wrote(O) the(O) Incesticide(B-album) song(O) Aero(B-song) Zeppelin(I-song) as(O) a(O) tribute(O) to(O) Led(B-band) Zeppelin(I-band) and(O) Aerosmith(B-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, location, organization, country, award, album, band, song, musical instrument, music genre, person, event and O.\nSentence: Nirvana occasionally played cover songs by these bands , including Led Zeppelin 's Heartbreaker , Moby Dick and Immigrant Song , Black Sabbath 's Hand of Doom , and Kiss ' Do You Love Me ? and wrote the Incesticide song Aero Zeppelin as a tribute to Led Zeppelin and Aerosmith .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Nirvana","occasionally","played","cover","songs","by","these","bands",",","including","Led","Zeppelin","'s","Heartbreaker",",","Moby","Dick","and","Immigrant","Song",",","Black","Sabbath","'s","Hand","of","Doom",",","and","Kiss","'","Do","You","Love","Me","?","and","wrote","the","Incesticide","song","Aero","Zeppelin","as","a","tribute","to","Led","Zeppelin","and","Aerosmith","."],"labels":["B-musical artist","O","O","O","O","O","O","O","O","O","B-band","I-band","O","B-song","O","B-song","I-song","O","B-song","I-song","O","B-band","I-band","O","B-song","I-song","I-song","O","O","B-song","I-song","I-song","I-song","I-song","I-song","I-song","O","O","O","B-album","O","B-song","I-song","O","O","O","O","B-band","I-band","O","B-band","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","location","organization","country","award","album","band","song","musical_instrument","music_genre","person","event"]}
{"id":"344","dataset":"crossner_music","split":"test","instance":{"id":"344","prompt_labels":"He(O) began(O) a(O) long-running(O) DJ(O) residency(O) at(O) London(B-location) 's(I-location) Met(I-location) Bar(I-location) in(O) 2003(O) ,(O) playing(O) a(O) mix(O) of(O) House(B-music genre) music(I-music genre) and(O) Hip(B-music genre) hop(I-music genre) music(I-music genre) ,(O) and(O) has(O) gone(O) on(O) to(O) become(O) a(O) renowned(O) DJ(O) over(O) recent(O) years(O) ,(O) performing(O) sets(O) at(O) Cielo(B-location) New(I-location) York(I-location) and(O) Pacha(B-location) Ibiza(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, band, album, musical artist, music genre, person, location, event, organization, musical instrument, song, country and O.\nSentence: He began a long-running DJ residency at London 's Met Bar in 2003 , playing a mix of House music and Hip hop music , and has gone on to become a renowned DJ over recent years , performing sets at Cielo New York and Pacha Ibiza .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","began","a","long-running","DJ","residency","at","London","'s","Met","Bar","in","2003",",","playing","a","mix","of","House","music","and","Hip","hop","music",",","and","has","gone","on","to","become","a","renowned","DJ","over","recent","years",",","performing","sets","at","Cielo","New","York","and","Pacha","Ibiza","."],"labels":["O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["award","band","album","musical_artist","music_genre","person","location","event","organization","musical_instrument","song","country"]}
{"id":"345","dataset":"crossner_music","split":"test","instance":{"id":"345","prompt_labels":"Other(O) artists(O) such(O) as(O) Brooks(B-band) and(I-band) Dunn(I-band) ((O) Boot(B-song) Scootin(I-song) '(I-song) Boogie(I-song) )(O) also(O) combined(O) conventional(O) country(B-music genre) with(O) slick(O) ,(O) rock(B-music genre) elements(O) ,(O) while(O) Lorrie(B-musical artist) Morgan(I-musical artist) ,(O) Mary(B-musical artist) Chapin(I-musical artist) Carpenter(I-musical artist) ,(O) and(O) Kathy(B-musical artist) Mattea(I-musical artist) updated(O) neotraditionalist(O) styles(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, album, country, event, band, music genre, location, song, person, musical instrument, organization, musical artist and O.\nSentence: Other artists such as Brooks and Dunn ( Boot Scootin ' Boogie ) also combined conventional country with slick , rock elements , while Lorrie Morgan , Mary Chapin Carpenter , and Kathy Mattea updated neotraditionalist styles .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","artists","such","as","Brooks","and","Dunn","(","Boot","Scootin","'","Boogie",")","also","combined","conventional","country","with","slick",",","rock","elements",",","while","Lorrie","Morgan",",","Mary","Chapin","Carpenter",",","and","Kathy","Mattea","updated","neotraditionalist","styles","."],"labels":["O","O","O","O","B-band","I-band","I-band","O","B-song","I-song","I-song","I-song","O","O","O","O","B-music genre","O","O","O","B-music genre","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","album","country","event","band","music_genre","location","song","person","musical_instrument","organization","musical_artist"]}
{"id":"350","dataset":"crossner_music","split":"test","instance":{"id":"350","prompt_labels":"Brooks(B-band) and(I-band) Dunn(I-band) have(O) more(O) Country(B-award) Music(I-award) Association(I-award) awards(I-award) and(O) Academy(B-award) of(I-award) Country(I-award) Music(I-award) awards(I-award) than(O) any(O) act(O) in(O) the(O) history(O) of(O) country(B-music genre) music(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, music genre, musical artist, organization, country, band, event, award, person, song, location, album and O.\nSentence: Brooks and Dunn have more Country Music Association awards and Academy of Country Music awards than any act in the history of country music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Brooks","and","Dunn","have","more","Country","Music","Association","awards","and","Academy","of","Country","Music","awards","than","any","act","in","the","history","of","country","music","."],"labels":["B-band","I-band","I-band","O","O","B-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","B-music genre","O","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","music_genre","musical_artist","organization","country","band","event","award","person","song","location","album"]}
{"id":"353","dataset":"crossner_music","split":"test","instance":{"id":"353","prompt_labels":"He(O) also(O) collected(O) in(O) Moldavia(B-country) ,(O) Wallachia(B-location) ,(O) and(O) ((O) in(O) 1913(O) )(O) Algeria(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, musical artist, musical instrument, location, award, band, event, organization, country, music genre, person, song and O.\nSentence: He also collected in Moldavia , Wallachia , and ( in 1913 ) Algeria .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","also","collected","in","Moldavia",",","Wallachia",",","and","(","in","1913",")","Algeria","."],"labels":["O","O","O","O","B-country","O","B-location","O","O","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["album","musical_artist","musical_instrument","location","award","band","event","organization","country","music_genre","person","song"]}
{"id":"354","dataset":"crossner_music","split":"test","instance":{"id":"354","prompt_labels":"Other(O) influential(O) folk(B-music genre) artists(O) include(O) Surinder(B-musical artist) Shinda(I-musical artist) -(O) famous(O) for(O) his(O) Putt(O) Jattan(O) De(O) -(O) Harbhajan(B-musical artist) Mann(I-musical artist) ,(O) Manmohan(B-musical artist) Waris(I-musical artist) ,(O) Meshi(B-musical artist) Eshara(I-musical artist) ,(O) Sarbjit(B-musical artist) Cheema(I-musical artist) ,(O) Hans(B-musical artist) Raj(I-musical artist) Hans(I-musical artist) ,(O) Sardool(B-musical artist) Sikander(I-musical artist) ,(O) Anakhi(B-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical instrument, song, musical artist, country, person, album, organization, location, music genre, band, event and O.\nSentence: Other influential folk artists include Surinder Shinda - famous for his Putt Jattan De - Harbhajan Mann , Manmohan Waris , Meshi Eshara , Sarbjit Cheema , Hans Raj Hans , Sardool Sikander , Anakhi .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","influential","folk","artists","include","Surinder","Shinda","-","famous","for","his","Putt","Jattan","De","-","Harbhajan","Mann",",","Manmohan","Waris",",","Meshi","Eshara",",","Sarbjit","Cheema",",","Hans","Raj","Hans",",","Sardool","Sikander",",","Anakhi","."],"labels":["O","O","B-music genre","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-band","O"],"target_index":null,"target_label":null},"label_list":["award","musical_instrument","song","musical_artist","country","person","album","organization","location","music_genre","band","event"]}
{"id":"359","dataset":"crossner_music","split":"test","instance":{"id":"359","prompt_labels":"More(O) recently(O) ,(O) Illinois(B-musical artist) Jacquet(I-musical artist) ,(O) Ray(B-musical artist) Pizzi(I-musical artist) ,(O) Frank(B-musical artist) Tiberi(I-musical artist) ,(O) and(O) Marshall(B-musical artist) Allen(I-musical artist) have(O) both(O) doubled(O) on(O) bassoon(B-musical instrument) in(O) addition(O) to(O) their(O) saxophone(B-musical instrument) performances(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical instrument, location, band, musical artist, country, event, song, person, award, album, organization and O.\nSentence: More recently , Illinois Jacquet , Ray Pizzi , Frank Tiberi , and Marshall Allen have both doubled on bassoon in addition to their saxophone performances .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["More","recently",",","Illinois","Jacquet",",","Ray","Pizzi",",","Frank","Tiberi",",","and","Marshall","Allen","have","both","doubled","on","bassoon","in","addition","to","their","saxophone","performances","."],"labels":["O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","O","B-musical artist","I-musical artist","O","O","O","O","B-musical instrument","O","O","O","O","B-musical instrument","O","O"],"target_index":null,"target_label":null},"label_list":["music_genre","musical_instrument","location","band","musical_artist","country","event","song","person","award","album","organization"]}
{"id":"367","dataset":"crossner_music","split":"test","instance":{"id":"367","prompt_labels":"In(O) 2019(O) Ulvaeus(B-musical artist) worked(O) with(O) Swedish(O) songwriter(O) Andreas(B-musical artist) Carlsson(I-musical artist) to(O) arrange(O) an(O) English(O) dub(O) of(O) Tomas(B-musical artist) Ledin(I-musical artist) jukebox(O) musical(O) film(O) En(B-song) del(I-song) av(I-song) mitt(I-song) hjärta(I-song) ((O) English(O) :(O) A(B-song) Piece(I-song) of(I-song) My(I-song) Heart(I-song) )(O) directed(O) by(O) Edward(B-person) af(I-person) Sillén(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, location, person, album, award, music genre, musical instrument, country, event, musical artist, organization, band and O.\nSentence: In 2019 Ulvaeus worked with Swedish songwriter Andreas Carlsson to arrange an English dub of Tomas Ledin jukebox musical film En del av mitt hjärta ( English : A Piece of My Heart ) directed by Edward af Sillén .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2019","Ulvaeus","worked","with","Swedish","songwriter","Andreas","Carlsson","to","arrange","an","English","dub","of","Tomas","Ledin","jukebox","musical","film","En","del","av","mitt","hjärta","(","English",":","A","Piece","of","My","Heart",")","directed","by","Edward","af","Sillén","."],"labels":["O","O","B-musical artist","O","O","O","O","B-musical artist","I-musical artist","O","O","O","O","O","O","B-musical artist","I-musical artist","O","O","O","B-song","I-song","I-song","I-song","I-song","O","O","O","B-song","I-song","I-song","I-song","I-song","O","O","O","B-person","I-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["song","location","person","album","award","music_genre","musical_instrument","country","event","musical_artist","organization","band"]}
{"id":"373","dataset":"crossner_music","split":"test","instance":{"id":"373","prompt_labels":"Delta(B-music genre) blues(I-music genre) was(O) also(O) an(O) inspiration(O) for(O) the(O) creation(O) of(O) British(O) skiffle(B-music genre) music(I-music genre) ,(O) from(O) which(O) eventually(O) came(O) the(O) British(O) invasion(O) bands(O) ,(O) while(O) simultaneously(O) influencing(O) British(B-music genre) blues(I-music genre) ,(O) which(O) led(O) to(O) the(O) birth(O) of(O) early(O) hard(B-music genre) rock(I-music genre) and(O) Heavy(B-music genre) metal(I-music genre) music(I-music genre) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, location, song, person, award, music genre, event, album, organization, musical artist, country, band and O.\nSentence: Delta blues was also an inspiration for the creation of British skiffle music , from which eventually came the British invasion bands , while simultaneously influencing British blues , which led to the birth of early hard rock and Heavy metal music .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Delta","blues","was","also","an","inspiration","for","the","creation","of","British","skiffle","music",",","from","which","eventually","came","the","British","invasion","bands",",","while","simultaneously","influencing","British","blues",",","which","led","to","the","birth","of","early","hard","rock","and","Heavy","metal","music","."],"labels":["B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O","O","B-music genre","I-music genre","O","B-music genre","I-music genre","I-music genre","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","location","song","person","award","music_genre","event","album","organization","musical_artist","country","band"]}
{"id":"379","dataset":"crossner_music","split":"test","instance":{"id":"379","prompt_labels":"Crust(B-music genre) punk(I-music genre) groups(O) ,(O) such(O) as(O) Antisect(B-band) ,(O) Sacrilege(B-band) and(O) Anti(B-band) System(I-band) took(O) some(O) influence(O) from(O) early(B-music genre) black(I-music genre) metal(I-music genre) bands(O) like(O) Venom(B-band) ,(O) Hellhammer(B-band) ,(O) and(O) Celtic(B-band) Frost(I-band) ,(O) In(O) addition(O) ,(O) Norwegian(O) band(O) Darkthrone(B-band) have(O) incorporated(O) crust(B-music genre) punk(I-music genre) traits(O) in(O) their(O) more(O) recent(O) material(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, musical instrument, music genre, award, country, album, band, musical artist, event, location, song, person and O.\nSentence: Crust punk groups , such as Antisect , Sacrilege and Anti System took some influence from early black metal bands like Venom , Hellhammer , and Celtic Frost , In addition , Norwegian band Darkthrone have incorporated crust punk traits in their more recent material .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Crust","punk","groups",",","such","as","Antisect",",","Sacrilege","and","Anti","System","took","some","influence","from","early","black","metal","bands","like","Venom",",","Hellhammer",",","and","Celtic","Frost",",","In","addition",",","Norwegian","band","Darkthrone","have","incorporated","crust","punk","traits","in","their","more","recent","material","."],"labels":["B-music genre","I-music genre","O","O","O","O","B-band","O","B-band","O","B-band","I-band","O","O","O","O","B-music genre","I-music genre","I-music genre","O","O","B-band","O","B-band","O","O","B-band","I-band","O","O","O","O","O","O","B-band","O","O","B-music genre","I-music genre","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","musical_instrument","music_genre","award","country","album","band","musical_artist","event","location","song","person"]}
{"id":"380","dataset":"crossner_music","split":"test","instance":{"id":"380","prompt_labels":"Brad(B-person) Shoup(I-person) of(O) Stereogum(O) surmised(O) that(O) ,(O) thanks(O) to(O) the(O) Ramones(B-band) '(O) praise(O) for(O) the(O) group(O) ,(O) many(O) punk(B-music genre) ,(O) pop(B-music genre) punk(I-music genre) ,(O) or(O) punk-adjacent(B-music genre) artists(O) showed(O) influence(O) from(O) the(B-band) Beach(I-band) Boys(I-band) ,(O) noting(O) cover(O) versions(O) of(O) the(O) band(O) 's(O) songs(O) recorded(O) by(O) Slickee(B-band) Boys(I-band) ,(O) Agent(B-band) Orange(I-band) ,(O) Bad(B-band) Religion(I-band) ,(O) Shonen(B-band) Knife(I-band) ,(O) the(B-band) Queers(I-band) ,(O) Hi-Standard(B-band) ,(O) the(B-band) Descendents(I-band) ,(O) the(B-band) Donnas(I-band) ,(O) M.O.D.(B-band) ,(O) and(O) the(B-band) Vandals(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: song, album, event, person, musical instrument, band, musical artist, location, award, country, organization, music genre and O.\nSentence: Brad Shoup of Stereogum surmised that , thanks to the Ramones ' praise for the group , many punk , pop punk , or punk-adjacent artists showed influence from the Beach Boys , noting cover versions of the band 's songs recorded by Slickee Boys , Agent Orange , Bad Religion , Shonen Knife , the Queers , Hi-Standard , the Descendents , the Donnas , M.O.D. , and the Vandals .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Brad","Shoup","of","Stereogum","surmised","that",",","thanks","to","the","Ramones","'","praise","for","the","group",",","many","punk",",","pop","punk",",","or","punk-adjacent","artists","showed","influence","from","the","Beach","Boys",",","noting","cover","versions","of","the","band","'s","songs","recorded","by","Slickee","Boys",",","Agent","Orange",",","Bad","Religion",",","Shonen","Knife",",","the","Queers",",","Hi-Standard",",","the","Descendents",",","the","Donnas",",","M.O.D.",",","and","the","Vandals","."],"labels":["B-person","I-person","O","O","O","O","O","O","O","O","B-band","O","O","O","O","O","O","O","B-music genre","O","B-music genre","I-music genre","O","O","B-music genre","O","O","O","O","B-band","I-band","I-band","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","O","B-band","I-band","O","B-band","O","B-band","I-band","O","B-band","I-band","O","B-band","O","O","B-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["song","album","event","person","musical_instrument","band","musical_artist","location","award","country","organization","music_genre"]}
{"id":"385","dataset":"crossner_music","split":"test","instance":{"id":"385","prompt_labels":"Van(B-person) Bentum(I-person) competed(O) in(O) three(O) consecutive(O) Summer(B-event) Olympics(I-event) for(O) her(O) native(O) country(O) ,(O) starting(O) in(O) Swimming(O) at(O) the(O) 1980(B-event) Summer(I-event) Olympics(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, musical artist, location, musical instrument, country, award, album, song, music genre, event, person, band and O.\nSentence: Van Bentum competed in three consecutive Summer Olympics for her native country , starting in Swimming at the 1980 Summer Olympics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Van","Bentum","competed","in","three","consecutive","Summer","Olympics","for","her","native","country",",","starting","in","Swimming","at","the","1980","Summer","Olympics","."],"labels":["B-person","I-person","O","O","O","O","B-event","I-event","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["organization","musical_artist","location","musical_instrument","country","award","album","song","music_genre","event","person","band"]}
{"id":"388","dataset":"crossner_music","split":"test","instance":{"id":"388","prompt_labels":"The(O) groups(O) which(O) made(O) up(O) the(O) Native(O) Tongues(O) Posse(O) tended(O) toward(O) jazzy(O) releases(O) :(O) these(O) include(O) the(O) Jungle(B-band) Brothers(I-band) '(O) debut(O) Straight(B-album) Out(I-album) the(I-album) Jungle(I-album) ((O) 1988(O) )(O) ,(O) and(O) A(B-band) Tribe(I-band) Called(I-band) Quest(I-band) '(O) s(O) People(B-album) 's(I-album) Instinctive(I-album) Travels(I-album) and(I-album) the(I-album) Paths(I-album) of(I-album) Rhythm(I-album) ((O) 1990(O) )(O) and(O) The(B-album) Low(I-album) End(I-album) Theory(I-album) ((O) 1991(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, band, song, musical instrument, album, music genre, person, country, musical artist, location, award and O.\nSentence: The groups which made up the Native Tongues Posse tended toward jazzy releases : these include the Jungle Brothers ' debut Straight Out the Jungle ( 1988 ) , and A Tribe Called Quest ' s People 's Instinctive Travels and the Paths of Rhythm ( 1990 ) and The Low End Theory ( 1991 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","groups","which","made","up","the","Native","Tongues","Posse","tended","toward","jazzy","releases",":","these","include","the","Jungle","Brothers","'","debut","Straight","Out","the","Jungle","(","1988",")",",","and","A","Tribe","Called","Quest","'","s","People","'s","Instinctive","Travels","and","the","Paths","of","Rhythm","(","1990",")","and","The","Low","End","Theory","(","1991",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-band","I-band","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","B-band","I-band","I-band","I-band","O","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","I-album","I-album","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","organization","band","song","musical_instrument","album","music_genre","person","country","musical_artist","location","award"]}
{"id":"391","dataset":"crossner_music","split":"test","instance":{"id":"391","prompt_labels":"The(O) album(O) was(O) a(O) commercial(O) disappointment(O) compared(O) to(O) the(O) band(O) 's(O) previous(O) effort(O) ,(O) though(O) it(O) charted(O) generally(O) high(O) at(O) #(O) 5(O) in(O) the(O) United(B-country) Kingdom(I-country) and(O) Germany(B-country) ,(O) #(O) 7(O) on(O) the(O) Austria(B-country) n(O) and(O) Switzerland(B-country) music(O) charts(O) and(O) #(O) 8(O) in(O) Norway(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, band, award, location, organization, music genre, event, musical instrument, song, album, musical artist and O.\nSentence: The album was a commercial disappointment compared to the band 's previous effort , though it charted generally high at # 5 in the United Kingdom and Germany , # 7 on the Austria n and Switzerland music charts and # 8 in Norway .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","album","was","a","commercial","disappointment","compared","to","the","band","'s","previous","effort",",","though","it","charted","generally","high","at","#","5","in","the","United","Kingdom","and","Germany",",","#","7","on","the","Austria","n","and","Switzerland","music","charts","and","#","8","in","Norway","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","B-country","O","O","O","O","O","B-country","O","O","B-country","O","O","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["person","country","band","award","location","organization","music_genre","event","musical_instrument","song","album","musical_artist"]}
{"id":"397","dataset":"crossner_music","split":"test","instance":{"id":"397","prompt_labels":"The(O) film(O) was(O) nominated(O) for(O) Academy(B-award) Awards(I-award) for(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Costume(I-award) Design(I-award) ,(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Score(I-award) and(O) Academy(B-award) Award(I-award) for(I-award) Best(I-award) Original(I-award) Screenplay(I-award) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical instrument, award, event, album, music genre, location, musical artist, band, country, song, person, organization and O.\nSentence: The film was nominated for Academy Awards for Academy Award for Best Costume Design , Academy Award for Best Original Score and Academy Award for Best Original Screenplay .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","film","was","nominated","for","Academy","Awards","for","Academy","Award","for","Best","Costume","Design",",","Academy","Award","for","Best","Original","Score","and","Academy","Award","for","Best","Original","Screenplay","."],"labels":["O","O","O","O","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O"],"target_index":null,"target_label":null},"label_list":["musical_instrument","award","event","album","music_genre","location","musical_artist","band","country","song","person","organization"]}
{"id":"400","dataset":"crossner_music","split":"test","instance":{"id":"400","prompt_labels":"The(B-band) Locust(I-band) ,(O) from(O) San(B-location) Diego(I-location) ,(O) In(O) Los(B-location) Angeles(I-location) ,(O) Hole(B-band) also(O) initially(O) drew(O) influence(O) from(O) grindcore(B-music genre) in(O) their(O) early(O) releases(O) ,(O) particularly(O) on(O) their(O) singles(O) Dicknail(B-song) and(O) Teenage(B-song) Whore(I-song) ,(O) as(O) well(O) as(O) on(O) their(O) debut(O) album(O) ,(O) Pretty(B-album) on(I-album) the(I-album) Inside(I-album) ((O) 1991(O) )(O) ,(O) {(O) {(O) cite(O) AV(O) media(O) notes(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: band, award, musical instrument, event, musical artist, location, album, song, person, organization, music genre, country and O.\nSentence: The Locust , from San Diego , In Los Angeles , Hole also initially drew influence from grindcore in their early releases , particularly on their singles Dicknail and Teenage Whore , as well as on their debut album , Pretty on the Inside ( 1991 ) , { { cite AV media notes","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Locust",",","from","San","Diego",",","In","Los","Angeles",",","Hole","also","initially","drew","influence","from","grindcore","in","their","early","releases",",","particularly","on","their","singles","Dicknail","and","Teenage","Whore",",","as","well","as","on","their","debut","album",",","Pretty","on","the","Inside","(","1991",")",",","{","{","cite","AV","media","notes"],"labels":["B-band","I-band","O","O","B-location","I-location","O","O","B-location","I-location","O","B-band","O","O","O","O","O","B-music genre","O","O","O","O","O","O","O","O","O","B-song","O","B-song","I-song","O","O","O","O","O","O","O","O","O","B-album","I-album","I-album","I-album","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["band","award","musical_instrument","event","musical_artist","location","album","song","person","organization","music_genre","country"]}
{"id":"401","dataset":"crossner_music","split":"test","instance":{"id":"401","prompt_labels":"In(O) English(O) skiffle(B-music genre) bands(O) ,(O) Australia(B-country) n(O) and(O) New(B-country) Zealand(I-country) bush(B-band) band(O) s(O) and(O) South(B-country) Africa(I-country) n(O) kwela(B-music genre) bands(O) ,(O) the(O) same(O) sort(O) of(O) bass(O) has(O) a(O) tea(O) chest(O) as(O) a(O) resonator(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, musical instrument, organization, song, music genre, album, award, location, country, band, musical artist, person and O.\nSentence: In English skiffle bands , Australia n and New Zealand bush band s and South Africa n kwela bands , the same sort of bass has a tea chest as a resonator .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","English","skiffle","bands",",","Australia","n","and","New","Zealand","bush","band","s","and","South","Africa","n","kwela","bands",",","the","same","sort","of","bass","has","a","tea","chest","as","a","resonator","."],"labels":["O","O","B-music genre","O","O","B-country","O","O","B-country","I-country","B-band","O","O","O","B-country","I-country","O","B-music genre","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","musical_instrument","organization","song","music_genre","album","award","location","country","band","musical_artist","person"]}
{"id":"403","dataset":"crossner_music","split":"test","instance":{"id":"403","prompt_labels":"Major(O) artists(O) of(O) the(O) Texas(O) style(O) are(O) Johnny(B-musical artist) Winter(I-musical artist) ,(O) Stevie(B-musical artist) Ray(I-musical artist) Vaughan(I-musical artist) ,(O) the(O) The(B-band) Fabulous(I-band) Thunderbirds(I-band) ((O) led(O) by(O) harmonica(B-musical instrument) player(O) and(O) singer-songwriter(O) Kim(B-musical artist) Wilson(I-musical artist) )(O) ,(O) and(O) ZZ(B-band) Top(I-band) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, music genre, country, location, organization, event, musical instrument, award, song, musical artist, band, person and O.\nSentence: Major artists of the Texas style are Johnny Winter , Stevie Ray Vaughan , the The Fabulous Thunderbirds ( led by harmonica player and singer-songwriter Kim Wilson ) , and ZZ Top .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Major","artists","of","the","Texas","style","are","Johnny","Winter",",","Stevie","Ray","Vaughan",",","the","The","Fabulous","Thunderbirds","(","led","by","harmonica","player","and","singer-songwriter","Kim","Wilson",")",",","and","ZZ","Top","."],"labels":["O","O","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","O","B-band","I-band","I-band","O","O","O","B-musical instrument","O","O","O","B-musical artist","I-musical artist","O","O","O","B-band","I-band","O"],"target_index":null,"target_label":null},"label_list":["album","music_genre","country","location","organization","event","musical_instrument","award","song","musical_artist","band","person"]}
{"id":"405","dataset":"crossner_music","split":"test","instance":{"id":"405","prompt_labels":"He(O) worked(O) with(O) Berlin(B-organization) State(I-organization) Opera(I-organization) ;(O) La(B-location) Scala(I-location) ,(O) Milan(B-location) ;(O) Royal(B-location) Opera(I-location) Stockholm(I-location) ;(O) the(O) Royal(B-location) Opera(I-location) House(I-location) at(O) Covent(B-location) Garden(I-location) ,(O) Chorégies(B-location) d(I-location) 'Orange(I-location) and(O) Houston(B-location) Grand(I-location) Opera(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, musical artist, organization, location, musical instrument, person, country, music genre, album, band, song, event and O.\nSentence: He worked with Berlin State Opera ; La Scala , Milan ; Royal Opera Stockholm ; the Royal Opera House at Covent Garden , Chorégies d 'Orange and Houston Grand Opera .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","worked","with","Berlin","State","Opera",";","La","Scala",",","Milan",";","Royal","Opera","Stockholm",";","the","Royal","Opera","House","at","Covent","Garden",",","Chorégies","d","'Orange","and","Houston","Grand","Opera","."],"labels":["O","O","O","B-organization","I-organization","I-organization","O","B-location","I-location","O","B-location","O","B-location","I-location","I-location","O","O","B-location","I-location","I-location","O","B-location","I-location","O","B-location","I-location","I-location","O","B-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["award","musical_artist","organization","location","musical_instrument","person","country","music_genre","album","band","song","event"]}
{"id":"409","dataset":"crossner_music","split":"test","instance":{"id":"409","prompt_labels":"Nine(B-band) Inch(I-band) Nails(I-band) covered(O) the(O) song(O) Metal(B-song) on(O) The(B-album) Fragile(I-album) remix(O) album(O) Things(B-album) Falling(I-album) Apart(I-album) as(O) did(O) Afrika(B-musical artist) Bambaataa(I-musical artist) ((O) with(O) Numan(B-musical artist) himself(O) )(O) on(O) the(O) album(O) Dark(B-album) Matter(I-album) Moving(I-album) at(I-album) the(I-album) Speed(I-album) of(I-album) Light(I-album) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: musical artist, country, award, person, location, band, album, song, music genre, event, organization, musical instrument and O.\nSentence: Nine Inch Nails covered the song Metal on The Fragile remix album Things Falling Apart as did Afrika Bambaataa ( with Numan himself ) on the album Dark Matter Moving at the Speed of Light .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Nine","Inch","Nails","covered","the","song","Metal","on","The","Fragile","remix","album","Things","Falling","Apart","as","did","Afrika","Bambaataa","(","with","Numan","himself",")","on","the","album","Dark","Matter","Moving","at","the","Speed","of","Light","."],"labels":["B-band","I-band","I-band","O","O","O","B-song","O","B-album","I-album","O","O","B-album","I-album","I-album","O","O","B-musical artist","I-musical artist","O","O","B-musical artist","O","O","O","O","O","B-album","I-album","I-album","I-album","I-album","I-album","I-album","I-album","O"],"target_index":null,"target_label":null},"label_list":["musical_artist","country","award","person","location","band","album","song","music_genre","event","organization","musical_instrument"]}
{"id":"423","dataset":"crossner_music","split":"test","instance":{"id":"423","prompt_labels":"The(O) musical(O) was(O) nominated(O) for(O) three(O) Tony(B-award) Awards(I-award) :(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Musical(I-award) ,(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Scenic(I-award) Design(I-award) ,(O) and(O) Tony(B-award) Award(I-award) for(I-award) Best(I-award) Lighting(I-award) Design(I-award) ,(O) winning(O) the(O) latter(O) two(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, album, music genre, event, song, person, award, musical artist, country, musical instrument, band and O.\nSentence: The musical was nominated for three Tony Awards : Tony Award for Best Musical , Tony Award for Best Scenic Design , and Tony Award for Best Lighting Design , winning the latter two .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","musical","was","nominated","for","three","Tony","Awards",":","Tony","Award","for","Best","Musical",",","Tony","Award","for","Best","Scenic","Design",",","and","Tony","Award","for","Best","Lighting","Design",",","winning","the","latter","two","."],"labels":["O","O","O","O","O","O","B-award","I-award","O","B-award","I-award","I-award","I-award","I-award","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","location","album","music_genre","event","song","person","award","musical_artist","country","musical_instrument","band"]}
{"id":"428","dataset":"crossner_music","split":"test","instance":{"id":"428","prompt_labels":"Others(O) ,(O) including(O) San(B-location) Francisco(I-location) Bay(I-location) Area(I-location) 's(O) Testament(B-band) and(O) Exodus(B-band) ,(O) New(B-location) Jersey(I-location) 's(O) Overkill(B-band) ,(O) and(O) Brazil(B-location) 's(O) Sepultura(B-band) and(O) Sarcófago(B-band) ,(O) also(O) had(O) a(O) significant(O) impact(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: album, location, person, organization, song, music genre, musical artist, band, musical instrument, award, event, country and O.\nSentence: Others , including San Francisco Bay Area 's Testament and Exodus , New Jersey 's Overkill , and Brazil 's Sepultura and Sarcófago , also had a significant impact .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Others",",","including","San","Francisco","Bay","Area","'s","Testament","and","Exodus",",","New","Jersey","'s","Overkill",",","and","Brazil","'s","Sepultura","and","Sarcófago",",","also","had","a","significant","impact","."],"labels":["O","O","O","B-location","I-location","I-location","I-location","O","B-band","O","B-band","O","B-location","I-location","O","B-band","O","O","B-location","O","B-band","O","B-band","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["album","location","person","organization","song","music_genre","musical_artist","band","musical_instrument","award","event","country"]}
{"id":"432","dataset":"crossner_music","split":"test","instance":{"id":"432","prompt_labels":"The(O) current(O) general(O) music(O) director(O) of(O) the(O) Berlin(B-location) State(I-location) Opera(I-location) and(O) the(O) Staatskapelle(B-band) Berlin(I-band) ,(O) Barenboim(B-musical artist) previously(O) served(O) as(O) Music(O) Director(O) of(O) the(O) Chicago(B-band) Symphony(I-band) Orchestra(I-band) ,(O) the(O) Orchestre(B-band) de(I-band) Paris(I-band) and(O) La(B-location) Scala(I-location) in(O) Milan(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: music genre, musical artist, album, band, person, musical instrument, location, country, award, organization, event, song and O.\nSentence: The current general music director of the Berlin State Opera and the Staatskapelle Berlin , Barenboim previously served as Music Director of the Chicago Symphony Orchestra , the Orchestre de Paris and La Scala in Milan .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","current","general","music","director","of","the","Berlin","State","Opera","and","the","Staatskapelle","Berlin",",","Barenboim","previously","served","as","Music","Director","of","the","Chicago","Symphony","Orchestra",",","the","Orchestre","de","Paris","and","La","Scala","in","Milan","."],"labels":["O","O","O","O","O","O","O","B-location","I-location","I-location","O","O","B-band","I-band","O","B-musical artist","O","O","O","O","O","O","O","B-band","I-band","I-band","O","O","B-band","I-band","I-band","O","B-location","I-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["music_genre","musical_artist","album","band","person","musical_instrument","location","country","award","organization","event","song"]}
{"id":"442","dataset":"crossner_music","split":"test","instance":{"id":"442","prompt_labels":"Boogie-woogie(B-music genre) was(O) pioneered(O) by(O) the(O) Chicago-based(B-location) Jimmy(B-musical artist) Yancey(I-musical artist) and(O) the(O) Boogie-Woogie(O) Trio(O) ((O) Albert(B-musical artist) Ammons(I-musical artist) ,(O) Pete(B-musical artist) Johnson(I-musical artist) and(O) Meade(B-musical artist) Lux(I-musical artist) Lewis(I-musical artist) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, musical artist, country, musical instrument, band, organization, song, location, music genre, event, album, award and O.\nSentence: Boogie-woogie was pioneered by the Chicago-based Jimmy Yancey and the Boogie-Woogie Trio ( Albert Ammons , Pete Johnson and Meade Lux Lewis ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Boogie-woogie","was","pioneered","by","the","Chicago-based","Jimmy","Yancey","and","the","Boogie-Woogie","Trio","(","Albert","Ammons",",","Pete","Johnson","and","Meade","Lux","Lewis",")","."],"labels":["B-music genre","O","O","O","O","B-location","B-musical artist","I-musical artist","O","O","O","O","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","O","B-musical artist","I-musical artist","I-musical artist","O","O"],"target_index":null,"target_label":null},"label_list":["person","musical_artist","country","musical_instrument","band","organization","song","location","music_genre","event","album","award"]}
{"id":"459","dataset":"crossner_music","split":"test","instance":{"id":"459","prompt_labels":"Rhythms(B-band) of(I-band) Resistance(I-band) formed(O) as(O) part(O) of(O) the(O) UK(B-organization) Earth(I-organization) First(I-organization) action(O) against(O) the(O) International(B-organization) Monetary(I-organization) Fund(I-organization) /(O) World(B-organization) Bank(I-organization) meeting(O) in(O) Prague(B-location) in(O) September(O) 2000(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, band, person, country, album, event, location, music genre, musical instrument, award, song, musical artist and O.\nSentence: Rhythms of Resistance formed as part of the UK Earth First action against the International Monetary Fund / World Bank meeting in Prague in September 2000 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Rhythms","of","Resistance","formed","as","part","of","the","UK","Earth","First","action","against","the","International","Monetary","Fund","/","World","Bank","meeting","in","Prague","in","September","2000","."],"labels":["B-band","I-band","I-band","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","O","O","B-location","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","band","person","country","album","event","location","music_genre","musical_instrument","award","song","musical_artist"]}
{"id":"5","dataset":"crossner_politics","split":"test","instance":{"id":"5","prompt_labels":"Locker-Lampson(B-politician) took(O) Einstein(B-person) to(O) meet(O) Winston(B-politician) Churchill(I-politician) at(O) his(O) home(O) ,(O) and(O) later(O) ,(O) Austen(B-politician) Chamberlain(I-politician) and(O) former(O) Prime(O) Minister(O) David(B-politician) Lloyd(I-politician) George(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, person, location, organization, election, political party, event, politician and O.\nSentence: Locker-Lampson took Einstein to meet Winston Churchill at his home , and later , Austen Chamberlain and former Prime Minister David Lloyd George .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Locker-Lampson","took","Einstein","to","meet","Winston","Churchill","at","his","home",",","and","later",",","Austen","Chamberlain","and","former","Prime","Minister","David","Lloyd","George","."],"labels":["B-politician","O","B-person","O","O","B-politician","I-politician","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","B-politician","I-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["country","person","location","organization","election","political_party","event","politician"]}
{"id":"23","dataset":"crossner_politics","split":"test","instance":{"id":"23","prompt_labels":"Before(O) making(O) the(O) announcement(O) ,(O) Alexander(B-person) did(O) not(O) consult(O) with(O) his(O) father(O) ,(O) who(O) had(O) been(O) on(O) vacation(O) in(O) Karlovy(B-location) Vary(I-location) and(O) making(O) arrangements(O) to(O) secure(O) the(O) hand(O) of(O) Germany(B-country) Princess(O) Alexandra(B-person) zu(I-person) Schaumburg-Lippe(I-person) for(O) his(O) son(O) ,(O) or(O) his(O) Prime(O) Minister(O) Dr.(O) Vladan(B-person) Đorđević(I-person) ,(O) who(O) was(O) visiting(O) the(O) Paris(B-location) Universal(O) Exhibition(O) at(O) the(O) time(O) of(O) the(O) announcement(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, organization, politician, country, political party, election, person and O.\nSentence: Before making the announcement , Alexander did not consult with his father , who had been on vacation in Karlovy Vary and making arrangements to secure the hand of Germany Princess Alexandra zu Schaumburg-Lippe for his son , or his Prime Minister Dr. Vladan Đorđević , who was visiting the Paris Universal Exhibition at the time of the announcement .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Before","making","the","announcement",",","Alexander","did","not","consult","with","his","father",",","who","had","been","on","vacation","in","Karlovy","Vary","and","making","arrangements","to","secure","the","hand","of","Germany","Princess","Alexandra","zu","Schaumburg-Lippe","for","his","son",",","or","his","Prime","Minister","Dr.","Vladan","Đorđević",",","who","was","visiting","the","Paris","Universal","Exhibition","at","the","time","of","the","announcement","."],"labels":["O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","B-country","O","B-person","I-person","I-person","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","location","organization","politician","country","political_party","election","person"]}
{"id":"37","dataset":"crossner_politics","split":"test","instance":{"id":"37","prompt_labels":"Allies(O) of(O) the(O) ACLU(B-organization) in(O) legal(O) actions(O) have(O) included(O) the(O) National(B-organization) Association(I-organization) for(I-organization) the(I-organization) Advancement(I-organization) of(I-organization) Colored(I-organization) People(I-organization) ,(O) the(O) American(B-organization) Jewish(I-organization) Congress(I-organization) ,(O) People(B-organization) For(I-organization) the(I-organization) American(I-organization) Way(I-organization) ,(O) the(O) National(B-organization) Rifle(I-organization) Association(I-organization) ,(O) the(O) Electronic(B-organization) Frontier(I-organization) Foundation(I-organization) ,(O) Americans(B-organization) United(I-organization) for(I-organization) Separation(I-organization) of(I-organization) Church(I-organization) and(I-organization) State(I-organization) ,(O) and(O) the(O) National(B-organization) Organization(I-organization) for(I-organization) Women(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, country, political party, location, organization, election, politician and O.\nSentence: Allies of the ACLU in legal actions have included the National Association for the Advancement of Colored People , the American Jewish Congress , People For the American Way , the National Rifle Association , the Electronic Frontier Foundation , Americans United for Separation of Church and State , and the National Organization for Women .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Allies","of","the","ACLU","in","legal","actions","have","included","the","National","Association","for","the","Advancement","of","Colored","People",",","the","American","Jewish","Congress",",","People","For","the","American","Way",",","the","National","Rifle","Association",",","the","Electronic","Frontier","Foundation",",","Americans","United","for","Separation","of","Church","and","State",",","and","the","National","Organization","for","Women","."],"labels":["O","O","O","B-organization","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["event","person","country","political_party","location","organization","election","politician"]}
{"id":"48","dataset":"crossner_politics","split":"test","instance":{"id":"48","prompt_labels":"They(O) also(O) raided(O) the(O) Levant(B-location) ,(O) Israel(B-country) and(O) Kingdom(B-country) of(I-country) Judah(I-country) ((O) where(O) Ashkelon(B-location) was(O) sacked(O) by(O) the(O) Scythians(O) )(O) and(O) all(O) the(O) way(O) into(O) Egypt(B-country) whose(O) coasts(O) were(O) ravaged(O) and(O) looted(O) with(O) impunity(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, election, organization, political party, event, person, location and O.\nSentence: They also raided the Levant , Israel and Kingdom of Judah ( where Ashkelon was sacked by the Scythians ) and all the way into Egypt whose coasts were ravaged and looted with impunity .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["They","also","raided","the","Levant",",","Israel","and","Kingdom","of","Judah","(","where","Ashkelon","was","sacked","by","the","Scythians",")","and","all","the","way","into","Egypt","whose","coasts","were","ravaged","and","looted","with","impunity","."],"labels":["O","O","O","O","B-location","O","B-country","O","B-country","I-country","I-country","O","O","B-location","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["politician","country","election","organization","political_party","event","person","location"]}
{"id":"56","dataset":"crossner_politics","split":"test","instance":{"id":"56","prompt_labels":"She(O) contested(O) the(O) seat(O) of(O) Burnley(B-location) in(O) Lancashire(B-location) in(O) the(O) 1979(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) and(O) then(O) ,(O) against(O) David(B-politician) Owen(I-politician) ,(O) the(O) Plymouth(B-location) Devonport(I-location) seat(O) in(O) the(O) 1983(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, organization, location, election, politician, event, political party and O.\nSentence: She contested the seat of Burnley in Lancashire in the 1979 United Kingdom general election and then , against David Owen , the Plymouth Devonport seat in the 1983 United Kingdom general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","contested","the","seat","of","Burnley","in","Lancashire","in","the","1979","United","Kingdom","general","election","and","then",",","against","David","Owen",",","the","Plymouth","Devonport","seat","in","the","1983","United","Kingdom","general","election","."],"labels":["O","O","O","O","O","B-location","O","B-location","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","B-politician","I-politician","O","O","B-location","I-location","O","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["person","country","organization","location","election","politician","event","political_party"]}
{"id":"59","dataset":"crossner_politics","split":"test","instance":{"id":"59","prompt_labels":"Early(O) loans(O) went(O) largely(O) to(O) Indonesia(B-country) ,(O) Thailand(B-country) ,(O) Malaysia(B-country) ,(O) South(B-country) Korea(I-country) and(O) the(O) Philippines(B-country) ;(O) these(O) nations(O) accounted(O) for(O) 78.48(O) %(O) of(O) the(O) total(O) ADB(B-organization) loans(O) between(O) 1967(O) and(O) 1972(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, person, politician, country, location, event, organization, political party and O.\nSentence: Early loans went largely to Indonesia , Thailand , Malaysia , South Korea and the Philippines ; these nations accounted for 78.48 % of the total ADB loans between 1967 and 1972 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Early","loans","went","largely","to","Indonesia",",","Thailand",",","Malaysia",",","South","Korea","and","the","Philippines",";","these","nations","accounted","for","78.48","%","of","the","total","ADB","loans","between","1967","and","1972","."],"labels":["O","O","O","O","O","B-country","O","B-country","O","B-country","O","B-country","I-country","O","O","B-country","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","person","politician","country","location","event","organization","political_party"]}
{"id":"61","dataset":"crossner_politics","split":"test","instance":{"id":"61","prompt_labels":"Abbas(B-person) II(I-person) ((O) full(O) name(O) :(O) Abbas(B-person) Hilmy(I-person) )(O) ,(O) the(O) great-great-grandson(O) of(O) Muhammad(B-person) Ali(I-person) ,(O) was(O) born(O) in(O) Alexandria(B-location) ,(O) Egypt(B-country) on(O) 14(O) July(O) 1874(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, person, politician, election, organization, political party, location and O.\nSentence: Abbas II ( full name : Abbas Hilmy ) , the great-great-grandson of Muhammad Ali , was born in Alexandria , Egypt on 14 July 1874 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Abbas","II","(","full","name",":","Abbas","Hilmy",")",",","the","great-great-grandson","of","Muhammad","Ali",",","was","born","in","Alexandria",",","Egypt","on","14","July","1874","."],"labels":["B-person","I-person","O","O","O","O","B-person","I-person","O","O","O","O","O","B-person","I-person","O","O","O","O","B-location","O","B-country","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","event","person","politician","election","organization","political_party","location"]}
{"id":"65","dataset":"crossner_politics","split":"test","instance":{"id":"65","prompt_labels":"The(O) youngest(O) daughter(O) of(O) Aung(B-politician) San(I-politician) ,(O) Father(O) of(O) the(O) Nation(O) of(O) modern-day(O) Myanmar(B-country) ,(O) and(O) Khin(B-politician) Kyi(I-politician) ,(O) Aung(B-politician) San(I-politician) Suu(I-politician) Kyi(I-politician) was(O) born(O) in(O) Yangon(B-location) ,(O) British(O) rule(O) in(O) Burma(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, location, organization, politician, event, person, election and O.\nSentence: The youngest daughter of Aung San , Father of the Nation of modern-day Myanmar , and Khin Kyi , Aung San Suu Kyi was born in Yangon , British rule in Burma .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","youngest","daughter","of","Aung","San",",","Father","of","the","Nation","of","modern-day","Myanmar",",","and","Khin","Kyi",",","Aung","San","Suu","Kyi","was","born","in","Yangon",",","British","rule","in","Burma","."],"labels":["O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","B-country","O","O","B-politician","I-politician","O","B-politician","I-politician","I-politician","I-politician","O","O","O","B-location","O","O","O","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["country","political_party","location","organization","politician","event","person","election"]}
{"id":"66","dataset":"crossner_politics","split":"test","instance":{"id":"66","prompt_labels":"On(O) 9(O) November(O) 1996(O) ,(O) the(O) motorcade(O) that(O) Aung(B-politician) San(I-politician) Suu(I-politician) Kyi(I-politician) was(O) traveling(O) in(O) with(O) other(O) National(B-political party) League(I-political party) for(I-political party) Democracy(I-political party) leaders(O) Tin(B-politician) Oo(I-politician) and(O) Kyi(B-politician) Maung(I-politician) ,(O) was(O) attacked(O) in(O) Yangon(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, election, politician, location, person, organization, political party, country and O.\nSentence: On 9 November 1996 , the motorcade that Aung San Suu Kyi was traveling in with other National League for Democracy leaders Tin Oo and Kyi Maung , was attacked in Yangon .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","9","November","1996",",","the","motorcade","that","Aung","San","Suu","Kyi","was","traveling","in","with","other","National","League","for","Democracy","leaders","Tin","Oo","and","Kyi","Maung",",","was","attacked","in","Yangon","."],"labels":["O","O","O","O","O","O","O","O","B-politician","I-politician","I-politician","I-politician","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["event","election","politician","location","person","organization","political_party","country"]}
{"id":"71","dataset":"crossner_politics","split":"test","instance":{"id":"71","prompt_labels":"In(O) the(O) United(B-country) States(I-country) ,(O) within(O) 100(O) years(O) ,(O) four(O) presidents(O) -(O) Abraham(B-politician) Lincoln(I-politician) ,(O) James(B-politician) A.(I-politician) Garfield(I-politician) ,(O) William(B-politician) McKinley(I-politician) and(O) John(B-politician) F.(I-politician) Kennedy(I-politician) -(O) died(O) at(O) the(O) hands(O) of(O) assassins(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, person, event, country, organization, election, location and O.\nSentence: In the United States , within 100 years , four presidents - Abraham Lincoln , James A. Garfield , William McKinley and John F. Kennedy - died at the hands of assassins .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","United","States",",","within","100","years",",","four","presidents","-","Abraham","Lincoln",",","James","A.","Garfield",",","William","McKinley","and","John","F.","Kennedy","-","died","at","the","hands","of","assassins","."],"labels":["O","O","B-country","I-country","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["politician","political_party","person","event","country","organization","election","location"]}
{"id":"80","dataset":"crossner_politics","split":"test","instance":{"id":"80","prompt_labels":"((O) alternatively(O) of(O) Kingdom(B-country) of(I-country) England(I-country) ,(O) Kingdom(B-country) of(I-country) Great(I-country) Britain(I-country) or(O) the(O) United(B-country) Kingdom(I-country) of(I-country) Great(I-country) Britain(I-country) and(I-country) Ireland(I-country) depending(O) on(O) the(O) period(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, event, organization, person, country, election, political party and O.\nSentence: ( alternatively of Kingdom of England , Kingdom of Great Britain or the United Kingdom of Great Britain and Ireland depending on the period ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["(","alternatively","of","Kingdom","of","England",",","Kingdom","of","Great","Britain","or","the","United","Kingdom","of","Great","Britain","and","Ireland","depending","on","the","period",")","."],"labels":["O","O","O","B-country","I-country","I-country","O","B-country","I-country","I-country","I-country","O","O","B-country","I-country","I-country","I-country","I-country","I-country","I-country","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","politician","event","organization","person","country","election","political_party"]}
{"id":"83","dataset":"crossner_politics","split":"test","instance":{"id":"83","prompt_labels":"On(O) the(O) Coalition(O) side(O) of(O) politics(O) ,(O) Pru(B-politician) Goward(I-politician) has(O) served(O) as(O) a(O) Minister(O) in(O) the(O) NSW(B-political party) state(I-political party) Liberal(I-political party) Government(O) ,(O) Scott(B-politician) Emerson(I-politician) and(O) Sarah(B-politician) Henderson(I-politician) all(O) held(O) ,(O) or(O) hold(O) ,(O) positions(O) at(O) the(O) ABC(B-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, event, organization, person, election, political party, location and O.\nSentence: On the Coalition side of politics , Pru Goward has served as a Minister in the NSW state Liberal Government , Scott Emerson and Sarah Henderson all held , or hold , positions at the ABC .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","the","Coalition","side","of","politics",",","Pru","Goward","has","served","as","a","Minister","in","the","NSW","state","Liberal","Government",",","Scott","Emerson","and","Sarah","Henderson","all","held",",","or","hold",",","positions","at","the","ABC","."],"labels":["O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","B-organization","O"],"target_index":null,"target_label":null},"label_list":["politician","country","event","organization","person","election","political_party","location"]}
{"id":"93","dataset":"crossner_politics","split":"test","instance":{"id":"93","prompt_labels":"In(O) addition(O) ,(O) he(O) secured(O) the(O) release(O) of(O) two(O) American(O) journalists(O) imprisoned(O) by(O) North(B-country) Korea(I-country) ,(O) visiting(O) the(O) capital(O) Pyongyang(B-location) in(O) 2009(O) and(O) negotiating(O) their(O) release(O) with(O) then-North(O) Korean(O) leader(O) Kim(B-politician) Jong-il(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, election, political party, politician, country, person, location and O.\nSentence: In addition , he secured the release of two American journalists imprisoned by North Korea , visiting the capital Pyongyang in 2009 and negotiating their release with then-North Korean leader Kim Jong-il .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition",",","he","secured","the","release","of","two","American","journalists","imprisoned","by","North","Korea",",","visiting","the","capital","Pyongyang","in","2009","and","negotiating","their","release","with","then-North","Korean","leader","Kim","Jong-il","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","B-location","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["organization","event","election","political_party","politician","country","person","location"]}
{"id":"94","dataset":"crossner_politics","split":"test","instance":{"id":"94","prompt_labels":"In(O) the(O) 1996(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) ,(O) Clinton(B-politician) was(O) re-elected(O) ,(O) receiving(O) 49.2(O) percent(O) of(O) the(O) popular(O) vote(O) over(O) Republican(O) Bob(B-politician) Dole(I-politician) ((O) 40.7(O) percent(O) of(O) the(O) popular(O) vote(O) )(O) and(O) Reform(B-political party) Party(I-political party) of(I-political party) the(I-political party) United(I-political party) States(I-political party) of(I-political party) America(I-political party) candidate(O) Ross(B-politician) Perot(I-politician) ((O) 8.4(O) percent(O) of(O) the(O) popular(O) vote(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, country, event, organization, location, election, person and O.\nSentence: In the 1996 United States presidential election , Clinton was re-elected , receiving 49.2 percent of the popular vote over Republican Bob Dole ( 40.7 percent of the popular vote ) and Reform Party of the United States of America candidate Ross Perot ( 8.4 percent of the popular vote ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","1996","United","States","presidential","election",",","Clinton","was","re-elected",",","receiving","49.2","percent","of","the","popular","vote","over","Republican","Bob","Dole","(","40.7","percent","of","the","popular","vote",")","and","Reform","Party","of","the","United","States","of","America","candidate","Ross","Perot","(","8.4","percent","of","the","popular","vote",")","."],"labels":["O","O","B-election","I-election","I-election","I-election","I-election","O","B-politician","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","politician","country","event","organization","location","election","person"]}
{"id":"98","dataset":"crossner_politics","split":"test","instance":{"id":"98","prompt_labels":"In(O) the(O) last(O) 2014(B-election) European(I-election) Parliament(I-election) election(I-election) ,(O) the(O) Portuguese(B-political party) Communist(I-political party) Party(I-political party) won(O) three(O) seats(O) and(O) the(O) Left(B-political party) Bloc(I-political party) won(O) one(O) seat(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, election, organization, country, event, person, location and O.\nSentence: In the last 2014 European Parliament election , the Portuguese Communist Party won three seats and the Left Bloc won one seat .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","last","2014","European","Parliament","election",",","the","Portuguese","Communist","Party","won","three","seats","and","the","Left","Bloc","won","one","seat","."],"labels":["O","O","O","B-election","I-election","I-election","I-election","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","B-political party","I-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","politician","election","organization","country","event","person","location"]}
{"id":"99","dataset":"crossner_politics","split":"test","instance":{"id":"99","prompt_labels":"In(O) the(O) 2019(B-election) European(I-election) Parliament(I-election) election(I-election) in(O) Portugal(B-country) ,(O) Left(B-political party) Bloc(I-political party) took(O) 9.83(O) %(O) and(O) gained(O) 1(O) seat(O) ,(O) Portuguese(B-political party) Communist(I-political party) Party(I-political party) working(O) in(O) coalition(O) with(O) Ecologist(B-political party) Party(I-political party) ,(O) The(B-political party) Greens(I-political party) took(O) 6.88(O) %(O) and(O) 2(O) seats(O) and(O) National(B-political party) Renovator(I-political party) Party(I-political party) ((O) PNR(B-political party) )(O) polled(O) just(O) 0.49(O) %(O) ,(O) with(O) no(O) seats(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, election, political party, event, location, organization, politician and O.\nSentence: In the 2019 European Parliament election in Portugal , Left Bloc took 9.83 % and gained 1 seat , Portuguese Communist Party working in coalition with Ecologist Party , The Greens took 6.88 % and 2 seats and National Renovator Party ( PNR ) polled just 0.49 % , with no seats .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","2019","European","Parliament","election","in","Portugal",",","Left","Bloc","took","9.83","%","and","gained","1","seat",",","Portuguese","Communist","Party","working","in","coalition","with","Ecologist","Party",",","The","Greens","took","6.88","%","and","2","seats","and","National","Renovator","Party","(","PNR",")","polled","just","0.49","%",",","with","no","seats","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","B-country","O","B-political party","I-political party","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","O","B-political party","I-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","country","election","political_party","event","location","organization","politician"]}
{"id":"109","dataset":"crossner_politics","split":"test","instance":{"id":"109","prompt_labels":"He(O) further(O) sought(O) the(O) Republican(O) nomination(O) for(O) president(O) in(O) 1964(B-election) Republican(I-election) Party(I-election) presidential(I-election) primaries(I-election) ,(O) 1968(B-election) Republican(I-election) Party(I-election) presidential(I-election) primaries(I-election) ,(O) 1976(B-election) Republican(I-election) Party(I-election) presidential(I-election) primaries(I-election) ,(O) 1980(B-election) Republican(I-election) Party(I-election) presidential(I-election) primaries(I-election) ,(O) 1984(B-election) Republican(I-election) Party(I-election) presidential(I-election) primaries(I-election) ,(O) 1988(B-election) Republican(I-election) Party(I-election) presidential(I-election) primaries(I-election) ,(O) and(O) 1992(B-election) Republican(I-election) Party(I-election) presidential(I-election) primaries(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, country, election, person, location, political party, event and O.\nSentence: He further sought the Republican nomination for president in 1964 Republican Party presidential primaries , 1968 Republican Party presidential primaries , 1976 Republican Party presidential primaries , 1980 Republican Party presidential primaries , 1984 Republican Party presidential primaries , 1988 Republican Party presidential primaries , and 1992 Republican Party presidential primaries .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","further","sought","the","Republican","nomination","for","president","in","1964","Republican","Party","presidential","primaries",",","1968","Republican","Party","presidential","primaries",",","1976","Republican","Party","presidential","primaries",",","1980","Republican","Party","presidential","primaries",",","1984","Republican","Party","presidential","primaries",",","1988","Republican","Party","presidential","primaries",",","and","1992","Republican","Party","presidential","primaries","."],"labels":["O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["organization","politician","country","election","person","location","political_party","event"]}
{"id":"120","dataset":"crossner_politics","split":"test","instance":{"id":"120","prompt_labels":"It(O) has(O) several(O) facilities(O) spread(O) across(O) India(B-country) including(O) Bengaluru(B-location) ,(O) Nasik(B-location) ,(O) Korwa(B-location) ,(O) Kanpur(B-location) ,(O) Koraput(B-location) ,(O) Lucknow(B-location) ,(O) Hyderabad(B-location) and(O) Kasaragod(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, politician, event, country, location, election, person and O.\nSentence: It has several facilities spread across India including Bengaluru , Nasik , Korwa , Kanpur , Koraput , Lucknow , Hyderabad and Kasaragod .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","has","several","facilities","spread","across","India","including","Bengaluru",",","Nasik",",","Korwa",",","Kanpur",",","Koraput",",","Lucknow",",","Hyderabad","and","Kasaragod","."],"labels":["O","O","O","O","O","O","B-country","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["political_party","organization","politician","event","country","location","election","person"]}
{"id":"139","dataset":"crossner_politics","split":"test","instance":{"id":"139","prompt_labels":"Ford(B-person) worked(O) as(O) a(O) managing(O) editor(O) and(O) editorial(O) writer(O) from(O) 1872(O) to(O) 1905(O) ,(O) at(O) six(O) different(O) newspapers(O) in(O) three(O) cities(O) Baltimore(B-location) ,(O) New(B-location) York(I-location) City(I-location) ,(O) and(O) Pittsburgh(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, location, country, person, politician, election, organization and O.\nSentence: Ford worked as a managing editor and editorial writer from 1872 to 1905 , at six different newspapers in three cities Baltimore , New York City , and Pittsburgh .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ford","worked","as","a","managing","editor","and","editorial","writer","from","1872","to","1905",",","at","six","different","newspapers","in","three","cities","Baltimore",",","New","York","City",",","and","Pittsburgh","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","I-location","I-location","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["event","political_party","location","country","person","politician","election","organization"]}
{"id":"145","dataset":"crossner_politics","split":"test","instance":{"id":"145","prompt_labels":"The(O) 2017(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) produced(O) a(O) mixed(O) result(O) for(O) the(O) party(O) as(O) it(O) gained(O) six(O) seat(O) and(O) increased(O) its(O) vote(O) by(O) 2.8(O) %(O) but(O) the(O) party(O) came(O) in(O) third(O) behind(O) the(O) Scottish(B-political party) National(I-political party) Party(I-political party) and(O) Scottish(B-political party) Conservatives(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, organization, country, person, location, politician, political party, event and O.\nSentence: The 2017 United Kingdom general election produced a mixed result for the party as it gained six seat and increased its vote by 2.8 % but the party came in third behind the Scottish National Party and Scottish Conservatives .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","2017","United","Kingdom","general","election","produced","a","mixed","result","for","the","party","as","it","gained","six","seat","and","increased","its","vote","by","2.8","%","but","the","party","came","in","third","behind","the","Scottish","National","Party","and","Scottish","Conservatives","."],"labels":["O","B-election","I-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["election","organization","country","person","location","politician","political_party","event"]}
{"id":"147","dataset":"crossner_politics","split":"test","instance":{"id":"147","prompt_labels":"Most(O) famously(O) ,(O) the(O) Houses(O) of(O) House(B-organization) of(I-organization) York(I-organization) and(O) House(B-organization) of(I-organization) Lancaster(I-organization) ,(O) whose(O) feuding(O) over(O) the(O) succession(O) to(O) the(O) English(O) throne(O) after(O) the(O) end(O) of(O) the(O) main(O) line(O) of(O) the(O) House(B-organization) of(I-organization) Plantagenet(I-organization) caused(O) the(O) Wars(B-event) of(I-event) the(I-event) Roses(I-event) ,(O) were(O) both(O) established(O) when(O) the(O) Duchies(O) of(O) York(O) and(O) Lancaster(O) were(O) given(O) as(O) appanages(O) for(O) Edmund(B-person) of(I-person) Langley(I-person) and(O) John(B-person) of(I-person) Gaunt(I-person) ,(O) the(O) younger(O) sons(O) of(O) King(O) Edward(B-person) III(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, election, organization, location, politician, person, event, country and O.\nSentence: Most famously , the Houses of House of York and House of Lancaster , whose feuding over the succession to the English throne after the end of the main line of the House of Plantagenet caused the Wars of the Roses , were both established when the Duchies of York and Lancaster were given as appanages for Edmund of Langley and John of Gaunt , the younger sons of King Edward III .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Most","famously",",","the","Houses","of","House","of","York","and","House","of","Lancaster",",","whose","feuding","over","the","succession","to","the","English","throne","after","the","end","of","the","main","line","of","the","House","of","Plantagenet","caused","the","Wars","of","the","Roses",",","were","both","established","when","the","Duchies","of","York","and","Lancaster","were","given","as","appanages","for","Edmund","of","Langley","and","John","of","Gaunt",",","the","younger","sons","of","King","Edward","III","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-event","I-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","O","B-person","I-person","I-person","O","O","O","O","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["political_party","election","organization","location","politician","person","event","country"]}
{"id":"149","dataset":"crossner_politics","split":"test","instance":{"id":"149","prompt_labels":"When(O) Liu(B-person) Bei(I-person) refused(O) ,(O) Sun(B-person) Quan(I-person) ordered(O) Lü(B-person) Meng(I-person) to(O) lead(O) troops(O) to(O) seize(O) three(O) commanderies(O) -(O) Changsha(B-location) ((O) 長沙(B-location) )(O) ,(O) Lingling(B-location) ((O) 零陵(B-location) ;(O) around(O) present-day(O) Yongzhou(B-location) ,(O) Hunan(B-location) )(O) and(O) Guiyang(B-location) ((O) 桂陽(B-location) ;(O) around(O) present-day(O) Chenzhou(B-location) ,(O) Hunan(B-location) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, country, political party, organization, location, election, politician and O.\nSentence: When Liu Bei refused , Sun Quan ordered Lü Meng to lead troops to seize three commanderies - Changsha ( 長沙 ) , Lingling ( 零陵 ; around present-day Yongzhou , Hunan ) and Guiyang ( 桂陽 ; around present-day Chenzhou , Hunan ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["When","Liu","Bei","refused",",","Sun","Quan","ordered","Lü","Meng","to","lead","troops","to","seize","three","commanderies","-","Changsha","(","長沙",")",",","Lingling","(","零陵",";","around","present-day","Yongzhou",",","Hunan",")","and","Guiyang","(","桂陽",";","around","present-day","Chenzhou",",","Hunan",")","."],"labels":["O","B-person","I-person","O","O","B-person","I-person","O","B-person","I-person","O","O","O","O","O","O","O","O","B-location","O","B-location","O","O","B-location","O","B-location","O","O","O","B-location","O","B-location","O","O","B-location","O","B-location","O","O","O","B-location","O","B-location","O","O"],"target_index":null,"target_label":null},"label_list":["event","person","country","political_party","organization","location","election","politician"]}
{"id":"152","dataset":"crossner_politics","split":"test","instance":{"id":"152","prompt_labels":"New(B-country) Zealand(I-country) 's(O) 48th(B-organization) Parliament(I-organization) operated(O) with(O) both(O) a(O) coalition(O) and(O) a(O) looser(O) agreement(O) :(O) the(O) government(O) was(O) a(O) coalition(O) between(O) the(O) New(B-political party) Zealand(I-political party) Labour(I-political party) Party(I-political party) and(O) the(O) Progressives(B-political party) ,(O) while(O) United(B-political party) Future(I-political party) and(O) New(B-political party) Zealand(I-political party) First(I-political party) had(O) an(O) agreement(O) to(O) support(O) the(O) government(O) on(O) confidence(O) matters(O) ,(O) while(O) the(O) Green(B-political party) Party(I-political party) abstained(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, event, political party, politician, person, location, election, country and O.\nSentence: New Zealand 's 48th Parliament operated with both a coalition and a looser agreement : the government was a coalition between the New Zealand Labour Party and the Progressives , while United Future and New Zealand First had an agreement to support the government on confidence matters , while the Green Party abstained .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["New","Zealand","'s","48th","Parliament","operated","with","both","a","coalition","and","a","looser","agreement",":","the","government","was","a","coalition","between","the","New","Zealand","Labour","Party","and","the","Progressives",",","while","United","Future","and","New","Zealand","First","had","an","agreement","to","support","the","government","on","confidence","matters",",","while","the","Green","Party","abstained","."],"labels":["B-country","I-country","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-political party","O","O","B-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O"],"target_index":null,"target_label":null},"label_list":["organization","event","political_party","politician","person","location","election","country"]}
{"id":"160","dataset":"crossner_politics","split":"test","instance":{"id":"160","prompt_labels":"The(O) current(O) prime(O) minister(O) of(O) Denmark(O) is(O) Mette(B-politician) Frederiksen(I-politician) who(O) ,(O) since(O) 27(O) June(O) 2019(O) ,(O) has(O) led(O) a(O) one-party(O) government(O) consisting(O) of(O) the(O) Social(O) Democrats(O) with(O) parliamentary(O) support(O) from(O) the(O) Danish(B-political party) Social(I-political party) Liberal(I-political party) Party(I-political party) ,(O) Socialist(B-political party) People(I-political party) 's(I-political party) Party(I-political party) ,(O) Red-Green(B-political party) Alliance(I-political party) ,(O) the(O) Faroese(O) Social(B-political party) Democratic(I-political party) Party(I-political party) and(O) Greenland(B-location) '(O) s(O) Inuit(B-political party) Ataqatigiit(I-political party) and(O) Siumut(B-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, event, election, country, political party, politician, organization and O.\nSentence: The current prime minister of Denmark is Mette Frederiksen who , since 27 June 2019 , has led a one-party government consisting of the Social Democrats with parliamentary support from the Danish Social Liberal Party , Socialist People 's Party , Red-Green Alliance , the Faroese Social Democratic Party and Greenland ' s Inuit Ataqatigiit and Siumut .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","current","prime","minister","of","Denmark","is","Mette","Frederiksen","who",",","since","27","June","2019",",","has","led","a","one-party","government","consisting","of","the","Social","Democrats","with","parliamentary","support","from","the","Danish","Social","Liberal","Party",",","Socialist","People","'s","Party",",","Red-Green","Alliance",",","the","Faroese","Social","Democratic","Party","and","Greenland","'","s","Inuit","Ataqatigiit","and","Siumut","."],"labels":["O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","O","B-location","O","O","B-political party","I-political party","O","B-political party","O"],"target_index":null,"target_label":null},"label_list":["location","person","event","election","country","political_party","politician","organization"]}
{"id":"185","dataset":"crossner_politics","split":"test","instance":{"id":"185","prompt_labels":"He(O) was(O) educated(O) at(O) Congregation(B-organization) of(I-organization) Christian(I-organization) Brothers(I-organization) in(O) Synge(B-location) Street(I-location) ,(O) Dublin(B-location) ;(O) University(B-organization) College(I-organization) Dublin(I-organization) and(O) Harvard(B-organization) Business(I-organization) School(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, event, person, location, organization, election, politician, country and O.\nSentence: He was educated at Congregation of Christian Brothers in Synge Street , Dublin ; University College Dublin and Harvard Business School .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","educated","at","Congregation","of","Christian","Brothers","in","Synge","Street",",","Dublin",";","University","College","Dublin","and","Harvard","Business","School","."],"labels":["O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-location","I-location","O","B-location","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["political_party","event","person","location","organization","election","politician","country"]}
{"id":"188","dataset":"crossner_politics","split":"test","instance":{"id":"188","prompt_labels":"After(O) serving(O) two(O) terms(O) in(O) the(O) New(B-organization) Hampshire(I-organization) Senate(I-organization) ,(O) Shaheen(B-politician) was(O) elected(O) governor(O) in(O) 1996(B-election) New(I-election) Hampshire(I-election) gubernatorial(I-election) election(I-election) and(O) reelected(O) in(O) 1998(B-election) New(I-election) Hampshire(I-election) gubernatorial(I-election) election(I-election) and(O) 2000(B-election) New(I-election) Hampshire(I-election) gubernatorial(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, organization, event, location, political party, person, election and O.\nSentence: After serving two terms in the New Hampshire Senate , Shaheen was elected governor in 1996 New Hampshire gubernatorial election and reelected in 1998 New Hampshire gubernatorial election and 2000 New Hampshire gubernatorial election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","serving","two","terms","in","the","New","Hampshire","Senate",",","Shaheen","was","elected","governor","in","1996","New","Hampshire","gubernatorial","election","and","reelected","in","1998","New","Hampshire","gubernatorial","election","and","2000","New","Hampshire","gubernatorial","election","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-politician","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["politician","country","organization","event","location","political_party","person","election"]}
{"id":"191","dataset":"crossner_politics","split":"test","instance":{"id":"191","prompt_labels":"This(O) tends(O) to(O) lead(O) to(O) the(O) chamber(O) being(O) dominated(O) by(O) two(O) major(O) political(O) groups(O) ,(O) the(O) centre-right(O) Coalition(O) ((O) consisting(O) of(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) and(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) Parties(O) )(O) and(O) the(O) centre-left(O) Australian(B-political party) Labor(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, political party, country, person, election, politician, organization and O.\nSentence: This tends to lead to the chamber being dominated by two major political groups , the centre-right Coalition ( consisting of the Liberal Party of Australia and National Party of Australia Parties ) and the centre-left Australian Labor Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","tends","to","lead","to","the","chamber","being","dominated","by","two","major","political","groups",",","the","centre-right","Coalition","(","consisting","of","the","Liberal","Party","of","Australia","and","National","Party","of","Australia","Parties",")","and","the","centre-left","Australian","Labor","Party","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["event","location","political_party","country","person","election","politician","organization"]}
{"id":"194","dataset":"crossner_politics","split":"test","instance":{"id":"194","prompt_labels":"Throughout(O) the(O) 2000s(O) ((O) decade(O) )(O) ,(O) Thai(O) aggressively(O) continued(O) its(O) route(O) network(O) expansion(O) with(O) new(O) services(O) to(O) Chengdu(B-location) ,(O) Busan(B-location) ,(O) Chennai(B-location) ,(O) Xiamen(B-location) ,(O) Milan(B-location) ,(O) Moscow(B-location) ,(O) Islamabad(B-location) ,(O) Hyderabad(B-location) ,(O) Johannesburg(B-location) ((O) later(O) suspended(O) )(O) and(O) Oslo(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, organization, election, politician, person, event, location, political party and O.\nSentence: Throughout the 2000s ( decade ) , Thai aggressively continued its route network expansion with new services to Chengdu , Busan , Chennai , Xiamen , Milan , Moscow , Islamabad , Hyderabad , Johannesburg ( later suspended ) and Oslo .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Throughout","the","2000s","(","decade",")",",","Thai","aggressively","continued","its","route","network","expansion","with","new","services","to","Chengdu",",","Busan",",","Chennai",",","Xiamen",",","Milan",",","Moscow",",","Islamabad",",","Hyderabad",",","Johannesburg","(","later","suspended",")","and","Oslo","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","O","O","O","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["country","organization","election","politician","person","event","location","political_party"]}
{"id":"200","dataset":"crossner_politics","split":"test","instance":{"id":"200","prompt_labels":"It(O) also(O) became(O) a(O) member(O) of(O) the(O) March(B-political party) 14(I-political party) Alliance(I-political party) ,(O) along(O) with(O) the(O) Future(B-political party) Movement(I-political party) ,(O) Progressive(B-political party) Socialist(I-political party) Party(I-political party) ,(O) Lebanese(B-political party) Forces(I-political party) and(O) other(O) minor(O) parties(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, politician, country, event, political party, election, organization and O.\nSentence: It also became a member of the March 14 Alliance , along with the Future Movement , Progressive Socialist Party , Lebanese Forces and other minor parties .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","also","became","a","member","of","the","March","14","Alliance",",","along","with","the","Future","Movement",",","Progressive","Socialist","Party",",","Lebanese","Forces","and","other","minor","parties","."],"labels":["O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","person","politician","country","event","political_party","election","organization"]}
{"id":"202","dataset":"crossner_politics","split":"test","instance":{"id":"202","prompt_labels":"Chandrika(B-politician) Kumaratunga(I-politician) briefly(O) took(O) over(O) the(O) leadership(O) of(O) her(O) husband(O) 's(O) party(O) ,(O) and(O) formed(O) the(O) United(B-political party) Socialist(I-political party) Alliance(I-political party) with(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Sri(I-political party) Lanka(I-political party) ,(O) the(O) Lanka(B-political party) Sama(I-political party) Samaja(I-political party) Party(I-political party) ,(O) and(O) the(O) Nava(B-political party) Sama(I-political party) Samaja(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, organization, person, country, politician, political party, election and O.\nSentence: Chandrika Kumaratunga briefly took over the leadership of her husband 's party , and formed the United Socialist Alliance with the Communist Party of Sri Lanka , the Lanka Sama Samaja Party , and the Nava Sama Samaja Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Chandrika","Kumaratunga","briefly","took","over","the","leadership","of","her","husband","'s","party",",","and","formed","the","United","Socialist","Alliance","with","the","Communist","Party","of","Sri","Lanka",",","the","Lanka","Sama","Samaja","Party",",","and","the","Nava","Sama","Samaja","Party","."],"labels":["B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["event","location","organization","person","country","politician","political_party","election"]}
{"id":"204","dataset":"crossner_politics","split":"test","instance":{"id":"204","prompt_labels":"In(O) the(O) end(O) ,(O) there(O) were(O) three(O) candidates(O) in(O) the(O) party(O) 's(O) 2004(B-election) Conservative(I-election) Party(I-election) of(I-election) Canada(I-election) leadership(I-election) election(I-election) :(O) former(O) Canadian(B-political party) Alliance(I-political party) leader(O) Stephen(B-politician) Harper(I-politician) ,(O) former(O) Magna(B-organization) International(I-organization) CEO(O) Belinda(B-politician) Stronach(I-politician) ,(O) and(O) former(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Ontario(I-political party) Cabinet(O) minister(O) Tony(B-politician) Clement(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, country, election, person, organization, political party, event and O.\nSentence: In the end , there were three candidates in the party 's 2004 Conservative Party of Canada leadership election : former Canadian Alliance leader Stephen Harper , former Magna International CEO Belinda Stronach , and former Progressive Conservative Party of Ontario Cabinet minister Tony Clement .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","end",",","there","were","three","candidates","in","the","party","'s","2004","Conservative","Party","of","Canada","leadership","election",":","former","Canadian","Alliance","leader","Stephen","Harper",",","former","Magna","International","CEO","Belinda","Stronach",",","and","former","Progressive","Conservative","Party","of","Ontario","Cabinet","minister","Tony","Clement","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","B-political party","I-political party","O","B-politician","I-politician","O","O","B-organization","I-organization","O","B-politician","I-politician","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["politician","location","country","election","person","organization","political_party","event"]}
{"id":"208","dataset":"crossner_politics","split":"test","instance":{"id":"208","prompt_labels":"Most(O) sitting(O) UFA(B-political party) MPs(O) joined(O) the(O) Co-operative(B-political party) Commonwealth(I-political party) Federation(I-political party) party(O) ,(O) and(O) all(O) the(O) UFA(B-political party) MPs(O) were(O) defeated(O) at(O) the(O) polls(O) in(O) the(O) 1935(B-election) Canadian(I-election) federal(I-election) election(I-election) by(O) the(O) Social(B-political party) Credit(I-political party) Party(I-political party) of(I-political party) Canada(I-political party) political(O) landslide(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, organization, country, politician, location, person, election and O.\nSentence: Most sitting UFA MPs joined the Co-operative Commonwealth Federation party , and all the UFA MPs were defeated at the polls in the 1935 Canadian federal election by the Social Credit Party of Canada political landslide .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Most","sitting","UFA","MPs","joined","the","Co-operative","Commonwealth","Federation","party",",","and","all","the","UFA","MPs","were","defeated","at","the","polls","in","the","1935","Canadian","federal","election","by","the","Social","Credit","Party","of","Canada","political","landslide","."],"labels":["O","O","B-political party","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","B-political party","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","political_party","organization","country","politician","location","person","election"]}
{"id":"215","dataset":"crossner_politics","split":"test","instance":{"id":"215","prompt_labels":"The(O) UFP(B-political party) presented(O) itself(O) as(O) an(O) alternative(O) to(O) the(O) main(O) three(O) parties(O) in(O) Québec(B-location) :(O) the(O) centre-left(O) Parti(B-political party) Québécois(I-political party) ,(O) the(O) centre-right(O) Quebec(B-political party) Liberal(I-political party) Party(I-political party) ,(O) and(O) the(O) conservative(O) Action(B-political party) démocratique(I-political party) du(I-political party) Québec(I-political party) /(O) Equipe(O) Mario(B-politician) Dumont(I-politician) ,(O) saying(O) that(O) all(O) three(O) are(O) but(O) different(O) faces(O) of(O) the(O) same(O) right-wing(O) ideology(O) called(O) neoliberalism(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, political party, event, person, election, location, country and O.\nSentence: The UFP presented itself as an alternative to the main three parties in Québec : the centre-left Parti Québécois , the centre-right Quebec Liberal Party , and the conservative Action démocratique du Québec / Equipe Mario Dumont , saying that all three are but different faces of the same right-wing ideology called neoliberalism .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","UFP","presented","itself","as","an","alternative","to","the","main","three","parties","in","Québec",":","the","centre-left","Parti","Québécois",",","the","centre-right","Quebec","Liberal","Party",",","and","the","conservative","Action","démocratique","du","Québec","/","Equipe","Mario","Dumont",",","saying","that","all","three","are","but","different","faces","of","the","same","right-wing","ideology","called","neoliberalism","."],"labels":["O","B-political party","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","O","B-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","politician","political_party","event","person","election","location","country"]}
{"id":"217","dataset":"crossner_politics","split":"test","instance":{"id":"217","prompt_labels":"The(O) NDP(B-political party) focused(O) the(O) campaign(O) on(O) winning(O) ridings(O) in(O) Canada(B-country) 's(O) urban(O) centres(O) ,(O) hoping(O) especially(O) to(O) win(O) seats(O) in(O) central(O) Toronto(B-location) ,(O) Hamilton(B-location) ,(O) Ottawa(B-location) and(O) Winnipeg(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, organization, election, person, location, event, country and O.\nSentence: The NDP focused the campaign on winning ridings in Canada 's urban centres , hoping especially to win seats in central Toronto , Hamilton , Ottawa and Winnipeg .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","NDP","focused","the","campaign","on","winning","ridings","in","Canada","'s","urban","centres",",","hoping","especially","to","win","seats","in","central","Toronto",",","Hamilton",",","Ottawa","and","Winnipeg","."],"labels":["O","B-political party","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["political_party","politician","organization","election","person","location","event","country"]}
{"id":"225","dataset":"crossner_politics","split":"test","instance":{"id":"225","prompt_labels":"The(O) National(B-political party) Union(I-political party) party(I-political party) won(O) seven(O) seats(O) ,(O) and(O) was(O) included(O) in(O) Ariel(B-politician) Sharon(I-politician) 's(O) coalition(O) ,(O) alongside(O) Likud(B-political party) ,(O) Shinui(B-political party) ,(O) the(O) National(B-political party) Religious(I-political party) Party(I-political party) ,(O) and(O) Yisrael(B-political party) BaAliyah(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, person, organization, politician, location, country, event and O.\nSentence: The National Union party won seven seats , and was included in Ariel Sharon 's coalition , alongside Likud , Shinui , the National Religious Party , and Yisrael BaAliyah .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","National","Union","party","won","seven","seats",",","and","was","included","in","Ariel","Sharon","'s","coalition",",","alongside","Likud",",","Shinui",",","the","National","Religious","Party",",","and","Yisrael","BaAliyah","."],"labels":["O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","B-political party","O","B-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["election","political_party","person","organization","politician","location","country","event"]}
{"id":"227","dataset":"crossner_politics","split":"test","instance":{"id":"227","prompt_labels":"He(O) resigned(O) from(O) the(O) governorship(O) to(O) briefly(O) co-lead(O) the(O) Sunrise(B-political party) Party(I-political party) ,(O) then(O) joined(O) the(O) Japan(B-political party) Restoration(I-political party) Party(I-political party) and(O) returned(O) to(O) the(O) House(B-organization) of(I-organization) Representatives(I-organization) in(O) the(O) 2012(B-election) Japanese(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, country, location, organization, political party, politician, event, election and O.\nSentence: He resigned from the governorship to briefly co-lead the Sunrise Party , then joined the Japan Restoration Party and returned to the House of Representatives in the 2012 Japanese general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","resigned","from","the","governorship","to","briefly","co-lead","the","Sunrise","Party",",","then","joined","the","Japan","Restoration","Party","and","returned","to","the","House","of","Representatives","in","the","2012","Japanese","general","election","."],"labels":["O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["person","country","location","organization","political_party","politician","event","election"]}
{"id":"231","dataset":"crossner_politics","split":"test","instance":{"id":"231","prompt_labels":"ALP(B-political party) =(O) Australian(B-political party) Labor(I-political party) Party(I-political party) ,(O) L(O) +(O) NP(O) =(O) grouping(O) of(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) /(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) /(O) Liberal(B-political party) National(I-political party) Party(I-political party) of(I-political party) Queensland(I-political party) /(O) Country(B-political party) Liberal(I-political party) Party(I-political party) Coalition(O) parties(O) ((O) and(O) predecessors(O) )(O) ,(O) Oth(O) =(O) other(O) parties(O) and(O) independents(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, event, country, location, political party, person, politician and O.\nSentence: ALP = Australian Labor Party , L + NP = grouping of Liberal Party of Australia / National Party of Australia / Liberal National Party of Queensland / Country Liberal Party Coalition parties ( and predecessors ) , Oth = other parties and independents .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["ALP","=","Australian","Labor","Party",",","L","+","NP","=","grouping","of","Liberal","Party","of","Australia","/","National","Party","of","Australia","/","Liberal","National","Party","of","Queensland","/","Country","Liberal","Party","Coalition","parties","(","and","predecessors",")",",","Oth","=","other","parties","and","independents","."],"labels":["B-political party","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","election","event","country","location","political_party","person","politician"]}
{"id":"255","dataset":"crossner_politics","split":"test","instance":{"id":"255","prompt_labels":"This(O) is(O) because(O) Sendai(B-location) is(O) near(O) several(O) major(O) fishing(O) ports(O) ,(O) such(O) as(O) Kesennuma(B-location) ,(O) Ishinomaki(B-location) ,(O) and(O) Shiogama(B-location) ,(O) and(O) the(O) fact(O) that(O) Miyagi(B-location) Prefecture(I-location) is(O) a(O) major(O) producer(O) of(O) rice(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, country, election, location, organization, person, political party, politician and O.\nSentence: This is because Sendai is near several major fishing ports , such as Kesennuma , Ishinomaki , and Shiogama , and the fact that Miyagi Prefecture is a major producer of rice .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","is","because","Sendai","is","near","several","major","fishing","ports",",","such","as","Kesennuma",",","Ishinomaki",",","and","Shiogama",",","and","the","fact","that","Miyagi","Prefecture","is","a","major","producer","of","rice","."],"labels":["O","O","O","B-location","O","O","O","O","O","O","O","O","O","B-location","O","B-location","O","O","B-location","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","country","election","location","organization","person","political_party","politician"]}
{"id":"258","dataset":"crossner_politics","split":"test","instance":{"id":"258","prompt_labels":"GO(B-organization) Transit(I-organization) 's(O) lines(O) extend(O) into(O) the(O) nearby(O) Niagara(B-location) and(O) Waterloo(B-location) Regions(O) ,(O) the(O) cities(O) of(O) Brantford(B-location) and(O) Peterborough(B-location) ,(O) Simcoe(B-location) ,(O) Dufferin(B-location) and(O) Wellington(B-location) Counties(O) an(O) area(O) largely(O) coextensive(O) with(O) the(O) Greater(B-location) Golden(I-location) Horseshoe(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, election, event, politician, country, organization, political party and O.\nSentence: GO Transit 's lines extend into the nearby Niagara and Waterloo Regions , the cities of Brantford and Peterborough , Simcoe , Dufferin and Wellington Counties an area largely coextensive with the Greater Golden Horseshoe .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["GO","Transit","'s","lines","extend","into","the","nearby","Niagara","and","Waterloo","Regions",",","the","cities","of","Brantford","and","Peterborough",",","Simcoe",",","Dufferin","and","Wellington","Counties","an","area","largely","coextensive","with","the","Greater","Golden","Horseshoe","."],"labels":["B-organization","I-organization","O","O","O","O","O","O","B-location","O","B-location","O","O","O","O","O","B-location","O","B-location","O","B-location","O","B-location","O","B-location","O","O","O","O","O","O","O","B-location","I-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["person","location","election","event","politician","country","organization","political_party"]}
{"id":"266","dataset":"crossner_politics","split":"test","instance":{"id":"266","prompt_labels":"Hampton(B-politician) sought(O) election(O) to(O) the(O) Ontario(B-organization) legislature(I-organization) under(O) the(O) NDP(B-political party) banner(O) in(O) the(O) 1977(B-election) Ontario(I-election) general(I-election) election(I-election) ,(O) placing(O) third(O) against(O) incumbent(O) Ontario(B-political party) Liberal(I-political party) Party(I-political party) T.(B-politician) Patrick(I-politician) Reid(I-politician) and(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Ontario(I-political party) Gordon(B-politician) Armstrong(I-politician) in(O) Rainy(B-location) River(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, event, country, election, location, person, political party and O.\nSentence: Hampton sought election to the Ontario legislature under the NDP banner in the 1977 Ontario general election , placing third against incumbent Ontario Liberal Party T. Patrick Reid and Progressive Conservative Party of Ontario Gordon Armstrong in Rainy River .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Hampton","sought","election","to","the","Ontario","legislature","under","the","NDP","banner","in","the","1977","Ontario","general","election",",","placing","third","against","incumbent","Ontario","Liberal","Party","T.","Patrick","Reid","and","Progressive","Conservative","Party","of","Ontario","Gordon","Armstrong","in","Rainy","River","."],"labels":["B-politician","O","O","O","O","B-organization","I-organization","O","O","B-political party","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","B-political party","I-political party","I-political party","B-politician","I-politician","I-politician","O","B-political party","I-political party","I-political party","I-political party","I-political party","B-politician","I-politician","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["politician","organization","event","country","election","location","person","political_party"]}
{"id":"277","dataset":"crossner_politics","split":"test","instance":{"id":"277","prompt_labels":"The(O) party(O) was(O) established(O) in(O) 1997(O) by(O) a(O) coalition(O) of(O) former(O) provincial(O) Progressive(B-political party) Conservative(I-political party) Party(I-political party) of(I-political party) Saskatchewan(I-political party) and(O) Saskatchewan(B-political party) Liberal(I-political party) Party(I-political party) party(O) members(O) and(O) supporters(O) who(O) sought(O) to(O) remove(O) the(O) Saskatchewan(B-political party) New(I-political party) Democratic(I-political party) Party(I-political party) ((O) NDP(B-political party) )(O) from(O) power(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, politician, country, political party, election, person, organization and O.\nSentence: The party was established in 1997 by a coalition of former provincial Progressive Conservative Party of Saskatchewan and Saskatchewan Liberal Party party members and supporters who sought to remove the Saskatchewan New Democratic Party ( NDP ) from power .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","party","was","established","in","1997","by","a","coalition","of","former","provincial","Progressive","Conservative","Party","of","Saskatchewan","and","Saskatchewan","Liberal","Party","party","members","and","supporters","who","sought","to","remove","the","Saskatchewan","New","Democratic","Party","(","NDP",")","from","power","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","location","politician","country","political_party","election","person","organization"]}
{"id":"281","dataset":"crossner_politics","split":"test","instance":{"id":"281","prompt_labels":"Liebknecht(B-politician) was(O) a(O) member(O) of(O) the(O) Independent(B-political party) Social(I-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Germany(I-political party) ((O) USPD(B-political party) )(O) ,(O) opposed(O) to(O) the(O) merger(O) with(O) the(O) Communist(B-political party) Party(I-political party) of(I-political party) Germany(I-political party) and(O) the(O) joining(O) of(O) the(O) Comintern(B-organization) but(O) also(O) to(O) the(O) reunification(O) of(O) the(O) party(O) with(O) the(O) Social(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Germany(I-political party) ,(O) he(O) continued(O) the(O) USPD(B-political party) as(O) an(O) independent(O) party(O) with(O) Georg(B-politician) Ledebour(I-politician) until(O) its(O) merger(O) into(O) the(O) Sozialistische(B-political party) Arbeiterpartei(I-political party) Deutschlands(I-political party) ((O) SAPD(B-political party) ,(O) Socialist(B-political party) Worker(I-political party) 's(I-political party) Party(I-political party) of(I-political party) Germany(I-political party) )(O) in(O) 1931(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, event, political party, country, person, politician, location and O.\nSentence: Liebknecht was a member of the Independent Social Democratic Party of Germany ( USPD ) , opposed to the merger with the Communist Party of Germany and the joining of the Comintern but also to the reunification of the party with the Social Democratic Party of Germany , he continued the USPD as an independent party with Georg Ledebour until its merger into the Sozialistische Arbeiterpartei Deutschlands ( SAPD , Socialist Worker 's Party of Germany ) in 1931 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Liebknecht","was","a","member","of","the","Independent","Social","Democratic","Party","of","Germany","(","USPD",")",",","opposed","to","the","merger","with","the","Communist","Party","of","Germany","and","the","joining","of","the","Comintern","but","also","to","the","reunification","of","the","party","with","the","Social","Democratic","Party","of","Germany",",","he","continued","the","USPD","as","an","independent","party","with","Georg","Ledebour","until","its","merger","into","the","Sozialistische","Arbeiterpartei","Deutschlands","(","SAPD",",","Socialist","Worker","'s","Party","of","Germany",")","in","1931","."],"labels":["B-politician","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O","B-political party","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","election","event","political_party","country","person","politician","location"]}
{"id":"285","dataset":"crossner_politics","split":"test","instance":{"id":"285","prompt_labels":"It(O) did(O) not(O) stand(O) candidates(O) in(O) the(O) Māori(O) electorates(O) in(O) the(O) 2002(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) 2005(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) ,(O) or(O) 2008(B-election) New(I-election) Zealand(I-election) general(I-election) elections(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, person, event, location, country, politician, political party, election and O.\nSentence: It did not stand candidates in the Māori electorates in the 2002 New Zealand general election , 2005 New Zealand general election , or 2008 New Zealand general elections .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","did","not","stand","candidates","in","the","Māori","electorates","in","the","2002","New","Zealand","general","election",",","2005","New","Zealand","general","election",",","or","2008","New","Zealand","general","elections","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["organization","person","event","location","country","politician","political_party","election"]}
{"id":"286","dataset":"crossner_politics","split":"test","instance":{"id":"286","prompt_labels":"In(O) the(O) only(O) election(O) held(O) in(O) Zimbabwe(B-location) Rhodesia(I-location) ,(O) Bishop(O) Abel(B-politician) Muzorewa(I-politician) '(O) s(O) United(B-political party) African(I-political party) National(I-political party) Council(I-political party) ((O) United(B-political party) African(I-political party) National(I-political party) Council(I-political party) )(O) won(O) a(O) majority(O) in(O) the(O) common-roll(O) seats(O) ,(O) while(O) Ian(B-politician) Smith(I-politician) 's(O) Rhodesian(B-political party) Front(I-political party) ((O) RF(B-political party) )(O) won(O) all(O) of(O) the(O) old(O) voter(O) roll(O) seats(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, country, election, person, political party, event, organization and O.\nSentence: In the only election held in Zimbabwe Rhodesia , Bishop Abel Muzorewa ' s United African National Council ( United African National Council ) won a majority in the common-roll seats , while Ian Smith 's Rhodesian Front ( RF ) won all of the old voter roll seats .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","only","election","held","in","Zimbabwe","Rhodesia",",","Bishop","Abel","Muzorewa","'","s","United","African","National","Council","(","United","African","National","Council",")","won","a","majority","in","the","common-roll","seats",",","while","Ian","Smith","'s","Rhodesian","Front","(","RF",")","won","all","of","the","old","voter","roll","seats","."],"labels":["O","O","O","O","O","O","B-location","I-location","O","O","B-politician","I-politician","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","politician","country","election","person","political_party","event","organization"]}
{"id":"289","dataset":"crossner_politics","split":"test","instance":{"id":"289","prompt_labels":"The(O) Imperial(B-location) Court(I-location) in(O) Kyoto(B-location) was(O) the(O) nominal(O) ruling(O) government(O) of(O) Japan(B-country) from(O) 794(O) AD(O) until(O) the(O) Meiji(O) period(O) ((O) 1868-1912(O) )(O) ,(O) after(O) which(O) the(O) court(O) was(O) moved(O) from(O) Kyoto(B-location) ((O) formerly(O) Heian-kyō(B-location) )(O) to(O) Tokyo(B-location) ((O) formerly(O) Edo(B-location) )(O) and(O) integrated(O) into(O) the(O) Meiji(O) government(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, country, election, location, event, organization, politician and O.\nSentence: The Imperial Court in Kyoto was the nominal ruling government of Japan from 794 AD until the Meiji period ( 1868-1912 ) , after which the court was moved from Kyoto ( formerly Heian-kyō ) to Tokyo ( formerly Edo ) and integrated into the Meiji government .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Imperial","Court","in","Kyoto","was","the","nominal","ruling","government","of","Japan","from","794","AD","until","the","Meiji","period","(","1868-1912",")",",","after","which","the","court","was","moved","from","Kyoto","(","formerly","Heian-kyō",")","to","Tokyo","(","formerly","Edo",")","and","integrated","into","the","Meiji","government","."],"labels":["O","B-location","I-location","O","B-location","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","O","O","B-location","O","O","B-location","O","O","B-location","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","political_party","country","election","location","event","organization","politician"]}
{"id":"301","dataset":"crossner_politics","split":"test","instance":{"id":"301","prompt_labels":"The(O) Bill(B-politician) was(O) backed(O) by(O) the(O) Labour(B-political party) Party(I-political party) ,(O) the(O) Liberal(B-political party) Democrats(I-political party) ,(O) Plaid(B-political party) Cymru(I-political party) ,(O) the(O) Scottish(B-political party) National(I-political party) Party(I-political party) and(O) the(O) Social(B-political party) Democratic(I-political party) and(I-political party) Labour(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, political party, location, politician, election, event, country and O.\nSentence: The Bill was backed by the Labour Party , the Liberal Democrats , Plaid Cymru , the Scottish National Party and the Social Democratic and Labour Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Bill","was","backed","by","the","Labour","Party",",","the","Liberal","Democrats",",","Plaid","Cymru",",","the","Scottish","National","Party","and","the","Social","Democratic","and","Labour","Party","."],"labels":["O","B-politician","O","O","O","O","B-political party","I-political party","O","O","B-political party","I-political party","O","B-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["person","organization","political_party","location","politician","election","event","country"]}
{"id":"305","dataset":"crossner_politics","split":"test","instance":{"id":"305","prompt_labels":"The(O) Union(B-political party) populaire(I-political party) was(O) succeeded(O) by(O) the(O) Parti(B-political party) nationaliste(I-political party) du(I-political party) Québec(I-political party) in(O) the(O) 1984(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) and(O) subsequently(O) by(O) the(O) Bloc(B-political party) Québécois(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, person, politician, country, organization, election, political party and O.\nSentence: The Union populaire was succeeded by the Parti nationaliste du Québec in the 1984 Canadian federal election , and subsequently by the Bloc Québécois .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Union","populaire","was","succeeded","by","the","Parti","nationaliste","du","Québec","in","the","1984","Canadian","federal","election",",","and","subsequently","by","the","Bloc","Québécois","."],"labels":["O","B-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["location","event","person","politician","country","organization","election","political_party"]}
{"id":"313","dataset":"crossner_politics","split":"test","instance":{"id":"313","prompt_labels":"Ben(B-politician) Gurion(I-politician) returned(O) as(O) Prime(O) Minister(O) ,(O) and(O) formed(O) a(O) coalition(O) with(O) the(O) National(O) Religious(O) Front(O) ((O) which(O) later(O) changed(O) its(O) name(O) to(O) the(O) National(B-political party) Religious(I-political party) Party(I-political party) )(O) ,(O) Mapam(B-political party) ,(O) Ahdut(B-political party) HaAvoda(I-political party) ,(O) and(O) the(O) three(O) Israeli(O) Arab(O) parties(O) ,(O) the(O) Democratic(O) List(O) for(O) Israeli(O) Arabs(O) ,(O) Progress(O) and(O) Work(O) and(O) Agriculture(O) and(O) Development(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, organization, election, location, person, event, politician and O.\nSentence: Ben Gurion returned as Prime Minister , and formed a coalition with the National Religious Front ( which later changed its name to the National Religious Party ) , Mapam , Ahdut HaAvoda , and the three Israeli Arab parties , the Democratic List for Israeli Arabs , Progress and Work and Agriculture and Development .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ben","Gurion","returned","as","Prime","Minister",",","and","formed","a","coalition","with","the","National","Religious","Front","(","which","later","changed","its","name","to","the","National","Religious","Party",")",",","Mapam",",","Ahdut","HaAvoda",",","and","the","three","Israeli","Arab","parties",",","the","Democratic","List","for","Israeli","Arabs",",","Progress","and","Work","and","Agriculture","and","Development","."],"labels":["B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","country","organization","election","location","person","event","politician"]}
{"id":"318","dataset":"crossner_politics","split":"test","instance":{"id":"318","prompt_labels":"Richardson(B-politician) was(O) re-elected(O) to(O) the(O) Senate(O) at(O) the(O) 1984(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) 1987(B-election) Australian(I-election) federal(I-election) election(I-election) ,(O) and(O) 1993(B-election) Australian(I-election) federal(I-election) election(I-election) federal(O) elections(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, organization, event, location, person, politician, election and O.\nSentence: Richardson was re-elected to the Senate at the 1984 Australian federal election , 1987 Australian federal election , and 1993 Australian federal election federal elections .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Richardson","was","re-elected","to","the","Senate","at","the","1984","Australian","federal","election",",","1987","Australian","federal","election",",","and","1993","Australian","federal","election","federal","elections","."],"labels":["B-politician","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","political_party","organization","event","location","person","politician","election"]}
{"id":"323","dataset":"crossner_politics","split":"test","instance":{"id":"323","prompt_labels":"Other(O) organisations(O) that(O) have(O) participated(O) at(O) the(O) Freedom(B-event) Festival(I-event) have(O) included(O) the(O) Adam(B-organization) Smith(I-organization) Institute(I-organization) ,(O) Big(B-organization) Brother(I-organization) Watch(I-organization) ,(O) Conservative(B-organization) Way(I-organization) Forward(I-organization) ,(O) the(O) Centre(B-organization) for(I-organization) Policy(I-organization) Studies(I-organization) ,(O) the(O) Institute(B-organization) of(I-organization) Economic(I-organization) Affairs(I-organization) ,(O) the(O) TaxPayers(B-organization) Alliance(I-organization) and(O) Global(B-organization) Britain(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, person, location, political party, country, election, politician and O.\nSentence: Other organisations that have participated at the Freedom Festival have included the Adam Smith Institute , Big Brother Watch , Conservative Way Forward , the Centre for Policy Studies , the Institute of Economic Affairs , the TaxPayers Alliance and Global Britain .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","organisations","that","have","participated","at","the","Freedom","Festival","have","included","the","Adam","Smith","Institute",",","Big","Brother","Watch",",","Conservative","Way","Forward",",","the","Centre","for","Policy","Studies",",","the","Institute","of","Economic","Affairs",",","the","TaxPayers","Alliance","and","Global","Britain","."],"labels":["O","O","O","O","O","O","O","B-event","I-event","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["event","organization","person","location","political_party","country","election","politician"]}
{"id":"324","dataset":"crossner_politics","split":"test","instance":{"id":"324","prompt_labels":"It(O) was(O) suspected(O) de(B-politician) Lille(I-politician) would(O) either(O) revive(O) her(O) old(O) political(O) party(O) ,(O) the(O) Independent(B-political party) Democrats(I-political party) ,(O) or(O) either(O) join(O) the(O) African(B-political party) National(I-political party) Congress(I-political party) or(O) the(O) Economic(B-political party) Freedom(I-political party) Fighters(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, politician, organization, election, person, political party, country and O.\nSentence: It was suspected de Lille would either revive her old political party , the Independent Democrats , or either join the African National Congress or the Economic Freedom Fighters .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","suspected","de","Lille","would","either","revive","her","old","political","party",",","the","Independent","Democrats",",","or","either","join","the","African","National","Congress","or","the","Economic","Freedom","Fighters","."],"labels":["O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["event","location","politician","organization","election","person","political_party","country"]}
{"id":"330","dataset":"crossner_politics","split":"test","instance":{"id":"330","prompt_labels":"François(B-politician) Bayrou(I-politician) ((O) ,(O) born(O) 25(O) May(O) 1951(O) )(O) is(O) a(O) French(O) centrist(O) politician(O) and(O) the(O) president(O) of(O) the(O) Democratic(B-political party) Movement(I-political party) ((O) MoDem(B-political party) )(O) ,(O) who(O) was(O) a(O) candidate(O) in(O) the(O) 2002(B-election) French(I-election) presidential(I-election) election(I-election) ,(O) 2007(B-election) French(I-election) presidential(I-election) election(I-election) and(O) 2012(B-election) French(I-election) presidential(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, political party, politician, person, country, event, organization, election and O.\nSentence: François Bayrou ( , born 25 May 1951 ) is a French centrist politician and the president of the Democratic Movement ( MoDem ) , who was a candidate in the 2002 French presidential election , 2007 French presidential election and 2012 French presidential election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["François","Bayrou","(",",","born","25","May","1951",")","is","a","French","centrist","politician","and","the","president","of","the","Democratic","Movement","(","MoDem",")",",","who","was","a","candidate","in","the","2002","French","presidential","election",",","2007","French","presidential","election","and","2012","French","presidential","election","."],"labels":["B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["location","political_party","politician","person","country","event","organization","election"]}
{"id":"359","dataset":"crossner_politics","split":"test","instance":{"id":"359","prompt_labels":"While(O) the(O) Conservative(B-political party) Party(I-political party) had(O) a(O) plurality(O) of(O) seats(O) ,(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) and(O) New(B-political party) Democratic(I-political party) Party(I-political party) ,(O) supported(O) by(O) The(O) Bloc(B-political party) Québécois(I-political party) ,(O) agreed(O) to(O) defeat(O) the(O) Conservatives(B-political party) in(O) favour(O) of(O) a(O) Liberal(B-organization) /(I-organization) NDP(I-organization) coalition(I-organization) government(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, politician, election, political party, location, country, event and O.\nSentence: While the Conservative Party had a plurality of seats , the Liberal Party of Canada and New Democratic Party , supported by The Bloc Québécois , agreed to defeat the Conservatives in favour of a Liberal / NDP coalition government .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["While","the","Conservative","Party","had","a","plurality","of","seats",",","the","Liberal","Party","of","Canada","and","New","Democratic","Party",",","supported","by","The","Bloc","Québécois",",","agreed","to","defeat","the","Conservatives","in","favour","of","a","Liberal","/","NDP","coalition","government","."],"labels":["O","O","B-political party","I-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","O","O","O","O","B-political party","I-political party","O","O","O","O","O","B-political party","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["person","organization","politician","election","political_party","location","country","event"]}
{"id":"373","dataset":"crossner_politics","split":"test","instance":{"id":"373","prompt_labels":"The(O) amendment(O) was(O) adopted(O) during(O) the(O) Fianna(B-political party) Fáil(I-political party) -(O) Progressive(B-political party) Democrats(I-political party) coalition(O) government(O) led(O) by(O) Bertie(B-politician) Ahern(I-politician) but(O) had(O) been(O) first(O) drafted(O) and(O) suggested(O) by(O) the(O) previous(O) Fine(B-political party) Gael(I-political party) -(O) Labour(B-political party) Party(I-political party) -(O) Democratic(B-political party) Left(I-political party) government(O) led(O) by(O) John(B-politician) Bruton(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, politician, location, organization, event, person, political party, country and O.\nSentence: The amendment was adopted during the Fianna Fáil - Progressive Democrats coalition government led by Bertie Ahern but had been first drafted and suggested by the previous Fine Gael - Labour Party - Democratic Left government led by John Bruton .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","amendment","was","adopted","during","the","Fianna","Fáil","-","Progressive","Democrats","coalition","government","led","by","Bertie","Ahern","but","had","been","first","drafted","and","suggested","by","the","previous","Fine","Gael","-","Labour","Party","-","Democratic","Left","government","led","by","John","Bruton","."],"labels":["O","O","O","O","O","O","B-political party","I-political party","O","B-political party","I-political party","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","B-political party","I-political party","O","B-political party","I-political party","O","O","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["election","politician","location","organization","event","person","political_party","country"]}
{"id":"375","dataset":"crossner_politics","split":"test","instance":{"id":"375","prompt_labels":"Strauss(B-politician) writes(O) that(O) ,(O) as(O) a(O) result(O) of(O) far-right(O) involvement(O) ,(O) a(O) bizarre(O) ideological(O) turf(O) war(O) has(O) broken(O) out(O) ,(O) whereby(O) anti-globalization(O) activists(O) are(O) fighting(O) a(O) two-front(O) battle(O) ,(O) one(O) against(O) the(O) World(B-organization) Trade(I-organization) Organization(I-organization) ,(O) International(B-organization) Monetary(I-organization) Fund(I-organization) ((O) IMF(B-organization) )(O) ,(O) and(O) World(B-organization) Bank(I-organization) ,(O) the(O) other(O) against(O) the(O) extremists(O) who(O) turn(O) up(O) at(O) their(O) rallies(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, location, event, organization, election, country, political party and O.\nSentence: Strauss writes that , as a result of far-right involvement , a bizarre ideological turf war has broken out , whereby anti-globalization activists are fighting a two-front battle , one against the World Trade Organization , International Monetary Fund ( IMF ) , and World Bank , the other against the extremists who turn up at their rallies .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Strauss","writes","that",",","as","a","result","of","far-right","involvement",",","a","bizarre","ideological","turf","war","has","broken","out",",","whereby","anti-globalization","activists","are","fighting","a","two-front","battle",",","one","against","the","World","Trade","Organization",",","International","Monetary","Fund","(","IMF",")",",","and","World","Bank",",","the","other","against","the","extremists","who","turn","up","at","their","rallies","."],"labels":["B-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","politician","location","event","organization","election","country","political_party"]}
{"id":"384","dataset":"crossner_politics","split":"test","instance":{"id":"384","prompt_labels":"If(O) serving(O) on(O) a(O) deputation(O) ,(O) they(O) may(O) be(O) employed(O) in(O) intergovernmental(O) organisation(O) s(O) such(O) as(O) the(O) World(B-organization) Bank(I-organization) ,(O) the(O) International(B-organization) Monetary(I-organization) Fund(I-organization) ,(O) the(O) Asian(B-organization) Development(I-organization) Bank(I-organization) ,(O) the(O) Asian(B-organization) Infrastructure(I-organization) Investment(I-organization) Bank(I-organization) ,(O) or(O) the(O) United(B-country) Nations(I-country) ,(O) or(O) its(O) agencies(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, political party, person, election, country, event, location and O.\nSentence: If serving on a deputation , they may be employed in intergovernmental organisation s such as the World Bank , the International Monetary Fund , the Asian Development Bank , the Asian Infrastructure Investment Bank , or the United Nations , or its agencies .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["If","serving","on","a","deputation",",","they","may","be","employed","in","intergovernmental","organisation","s","such","as","the","World","Bank",",","the","International","Monetary","Fund",",","the","Asian","Development","Bank",",","the","Asian","Infrastructure","Investment","Bank",",","or","the","United","Nations",",","or","its","agencies","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","B-country","I-country","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","politician","political_party","person","election","country","event","location"]}
{"id":"385","dataset":"crossner_politics","split":"test","instance":{"id":"385","prompt_labels":"It(O) formed(O) a(O) coalition(O) with(O) the(O) United(B-political party) New(I-political party) Zealand(I-political party) as(O) United(B-political party) Future(I-political party) New(I-political party) Zealand(I-political party) in(O) 2000(O) and(O) contested(O) the(O) 2002(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, political party, event, location, politician, person, election, organization and O.\nSentence: It formed a coalition with the United New Zealand as United Future New Zealand in 2000 and contested the 2002 New Zealand general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","formed","a","coalition","with","the","United","New","Zealand","as","United","Future","New","Zealand","in","2000","and","contested","the","2002","New","Zealand","general","election","."],"labels":["O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["country","political_party","event","location","politician","person","election","organization"]}
{"id":"386","dataset":"crossner_politics","split":"test","instance":{"id":"386","prompt_labels":"Future(B-political party) New(I-political party) Zealand(I-political party) joined(O) with(O) the(O) United(B-political party) New(I-political party) Zealand(I-political party) to(O) form(O) a(O) coalition(O) known(O) as(O) United(B-political party) Future(I-political party) New(I-political party) Zealand(I-political party) in(O) November(O) 2000(O) and(O) contested(O) the(O) 2002(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) as(O) such(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, organization, election, political party, politician, country, person and O.\nSentence: Future New Zealand joined with the United New Zealand to form a coalition known as United Future New Zealand in November 2000 and contested the 2002 New Zealand general election as such .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Future","New","Zealand","joined","with","the","United","New","Zealand","to","form","a","coalition","known","as","United","Future","New","Zealand","in","November","2000","and","contested","the","2002","New","Zealand","general","election","as","such","."],"labels":["B-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","location","organization","election","political_party","politician","country","person"]}
{"id":"391","dataset":"crossner_politics","split":"test","instance":{"id":"391","prompt_labels":"The(O) New(B-political party) Zealand(I-political party) National(I-political party) Party(I-political party) won(O) the(O) most(O) seats(O) overall(O) and(O) formed(O) a(O) minority(O) government(O) with(O) the(O) support(O) of(O) the(O) Māori(B-political party) Party(I-political party) ,(O) ACT(B-political party) New(I-political party) Zealand(I-political party) and(O) United(B-political party) Future(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, organization, location, political party, election, event, country, person and O.\nSentence: The New Zealand National Party won the most seats overall and formed a minority government with the support of the Māori Party , ACT New Zealand and United Future .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","New","Zealand","National","Party","won","the","most","seats","overall","and","formed","a","minority","government","with","the","support","of","the","Māori","Party",",","ACT","New","Zealand","and","United","Future","."],"labels":["O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","B-political party","I-political party","I-political party","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["politician","organization","location","political_party","election","event","country","person"]}
{"id":"401","dataset":"crossner_politics","split":"test","instance":{"id":"401","prompt_labels":"He(O) then(O) moved(O) to(O) UK(B-political party) Independence(I-political party) Party(I-political party) ,(O) for(O) which(O) he(O) stood(O) in(O) Kensington(B-location) and(I-location) Chelsea(I-location) at(O) the(O) 1999(B-election) Kensington(I-election) and(I-election) Chelsea(I-election) by-election(I-election) and(O) the(O) 2001(B-election) United(I-election) Kingdom(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, politician, event, election, political party, person, location, country and O.\nSentence: He then moved to UK Independence Party , for which he stood in Kensington and Chelsea at the 1999 Kensington and Chelsea by-election and the 2001 United Kingdom general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","then","moved","to","UK","Independence","Party",",","for","which","he","stood","in","Kensington","and","Chelsea","at","the","1999","Kensington","and","Chelsea","by-election","and","the","2001","United","Kingdom","general","election","."],"labels":["O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","B-location","I-location","I-location","O","O","B-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["organization","politician","event","election","political_party","person","location","country"]}
{"id":"404","dataset":"crossner_politics","split":"test","instance":{"id":"404","prompt_labels":"Helmer(B-politician) was(O) elected(O) to(O) the(O) European(B-organization) Parliament(I-organization) in(O) 1999(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) the(I-election) United(I-election) Kingdom(I-election) ,(O) 2004(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) the(I-election) United(I-election) Kingdom(I-election) ,(O) and(O) 2009(B-election) European(I-election) Parliament(I-election) election(I-election) in(I-election) the(I-election) United(I-election) Kingdom(I-election) as(O) a(O) member(O) of(O) the(O) Conservative(B-political party) Party(I-political party) and(O) in(O) 2014(O) as(O) a(O) member(O) of(O) the(O) UK(B-political party) Independence(I-political party) Party(I-political party) ((O) UKIP(B-political party) )(O) ,(O) having(O) defected(O) from(O) the(O) Conservatives(B-political party) to(O) UKIP(B-political party) in(O) March(O) 2012(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, country, event, election, organization, politician, person, location and O.\nSentence: Helmer was elected to the European Parliament in 1999 European Parliament election in the United Kingdom , 2004 European Parliament election in the United Kingdom , and 2009 European Parliament election in the United Kingdom as a member of the Conservative Party and in 2014 as a member of the UK Independence Party ( UKIP ) , having defected from the Conservatives to UKIP in March 2012 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Helmer","was","elected","to","the","European","Parliament","in","1999","European","Parliament","election","in","the","United","Kingdom",",","2004","European","Parliament","election","in","the","United","Kingdom",",","and","2009","European","Parliament","election","in","the","United","Kingdom","as","a","member","of","the","Conservative","Party","and","in","2014","as","a","member","of","the","UK","Independence","Party","(","UKIP",")",",","having","defected","from","the","Conservatives","to","UKIP","in","March","2012","."],"labels":["B-politician","O","O","O","O","B-organization","I-organization","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","I-election","I-election","I-election","I-election","O","O","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","B-political party","O","B-political party","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","country","event","election","organization","politician","person","location"]}
{"id":"409","dataset":"crossner_politics","split":"test","instance":{"id":"409","prompt_labels":"The(O) Democrats(B-political party) of(I-political party) the(I-political party) Left(I-political party) ((O) DS(B-political party) )(O) ,(O) Democracy(B-political party) is(I-political party) Freedom(I-political party) -(I-political party) The(I-political party) Daisy(I-political party) ((O) DL(B-political party) )(O) and(O) the(O) European(B-political party) Republicans(I-political party) Movement(I-political party) ((O) MRE(B-political party) )(O) decided(O) to(O) form(O) a(O) joint(O) electoral(O) list(O) for(O) the(O) 2006(B-election) Italian(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, election, location, person, event, politician, country and O.\nSentence: The Democrats of the Left ( DS ) , Democracy is Freedom - The Daisy ( DL ) and the European Republicans Movement ( MRE ) decided to form a joint electoral list for the 2006 Italian general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Democrats","of","the","Left","(","DS",")",",","Democracy","is","Freedom","-","The","Daisy","(","DL",")","and","the","European","Republicans","Movement","(","MRE",")","decided","to","form","a","joint","electoral","list","for","the","2006","Italian","general","election","."],"labels":["O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["political_party","organization","election","location","person","event","politician","country"]}
{"id":"416","dataset":"crossner_politics","split":"test","instance":{"id":"416","prompt_labels":"Both(O) the(O) Social(B-political party) Democratic(I-political party) Party(I-political party) of(I-political party) Finland(I-political party) and(O) the(O) Finnish(B-political party) Centre(I-political party) Party(I-political party) improved(O) their(O) vote(O) at(O) the(O) expense(O) of(O) the(O) conservative(O) National(B-political party) Coalition(I-political party) Party(I-political party) and(O) the(O) Green(B-political party) League(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, organization, politician, event, election, political party, person and O.\nSentence: Both the Social Democratic Party of Finland and the Finnish Centre Party improved their vote at the expense of the conservative National Coalition Party and the Green League .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Both","the","Social","Democratic","Party","of","Finland","and","the","Finnish","Centre","Party","improved","their","vote","at","the","expense","of","the","conservative","National","Coalition","Party","and","the","Green","League","."],"labels":["O","O","B-political party","I-political party","I-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["country","location","organization","politician","event","election","political_party","person"]}
{"id":"419","dataset":"crossner_politics","split":"test","instance":{"id":"419","prompt_labels":"Although(O) several(O) public(O) opinion(O) polls(O) predicted(O) that(O) the(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) would(O) result(O) in(O) either(O) a(O) strong(O) Conservative(O) minority(O) or(O) a(O) slight(O) majority(O) ,(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) enjoyed(O) a(O) last-minute(O) surge(O) but(O) were(O) unable(O) to(O) overtake(O) the(O) Conservative(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, country, political party, person, event, election, organization, location and O.\nSentence: Although several public opinion polls predicted that the 2006 Canadian federal election would result in either a strong Conservative minority or a slight majority , the Liberal Party of Canada enjoyed a last-minute surge but were unable to overtake the Conservative Party of Canada .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Although","several","public","opinion","polls","predicted","that","the","2006","Canadian","federal","election","would","result","in","either","a","strong","Conservative","minority","or","a","slight","majority",",","the","Liberal","Party","of","Canada","enjoyed","a","last-minute","surge","but","were","unable","to","overtake","the","Conservative","Party","of","Canada","."],"labels":["O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["politician","country","political_party","person","event","election","organization","location"]}
{"id":"438","dataset":"crossner_politics","split":"test","instance":{"id":"438","prompt_labels":"After(O) the(O) fall(O) of(O) the(O) Hungarian(B-political party) Democratic(I-political party) Forum(I-political party) conservative(O) government(O) at(O) the(O) following(O) 1994(B-election) Hungarian(I-election) parliamentary(I-election) election(I-election) ,(O) SZDSZ(B-political party) surprised(O) many(O) by(O) entering(O) into(O) a(O) coalition(O) with(O) the(O) Hungarian(B-political party) Socialist(I-political party) Party(I-political party) ((O) MSZP(B-political party) )(O) ,(O) legal(O) successors(O) to(O) the(O) communist(O) Hungarian(B-political party) Socialist(I-political party) Workers(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, person, organization, country, election, politician, political party and O.\nSentence: After the fall of the Hungarian Democratic Forum conservative government at the following 1994 Hungarian parliamentary election , SZDSZ surprised many by entering into a coalition with the Hungarian Socialist Party ( MSZP ) , legal successors to the communist Hungarian Socialist Workers Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["After","the","fall","of","the","Hungarian","Democratic","Forum","conservative","government","at","the","following","1994","Hungarian","parliamentary","election",",","SZDSZ","surprised","many","by","entering","into","a","coalition","with","the","Hungarian","Socialist","Party","(","MSZP",")",",","legal","successors","to","the","communist","Hungarian","Socialist","Workers","Party","."],"labels":["O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","B-election","I-election","I-election","I-election","O","B-political party","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["event","location","person","organization","country","election","politician","political_party"]}
{"id":"446","dataset":"crossner_politics","split":"test","instance":{"id":"446","prompt_labels":"In(O) the(O) 2013(B-election) Italian(I-election) general(I-election) election(I-election) the(O) UdC(B-political party) was(O) part(O) of(O) With(B-political party) Monti(I-political party) for(I-political party) Italy(I-political party) ,(O) the(O) coalition(O) formed(O) around(O) Mario(B-politician) Monti(I-politician) '(O) s(O) Civic(B-political party) Choice(I-political party) ,(O) and(O) obtained(O) a(O) mere(O) 1.8(O) %(O) of(O) the(O) vote(O) ,(O) down(O) from(O) 5.6(O) %(O) in(O) 2008(B-election) Italian(I-election) general(I-election) election(I-election) and(O) 6.8(O) %(O) in(O) 2006(B-election) Italian(I-election) general(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, organization, event, political party, country, location, politician, election and O.\nSentence: In the 2013 Italian general election the UdC was part of With Monti for Italy , the coalition formed around Mario Monti ' s Civic Choice , and obtained a mere 1.8 % of the vote , down from 5.6 % in 2008 Italian general election and 6.8 % in 2006 Italian general election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","2013","Italian","general","election","the","UdC","was","part","of","With","Monti","for","Italy",",","the","coalition","formed","around","Mario","Monti","'","s","Civic","Choice",",","and","obtained","a","mere","1.8","%","of","the","vote",",","down","from","5.6","%","in","2008","Italian","general","election","and","6.8","%","in","2006","Italian","general","election","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","B-political party","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","O","O","B-politician","I-politician","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","O","O","O","O","B-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["person","organization","event","political_party","country","location","politician","election"]}
{"id":"453","dataset":"crossner_politics","split":"test","instance":{"id":"453","prompt_labels":"Starting(O) in(O) 1992(O) ,(O) he(O) served(O) as(O) a(O) member(O) of(O) Meretz(B-political party) ,(O) a(O) dovish(O) left(O) wing(O) party(O) which(O) resulted(O) from(O) the(O) merger(O) of(O) Mapam(B-political party) ,(O) Ratz(B-political party) and(O) Shinui(B-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, event, politician, organization, country, location, person and O.\nSentence: Starting in 1992 , he served as a member of Meretz , a dovish left wing party which resulted from the merger of Mapam , Ratz and Shinui .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Starting","in","1992",",","he","served","as","a","member","of","Meretz",",","a","dovish","left","wing","party","which","resulted","from","the","merger","of","Mapam",",","Ratz","and","Shinui","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","B-political party","O","B-political party","O","B-political party","O"],"target_index":null,"target_label":null},"label_list":["election","political_party","event","politician","organization","country","location","person"]}
{"id":"455","dataset":"crossner_politics","split":"test","instance":{"id":"455","prompt_labels":"In(O) 2001(O) Claudio(B-politician) Martelli(I-politician) and(O) Bobo(B-politician) Craxi(I-politician) left(O) the(O) party(O) in(O) order(O) to(O) form(O) ,(O) along(O) with(O) Gianni(B-politician) De(I-politician) Michelis(I-politician) ,(O) the(O) New(B-political party) Italian(I-political party) Socialist(I-political party) Party(I-political party) ((O) NPSI(B-political party) )(O) ,(O) which(O) joined(O) the(O) House(B-political party) of(I-political party) Freedoms(I-political party) centre-right(O) coalition(O) ,(O) while(O) in(O) 2004(O) Giorgio(B-politician) Carta(I-politician) left(O) to(O) re-establish(O) the(O) Italian(B-political party) Democratic(I-political party) Socialist(I-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, political party, country, location, person, organization, politician, election and O.\nSentence: In 2001 Claudio Martelli and Bobo Craxi left the party in order to form , along with Gianni De Michelis , the New Italian Socialist Party ( NPSI ) , which joined the House of Freedoms centre-right coalition , while in 2004 Giorgio Carta left to re-establish the Italian Democratic Socialist Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2001","Claudio","Martelli","and","Bobo","Craxi","left","the","party","in","order","to","form",",","along","with","Gianni","De","Michelis",",","the","New","Italian","Socialist","Party","(","NPSI",")",",","which","joined","the","House","of","Freedoms","centre-right","coalition",",","while","in","2004","Giorgio","Carta","left","to","re-establish","the","Italian","Democratic","Socialist","Party","."],"labels":["O","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","I-politician","O","O","B-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["event","political_party","country","location","person","organization","politician","election"]}
{"id":"459","dataset":"crossner_politics","split":"test","instance":{"id":"459","prompt_labels":"Edgar(B-person) was(O) recognized(O) by(O) several(O) national(O) organizations(O) for(O) his(O) work(O) ,(O) including(O) by(O) the(O) American(B-organization) Legion(I-organization) ,(O) Vietnam(B-organization) Veterans(I-organization) of(I-organization) America(I-organization) and(O) the(O) National(B-organization) Taxpayers(I-organization) Union(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, country, election, event, organization, politician, political party and O.\nSentence: Edgar was recognized by several national organizations for his work , including by the American Legion , Vietnam Veterans of America and the National Taxpayers Union .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Edgar","was","recognized","by","several","national","organizations","for","his","work",",","including","by","the","American","Legion",",","Vietnam","Veterans","of","America","and","the","National","Taxpayers","Union","."],"labels":["B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["location","person","country","election","event","organization","politician","political_party"]}
{"id":"462","dataset":"crossner_politics","split":"test","instance":{"id":"462","prompt_labels":"Between(O) the(O) 1935(B-election) Canadian(I-election) federal(I-election) election(I-election) and(O) 1958(B-election) Canadian(I-election) federal(I-election) election(I-election) elections(O) ,(O) the(O) top(O) ranking(O) was(O) consistently(O) held(O) by(O) either(O) the(O) Co-operative(B-political party) Commonwealth(I-political party) Federation(I-political party) or(O) the(O) Labor-Progressive(B-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, election, event, political party, person, politician, location, country and O.\nSentence: Between the 1935 Canadian federal election and 1958 Canadian federal election elections , the top ranking was consistently held by either the Co-operative Commonwealth Federation or the Labor-Progressive Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Between","the","1935","Canadian","federal","election","and","1958","Canadian","federal","election","elections",",","the","top","ranking","was","consistently","held","by","either","the","Co-operative","Commonwealth","Federation","or","the","Labor-Progressive","Party","."],"labels":["O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["organization","election","event","political_party","person","politician","location","country"]}
{"id":"464","dataset":"crossner_politics","split":"test","instance":{"id":"464","prompt_labels":"Throughout(O) the(O) century(O) ,(O) the(O) state(O) voted(O) for(O) the(O) Federalist(B-political party) Party(I-political party) twice(O) ,(O) the(O) Democratic-Republican(B-political party) Party(I-political party) five(O) times(O) ,(O) the(O) National(B-political party) Republican(I-political party) Party(I-political party) once(O) ,(O) the(O) Whig(B-political party) Party(I-political party) four(O) times(O) ,(O) the(O) Democratic(B-political party) Party(I-political party) ten(O) times(O) ,(O) and(O) the(O) Republican(B-political party) Party(I-political party) three(O) times(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, political party, event, person, country, election, organization and O.\nSentence: Throughout the century , the state voted for the Federalist Party twice , the Democratic-Republican Party five times , the National Republican Party once , the Whig Party four times , the Democratic Party ten times , and the Republican Party three times .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Throughout","the","century",",","the","state","voted","for","the","Federalist","Party","twice",",","the","Democratic-Republican","Party","five","times",",","the","National","Republican","Party","once",",","the","Whig","Party","four","times",",","the","Democratic","Party","ten","times",",","and","the","Republican","Party","three","times","."],"labels":["O","O","O","O","O","O","O","O","O","B-political party","I-political party","O","O","O","B-political party","I-political party","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","O","O","O","O","B-political party","I-political party","O","O","O","O","O","B-political party","I-political party","O","O","O"],"target_index":null,"target_label":null},"label_list":["politician","location","political_party","event","person","country","election","organization"]}
{"id":"473","dataset":"crossner_politics","split":"test","instance":{"id":"473","prompt_labels":"Key(O) to(O) parties(O) :(O) APN(O) Irish(B-political party) Parliamentary(I-political party) Party(I-political party) ,(O) Ind(O) N(O) Independent(B-political party) Nationalist(I-political party) ,(O) Ind(O) N-H(O) Independent(B-political party) Nationalist(I-political party) ((O) supporter(O) of(O) Timothy(B-politician) Healy(I-politician) )(O) ,(O) Lab(O) Irish(B-political party) Labour(I-political party) Party(I-political party) ,(O) N(O) Irish(B-political party) Parliamentary(I-political party) Party(I-political party) ,(O) PN(O) Irish(B-political party) Parliamentary(I-political party) Party(I-political party) ,(O) SF(O) Sinn(B-political party) Féin(I-political party) ,(O) U(O) Unionist(B-political party) ,(O) LU(O) Liberal(B-political party) Unionist(I-political party) Party(I-political party) ,(O) L(O) Liberal(B-political party) Party(I-political party) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, location, election, political party, organization, country, event, politician and O.\nSentence: Key to parties : APN Irish Parliamentary Party , Ind N Independent Nationalist , Ind N-H Independent Nationalist ( supporter of Timothy Healy ) , Lab Irish Labour Party , N Irish Parliamentary Party , PN Irish Parliamentary Party , SF Sinn Féin , U Unionist , LU Liberal Unionist Party , L Liberal Party .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Key","to","parties",":","APN","Irish","Parliamentary","Party",",","Ind","N","Independent","Nationalist",",","Ind","N-H","Independent","Nationalist","(","supporter","of","Timothy","Healy",")",",","Lab","Irish","Labour","Party",",","N","Irish","Parliamentary","Party",",","PN","Irish","Parliamentary","Party",",","SF","Sinn","Féin",",","U","Unionist",",","LU","Liberal","Unionist","Party",",","L","Liberal","Party","."],"labels":["O","O","O","O","O","B-political party","I-political party","I-political party","O","O","O","B-political party","I-political party","O","O","O","B-political party","I-political party","O","O","O","B-politician","I-politician","O","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","O","O","B-political party","O","O","B-political party","I-political party","I-political party","O","O","B-political party","I-political party","O"],"target_index":null,"target_label":null},"label_list":["person","location","election","political_party","organization","country","event","politician"]}
{"id":"477","dataset":"crossner_politics","split":"test","instance":{"id":"477","prompt_labels":"The(O) decision(O) to(O) form(O) the(O) Royal(B-organization) Commission(I-organization) was(O) taken(O) by(O) the(O) Fourth(B-organization) Labour(I-organization) government(I-organization) ,(O) after(O) the(O) Labour(B-political party) party(I-political party) had(O) received(O) more(O) votes(O) ,(O) yet(O) won(O) fewer(O) seats(O) than(O) the(O) New(B-political party) Zealand(I-political party) National(I-political party) Party(I-political party) in(O) both(O) the(O) 1978(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) and(O) 1981(B-election) New(I-election) Zealand(I-election) general(I-election) election(I-election) elections(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, election, event, politician, political party, location, person and O.\nSentence: The decision to form the Royal Commission was taken by the Fourth Labour government , after the Labour party had received more votes , yet won fewer seats than the New Zealand National Party in both the 1978 New Zealand general election and 1981 New Zealand general election elections .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","decision","to","form","the","Royal","Commission","was","taken","by","the","Fourth","Labour","government",",","after","the","Labour","party","had","received","more","votes",",","yet","won","fewer","seats","than","the","New","Zealand","National","Party","in","both","the","1978","New","Zealand","general","election","and","1981","New","Zealand","general","election","elections","."],"labels":["O","O","O","O","O","B-organization","I-organization","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","B-political party","I-political party","O","O","O","O","O","O","O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","I-election","O","O"],"target_index":null,"target_label":null},"label_list":["organization","country","election","event","politician","political_party","location","person"]}
{"id":"492","dataset":"crossner_politics","split":"test","instance":{"id":"492","prompt_labels":"The(O) incumbent(O) Australian(B-political party) Labor(I-political party) Party(I-political party) ,(O) led(O) by(O) Prime(O) Minister(O) Bob(B-politician) Hawke(I-politician) ,(O) defeated(O) the(O) opposition(O) Liberal(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) ,(O) led(O) by(O) John(B-politician) Howard(I-politician) and(O) the(O) National(B-political party) Party(I-political party) of(I-political party) Australia(I-political party) led(O) by(O) Ian(B-politician) Sinclair(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, organization, political party, country, election, event, person and O.\nSentence: The incumbent Australian Labor Party , led by Prime Minister Bob Hawke , defeated the opposition Liberal Party of Australia , led by John Howard and the National Party of Australia led by Ian Sinclair .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","incumbent","Australian","Labor","Party",",","led","by","Prime","Minister","Bob","Hawke",",","defeated","the","opposition","Liberal","Party","of","Australia",",","led","by","John","Howard","and","the","National","Party","of","Australia","led","by","Ian","Sinclair","."],"labels":["O","O","B-political party","I-political party","I-political party","O","O","O","O","O","B-politician","I-politician","O","O","O","O","B-political party","I-political party","I-political party","I-political party","O","O","O","B-politician","I-politician","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["politician","location","organization","political_party","country","election","event","person"]}
{"id":"515","dataset":"crossner_politics","split":"test","instance":{"id":"515","prompt_labels":"Incumbent(O) Democrat(O) Alan(B-politician) Cranston(I-politician) easily(O) won(O) re-election(O) to(O) a(O) third(O) term(O) over(O) Paul(B-politician) Gann(I-politician) ,(O) political(O) activist(O) ,(O) even(O) as(O) the(O) state(O) 's(O) former(O) Republican(O) governor(O) ,(O) Ronald(B-politician) Reagan(I-politician) ,(O) claimed(O) a(O) landslide(O) victory(O) in(O) the(O) 1980(B-election) United(I-election) States(I-election) presidential(I-election) election(I-election) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, organization, politician, person, political party, election, event, country and O.\nSentence: Incumbent Democrat Alan Cranston easily won re-election to a third term over Paul Gann , political activist , even as the state 's former Republican governor , Ronald Reagan , claimed a landslide victory in the 1980 United States presidential election .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Incumbent","Democrat","Alan","Cranston","easily","won","re-election","to","a","third","term","over","Paul","Gann",",","political","activist",",","even","as","the","state","'s","former","Republican","governor",",","Ronald","Reagan",",","claimed","a","landslide","victory","in","the","1980","United","States","presidential","election","."],"labels":["O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","B-election","I-election","I-election","I-election","I-election","O"],"target_index":null,"target_label":null},"label_list":["location","organization","politician","person","political_party","election","event","country"]}
{"id":"517","dataset":"crossner_politics","split":"test","instance":{"id":"517","prompt_labels":"He(O) was(O) re-elected(O) in(O) 1997(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2000(B-election) Canadian(I-election) federal(I-election) election(I-election) ((O) as(O) a(O) member(O) of(O) the(O) Canadian(B-political party) Alliance(I-political party) )(O) ,(O) 2004(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) 2006(B-election) Canadian(I-election) federal(I-election) election(I-election) ,(O) and(O) 2008(B-election) Canadian(I-election) federal(I-election) election(I-election) ((O) as(O) a(O) Conservative(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, country, location, event, organization, political party, person, politician and O.\nSentence: He was re-elected in 1997 Canadian federal election , 2000 Canadian federal election ( as a member of the Canadian Alliance ) , 2004 Canadian federal election , 2006 Canadian federal election , and 2008 Canadian federal election ( as a Conservative ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","re-elected","in","1997","Canadian","federal","election",",","2000","Canadian","federal","election","(","as","a","member","of","the","Canadian","Alliance",")",",","2004","Canadian","federal","election",",","2006","Canadian","federal","election",",","and","2008","Canadian","federal","election","(","as","a","Conservative",")","."],"labels":["O","O","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O","B-political party","I-political party","O","O","B-election","I-election","I-election","I-election","O","B-election","I-election","I-election","I-election","O","O","B-election","I-election","I-election","I-election","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","country","location","event","organization","political_party","person","politician"]}
{"id":"518","dataset":"crossner_politics","split":"test","instance":{"id":"518","prompt_labels":"It(O) was(O) the(O) closest(O) the(O) Democrats(O) have(O) come(O) to(O) winning(O) a(O) Senate(B-organization) election(O) in(O) Kansas(B-location) since(O) George(B-politician) McGill(I-politician) won(O) re-election(O) in(O) 1932(B-election) United(I-election) States(I-election) Senate(I-election) elections(I-election) ((O) McGill(B-politician) was(O) defeated(O) by(O) Clyde(B-politician) M.(I-politician) Reed(I-politician) in(O) 1938(B-election) United(I-election) States(I-election) Senate(I-election) elections(I-election) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, political party, country, event, election, organization, politician, location and O.\nSentence: It was the closest the Democrats have come to winning a Senate election in Kansas since George McGill won re-election in 1932 United States Senate elections ( McGill was defeated by Clyde M. Reed in 1938 United States Senate elections ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","the","closest","the","Democrats","have","come","to","winning","a","Senate","election","in","Kansas","since","George","McGill","won","re-election","in","1932","United","States","Senate","elections","(","McGill","was","defeated","by","Clyde","M.","Reed","in","1938","United","States","Senate","elections",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","B-location","O","B-politician","I-politician","O","O","O","B-election","I-election","I-election","I-election","I-election","O","B-politician","O","O","O","B-politician","I-politician","I-politician","O","B-election","I-election","I-election","I-election","I-election","O","O"],"target_index":null,"target_label":null},"label_list":["person","political_party","country","event","election","organization","politician","location"]}
{"id":"521","dataset":"crossner_politics","split":"test","instance":{"id":"521","prompt_labels":"Green(O) deputies(O) Noël(B-politician) Mamère(I-politician) ,(O) Martine(B-politician) Billard(I-politician) and(O) Yves(B-politician) Cochet(I-politician) on(O) September(O) 10(O) ,(O) 2003(O) requested(O) a(O) Parliamentary(O) Commission(O) on(O) the(O) role(O) of(O) France(B-country) in(O) the(O) support(O) of(O) military(O) regimes(O) in(O) Latin(B-location) America(I-location) from(O) 1973(O) to(O) 1984(O) before(O) the(O) Foreign(B-organization) Affairs(I-organization) Commission(I-organization) of(I-organization) the(I-organization) National(I-organization) Assembly(I-organization) ,(O) presided(O) by(O) Edouard(B-politician) Balladur(I-politician) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, politician, event, election, country, person, political party and O.\nSentence: Green deputies Noël Mamère , Martine Billard and Yves Cochet on September 10 , 2003 requested a Parliamentary Commission on the role of France in the support of military regimes in Latin America from 1973 to 1984 before the Foreign Affairs Commission of the National Assembly , presided by Edouard Balladur .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Green","deputies","Noël","Mamère",",","Martine","Billard","and","Yves","Cochet","on","September","10",",","2003","requested","a","Parliamentary","Commission","on","the","role","of","France","in","the","support","of","military","regimes","in","Latin","America","from","1973","to","1984","before","the","Foreign","Affairs","Commission","of","the","National","Assembly",",","presided","by","Edouard","Balladur","."],"labels":["O","O","B-politician","I-politician","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","O","B-politician","I-politician","O"],"target_index":null,"target_label":null},"label_list":["organization","location","politician","event","election","country","person","political_party"]}
{"id":"524","dataset":"crossner_politics","split":"test","instance":{"id":"524","prompt_labels":"Rep.(O) Bella(B-politician) Abzug(I-politician) took(O) Manhattan(B-location) ,(O) Mayor(O) Abe(B-politician) Beame(I-politician) Brooklyn(B-location) ,(O) Rep.(O) Herman(B-politician) Badillo(I-politician) the(O) Bronx(B-location) ,(O) and(O) NY(B-location) Sec.(O) of(O) State(O) Mario(B-politician) Cuomo(I-politician) Queens(B-location) &(O) amp(O) ;(O) Staten(B-location) Island(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, organization, politician, political party, election, location, country and O.\nSentence: Rep. Bella Abzug took Manhattan , Mayor Abe Beame Brooklyn , Rep. Herman Badillo the Bronx , and NY Sec. of State Mario Cuomo Queens & amp ; Staten Island .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Rep.","Bella","Abzug","took","Manhattan",",","Mayor","Abe","Beame","Brooklyn",",","Rep.","Herman","Badillo","the","Bronx",",","and","NY","Sec.","of","State","Mario","Cuomo","Queens","&","amp",";","Staten","Island","."],"labels":["O","B-politician","I-politician","O","B-location","O","O","B-politician","I-politician","B-location","O","O","B-politician","I-politician","O","B-location","O","O","B-location","O","O","O","B-politician","I-politician","B-location","O","O","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["person","event","organization","politician","political_party","election","location","country"]}
{"id":"527","dataset":"crossner_politics","split":"test","instance":{"id":"527","prompt_labels":"He(O) was(O) elected(O) to(O) the(O) House(B-organization) of(I-organization) Commons(I-organization) of(I-organization) Canada(I-organization) in(O) the(O) 2000(B-election) Canadian(I-election) federal(I-election) election(I-election) as(O) the(O) Liberal(B-political party) Party(I-political party) of(I-political party) Canada(I-political party) MP(O) for(O) York(B-location) South(I-location) -(I-location) Weston(I-location) defeating(O) Independent(O) MP(O) ((O) and(O) former(O) Liberal(O) )(O) John(B-politician) Nunziata(I-politician) by(O) 1,497(O) votes(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, person, location, election, country, organization, event and O.\nSentence: He was elected to the House of Commons of Canada in the 2000 Canadian federal election as the Liberal Party of Canada MP for York South - Weston defeating Independent MP ( and former Liberal ) John Nunziata by 1,497 votes .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","elected","to","the","House","of","Commons","of","Canada","in","the","2000","Canadian","federal","election","as","the","Liberal","Party","of","Canada","MP","for","York","South","-","Weston","defeating","Independent","MP","(","and","former","Liberal",")","John","Nunziata","by","1,497","votes","."],"labels":["O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-election","I-election","I-election","I-election","O","O","B-political party","I-political party","I-political party","I-political party","O","O","B-location","I-location","I-location","I-location","O","O","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["politician","political_party","person","location","election","country","organization","event"]}
{"id":"530","dataset":"crossner_politics","split":"test","instance":{"id":"530","prompt_labels":"The(O) series(O) was(O) written(O) by(O) Russell(B-person) T(I-person) Davies(I-person) and(O) directed(O) by(O) Stephen(B-person) Frears(I-person) ,(O) with(O) Hugh(B-person) Grant(I-person) starring(O) as(O) Thorpe(B-person) and(O) Ben(B-person) Whishaw(I-person) as(O) Scott(B-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, event, election, organization, person, politician, political party and O.\nSentence: The series was written by Russell T Davies and directed by Stephen Frears , with Hugh Grant starring as Thorpe and Ben Whishaw as Scott .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","series","was","written","by","Russell","T","Davies","and","directed","by","Stephen","Frears",",","with","Hugh","Grant","starring","as","Thorpe","and","Ben","Whishaw","as","Scott","."],"labels":["O","O","O","O","O","B-person","I-person","I-person","O","O","O","B-person","I-person","O","O","B-person","I-person","O","O","B-person","O","B-person","I-person","O","B-person","O"],"target_index":null,"target_label":null},"label_list":["country","location","event","election","organization","person","politician","political_party"]}
{"id":"537","dataset":"crossner_politics","split":"test","instance":{"id":"537","prompt_labels":"Jihadi(B-person) John(I-person) is(O) notorious(O) for(O) executing(O) many(O) US(B-country) ,(O) UK(B-country) ,(O) and(O) Japanese(O) citizens(O) such(O) as(O) Steven(B-person) Sotloff(I-person) ,(O) David(B-person) Haines(I-person) ,(O) and(O) Alan(B-person) Henning(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, political party, person, event, organization, election, country, location and O.\nSentence: Jihadi John is notorious for executing many US , UK , and Japanese citizens such as Steven Sotloff , David Haines , and Alan Henning .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Jihadi","John","is","notorious","for","executing","many","US",",","UK",",","and","Japanese","citizens","such","as","Steven","Sotloff",",","David","Haines",",","and","Alan","Henning","."],"labels":["B-person","I-person","O","O","O","O","O","B-country","O","B-country","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["politician","political_party","person","event","organization","election","country","location"]}
{"id":"558","dataset":"crossner_politics","split":"test","instance":{"id":"558","prompt_labels":"Mary(B-person) Blathwayt(I-person) '(O) s(O) parents(O) planted(O) trees(O) there(O) between(O) April(O) 1909(O) and(O) July(O) 1911(O) to(O) commemorate(O) the(O) achievements(O) of(O) suffragettes(O) including(O) Emmeline(B-person) Pankhurst(I-person) ,(O) Christabel(B-person) Pankhurst(I-person) ,(O) Annie(B-person) Kenney(I-person) ,(O) Charlotte(B-person) Despard(I-person) ,(O) Millicent(B-person) Fawcett(I-person) and(O) Lady(B-person) Lytton(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, event, country, politician, organization, person, political party, location and O.\nSentence: Mary Blathwayt ' s parents planted trees there between April 1909 and July 1911 to commemorate the achievements of suffragettes including Emmeline Pankhurst , Christabel Pankhurst , Annie Kenney , Charlotte Despard , Millicent Fawcett and Lady Lytton .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Mary","Blathwayt","'","s","parents","planted","trees","there","between","April","1909","and","July","1911","to","commemorate","the","achievements","of","suffragettes","including","Emmeline","Pankhurst",",","Christabel","Pankhurst",",","Annie","Kenney",",","Charlotte","Despard",",","Millicent","Fawcett","and","Lady","Lytton","."],"labels":["B-person","I-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["election","event","country","politician","organization","person","political_party","location"]}
{"id":"562","dataset":"crossner_politics","split":"test","instance":{"id":"562","prompt_labels":"The(O) series(O) is(O) set(O) to(O) be(O) written(O) by(O) Oscar(O) -nominated(O) screenwriter(O) Billy(B-person) Ray(I-person) ,(O) who(O) will(O) also(O) executive(O) produce(O) the(O) series(O) alongside(O) Alex(B-person) Kurtzman(I-person) ,(O) Heather(B-person) Kadin(I-person) ,(O) and(O) Shane(B-person) Salerno(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, political party, person, event, organization, politician, country and O.\nSentence: The series is set to be written by Oscar -nominated screenwriter Billy Ray , who will also executive produce the series alongside Alex Kurtzman , Heather Kadin , and Shane Salerno .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","series","is","set","to","be","written","by","Oscar","-nominated","screenwriter","Billy","Ray",",","who","will","also","executive","produce","the","series","alongside","Alex","Kurtzman",",","Heather","Kadin",",","and","Shane","Salerno","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["election","location","political_party","person","event","organization","politician","country"]}
{"id":"567","dataset":"crossner_politics","split":"test","instance":{"id":"567","prompt_labels":"Corin(B-person) met(O) with(O) fellow(O) students(O) at(O) Stoneman(B-location) Douglas(I-location) ,(O) including(O) David(B-person) Hogg(I-person) ,(O) Emma(B-person) González(I-person) and(O) Cameron(B-person) Kasky(I-person) ,(O) at(O) Kasky(B-person) 's(O) house(O) ;(O) they(O) formed(O) the(O) Never(B-organization) Again(I-organization) MSD(I-organization) movement(O) during(O) these(O) meetings(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: politician, location, country, person, event, organization, political party, election and O.\nSentence: Corin met with fellow students at Stoneman Douglas , including David Hogg , Emma González and Cameron Kasky , at Kasky 's house ; they formed the Never Again MSD movement during these meetings .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Corin","met","with","fellow","students","at","Stoneman","Douglas",",","including","David","Hogg",",","Emma","González","and","Cameron","Kasky",",","at","Kasky","'s","house",";","they","formed","the","Never","Again","MSD","movement","during","these","meetings","."],"labels":["B-person","O","O","O","O","O","B-location","I-location","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","O","B-person","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["politician","location","country","person","event","organization","political_party","election"]}
{"id":"570","dataset":"crossner_politics","split":"test","instance":{"id":"570","prompt_labels":"The(O) Devil(O) 's(O) Skipper(O) is(O) a(O) 1928(O) American(O) silent(O) drama(O) film(O) directed(O) by(O) John(B-person) G.(I-person) Adolfi(I-person) and(O) starring(O) Belle(B-person) Bennett(I-person) ,(O) Montagu(B-person) Love(I-person) and(O) Gino(B-person) Corrado(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, event, person, political party, politician, election, location, organization and O.\nSentence: The Devil 's Skipper is a 1928 American silent drama film directed by John G. Adolfi and starring Belle Bennett , Montagu Love and Gino Corrado .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Devil","'s","Skipper","is","a","1928","American","silent","drama","film","directed","by","John","G.","Adolfi","and","starring","Belle","Bennett",",","Montagu","Love","and","Gino","Corrado","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","I-person","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["country","event","person","political_party","politician","election","location","organization"]}
{"id":"580","dataset":"crossner_politics","split":"test","instance":{"id":"580","prompt_labels":"Following(O) the(O) end(O) of(O) the(O) Second(B-event) World(I-event) War(I-event) and(O) the(O) wartime(O) alliance(O) between(O) the(O) Soviet(B-country) Union(I-country) and(O) the(O) Allies(B-country) of(O) World(B-event) War(I-event) II(I-event) of(O) the(O) United(B-country) States(I-country) ,(O) Great(B-country) Britain(I-country) ,(O) and(O) France(B-country) ,(O) a(O) new(O) Cold(B-country) War(I-country) erupted(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, location, politician, political party, person, country, event, organization and O.\nSentence: Following the end of the Second World War and the wartime alliance between the Soviet Union and the Allies of World War II of the United States , Great Britain , and France , a new Cold War erupted .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Following","the","end","of","the","Second","World","War","and","the","wartime","alliance","between","the","Soviet","Union","and","the","Allies","of","World","War","II","of","the","United","States",",","Great","Britain",",","and","France",",","a","new","Cold","War","erupted","."],"labels":["O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","B-country","I-country","O","O","B-country","O","B-event","I-event","I-event","O","O","B-country","I-country","O","B-country","I-country","O","O","B-country","O","O","O","B-country","I-country","O","O"],"target_index":null,"target_label":null},"label_list":["election","location","politician","political_party","person","country","event","organization"]}
{"id":"582","dataset":"crossner_politics","split":"test","instance":{"id":"582","prompt_labels":"The(O) campaign(O) 's(O) sustained(O) role(O) in(O) Russian(O) culture(O) may(O) be(O) seen(O) in(O) Tolstoy(B-person) '(O) s(O) War(O) and(O) Peace(O) ,(O) Tchaikovsky(O) '(O) s(O) 1812(O) Overture(O) ,(O) and(O) the(O) identification(O) of(O) it(O) with(O) the(O) German(B-event) invasion(I-event) of(I-event) 1941-45(I-event) ,(O) which(O) became(O) known(O) as(O) the(O) Great(B-event) Patriotic(I-event) War(I-event) in(O) the(O) Soviet(B-country) Union(I-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, person, election, event, country, location, organization and O.\nSentence: The campaign 's sustained role in Russian culture may be seen in Tolstoy ' s War and Peace , Tchaikovsky ' s 1812 Overture , and the identification of it with the German invasion of 1941-45 , which became known as the Great Patriotic War in the Soviet Union .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","campaign","'s","sustained","role","in","Russian","culture","may","be","seen","in","Tolstoy","'","s","War","and","Peace",",","Tchaikovsky","'","s","1812","Overture",",","and","the","identification","of","it","with","the","German","invasion","of","1941-45",",","which","became","known","as","the","Great","Patriotic","War","in","the","Soviet","Union","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","B-person","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","O","O","O","O","O","O","B-event","I-event","I-event","O","O","B-country","I-country","O"],"target_index":null,"target_label":null},"label_list":["political_party","politician","person","election","event","country","location","organization"]}
{"id":"590","dataset":"crossner_politics","split":"test","instance":{"id":"590","prompt_labels":"The(O) air(O) raid(O) was(O) part(O) of(O) the(O) Allies(B-country) of(O) World(B-event) War(I-event) II(I-event) '(O) aerial(O) campaign(O) against(O) the(O) Home(O) Islands(O) of(O) the(O) Empire(B-country) of(I-country) Japan(I-country) during(O) World(B-event) War(I-event) II(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, country, election, location, event, political party, person, politician and O.\nSentence: The air raid was part of the Allies of World War II ' aerial campaign against the Home Islands of the Empire of Japan during World War II .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","air","raid","was","part","of","the","Allies","of","World","War","II","'","aerial","campaign","against","the","Home","Islands","of","the","Empire","of","Japan","during","World","War","II","."],"labels":["O","O","O","O","O","O","O","B-country","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","B-country","I-country","I-country","O","B-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["organization","country","election","location","event","political_party","person","politician"]}
{"id":"593","dataset":"crossner_politics","split":"test","instance":{"id":"593","prompt_labels":"UNA(B-event) campaign(I-event) manager(O) Toby(B-politician) Tiangco(I-politician) said(O) that(O) the(O) percentages(O) could(O) not(O) be(O) the(O) same(O) for(O) every(O) place(O) in(O) the(O) Philippines(B-country) ;(O) meanwhile(O) ,(O) Team(B-organization) PNoy(I-organization) spokesperson(O) Miro(B-politician) Quimbo(I-politician) said(O) that(O) it(O) is(O) a(O) result(O) of(O) a(O) great(O) message(O) campaign(O) led(O) by(O) the(O) President(O) himself(O) and(O) an(O) aggressive(O) ground(O) war(O) pursued(O) by(O) local(O) parties(O) allied(O) with(O) the(O) President(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, country, location, political party, politician, event, person, organization and O.\nSentence: UNA campaign manager Toby Tiangco said that the percentages could not be the same for every place in the Philippines ; meanwhile , Team PNoy spokesperson Miro Quimbo said that it is a result of a great message campaign led by the President himself and an aggressive ground war pursued by local parties allied with the President .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["UNA","campaign","manager","Toby","Tiangco","said","that","the","percentages","could","not","be","the","same","for","every","place","in","the","Philippines",";","meanwhile",",","Team","PNoy","spokesperson","Miro","Quimbo","said","that","it","is","a","result","of","a","great","message","campaign","led","by","the","President","himself","and","an","aggressive","ground","war","pursued","by","local","parties","allied","with","the","President","."],"labels":["B-event","I-event","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","B-organization","I-organization","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["election","country","location","political_party","politician","event","person","organization"]}
{"id":"608","dataset":"crossner_politics","split":"test","instance":{"id":"608","prompt_labels":"The(O) conflict(O) began(O) when(O) the(O) Front(B-political party) for(I-political party) the(I-political party) National(I-political party) Liberation(I-political party) of(I-political party) the(I-political party) Congo(I-political party) ((O) FNLC(B-political party) )(O) ,(O) a(O) group(O) of(O) about(O) 2,000(O) Katangan(O) Congolese(O) soldiers(O) ((O) veterans(O) of(O) the(O) Congo(B-event) Crisis(I-event) ,(O) the(O) Angolan(B-event) War(I-event) of(I-event) Independence(I-event) ,(O) and(O) the(O) Angolan(B-event) Civil(I-event) War(I-event) )(O) crossed(O) the(O) border(O) into(O) Shaba(B-location) from(O) Angola(B-country) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, election, person, country, politician, organization, event, political party and O.\nSentence: The conflict began when the Front for the National Liberation of the Congo ( FNLC ) , a group of about 2,000 Katangan Congolese soldiers ( veterans of the Congo Crisis , the Angolan War of Independence , and the Angolan Civil War ) crossed the border into Shaba from Angola .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","conflict","began","when","the","Front","for","the","National","Liberation","of","the","Congo","(","FNLC",")",",","a","group","of","about","2,000","Katangan","Congolese","soldiers","(","veterans","of","the","Congo","Crisis",",","the","Angolan","War","of","Independence",",","and","the","Angolan","Civil","War",")","crossed","the","border","into","Shaba","from","Angola","."],"labels":["O","O","O","O","O","B-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","I-political party","O","B-political party","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","O","B-event","I-event","I-event","I-event","O","O","O","B-event","I-event","I-event","O","O","O","O","O","B-location","O","B-country","O"],"target_index":null,"target_label":null},"label_list":["location","election","person","country","politician","organization","event","political_party"]}
{"id":"619","dataset":"crossner_politics","split":"test","instance":{"id":"619","prompt_labels":"In(O) light(O) of(O) the(O) economic(O) and(O) political(O) challenges(O) many(O) countries(O) in(O) the(O) region(O) had(O) faced(O) since(O) the(O) 2001(B-event) Quebec(I-event) City(I-event) Summit(I-event) of(I-event) the(I-event) Americas(I-event) and(O) because(O) 14(O) Heads(O) of(O) State(O) and(O) Government(O) in(O) the(O) Hemisphere(O) had(O) assumed(O) office(O) since(O) then(O) ,(O) the(O) Government(O) of(O) Canada(B-country) proposed(O) holding(O) a(O) Special(B-event) Summit(I-event) of(I-event) the(I-event) Americas(I-event) to(O) reinforce(O) hemispheric(O) unity(O) before(O) the(O) Fourth(B-event) Summit(I-event) of(I-event) the(I-event) Americas(I-event) ,(O) which(O) took(O) place(O) in(O) Mar(B-location) del(I-location) Plata(I-location) ,(O) Argentina(B-country) ,(O) in(O) 2005(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, politician, election, political party, person, country, location, organization and O.\nSentence: In light of the economic and political challenges many countries in the region had faced since the 2001 Quebec City Summit of the Americas and because 14 Heads of State and Government in the Hemisphere had assumed office since then , the Government of Canada proposed holding a Special Summit of the Americas to reinforce hemispheric unity before the Fourth Summit of the Americas , which took place in Mar del Plata , Argentina , in 2005 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","light","of","the","economic","and","political","challenges","many","countries","in","the","region","had","faced","since","the","2001","Quebec","City","Summit","of","the","Americas","and","because","14","Heads","of","State","and","Government","in","the","Hemisphere","had","assumed","office","since","then",",","the","Government","of","Canada","proposed","holding","a","Special","Summit","of","the","Americas","to","reinforce","hemispheric","unity","before","the","Fourth","Summit","of","the","Americas",",","which","took","place","in","Mar","del","Plata",",","Argentina",",","in","2005","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","O","O","O","B-event","I-event","I-event","I-event","I-event","O","O","O","O","O","O","B-event","I-event","I-event","I-event","I-event","O","O","O","O","O","B-location","I-location","I-location","O","B-country","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","politician","election","political_party","person","country","location","organization"]}
{"id":"624","dataset":"crossner_politics","split":"test","instance":{"id":"624","prompt_labels":"When(O) the(O) Caucasus(B-event) Campaign(I-event) of(O) World(B-event) War(I-event) I(I-event) broke(O) out(O) between(O) the(O) Russian(B-country) Empire(I-country) and(O) the(O) Ottoman(B-country) Empire(I-country) ,(O) Britain(O) declared(O) martial(O) law(O) in(O) Egypt(B-country) ,(O) and(O) announced(O) that(O) it(O) would(O) shoulder(O) the(O) entire(O) burden(O) of(O) the(O) war(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, politician, event, political party, organization, location, country, election and O.\nSentence: When the Caucasus Campaign of World War I broke out between the Russian Empire and the Ottoman Empire , Britain declared martial law in Egypt , and announced that it would shoulder the entire burden of the war .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["When","the","Caucasus","Campaign","of","World","War","I","broke","out","between","the","Russian","Empire","and","the","Ottoman","Empire",",","Britain","declared","martial","law","in","Egypt",",","and","announced","that","it","would","shoulder","the","entire","burden","of","the","war","."],"labels":["O","O","B-event","I-event","O","B-event","I-event","I-event","O","O","O","O","B-country","I-country","O","O","B-country","I-country","O","O","O","O","O","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","politician","event","political_party","organization","location","country","election"]}
{"id":"630","dataset":"crossner_politics","split":"test","instance":{"id":"630","prompt_labels":"Campaign(O) for(O) a(O) Referendum(O) went(O) to(O) Manchester(B-location) for(O) the(O) 2011(O) Conservative(B-political party) Party(I-political party) conference(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, organization, election, country, location, politician, event, person and O.\nSentence: Campaign for a Referendum went to Manchester for the 2011 Conservative Party conference .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Campaign","for","a","Referendum","went","to","Manchester","for","the","2011","Conservative","Party","conference","."],"labels":["O","O","O","O","O","O","B-location","O","O","O","B-political party","I-political party","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","organization","election","country","location","politician","event","person"]}
{"id":"631","dataset":"crossner_politics","split":"test","instance":{"id":"631","prompt_labels":"The(O) Remain(O) Campaign(O) was(O) led(O) by(O) Britain(B-organization) Stronger(I-organization) in(I-organization) Europe(I-organization) ,(O) a(O) cross-party(O) lobbying(O) group(O) that(O) was(O) declared(O) as(O) the(O) official(O) Remain(O) campaign(O) for(O) the(O) referendum(O) by(O) the(O) Electoral(O) Commission(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, country, event, politician, election, political party, person and O.\nSentence: The Remain Campaign was led by Britain Stronger in Europe , a cross-party lobbying group that was declared as the official Remain campaign for the referendum by the Electoral Commission .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Remain","Campaign","was","led","by","Britain","Stronger","in","Europe",",","a","cross-party","lobbying","group","that","was","declared","as","the","official","Remain","campaign","for","the","referendum","by","the","Electoral","Commission","."],"labels":["O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","location","country","event","politician","election","political_party","person"]}
{"id":"634","dataset":"crossner_politics","split":"test","instance":{"id":"634","prompt_labels":"Hitler(B-politician) however(O) cut(O) a(O) deal(O) with(O) Joseph(B-politician) Stalin(I-politician) to(O) divide(O) Eastern(B-location) Europe(I-location) ;(O) when(O) Germany(B-country) did(O) invade(O) Poland(B-country) in(O) September(O) 1939(O) ,(O) Britain(B-country) and(O) France(B-country) declared(O) war(O) ;(O) the(O) British(B-organization) Commonwealth(I-organization) followed(O) London(B-location) 's(O) lead(O) Donald(B-politician) Cameron(I-politician) Watt(I-politician) ,(O) How(O) War(O) Came(O) :(O) Immediate(O) Origins(O) of(O) the(O) Second(O) World(O) War(O) ,(O) 1938-39(O) ((O) 1990(O) )(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: political party, politician, event, person, country, organization, election, location and O.\nSentence: Hitler however cut a deal with Joseph Stalin to divide Eastern Europe ; when Germany did invade Poland in September 1939 , Britain and France declared war ; the British Commonwealth followed London 's lead Donald Cameron Watt , How War Came : Immediate Origins of the Second World War , 1938-39 ( 1990 )","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Hitler","however","cut","a","deal","with","Joseph","Stalin","to","divide","Eastern","Europe",";","when","Germany","did","invade","Poland","in","September","1939",",","Britain","and","France","declared","war",";","the","British","Commonwealth","followed","London","'s","lead","Donald","Cameron","Watt",",","How","War","Came",":","Immediate","Origins","of","the","Second","World","War",",","1938-39","(","1990",")"],"labels":["B-politician","O","O","O","O","O","B-politician","I-politician","O","O","B-location","I-location","O","O","B-country","O","O","B-country","O","O","O","O","B-country","O","B-country","O","O","O","O","B-organization","I-organization","O","B-location","O","O","B-politician","I-politician","I-politician","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["political_party","politician","event","person","country","organization","election","location"]}
{"id":"639","dataset":"crossner_politics","split":"test","instance":{"id":"639","prompt_labels":"The(O) second(O) method(O) of(O) Russian(O) interference(O) saw(O) the(O) Russian(B-organization) intelligence(I-organization) service(I-organization) ,(O) the(O) GRU(B-organization) ,(O) hacking(O) into(O) email(O) accounts(O) owned(O) by(O) volunteers(O) and(O) employees(O) of(O) the(O) Clinton(B-event) presidential(I-event) campaign(I-event) ,(O) including(O) that(O) of(O) campaign(O) chairman(O) John(B-politician) Podesta(I-politician) ,(O) and(O) also(O) hacking(O) into(O) the(O) computer(O) networks(O) of(O) the(O) Democratic(B-organization) Congressional(I-organization) Campaign(I-organization) Committee(I-organization) ((O) DCCC(B-organization) )(O) and(O) the(O) Democratic(B-organization) National(I-organization) Committee(I-organization) ((O) DNC(B-organization) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, politician, event, election, country, person, organization, political party and O.\nSentence: The second method of Russian interference saw the Russian intelligence service , the GRU , hacking into email accounts owned by volunteers and employees of the Clinton presidential campaign , including that of campaign chairman John Podesta , and also hacking into the computer networks of the Democratic Congressional Campaign Committee ( DCCC ) and the Democratic National Committee ( DNC ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","second","method","of","Russian","interference","saw","the","Russian","intelligence","service",",","the","GRU",",","hacking","into","email","accounts","owned","by","volunteers","and","employees","of","the","Clinton","presidential","campaign",",","including","that","of","campaign","chairman","John","Podesta",",","and","also","hacking","into","the","computer","networks","of","the","Democratic","Congressional","Campaign","Committee","(","DCCC",")","and","the","Democratic","National","Committee","(","DNC",")","."],"labels":["O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","I-event","O","O","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O"],"target_index":null,"target_label":null},"label_list":["location","politician","event","election","country","person","organization","political_party"]}
{"id":"641","dataset":"crossner_politics","split":"test","instance":{"id":"641","prompt_labels":"The(O) first(O) phase(O) of(O) the(O) campaign(O) was(O) concurrent(O) with(O) another(O) SDF(B-organization) operation(O) ,(O) the(O) Raqqa(B-event) campaign(I-event) conducted(O) against(O) Raqqa(B-location) ,(O) ISIL(B-organization) 's(O) then-capital(O) city(O) and(O) stronghold(O) in(O) Syria(B-country) ,(O) as(O) well(O) as(O) with(O) the(O) Central(B-event) Syria(I-event) campaign(I-event) ,(O) the(O) Eastern(B-event) Syria(I-event) campaign(I-event) ,(O) and(O) the(O) Battle(B-event) of(I-event) Deir(I-event) ez-Zor(I-event) ,(O) in(O) which(O) the(O) Syrian(B-organization) Army(I-organization) ((O) SAA(B-organization) )(O) was(O) also(O) capturing(O) territory(O) from(O) the(O) Islamic(B-organization) State(I-organization) ;(O) the(O) Iraqi(B-organization) Army(I-organization) '(O) s(O) Western(B-event) Iraq(I-event) campaign(I-event) against(O) ISIL(B-organization) was(O) also(O) underway(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, person, political party, election, location, organization, politician, country and O.\nSentence: The first phase of the campaign was concurrent with another SDF operation , the Raqqa campaign conducted against Raqqa , ISIL 's then-capital city and stronghold in Syria , as well as with the Central Syria campaign , the Eastern Syria campaign , and the Battle of Deir ez-Zor , in which the Syrian Army ( SAA ) was also capturing territory from the Islamic State ; the Iraqi Army ' s Western Iraq campaign against ISIL was also underway .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","first","phase","of","the","campaign","was","concurrent","with","another","SDF","operation",",","the","Raqqa","campaign","conducted","against","Raqqa",",","ISIL","'s","then-capital","city","and","stronghold","in","Syria",",","as","well","as","with","the","Central","Syria","campaign",",","the","Eastern","Syria","campaign",",","and","the","Battle","of","Deir","ez-Zor",",","in","which","the","Syrian","Army","(","SAA",")","was","also","capturing","territory","from","the","Islamic","State",";","the","Iraqi","Army","'","s","Western","Iraq","campaign","against","ISIL","was","also","underway","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-organization","O","O","O","B-event","I-event","O","O","B-location","O","B-organization","O","O","O","O","O","O","B-country","O","O","O","O","O","O","B-event","I-event","I-event","O","O","B-event","I-event","I-event","O","O","O","B-event","I-event","I-event","I-event","O","O","O","O","B-organization","I-organization","O","B-organization","O","O","O","O","O","O","O","B-organization","I-organization","O","O","B-organization","I-organization","O","O","B-event","I-event","I-event","O","B-organization","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","person","political_party","election","location","organization","politician","country"]}
{"id":"648","dataset":"crossner_politics","split":"test","instance":{"id":"648","prompt_labels":"War(O) returned(O) in(O) September(O) 1939(O) when(O) the(O) French(O) and(O) British(O) governments(O) declared(O) war(O) on(O) Nazi(B-country) Germany(I-country) in(O) response(O) to(O) the(O) German(B-event) invasion(I-event) of(I-event) Poland(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: election, political party, event, politician, country, location, person, organization and O.\nSentence: War returned in September 1939 when the French and British governments declared war on Nazi Germany in response to the German invasion of Poland .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["War","returned","in","September","1939","when","the","French","and","British","governments","declared","war","on","Nazi","Germany","in","response","to","the","German","invasion","of","Poland","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-country","I-country","O","O","O","O","B-event","I-event","I-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["election","political_party","event","politician","country","location","person","organization"]}
{"id":"650","dataset":"crossner_politics","split":"test","instance":{"id":"650","prompt_labels":"The(O) Mueller(O) Report(O) showed(O) that(O) despite(O) assertions(O) by(O) Hope(B-politician) Hicks(I-politician) and(O) Jason(B-politician) Miller(I-politician) in(O) September(O) 2016(O) that(O) Carter(B-politician) Page(I-politician) never(O) had(O) any(O) involvement(O) with(O) the(O) campaign(O) ,(O) Page(B-politician) actually(O) produced(O) work(O) for(O) the(O) campaign(O) ,(O) traveled(O) with(O) Trump(B-politician) to(O) a(O) campaign(O) speech(O) and(O) Chief(O) policy(O) adviser(O) Sam(B-person) Clovis(I-person) expressed(O) appreciation(O) for(O) Page(B-politician) 's(O) work(O) and(O) praised(O) his(O) work(O) to(O) other(O) Campaign(O) officials(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, event, country, election, politician, organization, person, political party and O.\nSentence: The Mueller Report showed that despite assertions by Hope Hicks and Jason Miller in September 2016 that Carter Page never had any involvement with the campaign , Page actually produced work for the campaign , traveled with Trump to a campaign speech and Chief policy adviser Sam Clovis expressed appreciation for Page 's work and praised his work to other Campaign officials .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Mueller","Report","showed","that","despite","assertions","by","Hope","Hicks","and","Jason","Miller","in","September","2016","that","Carter","Page","never","had","any","involvement","with","the","campaign",",","Page","actually","produced","work","for","the","campaign",",","traveled","with","Trump","to","a","campaign","speech","and","Chief","policy","adviser","Sam","Clovis","expressed","appreciation","for","Page","'s","work","and","praised","his","work","to","other","Campaign","officials","."],"labels":["O","O","O","O","O","O","O","O","B-politician","I-politician","O","B-politician","I-politician","O","O","O","O","B-politician","I-politician","O","O","O","O","O","O","O","O","B-politician","O","O","O","O","O","O","O","O","O","B-politician","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","B-politician","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","event","country","election","politician","organization","person","political_party"]}
{"id":"2","dataset":"crossner_science","split":"test","instance":{"id":"2","prompt_labels":"That(O) first(O) evening(O) session(O) was(O) organized(O) by(O) Jack(B-person) Yardley(I-person) from(O) Johns(B-university) Hopkins(I-university) University(I-university) ,(O) and(O) included(O) Henry(B-person) Appelman(I-person) ((O) University(B-university) of(I-university) Michigan(I-university) )(O) ,(O) Harvey(B-scientist) Goldman(I-scientist) ((O) Beth(B-organization) Israel(I-organization) Deaconess(I-organization) Medical(I-organization) Center(I-organization) and(O) Harvard(B-university) Medical(I-university) School(I-university) )(O) ,(O) Bill(B-scientist) Hawk(I-scientist) ((O) The(O) Cleveland(B-organization) Clinic(I-organization) )(O) ,(O) Tom(B-person) Kent(I-person) ((O) University(B-university) of(I-university) Iowa(I-university) )(O) ,(O) Si-Chun(B-scientist) Ming(I-scientist) ((O) Temple(B-university) University(I-university) )(O) ,(O) Tom(B-scientist) Norris(I-scientist) ((O) University(B-university) of(I-university) Washington(I-university) )(O) ,(O) and(O) Robert(B-scientist) Riddell(I-scientist) ((O) University(B-university) of(I-university) Chicago(I-university) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, chemical compound, event, enzyme, discipline, chemical element, theory, protein, location, organization, university, person, academic journal, scientist, astronomical object, country and O.\nSentence: That first evening session was organized by Jack Yardley from Johns Hopkins University , and included Henry Appelman ( University of Michigan ) , Harvey Goldman ( Beth Israel Deaconess Medical Center and Harvard Medical School ) , Bill Hawk ( The Cleveland Clinic ) , Tom Kent ( University of Iowa ) , Si-Chun Ming ( Temple University ) , Tom Norris ( University of Washington ) , and Robert Riddell ( University of Chicago ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["That","first","evening","session","was","organized","by","Jack","Yardley","from","Johns","Hopkins","University",",","and","included","Henry","Appelman","(","University","of","Michigan",")",",","Harvey","Goldman","(","Beth","Israel","Deaconess","Medical","Center","and","Harvard","Medical","School",")",",","Bill","Hawk","(","The","Cleveland","Clinic",")",",","Tom","Kent","(","University","of","Iowa",")",",","Si-Chun","Ming","(","Temple","University",")",",","Tom","Norris","(","University","of","Washington",")",",","and","Robert","Riddell","(","University","of","Chicago",")","."],"labels":["O","O","O","O","O","O","O","B-person","I-person","O","B-university","I-university","I-university","O","O","O","B-person","I-person","O","B-university","I-university","I-university","O","O","B-scientist","I-scientist","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-university","I-university","I-university","O","O","B-scientist","I-scientist","O","O","B-organization","I-organization","O","O","B-person","I-person","O","B-university","I-university","I-university","O","O","B-scientist","I-scientist","O","B-university","I-university","O","O","B-scientist","I-scientist","O","B-university","I-university","I-university","O","O","O","B-scientist","I-scientist","O","B-university","I-university","I-university","O","O"],"target_index":null,"target_label":null},"label_list":["award","chemical_compound","event","enzyme","discipline","chemical_element","theory","protein","location","organization","university","person","academic_journal","scientist","astronomical_object","country"]}
{"id":"14","dataset":"crossner_science","split":"test","instance":{"id":"14","prompt_labels":"In(O) the(O) low(O) pressure(O) process(O) α-olefins(B-chemical compound) ((O) e.g.(O) 1-Butene(B-chemical compound) or(O) 1-Hexene(B-chemical compound) )(O) may(O) be(O) added(O) ,(O) which(O) are(O) incorporated(O) in(O) the(O) polymer(O) chain(O) during(O) polymerization(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, chemical compound, university, award, person, protein, discipline, event, organization, enzyme, location, theory, scientist, academic journal, astronomical object, chemical element and O.\nSentence: In the low pressure process α-olefins ( e.g. 1-Butene or 1-Hexene ) may be added , which are incorporated in the polymer chain during polymerization .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","low","pressure","process","α-olefins","(","e.g.","1-Butene","or","1-Hexene",")","may","be","added",",","which","are","incorporated","in","the","polymer","chain","during","polymerization","."],"labels":["O","O","O","O","O","B-chemical compound","O","O","B-chemical compound","O","B-chemical compound","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","chemical_compound","university","award","person","protein","discipline","event","organization","enzyme","location","theory","scientist","academic_journal","astronomical_object","chemical_element"]}
{"id":"30","dataset":"crossner_science","split":"test","instance":{"id":"30","prompt_labels":"The(O) Zeitschrift(B-academic journal) für(I-academic journal) Naturforschung(I-academic journal) ((O) English(O) :(O) Journal(B-academic journal) for(I-academic journal) Nature(I-academic journal) Research(I-academic journal) )(O) was(O) established(O) in(O) 1946(O) by(O) the(O) Max(B-organization) Planck(I-organization) Institute(I-organization) and(O) the(B-academic journal) physical(I-academic journal) sciences(I-academic journal) ((I-academic journal) Zeitschrift(I-academic journal) für(I-academic journal) Naturforschung(I-academic journal) A(I-academic journal) )(I-academic journal) were(O) separated(O) from(O) the(O) other(O) natural(O) sciences(O) ((O) Part(O) B(O) )(O) from(O) 1947(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, award, university, theory, discipline, chemical element, location, chemical compound, person, astronomical object, scientist, organization, academic journal, protein, country, enzyme and O.\nSentence: The Zeitschrift für Naturforschung ( English : Journal for Nature Research ) was established in 1946 by the Max Planck Institute and the physical sciences ( Zeitschrift für Naturforschung A ) were separated from the other natural sciences ( Part B ) from 1947 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Zeitschrift","für","Naturforschung","(","English",":","Journal","for","Nature","Research",")","was","established","in","1946","by","the","Max","Planck","Institute","and","the","physical","sciences","(","Zeitschrift","für","Naturforschung","A",")","were","separated","from","the","other","natural","sciences","(","Part","B",")","from","1947","."],"labels":["O","B-academic journal","I-academic journal","I-academic journal","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","award","university","theory","discipline","chemical_element","location","chemical_compound","person","astronomical_object","scientist","organization","academic_journal","protein","country","enzyme"]}
{"id":"31","dataset":"crossner_science","split":"test","instance":{"id":"31","prompt_labels":"A(O) second(O) meeting(O) was(O) held(O) soon(O) thereafter(O) and(O) included(O) Klaus(B-scientist) Clusius(I-scientist) ,(O) Robert(B-scientist) Döpel(I-scientist) ,(O) Werner(B-scientist) Heisenberg(I-scientist) ,(O) and(O) Carl(B-scientist) Friedrich(I-scientist) von(I-scientist) Weizsäcker(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, scientist, academic journal, discipline, theory, event, chemical element, university, astronomical object, organization, location, enzyme, chemical compound, person, country, award and O.\nSentence: A second meeting was held soon thereafter and included Klaus Clusius , Robert Döpel , Werner Heisenberg , and Carl Friedrich von Weizsäcker .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","second","meeting","was","held","soon","thereafter","and","included","Klaus","Clusius",",","Robert","Döpel",",","Werner","Heisenberg",",","and","Carl","Friedrich","von","Weizsäcker","."],"labels":["O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","I-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["protein","scientist","academic_journal","discipline","theory","event","chemical_element","university","astronomical_object","organization","location","enzyme","chemical_compound","person","country","award"]}
{"id":"44","dataset":"crossner_science","split":"test","instance":{"id":"44","prompt_labels":"Non-governmental(O) agencies(O) such(O) as(O) the(O) International(B-organization) Planned(I-organization) Parenthood(I-organization) Federation(I-organization) and(O) Marie(B-organization) Stopes(I-organization) International(I-organization) provide(O) contraceptive(O) advice(O) for(O) young(O) women(O) worldwide(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, chemical compound, organization, discipline, scientist, person, location, university, academic journal, astronomical object, protein, chemical element, award, country, enzyme, event and O.\nSentence: Non-governmental agencies such as the International Planned Parenthood Federation and Marie Stopes International provide contraceptive advice for young women worldwide .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Non-governmental","agencies","such","as","the","International","Planned","Parenthood","Federation","and","Marie","Stopes","International","provide","contraceptive","advice","for","young","women","worldwide","."],"labels":["O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["theory","chemical_compound","organization","discipline","scientist","person","location","university","academic_journal","astronomical_object","protein","chemical_element","award","country","enzyme","event"]}
{"id":"48","dataset":"crossner_science","split":"test","instance":{"id":"48","prompt_labels":"Writing(O) for(O) The(B-academic journal) Mathematical(I-academic journal) Gazette(I-academic journal) on(O) the(O) first(O) edition(O) ,(O) L.(B-scientist) Rosenhead(I-scientist) congratulated(O) Goldstein(B-scientist) for(O) a(O) lucid(O) account(O) of(O) classical(B-discipline) mechanics(I-discipline) leading(O) to(O) modern(O) theoretical(B-discipline) physics(I-discipline) ,(O) which(O) he(O) believed(O) would(O) stand(O) the(O) test(O) of(O) time(O) alongside(O) acknowledged(O) classics(O) such(O) as(O) E.(B-scientist) T.(I-scientist) Whittaker(I-scientist) '(O) s(O) Analytical(B-discipline) Dynamics(I-discipline) and(O) Arnold(B-scientist) Sommerfeld(I-scientist) '(O) s(O) Lectures(O) on(O) Theoretical(B-discipline) Physics(I-discipline) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, theory, university, location, enzyme, scientist, protein, country, astronomical object, chemical compound, event, chemical element, discipline, award, academic journal, person and O.\nSentence: Writing for The Mathematical Gazette on the first edition , L. Rosenhead congratulated Goldstein for a lucid account of classical mechanics leading to modern theoretical physics , which he believed would stand the test of time alongside acknowledged classics such as E. T. Whittaker ' s Analytical Dynamics and Arnold Sommerfeld ' s Lectures on Theoretical Physics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Writing","for","The","Mathematical","Gazette","on","the","first","edition",",","L.","Rosenhead","congratulated","Goldstein","for","a","lucid","account","of","classical","mechanics","leading","to","modern","theoretical","physics",",","which","he","believed","would","stand","the","test","of","time","alongside","acknowledged","classics","such","as","E.","T.","Whittaker","'","s","Analytical","Dynamics","and","Arnold","Sommerfeld","'","s","Lectures","on","Theoretical","Physics","."],"labels":["O","O","B-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","O","O","O","O","O","B-discipline","I-discipline","O","O","O","B-discipline","I-discipline","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","O","B-discipline","I-discipline","O","B-scientist","I-scientist","O","O","O","O","B-discipline","I-discipline","O"],"target_index":null,"target_label":null},"label_list":["organization","theory","university","location","enzyme","scientist","protein","country","astronomical_object","chemical_compound","event","chemical_element","discipline","award","academic_journal","person"]}
{"id":"51","dataset":"crossner_science","split":"test","instance":{"id":"51","prompt_labels":"The(O) California(B-location) portion(O) of(O) the(O) desert(O) also(O) contains(O) Edwards(B-location) Air(I-location) Force(I-location) Base(I-location) and(O) Naval(B-location) Air(I-location) Weapons(I-location) Station(I-location) China(I-location) Lake(I-location) ,(O) noted(O) for(O) experimental(O) aviation(O) and(O) weapons(O) projects(O) ,(O) and(O) the(O) Marine(B-location) Corps(I-location) Air(I-location) Ground(I-location) Combat(I-location) Center(I-location) Twentynine(I-location) Palms(I-location) in(O) the(O) world(O) at(O) Twentynine(B-location) Palms(I-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, organization, chemical compound, scientist, protein, academic journal, theory, enzyme, chemical element, discipline, person, award, location, university, astronomical object, country and O.\nSentence: The California portion of the desert also contains Edwards Air Force Base and Naval Air Weapons Station China Lake , noted for experimental aviation and weapons projects , and the Marine Corps Air Ground Combat Center Twentynine Palms in the world at Twentynine Palms .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","California","portion","of","the","desert","also","contains","Edwards","Air","Force","Base","and","Naval","Air","Weapons","Station","China","Lake",",","noted","for","experimental","aviation","and","weapons","projects",",","and","the","Marine","Corps","Air","Ground","Combat","Center","Twentynine","Palms","in","the","world","at","Twentynine","Palms","."],"labels":["O","B-location","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","B-location","I-location","I-location","I-location","I-location","I-location","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","I-location","I-location","I-location","I-location","O","O","O","O","B-location","I-location","O"],"target_index":null,"target_label":null},"label_list":["event","organization","chemical_compound","scientist","protein","academic_journal","theory","enzyme","chemical_element","discipline","person","award","location","university","astronomical_object","country"]}
{"id":"52","dataset":"crossner_science","split":"test","instance":{"id":"52","prompt_labels":"Theoretical(O) modelling(O) of(O) two(O) of(O) these(O) super-Earths(O) ,(O) Kepler-62e(B-astronomical object) and(O) Kepler-62f(B-astronomical object) ,(O) suggests(O) both(O) could(O) be(O) solid(O) ,(O) either(O) rocky(O) or(O) rocky(O) with(O) frozen(O) water(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical compound, academic journal, event, country, theory, organization, chemical element, location, enzyme, scientist, award, protein, university, astronomical object, person and O.\nSentence: Theoretical modelling of two of these super-Earths , Kepler-62e and Kepler-62f , suggests both could be solid , either rocky or rocky with frozen water .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Theoretical","modelling","of","two","of","these","super-Earths",",","Kepler-62e","and","Kepler-62f",",","suggests","both","could","be","solid",",","either","rocky","or","rocky","with","frozen","water","."],"labels":["O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","chemical_compound","academic_journal","event","country","theory","organization","chemical_element","location","enzyme","scientist","award","protein","university","astronomical_object","person"]}
{"id":"58","dataset":"crossner_science","split":"test","instance":{"id":"58","prompt_labels":"Some(O) of(O) the(O) many(O) prestigious(O) residents(O) include(O) :(O) Academy(B-award) Award(I-award) winners(O) Rustam(B-person) Ibrahimbeyov(I-person) and(O) Vladimir(B-person) Menshov(I-person) ,(O) one(O) of(O) the(O) founders(O) and(O) head(O) of(O) the(O) Soviet(B-organization) space(I-organization) program(I-organization) Kerim(B-scientist) Kerimov(I-scientist) ,(O) Nobel(B-award) Prize(I-award) winner(O) and(O) physicist(O) Lev(B-scientist) Landau(I-scientist) and(O) famous(O) musicians(O) such(O) as(O) Gara(B-person) Garayev(I-person) ,(O) Uzeyir(B-person) Hajibeyov(I-person) ,(O) Muslim(B-person) Magomayev(I-person) ,(O) Vagif(B-person) Mustafazadeh(I-person) and(O) Alim(B-person) Qasimov(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, enzyme, protein, theory, person, chemical compound, award, academic journal, event, discipline, organization, university, scientist, astronomical object, chemical element, country and O.\nSentence: Some of the many prestigious residents include : Academy Award winners Rustam Ibrahimbeyov and Vladimir Menshov , one of the founders and head of the Soviet space program Kerim Kerimov , Nobel Prize winner and physicist Lev Landau and famous musicians such as Gara Garayev , Uzeyir Hajibeyov , Muslim Magomayev , Vagif Mustafazadeh and Alim Qasimov .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","of","the","many","prestigious","residents","include",":","Academy","Award","winners","Rustam","Ibrahimbeyov","and","Vladimir","Menshov",",","one","of","the","founders","and","head","of","the","Soviet","space","program","Kerim","Kerimov",",","Nobel","Prize","winner","and","physicist","Lev","Landau","and","famous","musicians","such","as","Gara","Garayev",",","Uzeyir","Hajibeyov",",","Muslim","Magomayev",",","Vagif","Mustafazadeh","and","Alim","Qasimov","."],"labels":["O","O","O","O","O","O","O","O","B-award","I-award","O","B-person","I-person","O","B-person","I-person","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","B-scientist","I-scientist","O","B-award","I-award","O","O","O","B-scientist","I-scientist","O","O","O","O","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["location","enzyme","protein","theory","person","chemical_compound","award","academic_journal","event","discipline","organization","university","scientist","astronomical_object","chemical_element","country"]}
{"id":"60","dataset":"crossner_science","split":"test","instance":{"id":"60","prompt_labels":"In(O) 2007(O) the(O) British(B-organization) Biochemical(I-organization) Society(I-organization) was(O) given(O) a(O) grant(O) by(O) the(O) Wellcome(B-organization) Trust(I-organization) to(O) catalogue(O) and(O) preserve(O) the(O) 35(O) laboratory(O) notebooks(O) in(O) which(O) Sanger(B-scientist) recorded(O) his(O) research(O) from(O) 1944(O) to(O) 1983(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, university, enzyme, person, chemical compound, award, event, discipline, organization, theory, location, chemical element, academic journal, protein, scientist, astronomical object and O.\nSentence: In 2007 the British Biochemical Society was given a grant by the Wellcome Trust to catalogue and preserve the 35 laboratory notebooks in which Sanger recorded his research from 1944 to 1983 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","2007","the","British","Biochemical","Society","was","given","a","grant","by","the","Wellcome","Trust","to","catalogue","and","preserve","the","35","laboratory","notebooks","in","which","Sanger","recorded","his","research","from","1944","to","1983","."],"labels":["O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","B-scientist","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["country","university","enzyme","person","chemical_compound","award","event","discipline","organization","theory","location","chemical_element","academic_journal","protein","scientist","astronomical_object"]}
{"id":"64","dataset":"crossner_science","split":"test","instance":{"id":"64","prompt_labels":"The(O) WH2(O) domain(O) is(O) found(O) as(O) a(O) modular(O) part(O) of(O) larger(O) proteins(O) ;(O) it(O) can(O) be(O) associated(O) with(O) the(O) WH1(O) domain(O) or(O) EVH1(O) domain(O) and(O) with(O) the(O) CRIB(O) domain(O) ,(O) and(O) the(O) WH2(O) domain(O) can(O) occur(O) as(O) a(O) tandem(O) repeat(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, protein, event, academic journal, country, university, chemical compound, organization, discipline, chemical element, award, astronomical object, theory, enzyme, scientist, location and O.\nSentence: The WH2 domain is found as a modular part of larger proteins ; it can be associated with the WH1 domain or EVH1 domain and with the CRIB domain , and the WH2 domain can occur as a tandem repeat .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","WH2","domain","is","found","as","a","modular","part","of","larger","proteins",";","it","can","be","associated","with","the","WH1","domain","or","EVH1","domain","and","with","the","CRIB","domain",",","and","the","WH2","domain","can","occur","as","a","tandem","repeat","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","protein","event","academic_journal","country","university","chemical_compound","organization","discipline","chemical_element","award","astronomical_object","theory","enzyme","scientist","location"]}
{"id":"72","dataset":"crossner_science","split":"test","instance":{"id":"72","prompt_labels":"The(O) concept(O) had(O) been(O) advocated(O) by(O) Jean-Baptiste(B-scientist) Lamarck(I-scientist) ,(O) Étienne(B-scientist) Geoffroy(I-scientist) Saint-Hilaire(I-scientist) ,(O) Erasmus(B-scientist) Darwin(I-scientist) ,(O) and(O) Robert(B-scientist) Grant(I-scientist) ,(O) among(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, astronomical object, protein, person, university, chemical element, country, scientist, award, discipline, enzyme, theory, organization, location, chemical compound, academic journal and O.\nSentence: The concept had been advocated by Jean-Baptiste Lamarck , Étienne Geoffroy Saint-Hilaire , Erasmus Darwin , and Robert Grant , among others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","concept","had","been","advocated","by","Jean-Baptiste","Lamarck",",","Étienne","Geoffroy","Saint-Hilaire",",","Erasmus","Darwin",",","and","Robert","Grant",",","among","others","."],"labels":["O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","O","O","B-scientist","I-scientist","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","astronomical_object","protein","person","university","chemical_element","country","scientist","award","discipline","enzyme","theory","organization","location","chemical_compound","academic_journal"]}
{"id":"73","dataset":"crossner_science","split":"test","instance":{"id":"73","prompt_labels":"On(O) 11(O) August(O) 2014(O) ,(O) astronomers(O) released(O) studies(O) ,(O) using(O) the(O) Atacama(O) Large(O) Millimeter(O) /(O) Submillimeter(O) Array(O) ((O) ALMA(O) )(O) for(O) the(O) first(O) time(O) ,(O) that(O) detailed(O) the(O) distribution(O) of(O) HCN(O) ,(O) Hydrogen(B-chemical compound) isocyanide(I-chemical compound) ,(O) Formaldehyde(B-chemical compound) ,(O) and(O) dust(O) inside(O) the(O) comae(O) of(O) comet(O) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, chemical element, country, protein, chemical compound, organization, astronomical object, academic journal, enzyme, scientist, discipline, theory, event, person, university and O.\nSentence: On 11 August 2014 , astronomers released studies , using the Atacama Large Millimeter / Submillimeter Array ( ALMA ) for the first time , that detailed the distribution of HCN , Hydrogen isocyanide , Formaldehyde , and dust inside the comae of comet s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","11","August","2014",",","astronomers","released","studies",",","using","the","Atacama","Large","Millimeter","/","Submillimeter","Array","(","ALMA",")","for","the","first","time",",","that","detailed","the","distribution","of","HCN",",","Hydrogen","isocyanide",",","Formaldehyde",",","and","dust","inside","the","comae","of","comet","s","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","B-chemical compound","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","location","chemical_element","country","protein","chemical_compound","organization","astronomical_object","academic_journal","enzyme","scientist","discipline","theory","event","person","university"]}
{"id":"76","dataset":"crossner_science","split":"test","instance":{"id":"76","prompt_labels":"He(O) has(O) co-authored(O) a(O) number(O) of(O) frequently-cited(O) articles(O) in(O) Circulation(B-academic journal) Research(I-academic journal) ,(O) the(O) Journal(B-academic journal) of(I-academic journal) Biological(I-academic journal) Chemistry(I-academic journal) ,(O) and(O) the(O) American(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physiology(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, organization, chemical compound, theory, protein, event, person, country, discipline, chemical element, astronomical object, academic journal, scientist, location, award, university and O.\nSentence: He has co-authored a number of frequently-cited articles in Circulation Research , the Journal of Biological Chemistry , and the American Journal of Physiology .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","has","co-authored","a","number","of","frequently-cited","articles","in","Circulation","Research",",","the","Journal","of","Biological","Chemistry",",","and","the","American","Journal","of","Physiology","."],"labels":["O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["enzyme","organization","chemical_compound","theory","protein","event","person","country","discipline","chemical_element","astronomical_object","academic_journal","scientist","location","award","university"]}
{"id":"78","dataset":"crossner_science","split":"test","instance":{"id":"78","prompt_labels":"Young(O) began(O) to(O) study(O) medicine(O) in(O) London(B-location) at(O) St(B-organization) Bartholomew(I-organization) 's(I-organization) Hospital(I-organization) in(O) 1792(O) ,(O) moved(O) to(O) the(O) University(B-university) of(I-university) Edinburgh(I-university) Medical(I-university) School(I-university) in(O) 1794(O) ,(O) and(O) a(O) year(O) later(O) went(O) to(O) Göttingen(B-location) ,(O) Lower(B-location) Saxony(I-location) ,(O) Germany(B-country) ,(O) where(O) he(O) obtained(O) the(O) degree(O) of(O) doctor(O) of(O) medicine(O) in(O) 1796(O) from(O) the(O) University(B-university) of(I-university) Göttingen(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, event, theory, person, university, enzyme, protein, astronomical object, discipline, scientist, location, academic journal, chemical element, organization, country, award and O.\nSentence: Young began to study medicine in London at St Bartholomew 's Hospital in 1792 , moved to the University of Edinburgh Medical School in 1794 , and a year later went to Göttingen , Lower Saxony , Germany , where he obtained the degree of doctor of medicine in 1796 from the University of Göttingen .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Young","began","to","study","medicine","in","London","at","St","Bartholomew","'s","Hospital","in","1792",",","moved","to","the","University","of","Edinburgh","Medical","School","in","1794",",","and","a","year","later","went","to","Göttingen",",","Lower","Saxony",",","Germany",",","where","he","obtained","the","degree","of","doctor","of","medicine","in","1796","from","the","University","of","Göttingen","."],"labels":["O","O","O","O","O","O","B-location","O","B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","B-university","I-university","I-university","I-university","I-university","O","O","O","O","O","O","O","O","O","B-location","O","B-location","I-location","O","B-country","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","event","theory","person","university","enzyme","protein","astronomical_object","discipline","scientist","location","academic_journal","chemical_element","organization","country","award"]}
{"id":"86","dataset":"crossner_science","split":"test","instance":{"id":"86","prompt_labels":"Lycomedes(B-astronomical object) was(O) discovered(O) on(O) 26(O) September(O) 1960(O) ,(O) by(O) Dutch(O) astronomer(O) couple(O) Ingrid(B-scientist) van(I-scientist) Houten-Groeneveld(I-scientist) and(O) Cornelis(B-scientist) van(I-scientist) Houten(I-scientist) at(O) Leiden(B-location) ,(O) on(O) photographic(O) plate(O) s(O) taken(O) by(O) astronomer(O) Tom(B-scientist) Gehrels(I-scientist) at(O) the(O) Palomar(B-location) Observatory(I-location) in(O) California(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, enzyme, protein, astronomical object, chemical compound, academic journal, university, theory, country, chemical element, discipline, person, award, organization, scientist and O.\nSentence: Lycomedes was discovered on 26 September 1960 , by Dutch astronomer couple Ingrid van Houten-Groeneveld and Cornelis van Houten at Leiden , on photographic plate s taken by astronomer Tom Gehrels at the Palomar Observatory in California .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Lycomedes","was","discovered","on","26","September","1960",",","by","Dutch","astronomer","couple","Ingrid","van","Houten-Groeneveld","and","Cornelis","van","Houten","at","Leiden",",","on","photographic","plate","s","taken","by","astronomer","Tom","Gehrels","at","the","Palomar","Observatory","in","California","."],"labels":["B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","B-location","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","O","B-location","I-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["event","location","enzyme","protein","astronomical_object","chemical_compound","academic_journal","university","theory","country","chemical_element","discipline","person","award","organization","scientist"]}
{"id":"95","dataset":"crossner_science","split":"test","instance":{"id":"95","prompt_labels":"Reverse(B-enzyme) transcriptase(I-enzyme) protocols(O) at(O) the(O) time(O) had(O) difficulties(O) with(O) the(O) secondary(O) structure(O) of(O) mRNA(O) ,(O) leading(O) to(O) abbreviated(O) cDNAs(O) that(O) were(O) difficult(O) to(O) align(O) and(O) invited(O) further(O) complications(O) in(O) downstream(O) analysis(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, chemical compound, scientist, enzyme, theory, protein, university, organization, astronomical object, award, discipline, location, chemical element, academic journal, person, country and O.\nSentence: Reverse transcriptase protocols at the time had difficulties with the secondary structure of mRNA , leading to abbreviated cDNAs that were difficult to align and invited further complications in downstream analysis .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Reverse","transcriptase","protocols","at","the","time","had","difficulties","with","the","secondary","structure","of","mRNA",",","leading","to","abbreviated","cDNAs","that","were","difficult","to","align","and","invited","further","complications","in","downstream","analysis","."],"labels":["B-enzyme","I-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","chemical_compound","scientist","enzyme","theory","protein","university","organization","astronomical_object","award","discipline","location","chemical_element","academic_journal","person","country"]}
{"id":"101","dataset":"crossner_science","split":"test","instance":{"id":"101","prompt_labels":"The(O) minor(O) planets(O) 1062(B-astronomical object) Ljuba(I-astronomical object) and(O) 1086(B-astronomical object) Nata(I-astronomical object) were(O) also(O) named(O) after(O) Soviet(O) female(O) paratroopers(O) Lyuba(B-person) Berlin(I-person) ((O) 1915-1936(O) )(O) and(O) Nata(B-person) Babushkina(I-person) ((O) 1915-1936(O) )(O) ,(O) respectively(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, astronomical object, award, discipline, university, country, theory, location, organization, person, protein, chemical element, enzyme, event, scientist, academic journal and O.\nSentence: The minor planets 1062 Ljuba and 1086 Nata were also named after Soviet female paratroopers Lyuba Berlin ( 1915-1936 ) and Nata Babushkina ( 1915-1936 ) , respectively .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","minor","planets","1062","Ljuba","and","1086","Nata","were","also","named","after","Soviet","female","paratroopers","Lyuba","Berlin","(","1915-1936",")","and","Nata","Babushkina","(","1915-1936",")",",","respectively","."],"labels":["O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","B-person","I-person","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","astronomical_object","award","discipline","university","country","theory","location","organization","person","protein","chemical_element","enzyme","event","scientist","academic_journal"]}
{"id":"110","dataset":"crossner_science","split":"test","instance":{"id":"110","prompt_labels":"Excessive(O) amounts(O) of(O) chlorine(B-chemical element) in(O) the(O) buffer(O) should(O) also(O) be(O) avoided(O) ,(O) since(O) this(O) will(O) overlap(O) with(O) the(O) sulfur(B-chemical element) peak(O) ;(O) Potassium(B-chemical compound) bromide(I-chemical compound) and(O) Sodium(B-chemical compound) bromide(I-chemical compound) are(O) suitable(O) alternatives(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, academic journal, chemical compound, country, event, university, chemical element, award, protein, scientist, person, organization, location, discipline, astronomical object, enzyme and O.\nSentence: Excessive amounts of chlorine in the buffer should also be avoided , since this will overlap with the sulfur peak ; Potassium bromide and Sodium bromide are suitable alternatives .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Excessive","amounts","of","chlorine","in","the","buffer","should","also","be","avoided",",","since","this","will","overlap","with","the","sulfur","peak",";","Potassium","bromide","and","Sodium","bromide","are","suitable","alternatives","."],"labels":["O","O","O","B-chemical element","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical element","O","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["theory","academic_journal","chemical_compound","country","event","university","chemical_element","award","protein","scientist","person","organization","location","discipline","astronomical_object","enzyme"]}
{"id":"111","dataset":"crossner_science","split":"test","instance":{"id":"111","prompt_labels":"The(O) original(O) body(O) farm(O) is(O) the(O) University(B-university) of(I-university) Tennessee(I-university) Anthropological(I-university) Research(I-university) Facility(I-university) located(O) a(O) few(O) miles(O) from(O) downtown(O) on(O) Alcoa(B-location) Highway(I-location) in(I-location) Tennessee(B-location) ,(O) behind(O) the(O) University(B-university) of(I-university) Tennessee(I-university) Medical(I-university) Center(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, person, enzyme, event, organization, award, theory, academic journal, location, country, chemical element, astronomical object, scientist, university, protein, discipline and O.\nSentence: The original body farm is the University of Tennessee Anthropological Research Facility located a few miles from downtown on Alcoa Highway in Tennessee , behind the University of Tennessee Medical Center .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","original","body","farm","is","the","University","of","Tennessee","Anthropological","Research","Facility","located","a","few","miles","from","downtown","on","Alcoa","Highway","in","Tennessee",",","behind","the","University","of","Tennessee","Medical","Center","."],"labels":["O","O","O","O","O","O","B-university","I-university","I-university","I-university","I-university","I-university","O","O","O","O","O","O","O","B-location","I-location","I-location","B-location","O","O","O","B-university","I-university","I-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","person","enzyme","event","organization","award","theory","academic_journal","location","country","chemical_element","astronomical_object","scientist","university","protein","discipline"]}
{"id":"113","dataset":"crossner_science","split":"test","instance":{"id":"113","prompt_labels":"The(O) three(O) required(O) enzyme(O) activities(O) are(O) :(O) exonuclease(B-enzyme) ,(O) DNA(B-enzyme) polymerase(I-enzyme) ,(O) and(O) DNA(B-enzyme) ligase(I-enzyme) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, country, person, chemical element, organization, location, award, enzyme, scientist, theory, university, event, chemical compound, academic journal, astronomical object, discipline and O.\nSentence: The three required enzyme activities are : exonuclease , DNA polymerase , and DNA ligase .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","three","required","enzyme","activities","are",":","exonuclease",",","DNA","polymerase",",","and","DNA","ligase","."],"labels":["O","O","O","O","O","O","O","B-enzyme","O","B-enzyme","I-enzyme","O","O","B-enzyme","I-enzyme","O"],"target_index":null,"target_label":null},"label_list":["protein","country","person","chemical_element","organization","location","award","enzyme","scientist","theory","university","event","chemical_compound","academic_journal","astronomical_object","discipline"]}
{"id":"114","dataset":"crossner_science","split":"test","instance":{"id":"114","prompt_labels":"Computer(O) programs(O) that(O) solve(O) these(O) problems(O) have(O) been(O) used(O) to(O) research(O) a(O) broad(O) range(O) of(O) scientific(O) topics(O) from(O) Adenosine(B-chemical compound) diphosphate(I-chemical compound) to(O) breast(O) cancer(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, chemical element, theory, chemical compound, location, academic journal, enzyme, scientist, event, discipline, university, astronomical object, protein, country, organization, award and O.\nSentence: Computer programs that solve these problems have been used to research a broad range of scientific topics from Adenosine diphosphate to breast cancer .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Computer","programs","that","solve","these","problems","have","been","used","to","research","a","broad","range","of","scientific","topics","from","Adenosine","diphosphate","to","breast","cancer","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","chemical_element","theory","chemical_compound","location","academic_journal","enzyme","scientist","event","discipline","university","astronomical_object","protein","country","organization","award"]}
{"id":"115","dataset":"crossner_science","split":"test","instance":{"id":"115","prompt_labels":"Observations(O) are(O) currently(O) being(O) coordinated(O) by(O) the(O) Association(B-organization) of(I-organization) Lunar(I-organization) and(I-organization) Planetary(I-organization) Observers(I-organization) and(O) the(O) British(B-organization) Astronomical(I-organization) Association(I-organization) to(O) re-observe(O) sites(O) where(O) transient(O) lunar(O) phenomena(O) were(O) reported(O) in(O) the(O) past(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, enzyme, country, chemical element, organization, person, discipline, astronomical object, chemical compound, theory, event, scientist, academic journal, university, award, protein and O.\nSentence: Observations are currently being coordinated by the Association of Lunar and Planetary Observers and the British Astronomical Association to re-observe sites where transient lunar phenomena were reported in the past .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Observations","are","currently","being","coordinated","by","the","Association","of","Lunar","and","Planetary","Observers","and","the","British","Astronomical","Association","to","re-observe","sites","where","transient","lunar","phenomena","were","reported","in","the","past","."],"labels":["O","O","O","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","enzyme","country","chemical_element","organization","person","discipline","astronomical_object","chemical_compound","theory","event","scientist","academic_journal","university","award","protein"]}
{"id":"134","dataset":"crossner_science","split":"test","instance":{"id":"134","prompt_labels":"Topics(O) included(O) matter(O) creation(O) and(O) annihilation(O) ,(O) the(O) fundamental(O) interaction(O) s(O) ,(O) elementary(O) particle(O) s(O) and(O) their(O) currents(O) ,(O) hadron(O) ic(O) and(O) lepton(O) ic(O) physics(B-discipline) ,(O) and(O) the(O) parton(O) model(O) ,(O) published(O) in(O) professional(O) peer-reviewed(O) scientific(O) journal(O) s(O) including(O) Nuclear(B-academic journal) Physics(I-academic journal) B(I-academic journal) ,(O) Australian(B-academic journal) Journal(I-academic journal) of(I-academic journal) Physics(I-academic journal) ,(O) Nuovo(B-academic journal) Cimento(I-academic journal) ,(O) and(O) Physical(B-academic journal) Review(I-academic journal) D(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, academic journal, country, astronomical object, organization, chemical compound, event, scientist, discipline, person, chemical element, enzyme, university, protein, location, award and O.\nSentence: Topics included matter creation and annihilation , the fundamental interaction s , elementary particle s and their currents , hadron ic and lepton ic physics , and the parton model , published in professional peer-reviewed scientific journal s including Nuclear Physics B , Australian Journal of Physics , Nuovo Cimento , and Physical Review D .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Topics","included","matter","creation","and","annihilation",",","the","fundamental","interaction","s",",","elementary","particle","s","and","their","currents",",","hadron","ic","and","lepton","ic","physics",",","and","the","parton","model",",","published","in","professional","peer-reviewed","scientific","journal","s","including","Nuclear","Physics","B",",","Australian","Journal","of","Physics",",","Nuovo","Cimento",",","and","Physical","Review","D","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-discipline","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","O","O","B-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["theory","academic_journal","country","astronomical_object","organization","chemical_compound","event","scientist","discipline","person","chemical_element","enzyme","university","protein","location","award"]}
{"id":"145","dataset":"crossner_science","split":"test","instance":{"id":"145","prompt_labels":"Three(O) of(O) the(O) newly(O) confirmed(O) exoplanets(O) were(O) found(O) to(O) orbit(O) within(O) habitable(O) zones(O) of(O) their(O) related(O) star(O) s(O) :(O) two(O) of(O) the(O) three(O) ,(O) Kepler-438b(B-astronomical object) and(O) Kepler-442b(B-astronomical object) ,(O) are(O) near-Earth-size(O) and(O) likely(O) rocky(O) ;(O) the(O) third(O) ,(O) Kepler-440b(B-astronomical object) ,(O) is(O) a(O) super-Earth(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, enzyme, country, university, discipline, organization, theory, academic journal, location, award, protein, event, chemical compound, astronomical object, scientist, chemical element and O.\nSentence: Three of the newly confirmed exoplanets were found to orbit within habitable zones of their related star s : two of the three , Kepler-438b and Kepler-442b , are near-Earth-size and likely rocky ; the third , Kepler-440b , is a super-Earth .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Three","of","the","newly","confirmed","exoplanets","were","found","to","orbit","within","habitable","zones","of","their","related","star","s",":","two","of","the","three",",","Kepler-438b","and","Kepler-442b",",","are","near-Earth-size","and","likely","rocky",";","the","third",",","Kepler-440b",",","is","a","super-Earth","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","enzyme","country","university","discipline","organization","theory","academic_journal","location","award","protein","event","chemical_compound","astronomical_object","scientist","chemical_element"]}
{"id":"150","dataset":"crossner_science","split":"test","instance":{"id":"150","prompt_labels":"With(O) the(O) first(O) group(O) interacts(O) one(O) plaque(B-protein) protein(I-protein) without(O) PDZ(O) domain(O) ,(O) called(O) cingulin(B-protein) ,(O) which(O) plays(O) a(O) key(O) role(O) in(O) the(O) cell(O) adhesion(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, theory, protein, organization, enzyme, chemical compound, location, discipline, scientist, chemical element, country, university, academic journal, astronomical object, event, person and O.\nSentence: With the first group interacts one plaque protein without PDZ domain , called cingulin , which plays a key role in the cell adhesion .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["With","the","first","group","interacts","one","plaque","protein","without","PDZ","domain",",","called","cingulin",",","which","plays","a","key","role","in","the","cell","adhesion","."],"labels":["O","O","O","O","O","O","B-protein","I-protein","O","O","O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","theory","protein","organization","enzyme","chemical_compound","location","discipline","scientist","chemical_element","country","university","academic_journal","astronomical_object","event","person"]}
{"id":"156","dataset":"crossner_science","split":"test","instance":{"id":"156","prompt_labels":"Encke(B-astronomical object) ((O) 1850(O) )(O) has(O) further(O) symbols(O) for(O) 5(B-astronomical object) Astraea(I-astronomical object) ,(O) 6(B-astronomical object) Hebe(I-astronomical object) ,(O) 7(B-astronomical object) Iris(I-astronomical object) ,(O) 8(B-astronomical object) Flora(I-astronomical object) and(O) 9(B-astronomical object) Metis(I-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, person, discipline, chemical element, country, organization, theory, award, chemical compound, astronomical object, enzyme, university, scientist, event, protein, location and O.\nSentence: Encke ( 1850 ) has further symbols for 5 Astraea , 6 Hebe , 7 Iris , 8 Flora and 9 Metis .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Encke","(","1850",")","has","further","symbols","for","5","Astraea",",","6","Hebe",",","7","Iris",",","8","Flora","and","9","Metis","."],"labels":["B-astronomical object","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","person","discipline","chemical_element","country","organization","theory","award","chemical_compound","astronomical_object","enzyme","university","scientist","event","protein","location"]}
{"id":"157","dataset":"crossner_science","split":"test","instance":{"id":"157","prompt_labels":"ABC(B-organization) Family(I-organization) Worldwide(I-organization) Inc(I-organization) ((O) ABC(O) Family(O) Channel(O) )(O) ,(O) Regent(B-university) University(I-university) ,(O) the(O) American(B-organization) Center(I-organization) for(I-organization) Law(I-organization) &(I-organization) Justice(I-organization) ((O) ACLJ(B-organization) )(O) ,(O) the(O) Founders(B-organization) Inn(I-organization) and(I-organization) Conference(I-organization) Center(I-organization) ,(O) the(O) Christian(B-organization) Coalition(I-organization) of(I-organization) America(I-organization) ,(O) an(O) L-1011(B-organization) Flying(I-organization) Hospital(I-organization) ,(O) Operation(B-organization) Blessing(I-organization) International(I-organization) ,(O) and(O) CBN(B-organization) Asia(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, chemical element, country, protein, academic journal, theory, university, discipline, award, person, chemical compound, enzyme, organization, location, astronomical object, event and O.\nSentence: ABC Family Worldwide Inc ( ABC Family Channel ) , Regent University , the American Center for Law & Justice ( ACLJ ) , the Founders Inn and Conference Center , the Christian Coalition of America , an L-1011 Flying Hospital , Operation Blessing International , and CBN Asia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["ABC","Family","Worldwide","Inc","(","ABC","Family","Channel",")",",","Regent","University",",","the","American","Center","for","Law","&","Justice","(","ACLJ",")",",","the","Founders","Inn","and","Conference","Center",",","the","Christian","Coalition","of","America",",","an","L-1011","Flying","Hospital",",","Operation","Blessing","International",",","and","CBN","Asia","."],"labels":["B-organization","I-organization","I-organization","I-organization","O","O","O","O","O","O","B-university","I-university","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["scientist","chemical_element","country","protein","academic_journal","theory","university","discipline","award","person","chemical_compound","enzyme","organization","location","astronomical_object","event"]}
{"id":"168","dataset":"crossner_science","split":"test","instance":{"id":"168","prompt_labels":"When(O) he(O) was(O) about(O) eighteen(O) he(O) went(O) to(O) the(O) University(B-university) of(I-university) Copenhagen(I-university) and(O) afterwards(O) studied(O) at(O) University(B-university) of(I-university) Rostock(I-university) and(I-university) Wittenberg(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, enzyme, chemical compound, discipline, scientist, academic journal, organization, event, theory, award, protein, chemical element, person, country, university, location and O.\nSentence: When he was about eighteen he went to the University of Copenhagen and afterwards studied at University of Rostock and Wittenberg .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["When","he","was","about","eighteen","he","went","to","the","University","of","Copenhagen","and","afterwards","studied","at","University","of","Rostock","and","Wittenberg","."],"labels":["O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","O","O","O","O","B-university","I-university","I-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","enzyme","chemical_compound","discipline","scientist","academic_journal","organization","event","theory","award","protein","chemical_element","person","country","university","location"]}
{"id":"169","dataset":"crossner_science","split":"test","instance":{"id":"169","prompt_labels":"Together(O) with(O) Roger(B-scientist) Guillemin(I-scientist) he(O) described(O) the(O) neurohormone(O) GnRH(O) that(O) controls(O) Follicle-stimulating(B-protein) hormone(I-protein) and(O) Luteinizing(B-protein) hormone(I-protein) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, country, enzyme, person, organization, chemical element, astronomical object, scientist, protein, theory, award, chemical compound, location, academic journal, discipline, event and O.\nSentence: Together with Roger Guillemin he described the neurohormone GnRH that controls Follicle-stimulating hormone and Luteinizing hormone .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Together","with","Roger","Guillemin","he","described","the","neurohormone","GnRH","that","controls","Follicle-stimulating","hormone","and","Luteinizing","hormone","."],"labels":["O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","B-protein","I-protein","O","B-protein","I-protein","O"],"target_index":null,"target_label":null},"label_list":["university","country","enzyme","person","organization","chemical_element","astronomical_object","scientist","protein","theory","award","chemical_compound","location","academic_journal","discipline","event"]}
{"id":"184","dataset":"crossner_science","split":"test","instance":{"id":"184","prompt_labels":"In(O) the(O) 1840s(O) ,(O) Urbain(B-scientist) Le(I-scientist) Verrier(I-scientist) used(O) Newtonian(B-discipline) mechanics(I-discipline) to(O) predict(O) the(O) position(O) of(O) the(O) then-undiscovered(O) planet(O) Neptune(B-astronomical object) after(O) analyzing(O) perturbations(O) in(O) the(O) orbit(O) of(O) Uranus(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, theory, country, chemical element, enzyme, protein, discipline, astronomical object, event, location, chemical compound, scientist, person, university, award, organization and O.\nSentence: In the 1840s , Urbain Le Verrier used Newtonian mechanics to predict the position of the then-undiscovered planet Neptune after analyzing perturbations in the orbit of Uranus .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","the","1840s",",","Urbain","Le","Verrier","used","Newtonian","mechanics","to","predict","the","position","of","the","then-undiscovered","planet","Neptune","after","analyzing","perturbations","in","the","orbit","of","Uranus","."],"labels":["O","O","O","O","B-scientist","I-scientist","I-scientist","O","B-discipline","I-discipline","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","theory","country","chemical_element","enzyme","protein","discipline","astronomical_object","event","location","chemical_compound","scientist","person","university","award","organization"]}
{"id":"187","dataset":"crossner_science","split":"test","instance":{"id":"187","prompt_labels":"The(O) quarry(O) is(O) at(O) Herniss(B-location) ,(O) to(O) the(O) north(O) of(O) the(O) A(B-location) 394(I-location) road(I-location) ,(O) between(O) Rame(B-location) and(O) Longdowns(B-location) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, award, organization, chemical compound, discipline, chemical element, university, country, academic journal, location, event, protein, enzyme, theory, scientist, person and O.\nSentence: The quarry is at Herniss , to the north of the A 394 road , between Rame and Longdowns .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","quarry","is","at","Herniss",",","to","the","north","of","the","A","394","road",",","between","Rame","and","Longdowns","."],"labels":["O","O","O","O","B-location","O","O","O","O","O","O","B-location","I-location","I-location","O","O","B-location","O","B-location","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","award","organization","chemical_compound","discipline","chemical_element","university","country","academic_journal","location","event","protein","enzyme","theory","scientist","person"]}
{"id":"188","dataset":"crossner_science","split":"test","instance":{"id":"188","prompt_labels":"The(O) schemas(O) targeted(O) by(O) emotional(O) selection(O) are(O) those(O) essential(O) for(O) meeting(O) human(O) needs(O) ,(O) such(O) as(O) those(O) define(O) by(O) Abraham(B-scientist) Maslow(I-scientist) and(O) Henry(B-scientist) Murray(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, theory, location, scientist, astronomical object, enzyme, person, event, organization, academic journal, discipline, chemical element, award, country, chemical compound, university and O.\nSentence: The schemas targeted by emotional selection are those essential for meeting human needs , such as those define by Abraham Maslow and Henry Murray .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","schemas","targeted","by","emotional","selection","are","those","essential","for","meeting","human","needs",",","such","as","those","define","by","Abraham","Maslow","and","Henry","Murray","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["protein","theory","location","scientist","astronomical_object","enzyme","person","event","organization","academic_journal","discipline","chemical_element","award","country","chemical_compound","university"]}
{"id":"192","dataset":"crossner_science","split":"test","instance":{"id":"192","prompt_labels":"Pathogenic(O) spirochetes(O) ,(O) including(O) B.(O) burgdorferi(O) and(O) T.(O) pallidum(O) ,(O) use(O) their(O) proteolipid(O) Bacterial(O) adhesin(O) s(O) to(O) stick(O) to(O) victim(O) cells(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, theory, scientist, person, enzyme, country, astronomical object, chemical compound, event, chemical element, location, discipline, protein, organization, award, academic journal and O.\nSentence: Pathogenic spirochetes , including B. burgdorferi and T. pallidum , use their proteolipid Bacterial adhesin s to stick to victim cells .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Pathogenic","spirochetes",",","including","B.","burgdorferi","and","T.","pallidum",",","use","their","proteolipid","Bacterial","adhesin","s","to","stick","to","victim","cells","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","theory","scientist","person","enzyme","country","astronomical_object","chemical_compound","event","chemical_element","location","discipline","protein","organization","award","academic_journal"]}
{"id":"195","dataset":"crossner_science","split":"test","instance":{"id":"195","prompt_labels":"He(O) studied(O) physics(B-discipline) at(O) the(O) University(B-university) of(I-university) Heidelberg(I-university) ,(O) the(O) University(B-university) of(I-university) Lausanne(I-university) ,(O) and(O) ,(O) ultimately(O) ,(O) the(O) Humboldt(B-university) University(I-university) of(I-university) Berlin(I-university) under(O) Max(B-scientist) Planck(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, protein, location, award, organization, academic journal, event, scientist, country, discipline, person, university, chemical element, chemical compound, enzyme, theory and O.\nSentence: He studied physics at the University of Heidelberg , the University of Lausanne , and , ultimately , the Humboldt University of Berlin under Max Planck .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","studied","physics","at","the","University","of","Heidelberg",",","the","University","of","Lausanne",",","and",",","ultimately",",","the","Humboldt","University","of","Berlin","under","Max","Planck","."],"labels":["O","O","B-discipline","O","O","B-university","I-university","I-university","O","O","B-university","I-university","I-university","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","protein","location","award","organization","academic_journal","event","scientist","country","discipline","person","university","chemical_element","chemical_compound","enzyme","theory"]}
{"id":"197","dataset":"crossner_science","split":"test","instance":{"id":"197","prompt_labels":"The(O) Chun-Tsung(B-award) scholarships(I-award) ,(O) supervised(O) by(O) the(O) United(B-organization) Board(I-organization) for(I-organization) Christian(I-organization) Higher(I-organization) Education(I-organization) in(I-organization) Asia(I-organization) ((O) New(B-location) York(I-location) )(O) ,(O) are(O) awarded(O) to(O) undergraduates(O) ,(O) usually(O) in(O) their(O) 2nd(O) or(O) 3rd(O) year(O) ,(O) at(O) six(O) universities(O) ,(O) which(O) are(O) Shanghai(B-university) Jiaotong(I-university) University(I-university) ,(O) Fudan(B-university) University(I-university) ,(O) Lanzhou(B-university) University(I-university) ,(O) Soochow(B-university) University(I-university) ,(O) Peking(B-university) University(I-university) and(O) Taiwan(B-university) National(I-university) Tsing(I-university) Hua(I-university) University(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, chemical element, country, discipline, award, protein, astronomical object, location, organization, enzyme, academic journal, event, scientist, university, chemical compound, person and O.\nSentence: The Chun-Tsung scholarships , supervised by the United Board for Christian Higher Education in Asia ( New York ) , are awarded to undergraduates , usually in their 2nd or 3rd year , at six universities , which are Shanghai Jiaotong University , Fudan University , Lanzhou University , Soochow University , Peking University and Taiwan National Tsing Hua University .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Chun-Tsung","scholarships",",","supervised","by","the","United","Board","for","Christian","Higher","Education","in","Asia","(","New","York",")",",","are","awarded","to","undergraduates",",","usually","in","their","2nd","or","3rd","year",",","at","six","universities",",","which","are","Shanghai","Jiaotong","University",",","Fudan","University",",","Lanzhou","University",",","Soochow","University",",","Peking","University","and","Taiwan","National","Tsing","Hua","University","."],"labels":["O","B-award","I-award","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-university","I-university","I-university","O","B-university","I-university","O","B-university","I-university","O","B-university","I-university","O","B-university","I-university","O","B-university","I-university","I-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["theory","chemical_element","country","discipline","award","protein","astronomical_object","location","organization","enzyme","academic_journal","event","scientist","university","chemical_compound","person"]}
{"id":"200","dataset":"crossner_science","split":"test","instance":{"id":"200","prompt_labels":"The(O) Arts(B-organization) Centre(I-organization) Melbourne(I-organization) ((O) which(O) includes(O) the(O) State(B-location) Theatre(I-location) ,(O) Hamer(B-location) Hall(I-location) ,(O) the(O) Playhouse(B-location) and(O) the(O) Fairfax(B-location) Studio(I-location) )(O) ,(O) and(O) the(O) Melbourne(B-location) Recital(I-location) Centre(I-location) are(O) located(O) just(O) to(O) the(O) south(O) of(O) the(O) CBD(B-location) ,(O) with(O) the(O) Sidney(B-location) Myer(I-location) Music(I-location) Bowl(I-location) in(O) parklands(B-location) to(O) the(O) east(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, chemical compound, location, enzyme, protein, academic journal, theory, university, event, discipline, astronomical object, person, scientist, chemical element, award, country and O.\nSentence: The Arts Centre Melbourne ( which includes the State Theatre , Hamer Hall , the Playhouse and the Fairfax Studio ) , and the Melbourne Recital Centre are located just to the south of the CBD , with the Sidney Myer Music Bowl in parklands to the east .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Arts","Centre","Melbourne","(","which","includes","the","State","Theatre",",","Hamer","Hall",",","the","Playhouse","and","the","Fairfax","Studio",")",",","and","the","Melbourne","Recital","Centre","are","located","just","to","the","south","of","the","CBD",",","with","the","Sidney","Myer","Music","Bowl","in","parklands","to","the","east","."],"labels":["O","B-organization","I-organization","I-organization","O","O","O","O","B-location","I-location","O","B-location","I-location","O","O","B-location","O","O","B-location","I-location","O","O","O","O","B-location","I-location","I-location","O","O","O","O","O","O","O","O","B-location","O","O","O","B-location","I-location","I-location","I-location","O","B-location","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","chemical_compound","location","enzyme","protein","academic_journal","theory","university","event","discipline","astronomical_object","person","scientist","chemical_element","award","country"]}
{"id":"201","dataset":"crossner_science","split":"test","instance":{"id":"201","prompt_labels":"In(O) addition(O) ,(O) he(O) is(O) the(O) founding(O) editor(O) of(O) Theoretical(B-academic journal) Population(I-academic journal) Biology(I-academic journal) ((O) 1971-2013(O) )(O) and(O) an(O) associate(O) editor(O) of(O) Genetics(B-academic journal) ,(O) Human(B-academic journal) Genetics(I-academic journal) ,(O) Annals(B-academic journal) of(I-academic journal) Human(I-academic journal) Genetics(I-academic journal) ,(O) Annals(B-academic journal) of(I-academic journal) Human(I-academic journal) Biology(I-academic journal) ,(O) and(O) Complexity(B-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, chemical element, astronomical object, location, theory, person, university, chemical compound, scientist, protein, event, academic journal, enzyme, award, organization, country and O.\nSentence: In addition , he is the founding editor of Theoretical Population Biology ( 1971-2013 ) and an associate editor of Genetics , Human Genetics , Annals of Human Genetics , Annals of Human Biology , and Complexity .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","addition",",","he","is","the","founding","editor","of","Theoretical","Population","Biology","(","1971-2013",")","and","an","associate","editor","of","Genetics",",","Human","Genetics",",","Annals","of","Human","Genetics",",","Annals","of","Human","Biology",",","and","Complexity","."],"labels":["O","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","O","O","B-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","B-academic journal","O"],"target_index":null,"target_label":null},"label_list":["discipline","chemical_element","astronomical_object","location","theory","person","university","chemical_compound","scientist","protein","event","academic_journal","enzyme","award","organization","country"]}
{"id":"205","dataset":"crossner_science","split":"test","instance":{"id":"205","prompt_labels":"His(O) work(O) has(O) appeared(O) in(O) leading(O) scientific(O) journals(O) such(O) as(O) Nature(B-academic journal) ,(O) Science(B-academic journal) ,(O) Nature(B-academic journal) Genetics(I-academic journal) ,(O) Molecular(B-academic journal) Biology(I-academic journal) and(I-academic journal) Evolution(I-academic journal) ,(O) Journal(B-academic journal) of(I-academic journal) Evolutionary(I-academic journal) Biology(I-academic journal) ,(O) as(O) well(O) as(O) popular(O) magazines(O) such(O) as(O) Scientific(B-academic journal) American(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, location, enzyme, chemical compound, discipline, university, person, country, academic journal, scientist, protein, chemical element, award, event, theory, astronomical object and O.\nSentence: His work has appeared in leading scientific journals such as Nature , Science , Nature Genetics , Molecular Biology and Evolution , Journal of Evolutionary Biology , as well as popular magazines such as Scientific American .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","work","has","appeared","in","leading","scientific","journals","such","as","Nature",",","Science",",","Nature","Genetics",",","Molecular","Biology","and","Evolution",",","Journal","of","Evolutionary","Biology",",","as","well","as","popular","magazines","such","as","Scientific","American","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-academic journal","O","B-academic journal","O","B-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","O","O","B-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["organization","location","enzyme","chemical_compound","discipline","university","person","country","academic_journal","scientist","protein","chemical_element","award","event","theory","astronomical_object"]}
{"id":"207","dataset":"crossner_science","split":"test","instance":{"id":"207","prompt_labels":"Bangladesh(B-country) is(O) a(O) member(O) of(O) the(O) UN(B-organization) ,(O) World(B-organization) Trade(I-organization) Organization(I-organization) ,(O) International(B-organization) Monetary(I-organization) Fund(I-organization) ,(O) the(O) World(B-organization) Bank(I-organization) ,(O) Asian(B-organization) Development(I-organization) Bank(I-organization) ,(O) OIC(B-organization) ,(O) Islamic(B-organization) Development(I-organization) Bank(I-organization) ,(O) SAARC(B-organization) ,(O) BIMSTEC(B-organization) and(O) the(O) Islamic(B-organization) Military(I-organization) Counter(I-organization) Terrorism(I-organization) Coalition(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, country, theory, university, location, chemical compound, organization, astronomical object, enzyme, chemical element, person, event, protein, academic journal, discipline, scientist and O.\nSentence: Bangladesh is a member of the UN , World Trade Organization , International Monetary Fund , the World Bank , Asian Development Bank , OIC , Islamic Development Bank , SAARC , BIMSTEC and the Islamic Military Counter Terrorism Coalition .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Bangladesh","is","a","member","of","the","UN",",","World","Trade","Organization",",","International","Monetary","Fund",",","the","World","Bank",",","Asian","Development","Bank",",","OIC",",","Islamic","Development","Bank",",","SAARC",",","BIMSTEC","and","the","Islamic","Military","Counter","Terrorism","Coalition","."],"labels":["B-country","O","O","O","O","O","B-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","B-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","O","B-organization","I-organization","I-organization","O","B-organization","O","B-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["award","country","theory","university","location","chemical_compound","organization","astronomical_object","enzyme","chemical_element","person","event","protein","academic_journal","discipline","scientist"]}
{"id":"242","dataset":"crossner_science","split":"test","instance":{"id":"242","prompt_labels":"The(O) Anti-Defamation(B-organization) League(I-organization) and(O) the(O) Southern(B-organization) Poverty(I-organization) Law(I-organization) Center(I-organization) assert(O) that(O) Pierce(B-person) created(O) cosmotheism(B-theory) in(O) order(O) to(O) acquire(O) tax-exempt(O) status(O) for(O) the(O) National(B-organization) Alliance(I-organization) after(O) he(O) had(O) failed(O) to(O) do(O) so(O) earlier(O) ,(O) and(O) the(O) SPLC(B-organization) refers(O) to(O) it(O) as(O) a(O) bogus(O) religion(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, person, event, chemical compound, university, organization, astronomical object, enzyme, academic journal, discipline, scientist, chemical element, protein, country, award, theory and O.\nSentence: The Anti-Defamation League and the Southern Poverty Law Center assert that Pierce created cosmotheism in order to acquire tax-exempt status for the National Alliance after he had failed to do so earlier , and the SPLC refers to it as a bogus religion .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","Anti-Defamation","League","and","the","Southern","Poverty","Law","Center","assert","that","Pierce","created","cosmotheism","in","order","to","acquire","tax-exempt","status","for","the","National","Alliance","after","he","had","failed","to","do","so","earlier",",","and","the","SPLC","refers","to","it","as","a","bogus","religion","."],"labels":["O","B-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","O","O","B-person","O","B-theory","O","O","O","O","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","B-organization","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","person","event","chemical_compound","university","organization","astronomical_object","enzyme","academic_journal","discipline","scientist","chemical_element","protein","country","award","theory"]}
{"id":"263","dataset":"crossner_science","split":"test","instance":{"id":"263","prompt_labels":"((O) See(O) Noyori(O) asymmetric(O) hydrogenation(O) )(O) Asymmetric(O) hydrogenation(O) of(O) an(O) alkene(B-chemical compound) in(O) the(O) presence(O) of(O) ((O) ((O) S(B-chemical element) )(O) -(O) BINAP(B-chemical compound) )(O) Ru(B-chemical element) ((O) Acetate(B-chemical compound) )(O) sub2(O) /(O) sub(O) is(O) used(O) for(O) the(O) commercial(O) production(O) of(O) enantiomerically(O) pure(O) ((O) 97(O) %(O) ee(B-chemical compound) )(O) naproxen(B-chemical compound) ,(O) used(O) as(O) an(O) anti-inflammatory(O) drug(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, person, discipline, theory, chemical element, enzyme, academic journal, university, location, award, protein, event, astronomical object, country, organization, scientist and O.\nSentence: ( See Noyori asymmetric hydrogenation ) Asymmetric hydrogenation of an alkene in the presence of ( ( S ) - BINAP ) Ru ( Acetate ) sub2 / sub is used for the commercial production of enantiomerically pure ( 97 % ee ) naproxen , used as an anti-inflammatory drug .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["(","See","Noyori","asymmetric","hydrogenation",")","Asymmetric","hydrogenation","of","an","alkene","in","the","presence","of","(","(","S",")","-","BINAP",")","Ru","(","Acetate",")","sub2","/","sub","is","used","for","the","commercial","production","of","enantiomerically","pure","(","97","%","ee",")","naproxen",",","used","as","an","anti-inflammatory","drug","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-chemical compound","O","O","O","O","O","O","B-chemical element","O","O","B-chemical compound","O","B-chemical element","O","B-chemical compound","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","O","B-chemical compound","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","person","discipline","theory","chemical_element","enzyme","academic_journal","university","location","award","protein","event","astronomical_object","country","organization","scientist"]}
{"id":"264","dataset":"crossner_science","split":"test","instance":{"id":"264","prompt_labels":"At(O) the(O) end(O) of(O) May(O) ,(O) Mercury(B-astronomical object) ,(O) Venus(B-astronomical object) and(O) Jupiter(B-astronomical object) went(O) through(O) a(O) series(O) of(O) conjunctions(O) only(O) a(O) few(O) days(O) apart(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, enzyme, event, location, organization, country, theory, award, scientist, academic journal, chemical compound, protein, person, chemical element, discipline, university and O.\nSentence: At the end of May , Mercury , Venus and Jupiter went through a series of conjunctions only a few days apart .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["At","the","end","of","May",",","Mercury",",","Venus","and","Jupiter","went","through","a","series","of","conjunctions","only","a","few","days","apart","."],"labels":["O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","enzyme","event","location","organization","country","theory","award","scientist","academic_journal","chemical_compound","protein","person","chemical_element","discipline","university"]}
{"id":"269","dataset":"crossner_science","split":"test","instance":{"id":"269","prompt_labels":"Three(O) of(O) the(O) country(O) 's(O) largest(O) universities(O) are(O) located(O) in(O) central(B-location) Thessaloniki(I-location) :(O) Aristotle(B-university) University(I-university) of(I-university) Thessaloniki(I-university) ,(O) the(O) University(B-university) of(I-university) Macedonia(I-university) and(O) the(O) International(B-university) Hellenic(I-university) University(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical element, protein, country, organization, award, discipline, chemical compound, person, event, astronomical object, scientist, theory, academic journal, location, university and O.\nSentence: Three of the country 's largest universities are located in central Thessaloniki : Aristotle University of Thessaloniki , the University of Macedonia and the International Hellenic University .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Three","of","the","country","'s","largest","universities","are","located","in","central","Thessaloniki",":","Aristotle","University","of","Thessaloniki",",","the","University","of","Macedonia","and","the","International","Hellenic","University","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","B-university","I-university","I-university","I-university","O","O","B-university","I-university","I-university","O","O","B-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["enzyme","chemical_element","protein","country","organization","award","discipline","chemical_compound","person","event","astronomical_object","scientist","theory","academic_journal","location","university"]}
{"id":"274","dataset":"crossner_science","split":"test","instance":{"id":"274","prompt_labels":"Under(O) his(O) leadership(O) the(O) neutron(O) was(O) discovered(O) by(O) James(B-scientist) Chadwick(I-scientist) in(O) 1932(O) and(O) in(O) the(O) same(O) year(O) the(O) first(O) experiment(O) to(O) split(O) the(O) nucleus(O) in(O) a(O) fully(O) controlled(O) manner(O) was(O) performed(O) by(O) students(O) working(O) under(O) his(O) direction(O) ,(O) John(B-scientist) Cockcroft(I-scientist) and(O) Ernest(B-scientist) Walton(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, country, theory, astronomical object, event, chemical element, discipline, enzyme, person, protein, university, scientist, award, organization, chemical compound, academic journal and O.\nSentence: Under his leadership the neutron was discovered by James Chadwick in 1932 and in the same year the first experiment to split the nucleus in a fully controlled manner was performed by students working under his direction , John Cockcroft and Ernest Walton .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Under","his","leadership","the","neutron","was","discovered","by","James","Chadwick","in","1932","and","in","the","same","year","the","first","experiment","to","split","the","nucleus","in","a","fully","controlled","manner","was","performed","by","students","working","under","his","direction",",","John","Cockcroft","and","Ernest","Walton","."],"labels":["O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["location","country","theory","astronomical_object","event","chemical_element","discipline","enzyme","person","protein","university","scientist","award","organization","chemical_compound","academic_journal"]}
{"id":"275","dataset":"crossner_science","split":"test","instance":{"id":"275","prompt_labels":"Whereas(O) the(O) xenon(B-chemical compound) fluorides(I-chemical compound) are(O) well(O) characterized(O) ,(O) with(O) the(O) exception(O) of(O) dichloride(B-chemical compound) Xenon(B-chemical compound) dichloride(I-chemical compound) and(O) Xenon(B-chemical compound) tetrachloride(I-chemical compound) ,(O) the(O) other(O) halides(O) are(O) not(O) known(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, organization, event, scientist, country, chemical compound, person, location, academic journal, chemical element, award, astronomical object, enzyme, theory, discipline, university and O.\nSentence: Whereas the xenon fluorides are well characterized , with the exception of dichloride Xenon dichloride and Xenon tetrachloride , the other halides are not known .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Whereas","the","xenon","fluorides","are","well","characterized",",","with","the","exception","of","dichloride","Xenon","dichloride","and","Xenon","tetrachloride",",","the","other","halides","are","not","known","."],"labels":["O","O","B-chemical compound","I-chemical compound","O","O","O","O","O","O","O","O","B-chemical compound","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["protein","organization","event","scientist","country","chemical_compound","person","location","academic_journal","chemical_element","award","astronomical_object","enzyme","theory","discipline","university"]}
{"id":"277","dataset":"crossner_science","split":"test","instance":{"id":"277","prompt_labels":"Precursors(B-chemical compound) of(O) T(O) cells(O) originate(O) in(O) the(O) bone(O) marrow(O) from(O) which(O) they(O) migrate(O) via(O) bloodstream(O) into(O) thymic(O) cortex(O) ,(O) where(O) they(O) encounter(O) stromal(O) cell(O) s(O) including(O) cTECs(O) ,(O) which(O) form(O) the(O) microenvironment(O) crucial(O) for(O) proliferation(O) and(O) development(O) of(O) T(O) cells(O) by(O) expression(O) of(O) DLL4(B-protein) ((O) delta-like(B-protein) notch(I-protein) ligand(I-protein) 4(I-protein) )(O) ,(O) cytokine(O) s(O) IL-7(B-protein) ,(O) TGFβ(B-protein) or(O) stem(B-protein) cell(I-protein) factor(I-protein) and(O) chemokine(B-protein) s(O) CCL25(B-protein) ,(O) CXCL12(B-protein) or(O) CCRL1(B-protein) etc(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, academic journal, discipline, organization, university, astronomical object, location, chemical compound, person, chemical element, country, enzyme, theory, award, protein, scientist and O.\nSentence: Precursors of T cells originate in the bone marrow from which they migrate via bloodstream into thymic cortex , where they encounter stromal cell s including cTECs , which form the microenvironment crucial for proliferation and development of T cells by expression of DLL4 ( delta-like notch ligand 4 ) , cytokine s IL-7 , TGFβ or stem cell factor and chemokine s CCL25 , CXCL12 or CCRL1 etc .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Precursors","of","T","cells","originate","in","the","bone","marrow","from","which","they","migrate","via","bloodstream","into","thymic","cortex",",","where","they","encounter","stromal","cell","s","including","cTECs",",","which","form","the","microenvironment","crucial","for","proliferation","and","development","of","T","cells","by","expression","of","DLL4","(","delta-like","notch","ligand","4",")",",","cytokine","s","IL-7",",","TGFβ","or","stem","cell","factor","and","chemokine","s","CCL25",",","CXCL12","or","CCRL1","etc","."],"labels":["B-chemical compound","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","O","B-protein","I-protein","I-protein","I-protein","O","O","O","O","B-protein","O","B-protein","O","B-protein","I-protein","I-protein","O","B-protein","O","B-protein","O","B-protein","O","B-protein","O","O"],"target_index":null,"target_label":null},"label_list":["event","academic_journal","discipline","organization","university","astronomical_object","location","chemical_compound","person","chemical_element","country","enzyme","theory","award","protein","scientist"]}
{"id":"291","dataset":"crossner_science","split":"test","instance":{"id":"291","prompt_labels":"She(O) played(O) a(O) substantial(O) role(O) in(O) the(O) discovery(O) of(O) germ(O) cell(O) migratory(O) pathways(O) ((O) namely(O) those(O) involving(O) gap(O) junction(O) s(O) ,(O) G(B-protein) protein-coupled(I-protein) receptor(I-protein) like(O) Tre-1(B-protein) ,(O) and(O) isoprenoids(B-protein) )(O) ,(O) particularly(O) those(O) concerning(O) migration(O) into(O) the(O) ovaries(O) and(O) testis(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, university, country, location, award, theory, scientist, organization, chemical element, chemical compound, event, enzyme, astronomical object, person, discipline, protein and O.\nSentence: She played a substantial role in the discovery of germ cell migratory pathways ( namely those involving gap junction s , G protein-coupled receptor like Tre-1 , and isoprenoids ) , particularly those concerning migration into the ovaries and testis .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","played","a","substantial","role","in","the","discovery","of","germ","cell","migratory","pathways","(","namely","those","involving","gap","junction","s",",","G","protein-coupled","receptor","like","Tre-1",",","and","isoprenoids",")",",","particularly","those","concerning","migration","into","the","ovaries","and","testis","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","I-protein","I-protein","O","B-protein","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","university","country","location","award","theory","scientist","organization","chemical_element","chemical_compound","event","enzyme","astronomical_object","person","discipline","protein"]}
{"id":"293","dataset":"crossner_science","split":"test","instance":{"id":"293","prompt_labels":"It(O) was(O) developed(O) as(O) a(O) treatment(O) for(O) hepatitis(O) C(O) ,(O) acting(O) as(O) a(O) NS5B(B-chemical compound) RNA(I-chemical compound) polymerase(I-chemical compound) inhibitor(I-chemical compound) ,(O) but(O) while(O) it(O) showed(O) a(O) good(O) safety(O) profile(O) in(O) clinical(O) trials(O) ,(O) it(O) was(O) not(O) sufficiently(O) effective(O) to(O) be(O) used(O) as(O) a(O) stand-alone(O) agent(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, theory, university, organization, event, astronomical object, academic journal, enzyme, protein, person, country, location, chemical element, chemical compound, award, discipline and O.\nSentence: It was developed as a treatment for hepatitis C , acting as a NS5B RNA polymerase inhibitor , but while it showed a good safety profile in clinical trials , it was not sufficiently effective to be used as a stand-alone agent .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","was","developed","as","a","treatment","for","hepatitis","C",",","acting","as","a","NS5B","RNA","polymerase","inhibitor",",","but","while","it","showed","a","good","safety","profile","in","clinical","trials",",","it","was","not","sufficiently","effective","to","be","used","as","a","stand-alone","agent","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","I-chemical compound","I-chemical compound","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","theory","university","organization","event","astronomical_object","academic_journal","enzyme","protein","person","country","location","chemical_element","chemical_compound","award","discipline"]}
{"id":"296","dataset":"crossner_science","split":"test","instance":{"id":"296","prompt_labels":"Manchester(B-person) hosted(O) the(O) 2002(B-event) Commonwealth(I-event) Games(I-event) ,(O) which(O) left(O) it(O) a(O) legacy(O) of(O) sporting(O) facilities(O) including(O) the(O) City(B-location) of(I-location) Manchester(I-location) Stadium(I-location) ,(O) Manchester(B-location) Aquatics(I-location) Centre(I-location) and(O) the(O) National(B-location) Cycling(I-location) Centre(I-location) ,(O) headquarters(O) of(O) British(B-organization) Cycling(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, protein, chemical compound, astronomical object, scientist, country, organization, enzyme, person, chemical element, university, award, location, event, theory, discipline and O.\nSentence: Manchester hosted the 2002 Commonwealth Games , which left it a legacy of sporting facilities including the City of Manchester Stadium , Manchester Aquatics Centre and the National Cycling Centre , headquarters of British Cycling .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Manchester","hosted","the","2002","Commonwealth","Games",",","which","left","it","a","legacy","of","sporting","facilities","including","the","City","of","Manchester","Stadium",",","Manchester","Aquatics","Centre","and","the","National","Cycling","Centre",",","headquarters","of","British","Cycling","."],"labels":["B-person","O","O","B-event","I-event","I-event","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","I-location","I-location","O","B-location","I-location","I-location","O","O","B-location","I-location","I-location","O","O","O","B-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","protein","chemical_compound","astronomical_object","scientist","country","organization","enzyme","person","chemical_element","university","award","location","event","theory","discipline"]}
{"id":"299","dataset":"crossner_science","split":"test","instance":{"id":"299","prompt_labels":"Some(O) exceptions(O) are(O) the(O) Taraxacum(B-enzyme) officinale(I-enzyme) ((O) Taraxacum(B-enzyme) officinale(I-enzyme) )(O) ,(O) which(O) since(O) its(O) introduction(O) to(O) the(O) region(O) has(O) proliferated(O) on(O) beaches(O) and(O) shores(O) and(O) near(O) newly(O) opened(O) roads(O) on(O) Baffin(B-location) Island(I-location) ,(O) Barley(O) ((O) Hordeum(O) vulgare(O) )(O) ,(O) shepherd(O) 's(O) purse(O) ((O) Capsella(O) bursa-pastoris(O) )(O) and(O) the(O) opium(O) poppy(O) ((O) Papaver(O) somniferum(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, person, event, academic journal, organization, award, chemical compound, chemical element, protein, country, university, astronomical object, location, scientist, discipline, theory and O.\nSentence: Some exceptions are the Taraxacum officinale ( Taraxacum officinale ) , which since its introduction to the region has proliferated on beaches and shores and near newly opened roads on Baffin Island , Barley ( Hordeum vulgare ) , shepherd 's purse ( Capsella bursa-pastoris ) and the opium poppy ( Papaver somniferum ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","exceptions","are","the","Taraxacum","officinale","(","Taraxacum","officinale",")",",","which","since","its","introduction","to","the","region","has","proliferated","on","beaches","and","shores","and","near","newly","opened","roads","on","Baffin","Island",",","Barley","(","Hordeum","vulgare",")",",","shepherd","'s","purse","(","Capsella","bursa-pastoris",")","and","the","opium","poppy","(","Papaver","somniferum",")","."],"labels":["O","O","O","O","B-enzyme","I-enzyme","O","B-enzyme","I-enzyme","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-location","I-location","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","person","event","academic_journal","organization","award","chemical_compound","chemical_element","protein","country","university","astronomical_object","location","scientist","discipline","theory"]}
{"id":"310","dataset":"crossner_science","split":"test","instance":{"id":"310","prompt_labels":"Red(O) Mars(O) won(O) the(O) BSFA(B-award) Award(I-award) in(O) 1992(O) and(O) Nebula(B-award) Award(I-award) for(I-award) Best(I-award) Novel(I-award) in(O) 1993(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, event, theory, person, location, chemical compound, chemical element, country, enzyme, scientist, discipline, award, university, academic journal, organization, protein and O.\nSentence: Red Mars won the BSFA Award in 1992 and Nebula Award for Best Novel in 1993 .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Red","Mars","won","the","BSFA","Award","in","1992","and","Nebula","Award","for","Best","Novel","in","1993","."],"labels":["O","O","O","O","B-award","I-award","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","event","theory","person","location","chemical_compound","chemical_element","country","enzyme","scientist","discipline","award","university","academic_journal","organization","protein"]}
{"id":"312","dataset":"crossner_science","split":"test","instance":{"id":"312","prompt_labels":"Aryl(O) hydrocarbon(O) receptor(O) repressor(O) and(O) F2RL3(O) are(O) similarly(O) hypomethylated(O) in(O) smokers(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, enzyme, award, chemical compound, academic journal, astronomical object, scientist, country, location, protein, chemical element, person, theory, event, university, discipline and O.\nSentence: Aryl hydrocarbon receptor repressor and F2RL3 are similarly hypomethylated in smokers .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Aryl","hydrocarbon","receptor","repressor","and","F2RL3","are","similarly","hypomethylated","in","smokers","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["organization","enzyme","award","chemical_compound","academic_journal","astronomical_object","scientist","country","location","protein","chemical_element","person","theory","event","university","discipline"]}
{"id":"315","dataset":"crossner_science","split":"test","instance":{"id":"315","prompt_labels":"Several(O) TLR(B-protein) immunodeficiencies(O) have(O) been(O) described(O) in(O) which(O) cellular(O) proteins(O) that(O) should(O) transmit(O) the(O) message(O) from(O) the(O) Toll-like(B-protein) receptor(I-protein) to(O) the(O) nucleus(O) are(O) abnormal(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, astronomical object, discipline, academic journal, country, university, organization, enzyme, chemical compound, chemical element, protein, person, award, scientist, theory, location and O.\nSentence: Several TLR immunodeficiencies have been described in which cellular proteins that should transmit the message from the Toll-like receptor to the nucleus are abnormal .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Several","TLR","immunodeficiencies","have","been","described","in","which","cellular","proteins","that","should","transmit","the","message","from","the","Toll-like","receptor","to","the","nucleus","are","abnormal","."],"labels":["O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","I-protein","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["event","astronomical_object","discipline","academic_journal","country","university","organization","enzyme","chemical_compound","chemical_element","protein","person","award","scientist","theory","location"]}
{"id":"318","dataset":"crossner_science","split":"test","instance":{"id":"318","prompt_labels":"The(O) term(O) can(O) also(O) be(O) used(O) to(O) describe(O) the(O) motion(O) of(O) a(O) satellite(O) across(O) its(O) parent(O) planet(O) ,(O) for(O) instance(O) one(O) of(O) the(O) Galilean(O) satellites(O) ((O) Io(B-astronomical object) ,(O) Europa(B-astronomical object) ,(O) Ganymede(B-astronomical object) ,(O) Callisto(B-astronomical object) )(O) across(O) Jupiter(B-astronomical object) ,(O) as(O) seen(O) from(O) Earth(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: event, location, astronomical object, enzyme, protein, organization, university, person, chemical element, discipline, chemical compound, academic journal, country, theory, scientist, award and O.\nSentence: The term can also be used to describe the motion of a satellite across its parent planet , for instance one of the Galilean satellites ( Io , Europa , Ganymede , Callisto ) across Jupiter , as seen from Earth .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","term","can","also","be","used","to","describe","the","motion","of","a","satellite","across","its","parent","planet",",","for","instance","one","of","the","Galilean","satellites","(","Io",",","Europa",",","Ganymede",",","Callisto",")","across","Jupiter",",","as","seen","from","Earth","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","B-astronomical object","O","O","O","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["event","location","astronomical_object","enzyme","protein","organization","university","person","chemical_element","discipline","chemical_compound","academic_journal","country","theory","scientist","award"]}
{"id":"328","dataset":"crossner_science","split":"test","instance":{"id":"328","prompt_labels":"The(O) traditional(O) emblem(O) for(O) the(O) House(B-organization) of(I-organization) Lancaster(I-organization) is(O) a(O) red(O) rose(O) ,(O) the(O) Red(O) Rose(O) of(O) Lancaster(O) ,(O) similar(O) to(O) that(O) of(O) the(O) House(O) of(O) York(O) ,(O) which(O) is(O) a(O) white(O) rose(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, discipline, university, scientist, organization, chemical element, location, academic journal, chemical compound, award, protein, person, theory, event, country, astronomical object and O.\nSentence: The traditional emblem for the House of Lancaster is a red rose , the Red Rose of Lancaster , similar to that of the House of York , which is a white rose .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","traditional","emblem","for","the","House","of","Lancaster","is","a","red","rose",",","the","Red","Rose","of","Lancaster",",","similar","to","that","of","the","House","of","York",",","which","is","a","white","rose","."],"labels":["O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","discipline","university","scientist","organization","chemical_element","location","academic_journal","chemical_compound","award","protein","person","theory","event","country","astronomical_object"]}
{"id":"329","dataset":"crossner_science","split":"test","instance":{"id":"329","prompt_labels":";(O) the(O) most(O) exclusive(O) sports(O) clubs(O) :(O) CCI(B-organization) ,(O) The(B-organization) Willingdon(I-organization) Sports(I-organization) Club(I-organization) as(O) well(O) as(O) Bombay(B-organization) Gymkhana(I-organization) and(O) the(O) most(O) expensive(O) hospitals(O) :(O) Breach(B-location) Candy(I-location) Hospital(I-location) ,(O) Bombay(B-location) Hospital(I-location) ,(O) Jaslok(B-location) Hospital(I-location) and(O) Hurkisondas(B-organization) Hospital(I-organization) ;(O) in(O) the(O) nation(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, location, astronomical object, academic journal, chemical compound, scientist, country, person, organization, protein, chemical element, enzyme, university, event, theory, discipline and O.\nSentence: ; the most exclusive sports clubs : CCI , The Willingdon Sports Club as well as Bombay Gymkhana and the most expensive hospitals : Breach Candy Hospital , Bombay Hospital , Jaslok Hospital and Hurkisondas Hospital ; in the nation .","prediction_output":null,"prediction_outputs":null,"group":null,"words":[";","the","most","exclusive","sports","clubs",":","CCI",",","The","Willingdon","Sports","Club","as","well","as","Bombay","Gymkhana","and","the","most","expensive","hospitals",":","Breach","Candy","Hospital",",","Bombay","Hospital",",","Jaslok","Hospital","and","Hurkisondas","Hospital",";","in","the","nation","."],"labels":["O","O","O","O","O","O","O","B-organization","O","B-organization","I-organization","I-organization","I-organization","O","O","O","B-organization","I-organization","O","O","O","O","O","O","B-location","I-location","I-location","O","B-location","I-location","O","B-location","I-location","O","B-organization","I-organization","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["award","location","astronomical_object","academic_journal","chemical_compound","scientist","country","person","organization","protein","chemical_element","enzyme","university","event","theory","discipline"]}
{"id":"339","dataset":"crossner_science","split":"test","instance":{"id":"339","prompt_labels":"The(O) scientists(O) used(O) tripronuclear(O) ((O) 3PN(O) )(O) zygotes(O) ,(O) zygotes(O) fertilized(O) by(O) two(O) sperm(O) and(O) therefore(O) non-viable(O) ,(O) to(O) investigate(O) CRISPR(O) -mediated(O) gene(O) editing(O) in(O) human(O) cells(O) ,(O) something(O) that(O) had(O) never(O) been(O) attempted(O) before(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, chemical compound, scientist, theory, organization, award, location, university, discipline, astronomical object, event, protein, chemical element, country, person, academic journal and O.\nSentence: The scientists used tripronuclear ( 3PN ) zygotes , zygotes fertilized by two sperm and therefore non-viable , to investigate CRISPR -mediated gene editing in human cells , something that had never been attempted before .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","scientists","used","tripronuclear","(","3PN",")","zygotes",",","zygotes","fertilized","by","two","sperm","and","therefore","non-viable",",","to","investigate","CRISPR","-mediated","gene","editing","in","human","cells",",","something","that","had","never","been","attempted","before","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","chemical_compound","scientist","theory","organization","award","location","university","discipline","astronomical_object","event","protein","chemical_element","country","person","academic_journal"]}
{"id":"347","dataset":"crossner_science","split":"test","instance":{"id":"347","prompt_labels":"The(O) chalcophile(O) elements(O) include(O) :(O) Silver(B-chemical element) ,(O) Arsenic(B-chemical element) ,(O) Bi(B-chemical element) ,(O) Cadmium(B-chemical element) ,(O) Cu(B-chemical element) ,(O) Gallium(B-chemical element) ,(O) Ge(B-chemical element) ,(O) Hg(B-chemical element) ,(O) Indium(B-chemical element) ,(O) Pb(B-chemical element) ,(O) Sulfur(B-chemical element) ,(O) Sb(B-chemical element) ,(O) Selenium(B-chemical element) ,(O) Sn(B-chemical element) ,(O) Tellurium(B-chemical element) ,(O) Thallium(B-chemical element) and(O) Zn(B-chemical element) .(O) Allaby(B-person) ,(I-person) M.(I-person) ((O) 2013(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, organization, protein, theory, discipline, person, university, location, academic journal, chemical element, event, scientist, country, award, enzyme, astronomical object and O.\nSentence: The chalcophile elements include : Silver , Arsenic , Bi , Cadmium , Cu , Gallium , Ge , Hg , Indium , Pb , Sulfur , Sb , Selenium , Sn , Tellurium , Thallium and Zn . Allaby , M. ( 2013 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","chalcophile","elements","include",":","Silver",",","Arsenic",",","Bi",",","Cadmium",",","Cu",",","Gallium",",","Ge",",","Hg",",","Indium",",","Pb",",","Sulfur",",","Sb",",","Selenium",",","Sn",",","Tellurium",",","Thallium","and","Zn",".","Allaby",",","M.","(","2013",")","."],"labels":["O","O","O","O","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-chemical element","O","B-person","I-person","I-person","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","organization","protein","theory","discipline","person","university","location","academic_journal","chemical_element","event","scientist","country","award","enzyme","astronomical_object"]}
{"id":"350","dataset":"crossner_science","split":"test","instance":{"id":"350","prompt_labels":"Chinese(B-academic journal) Physics(I-academic journal) Letters(I-academic journal) is(O) a(O) part(O) of(O) a(O) small(O) group(O) of(O) four(O) journals(O) from(O) the(O) Chinese(B-organization) Physical(I-organization) Society(I-organization) ,(O) the(O) other(O) three(O) are(O) :(O) Communications(B-academic journal) in(I-academic journal) Theoretical(I-academic journal) Physics(I-academic journal) ((O) in(O) English(O) ,(O) subtitled(O) Chinese(B-academic journal) Physics(I-academic journal) A(I-academic journal) )(O) ,(O) Chinese(B-academic journal) Physics(I-academic journal) B(I-academic journal) ((O) in(O) English(O) )(O) ,(O) and(O) Chinese(B-academic journal) Physics(I-academic journal) C(I-academic journal) ((O) in(O) English(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, award, protein, scientist, discipline, person, theory, organization, enzyme, astronomical object, country, location, chemical compound, event, chemical element, university and O.\nSentence: Chinese Physics Letters is a part of a small group of four journals from the Chinese Physical Society , the other three are : Communications in Theoretical Physics ( in English , subtitled Chinese Physics A ) , Chinese Physics B ( in English ) , and Chinese Physics C ( in English ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Chinese","Physics","Letters","is","a","part","of","a","small","group","of","four","journals","from","the","Chinese","Physical","Society",",","the","other","three","are",":","Communications","in","Theoretical","Physics","(","in","English",",","subtitled","Chinese","Physics","A",")",",","Chinese","Physics","B","(","in","English",")",",","and","Chinese","Physics","C","(","in","English",")","."],"labels":["B-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","O","B-academic journal","I-academic journal","I-academic journal","O","O","O","O","O","O","B-academic journal","I-academic journal","I-academic journal","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","award","protein","scientist","discipline","person","theory","organization","enzyme","astronomical_object","country","location","chemical_compound","event","chemical_element","university"]}
{"id":"353","dataset":"crossner_science","split":"test","instance":{"id":"353","prompt_labels":"Confirmed(O) transiting(O) hot(O) Jupiters(B-astronomical object) that(O) have(O) orbital(O) periods(O) of(O) less(O) than(O) one(O) day(O) include(O) WASP-18b(B-astronomical object) ,(O) WASP-19b(B-astronomical object) ,(O) WASP-43b(B-astronomical object) ,(O) and(O) WASP-103b(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, event, theory, location, discipline, award, person, university, academic journal, enzyme, chemical compound, protein, chemical element, country, organization, astronomical object and O.\nSentence: Confirmed transiting hot Jupiters that have orbital periods of less than one day include WASP-18b , WASP-19b , WASP-43b , and WASP-103b .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Confirmed","transiting","hot","Jupiters","that","have","orbital","periods","of","less","than","one","day","include","WASP-18b",",","WASP-19b",",","WASP-43b",",","and","WASP-103b","."],"labels":["O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","B-astronomical object","O","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["scientist","event","theory","location","discipline","award","person","university","academic_journal","enzyme","chemical_compound","protein","chemical_element","country","organization","astronomical_object"]}
{"id":"356","dataset":"crossner_science","split":"test","instance":{"id":"356","prompt_labels":"The(O) gamma(O) pseudogene(O) contains(O) an(O) inverted(O) long(O) interspersed(O) nuclear(O) element(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, academic journal, discipline, chemical compound, enzyme, university, organization, theory, protein, scientist, location, event, person, astronomical object, country, award and O.\nSentence: The gamma pseudogene contains an inverted long interspersed nuclear element .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","gamma","pseudogene","contains","an","inverted","long","interspersed","nuclear","element","."],"labels":["O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","academic_journal","discipline","chemical_compound","enzyme","university","organization","theory","protein","scientist","location","event","person","astronomical_object","country","award"]}
{"id":"362","dataset":"crossner_science","split":"test","instance":{"id":"362","prompt_labels":"Under(O) typical(O) dark(O) sky(O) conditions(O) Uranus(B-astronomical object) ((O) magnitude(O) +(O) 5.8(O) )(O) can(O) be(O) seen(O) as(O) well(O) with(O) averted(O) vision(O) ,(O) as(O) can(O) the(O) asteroid(O) 4(B-astronomical object) Vesta(I-astronomical object) at(O) its(O) brighter(O) oppositions(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, enzyme, country, scientist, protein, astronomical object, location, theory, organization, chemical element, person, event, chemical compound, university, discipline, award and O.\nSentence: Under typical dark sky conditions Uranus ( magnitude + 5.8 ) can be seen as well with averted vision , as can the asteroid 4 Vesta at its brighter oppositions .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Under","typical","dark","sky","conditions","Uranus","(","magnitude","+","5.8",")","can","be","seen","as","well","with","averted","vision",",","as","can","the","asteroid","4","Vesta","at","its","brighter","oppositions","."],"labels":["O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","enzyme","country","scientist","protein","astronomical_object","location","theory","organization","chemical_element","person","event","chemical_compound","university","discipline","award"]}
{"id":"366","dataset":"crossner_science","split":"test","instance":{"id":"366","prompt_labels":"It(O) works(O) as(O) a(O) transmitter(O) from(O) and(O) to(O) the(O) tight(O) junction(O) ,(O) because(O) of(O) its(O) association(O) with(O) signaling(O) molecules(O) ((O) Phosphoinositide(B-enzyme) 3-kinase(I-enzyme) ,(O) Protein(B-enzyme) kinase(I-enzyme) C(I-enzyme) ,(O) YES(B-enzyme) ,(O) Protein(B-enzyme) phosphatase(I-enzyme) 2(I-enzyme) ,(I-enzyme) 1(I-enzyme) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, location, protein, theory, chemical compound, discipline, person, event, academic journal, chemical element, organization, award, enzyme, university, country, astronomical object and O.\nSentence: It works as a transmitter from and to the tight junction , because of its association with signaling molecules ( Phosphoinositide 3-kinase , Protein kinase C , YES , Protein phosphatase 2 , 1 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["It","works","as","a","transmitter","from","and","to","the","tight","junction",",","because","of","its","association","with","signaling","molecules","(","Phosphoinositide","3-kinase",",","Protein","kinase","C",",","YES",",","Protein","phosphatase","2",",","1",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-enzyme","I-enzyme","O","B-enzyme","I-enzyme","I-enzyme","O","B-enzyme","O","B-enzyme","I-enzyme","I-enzyme","I-enzyme","I-enzyme","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","location","protein","theory","chemical_compound","discipline","person","event","academic_journal","chemical_element","organization","award","enzyme","university","country","astronomical_object"]}
{"id":"367","dataset":"crossner_science","split":"test","instance":{"id":"367","prompt_labels":"Other(O) enzymes(O) such(O) as(O) ((O) Transglutaminase(B-enzyme) )(O) control(O) chromatin(O) remodeling(O) through(O) proteins(O) such(O) as(O) sirtuin1(B-protein) ((O) SIRT1(B-protein) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, organization, location, discipline, award, university, theory, person, chemical element, event, country, protein, astronomical object, chemical compound, academic journal, enzyme and O.\nSentence: Other enzymes such as ( Transglutaminase ) control chromatin remodeling through proteins such as sirtuin1 ( SIRT1 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Other","enzymes","such","as","(","Transglutaminase",")","control","chromatin","remodeling","through","proteins","such","as","sirtuin1","(","SIRT1",")","."],"labels":["O","O","O","O","O","B-enzyme","O","O","O","O","O","O","O","O","B-protein","O","B-protein","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","organization","location","discipline","award","university","theory","person","chemical_element","event","country","protein","astronomical_object","chemical_compound","academic_journal","enzyme"]}
{"id":"369","dataset":"crossner_science","split":"test","instance":{"id":"369","prompt_labels":"Juno(B-astronomical object) can(O) reach(O) +(O) 7.5(O) at(O) a(O) favourable(O) opposition(O) ,(O) which(O) is(O) brighter(O) than(O) Neptune(B-astronomical object) or(O) Titan(B-astronomical object) ,(O) and(O) is(O) the(O) reason(O) for(O) it(O) being(O) discovered(O) before(O) the(O) larger(O) asteroids(O) 10(B-astronomical object) Hygiea(I-astronomical object) ,(O) 52(B-astronomical object) Europa(I-astronomical object) ,(O) 511(B-astronomical object) Davida(I-astronomical object) ,(O) and(O) 704(B-astronomical object) Interamnia(I-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, chemical element, country, discipline, scientist, university, chemical compound, theory, person, astronomical object, enzyme, location, protein, event, academic journal and O.\nSentence: Juno can reach + 7.5 at a favourable opposition , which is brighter than Neptune or Titan , and is the reason for it being discovered before the larger asteroids 10 Hygiea , 52 Europa , 511 Davida , and 704 Interamnia .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Juno","can","reach","+","7.5","at","a","favourable","opposition",",","which","is","brighter","than","Neptune","or","Titan",",","and","is","the","reason","for","it","being","discovered","before","the","larger","asteroids","10","Hygiea",",","52","Europa",",","511","Davida",",","and","704","Interamnia","."],"labels":["B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","O","B-astronomical object","I-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["organization","award","chemical_element","country","discipline","scientist","university","chemical_compound","theory","person","astronomical_object","enzyme","location","protein","event","academic_journal"]}
{"id":"370","dataset":"crossner_science","split":"test","instance":{"id":"370","prompt_labels":"She(O) also(O) played(O) at(O) 1986(O) ,(O) 1989(O) ,(O) 1993(O) ,(O) 1995(O) AFC(B-event) Championship(I-event) ,(O) 1990(O) ,(O) Football(O) at(O) the(O) 1994(O) Asian(B-event) Games(I-event) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, chemical compound, country, scientist, organization, award, chemical element, theory, protein, academic journal, event, discipline, astronomical object, person, enzyme, location and O.\nSentence: She also played at 1986 , 1989 , 1993 , 1995 AFC Championship , 1990 , Football at the 1994 Asian Games .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["She","also","played","at","1986",",","1989",",","1993",",","1995","AFC","Championship",",","1990",",","Football","at","the","1994","Asian","Games","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-event","I-event","O","O","O","O","O","O","O","B-event","I-event","O"],"target_index":null,"target_label":null},"label_list":["university","chemical_compound","country","scientist","organization","award","chemical_element","theory","protein","academic_journal","event","discipline","astronomical_object","person","enzyme","location"]}
{"id":"374","dataset":"crossner_science","split":"test","instance":{"id":"374","prompt_labels":"Assisted(O) by(O) activists(O) from(O) Southern(B-organization) Christian(I-organization) Leadership(I-organization) Conference(I-organization) ,(O) Congress(B-organization) of(I-organization) Racial(I-organization) Equality(I-organization) ,(O) SNCC(B-organization) ,(O) and(O) the(O) NAACP(B-organization) ,(O) African(O) Americans(O) and(O) supporters(O) took(O) a(O) stand(O) to(O) fight(O) segregation(O) through(O) nonviolence(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, country, astronomical object, discipline, person, award, organization, scientist, location, protein, academic journal, theory, university, event, chemical compound, chemical element and O.\nSentence: Assisted by activists from Southern Christian Leadership Conference , Congress of Racial Equality , SNCC , and the NAACP , African Americans and supporters took a stand to fight segregation through nonviolence .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Assisted","by","activists","from","Southern","Christian","Leadership","Conference",",","Congress","of","Racial","Equality",",","SNCC",",","and","the","NAACP",",","African","Americans","and","supporters","took","a","stand","to","fight","segregation","through","nonviolence","."],"labels":["O","O","O","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","O","O","O","B-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","country","astronomical_object","discipline","person","award","organization","scientist","location","protein","academic_journal","theory","university","event","chemical_compound","chemical_element"]}
{"id":"375","dataset":"crossner_science","split":"test","instance":{"id":"375","prompt_labels":"DSBs(O) can(O) be(O) artificially(O) induced(O) using(O) genome(O) editing(O) technologies(O) such(O) as(O) CRISPR(O) or(O) TALEN(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, chemical element, university, chemical compound, theory, organization, location, person, academic journal, country, discipline, astronomical object, enzyme, award, event, protein and O.\nSentence: DSBs can be artificially induced using genome editing technologies such as CRISPR or TALEN .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["DSBs","can","be","artificially","induced","using","genome","editing","technologies","such","as","CRISPR","or","TALEN","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","chemical_element","university","chemical_compound","theory","organization","location","person","academic_journal","country","discipline","astronomical_object","enzyme","award","event","protein"]}
{"id":"380","dataset":"crossner_science","split":"test","instance":{"id":"380","prompt_labels":"There(O) are(O) two(O) voltage-gated(O) calcium(O) channels(O) within(O) cardiac(O) muscle(O) :(O) L-type(O) calcium(O) channel(O) s(O) ((O) '(O) L(O) '(O) for(O) Long-lasting(O) )(O) and(O) T-type(O) calcium(O) channel(O) s(O) ((O) '(O) T(O) '(O) for(O) Transient(O) ,(O) i.e.(O) short(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, academic journal, theory, event, chemical compound, person, enzyme, organization, university, country, astronomical object, chemical element, location, discipline, protein, award and O.\nSentence: There are two voltage-gated calcium channels within cardiac muscle : L-type calcium channel s ( ' L ' for Long-lasting ) and T-type calcium channel s ( ' T ' for Transient , i.e. short ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["There","are","two","voltage-gated","calcium","channels","within","cardiac","muscle",":","L-type","calcium","channel","s","(","'","L","'","for","Long-lasting",")","and","T-type","calcium","channel","s","(","'","T","'","for","Transient",",","i.e.","short",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","academic_journal","theory","event","chemical_compound","person","enzyme","organization","university","country","astronomical_object","chemical_element","location","discipline","protein","award"]}
{"id":"382","dataset":"crossner_science","split":"test","instance":{"id":"382","prompt_labels":"In(O) chemistry(B-discipline) ,(O) Erwin(B-scientist) Schrödinger(I-scientist) ,(O) Linus(B-scientist) Pauling(I-scientist) ,(O) Robert(B-scientist) S.(I-scientist) Mulliken(I-scientist) and(O) others(O) noted(O) that(O) the(O) consequence(O) of(O) Heisenberg(B-theory) 's(I-theory) relation(I-theory) was(O) that(O) the(O) electron(O) ,(O) as(O) a(O) wave(O) packet(O) ,(O) could(O) not(O) be(O) considered(O) to(O) have(O) an(O) exact(O) location(O) in(O) its(O) orbital(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, protein, award, theory, organization, event, country, academic journal, location, enzyme, astronomical object, discipline, scientist, chemical element, person, university and O.\nSentence: In chemistry , Erwin Schrödinger , Linus Pauling , Robert S. Mulliken and others noted that the consequence of Heisenberg 's relation was that the electron , as a wave packet , could not be considered to have an exact location in its orbital .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","chemistry",",","Erwin","Schrödinger",",","Linus","Pauling",",","Robert","S.","Mulliken","and","others","noted","that","the","consequence","of","Heisenberg","'s","relation","was","that","the","electron",",","as","a","wave","packet",",","could","not","be","considered","to","have","an","exact","location","in","its","orbital","."],"labels":["O","B-discipline","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","I-scientist","O","O","O","O","O","O","O","B-theory","I-theory","I-theory","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","protein","award","theory","organization","event","country","academic_journal","location","enzyme","astronomical_object","discipline","scientist","chemical_element","person","university"]}
{"id":"396","dataset":"crossner_science","split":"test","instance":{"id":"396","prompt_labels":"Mikhail(B-scientist) Lomonosov(I-scientist) ((O) 1711-1765(O) )(O) had(O) previously(O) expressed(O) similar(O) ideas(O) in(O) 1748(O) and(O) proved(O) them(O) in(O) experiments(O) ;(O) others(O) whose(O) ideas(O) pre-date(O) the(O) work(O) of(O) Lavoisier(B-scientist) include(O) Jean(B-scientist) Rey(I-scientist) ((O) 1583-1645(O) )(O) ,(O) Joseph(B-scientist) Black(I-scientist) ((O) 1728-1799(O) )(O) ,(O) and(O) Henry(B-scientist) Cavendish(I-scientist) ((O) 1731-1810(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: university, academic journal, enzyme, scientist, protein, country, award, chemical compound, event, location, chemical element, person, organization, theory, discipline, astronomical object and O.\nSentence: Mikhail Lomonosov ( 1711-1765 ) had previously expressed similar ideas in 1748 and proved them in experiments ; others whose ideas pre-date the work of Lavoisier include Jean Rey ( 1583-1645 ) , Joseph Black ( 1728-1799 ) , and Henry Cavendish ( 1731-1810 ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Mikhail","Lomonosov","(","1711-1765",")","had","previously","expressed","similar","ideas","in","1748","and","proved","them","in","experiments",";","others","whose","ideas","pre-date","the","work","of","Lavoisier","include","Jean","Rey","(","1583-1645",")",",","Joseph","Black","(","1728-1799",")",",","and","Henry","Cavendish","(","1731-1810",")","."],"labels":["B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","O","B-scientist","I-scientist","O","O","O","O","B-scientist","I-scientist","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["university","academic_journal","enzyme","scientist","protein","country","award","chemical_compound","event","location","chemical_element","person","organization","theory","discipline","astronomical_object"]}
{"id":"401","dataset":"crossner_science","split":"test","instance":{"id":"401","prompt_labels":"The(O) transform(O) provides(O) an(O) alternative(O) approach(O) to(O) analytic(O) wave(O) front(O) set(O) s(O) of(O) distributions(O) ,(O) developed(O) independently(O) by(O) the(O) Japanese(O) mathematicians(O) Mikio(B-scientist) Sato(I-scientist) ,(O) Masaki(B-scientist) Kashiwara(I-scientist) and(O) Takahiro(B-scientist) Kawai(I-scientist) in(O) their(O) approach(O) to(O) microlocal(O) analysis(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, person, chemical element, scientist, academic journal, country, location, theory, award, chemical compound, protein, organization, enzyme, university, astronomical object, event and O.\nSentence: The transform provides an alternative approach to analytic wave front set s of distributions , developed independently by the Japanese mathematicians Mikio Sato , Masaki Kashiwara and Takahiro Kawai in their approach to microlocal analysis .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","transform","provides","an","alternative","approach","to","analytic","wave","front","set","s","of","distributions",",","developed","independently","by","the","Japanese","mathematicians","Mikio","Sato",",","Masaki","Kashiwara","and","Takahiro","Kawai","in","their","approach","to","microlocal","analysis","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","person","chemical_element","scientist","academic_journal","country","location","theory","award","chemical_compound","protein","organization","enzyme","university","astronomical_object","event"]}
{"id":"405","dataset":"crossner_science","split":"test","instance":{"id":"405","prompt_labels":"The(O) hydroxyl(B-chemical compound) radical(I-chemical compound) can(O) damage(O) virtually(O) all(O) types(O) of(O) macromolecules(O) :(O) carbohydrates(B-chemical compound) ,(O) nucleic(B-chemical compound) acids(I-chemical compound) ((O) mutation(O) s(O) )(O) ,(O) lipids(B-chemical compound) ((O) lipid(O) peroxidation(O) )(O) ,(O) and(O) amino(B-chemical compound) acids(I-chemical compound) ((O) e.g.(O) conversion(O) of(O) Phenylalanine(B-chemical compound) to(O) m(B-chemical compound) -(I-chemical compound) Tyrosine(I-chemical compound) and(O) o(B-chemical compound) -(I-chemical compound) Tyrosine(I-chemical compound) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, person, academic journal, chemical element, chemical compound, theory, enzyme, country, event, award, protein, organization, university, location, discipline, scientist and O.\nSentence: The hydroxyl radical can damage virtually all types of macromolecules : carbohydrates , nucleic acids ( mutation s ) , lipids ( lipid peroxidation ) , and amino acids ( e.g. conversion of Phenylalanine to m - Tyrosine and o - Tyrosine ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","hydroxyl","radical","can","damage","virtually","all","types","of","macromolecules",":","carbohydrates",",","nucleic","acids","(","mutation","s",")",",","lipids","(","lipid","peroxidation",")",",","and","amino","acids","(","e.g.","conversion","of","Phenylalanine","to","m","-","Tyrosine","and","o","-","Tyrosine",")","."],"labels":["O","B-chemical compound","I-chemical compound","O","O","O","O","O","O","O","O","B-chemical compound","O","B-chemical compound","I-chemical compound","O","O","O","O","O","B-chemical compound","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","O","O","O","B-chemical compound","O","B-chemical compound","I-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","I-chemical compound","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","person","academic_journal","chemical_element","chemical_compound","theory","enzyme","country","event","award","protein","organization","university","location","discipline","scientist"]}
{"id":"408","dataset":"crossner_science","split":"test","instance":{"id":"408","prompt_labels":"The(O) conclusion(O) of(O) the(O) study(O) would(O) suggest(O) that(O) p53(B-protein) inhibition(O) might(O) increase(O) efficiency(O) of(O) human(O) germline(O) editing(O) and(O) that(O) p53(B-protein) function(O) would(O) need(O) to(O) be(O) watched(O) when(O) developing(O) CRISPR(O) based(O) therapy(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, enzyme, discipline, astronomical object, chemical element, event, university, organization, protein, country, award, academic journal, person, scientist, chemical compound, theory and O.\nSentence: The conclusion of the study would suggest that p53 inhibition might increase efficiency of human germline editing and that p53 function would need to be watched when developing CRISPR based therapy .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","conclusion","of","the","study","would","suggest","that","p53","inhibition","might","increase","efficiency","of","human","germline","editing","and","that","p53","function","would","need","to","be","watched","when","developing","CRISPR","based","therapy","."],"labels":["O","O","O","O","O","O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","enzyme","discipline","astronomical_object","chemical_element","event","university","organization","protein","country","award","academic_journal","person","scientist","chemical_compound","theory"]}
{"id":"412","dataset":"crossner_science","split":"test","instance":{"id":"412","prompt_labels":"Supercritical(O) fluids(O) occur(O) in(O) the(O) atmospheres(O) of(O) the(O) gas(O) giant(O) s(O) Jupiter(B-astronomical object) and(O) Saturn(B-astronomical object) ,(O) and(O) probably(O) in(O) those(O) of(O) the(O) ice(O) giant(O) s(O) Uranus(B-astronomical object) and(O) Neptune(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, scientist, theory, organization, protein, astronomical object, person, award, academic journal, university, chemical element, enzyme, discipline, event, location, country and O.\nSentence: Supercritical fluids occur in the atmospheres of the gas giant s Jupiter and Saturn , and probably in those of the ice giant s Uranus and Neptune .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Supercritical","fluids","occur","in","the","atmospheres","of","the","gas","giant","s","Jupiter","and","Saturn",",","and","probably","in","those","of","the","ice","giant","s","Uranus","and","Neptune","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","scientist","theory","organization","protein","astronomical_object","person","award","academic_journal","university","chemical_element","enzyme","discipline","event","location","country"]}
{"id":"419","dataset":"crossner_science","split":"test","instance":{"id":"419","prompt_labels":"East(B-location) Malaysia(I-location) currently(O) has(O) two(O) public(O) universities(O) ,(O) namely(O) Universiti(B-university) Malaysia(I-university) Sarawak(I-university) ((O) UNIMAS(B-university) )(O) and(O) Universiti(B-university) Malaysia(I-university) Sabah(I-university) ((O) UMS(B-university) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, person, discipline, academic journal, scientist, event, enzyme, astronomical object, university, chemical element, country, chemical compound, theory, protein, location, organization and O.\nSentence: East Malaysia currently has two public universities , namely Universiti Malaysia Sarawak ( UNIMAS ) and Universiti Malaysia Sabah ( UMS ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["East","Malaysia","currently","has","two","public","universities",",","namely","Universiti","Malaysia","Sarawak","(","UNIMAS",")","and","Universiti","Malaysia","Sabah","(","UMS",")","."],"labels":["B-location","I-location","O","O","O","O","O","O","O","B-university","I-university","I-university","O","B-university","O","O","B-university","I-university","I-university","O","B-university","O","O"],"target_index":null,"target_label":null},"label_list":["award","person","discipline","academic_journal","scientist","event","enzyme","astronomical_object","university","chemical_element","country","chemical_compound","theory","protein","location","organization"]}
{"id":"425","dataset":"crossner_science","split":"test","instance":{"id":"425","prompt_labels":"On(O) 15(O) June(O) 1826(O) he(O) was(O) elected(O) Fellow(B-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) ,(O) and(O) in(O) 1830(O) was(O) one(O) of(O) the(O) founders(O) of(O) the(O) Royal(B-organization) Geographical(I-organization) Society(I-organization) ((O) RGS(B-organization) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: award, chemical compound, event, organization, astronomical object, location, country, enzyme, university, discipline, chemical element, protein, theory, academic journal, person, scientist and O.\nSentence: On 15 June 1826 he was elected Fellow of the Royal Society , and in 1830 was one of the founders of the Royal Geographical Society ( RGS ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","15","June","1826","he","was","elected","Fellow","of","the","Royal","Society",",","and","in","1830","was","one","of","the","founders","of","the","Royal","Geographical","Society","(","RGS",")","."],"labels":["O","O","O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","B-organization","I-organization","I-organization","O","B-organization","O","O"],"target_index":null,"target_label":null},"label_list":["award","chemical_compound","event","organization","astronomical_object","location","country","enzyme","university","discipline","chemical_element","protein","theory","academic_journal","person","scientist"]}
{"id":"427","dataset":"crossner_science","split":"test","instance":{"id":"427","prompt_labels":"This(O) includes(O) Pluto(B-astronomical object) ,(O) which(O) is(O) constrained(O) in(O) its(O) orbit(O) by(O) the(O) gravity(O) of(O) Neptune(B-astronomical object) and(O) shares(O) its(O) orbital(O) neighbourhood(O) with(O) many(O) Kuiper(O) belt(O) objects(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, event, award, chemical compound, university, theory, enzyme, person, organization, protein, chemical element, country, discipline, scientist, location, academic journal and O.\nSentence: This includes Pluto , which is constrained in its orbit by the gravity of Neptune and shares its orbital neighbourhood with many Kuiper belt objects .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","includes","Pluto",",","which","is","constrained","in","its","orbit","by","the","gravity","of","Neptune","and","shares","its","orbital","neighbourhood","with","many","Kuiper","belt","objects","."],"labels":["O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","event","award","chemical_compound","university","theory","enzyme","person","organization","protein","chemical_element","country","discipline","scientist","location","academic_journal"]}
{"id":"431","dataset":"crossner_science","split":"test","instance":{"id":"431","prompt_labels":"He(O) was(O) later(O) awarded(O) the(O) Gold(B-award) Medal(I-award) of(I-award) the(I-award) Royal(I-award) Astronomical(I-award) Society(I-award) and(O) the(O) Royal(B-award) Medal(I-award) of(I-award) the(I-award) Royal(I-award) Society(I-award) for(O) this(O) work(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical compound, person, university, country, discipline, theory, academic journal, astronomical object, protein, chemical element, award, organization, event, scientist, location, enzyme and O.\nSentence: He was later awarded the Gold Medal of the Royal Astronomical Society and the Royal Medal of the Royal Society for this work .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","was","later","awarded","the","Gold","Medal","of","the","Royal","Astronomical","Society","and","the","Royal","Medal","of","the","Royal","Society","for","this","work","."],"labels":["O","O","O","O","O","B-award","I-award","I-award","I-award","I-award","I-award","I-award","O","O","B-award","I-award","I-award","I-award","I-award","I-award","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_compound","person","university","country","discipline","theory","academic_journal","astronomical_object","protein","chemical_element","award","organization","event","scientist","location","enzyme"]}
{"id":"436","dataset":"crossner_science","split":"test","instance":{"id":"436","prompt_labels":"Epigenetic(O) modifications(O) to(O) the(O) genome(O) ,(O) including(O) Histone(B-protein) ,(O) DNA(O) methylation(O) ,(O) and(O) the(O) modulation(O) of(O) RNAi(O) ,(O) are(O) major(O) epigenetic(O) events(O) used(O) to(O) modulate(O) gene(O) expression(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: location, astronomical object, event, theory, award, protein, discipline, academic journal, country, enzyme, organization, university, chemical element, scientist, chemical compound, person and O.\nSentence: Epigenetic modifications to the genome , including Histone , DNA methylation , and the modulation of RNAi , are major epigenetic events used to modulate gene expression .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Epigenetic","modifications","to","the","genome",",","including","Histone",",","DNA","methylation",",","and","the","modulation","of","RNAi",",","are","major","epigenetic","events","used","to","modulate","gene","expression","."],"labels":["O","O","O","O","O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["location","astronomical_object","event","theory","award","protein","discipline","academic_journal","country","enzyme","organization","university","chemical_element","scientist","chemical_compound","person"]}
{"id":"441","dataset":"crossner_science","split":"test","instance":{"id":"441","prompt_labels":"In(O) fact(O) ,(O) when(O) math(O) E(O) /(O) math(O) belongs(O) to(O) the(O) spectrum(O) ,(O) the(O) inequality(O) becomes(O) an(O) equality(O) ((O) the(O) Aubry-André(B-theory) formula(I-theory) )(O) ,(O) proved(O) by(O) Jean(B-scientist) Bourgain(I-scientist) and(O) Svetlana(B-scientist) Jitomirskaya(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: astronomical object, enzyme, university, location, award, event, discipline, academic journal, protein, scientist, theory, person, chemical element, country, chemical compound, organization and O.\nSentence: In fact , when math E / math belongs to the spectrum , the inequality becomes an equality ( the Aubry-André formula ) , proved by Jean Bourgain and Svetlana Jitomirskaya .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["In","fact",",","when","math","E","/","math","belongs","to","the","spectrum",",","the","inequality","becomes","an","equality","(","the","Aubry-André","formula",")",",","proved","by","Jean","Bourgain","and","Svetlana","Jitomirskaya","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-theory","I-theory","O","O","O","O","B-scientist","I-scientist","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["astronomical_object","enzyme","university","location","award","event","discipline","academic_journal","protein","scientist","theory","person","chemical_element","country","chemical_compound","organization"]}
{"id":"453","dataset":"crossner_science","split":"test","instance":{"id":"453","prompt_labels":"On(O) 23(O) January(O) 1960(O) ,(O) the(O) Swiss-designed(O) Trieste(B-location) ,(O) originally(O) built(O) in(O) Italy(B-country) and(O) acquired(O) by(O) the(O) U.S.(B-organization) Navy(I-organization) ,(O) ,(O) supported(O) by(O) the(O) USS(O) Wandank(O) ((O) ATF(O) 204(O) )(O) and(O) escorted(O) by(O) the(O) USS(O) Lewis(O) ((O) DE(O) 535(O) )(O) ,(O) descended(O) to(O) the(O) ocean(O) floor(O) in(O) the(O) trench(O) manned(O) by(O) Jacques(B-scientist) Piccard(I-scientist) ((O) who(O) co-designed(O) the(O) submersible(O) along(O) with(O) his(O) father(O) ,(O) Auguste(B-scientist) Piccard(I-scientist) )(O) and(O) USN(O) Lieutenant(O) Don(B-scientist) Walsh(I-scientist) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: country, location, theory, person, academic journal, discipline, astronomical object, organization, enzyme, event, chemical compound, protein, chemical element, university, scientist, award and O.\nSentence: On 23 January 1960 , the Swiss-designed Trieste , originally built in Italy and acquired by the U.S. Navy , , supported by the USS Wandank ( ATF 204 ) and escorted by the USS Lewis ( DE 535 ) , descended to the ocean floor in the trench manned by Jacques Piccard ( who co-designed the submersible along with his father , Auguste Piccard ) and USN Lieutenant Don Walsh .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["On","23","January","1960",",","the","Swiss-designed","Trieste",",","originally","built","in","Italy","and","acquired","by","the","U.S.","Navy",",",",","supported","by","the","USS","Wandank","(","ATF","204",")","and","escorted","by","the","USS","Lewis","(","DE","535",")",",","descended","to","the","ocean","floor","in","the","trench","manned","by","Jacques","Piccard","(","who","co-designed","the","submersible","along","with","his","father",",","Auguste","Piccard",")","and","USN","Lieutenant","Don","Walsh","."],"labels":["O","O","O","O","O","O","O","B-location","O","O","O","O","B-country","O","O","O","O","B-organization","I-organization","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","B-scientist","I-scientist","O"],"target_index":null,"target_label":null},"label_list":["country","location","theory","person","academic_journal","discipline","astronomical_object","organization","enzyme","event","chemical_compound","protein","chemical_element","university","scientist","award"]}
{"id":"460","dataset":"crossner_science","split":"test","instance":{"id":"460","prompt_labels":"San(B-organization) Juan(I-organization) de(I-organization) Dios(I-organization) Hospital(I-organization) ,(O) Philippine(B-organization) Children(I-organization) 's(I-organization) Medical(I-organization) Center(I-organization) ,(O) San(B-organization) Lazaro(I-organization) Hospital(I-organization) ,(O) Ospital(B-organization) ng(I-organization) Muntinlupa(I-organization) ,(O) Lung(B-organization) Center(I-organization) of(I-organization) the(I-organization) Philippines(I-organization) ,(O) the(O) National(B-organization) Kidney(I-organization) and(I-organization) Transplant(I-organization) Institute(I-organization) ,(O) Manila(B-organization) Adventist(I-organization) Medical(I-organization) Center(I-organization) and(O) Las(B-organization) Piñas(I-organization) Hospital(I-organization) also(O) made(O) steps(O) to(O) ban(O) the(O) toxic(O) chemical(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, university, discipline, chemical compound, protein, person, organization, academic journal, chemical element, country, astronomical object, scientist, event, enzyme, award, location and O.\nSentence: San Juan de Dios Hospital , Philippine Children 's Medical Center , San Lazaro Hospital , Ospital ng Muntinlupa , Lung Center of the Philippines , the National Kidney and Transplant Institute , Manila Adventist Medical Center and Las Piñas Hospital also made steps to ban the toxic chemical .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["San","Juan","de","Dios","Hospital",",","Philippine","Children","'s","Medical","Center",",","San","Lazaro","Hospital",",","Ospital","ng","Muntinlupa",",","Lung","Center","of","the","Philippines",",","the","National","Kidney","and","Transplant","Institute",",","Manila","Adventist","Medical","Center","and","Las","Piñas","Hospital","also","made","steps","to","ban","the","toxic","chemical","."],"labels":["B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","I-organization","O","B-organization","I-organization","I-organization","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["theory","university","discipline","chemical_compound","protein","person","organization","academic_journal","chemical_element","country","astronomical_object","scientist","event","enzyme","award","location"]}
{"id":"461","dataset":"crossner_science","split":"test","instance":{"id":"461","prompt_labels":"A(O) similar(O) example(O) is(O) given(O) by(O) the(O) Senegalese(O) sole(O) ((O) Solea(O) senegalensis(O) )(O) ,(O) which(O) ,(O) when(O) acclimated(O) to(O) temperatures(O) of(O) 26(O) °(O) C(O) ,(O) produced(O) a(O) significantly(O) higher(O) amount(O) of(O) taurine(B-chemical compound) ,(O) Glutamic(B-chemical compound) acid(I-chemical compound) ,(O) Gamma-Aminobutyric(B-chemical compound) acid(I-chemical compound) and(O) glycine(B-chemical compound) compared(O) to(O) acclimation(O) to(O) 12(O) °(O) C.(O) This(O) may(O) mean(O) that(O) the(O) aforementioned(O) compounds(O) aid(O) in(O) antioxidant(B-theory) defense(I-theory) ,(O) osmoregulatory(O) processes(O) ,(O) or(O) energetic(O) purposes(O) at(O) these(O) temperatures(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, country, university, organization, award, scientist, astronomical object, discipline, chemical compound, person, chemical element, academic journal, protein, location, event, theory and O.\nSentence: A similar example is given by the Senegalese sole ( Solea senegalensis ) , which , when acclimated to temperatures of 26 ° C , produced a significantly higher amount of taurine , Glutamic acid , Gamma-Aminobutyric acid and glycine compared to acclimation to 12 ° C. This may mean that the aforementioned compounds aid in antioxidant defense , osmoregulatory processes , or energetic purposes at these temperatures .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["A","similar","example","is","given","by","the","Senegalese","sole","(","Solea","senegalensis",")",",","which",",","when","acclimated","to","temperatures","of","26","°","C",",","produced","a","significantly","higher","amount","of","taurine",",","Glutamic","acid",",","Gamma-Aminobutyric","acid","and","glycine","compared","to","acclimation","to","12","°","C.","This","may","mean","that","the","aforementioned","compounds","aid","in","antioxidant","defense",",","osmoregulatory","processes",",","or","energetic","purposes","at","these","temperatures","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-chemical compound","O","B-chemical compound","I-chemical compound","O","B-chemical compound","I-chemical compound","O","B-chemical compound","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-theory","I-theory","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["enzyme","country","university","organization","award","scientist","astronomical_object","discipline","chemical_compound","person","chemical_element","academic_journal","protein","location","event","theory"]}
{"id":"474","dataset":"crossner_science","split":"test","instance":{"id":"474","prompt_labels":"The(O) next(O) year(O) ,(O) in(O) 1995(O) ,(O) a(O) film(O) adaptation(O) of(O) the(O) book(O) ,(O) Apollo(O) 13(O) ,(O) was(O) released(O) ,(O) directed(O) by(O) Ron(B-person) Howard(I-person) and(O) starring(O) Tom(B-person) Hanks(I-person) as(O) Lovell(B-person) ,(O) Bill(B-person) Paxton(I-person) as(O) Haise(B-person) ,(O) Kevin(B-person) Bacon(I-person) as(O) Swigert(B-person) ,(O) Gary(B-person) Sinise(I-person) as(O) Mattingly(B-person) ,(O) Ed(B-person) Harris(I-person) as(O) Kranz(B-person) ,(O) and(O) Kathleen(B-person) Quinlan(I-person) as(O) Marilyn(B-person) Lovell(I-person) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, award, chemical compound, organization, astronomical object, event, discipline, country, university, academic journal, enzyme, location, person, scientist, chemical element, theory and O.\nSentence: The next year , in 1995 , a film adaptation of the book , Apollo 13 , was released , directed by Ron Howard and starring Tom Hanks as Lovell , Bill Paxton as Haise , Kevin Bacon as Swigert , Gary Sinise as Mattingly , Ed Harris as Kranz , and Kathleen Quinlan as Marilyn Lovell .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","next","year",",","in","1995",",","a","film","adaptation","of","the","book",",","Apollo","13",",","was","released",",","directed","by","Ron","Howard","and","starring","Tom","Hanks","as","Lovell",",","Bill","Paxton","as","Haise",",","Kevin","Bacon","as","Swigert",",","Gary","Sinise","as","Mattingly",",","Ed","Harris","as","Kranz",",","and","Kathleen","Quinlan","as","Marilyn","Lovell","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","B-person","I-person","O","B-person","O","B-person","I-person","O","B-person","O","B-person","I-person","O","B-person","O","B-person","I-person","O","B-person","O","B-person","I-person","O","B-person","O","O","B-person","I-person","O","B-person","I-person","O"],"target_index":null,"target_label":null},"label_list":["protein","award","chemical_compound","organization","astronomical_object","event","discipline","country","university","academic_journal","enzyme","location","person","scientist","chemical_element","theory"]}
{"id":"480","dataset":"crossner_science","split":"test","instance":{"id":"480","prompt_labels":"His(O) father(O) encouraged(O) him(O) to(O) apply(O) to(O) United(B-university) States(I-university) Military(I-university) Academy(I-university) ,(O) but(O) he(O) decided(O) to(O) enroll(O) in(O) the(O) United(B-university) States(I-university) Naval(I-university) Academy(I-university) instead(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, discipline, event, location, protein, chemical element, astronomical object, university, academic journal, theory, chemical compound, scientist, enzyme, award, organization, country and O.\nSentence: His father encouraged him to apply to United States Military Academy , but he decided to enroll in the United States Naval Academy instead .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["His","father","encouraged","him","to","apply","to","United","States","Military","Academy",",","but","he","decided","to","enroll","in","the","United","States","Naval","Academy","instead","."],"labels":["O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","O","O","O","O","O","O","O","B-university","I-university","I-university","I-university","O","O"],"target_index":null,"target_label":null},"label_list":["person","discipline","event","location","protein","chemical_element","astronomical_object","university","academic_journal","theory","chemical_compound","scientist","enzyme","award","organization","country"]}
{"id":"482","dataset":"crossner_science","split":"test","instance":{"id":"482","prompt_labels":"Toutatis(B-astronomical object) is(O) also(O) a(O) Mars-crosser(O) asteroid(O) with(O) a(O) chaotic(O) orbit(O) produced(O) by(O) a(O) 3(O) :(O) 1(O) resonance(O) with(O) the(O) planet(O) Jupiter(B-astronomical object) ,(O) a(O) 1(O) :(O) 4(O) resonance(O) with(O) the(O) planet(O) Earth(B-astronomical object) ,(O) and(O) frequent(O) close(O) approaches(O) to(O) the(O) terrestrial(O) planet(O) s(O) ,(O) including(O) Earth(B-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: enzyme, protein, chemical compound, university, scientist, event, organization, location, chemical element, person, astronomical object, country, award, discipline, theory, academic journal and O.\nSentence: Toutatis is also a Mars-crosser asteroid with a chaotic orbit produced by a 3 : 1 resonance with the planet Jupiter , a 1 : 4 resonance with the planet Earth , and frequent close approaches to the terrestrial planet s , including Earth .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Toutatis","is","also","a","Mars-crosser","asteroid","with","a","chaotic","orbit","produced","by","a","3",":","1","resonance","with","the","planet","Jupiter",",","a","1",":","4","resonance","with","the","planet","Earth",",","and","frequent","close","approaches","to","the","terrestrial","planet","s",",","including","Earth","."],"labels":["B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["enzyme","protein","chemical_compound","university","scientist","event","organization","location","chemical_element","person","astronomical_object","country","award","discipline","theory","academic_journal"]}
{"id":"484","dataset":"crossner_science","split":"test","instance":{"id":"484","prompt_labels":"Her(O) research(O) group(O) demonstrated(O) that(O) one(O) such(O) small(O) protein(O) ,(O) AcrZ(B-protein) ,(O) binds(O) to(O) the(O) multidrug(O) efflux(O) pump(O) protein(O) Acriflavine(O) resistance(O) protein(O) family(O) to(O) affect(O) its(O) ability(O) to(O) export(O) certain(O) classes(O) of(O) antibiotics(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, discipline, event, astronomical object, chemical compound, enzyme, protein, organization, scientist, award, academic journal, person, location, theory, country, university and O.\nSentence: Her research group demonstrated that one such small protein , AcrZ , binds to the multidrug efflux pump protein Acriflavine resistance protein family to affect its ability to export certain classes of antibiotics .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Her","research","group","demonstrated","that","one","such","small","protein",",","AcrZ",",","binds","to","the","multidrug","efflux","pump","protein","Acriflavine","resistance","protein","family","to","affect","its","ability","to","export","certain","classes","of","antibiotics","."],"labels":["O","O","O","O","O","O","O","O","O","O","B-protein","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","discipline","event","astronomical_object","chemical_compound","enzyme","protein","organization","scientist","award","academic_journal","person","location","theory","country","university"]}
{"id":"488","dataset":"crossner_science","split":"test","instance":{"id":"488","prompt_labels":"Ras(B-protein) stimulates(O) Mitogen-activated(B-enzyme) protein(I-enzyme) kinase(I-enzyme) to(O) phosphorylate(B-protein) ERK(I-protein) 1(I-protein) /(I-protein) 2(I-protein) which(O) induce(O) outgrowth(O) of(O) neurite(O) s(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: theory, discipline, country, location, astronomical object, enzyme, person, university, chemical compound, chemical element, event, protein, organization, scientist, award, academic journal and O.\nSentence: Ras stimulates Mitogen-activated protein kinase to phosphorylate ERK 1 / 2 which induce outgrowth of neurite s .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Ras","stimulates","Mitogen-activated","protein","kinase","to","phosphorylate","ERK","1","/","2","which","induce","outgrowth","of","neurite","s","."],"labels":["B-protein","O","B-enzyme","I-enzyme","I-enzyme","O","B-protein","I-protein","I-protein","I-protein","I-protein","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["theory","discipline","country","location","astronomical_object","enzyme","person","university","chemical_compound","chemical_element","event","protein","organization","scientist","award","academic_journal"]}
{"id":"490","dataset":"crossner_science","split":"test","instance":{"id":"490","prompt_labels":"Hund(B-scientist) worked(O) at(O) the(O) Universities(O) of(O) University(B-university) of(I-university) Rostock(I-university) ,(O) Leipzig(B-location) ,(O) University(B-university) of(I-university) Jena(I-university) ,(O) Frankfurt(B-location) am(I-location) Main(I-location) ,(O) and(O) University(B-university) of(I-university) Göttingen(I-university) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: organization, award, person, country, enzyme, academic journal, chemical element, protein, discipline, event, theory, university, scientist, astronomical object, location, chemical compound and O.\nSentence: Hund worked at the Universities of University of Rostock , Leipzig , University of Jena , Frankfurt am Main , and University of Göttingen .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Hund","worked","at","the","Universities","of","University","of","Rostock",",","Leipzig",",","University","of","Jena",",","Frankfurt","am","Main",",","and","University","of","Göttingen","."],"labels":["B-scientist","O","O","O","O","O","B-university","I-university","I-university","O","B-location","O","B-university","I-university","I-university","O","B-location","I-location","I-location","O","O","B-university","I-university","I-university","O"],"target_index":null,"target_label":null},"label_list":["organization","award","person","country","enzyme","academic_journal","chemical_element","protein","discipline","event","theory","university","scientist","astronomical_object","location","chemical_compound"]}
{"id":"503","dataset":"crossner_science","split":"test","instance":{"id":"503","prompt_labels":"The(O) 2013(B-award) Nobel(I-award) Peace(I-award) Prize(I-award) was(O) awarded(O) to(O) the(O) organization(O) because(O) it(O) had(O) ,(O) with(O) the(O) Chemical(O) Weapons(O) Convention(O) ,(O) defined(O) the(O) use(O) of(O) chemical(O) weapons(O) as(O) a(O) taboo(O) under(O) international(O) law(O) according(O) to(O) Thorbjørn(B-person) Jagland(I-person) ,(O) Chairman(O) of(O) the(O) Norwegian(B-organization) Nobel(I-organization) Committee(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, protein, enzyme, organization, event, university, chemical compound, award, academic journal, astronomical object, country, scientist, person, location, theory, discipline and O.\nSentence: The 2013 Nobel Peace Prize was awarded to the organization because it had , with the Chemical Weapons Convention , defined the use of chemical weapons as a taboo under international law according to Thorbjørn Jagland , Chairman of the Norwegian Nobel Committee .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["The","2013","Nobel","Peace","Prize","was","awarded","to","the","organization","because","it","had",",","with","the","Chemical","Weapons","Convention",",","defined","the","use","of","chemical","weapons","as","a","taboo","under","international","law","according","to","Thorbjørn","Jagland",",","Chairman","of","the","Norwegian","Nobel","Committee","."],"labels":["O","B-award","I-award","I-award","I-award","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-person","I-person","O","O","O","O","B-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","protein","enzyme","organization","event","university","chemical_compound","award","academic_journal","astronomical_object","country","scientist","person","location","theory","discipline"]}
{"id":"505","dataset":"crossner_science","split":"test","instance":{"id":"505","prompt_labels":"Some(O) notable(O) examples(O) include(O) Journal(B-academic journal) of(I-academic journal) Computational(I-academic journal) Biology(I-academic journal) and(O) PLOS(B-academic journal) Computational(I-academic journal) Biology(I-academic journal) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: chemical element, protein, location, award, enzyme, event, organization, chemical compound, scientist, astronomical object, discipline, theory, university, country, academic journal, person and O.\nSentence: Some notable examples include Journal of Computational Biology and PLOS Computational Biology .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Some","notable","examples","include","Journal","of","Computational","Biology","and","PLOS","Computational","Biology","."],"labels":["O","O","O","O","B-academic journal","I-academic journal","I-academic journal","I-academic journal","O","B-academic journal","I-academic journal","I-academic journal","O"],"target_index":null,"target_label":null},"label_list":["chemical_element","protein","location","award","enzyme","event","organization","chemical_compound","scientist","astronomical_object","discipline","theory","university","country","academic_journal","person"]}
{"id":"506","dataset":"crossner_science","split":"test","instance":{"id":"506","prompt_labels":"Nectin(O) s(O) are(O) a(O) distinct(O) family(O) of(O) cell(O) adhesion(O) molecules(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, event, astronomical object, organization, protein, academic journal, theory, location, enzyme, chemical compound, university, discipline, award, country, chemical element, scientist and O.\nSentence: Nectin s are a distinct family of cell adhesion molecules .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Nectin","s","are","a","distinct","family","of","cell","adhesion","molecules","."],"labels":["O","O","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["person","event","astronomical_object","organization","protein","academic_journal","theory","location","enzyme","chemical_compound","university","discipline","award","country","chemical_element","scientist"]}
{"id":"520","dataset":"crossner_science","split":"test","instance":{"id":"520","prompt_labels":"This(O) interaction(O) stabilizes(O) both(O) TOC1(O) and(O) PRR5(O) and(O) prevents(O) their(O) degradation(O) by(O) the(O) F-box(B-protein) protein(I-protein) ZEITLUPE(B-protein) ((O) ZTL(B-protein) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, academic journal, location, scientist, enzyme, astronomical object, award, person, chemical element, country, chemical compound, theory, organization, university, protein, event and O.\nSentence: This interaction stabilizes both TOC1 and PRR5 and prevents their degradation by the F-box protein ZEITLUPE ( ZTL ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["This","interaction","stabilizes","both","TOC1","and","PRR5","and","prevents","their","degradation","by","the","F-box","protein","ZEITLUPE","(","ZTL",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","B-protein","I-protein","B-protein","O","B-protein","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","academic_journal","location","scientist","enzyme","astronomical_object","award","person","chemical_element","country","chemical_compound","theory","organization","university","protein","event"]}
{"id":"533","dataset":"crossner_science","split":"test","instance":{"id":"533","prompt_labels":"An(O) E3(B-protein) uibiquitin(I-protein) ligase(I-protein) complex(I-protein) composed(O) of(O) Cullin(B-protein) ((O) CUL3(B-protein) )(O) ,(O) speckle-type(B-protein) POZ(I-protein) protein(I-protein) ((O) SPOP(B-protein) )(O) ,(O) and(O) RING-box(B-protein) protein(I-protein) 1(O) ((O) RBX1(B-protein) )(O) has(O) been(O) shown(O) to(O) mark(O) the(O) Bmi-1(B-protein) with(O) ubiquitin(B-protein) ((O) a(O) process(O) known(O) as(O) ubiquitin(O) ation(O) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: discipline, university, enzyme, scientist, theory, location, event, chemical element, astronomical object, person, chemical compound, protein, country, academic journal, organization, award and O.\nSentence: An E3 uibiquitin ligase complex composed of Cullin ( CUL3 ) , speckle-type POZ protein ( SPOP ) , and RING-box protein 1 ( RBX1 ) has been shown to mark the Bmi-1 with ubiquitin ( a process known as ubiquitin ation ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["An","E3","uibiquitin","ligase","complex","composed","of","Cullin","(","CUL3",")",",","speckle-type","POZ","protein","(","SPOP",")",",","and","RING-box","protein","1","(","RBX1",")","has","been","shown","to","mark","the","Bmi-1","with","ubiquitin","(","a","process","known","as","ubiquitin","ation",")","."],"labels":["O","B-protein","I-protein","I-protein","I-protein","O","O","B-protein","O","B-protein","O","O","B-protein","I-protein","I-protein","O","B-protein","O","O","O","B-protein","I-protein","O","O","B-protein","O","O","O","O","O","O","O","B-protein","O","B-protein","O","O","O","O","O","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["discipline","university","enzyme","scientist","theory","location","event","chemical_element","astronomical_object","person","chemical_compound","protein","country","academic_journal","organization","award"]}
{"id":"534","dataset":"crossner_science","split":"test","instance":{"id":"534","prompt_labels":"He(O) then(O) stated(O) that(O) ,(O) based(O) on(O) his(O) data(O) on(O) various(O) comet(O) s(O) ,(O) the(O) size(O) of(O) the(O) Solar(O) system(O) can(O) be(O) 100(O) AU(O) or(O) even(O) more(O) ,(O) and(O) that(O) it(O) could(O) be(O) other(O) planet(O) s(O) there(O) that(O) perturb(O) the(O) orbit(O) of(O) Uranus(B-astronomical object) ((O) although(O) the(O) position(O) of(O) the(O) eventual(O) Neptune(B-astronomical object) was(O) not(O) calculated(O) until(O) much(O) later(O) by(O) Urbain(B-scientist) Le(I-scientist) Verrier(I-scientist) )(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: protein, chemical element, event, astronomical object, theory, organization, location, scientist, person, country, chemical compound, academic journal, discipline, award, enzyme, university and O.\nSentence: He then stated that , based on his data on various comet s , the size of the Solar system can be 100 AU or even more , and that it could be other planet s there that perturb the orbit of Uranus ( although the position of the eventual Neptune was not calculated until much later by Urbain Le Verrier ) .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["He","then","stated","that",",","based","on","his","data","on","various","comet","s",",","the","size","of","the","Solar","system","can","be","100","AU","or","even","more",",","and","that","it","could","be","other","planet","s","there","that","perturb","the","orbit","of","Uranus","(","although","the","position","of","the","eventual","Neptune","was","not","calculated","until","much","later","by","Urbain","Le","Verrier",")","."],"labels":["O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","B-astronomical object","O","O","O","O","O","O","O","B-scientist","I-scientist","I-scientist","O","O"],"target_index":null,"target_label":null},"label_list":["protein","chemical_element","event","astronomical_object","theory","organization","location","scientist","person","country","chemical_compound","academic_journal","discipline","award","enzyme","university"]}
{"id":"537","dataset":"crossner_science","split":"test","instance":{"id":"537","prompt_labels":"Overall(O) Bamberga(B-astronomical object) is(O) the(O) tenth-brightest(O) main-belt(O) asteroid(O) after(O) ,(O) in(O) order(O) ,(O) 4(B-astronomical object) Vesta(I-astronomical object) ,(O) 2(B-astronomical object) Pallas(I-astronomical object) ,(O) Ceres(B-astronomical object) ,(O) 7(B-astronomical object) Iris(I-astronomical object) ,(O) 6(B-astronomical object) Hebe(I-astronomical object) ,(O) 3(B-astronomical object) Juno(I-astronomical object) ,(O) 18(B-astronomical object) Melpomene(I-astronomical object) ,(O) 15(B-astronomical object) Eunomia(I-astronomical object) and(O) 8(B-astronomical object) Flora(I-astronomical object) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: person, astronomical object, chemical element, academic journal, enzyme, location, discipline, organization, protein, theory, university, chemical compound, event, country, award, scientist and O.\nSentence: Overall Bamberga is the tenth-brightest main-belt asteroid after , in order , 4 Vesta , 2 Pallas , Ceres , 7 Iris , 6 Hebe , 3 Juno , 18 Melpomene , 15 Eunomia and 8 Flora .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Overall","Bamberga","is","the","tenth-brightest","main-belt","asteroid","after",",","in","order",",","4","Vesta",",","2","Pallas",",","Ceres",",","7","Iris",",","6","Hebe",",","3","Juno",",","18","Melpomene",",","15","Eunomia","and","8","Flora","."],"labels":["O","B-astronomical object","O","O","O","O","O","O","O","O","O","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O","B-astronomical object","I-astronomical object","O"],"target_index":null,"target_label":null},"label_list":["person","astronomical_object","chemical_element","academic_journal","enzyme","location","discipline","organization","protein","theory","university","chemical_compound","event","country","award","scientist"]}
{"id":"540","dataset":"crossner_science","split":"test","instance":{"id":"540","prompt_labels":"Current(O) research(O) in(O) the(O) Liang(B-scientist) Tong(I-scientist) 's(O) lab(O) focuses(O) on(O) enzymes(O) involved(O) in(O) fatty(B-chemical compound) acid(I-chemical compound) metabolism(O) ,(O) including(O) Acetyl-CoA(B-enzyme) carboxylase(I-enzyme) ,(O) carnitine(B-enzyme) acyltransferase(I-enzyme) ,(O) AMP-activated(B-enzyme) protein(I-enzyme) kinase(I-enzyme) ,(O) and(O) others(O) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: scientist, theory, academic journal, country, award, astronomical object, university, event, protein, chemical element, discipline, chemical compound, person, organization, enzyme, location and O.\nSentence: Current research in the Liang Tong 's lab focuses on enzymes involved in fatty acid metabolism , including Acetyl-CoA carboxylase , carnitine acyltransferase , AMP-activated protein kinase , and others .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Current","research","in","the","Liang","Tong","'s","lab","focuses","on","enzymes","involved","in","fatty","acid","metabolism",",","including","Acetyl-CoA","carboxylase",",","carnitine","acyltransferase",",","AMP-activated","protein","kinase",",","and","others","."],"labels":["O","O","O","O","B-scientist","I-scientist","O","O","O","O","O","O","O","B-chemical compound","I-chemical compound","O","O","O","B-enzyme","I-enzyme","O","B-enzyme","I-enzyme","O","B-enzyme","I-enzyme","I-enzyme","O","O","O","O"],"target_index":null,"target_label":null},"label_list":["scientist","theory","academic_journal","country","award","astronomical_object","university","event","protein","chemical_element","discipline","chemical_compound","person","organization","enzyme","location"]}
{"id":"542","dataset":"crossner_science","split":"test","instance":{"id":"542","prompt_labels":"Mattauch(B-scientist) and(O) Fritz(B-scientist) Strassmann(I-scientist) actively(O) supported(O) the(O) proposed(O) appointment(O) of(O) Lise(B-scientist) Meitner(I-scientist) as(O) head(O) of(O) the(O) physics(B-organization) department(I-organization) of(I-organization) the(I-organization) University(I-organization) of(I-organization) Mainz(I-organization) .(O)","instruction_inputs":"Please analyze the sentence provided, identifying the type of entity for each word on a token-by-token basis.\nOutput format is: word_1(label_1), word_2(label_2), ...\nWe'll use the BIO-format to label the entities, where:\n1. B- (Begin) indicates the start of a named entity.\n2. I- (Inside) is used for words within a named entity but are not the first word.\n3. O (Outside) denotes words that are not part of a named entity.\n\nUse the specific entity tags: academic journal, location, country, organization, award, person, chemical compound, scientist, protein, university, event, theory, enzyme, astronomical object, chemical element, discipline and O.\nSentence: Mattauch and Fritz Strassmann actively supported the proposed appointment of Lise Meitner as head of the physics department of the University of Mainz .","prediction_output":null,"prediction_outputs":null,"group":null,"words":["Mattauch","and","Fritz","Strassmann","actively","supported","the","proposed","appointment","of","Lise","Meitner","as","head","of","the","physics","department","of","the","University","of","Mainz","."],"labels":["B-scientist","O","B-scientist","I-scientist","O","O","O","O","O","O","B-scientist","I-scientist","O","O","O","O","B-organization","I-organization","I-organization","I-organization","I-organization","I-organization","I-organization","O"],"target_index":null,"target_label":null},"label_list":["academic_journal","location","country","organization","award","person","chemical_compound","scientist","protein","university","event","theory","enzyme","astronomical_object","chemical_element","discipline"]}
