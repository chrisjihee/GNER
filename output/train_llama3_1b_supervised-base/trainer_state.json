{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 5.992059991177768,
  "eval_steps": 8,
  "global_step": 1698,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0035288928098809,
      "grad_norm": 4.7152389702052835,
      "learning_rate": 0.0,
      "loss": 1.7772,
      "step": 1
    },
    {
      "epoch": 0.0070577856197618,
      "grad_norm": 3.1796809015978833,
      "learning_rate": 3.2854409992410047e-06,
      "loss": 0.982,
      "step": 2
    },
    {
      "epoch": 0.0105866784296427,
      "grad_norm": 8.052387371631626,
      "learning_rate": 5.207300782128838e-06,
      "loss": 0.9976,
      "step": 3
    },
    {
      "epoch": 0.0141155712395236,
      "grad_norm": 2.1144409382298215,
      "learning_rate": 6.570881998482009e-06,
      "loss": 0.5453,
      "step": 4
    },
    {
      "epoch": 0.0176444640494045,
      "grad_norm": 3.097652401627102,
      "learning_rate": 7.628557760232498e-06,
      "loss": 0.4329,
      "step": 5
    },
    {
      "epoch": 0.0211733568592854,
      "grad_norm": 1.3094502848465848,
      "learning_rate": 8.492741781369842e-06,
      "loss": 0.3615,
      "step": 6
    },
    {
      "epoch": 0.024702249669166298,
      "grad_norm": 2.4475004018095503,
      "learning_rate": 9.223398960349088e-06,
      "loss": 0.3272,
      "step": 7
    },
    {
      "epoch": 0.0282311424790472,
      "grad_norm": 2.000660310461635,
      "learning_rate": 9.856322997723013e-06,
      "loss": 0.3273,
      "step": 8
    },
    {
      "epoch": 0.0282311424790472,
      "eval_average_f1": 0.08473896633306903,
      "eval_crossner_ai_f1": 0.14999999996175,
      "eval_crossner_ai_precision": 0.299999999997,
      "eval_crossner_ai_recall": 0.09999999999966667,
      "eval_crossner_literature_f1": 0.048780487759666875,
      "eval_crossner_literature_precision": 0.07142857142806122,
      "eval_crossner_literature_recall": 0.03703703703689987,
      "eval_crossner_music_f1": 0.03921568623212226,
      "eval_crossner_music_precision": 0.06451612903204995,
      "eval_crossner_music_recall": 0.028169014084467366,
      "eval_crossner_politics_f1": 0.017094017054247936,
      "eval_crossner_politics_precision": 0.031249999999902342,
      "eval_crossner_politics_recall": 0.0117647058823391,
      "eval_crossner_science_f1": 0.062499999956640614,
      "eval_crossner_science_precision": 0.099999999999,
      "eval_crossner_science_recall": 0.04545454545433884,
      "eval_mit-movie_f1": 0.1904761904275581,
      "eval_mit-movie_precision": 0.23188405797067843,
      "eval_mit-movie_recall": 0.16161616161599837,
      "eval_mit-restaurant_f1": 0.0851063829394975,
      "eval_mit-restaurant_precision": 0.15999999999936002,
      "eval_mit-restaurant_recall": 0.05797101449266961,
      "eval_runtime": 41.6307,
      "eval_samples_per_second": 3.075,
      "eval_steps_per_second": 0.096,
      "step": 8
    },
    {
      "epoch": 0.031760035288928096,
      "grad_norm": 2.1774717163981974,
      "learning_rate": 1.0414601564257675e-05,
      "loss": 0.2725,
      "step": 9
    },
    {
      "epoch": 0.035288928098809,
      "grad_norm": 2.1658506201592007,
      "learning_rate": 1.0913998759473504e-05,
      "loss": 0.2624,
      "step": 10
    },
    {
      "epoch": 0.0388178209086899,
      "grad_norm": 1.0330129062514741,
      "learning_rate": 1.1365758473941648e-05,
      "loss": 0.2289,
      "step": 11
    },
    {
      "epoch": 0.0423467137185708,
      "grad_norm": 1.3419936478659187,
      "learning_rate": 1.1778182780610847e-05,
      "loss": 0.2052,
      "step": 12
    },
    {
      "epoch": 0.0458756065284517,
      "grad_norm": 1.6802890144112987,
      "learning_rate": 1.2157576365200572e-05,
      "loss": 0.1975,
      "step": 13
    },
    {
      "epoch": 0.049404499338332596,
      "grad_norm": 0.9721558903969361,
      "learning_rate": 1.2508839959590092e-05,
      "loss": 0.1889,
      "step": 14
    },
    {
      "epoch": 0.0529333921482135,
      "grad_norm": 1.1183353742608342,
      "learning_rate": 1.2835858542361334e-05,
      "loss": 0.1423,
      "step": 15
    },
    {
      "epoch": 0.0564622849580944,
      "grad_norm": 0.6282202966243406,
      "learning_rate": 1.3141763996964019e-05,
      "loss": 0.138,
      "step": 16
    },
    {
      "epoch": 0.0564622849580944,
      "eval_average_f1": 0.2558984351766111,
      "eval_crossner_ai_f1": 0.23076923071952665,
      "eval_crossner_ai_precision": 0.2727272727260331,
      "eval_crossner_ai_recall": 0.19999999999933335,
      "eval_crossner_literature_f1": 0.15789473680013852,
      "eval_crossner_literature_precision": 0.2727272727247934,
      "eval_crossner_literature_recall": 0.1111111111106996,
      "eval_crossner_music_f1": 0.43636363631707437,
      "eval_crossner_music_precision": 0.6153846153830375,
      "eval_crossner_music_recall": 0.3380281690136084,
      "eval_crossner_politics_f1": 0.06451612898903485,
      "eval_crossner_politics_precision": 0.10256410256383958,
      "eval_crossner_politics_recall": 0.0470588235293564,
      "eval_crossner_science_f1": 0.0,
      "eval_crossner_science_precision": 0.0,
      "eval_crossner_science_recall": 0.0,
      "eval_mit-movie_f1": 0.5824175823673288,
      "eval_mit-movie_precision": 0.6385542168667006,
      "eval_mit-movie_recall": 0.5353535353529946,
      "eval_mit-restaurant_f1": 0.3193277310431749,
      "eval_mit-restaurant_precision": 0.37999999999924,
      "eval_mit-restaurant_recall": 0.27536231884018064,
      "eval_runtime": 54.4955,
      "eval_samples_per_second": 2.349,
      "eval_steps_per_second": 0.073,
      "step": 16
    },
    {
      "epoch": 0.0599911777679753,
      "grad_norm": 1.1004113208583608,
      "learning_rate": 1.342911800151799e-05,
      "loss": 0.1406,
      "step": 17
    },
    {
      "epoch": 0.06352007057785619,
      "grad_norm": 1.0311728939938924,
      "learning_rate": 1.370004256349868e-05,
      "loss": 0.1492,
      "step": 18
    },
    {
      "epoch": 0.0670489633877371,
      "grad_norm": 0.5672560006804004,
      "learning_rate": 1.395631521447145e-05,
      "loss": 0.0789,
      "step": 19
    },
    {
      "epoch": 0.070577856197618,
      "grad_norm": 0.49073413436902696,
      "learning_rate": 1.4199439758714506e-05,
      "loss": 0.1176,
      "step": 20
    },
    {
      "epoch": 0.0741067490074989,
      "grad_norm": 0.6858356878194646,
      "learning_rate": 1.4430699742477926e-05,
      "loss": 0.0997,
      "step": 21
    },
    {
      "epoch": 0.0776356418173798,
      "grad_norm": 0.593040469007753,
      "learning_rate": 1.4651199473182654e-05,
      "loss": 0.1056,
      "step": 22
    },
    {
      "epoch": 0.0811645346272607,
      "grad_norm": 0.6558726679151778,
      "learning_rate": 1.4861895913036545e-05,
      "loss": 0.0748,
      "step": 23
    },
    {
      "epoch": 0.0846934274371416,
      "grad_norm": 0.5671724333220075,
      "learning_rate": 1.5063623779851853e-05,
      "loss": 0.087,
      "step": 24
    },
    {
      "epoch": 0.0846934274371416,
      "eval_average_f1": 0.43134661258078505,
      "eval_crossner_ai_f1": 0.31818181813698343,
      "eval_crossner_ai_precision": 0.4999999999964286,
      "eval_crossner_ai_recall": 0.23333333333255557,
      "eval_crossner_literature_f1": 0.39024390239214757,
      "eval_crossner_literature_precision": 0.5714285714244898,
      "eval_crossner_literature_recall": 0.29629629629519894,
      "eval_crossner_music_f1": 0.4999999999516498,
      "eval_crossner_music_precision": 0.6444444444430123,
      "eval_crossner_music_recall": 0.4084507042247768,
      "eval_crossner_politics_f1": 0.28571428566617346,
      "eval_crossner_politics_precision": 0.36363636363570245,
      "eval_crossner_politics_recall": 0.23529411764678201,
      "eval_crossner_science_f1": 0.1290322580224766,
      "eval_crossner_science_precision": 0.22222222221975307,
      "eval_crossner_science_recall": 0.09090909090867769,
      "eval_mit-movie_f1": 0.7843137254394704,
      "eval_mit-movie_precision": 0.7619047619040362,
      "eval_mit-movie_recall": 0.8080808080799918,
      "eval_mit-restaurant_f1": 0.6119402984565938,
      "eval_mit-restaurant_precision": 0.6307692307682603,
      "eval_mit-restaurant_recall": 0.5942028985498634,
      "eval_runtime": 15.6425,
      "eval_samples_per_second": 8.183,
      "eval_steps_per_second": 0.256,
      "step": 24
    },
    {
      "epoch": 0.08822232024702249,
      "grad_norm": 0.5019876193651395,
      "learning_rate": 1.5257115520464996e-05,
      "loss": 0.0648,
      "step": 25
    },
    {
      "epoch": 0.0917512130569034,
      "grad_norm": 0.41314476802698136,
      "learning_rate": 1.5443017364441578e-05,
      "loss": 0.0957,
      "step": 26
    },
    {
      "epoch": 0.0952801058667843,
      "grad_norm": 0.5898072945147907,
      "learning_rate": 1.562190234638651e-05,
      "loss": 0.0791,
      "step": 27
    },
    {
      "epoch": 0.09880899867666519,
      "grad_norm": 0.5905272206097678,
      "learning_rate": 1.5794280958831095e-05,
      "loss": 0.0622,
      "step": 28
    },
    {
      "epoch": 0.1023378914865461,
      "grad_norm": 0.4271316811092361,
      "learning_rate": 1.596060993492574e-05,
      "loss": 0.0742,
      "step": 29
    },
    {
      "epoch": 0.105866784296427,
      "grad_norm": 0.6132963142245703,
      "learning_rate": 1.612129954160234e-05,
      "loss": 0.0615,
      "step": 30
    },
    {
      "epoch": 0.10939567710630789,
      "grad_norm": 1.0389150070255098,
      "learning_rate": 1.6276719676433554e-05,
      "loss": 0.0714,
      "step": 31
    },
    {
      "epoch": 0.1129245699161888,
      "grad_norm": 0.44643064294061974,
      "learning_rate": 1.6427204996205023e-05,
      "loss": 0.067,
      "step": 32
    },
    {
      "epoch": 0.1129245699161888,
      "eval_average_f1": 0.4988574009355881,
      "eval_crossner_ai_f1": 0.3333333332827161,
      "eval_crossner_ai_precision": 0.3749999999984375,
      "eval_crossner_ai_recall": 0.299999999999,
      "eval_crossner_literature_f1": 0.5217391303840265,
      "eval_crossner_literature_precision": 0.631578947365097,
      "eval_crossner_literature_recall": 0.4444444444427984,
      "eval_crossner_music_f1": 0.38399999995031037,
      "eval_crossner_music_precision": 0.4444444444436214,
      "eval_crossner_music_recall": 0.3380281690136084,
      "eval_crossner_politics_f1": 0.3589743589243014,
      "eval_crossner_politics_precision": 0.3943661971825431,
      "eval_crossner_politics_recall": 0.3294117647054948,
      "eval_crossner_science_f1": 0.31578947363379506,
      "eval_crossner_science_precision": 0.3749999999976563,
      "eval_crossner_science_recall": 0.2727272727260331,
      "eval_mit-movie_f1": 0.8457711442277766,
      "eval_mit-movie_precision": 0.8333333333325164,
      "eval_mit-movie_recall": 0.8585858585849913,
      "eval_mit-restaurant_f1": 0.7323943661461912,
      "eval_mit-restaurant_precision": 0.7123287671223119,
      "eval_mit-restaurant_recall": 0.7536231884047049,
      "eval_runtime": 16.7714,
      "eval_samples_per_second": 7.632,
      "eval_steps_per_second": 0.239,
      "step": 32
    },
    {
      "epoch": 0.1164534627260697,
      "grad_norm": 0.8669783193875997,
      "learning_rate": 1.6573059256070484e-05,
      "loss": 0.0599,
      "step": 33
    },
    {
      "epoch": 0.1199823555359506,
      "grad_norm": 0.33497734140872776,
      "learning_rate": 1.6714559000758998e-05,
      "loss": 0.056,
      "step": 34
    },
    {
      "epoch": 0.12351124834583149,
      "grad_norm": 0.5986241251329142,
      "learning_rate": 1.6851956720581588e-05,
      "loss": 0.0428,
      "step": 35
    },
    {
      "epoch": 0.12704014115571238,
      "grad_norm": 0.5465047672985186,
      "learning_rate": 1.6985483562739683e-05,
      "loss": 0.0769,
      "step": 36
    },
    {
      "epoch": 0.1305690339655933,
      "grad_norm": 0.4293983006436623,
      "learning_rate": 1.711535167107139e-05,
      "loss": 0.0652,
      "step": 37
    },
    {
      "epoch": 0.1340979267754742,
      "grad_norm": 0.447271821757871,
      "learning_rate": 1.7241756213712455e-05,
      "loss": 0.0686,
      "step": 38
    },
    {
      "epoch": 0.13762681958535508,
      "grad_norm": 0.3328564319359615,
      "learning_rate": 1.736487714732941e-05,
      "loss": 0.0565,
      "step": 39
    },
    {
      "epoch": 0.141155712395236,
      "grad_norm": 0.6576212226601122,
      "learning_rate": 1.748488075795551e-05,
      "loss": 0.0511,
      "step": 40
    },
    {
      "epoch": 0.141155712395236,
      "eval_average_f1": 0.5675295900437398,
      "eval_crossner_ai_f1": 0.38095238090128497,
      "eval_crossner_ai_precision": 0.3636363636352617,
      "eval_crossner_ai_recall": 0.3999999999986667,
      "eval_crossner_literature_f1": 0.5283018867404771,
      "eval_crossner_literature_precision": 0.5384615384594675,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.4732824426977215,
      "eval_crossner_music_precision": 0.5166666666658055,
      "eval_crossner_music_recall": 0.4366197183092442,
      "eval_crossner_politics_f1": 0.3954802259383319,
      "eval_crossner_politics_precision": 0.3804347826082821,
      "eval_crossner_politics_recall": 0.4117647058818685,
      "eval_crossner_science_f1": 0.48888888883674075,
      "eval_crossner_science_precision": 0.478260869563138,
      "eval_crossner_science_recall": 0.4999999999977273,
      "eval_mit-movie_f1": 0.8629441623856734,
      "eval_mit-movie_precision": 0.8673469387746251,
      "eval_mit-movie_recall": 0.8585858585849913,
      "eval_mit-restaurant_f1": 0.8428571428059489,
      "eval_mit-restaurant_precision": 0.8309859154917874,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 29.3156,
      "eval_samples_per_second": 4.366,
      "eval_steps_per_second": 0.136,
      "step": 40
    },
    {
      "epoch": 0.1446846052051169,
      "grad_norm": 0.6084708818193885,
      "learning_rate": 1.7601921011538085e-05,
      "loss": 0.0685,
      "step": 41
    },
    {
      "epoch": 0.1482134980149978,
      "grad_norm": 0.3964987315317575,
      "learning_rate": 1.771614074171893e-05,
      "loss": 0.0582,
      "step": 42
    },
    {
      "epoch": 0.1517423908248787,
      "grad_norm": 0.36562876903188896,
      "learning_rate": 1.7827672697834706e-05,
      "loss": 0.0603,
      "step": 43
    },
    {
      "epoch": 0.1552712836347596,
      "grad_norm": 0.2553299821543666,
      "learning_rate": 1.7936640472423658e-05,
      "loss": 0.0607,
      "step": 44
    },
    {
      "epoch": 0.1588001764446405,
      "grad_norm": 0.4337166792358861,
      "learning_rate": 1.8043159324490172e-05,
      "loss": 0.054,
      "step": 45
    },
    {
      "epoch": 0.1623290692545214,
      "grad_norm": 0.28246566517391786,
      "learning_rate": 1.814733691227755e-05,
      "loss": 0.0475,
      "step": 46
    },
    {
      "epoch": 0.1658579620644023,
      "grad_norm": 0.3874382685127396,
      "learning_rate": 1.824927394722872e-05,
      "loss": 0.0399,
      "step": 47
    },
    {
      "epoch": 0.1693868548742832,
      "grad_norm": 0.2796590497555276,
      "learning_rate": 1.8349064779092856e-05,
      "loss": 0.0402,
      "step": 48
    },
    {
      "epoch": 0.1693868548742832,
      "eval_average_f1": 0.5682105449818872,
      "eval_crossner_ai_f1": 0.43999999995024,
      "eval_crossner_ai_precision": 0.54999999999725,
      "eval_crossner_ai_recall": 0.36666666666544445,
      "eval_crossner_literature_f1": 0.36363636358505785,
      "eval_crossner_literature_precision": 0.35714285714158167,
      "eval_crossner_literature_recall": 0.37037037036899867,
      "eval_crossner_music_f1": 0.6715328466644147,
      "eval_crossner_music_precision": 0.696969696968641,
      "eval_crossner_music_recall": 0.6478873239427494,
      "eval_crossner_politics_f1": 0.48484848479794307,
      "eval_crossner_politics_precision": 0.499999999999375,
      "eval_crossner_politics_recall": 0.47058823529356403,
      "eval_crossner_science_f1": 0.37837837832812277,
      "eval_crossner_science_precision": 0.46666666666355555,
      "eval_crossner_science_recall": 0.31818181818037194,
      "eval_mit-movie_f1": 0.8743718592456049,
      "eval_mit-movie_precision": 0.86999999999913,
      "eval_mit-movie_recall": 0.8787878787869912,
      "eval_mit-restaurant_f1": 0.7647058823018275,
      "eval_mit-restaurant_precision": 0.7761194029839162,
      "eval_mit-restaurant_recall": 0.7536231884047049,
      "eval_runtime": 40.7293,
      "eval_samples_per_second": 3.143,
      "eval_steps_per_second": 0.098,
      "step": 48
    },
    {
      "epoch": 0.1729157476841641,
      "grad_norm": 0.35501237014924475,
      "learning_rate": 1.8446797920698176e-05,
      "loss": 0.0415,
      "step": 49
    },
    {
      "epoch": 0.17644464049404499,
      "grad_norm": 0.3567118653991938,
      "learning_rate": 1.8542556519706e-05,
      "loss": 0.057,
      "step": 50
    },
    {
      "epoch": 0.1799735333039259,
      "grad_norm": 0.440423986562977,
      "learning_rate": 1.8636418783646828e-05,
      "loss": 0.044,
      "step": 51
    },
    {
      "epoch": 0.1835024261138068,
      "grad_norm": 0.37052856371114057,
      "learning_rate": 1.872845836368258e-05,
      "loss": 0.0572,
      "step": 52
    },
    {
      "epoch": 0.18703131892368768,
      "grad_norm": 0.1775722392928163,
      "learning_rate": 1.8818744701813106e-05,
      "loss": 0.0362,
      "step": 53
    },
    {
      "epoch": 0.1905602117335686,
      "grad_norm": 0.21106354161989094,
      "learning_rate": 1.8907343345627517e-05,
      "loss": 0.0455,
      "step": 54
    },
    {
      "epoch": 0.1940891045434495,
      "grad_norm": 0.2912664676480331,
      "learning_rate": 1.899431623417415e-05,
      "loss": 0.0339,
      "step": 55
    },
    {
      "epoch": 0.19761799735333038,
      "grad_norm": 0.33880749927075027,
      "learning_rate": 1.9079721958072103e-05,
      "loss": 0.0371,
      "step": 56
    },
    {
      "epoch": 0.19761799735333038,
      "eval_average_f1": 0.6267919361476005,
      "eval_crossner_ai_f1": 0.3921568626951173,
      "eval_crossner_ai_precision": 0.47619047618820864,
      "eval_crossner_ai_recall": 0.33333333333222226,
      "eval_crossner_literature_f1": 0.3999999999502223,
      "eval_crossner_literature_precision": 0.4999999999972223,
      "eval_crossner_literature_recall": 0.3333333333320988,
      "eval_crossner_music_f1": 0.7538461537954201,
      "eval_crossner_music_precision": 0.8305084745748635,
      "eval_crossner_music_recall": 0.6901408450694505,
      "eval_crossner_politics_f1": 0.5664739883886665,
      "eval_crossner_politics_precision": 0.556818181817549,
      "eval_crossner_politics_recall": 0.5764705882346159,
      "eval_crossner_science_f1": 0.5714285713759637,
      "eval_crossner_science_precision": 0.599999999997,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.8775510203572731,
      "eval_mit-movie_precision": 0.8865979381434158,
      "eval_mit-movie_recall": 0.8686868686859912,
      "eval_mit-restaurant_f1": 0.8260869564705419,
      "eval_mit-restaurant_precision": 0.8260869565205419,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 15.8596,
      "eval_samples_per_second": 8.071,
      "eval_steps_per_second": 0.252,
      "step": 56
    },
    {
      "epoch": 0.2011468901632113,
      "grad_norm": 0.4090741005368046,
      "learning_rate": 1.9163615996600285e-05,
      "loss": 0.0506,
      "step": 57
    },
    {
      "epoch": 0.2046757829730922,
      "grad_norm": 0.3443924795239023,
      "learning_rate": 1.9246050934166744e-05,
      "loss": 0.0366,
      "step": 58
    },
    {
      "epoch": 0.20820467578297308,
      "grad_norm": 0.26236117927040364,
      "learning_rate": 1.932707665827352e-05,
      "loss": 0.0381,
      "step": 59
    },
    {
      "epoch": 0.211733568592854,
      "grad_norm": 0.33485415344168634,
      "learning_rate": 1.9406740540843342e-05,
      "loss": 0.0357,
      "step": 60
    },
    {
      "epoch": 0.2152624614027349,
      "grad_norm": 0.3952679318722143,
      "learning_rate": 1.9485087604558547e-05,
      "loss": 0.0574,
      "step": 61
    },
    {
      "epoch": 0.21879135421261578,
      "grad_norm": 0.37055130513756257,
      "learning_rate": 1.956216067567456e-05,
      "loss": 0.0563,
      "step": 62
    },
    {
      "epoch": 0.2223202470224967,
      "grad_norm": 0.28972893006107164,
      "learning_rate": 1.9638000524606763e-05,
      "loss": 0.0463,
      "step": 63
    },
    {
      "epoch": 0.2258491398323776,
      "grad_norm": 0.2299718834961208,
      "learning_rate": 1.9712645995446026e-05,
      "loss": 0.0511,
      "step": 64
    },
    {
      "epoch": 0.2258491398323776,
      "eval_average_f1": 0.6576989206176858,
      "eval_crossner_ai_f1": 0.4838709676904266,
      "eval_crossner_ai_precision": 0.4687499999985351,
      "eval_crossner_ai_recall": 0.4999999999983334,
      "eval_crossner_literature_f1": 0.5384615384095415,
      "eval_crossner_literature_precision": 0.5599999999977601,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.797101449224249,
      "eval_crossner_music_precision": 0.8208955223868345,
      "eval_crossner_music_recall": 0.7746478873228526,
      "eval_crossner_politics_f1": 0.6021505375841311,
      "eval_crossner_politics_precision": 0.5544554455440055,
      "eval_crossner_politics_recall": 0.6588235294109897,
      "eval_crossner_science_f1": 0.5116279069243916,
      "eval_crossner_science_precision": 0.5238095238070295,
      "eval_crossner_science_recall": 0.4999999999977273,
      "eval_mit-movie_f1": 0.9025641025131886,
      "eval_mit-movie_precision": 0.9166666666657118,
      "eval_mit-movie_recall": 0.888888888887991,
      "eval_mit-restaurant_f1": 0.7681159419778721,
      "eval_mit-restaurant_precision": 0.7681159420278723,
      "eval_mit-restaurant_recall": 0.7681159420278723,
      "eval_runtime": 16.9509,
      "eval_samples_per_second": 7.551,
      "eval_steps_per_second": 0.236,
      "step": 64
    },
    {
      "epoch": 0.22937803264225848,
      "grad_norm": 0.3307695651975018,
      "learning_rate": 1.9786134125433067e-05,
      "loss": 0.0488,
      "step": 65
    },
    {
      "epoch": 0.2329069254521394,
      "grad_norm": 0.6659962715625931,
      "learning_rate": 1.9858500255311488e-05,
      "loss": 0.0715,
      "step": 66
    },
    {
      "epoch": 0.2364358182620203,
      "grad_norm": 0.27644774270535827,
      "learning_rate": 1.992977813138264e-05,
      "loss": 0.0448,
      "step": 67
    },
    {
      "epoch": 0.2399647110719012,
      "grad_norm": 0.3277027501339797,
      "learning_rate": 2e-05,
      "loss": 0.055,
      "step": 68
    },
    {
      "epoch": 0.2434936038817821,
      "grad_norm": 0.30536713971200014,
      "learning_rate": 2e-05,
      "loss": 0.0511,
      "step": 69
    },
    {
      "epoch": 0.24702249669166298,
      "grad_norm": 0.45650158046854106,
      "learning_rate": 1.9987730061349696e-05,
      "loss": 0.0495,
      "step": 70
    },
    {
      "epoch": 0.2505513895015439,
      "grad_norm": 0.3328423522491518,
      "learning_rate": 1.997546012269939e-05,
      "loss": 0.0503,
      "step": 71
    },
    {
      "epoch": 0.25408028231142477,
      "grad_norm": 0.2916760216075544,
      "learning_rate": 1.996319018404908e-05,
      "loss": 0.0442,
      "step": 72
    },
    {
      "epoch": 0.25408028231142477,
      "eval_average_f1": 0.6127276800454268,
      "eval_crossner_ai_f1": 0.39999999994895863,
      "eval_crossner_ai_precision": 0.43999999999824,
      "eval_crossner_ai_recall": 0.36666666666544445,
      "eval_crossner_literature_f1": 0.5283018867404771,
      "eval_crossner_literature_precision": 0.5384615384594675,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.8115942028474165,
      "eval_crossner_music_precision": 0.8358208955211406,
      "eval_crossner_music_recall": 0.7887323943650862,
      "eval_crossner_politics_f1": 0.583333333282646,
      "eval_crossner_politics_precision": 0.5903614457824212,
      "eval_crossner_politics_recall": 0.5764705882346159,
      "eval_crossner_science_f1": 0.31578947363379506,
      "eval_crossner_science_precision": 0.3749999999976563,
      "eval_crossner_science_recall": 0.2727272727260331,
      "eval_mit-movie_f1": 0.8730964466496225,
      "eval_mit-movie_precision": 0.8775510204072677,
      "eval_mit-movie_recall": 0.8686868686859912,
      "eval_mit-restaurant_f1": 0.7769784172150715,
      "eval_mit-restaurant_precision": 0.7714285714274693,
      "eval_mit-restaurant_recall": 0.7826086956510396,
      "eval_runtime": 16.7053,
      "eval_samples_per_second": 7.662,
      "eval_steps_per_second": 0.239,
      "step": 72
    },
    {
      "epoch": 0.2576091751213057,
      "grad_norm": 0.20865872224911683,
      "learning_rate": 1.9950920245398774e-05,
      "loss": 0.0481,
      "step": 73
    },
    {
      "epoch": 0.2611380679311866,
      "grad_norm": 0.40880939416678075,
      "learning_rate": 1.9938650306748468e-05,
      "loss": 0.0451,
      "step": 74
    },
    {
      "epoch": 0.2646669607410675,
      "grad_norm": 0.24714531344396834,
      "learning_rate": 1.9926380368098162e-05,
      "loss": 0.0392,
      "step": 75
    },
    {
      "epoch": 0.2681958535509484,
      "grad_norm": 0.4632364421873888,
      "learning_rate": 1.9914110429447856e-05,
      "loss": 0.0442,
      "step": 76
    },
    {
      "epoch": 0.2717247463608293,
      "grad_norm": 0.25838533975243394,
      "learning_rate": 1.9901840490797547e-05,
      "loss": 0.0467,
      "step": 77
    },
    {
      "epoch": 0.27525363917071016,
      "grad_norm": 0.31744893485644776,
      "learning_rate": 1.988957055214724e-05,
      "loss": 0.0463,
      "step": 78
    },
    {
      "epoch": 0.2787825319805911,
      "grad_norm": 0.41769164694976857,
      "learning_rate": 1.9877300613496935e-05,
      "loss": 0.0568,
      "step": 79
    },
    {
      "epoch": 0.282311424790472,
      "grad_norm": 0.20195889455496813,
      "learning_rate": 1.986503067484663e-05,
      "loss": 0.0442,
      "step": 80
    },
    {
      "epoch": 0.282311424790472,
      "eval_average_f1": 0.6588073702823445,
      "eval_crossner_ai_f1": 0.4210526315276085,
      "eval_crossner_ai_precision": 0.4444444444427984,
      "eval_crossner_ai_recall": 0.3999999999986667,
      "eval_crossner_literature_f1": 0.59999999994792,
      "eval_crossner_literature_precision": 0.6521739130406428,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.8321167882700198,
      "eval_crossner_music_precision": 0.863636363635055,
      "eval_crossner_music_recall": 0.80281690140732,
      "eval_crossner_politics_f1": 0.5714285713780178,
      "eval_crossner_politics_precision": 0.6052631578939405,
      "eval_crossner_politics_recall": 0.5411764705875987,
      "eval_crossner_science_f1": 0.46511627901763114,
      "eval_crossner_science_precision": 0.47619047618820864,
      "eval_crossner_science_recall": 0.45454545454338846,
      "eval_mit-movie_f1": 0.892307692256789,
      "eval_mit-movie_precision": 0.906249999999056,
      "eval_mit-movie_recall": 0.8787878787869912,
      "eval_mit-restaurant_f1": 0.8296296295784252,
      "eval_mit-restaurant_precision": 0.8484848484835629,
      "eval_mit-restaurant_recall": 0.8115942028973745,
      "eval_runtime": 17.1011,
      "eval_samples_per_second": 7.485,
      "eval_steps_per_second": 0.234,
      "step": 80
    },
    {
      "epoch": 0.2858403176003529,
      "grad_norm": 0.28367643595868525,
      "learning_rate": 1.9852760736196323e-05,
      "loss": 0.0446,
      "step": 81
    },
    {
      "epoch": 0.2893692104102338,
      "grad_norm": 0.3553436604158273,
      "learning_rate": 1.9840490797546016e-05,
      "loss": 0.0486,
      "step": 82
    },
    {
      "epoch": 0.29289810322011467,
      "grad_norm": 0.33326152186926433,
      "learning_rate": 1.9828220858895707e-05,
      "loss": 0.0666,
      "step": 83
    },
    {
      "epoch": 0.2964269960299956,
      "grad_norm": 0.27587554389457514,
      "learning_rate": 1.98159509202454e-05,
      "loss": 0.0285,
      "step": 84
    },
    {
      "epoch": 0.2999558888398765,
      "grad_norm": 0.24133252928652135,
      "learning_rate": 1.980368098159509e-05,
      "loss": 0.0515,
      "step": 85
    },
    {
      "epoch": 0.3034847816497574,
      "grad_norm": 0.29811569759162837,
      "learning_rate": 1.9791411042944786e-05,
      "loss": 0.0345,
      "step": 86
    },
    {
      "epoch": 0.3070136744596383,
      "grad_norm": 0.27165673636841947,
      "learning_rate": 1.977914110429448e-05,
      "loss": 0.0336,
      "step": 87
    },
    {
      "epoch": 0.3105425672695192,
      "grad_norm": 0.26629979308260343,
      "learning_rate": 1.9766871165644174e-05,
      "loss": 0.0313,
      "step": 88
    },
    {
      "epoch": 0.3105425672695192,
      "eval_average_f1": 0.6518143933924011,
      "eval_crossner_ai_f1": 0.5614035087200987,
      "eval_crossner_ai_precision": 0.5925925925903979,
      "eval_crossner_ai_recall": 0.5333333333315556,
      "eval_crossner_literature_f1": 0.5306122448463141,
      "eval_crossner_literature_precision": 0.590909090906405,
      "eval_crossner_literature_recall": 0.48148148147969827,
      "eval_crossner_music_f1": 0.764705882301914,
      "eval_crossner_music_precision": 0.7999999999987693,
      "eval_crossner_music_recall": 0.7323943661961515,
      "eval_crossner_politics_f1": 0.4499999999496328,
      "eval_crossner_politics_precision": 0.47999999999936,
      "eval_crossner_politics_recall": 0.4235294117642076,
      "eval_crossner_science_f1": 0.5238095237571428,
      "eval_crossner_science_precision": 0.54999999999725,
      "eval_crossner_science_recall": 0.4999999999977273,
      "eval_mit-movie_f1": 0.8832487309135716,
      "eval_mit-movie_precision": 0.8877551020399105,
      "eval_mit-movie_recall": 0.8787878787869912,
      "eval_mit-restaurant_f1": 0.8489208632581337,
      "eval_mit-restaurant_precision": 0.8428571428559387,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.3956,
      "eval_samples_per_second": 7.807,
      "eval_steps_per_second": 0.244,
      "step": 88
    },
    {
      "epoch": 0.31407146007940007,
      "grad_norm": 0.220636560310099,
      "learning_rate": 1.9754601226993868e-05,
      "loss": 0.0423,
      "step": 89
    },
    {
      "epoch": 0.317600352889281,
      "grad_norm": 0.19383435720182604,
      "learning_rate": 1.9742331288343558e-05,
      "loss": 0.0305,
      "step": 90
    },
    {
      "epoch": 0.3211292456991619,
      "grad_norm": 0.3286746280991007,
      "learning_rate": 1.9730061349693252e-05,
      "loss": 0.0572,
      "step": 91
    },
    {
      "epoch": 0.3246581385090428,
      "grad_norm": 0.3780684893917698,
      "learning_rate": 1.9717791411042946e-05,
      "loss": 0.0462,
      "step": 92
    },
    {
      "epoch": 0.3281870313189237,
      "grad_norm": 0.5571192774830734,
      "learning_rate": 1.970552147239264e-05,
      "loss": 0.0387,
      "step": 93
    },
    {
      "epoch": 0.3317159241288046,
      "grad_norm": 0.3141679121915242,
      "learning_rate": 1.9693251533742334e-05,
      "loss": 0.0418,
      "step": 94
    },
    {
      "epoch": 0.33524481693868546,
      "grad_norm": 0.1464606520198816,
      "learning_rate": 1.9680981595092025e-05,
      "loss": 0.0338,
      "step": 95
    },
    {
      "epoch": 0.3387737097485664,
      "grad_norm": 0.2162509264402445,
      "learning_rate": 1.966871165644172e-05,
      "loss": 0.0374,
      "step": 96
    },
    {
      "epoch": 0.3387737097485664,
      "eval_average_f1": 0.7143487473272429,
      "eval_crossner_ai_f1": 0.5714285713768708,
      "eval_crossner_ai_precision": 0.5454545454528925,
      "eval_crossner_ai_recall": 0.599999999998,
      "eval_crossner_literature_f1": 0.5714285713767597,
      "eval_crossner_literature_precision": 0.6363636363607439,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.8633093524667668,
      "eval_crossner_music_precision": 0.8823529411751729,
      "eval_crossner_music_recall": 0.845070422534021,
      "eval_crossner_politics_f1": 0.5620915032178563,
      "eval_crossner_politics_precision": 0.6323529411755406,
      "eval_crossner_politics_recall": 0.5058823529405813,
      "eval_crossner_science_f1": 0.7428571428062041,
      "eval_crossner_science_precision": 0.9999999999923077,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.8979591836225583,
      "eval_mit-movie_precision": 0.9072164948444256,
      "eval_mit-movie_recall": 0.888888888887991,
      "eval_mit-restaurant_f1": 0.791366906423684,
      "eval_mit-restaurant_precision": 0.7857142857131633,
      "eval_mit-restaurant_recall": 0.7971014492742071,
      "eval_runtime": 28.6295,
      "eval_samples_per_second": 4.471,
      "eval_steps_per_second": 0.14,
      "step": 96
    },
    {
      "epoch": 0.3423026025584473,
      "grad_norm": 0.2295421221360942,
      "learning_rate": 1.9656441717791413e-05,
      "loss": 0.0345,
      "step": 97
    },
    {
      "epoch": 0.3458314953683282,
      "grad_norm": 0.31896034807972085,
      "learning_rate": 1.9644171779141106e-05,
      "loss": 0.0459,
      "step": 98
    },
    {
      "epoch": 0.3493603881782091,
      "grad_norm": 0.29238014857034833,
      "learning_rate": 1.96319018404908e-05,
      "loss": 0.0463,
      "step": 99
    },
    {
      "epoch": 0.35288928098808997,
      "grad_norm": 0.20928648921121198,
      "learning_rate": 1.961963190184049e-05,
      "loss": 0.0637,
      "step": 100
    },
    {
      "epoch": 0.35641817379797086,
      "grad_norm": 0.2615525301419851,
      "learning_rate": 1.9607361963190185e-05,
      "loss": 0.0409,
      "step": 101
    },
    {
      "epoch": 0.3599470666078518,
      "grad_norm": 0.20970215929526845,
      "learning_rate": 1.959509202453988e-05,
      "loss": 0.0412,
      "step": 102
    },
    {
      "epoch": 0.3634759594177327,
      "grad_norm": 0.18304043441428305,
      "learning_rate": 1.9582822085889573e-05,
      "loss": 0.044,
      "step": 103
    },
    {
      "epoch": 0.3670048522276136,
      "grad_norm": 0.26064593077848786,
      "learning_rate": 1.9570552147239267e-05,
      "loss": 0.0428,
      "step": 104
    },
    {
      "epoch": 0.3670048522276136,
      "eval_average_f1": 0.6134957963526299,
      "eval_crossner_ai_f1": 0.35999999995056003,
      "eval_crossner_ai_precision": 0.44999999999775003,
      "eval_crossner_ai_recall": 0.299999999999,
      "eval_crossner_literature_f1": 0.5531914893104571,
      "eval_crossner_literature_precision": 0.6499999999967501,
      "eval_crossner_literature_recall": 0.48148148147969827,
      "eval_crossner_music_f1": 0.7999999999491952,
      "eval_crossner_music_precision": 0.881355932201896,
      "eval_crossner_music_recall": 0.7323943661961515,
      "eval_crossner_politics_f1": 0.39097744356230424,
      "eval_crossner_politics_precision": 0.5416666666655382,
      "eval_crossner_politics_recall": 0.3058823529408166,
      "eval_crossner_science_f1": 0.44444444439444447,
      "eval_crossner_science_precision": 0.5714285714244898,
      "eval_crossner_science_recall": 0.36363636363471075,
      "eval_mit-movie_f1": 0.8865979380934371,
      "eval_mit-movie_precision": 0.9052631578937839,
      "eval_mit-movie_recall": 0.8686868686859912,
      "eval_mit-restaurant_f1": 0.8592592592080108,
      "eval_mit-restaurant_precision": 0.8787878787865473,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 27.9693,
      "eval_samples_per_second": 4.576,
      "eval_steps_per_second": 0.143,
      "step": 104
    },
    {
      "epoch": 0.3705337450374945,
      "grad_norm": 0.2382094087331773,
      "learning_rate": 1.9558282208588958e-05,
      "loss": 0.0464,
      "step": 105
    },
    {
      "epoch": 0.37406263784737537,
      "grad_norm": 0.3176436423535978,
      "learning_rate": 1.954601226993865e-05,
      "loss": 0.0389,
      "step": 106
    },
    {
      "epoch": 0.3775915306572563,
      "grad_norm": 0.16120347294349757,
      "learning_rate": 1.9533742331288345e-05,
      "loss": 0.0331,
      "step": 107
    },
    {
      "epoch": 0.3811204234671372,
      "grad_norm": 0.3877613194004445,
      "learning_rate": 1.952147239263804e-05,
      "loss": 0.0497,
      "step": 108
    },
    {
      "epoch": 0.3846493162770181,
      "grad_norm": 0.20986771399146478,
      "learning_rate": 1.9509202453987733e-05,
      "loss": 0.0343,
      "step": 109
    },
    {
      "epoch": 0.388178209086899,
      "grad_norm": 0.1959319685956777,
      "learning_rate": 1.9496932515337424e-05,
      "loss": 0.036,
      "step": 110
    },
    {
      "epoch": 0.3917071018967799,
      "grad_norm": 0.30440249929808766,
      "learning_rate": 1.9484662576687118e-05,
      "loss": 0.0398,
      "step": 111
    },
    {
      "epoch": 0.39523599470666076,
      "grad_norm": 0.22400924285111648,
      "learning_rate": 1.9472392638036812e-05,
      "loss": 0.036,
      "step": 112
    },
    {
      "epoch": 0.39523599470666076,
      "eval_average_f1": 0.6181648616440857,
      "eval_crossner_ai_f1": 0.5769230768720415,
      "eval_crossner_ai_precision": 0.6818181818150827,
      "eval_crossner_ai_recall": 0.4999999999983334,
      "eval_crossner_literature_f1": 0.499999999948698,
      "eval_crossner_literature_precision": 0.5714285714258504,
      "eval_crossner_literature_recall": 0.4444444444427984,
      "eval_crossner_music_f1": 0.7519999999497216,
      "eval_crossner_music_precision": 0.8703703703687585,
      "eval_crossner_music_recall": 0.6619718309849831,
      "eval_crossner_politics_f1": 0.35135135130198136,
      "eval_crossner_politics_precision": 0.4126984126977576,
      "eval_crossner_politics_recall": 0.3058823529408166,
      "eval_crossner_science_f1": 0.42105263152797784,
      "eval_crossner_science_precision": 0.49999999999687506,
      "eval_crossner_science_recall": 0.36363636363471075,
      "eval_mit-movie_f1": 0.9166666666157606,
      "eval_mit-movie_precision": 0.9462365591387675,
      "eval_mit-movie_recall": 0.888888888887991,
      "eval_mit-restaurant_f1": 0.8091603052924188,
      "eval_mit-restaurant_precision": 0.8548387096760406,
      "eval_mit-restaurant_recall": 0.7681159420278723,
      "eval_runtime": 14.9502,
      "eval_samples_per_second": 8.562,
      "eval_steps_per_second": 0.268,
      "step": 112
    },
    {
      "epoch": 0.3987648875165417,
      "grad_norm": 0.3198143877454922,
      "learning_rate": 1.9460122699386506e-05,
      "loss": 0.0437,
      "step": 113
    },
    {
      "epoch": 0.4022937803264226,
      "grad_norm": 0.17483415834709817,
      "learning_rate": 1.94478527607362e-05,
      "loss": 0.0425,
      "step": 114
    },
    {
      "epoch": 0.4058226731363035,
      "grad_norm": 0.13292202921017976,
      "learning_rate": 1.9435582822085894e-05,
      "loss": 0.0354,
      "step": 115
    },
    {
      "epoch": 0.4093515659461844,
      "grad_norm": 0.3121124964475603,
      "learning_rate": 1.9423312883435584e-05,
      "loss": 0.0454,
      "step": 116
    },
    {
      "epoch": 0.41288045875606527,
      "grad_norm": 0.34372279753123985,
      "learning_rate": 1.9411042944785275e-05,
      "loss": 0.0563,
      "step": 117
    },
    {
      "epoch": 0.41640935156594616,
      "grad_norm": 0.19439670436966378,
      "learning_rate": 1.939877300613497e-05,
      "loss": 0.0361,
      "step": 118
    },
    {
      "epoch": 0.4199382443758271,
      "grad_norm": 0.24042063369871827,
      "learning_rate": 1.9386503067484663e-05,
      "loss": 0.0447,
      "step": 119
    },
    {
      "epoch": 0.423467137185708,
      "grad_norm": 0.19326468759871174,
      "learning_rate": 1.9374233128834357e-05,
      "loss": 0.0336,
      "step": 120
    },
    {
      "epoch": 0.423467137185708,
      "eval_average_f1": 0.6511261606570092,
      "eval_crossner_ai_f1": 0.5769230768720415,
      "eval_crossner_ai_precision": 0.6818181818150827,
      "eval_crossner_ai_recall": 0.4999999999983334,
      "eval_crossner_literature_f1": 0.5217391303840265,
      "eval_crossner_literature_precision": 0.631578947365097,
      "eval_crossner_literature_recall": 0.4444444444427984,
      "eval_crossner_music_f1": 0.6999999999490102,
      "eval_crossner_music_precision": 0.7101449275352026,
      "eval_crossner_music_recall": 0.6901408450694505,
      "eval_crossner_politics_f1": 0.45569620248135717,
      "eval_crossner_politics_precision": 0.4931506849308313,
      "eval_crossner_politics_recall": 0.4235294117642076,
      "eval_crossner_science_f1": 0.6046511627379124,
      "eval_crossner_science_precision": 0.6190476190446712,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9109947643470191,
      "eval_mit-movie_precision": 0.9456521739120156,
      "eval_mit-movie_recall": 0.8787878787869912,
      "eval_mit-restaurant_f1": 0.7878787878276975,
      "eval_mit-restaurant_precision": 0.8253968253955152,
      "eval_mit-restaurant_recall": 0.7536231884047049,
      "eval_runtime": 16.3771,
      "eval_samples_per_second": 7.816,
      "eval_steps_per_second": 0.244,
      "step": 120
    },
    {
      "epoch": 0.4269960299955889,
      "grad_norm": 0.6186208137359772,
      "learning_rate": 1.936196319018405e-05,
      "loss": 0.0599,
      "step": 121
    },
    {
      "epoch": 0.4305249228054698,
      "grad_norm": 0.2565352707003024,
      "learning_rate": 1.9349693251533745e-05,
      "loss": 0.0526,
      "step": 122
    },
    {
      "epoch": 0.43405381561535067,
      "grad_norm": 0.19058847898308276,
      "learning_rate": 1.9337423312883435e-05,
      "loss": 0.0396,
      "step": 123
    },
    {
      "epoch": 0.43758270842523156,
      "grad_norm": 0.35907108475903005,
      "learning_rate": 1.932515337423313e-05,
      "loss": 0.0481,
      "step": 124
    },
    {
      "epoch": 0.4411116012351125,
      "grad_norm": 0.2648708352161604,
      "learning_rate": 1.9312883435582823e-05,
      "loss": 0.0279,
      "step": 125
    },
    {
      "epoch": 0.4446404940449934,
      "grad_norm": 0.3637236086129798,
      "learning_rate": 1.9300613496932517e-05,
      "loss": 0.0534,
      "step": 126
    },
    {
      "epoch": 0.4481693868548743,
      "grad_norm": 0.29703296674927593,
      "learning_rate": 1.928834355828221e-05,
      "loss": 0.0361,
      "step": 127
    },
    {
      "epoch": 0.4516982796647552,
      "grad_norm": 0.32642503153889996,
      "learning_rate": 1.9276073619631902e-05,
      "loss": 0.0515,
      "step": 128
    },
    {
      "epoch": 0.4516982796647552,
      "eval_average_f1": 0.6647061663952556,
      "eval_crossner_ai_f1": 0.39999999994895863,
      "eval_crossner_ai_precision": 0.43999999999824,
      "eval_crossner_ai_recall": 0.36666666666544445,
      "eval_crossner_literature_f1": 0.5714285713767597,
      "eval_crossner_literature_precision": 0.6363636363607439,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.9014084506529557,
      "eval_crossner_music_precision": 0.9014084507029557,
      "eval_crossner_music_recall": 0.9014084507029557,
      "eval_crossner_politics_f1": 0.3717948717447978,
      "eval_crossner_politics_precision": 0.4084507042247768,
      "eval_crossner_politics_recall": 0.3411764705878339,
      "eval_crossner_science_f1": 0.6521739129907372,
      "eval_crossner_science_precision": 0.6249999999973959,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9072164947944468,
      "eval_mit-movie_precision": 0.9263157894727091,
      "eval_mit-movie_recall": 0.888888888887991,
      "eval_mit-restaurant_f1": 0.8489208632581337,
      "eval_mit-restaurant_precision": 0.8428571428559387,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.5633,
      "eval_samples_per_second": 7.728,
      "eval_steps_per_second": 0.241,
      "step": 128
    },
    {
      "epoch": 0.45522717247463607,
      "grad_norm": 0.3406945463804941,
      "learning_rate": 1.9263803680981596e-05,
      "loss": 0.0357,
      "step": 129
    },
    {
      "epoch": 0.45875606528451696,
      "grad_norm": 0.15295115450541502,
      "learning_rate": 1.925153374233129e-05,
      "loss": 0.0446,
      "step": 130
    },
    {
      "epoch": 0.4622849580943979,
      "grad_norm": 0.1601716069092822,
      "learning_rate": 1.9239263803680984e-05,
      "loss": 0.0353,
      "step": 131
    },
    {
      "epoch": 0.4658138509042788,
      "grad_norm": 0.12304533473065021,
      "learning_rate": 1.9226993865030678e-05,
      "loss": 0.0408,
      "step": 132
    },
    {
      "epoch": 0.4693427437141597,
      "grad_norm": 0.34399606827357565,
      "learning_rate": 1.921472392638037e-05,
      "loss": 0.0359,
      "step": 133
    },
    {
      "epoch": 0.4728716365240406,
      "grad_norm": 0.1723498644483397,
      "learning_rate": 1.9202453987730062e-05,
      "loss": 0.0455,
      "step": 134
    },
    {
      "epoch": 0.47640052933392146,
      "grad_norm": 0.3248619226434045,
      "learning_rate": 1.9190184049079756e-05,
      "loss": 0.0425,
      "step": 135
    },
    {
      "epoch": 0.4799294221438024,
      "grad_norm": 0.21188294494271442,
      "learning_rate": 1.917791411042945e-05,
      "loss": 0.0429,
      "step": 136
    },
    {
      "epoch": 0.4799294221438024,
      "eval_average_f1": 0.7296138632786685,
      "eval_crossner_ai_f1": 0.5666666666147778,
      "eval_crossner_ai_precision": 0.5666666666647778,
      "eval_crossner_ai_recall": 0.5666666666647778,
      "eval_crossner_literature_f1": 0.5925925925403979,
      "eval_crossner_literature_precision": 0.5925925925903979,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9230769230256345,
      "eval_crossner_music_precision": 0.9166666666653935,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.5764705881846159,
      "eval_crossner_politics_precision": 0.5764705882346159,
      "eval_crossner_politics_recall": 0.5764705882346159,
      "eval_crossner_science_f1": 0.6521739129907372,
      "eval_crossner_science_precision": 0.6249999999973959,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9230769230259881,
      "eval_mit-movie_precision": 0.9374999999990234,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.8732394365685281,
      "eval_mit-restaurant_precision": 0.8493150684919872,
      "eval_mit-restaurant_recall": 0.8985507246363789,
      "eval_runtime": 17.1957,
      "eval_samples_per_second": 7.444,
      "eval_steps_per_second": 0.233,
      "step": 136
    },
    {
      "epoch": 0.4834583149536833,
      "grad_norm": 0.22859113211967755,
      "learning_rate": 1.9165644171779144e-05,
      "loss": 0.0308,
      "step": 137
    },
    {
      "epoch": 0.4869872077635642,
      "grad_norm": 0.28698849890755357,
      "learning_rate": 1.9153374233128835e-05,
      "loss": 0.048,
      "step": 138
    },
    {
      "epoch": 0.4905161005734451,
      "grad_norm": 0.2528914050731583,
      "learning_rate": 1.914110429447853e-05,
      "loss": 0.0374,
      "step": 139
    },
    {
      "epoch": 0.49404499338332597,
      "grad_norm": 0.19943705920293542,
      "learning_rate": 1.9128834355828223e-05,
      "loss": 0.0357,
      "step": 140
    },
    {
      "epoch": 0.49757388619320686,
      "grad_norm": 0.14941909881281737,
      "learning_rate": 1.9116564417177917e-05,
      "loss": 0.0253,
      "step": 141
    },
    {
      "epoch": 0.5011027790030878,
      "grad_norm": 0.20432145534293927,
      "learning_rate": 1.910429447852761e-05,
      "loss": 0.0405,
      "step": 142
    },
    {
      "epoch": 0.5046316718129686,
      "grad_norm": 0.16398679030894578,
      "learning_rate": 1.90920245398773e-05,
      "loss": 0.0256,
      "step": 143
    },
    {
      "epoch": 0.5081605646228495,
      "grad_norm": 0.18406249127282737,
      "learning_rate": 1.9079754601226995e-05,
      "loss": 0.0442,
      "step": 144
    },
    {
      "epoch": 0.5081605646228495,
      "eval_average_f1": 0.7141005673273133,
      "eval_crossner_ai_f1": 0.5263157894219761,
      "eval_crossner_ai_precision": 0.555555555553498,
      "eval_crossner_ai_recall": 0.4999999999983334,
      "eval_crossner_literature_f1": 0.63999999994776,
      "eval_crossner_literature_precision": 0.695652173910019,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.8484848484338499,
      "eval_crossner_music_precision": 0.9180327868837409,
      "eval_crossner_music_recall": 0.7887323943650862,
      "eval_crossner_politics_f1": 0.6193548386593465,
      "eval_crossner_politics_precision": 0.6857142857133061,
      "eval_crossner_politics_recall": 0.5647058823522768,
      "eval_crossner_science_f1": 0.6666666666137285,
      "eval_crossner_science_precision": 0.6521739130406428,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.8717948717439895,
      "eval_mit-movie_precision": 0.8854166666657444,
      "eval_mit-movie_recall": 0.8585858585849913,
      "eval_mit-restaurant_f1": 0.8260869564705419,
      "eval_mit-restaurant_precision": 0.8260869565205419,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 16.4467,
      "eval_samples_per_second": 7.783,
      "eval_steps_per_second": 0.243,
      "step": 144
    },
    {
      "epoch": 0.5116894574327305,
      "grad_norm": 0.20596453425567396,
      "learning_rate": 1.906748466257669e-05,
      "loss": 0.0552,
      "step": 145
    },
    {
      "epoch": 0.5152183502426114,
      "grad_norm": 0.23758149192002348,
      "learning_rate": 1.9055214723926383e-05,
      "loss": 0.0499,
      "step": 146
    },
    {
      "epoch": 0.5187472430524923,
      "grad_norm": 0.2501251980095899,
      "learning_rate": 1.9042944785276077e-05,
      "loss": 0.0342,
      "step": 147
    },
    {
      "epoch": 0.5222761358623732,
      "grad_norm": 0.17418429500134722,
      "learning_rate": 1.9030674846625768e-05,
      "loss": 0.0496,
      "step": 148
    },
    {
      "epoch": 0.5258050286722541,
      "grad_norm": 0.25426778729520977,
      "learning_rate": 1.9018404907975462e-05,
      "loss": 0.0439,
      "step": 149
    },
    {
      "epoch": 0.529333921482135,
      "grad_norm": 0.39735825545535125,
      "learning_rate": 1.9006134969325152e-05,
      "loss": 0.0379,
      "step": 150
    },
    {
      "epoch": 0.5328628142920159,
      "grad_norm": 0.15201997585777102,
      "learning_rate": 1.8993865030674846e-05,
      "loss": 0.0452,
      "step": 151
    },
    {
      "epoch": 0.5363917071018968,
      "grad_norm": 0.28175971414105977,
      "learning_rate": 1.898159509202454e-05,
      "loss": 0.0401,
      "step": 152
    },
    {
      "epoch": 0.5363917071018968,
      "eval_average_f1": 0.6590247532679785,
      "eval_crossner_ai_f1": 0.44444444439314684,
      "eval_crossner_ai_precision": 0.4242424242411386,
      "eval_crossner_ai_recall": 0.46666666666511114,
      "eval_crossner_literature_f1": 0.39215686269373323,
      "eval_crossner_literature_precision": 0.4166666666649306,
      "eval_crossner_literature_recall": 0.37037037036899867,
      "eval_crossner_music_f1": 0.9370629370116289,
      "eval_crossner_music_precision": 0.9305555555542631,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.5730994151540099,
      "eval_crossner_politics_precision": 0.5697674418598025,
      "eval_crossner_politics_recall": 0.5764705882346159,
      "eval_crossner_science_f1": 0.6222222221694815,
      "eval_crossner_science_precision": 0.6086956521712665,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.892307692256789,
      "eval_mit-movie_precision": 0.906249999999056,
      "eval_mit-movie_recall": 0.8787878787869912,
      "eval_mit-restaurant_f1": 0.7518796991970603,
      "eval_mit-restaurant_precision": 0.7812499999987793,
      "eval_mit-restaurant_recall": 0.72463768115837,
      "eval_runtime": 16.6098,
      "eval_samples_per_second": 7.706,
      "eval_steps_per_second": 0.241,
      "step": 152
    },
    {
      "epoch": 0.5399205999117777,
      "grad_norm": 0.4389903705036108,
      "learning_rate": 1.8969325153374234e-05,
      "loss": 0.0296,
      "step": 153
    },
    {
      "epoch": 0.5434494927216585,
      "grad_norm": 0.1695501163198499,
      "learning_rate": 1.8957055214723928e-05,
      "loss": 0.0343,
      "step": 154
    },
    {
      "epoch": 0.5469783855315394,
      "grad_norm": 0.22092820396182894,
      "learning_rate": 1.8944785276073622e-05,
      "loss": 0.032,
      "step": 155
    },
    {
      "epoch": 0.5505072783414203,
      "grad_norm": 0.29444160260747443,
      "learning_rate": 1.8932515337423313e-05,
      "loss": 0.0398,
      "step": 156
    },
    {
      "epoch": 0.5540361711513013,
      "grad_norm": 0.270931641758434,
      "learning_rate": 1.8920245398773007e-05,
      "loss": 0.0446,
      "step": 157
    },
    {
      "epoch": 0.5575650639611822,
      "grad_norm": 0.1881883324668358,
      "learning_rate": 1.89079754601227e-05,
      "loss": 0.0489,
      "step": 158
    },
    {
      "epoch": 0.5610939567710631,
      "grad_norm": 0.28554200508776356,
      "learning_rate": 1.8895705521472395e-05,
      "loss": 0.0599,
      "step": 159
    },
    {
      "epoch": 0.564622849580944,
      "grad_norm": 0.18608437034869196,
      "learning_rate": 1.888343558282209e-05,
      "loss": 0.0339,
      "step": 160
    },
    {
      "epoch": 0.564622849580944,
      "eval_average_f1": 0.6845029372389604,
      "eval_crossner_ai_f1": 0.45161290317440167,
      "eval_crossner_ai_precision": 0.4374999999986328,
      "eval_crossner_ai_recall": 0.46666666666511114,
      "eval_crossner_literature_f1": 0.5660377357969385,
      "eval_crossner_literature_precision": 0.576923076920858,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.8732394365684885,
      "eval_crossner_music_precision": 0.8732394366184884,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.6358381502382973,
      "eval_crossner_politics_precision": 0.6249999999992898,
      "eval_crossner_politics_recall": 0.6470588235286505,
      "eval_crossner_science_f1": 0.5106382978203713,
      "eval_crossner_science_precision": 0.47999999999808,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.9340101522333171,
      "eval_mit-movie_precision": 0.9387755102031237,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8201438848409087,
      "eval_mit-restaurant_precision": 0.814285714284551,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 16.9707,
      "eval_samples_per_second": 7.542,
      "eval_steps_per_second": 0.236,
      "step": 160
    },
    {
      "epoch": 0.5681517423908249,
      "grad_norm": 0.12325786107022381,
      "learning_rate": 1.887116564417178e-05,
      "loss": 0.0242,
      "step": 161
    },
    {
      "epoch": 0.5716806352007058,
      "grad_norm": 0.6031793441402895,
      "learning_rate": 1.8858895705521473e-05,
      "loss": 0.0358,
      "step": 162
    },
    {
      "epoch": 0.5752095280105867,
      "grad_norm": 0.12770558887393263,
      "learning_rate": 1.8846625766871167e-05,
      "loss": 0.026,
      "step": 163
    },
    {
      "epoch": 0.5787384208204676,
      "grad_norm": 0.3747061332988145,
      "learning_rate": 1.883435582822086e-05,
      "loss": 0.0446,
      "step": 164
    },
    {
      "epoch": 0.5822673136303484,
      "grad_norm": 0.31725986153281127,
      "learning_rate": 1.8822085889570555e-05,
      "loss": 0.039,
      "step": 165
    },
    {
      "epoch": 0.5857962064402293,
      "grad_norm": 0.10090901053956668,
      "learning_rate": 1.8809815950920246e-05,
      "loss": 0.0315,
      "step": 166
    },
    {
      "epoch": 0.5893250992501102,
      "grad_norm": 0.18912271809572262,
      "learning_rate": 1.879754601226994e-05,
      "loss": 0.0463,
      "step": 167
    },
    {
      "epoch": 0.5928539920599912,
      "grad_norm": 0.15969716542997256,
      "learning_rate": 1.8785276073619634e-05,
      "loss": 0.0408,
      "step": 168
    },
    {
      "epoch": 0.5928539920599912,
      "eval_average_f1": 0.6663611010750982,
      "eval_crossner_ai_f1": 0.5185185184672153,
      "eval_crossner_ai_precision": 0.5833333333309029,
      "eval_crossner_ai_recall": 0.46666666666511114,
      "eval_crossner_literature_f1": 0.55999999994808,
      "eval_crossner_literature_precision": 0.6086956521712665,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.797101449224249,
      "eval_crossner_music_precision": 0.8208955223868345,
      "eval_crossner_music_recall": 0.7746478873228526,
      "eval_crossner_politics_f1": 0.6037735848551402,
      "eval_crossner_politics_precision": 0.648648648647772,
      "eval_crossner_politics_recall": 0.5647058823522768,
      "eval_crossner_science_f1": 0.3999999999482469,
      "eval_crossner_science_precision": 0.39130434782438567,
      "eval_crossner_science_recall": 0.4090909090890496,
      "eval_mit-movie_f1": 0.9137055837054188,
      "eval_mit-movie_precision": 0.9183673469378384,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.8714285713773366,
      "eval_mit-restaurant_precision": 0.8591549295762547,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.0198,
      "eval_samples_per_second": 7.99,
      "eval_steps_per_second": 0.25,
      "step": 168
    },
    {
      "epoch": 0.5963828848698721,
      "grad_norm": 0.17215464881786646,
      "learning_rate": 1.8773006134969328e-05,
      "loss": 0.0288,
      "step": 169
    },
    {
      "epoch": 0.599911777679753,
      "grad_norm": 0.14874358366703652,
      "learning_rate": 1.876073619631902e-05,
      "loss": 0.0354,
      "step": 170
    },
    {
      "epoch": 0.6034406704896339,
      "grad_norm": 0.2803471033135801,
      "learning_rate": 1.8748466257668712e-05,
      "loss": 0.0465,
      "step": 171
    },
    {
      "epoch": 0.6069695632995148,
      "grad_norm": 0.12385700352384636,
      "learning_rate": 1.8736196319018406e-05,
      "loss": 0.0345,
      "step": 172
    },
    {
      "epoch": 0.6104984561093957,
      "grad_norm": 0.4290832953195906,
      "learning_rate": 1.87239263803681e-05,
      "loss": 0.0422,
      "step": 173
    },
    {
      "epoch": 0.6140273489192766,
      "grad_norm": 0.36315029614265903,
      "learning_rate": 1.8711656441717794e-05,
      "loss": 0.0395,
      "step": 174
    },
    {
      "epoch": 0.6175562417291575,
      "grad_norm": 0.24581642267825154,
      "learning_rate": 1.8699386503067488e-05,
      "loss": 0.034,
      "step": 175
    },
    {
      "epoch": 0.6210851345390384,
      "grad_norm": 0.2218992179763134,
      "learning_rate": 1.868711656441718e-05,
      "loss": 0.0354,
      "step": 176
    },
    {
      "epoch": 0.6210851345390384,
      "eval_average_f1": 0.6259851967018648,
      "eval_crossner_ai_f1": 0.43636363631246283,
      "eval_crossner_ai_precision": 0.47999999999808,
      "eval_crossner_ai_recall": 0.3999999999986667,
      "eval_crossner_literature_f1": 0.5531914893104571,
      "eval_crossner_literature_precision": 0.6499999999967501,
      "eval_crossner_literature_recall": 0.48148148147969827,
      "eval_crossner_music_f1": 0.7794117646548334,
      "eval_crossner_music_precision": 0.8153846153833609,
      "eval_crossner_music_recall": 0.7464788732383852,
      "eval_crossner_politics_f1": 0.4822695034975303,
      "eval_crossner_politics_precision": 0.607142857141773,
      "eval_crossner_politics_recall": 0.3999999999995294,
      "eval_crossner_science_f1": 0.38095238090068023,
      "eval_crossner_science_precision": 0.399999999998,
      "eval_crossner_science_recall": 0.36363636363471075,
      "eval_mit-movie_f1": 0.9052631578438725,
      "eval_mit-movie_precision": 0.9450549450539065,
      "eval_mit-movie_recall": 0.8686868686859912,
      "eval_mit-restaurant_f1": 0.8444444443932181,
      "eval_mit-restaurant_precision": 0.863636363635055,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 15.852,
      "eval_samples_per_second": 8.075,
      "eval_steps_per_second": 0.252,
      "step": 176
    },
    {
      "epoch": 0.6246140273489192,
      "grad_norm": 0.2544914983358003,
      "learning_rate": 1.8674846625766873e-05,
      "loss": 0.0321,
      "step": 177
    },
    {
      "epoch": 0.6281429201588001,
      "grad_norm": 0.21865036943506086,
      "learning_rate": 1.8662576687116567e-05,
      "loss": 0.0408,
      "step": 178
    },
    {
      "epoch": 0.631671812968681,
      "grad_norm": 0.2160011931663469,
      "learning_rate": 1.8650306748466257e-05,
      "loss": 0.0313,
      "step": 179
    },
    {
      "epoch": 0.635200705778562,
      "grad_norm": 0.30852522270244676,
      "learning_rate": 1.863803680981595e-05,
      "loss": 0.0458,
      "step": 180
    },
    {
      "epoch": 0.6387295985884429,
      "grad_norm": 0.10523636258860108,
      "learning_rate": 1.8625766871165645e-05,
      "loss": 0.0327,
      "step": 181
    },
    {
      "epoch": 0.6422584913983238,
      "grad_norm": 0.4582432871564633,
      "learning_rate": 1.861349693251534e-05,
      "loss": 0.0428,
      "step": 182
    },
    {
      "epoch": 0.6457873842082047,
      "grad_norm": 0.3140302741185218,
      "learning_rate": 1.860122699386503e-05,
      "loss": 0.0413,
      "step": 183
    },
    {
      "epoch": 0.6493162770180856,
      "grad_norm": 0.23552045658176482,
      "learning_rate": 1.8588957055214724e-05,
      "loss": 0.0589,
      "step": 184
    },
    {
      "epoch": 0.6493162770180856,
      "eval_average_f1": 0.6828634945146288,
      "eval_crossner_ai_f1": 0.4923076922564734,
      "eval_crossner_ai_precision": 0.457142857141551,
      "eval_crossner_ai_recall": 0.5333333333315556,
      "eval_crossner_literature_f1": 0.55999999994808,
      "eval_crossner_literature_precision": 0.6086956521712665,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.8671328670816567,
      "eval_crossner_music_precision": 0.8611111111099151,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.3678160919036266,
      "eval_crossner_politics_precision": 0.3595505617973488,
      "eval_crossner_politics_recall": 0.3764705882348512,
      "eval_crossner_science_f1": 0.7111111110579753,
      "eval_crossner_science_precision": 0.695652173910019,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9183673468878436,
      "eval_mit-movie_precision": 0.9278350515454352,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.863309352466746,
      "eval_mit-restaurant_precision": 0.8571428571416326,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.7363,
      "eval_samples_per_second": 7.648,
      "eval_steps_per_second": 0.239,
      "step": 184
    },
    {
      "epoch": 0.6528451698279665,
      "grad_norm": 0.20148299519605198,
      "learning_rate": 1.8576687116564418e-05,
      "loss": 0.0309,
      "step": 185
    },
    {
      "epoch": 0.6563740626378474,
      "grad_norm": 0.25912123181169827,
      "learning_rate": 1.856441717791411e-05,
      "loss": 0.0406,
      "step": 186
    },
    {
      "epoch": 0.6599029554477283,
      "grad_norm": 0.23328190052889694,
      "learning_rate": 1.8552147239263806e-05,
      "loss": 0.0494,
      "step": 187
    },
    {
      "epoch": 0.6634318482576091,
      "grad_norm": 0.2678985069897561,
      "learning_rate": 1.85398773006135e-05,
      "loss": 0.0454,
      "step": 188
    },
    {
      "epoch": 0.66696074106749,
      "grad_norm": 0.1093773713876598,
      "learning_rate": 1.852760736196319e-05,
      "loss": 0.0252,
      "step": 189
    },
    {
      "epoch": 0.6704896338773709,
      "grad_norm": 0.3777307188691459,
      "learning_rate": 1.8515337423312884e-05,
      "loss": 0.0478,
      "step": 190
    },
    {
      "epoch": 0.6740185266872519,
      "grad_norm": 0.29937326659734065,
      "learning_rate": 1.8503067484662578e-05,
      "loss": 0.0332,
      "step": 191
    },
    {
      "epoch": 0.6775474194971328,
      "grad_norm": 0.14200868943568992,
      "learning_rate": 1.8490797546012272e-05,
      "loss": 0.0313,
      "step": 192
    },
    {
      "epoch": 0.6775474194971328,
      "eval_average_f1": 0.6711448972408284,
      "eval_crossner_ai_f1": 0.4637681158915354,
      "eval_crossner_ai_precision": 0.4102564102553583,
      "eval_crossner_ai_recall": 0.5333333333315556,
      "eval_crossner_literature_f1": 0.6315789473163435,
      "eval_crossner_literature_precision": 0.599999999998,
      "eval_crossner_literature_recall": 0.6666666666641976,
      "eval_crossner_music_f1": 0.9115646257991578,
      "eval_crossner_music_precision": 0.8815789473672611,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.41420118338146417,
      "eval_crossner_politics_precision": 0.41666666666617064,
      "eval_crossner_politics_recall": 0.4117647058818685,
      "eval_crossner_science_f1": 0.51999999994864,
      "eval_crossner_science_precision": 0.46428571428405613,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9399999999490649,
      "eval_mit-movie_precision": 0.9306930693060091,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8169014083995934,
      "eval_mit-restaurant_precision": 0.7945205479441171,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 27.811,
      "eval_samples_per_second": 4.603,
      "eval_steps_per_second": 0.144,
      "step": 192
    },
    {
      "epoch": 0.6810763123070137,
      "grad_norm": 0.2766848649053942,
      "learning_rate": 1.8478527607361966e-05,
      "loss": 0.0512,
      "step": 193
    },
    {
      "epoch": 0.6846052051168946,
      "grad_norm": 0.21239195173426295,
      "learning_rate": 1.8466257668711657e-05,
      "loss": 0.0317,
      "step": 194
    },
    {
      "epoch": 0.6881340979267755,
      "grad_norm": 0.09577031598993167,
      "learning_rate": 1.845398773006135e-05,
      "loss": 0.0372,
      "step": 195
    },
    {
      "epoch": 0.6916629907366564,
      "grad_norm": 0.33368210750280236,
      "learning_rate": 1.8441717791411045e-05,
      "loss": 0.0344,
      "step": 196
    },
    {
      "epoch": 0.6951918835465373,
      "grad_norm": 0.2951338050753495,
      "learning_rate": 1.842944785276074e-05,
      "loss": 0.049,
      "step": 197
    },
    {
      "epoch": 0.6987207763564182,
      "grad_norm": 0.1756523401388177,
      "learning_rate": 1.8417177914110433e-05,
      "loss": 0.0404,
      "step": 198
    },
    {
      "epoch": 0.702249669166299,
      "grad_norm": 0.10947912232821239,
      "learning_rate": 1.8404907975460123e-05,
      "loss": 0.033,
      "step": 199
    },
    {
      "epoch": 0.7057785619761799,
      "grad_norm": 0.2643466086958547,
      "learning_rate": 1.8392638036809817e-05,
      "loss": 0.0292,
      "step": 200
    },
    {
      "epoch": 0.7057785619761799,
      "eval_average_f1": 0.6770399361276581,
      "eval_crossner_ai_f1": 0.39393939388861343,
      "eval_crossner_ai_precision": 0.361111111110108,
      "eval_crossner_ai_recall": 0.4333333333318889,
      "eval_crossner_literature_f1": 0.5660377357969385,
      "eval_crossner_literature_precision": 0.576923076920858,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.8671328670816567,
      "eval_crossner_music_precision": 0.8611111111099151,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.5090909090403379,
      "eval_crossner_politics_precision": 0.5249999999993438,
      "eval_crossner_politics_recall": 0.4941176470582422,
      "eval_crossner_science_f1": 0.6363636363107439,
      "eval_crossner_science_precision": 0.6363636363607439,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.923857867969368,
      "eval_mit-movie_precision": 0.928571428570481,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8428571428059489,
      "eval_mit-restaurant_precision": 0.8309859154917874,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.0207,
      "eval_samples_per_second": 7.99,
      "eval_steps_per_second": 0.25,
      "step": 200
    },
    {
      "epoch": 0.7093074547860608,
      "grad_norm": 0.2278032202066855,
      "learning_rate": 1.838036809815951e-05,
      "loss": 0.0414,
      "step": 201
    },
    {
      "epoch": 0.7128363475959417,
      "grad_norm": 0.16557229256029832,
      "learning_rate": 1.8368098159509205e-05,
      "loss": 0.0319,
      "step": 202
    },
    {
      "epoch": 0.7163652404058227,
      "grad_norm": 0.20309445261507225,
      "learning_rate": 1.83558282208589e-05,
      "loss": 0.0338,
      "step": 203
    },
    {
      "epoch": 0.7198941332157036,
      "grad_norm": 0.19382071127622372,
      "learning_rate": 1.834355828220859e-05,
      "loss": 0.0377,
      "step": 204
    },
    {
      "epoch": 0.7234230260255845,
      "grad_norm": 0.15202503611145946,
      "learning_rate": 1.8331288343558284e-05,
      "loss": 0.0454,
      "step": 205
    },
    {
      "epoch": 0.7269519188354654,
      "grad_norm": 0.38140220105476763,
      "learning_rate": 1.8319018404907978e-05,
      "loss": 0.029,
      "step": 206
    },
    {
      "epoch": 0.7304808116453463,
      "grad_norm": 0.34866817946786594,
      "learning_rate": 1.830674846625767e-05,
      "loss": 0.043,
      "step": 207
    },
    {
      "epoch": 0.7340097044552272,
      "grad_norm": 0.3646260259245732,
      "learning_rate": 1.8294478527607365e-05,
      "loss": 0.0361,
      "step": 208
    },
    {
      "epoch": 0.7340097044552272,
      "eval_average_f1": 0.7249811496839111,
      "eval_crossner_ai_f1": 0.5614035087200987,
      "eval_crossner_ai_precision": 0.5925925925903979,
      "eval_crossner_ai_recall": 0.5333333333315556,
      "eval_crossner_literature_f1": 0.6122448979072054,
      "eval_crossner_literature_precision": 0.6818181818150827,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9295774647374231,
      "eval_crossner_music_precision": 0.9295774647874231,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.5818181817675225,
      "eval_crossner_politics_precision": 0.59999999999925,
      "eval_crossner_politics_recall": 0.5647058823522768,
      "eval_crossner_science_f1": 0.6046511627379124,
      "eval_crossner_science_precision": 0.6190476190446712,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9175257731449517,
      "eval_mit-movie_precision": 0.9368421052621717,
      "eval_mit-movie_recall": 0.898989898988991,
      "eval_mit-restaurant_f1": 0.8676470587722643,
      "eval_mit-restaurant_precision": 0.8805970149240588,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 29.2732,
      "eval_samples_per_second": 4.373,
      "eval_steps_per_second": 0.137,
      "step": 208
    },
    {
      "epoch": 0.7375385972651081,
      "grad_norm": 0.1123214045811995,
      "learning_rate": 1.828220858895706e-05,
      "loss": 0.0312,
      "step": 209
    },
    {
      "epoch": 0.741067490074989,
      "grad_norm": 0.1627590363761866,
      "learning_rate": 1.826993865030675e-05,
      "loss": 0.0291,
      "step": 210
    },
    {
      "epoch": 0.7445963828848698,
      "grad_norm": 0.338469588113768,
      "learning_rate": 1.825766871165644e-05,
      "loss": 0.0348,
      "step": 211
    },
    {
      "epoch": 0.7481252756947507,
      "grad_norm": 0.20051812917363968,
      "learning_rate": 1.8245398773006135e-05,
      "loss": 0.034,
      "step": 212
    },
    {
      "epoch": 0.7516541685046316,
      "grad_norm": 0.2034190781774132,
      "learning_rate": 1.823312883435583e-05,
      "loss": 0.0366,
      "step": 213
    },
    {
      "epoch": 0.7551830613145126,
      "grad_norm": 0.13813192392273477,
      "learning_rate": 1.8220858895705523e-05,
      "loss": 0.0373,
      "step": 214
    },
    {
      "epoch": 0.7587119541243935,
      "grad_norm": 0.14035394825275932,
      "learning_rate": 1.8208588957055216e-05,
      "loss": 0.0284,
      "step": 215
    },
    {
      "epoch": 0.7622408469342744,
      "grad_norm": 0.1935134716784258,
      "learning_rate": 1.819631901840491e-05,
      "loss": 0.0369,
      "step": 216
    },
    {
      "epoch": 0.7622408469342744,
      "eval_average_f1": 0.6860811845562775,
      "eval_crossner_ai_f1": 0.5762711863887389,
      "eval_crossner_ai_precision": 0.5862068965497028,
      "eval_crossner_ai_recall": 0.5666666666647778,
      "eval_crossner_literature_f1": 0.5106382978212767,
      "eval_crossner_literature_precision": 0.599999999997,
      "eval_crossner_literature_recall": 0.4444444444427984,
      "eval_crossner_music_f1": 0.9361702127146321,
      "eval_crossner_music_precision": 0.9428571428557959,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.4596273291421319,
      "eval_crossner_politics_precision": 0.4868421052625173,
      "eval_crossner_politics_recall": 0.4352941176465467,
      "eval_crossner_science_f1": 0.6363636363107439,
      "eval_crossner_science_precision": 0.6363636363607439,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.8659793813924275,
      "eval_mit-movie_precision": 0.8842105263148587,
      "eval_mit-movie_recall": 0.8484848484839914,
      "eval_mit-restaurant_f1": 0.8175182481239917,
      "eval_mit-restaurant_precision": 0.8235294117634948,
      "eval_mit-restaurant_recall": 0.8115942028973745,
      "eval_runtime": 41.3515,
      "eval_samples_per_second": 3.095,
      "eval_steps_per_second": 0.097,
      "step": 216
    },
    {
      "epoch": 0.7657697397441553,
      "grad_norm": 0.14936792371859645,
      "learning_rate": 1.81840490797546e-05,
      "loss": 0.0423,
      "step": 217
    },
    {
      "epoch": 0.7692986325540362,
      "grad_norm": 0.22983662344635272,
      "learning_rate": 1.8171779141104295e-05,
      "loss": 0.0346,
      "step": 218
    },
    {
      "epoch": 0.7728275253639171,
      "grad_norm": 0.15797322841374747,
      "learning_rate": 1.815950920245399e-05,
      "loss": 0.0432,
      "step": 219
    },
    {
      "epoch": 0.776356418173798,
      "grad_norm": 0.16027826220590508,
      "learning_rate": 1.8147239263803683e-05,
      "loss": 0.0363,
      "step": 220
    },
    {
      "epoch": 0.7798853109836789,
      "grad_norm": 0.19748573487105836,
      "learning_rate": 1.8134969325153377e-05,
      "loss": 0.041,
      "step": 221
    },
    {
      "epoch": 0.7834142037935597,
      "grad_norm": 0.14643548785466362,
      "learning_rate": 1.8122699386503068e-05,
      "loss": 0.0289,
      "step": 222
    },
    {
      "epoch": 0.7869430966034406,
      "grad_norm": 0.10065866822254586,
      "learning_rate": 1.811042944785276e-05,
      "loss": 0.0278,
      "step": 223
    },
    {
      "epoch": 0.7904719894133215,
      "grad_norm": 0.221502174187015,
      "learning_rate": 1.8098159509202455e-05,
      "loss": 0.0346,
      "step": 224
    },
    {
      "epoch": 0.7904719894133215,
      "eval_average_f1": 0.7428761526079594,
      "eval_crossner_ai_f1": 0.6349206348687326,
      "eval_crossner_ai_precision": 0.6060606060587694,
      "eval_crossner_ai_recall": 0.6666666666644445,
      "eval_crossner_literature_f1": 0.59999999994792,
      "eval_crossner_literature_precision": 0.6521739130406428,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9436619717796568,
      "eval_crossner_music_precision": 0.9436619718296568,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.5357142856636551,
      "eval_crossner_politics_precision": 0.542168674698142,
      "eval_crossner_politics_recall": 0.5294117647052595,
      "eval_crossner_science_f1": 0.6511627906446729,
      "eval_crossner_science_precision": 0.6666666666634922,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.944162436497266,
      "eval_mit-movie_precision": 0.9489795918357663,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8905109488538122,
      "eval_mit-restaurant_precision": 0.8970588235280925,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.6674,
      "eval_samples_per_second": 7.68,
      "eval_steps_per_second": 0.24,
      "step": 224
    },
    {
      "epoch": 0.7940008822232024,
      "grad_norm": 0.25052554799571836,
      "learning_rate": 1.808588957055215e-05,
      "loss": 0.0366,
      "step": 225
    },
    {
      "epoch": 0.7975297750330834,
      "grad_norm": 0.11072653385652648,
      "learning_rate": 1.8073619631901843e-05,
      "loss": 0.0255,
      "step": 226
    },
    {
      "epoch": 0.8010586678429643,
      "grad_norm": 0.17220656967836737,
      "learning_rate": 1.8061349693251534e-05,
      "loss": 0.0373,
      "step": 227
    },
    {
      "epoch": 0.8045875606528452,
      "grad_norm": 0.3281112168459886,
      "learning_rate": 1.8049079754601228e-05,
      "loss": 0.0258,
      "step": 228
    },
    {
      "epoch": 0.8081164534627261,
      "grad_norm": 0.2630316839088794,
      "learning_rate": 1.8036809815950922e-05,
      "loss": 0.0342,
      "step": 229
    },
    {
      "epoch": 0.811645346272607,
      "grad_norm": 0.34226252852456096,
      "learning_rate": 1.8024539877300616e-05,
      "loss": 0.0523,
      "step": 230
    },
    {
      "epoch": 0.8151742390824879,
      "grad_norm": 0.20390215171700127,
      "learning_rate": 1.801226993865031e-05,
      "loss": 0.0424,
      "step": 231
    },
    {
      "epoch": 0.8187031318923688,
      "grad_norm": 0.22195050002697234,
      "learning_rate": 1.8e-05,
      "loss": 0.0374,
      "step": 232
    },
    {
      "epoch": 0.8187031318923688,
      "eval_average_f1": 0.7157022809189606,
      "eval_crossner_ai_f1": 0.5666666666147778,
      "eval_crossner_ai_precision": 0.5666666666647778,
      "eval_crossner_ai_recall": 0.5666666666647778,
      "eval_crossner_literature_f1": 0.5490196077911573,
      "eval_crossner_literature_precision": 0.5833333333309029,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.8732394365684885,
      "eval_crossner_music_precision": 0.8732394366184884,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.6971428570921011,
      "eval_crossner_politics_precision": 0.6777777777770246,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.553191489309552,
      "eval_crossner_science_precision": 0.51999999999792,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.8865979380934371,
      "eval_mit-movie_precision": 0.9052631578937839,
      "eval_mit-movie_recall": 0.8686868686859912,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.3931,
      "eval_samples_per_second": 7.808,
      "eval_steps_per_second": 0.244,
      "step": 232
    },
    {
      "epoch": 0.8222320247022497,
      "grad_norm": 0.18073003440743893,
      "learning_rate": 1.7987730061349694e-05,
      "loss": 0.0301,
      "step": 233
    },
    {
      "epoch": 0.8257609175121305,
      "grad_norm": 0.13964172160398958,
      "learning_rate": 1.797546012269939e-05,
      "loss": 0.0329,
      "step": 234
    },
    {
      "epoch": 0.8292898103220114,
      "grad_norm": 0.34299683423291294,
      "learning_rate": 1.7963190184049082e-05,
      "loss": 0.0431,
      "step": 235
    },
    {
      "epoch": 0.8328187031318923,
      "grad_norm": 0.08543493969003466,
      "learning_rate": 1.7950920245398776e-05,
      "loss": 0.0329,
      "step": 236
    },
    {
      "epoch": 0.8363475959417732,
      "grad_norm": 0.17561015173228348,
      "learning_rate": 1.7938650306748467e-05,
      "loss": 0.0282,
      "step": 237
    },
    {
      "epoch": 0.8398764887516542,
      "grad_norm": 0.12481967391298246,
      "learning_rate": 1.792638036809816e-05,
      "loss": 0.0316,
      "step": 238
    },
    {
      "epoch": 0.8434053815615351,
      "grad_norm": 0.15301192302699473,
      "learning_rate": 1.7914110429447855e-05,
      "loss": 0.0369,
      "step": 239
    },
    {
      "epoch": 0.846934274371416,
      "grad_norm": 0.3769438154424918,
      "learning_rate": 1.790184049079755e-05,
      "loss": 0.0425,
      "step": 240
    },
    {
      "epoch": 0.846934274371416,
      "eval_average_f1": 0.7342194763042907,
      "eval_crossner_ai_f1": 0.6451612902705516,
      "eval_crossner_ai_precision": 0.6249999999980469,
      "eval_crossner_ai_recall": 0.6666666666644445,
      "eval_crossner_literature_f1": 0.5769230768709319,
      "eval_crossner_literature_precision": 0.5999999999976,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.8251748251236735,
      "eval_crossner_music_precision": 0.8194444444433063,
      "eval_crossner_music_recall": 0.8309859154917874,
      "eval_crossner_politics_f1": 0.7052023120879282,
      "eval_crossner_politics_precision": 0.6931818181810304,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.6666666666137285,
      "eval_crossner_science_precision": 0.6521739130406428,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.8775510203572731,
      "eval_mit-movie_precision": 0.8865979381434158,
      "eval_mit-movie_recall": 0.8686868686859912,
      "eval_mit-restaurant_f1": 0.8428571428059489,
      "eval_mit-restaurant_precision": 0.8309859154917874,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 15.9939,
      "eval_samples_per_second": 8.003,
      "eval_steps_per_second": 0.25,
      "step": 240
    },
    {
      "epoch": 0.8504631671812969,
      "grad_norm": 0.20416229658575055,
      "learning_rate": 1.7889570552147243e-05,
      "loss": 0.0346,
      "step": 241
    },
    {
      "epoch": 0.8539920599911778,
      "grad_norm": 0.34999046653297705,
      "learning_rate": 1.7877300613496933e-05,
      "loss": 0.0323,
      "step": 242
    },
    {
      "epoch": 0.8575209528010587,
      "grad_norm": 0.2508063722665644,
      "learning_rate": 1.7865030674846627e-05,
      "loss": 0.0218,
      "step": 243
    },
    {
      "epoch": 0.8610498456109396,
      "grad_norm": 0.2533036049279463,
      "learning_rate": 1.7852760736196318e-05,
      "loss": 0.0373,
      "step": 244
    },
    {
      "epoch": 0.8645787384208204,
      "grad_norm": 0.10217137033717376,
      "learning_rate": 1.7840490797546012e-05,
      "loss": 0.0283,
      "step": 245
    },
    {
      "epoch": 0.8681076312307013,
      "grad_norm": 0.13653589691152426,
      "learning_rate": 1.7828220858895706e-05,
      "loss": 0.022,
      "step": 246
    },
    {
      "epoch": 0.8716365240405822,
      "grad_norm": 0.3708384991225289,
      "learning_rate": 1.78159509202454e-05,
      "loss": 0.037,
      "step": 247
    },
    {
      "epoch": 0.8751654168504631,
      "grad_norm": 0.18272008393937414,
      "learning_rate": 1.7803680981595094e-05,
      "loss": 0.0324,
      "step": 248
    },
    {
      "epoch": 0.8751654168504631,
      "eval_average_f1": 0.7608522671109614,
      "eval_crossner_ai_f1": 0.6984126983605946,
      "eval_crossner_ai_precision": 0.6666666666646465,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.5714285713767597,
      "eval_crossner_literature_precision": 0.6363636363607439,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.8749999999487943,
      "eval_crossner_music_precision": 0.8630136986289547,
      "eval_crossner_music_recall": 0.8873239436607221,
      "eval_crossner_politics_f1": 0.6820809248047178,
      "eval_crossner_politics_precision": 0.6704545454537836,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.7111111110579753,
      "eval_crossner_science_precision": 0.695652173910019,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9183673468878436,
      "eval_mit-movie_precision": 0.9278350515454352,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.8695652173400441,
      "eval_mit-restaurant_precision": 0.8695652173900441,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.8699,
      "eval_samples_per_second": 7.587,
      "eval_steps_per_second": 0.237,
      "step": 248
    },
    {
      "epoch": 0.8786943096603441,
      "grad_norm": 0.2927389642474327,
      "learning_rate": 1.7791411042944788e-05,
      "loss": 0.0285,
      "step": 249
    },
    {
      "epoch": 0.882223202470225,
      "grad_norm": 0.19846073562773384,
      "learning_rate": 1.777914110429448e-05,
      "loss": 0.0344,
      "step": 250
    },
    {
      "epoch": 0.8857520952801059,
      "grad_norm": 0.15514136641570433,
      "learning_rate": 1.7766871165644172e-05,
      "loss": 0.0326,
      "step": 251
    },
    {
      "epoch": 0.8892809880899868,
      "grad_norm": 0.19370206655262526,
      "learning_rate": 1.7754601226993866e-05,
      "loss": 0.0321,
      "step": 252
    },
    {
      "epoch": 0.8928098808998677,
      "grad_norm": 0.13196164954539183,
      "learning_rate": 1.774233128834356e-05,
      "loss": 0.0316,
      "step": 253
    },
    {
      "epoch": 0.8963387737097486,
      "grad_norm": 0.27605307153617725,
      "learning_rate": 1.7730061349693254e-05,
      "loss": 0.0412,
      "step": 254
    },
    {
      "epoch": 0.8998676665196295,
      "grad_norm": 0.09513350705117786,
      "learning_rate": 1.7717791411042945e-05,
      "loss": 0.0251,
      "step": 255
    },
    {
      "epoch": 0.9033965593295104,
      "grad_norm": 0.26558990807699473,
      "learning_rate": 1.770552147239264e-05,
      "loss": 0.042,
      "step": 256
    },
    {
      "epoch": 0.9033965593295104,
      "eval_average_f1": 0.7332714423310895,
      "eval_crossner_ai_f1": 0.5614035087200987,
      "eval_crossner_ai_precision": 0.5925925925903979,
      "eval_crossner_ai_recall": 0.5333333333315556,
      "eval_crossner_literature_f1": 0.5306122448463141,
      "eval_crossner_literature_precision": 0.590909090906405,
      "eval_crossner_literature_recall": 0.48148148147969827,
      "eval_crossner_music_f1": 0.8873239436107221,
      "eval_crossner_music_precision": 0.8873239436607221,
      "eval_crossner_music_recall": 0.8873239436607221,
      "eval_crossner_politics_f1": 0.6993865030167188,
      "eval_crossner_politics_precision": 0.7307692307682939,
      "eval_crossner_politics_recall": 0.6705882352933287,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9128205127695883,
      "eval_mit-movie_precision": 0.9270833333323676,
      "eval_mit-movie_recall": 0.898989898988991,
      "eval_mit-restaurant_f1": 0.8270676691217593,
      "eval_mit-restaurant_precision": 0.8593749999986572,
      "eval_mit-restaurant_recall": 0.7971014492742071,
      "eval_runtime": 16.7692,
      "eval_samples_per_second": 7.633,
      "eval_steps_per_second": 0.239,
      "step": 256
    },
    {
      "epoch": 0.9069254521393912,
      "grad_norm": 0.18905209835853315,
      "learning_rate": 1.7693251533742333e-05,
      "loss": 0.0314,
      "step": 257
    },
    {
      "epoch": 0.9104543449492721,
      "grad_norm": 0.16270690546943756,
      "learning_rate": 1.7680981595092027e-05,
      "loss": 0.0401,
      "step": 258
    },
    {
      "epoch": 0.913983237759153,
      "grad_norm": 0.1320166655955579,
      "learning_rate": 1.766871165644172e-05,
      "loss": 0.0273,
      "step": 259
    },
    {
      "epoch": 0.9175121305690339,
      "grad_norm": 0.22841501874188921,
      "learning_rate": 1.765644171779141e-05,
      "loss": 0.0376,
      "step": 260
    },
    {
      "epoch": 0.9210410233789149,
      "grad_norm": 0.1282916953054402,
      "learning_rate": 1.7644171779141105e-05,
      "loss": 0.0322,
      "step": 261
    },
    {
      "epoch": 0.9245699161887958,
      "grad_norm": 0.12860987288724285,
      "learning_rate": 1.76319018404908e-05,
      "loss": 0.0297,
      "step": 262
    },
    {
      "epoch": 0.9280988089986767,
      "grad_norm": 0.19526997103192584,
      "learning_rate": 1.7619631901840493e-05,
      "loss": 0.0335,
      "step": 263
    },
    {
      "epoch": 0.9316277018085576,
      "grad_norm": 0.18386134470434617,
      "learning_rate": 1.7607361963190187e-05,
      "loss": 0.0303,
      "step": 264
    },
    {
      "epoch": 0.9316277018085576,
      "eval_average_f1": 0.7297943648964312,
      "eval_crossner_ai_f1": 0.5074626865161952,
      "eval_crossner_ai_precision": 0.45945945945821765,
      "eval_crossner_ai_recall": 0.5666666666647778,
      "eval_crossner_literature_f1": 0.5490196077911573,
      "eval_crossner_literature_precision": 0.5833333333309029,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.9370629370116289,
      "eval_crossner_music_precision": 0.9305555555542631,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.6162790697167319,
      "eval_crossner_politics_precision": 0.6091954022981504,
      "eval_crossner_politics_recall": 0.6235294117639723,
      "eval_crossner_science_f1": 0.6818181817650827,
      "eval_crossner_science_precision": 0.6818181818150827,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9183673468878436,
      "eval_mit-movie_precision": 0.9278350515454352,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.8985507245863789,
      "eval_mit-restaurant_precision": 0.8985507246363789,
      "eval_mit-restaurant_recall": 0.8985507246363789,
      "eval_runtime": 17.0468,
      "eval_samples_per_second": 7.509,
      "eval_steps_per_second": 0.235,
      "step": 264
    },
    {
      "epoch": 0.9351565946184385,
      "grad_norm": 0.22471669991525822,
      "learning_rate": 1.7595092024539878e-05,
      "loss": 0.0443,
      "step": 265
    },
    {
      "epoch": 0.9386854874283194,
      "grad_norm": 0.18762840404256256,
      "learning_rate": 1.7582822085889572e-05,
      "loss": 0.0455,
      "step": 266
    },
    {
      "epoch": 0.9422143802382003,
      "grad_norm": 0.18912963187640294,
      "learning_rate": 1.7570552147239266e-05,
      "loss": 0.0278,
      "step": 267
    },
    {
      "epoch": 0.9457432730480811,
      "grad_norm": 0.4438770145992538,
      "learning_rate": 1.755828220858896e-05,
      "loss": 0.0348,
      "step": 268
    },
    {
      "epoch": 0.949272165857962,
      "grad_norm": 0.10290587075491121,
      "learning_rate": 1.7546012269938654e-05,
      "loss": 0.0376,
      "step": 269
    },
    {
      "epoch": 0.9528010586678429,
      "grad_norm": 0.1601883286240006,
      "learning_rate": 1.7533742331288344e-05,
      "loss": 0.027,
      "step": 270
    },
    {
      "epoch": 0.9563299514777238,
      "grad_norm": 0.1673685038803777,
      "learning_rate": 1.7521472392638038e-05,
      "loss": 0.0444,
      "step": 271
    },
    {
      "epoch": 0.9598588442876048,
      "grad_norm": 0.1470143078560441,
      "learning_rate": 1.7509202453987732e-05,
      "loss": 0.0275,
      "step": 272
    },
    {
      "epoch": 0.9598588442876048,
      "eval_average_f1": 0.7260327111798996,
      "eval_crossner_ai_f1": 0.5714285713768708,
      "eval_crossner_ai_precision": 0.5454545454528925,
      "eval_crossner_ai_recall": 0.599999999998,
      "eval_crossner_literature_f1": 0.6122448979072054,
      "eval_crossner_literature_precision": 0.6818181818150827,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9027777777265336,
      "eval_crossner_music_precision": 0.8904109589028898,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.5862068965010767,
      "eval_crossner_politics_precision": 0.5730337078645247,
      "eval_crossner_politics_recall": 0.5999999999992941,
      "eval_crossner_science_f1": 0.6956521738601135,
      "eval_crossner_science_precision": 0.666666666663889,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.908163265255201,
      "eval_mit-movie_precision": 0.9175257731949303,
      "eval_mit-movie_recall": 0.898989898988991,
      "eval_mit-restaurant_f1": 0.8057553956322965,
      "eval_mit-restaurant_precision": 0.7999999999988571,
      "eval_mit-restaurant_recall": 0.8115942028973745,
      "eval_runtime": 16.3081,
      "eval_samples_per_second": 7.849,
      "eval_steps_per_second": 0.245,
      "step": 272
    },
    {
      "epoch": 0.9633877370974857,
      "grad_norm": 0.1775399706390286,
      "learning_rate": 1.7496932515337423e-05,
      "loss": 0.0447,
      "step": 273
    },
    {
      "epoch": 0.9669166299073666,
      "grad_norm": 0.1348902982572842,
      "learning_rate": 1.7484662576687117e-05,
      "loss": 0.0274,
      "step": 274
    },
    {
      "epoch": 0.9704455227172475,
      "grad_norm": 0.19113493198799913,
      "learning_rate": 1.747239263803681e-05,
      "loss": 0.0512,
      "step": 275
    },
    {
      "epoch": 0.9739744155271284,
      "grad_norm": 0.3249827540847057,
      "learning_rate": 1.7460122699386505e-05,
      "loss": 0.0426,
      "step": 276
    },
    {
      "epoch": 0.9775033083370093,
      "grad_norm": 0.18146291879370552,
      "learning_rate": 1.7447852760736195e-05,
      "loss": 0.0343,
      "step": 277
    },
    {
      "epoch": 0.9810322011468902,
      "grad_norm": 0.363079732972188,
      "learning_rate": 1.743558282208589e-05,
      "loss": 0.0289,
      "step": 278
    },
    {
      "epoch": 0.984561093956771,
      "grad_norm": 0.19126845284805274,
      "learning_rate": 1.7423312883435583e-05,
      "loss": 0.0298,
      "step": 279
    },
    {
      "epoch": 0.9880899867666519,
      "grad_norm": 0.17660001170610884,
      "learning_rate": 1.7411042944785277e-05,
      "loss": 0.0307,
      "step": 280
    },
    {
      "epoch": 0.9880899867666519,
      "eval_average_f1": 0.7512000849964948,
      "eval_crossner_ai_f1": 0.6037735848542543,
      "eval_crossner_ai_precision": 0.695652173910019,
      "eval_crossner_ai_recall": 0.5333333333315556,
      "eval_crossner_literature_f1": 0.5714285713767597,
      "eval_crossner_literature_precision": 0.6363636363607439,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.8999999999487245,
      "eval_crossner_music_precision": 0.9130434782595462,
      "eval_crossner_music_recall": 0.8873239436607221,
      "eval_crossner_politics_f1": 0.7160493826652872,
      "eval_crossner_politics_precision": 0.753246753245775,
      "eval_crossner_politics_recall": 0.6823529411756678,
      "eval_crossner_science_f1": 0.6976744185514333,
      "eval_crossner_science_precision": 0.714285714282313,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.8854166666157931,
      "eval_mit-movie_precision": 0.9139784946226731,
      "eval_mit-movie_recall": 0.8585858585849913,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.5362,
      "eval_samples_per_second": 7.741,
      "eval_steps_per_second": 0.242,
      "step": 280
    },
    {
      "epoch": 0.9916188795765328,
      "grad_norm": 0.1527363962395823,
      "learning_rate": 1.739877300613497e-05,
      "loss": 0.037,
      "step": 281
    },
    {
      "epoch": 0.9951477723864137,
      "grad_norm": 0.14862236006247387,
      "learning_rate": 1.7386503067484665e-05,
      "loss": 0.0338,
      "step": 282
    },
    {
      "epoch": 0.9986766651962946,
      "grad_norm": 0.13532724010582106,
      "learning_rate": 1.7374233128834356e-05,
      "loss": 0.0372,
      "step": 283
    },
    {
      "epoch": 1.0022055580061755,
      "grad_norm": 0.5215200061983798,
      "learning_rate": 1.736196319018405e-05,
      "loss": 0.0681,
      "step": 284
    },
    {
      "epoch": 1.0057344508160564,
      "grad_norm": 0.45990882464931926,
      "learning_rate": 1.7349693251533744e-05,
      "loss": 0.0293,
      "step": 285
    },
    {
      "epoch": 1.0092633436259373,
      "grad_norm": 0.12608196225899995,
      "learning_rate": 1.7337423312883438e-05,
      "loss": 0.0214,
      "step": 286
    },
    {
      "epoch": 1.0127922364358182,
      "grad_norm": 0.9185623522273838,
      "learning_rate": 1.732515337423313e-05,
      "loss": 0.0319,
      "step": 287
    },
    {
      "epoch": 1.016321129245699,
      "grad_norm": 0.17611673447962264,
      "learning_rate": 1.7312883435582822e-05,
      "loss": 0.0364,
      "step": 288
    },
    {
      "epoch": 1.016321129245699,
      "eval_average_f1": 0.7813724859730987,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6249999999481771,
      "eval_crossner_literature_precision": 0.714285714282313,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.895522388008543,
      "eval_crossner_music_precision": 0.9523809523794406,
      "eval_crossner_music_recall": 0.845070422534021,
      "eval_crossner_politics_f1": 0.7303370786009659,
      "eval_crossner_politics_precision": 0.6989247311820441,
      "eval_crossner_politics_recall": 0.7647058823520415,
      "eval_crossner_science_f1": 0.7441860464581938,
      "eval_crossner_science_precision": 0.7619047619011339,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.892307692256789,
      "eval_mit-movie_precision": 0.906249999999056,
      "eval_mit-movie_recall": 0.8787878787869912,
      "eval_mit-restaurant_f1": 0.8489208632581337,
      "eval_mit-restaurant_precision": 0.8428571428559387,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 28.9478,
      "eval_samples_per_second": 4.422,
      "eval_steps_per_second": 0.138,
      "step": 288
    },
    {
      "epoch": 1.01985002205558,
      "grad_norm": 0.30277553401610463,
      "learning_rate": 1.7300613496932516e-05,
      "loss": 0.0305,
      "step": 289
    },
    {
      "epoch": 1.023378914865461,
      "grad_norm": 0.09396094194175765,
      "learning_rate": 1.728834355828221e-05,
      "loss": 0.0257,
      "step": 290
    },
    {
      "epoch": 1.026907807675342,
      "grad_norm": 0.21147709283315153,
      "learning_rate": 1.7276073619631904e-05,
      "loss": 0.036,
      "step": 291
    },
    {
      "epoch": 1.0304367004852228,
      "grad_norm": 0.3766554215252066,
      "learning_rate": 1.7263803680981598e-05,
      "loss": 0.0344,
      "step": 292
    },
    {
      "epoch": 1.0339655932951037,
      "grad_norm": 0.16164584128848547,
      "learning_rate": 1.725153374233129e-05,
      "loss": 0.0292,
      "step": 293
    },
    {
      "epoch": 1.0374944861049846,
      "grad_norm": 0.18964952926242917,
      "learning_rate": 1.7239263803680983e-05,
      "loss": 0.0202,
      "step": 294
    },
    {
      "epoch": 1.0410233789148655,
      "grad_norm": 0.19324169045517975,
      "learning_rate": 1.7226993865030677e-05,
      "loss": 0.0278,
      "step": 295
    },
    {
      "epoch": 1.0445522717247464,
      "grad_norm": 0.1947963761629018,
      "learning_rate": 1.721472392638037e-05,
      "loss": 0.0288,
      "step": 296
    },
    {
      "epoch": 1.0445522717247464,
      "eval_average_f1": 0.7421680782939454,
      "eval_crossner_ai_f1": 0.6774193547865766,
      "eval_crossner_ai_precision": 0.6562499999979492,
      "eval_crossner_ai_recall": 0.6999999999976667,
      "eval_crossner_literature_f1": 0.5833333332816841,
      "eval_crossner_literature_precision": 0.6666666666634922,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.8857142856630306,
      "eval_crossner_music_precision": 0.8985507246363789,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.6741573033201299,
      "eval_crossner_politics_precision": 0.645161290321887,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.6976744185514333,
      "eval_crossner_science_precision": 0.714285714282313,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.8279569891966296,
      "eval_mit-movie_precision": 0.8850574712633504,
      "eval_mit-movie_recall": 0.7777777777769921,
      "eval_mit-restaurant_f1": 0.8489208632581337,
      "eval_mit-restaurant_precision": 0.8428571428559387,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.643,
      "eval_samples_per_second": 7.691,
      "eval_steps_per_second": 0.24,
      "step": 296
    },
    {
      "epoch": 1.0480811645346273,
      "grad_norm": 0.23120244477998161,
      "learning_rate": 1.7202453987730065e-05,
      "loss": 0.0284,
      "step": 297
    },
    {
      "epoch": 1.0516100573445082,
      "grad_norm": 0.4628198142958001,
      "learning_rate": 1.7190184049079755e-05,
      "loss": 0.0398,
      "step": 298
    },
    {
      "epoch": 1.055138950154389,
      "grad_norm": 0.16389112377281786,
      "learning_rate": 1.717791411042945e-05,
      "loss": 0.0249,
      "step": 299
    },
    {
      "epoch": 1.05866784296427,
      "grad_norm": 0.098713006570414,
      "learning_rate": 1.7165644171779143e-05,
      "loss": 0.0202,
      "step": 300
    },
    {
      "epoch": 1.0621967357741509,
      "grad_norm": 0.15394164491672838,
      "learning_rate": 1.7153374233128837e-05,
      "loss": 0.0205,
      "step": 301
    },
    {
      "epoch": 1.0657256285840317,
      "grad_norm": 0.1553350482322956,
      "learning_rate": 1.714110429447853e-05,
      "loss": 0.0408,
      "step": 302
    },
    {
      "epoch": 1.0692545213939126,
      "grad_norm": 0.1592225210213063,
      "learning_rate": 1.712883435582822e-05,
      "loss": 0.0332,
      "step": 303
    },
    {
      "epoch": 1.0727834142037935,
      "grad_norm": 0.30312229037302746,
      "learning_rate": 1.7116564417177916e-05,
      "loss": 0.0399,
      "step": 304
    },
    {
      "epoch": 1.0727834142037935,
      "eval_average_f1": 0.7782252851877846,
      "eval_crossner_ai_f1": 0.7868852458490728,
      "eval_crossner_ai_precision": 0.7741935483845994,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.5098039215168012,
      "eval_crossner_literature_precision": 0.5416666666644098,
      "eval_crossner_literature_recall": 0.48148148147969827,
      "eval_crossner_music_f1": 0.9361702127146321,
      "eval_crossner_music_precision": 0.9428571428557959,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.6931818181310885,
      "eval_crossner_politics_precision": 0.6703296703289336,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.7619047618512471,
      "eval_crossner_science_precision": 0.799999999996,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9175257731449517,
      "eval_mit-movie_precision": 0.9368421052621717,
      "eval_mit-movie_recall": 0.898989898988991,
      "eval_mit-restaurant_f1": 0.8421052631066991,
      "eval_mit-restaurant_precision": 0.8749999999986328,
      "eval_mit-restaurant_recall": 0.8115942028973745,
      "eval_runtime": 16.8943,
      "eval_samples_per_second": 7.577,
      "eval_steps_per_second": 0.237,
      "step": 304
    },
    {
      "epoch": 1.0763123070136744,
      "grad_norm": 0.26212192908977205,
      "learning_rate": 1.7104294478527606e-05,
      "loss": 0.0274,
      "step": 305
    },
    {
      "epoch": 1.0798411998235553,
      "grad_norm": 0.12374727410507608,
      "learning_rate": 1.70920245398773e-05,
      "loss": 0.0203,
      "step": 306
    },
    {
      "epoch": 1.0833700926334362,
      "grad_norm": 0.17381446227482075,
      "learning_rate": 1.7079754601226994e-05,
      "loss": 0.0279,
      "step": 307
    },
    {
      "epoch": 1.086898985443317,
      "grad_norm": 0.2628019605648381,
      "learning_rate": 1.7067484662576688e-05,
      "loss": 0.0268,
      "step": 308
    },
    {
      "epoch": 1.090427878253198,
      "grad_norm": 0.13533520923152761,
      "learning_rate": 1.7055214723926382e-05,
      "loss": 0.0173,
      "step": 309
    },
    {
      "epoch": 1.0939567710630789,
      "grad_norm": 0.1700642307337826,
      "learning_rate": 1.7042944785276073e-05,
      "loss": 0.026,
      "step": 310
    },
    {
      "epoch": 1.0974856638729598,
      "grad_norm": 0.20200378660569124,
      "learning_rate": 1.7030674846625767e-05,
      "loss": 0.0211,
      "step": 311
    },
    {
      "epoch": 1.1010145566828409,
      "grad_norm": 0.16729801460370353,
      "learning_rate": 1.701840490797546e-05,
      "loss": 0.027,
      "step": 312
    },
    {
      "epoch": 1.1010145566828409,
      "eval_average_f1": 0.7890526546094303,
      "eval_crossner_ai_f1": 0.707692307640426,
      "eval_crossner_ai_precision": 0.6571428571409795,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9230769230256345,
      "eval_crossner_music_precision": 0.9166666666653935,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.6628571428064262,
      "eval_crossner_politics_precision": 0.6444444444437284,
      "eval_crossner_politics_recall": 0.6823529411756678,
      "eval_crossner_science_f1": 0.7727272726737604,
      "eval_crossner_science_precision": 0.7727272727237604,
      "eval_crossner_science_recall": 0.7727272727237604,
      "eval_mit-movie_f1": 0.9137055837054188,
      "eval_mit-movie_precision": 0.9183673469378384,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.863309352466746,
      "eval_mit-restaurant_precision": 0.8571428571416326,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.6911,
      "eval_samples_per_second": 7.669,
      "eval_steps_per_second": 0.24,
      "step": 312
    },
    {
      "epoch": 1.1045434494927218,
      "grad_norm": 0.17400461516823046,
      "learning_rate": 1.7006134969325155e-05,
      "loss": 0.0355,
      "step": 313
    },
    {
      "epoch": 1.1080723423026027,
      "grad_norm": 0.17191507132454592,
      "learning_rate": 1.699386503067485e-05,
      "loss": 0.0247,
      "step": 314
    },
    {
      "epoch": 1.1116012351124835,
      "grad_norm": 0.2348034915439862,
      "learning_rate": 1.6981595092024543e-05,
      "loss": 0.032,
      "step": 315
    },
    {
      "epoch": 1.1151301279223644,
      "grad_norm": 0.17142041193061572,
      "learning_rate": 1.6969325153374233e-05,
      "loss": 0.0227,
      "step": 316
    },
    {
      "epoch": 1.1186590207322453,
      "grad_norm": 0.14505229284475687,
      "learning_rate": 1.6957055214723927e-05,
      "loss": 0.0211,
      "step": 317
    },
    {
      "epoch": 1.1221879135421262,
      "grad_norm": 0.09165284850087194,
      "learning_rate": 1.694478527607362e-05,
      "loss": 0.0207,
      "step": 318
    },
    {
      "epoch": 1.125716806352007,
      "grad_norm": 0.11472219664722452,
      "learning_rate": 1.6932515337423315e-05,
      "loss": 0.0236,
      "step": 319
    },
    {
      "epoch": 1.129245699161888,
      "grad_norm": 0.15567574475149912,
      "learning_rate": 1.692024539877301e-05,
      "loss": 0.0279,
      "step": 320
    },
    {
      "epoch": 1.129245699161888,
      "eval_average_f1": 0.7462615182723312,
      "eval_crossner_ai_f1": 0.7931034482231868,
      "eval_crossner_ai_precision": 0.8214285714256379,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.5652173912534029,
      "eval_crossner_literature_precision": 0.6842105263121885,
      "eval_crossner_literature_recall": 0.48148148147969827,
      "eval_crossner_music_f1": 0.9027777777265336,
      "eval_crossner_music_precision": 0.8904109589028898,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.583333333282646,
      "eval_crossner_politics_precision": 0.5903614457824212,
      "eval_crossner_politics_recall": 0.5764705882346159,
      "eval_crossner_science_f1": 0.6818181817650827,
      "eval_crossner_science_precision": 0.6818181818150827,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.923857867969368,
      "eval_mit-movie_precision": 0.928571428570481,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.7737226276860993,
      "eval_mit-restaurant_precision": 0.7794117647047362,
      "eval_mit-restaurant_recall": 0.7681159420278723,
      "eval_runtime": 16.5554,
      "eval_samples_per_second": 7.732,
      "eval_steps_per_second": 0.242,
      "step": 320
    },
    {
      "epoch": 1.1327745919717689,
      "grad_norm": 0.510424863658662,
      "learning_rate": 1.69079754601227e-05,
      "loss": 0.0344,
      "step": 321
    },
    {
      "epoch": 1.1363034847816498,
      "grad_norm": 0.1298493865748683,
      "learning_rate": 1.6895705521472394e-05,
      "loss": 0.029,
      "step": 322
    },
    {
      "epoch": 1.1398323775915307,
      "grad_norm": 0.1821259301481929,
      "learning_rate": 1.6883435582822088e-05,
      "loss": 0.0234,
      "step": 323
    },
    {
      "epoch": 1.1433612704014116,
      "grad_norm": 0.138174103859207,
      "learning_rate": 1.687116564417178e-05,
      "loss": 0.0257,
      "step": 324
    },
    {
      "epoch": 1.1468901632112924,
      "grad_norm": 0.11201875193963616,
      "learning_rate": 1.6858895705521475e-05,
      "loss": 0.0183,
      "step": 325
    },
    {
      "epoch": 1.1504190560211733,
      "grad_norm": 0.1435750959771998,
      "learning_rate": 1.6846625766871166e-05,
      "loss": 0.024,
      "step": 326
    },
    {
      "epoch": 1.1539479488310542,
      "grad_norm": 0.15807372558571117,
      "learning_rate": 1.683435582822086e-05,
      "loss": 0.0225,
      "step": 327
    },
    {
      "epoch": 1.1574768416409351,
      "grad_norm": 0.13888907548441537,
      "learning_rate": 1.6822085889570554e-05,
      "loss": 0.0272,
      "step": 328
    },
    {
      "epoch": 1.1574768416409351,
      "eval_average_f1": 0.7475668566072985,
      "eval_crossner_ai_f1": 0.7118644067272623,
      "eval_crossner_ai_precision": 0.7241379310319858,
      "eval_crossner_ai_recall": 0.6999999999976667,
      "eval_crossner_literature_f1": 0.5833333332816841,
      "eval_crossner_literature_precision": 0.6666666666634922,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.9428571428058061,
      "eval_crossner_music_precision": 0.9565217391290485,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.6508875739137285,
      "eval_crossner_politics_precision": 0.6547619047611253,
      "eval_crossner_politics_recall": 0.6470588235286505,
      "eval_crossner_science_f1": 0.6818181817650827,
      "eval_crossner_science_precision": 0.6818181818150827,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9230769230259881,
      "eval_mit-movie_precision": 0.9374999999990234,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.7391304347315375,
      "eval_mit-restaurant_precision": 0.7391304347815375,
      "eval_mit-restaurant_recall": 0.7391304347815375,
      "eval_runtime": 16.9034,
      "eval_samples_per_second": 7.572,
      "eval_steps_per_second": 0.237,
      "step": 328
    },
    {
      "epoch": 1.161005734450816,
      "grad_norm": 0.17651270632657826,
      "learning_rate": 1.6809815950920248e-05,
      "loss": 0.0285,
      "step": 329
    },
    {
      "epoch": 1.164534627260697,
      "grad_norm": 0.22426356356246174,
      "learning_rate": 1.6797546012269942e-05,
      "loss": 0.0298,
      "step": 330
    },
    {
      "epoch": 1.1680635200705778,
      "grad_norm": 0.1360006752869551,
      "learning_rate": 1.6785276073619633e-05,
      "loss": 0.026,
      "step": 331
    },
    {
      "epoch": 1.1715924128804587,
      "grad_norm": 0.2526032244609027,
      "learning_rate": 1.6773006134969326e-05,
      "loss": 0.031,
      "step": 332
    },
    {
      "epoch": 1.1751213056903396,
      "grad_norm": 0.17488722211345228,
      "learning_rate": 1.676073619631902e-05,
      "loss": 0.0194,
      "step": 333
    },
    {
      "epoch": 1.1786501985002205,
      "grad_norm": 0.1027103038697789,
      "learning_rate": 1.6748466257668714e-05,
      "loss": 0.019,
      "step": 334
    },
    {
      "epoch": 1.1821790913101013,
      "grad_norm": 0.3359695352760344,
      "learning_rate": 1.673619631901841e-05,
      "loss": 0.0253,
      "step": 335
    },
    {
      "epoch": 1.1857079841199822,
      "grad_norm": 0.14412023767128398,
      "learning_rate": 1.67239263803681e-05,
      "loss": 0.0273,
      "step": 336
    },
    {
      "epoch": 1.1857079841199822,
      "eval_average_f1": 0.8100448980963973,
      "eval_crossner_ai_f1": 0.7241379309820452,
      "eval_crossner_ai_precision": 0.7499999999973215,
      "eval_crossner_ai_recall": 0.6999999999976667,
      "eval_crossner_literature_f1": 0.653061224437651,
      "eval_crossner_literature_precision": 0.7272727272694215,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9510489509976233,
      "eval_crossner_music_precision": 0.9444444444431327,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.768361581870114,
      "eval_crossner_politics_precision": 0.7391304347818053,
      "eval_crossner_politics_recall": 0.7999999999990588,
      "eval_crossner_science_f1": 0.7804878048245093,
      "eval_crossner_science_precision": 0.8421052631534627,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9381443298459613,
      "eval_mit-movie_precision": 0.9578947368410969,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8550724637168767,
      "eval_mit-restaurant_precision": 0.8550724637668767,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.5559,
      "eval_samples_per_second": 7.731,
      "eval_steps_per_second": 0.242,
      "step": 336
    },
    {
      "epoch": 1.1892368769298634,
      "grad_norm": 0.1558068547876851,
      "learning_rate": 1.6711656441717793e-05,
      "loss": 0.0226,
      "step": 337
    },
    {
      "epoch": 1.1927657697397442,
      "grad_norm": 0.15324742109165274,
      "learning_rate": 1.6699386503067484e-05,
      "loss": 0.0215,
      "step": 338
    },
    {
      "epoch": 1.1962946625496251,
      "grad_norm": 0.1428771671508773,
      "learning_rate": 1.6687116564417178e-05,
      "loss": 0.0317,
      "step": 339
    },
    {
      "epoch": 1.199823555359506,
      "grad_norm": 0.11398463726994001,
      "learning_rate": 1.667484662576687e-05,
      "loss": 0.0208,
      "step": 340
    },
    {
      "epoch": 1.203352448169387,
      "grad_norm": 0.14725113779747234,
      "learning_rate": 1.6662576687116565e-05,
      "loss": 0.0237,
      "step": 341
    },
    {
      "epoch": 1.2068813409792678,
      "grad_norm": 0.07437933314146677,
      "learning_rate": 1.665030674846626e-05,
      "loss": 0.0099,
      "step": 342
    },
    {
      "epoch": 1.2104102337891487,
      "grad_norm": 0.16491335533757062,
      "learning_rate": 1.6638036809815953e-05,
      "loss": 0.0296,
      "step": 343
    },
    {
      "epoch": 1.2139391265990296,
      "grad_norm": 0.21538912303900382,
      "learning_rate": 1.6625766871165644e-05,
      "loss": 0.0277,
      "step": 344
    },
    {
      "epoch": 1.2139391265990296,
      "eval_average_f1": 0.8002936778535218,
      "eval_crossner_ai_f1": 0.819672131094867,
      "eval_crossner_ai_precision": 0.8064516129006244,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.6249999999481771,
      "eval_crossner_literature_precision": 0.714285714282313,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9436619717796568,
      "eval_crossner_music_precision": 0.9436619718296568,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7613636363128292,
      "eval_crossner_politics_precision": 0.7362637362629272,
      "eval_crossner_politics_recall": 0.7882352941167197,
      "eval_crossner_science_f1": 0.6829268292152292,
      "eval_crossner_science_precision": 0.7368421052592798,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9183673468878436,
      "eval_mit-movie_precision": 0.9278350515454352,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.8510638297360494,
      "eval_mit-restaurant_precision": 0.8333333333321759,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.4797,
      "eval_samples_per_second": 7.767,
      "eval_steps_per_second": 0.243,
      "step": 344
    },
    {
      "epoch": 1.2174680194089105,
      "grad_norm": 0.15308768245854035,
      "learning_rate": 1.6613496932515338e-05,
      "loss": 0.025,
      "step": 345
    },
    {
      "epoch": 1.2209969122187914,
      "grad_norm": 0.12112353327791206,
      "learning_rate": 1.6601226993865032e-05,
      "loss": 0.0196,
      "step": 346
    },
    {
      "epoch": 1.2245258050286723,
      "grad_norm": 0.15066990372973266,
      "learning_rate": 1.6588957055214726e-05,
      "loss": 0.035,
      "step": 347
    },
    {
      "epoch": 1.2280546978385531,
      "grad_norm": 0.09408482722237302,
      "learning_rate": 1.657668711656442e-05,
      "loss": 0.0156,
      "step": 348
    },
    {
      "epoch": 1.231583590648434,
      "grad_norm": 0.1654736817285081,
      "learning_rate": 1.656441717791411e-05,
      "loss": 0.0239,
      "step": 349
    },
    {
      "epoch": 1.235112483458315,
      "grad_norm": 0.22407887877769828,
      "learning_rate": 1.6552147239263804e-05,
      "loss": 0.0267,
      "step": 350
    },
    {
      "epoch": 1.2386413762681958,
      "grad_norm": 0.21511955764210153,
      "learning_rate": 1.65398773006135e-05,
      "loss": 0.026,
      "step": 351
    },
    {
      "epoch": 1.2421702690780767,
      "grad_norm": 0.16434235772679956,
      "learning_rate": 1.6527607361963192e-05,
      "loss": 0.0207,
      "step": 352
    },
    {
      "epoch": 1.2421702690780767,
      "eval_average_f1": 0.8012868403114354,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6666666666146702,
      "eval_crossner_literature_precision": 0.7619047619011339,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9014084506529557,
      "eval_crossner_music_precision": 0.9014084507029557,
      "eval_crossner_music_recall": 0.9014084507029557,
      "eval_crossner_politics_f1": 0.693641618446323,
      "eval_crossner_politics_precision": 0.681818181817407,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.8095238094700682,
      "eval_crossner_science_precision": 0.84999999999575,
      "eval_crossner_science_recall": 0.7727272727237604,
      "eval_mit-movie_f1": 0.8979591836225583,
      "eval_mit-movie_precision": 0.9072164948444256,
      "eval_mit-movie_recall": 0.888888888887991,
      "eval_mit-restaurant_f1": 0.9064748200925831,
      "eval_mit-restaurant_precision": 0.8999999999987143,
      "eval_mit-restaurant_recall": 0.9130434782595462,
      "eval_runtime": 16.5491,
      "eval_samples_per_second": 7.735,
      "eval_steps_per_second": 0.242,
      "step": 352
    },
    {
      "epoch": 1.2456991618879576,
      "grad_norm": 0.13337010547707634,
      "learning_rate": 1.6515337423312886e-05,
      "loss": 0.0159,
      "step": 353
    },
    {
      "epoch": 1.2492280546978385,
      "grad_norm": 0.18436852783222843,
      "learning_rate": 1.6503067484662577e-05,
      "loss": 0.0204,
      "step": 354
    },
    {
      "epoch": 1.2527569475077194,
      "grad_norm": 0.14983012046921274,
      "learning_rate": 1.649079754601227e-05,
      "loss": 0.0264,
      "step": 355
    },
    {
      "epoch": 1.2562858403176005,
      "grad_norm": 0.18752512167484336,
      "learning_rate": 1.6478527607361965e-05,
      "loss": 0.0251,
      "step": 356
    },
    {
      "epoch": 1.2598147331274814,
      "grad_norm": 0.16289561124529467,
      "learning_rate": 1.646625766871166e-05,
      "loss": 0.0265,
      "step": 357
    },
    {
      "epoch": 1.2633436259373623,
      "grad_norm": 0.09379932476907099,
      "learning_rate": 1.6453987730061353e-05,
      "loss": 0.0158,
      "step": 358
    },
    {
      "epoch": 1.2668725187472432,
      "grad_norm": 0.1564101233179011,
      "learning_rate": 1.6441717791411043e-05,
      "loss": 0.0242,
      "step": 359
    },
    {
      "epoch": 1.270401411557124,
      "grad_norm": 0.15919555982780273,
      "learning_rate": 1.6429447852760737e-05,
      "loss": 0.0223,
      "step": 360
    },
    {
      "epoch": 1.270401411557124,
      "eval_average_f1": 0.7898691714238586,
      "eval_crossner_ai_f1": 0.7666666666141112,
      "eval_crossner_ai_precision": 0.7666666666641112,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6382978722888186,
      "eval_crossner_literature_precision": 0.7499999999962501,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9295774647374231,
      "eval_crossner_music_precision": 0.9295774647874231,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.7017543859140932,
      "eval_crossner_politics_precision": 0.6976744186038399,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.7111111110579753,
      "eval_crossner_science_precision": 0.695652173910019,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9183673468878436,
      "eval_mit-movie_precision": 0.9278350515454352,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.863309352466746,
      "eval_mit-restaurant_precision": 0.8571428571416326,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.6327,
      "eval_samples_per_second": 7.696,
      "eval_steps_per_second": 0.24,
      "step": 360
    },
    {
      "epoch": 1.273930304367005,
      "grad_norm": 0.13256283743301459,
      "learning_rate": 1.641717791411043e-05,
      "loss": 0.0169,
      "step": 361
    },
    {
      "epoch": 1.2774591971768858,
      "grad_norm": 0.15490508021678123,
      "learning_rate": 1.6404907975460125e-05,
      "loss": 0.0226,
      "step": 362
    },
    {
      "epoch": 1.2809880899867667,
      "grad_norm": 0.2596266812328402,
      "learning_rate": 1.639263803680982e-05,
      "loss": 0.0254,
      "step": 363
    },
    {
      "epoch": 1.2845169827966476,
      "grad_norm": 0.1617859578651467,
      "learning_rate": 1.638036809815951e-05,
      "loss": 0.0211,
      "step": 364
    },
    {
      "epoch": 1.2880458756065285,
      "grad_norm": 0.16327641835889678,
      "learning_rate": 1.6368098159509204e-05,
      "loss": 0.0335,
      "step": 365
    },
    {
      "epoch": 1.2915747684164094,
      "grad_norm": 0.35266643558475397,
      "learning_rate": 1.6355828220858898e-05,
      "loss": 0.0249,
      "step": 366
    },
    {
      "epoch": 1.2951036612262903,
      "grad_norm": 0.4217613914954772,
      "learning_rate": 1.634355828220859e-05,
      "loss": 0.0205,
      "step": 367
    },
    {
      "epoch": 1.2986325540361712,
      "grad_norm": 0.12812726582872952,
      "learning_rate": 1.6331288343558282e-05,
      "loss": 0.03,
      "step": 368
    },
    {
      "epoch": 1.2986325540361712,
      "eval_average_f1": 0.8012383392572381,
      "eval_crossner_ai_f1": 0.7868852458490728,
      "eval_crossner_ai_precision": 0.7741935483845994,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6249999999481771,
      "eval_crossner_literature_precision": 0.714285714282313,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9361702127146321,
      "eval_crossner_music_precision": 0.9428571428557959,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.678787878737102,
      "eval_crossner_politics_precision": 0.699999999999125,
      "eval_crossner_politics_recall": 0.6588235294109897,
      "eval_crossner_science_f1": 0.7999999999464691,
      "eval_crossner_science_precision": 0.7826086956487713,
      "eval_crossner_science_recall": 0.8181818181780992,
      "eval_mit-movie_f1": 0.9435897435387876,
      "eval_mit-movie_precision": 0.9583333333323351,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8382352940664252,
      "eval_mit-restaurant_precision": 0.8507462686554467,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 16.8471,
      "eval_samples_per_second": 7.598,
      "eval_steps_per_second": 0.237,
      "step": 368
    },
    {
      "epoch": 1.302161446846052,
      "grad_norm": 0.1300895451066152,
      "learning_rate": 1.6319018404907976e-05,
      "loss": 0.0256,
      "step": 369
    },
    {
      "epoch": 1.305690339655933,
      "grad_norm": 0.19255938874071005,
      "learning_rate": 1.630674846625767e-05,
      "loss": 0.0335,
      "step": 370
    },
    {
      "epoch": 1.3092192324658138,
      "grad_norm": 0.14094810983006845,
      "learning_rate": 1.629447852760736e-05,
      "loss": 0.0244,
      "step": 371
    },
    {
      "epoch": 1.3127481252756947,
      "grad_norm": 0.2370677832963369,
      "learning_rate": 1.6282208588957055e-05,
      "loss": 0.0186,
      "step": 372
    },
    {
      "epoch": 1.3162770180855756,
      "grad_norm": 0.19470726652204834,
      "learning_rate": 1.626993865030675e-05,
      "loss": 0.0249,
      "step": 373
    },
    {
      "epoch": 1.3198059108954565,
      "grad_norm": 0.11021480272323761,
      "learning_rate": 1.6257668711656443e-05,
      "loss": 0.0218,
      "step": 374
    },
    {
      "epoch": 1.3233348037053374,
      "grad_norm": 0.09704452129802409,
      "learning_rate": 1.6245398773006137e-05,
      "loss": 0.022,
      "step": 375
    },
    {
      "epoch": 1.3268636965152183,
      "grad_norm": 0.21286694806226492,
      "learning_rate": 1.623312883435583e-05,
      "loss": 0.0312,
      "step": 376
    },
    {
      "epoch": 1.3268636965152183,
      "eval_average_f1": 0.785259116106216,
      "eval_crossner_ai_f1": 0.6874999999480469,
      "eval_crossner_ai_precision": 0.6470588235275087,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9295774647374231,
      "eval_crossner_music_precision": 0.9295774647874231,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.694610778392298,
      "eval_crossner_politics_precision": 0.7073170731698691,
      "eval_crossner_politics_recall": 0.6823529411756678,
      "eval_crossner_science_f1": 0.7619047618512471,
      "eval_crossner_science_precision": 0.799999999996,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9230769230259881,
      "eval_mit-movie_precision": 0.9374999999990234,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.8201438848409087,
      "eval_mit-restaurant_precision": 0.814285714284551,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 16.9662,
      "eval_samples_per_second": 7.544,
      "eval_steps_per_second": 0.236,
      "step": 376
    },
    {
      "epoch": 1.3303925893250992,
      "grad_norm": 0.14388039029370248,
      "learning_rate": 1.622085889570552e-05,
      "loss": 0.0339,
      "step": 377
    },
    {
      "epoch": 1.33392148213498,
      "grad_norm": 0.23119047275105636,
      "learning_rate": 1.6208588957055215e-05,
      "loss": 0.026,
      "step": 378
    },
    {
      "epoch": 1.337450374944861,
      "grad_norm": 0.28569604132082416,
      "learning_rate": 1.619631901840491e-05,
      "loss": 0.026,
      "step": 379
    },
    {
      "epoch": 1.3409792677547419,
      "grad_norm": 0.1663227158182468,
      "learning_rate": 1.6184049079754603e-05,
      "loss": 0.0218,
      "step": 380
    },
    {
      "epoch": 1.3445081605646227,
      "grad_norm": 0.23808724682317248,
      "learning_rate": 1.6171779141104297e-05,
      "loss": 0.0221,
      "step": 381
    },
    {
      "epoch": 1.3480370533745036,
      "grad_norm": 0.13173140867728406,
      "learning_rate": 1.6159509202453988e-05,
      "loss": 0.0297,
      "step": 382
    },
    {
      "epoch": 1.3515659461843845,
      "grad_norm": 0.27816462930954505,
      "learning_rate": 1.6147239263803682e-05,
      "loss": 0.0263,
      "step": 383
    },
    {
      "epoch": 1.3550948389942654,
      "grad_norm": 0.11447562543360142,
      "learning_rate": 1.6134969325153376e-05,
      "loss": 0.0253,
      "step": 384
    },
    {
      "epoch": 1.3550948389942654,
      "eval_average_f1": 0.8061057934684126,
      "eval_crossner_ai_f1": 0.7936507935983874,
      "eval_crossner_ai_precision": 0.7575757575734618,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.653061224437651,
      "eval_crossner_literature_precision": 0.7272727272694215,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9014084506529557,
      "eval_crossner_music_precision": 0.9014084507029557,
      "eval_crossner_music_recall": 0.9014084507029557,
      "eval_crossner_politics_f1": 0.6857142856635428,
      "eval_crossner_politics_precision": 0.6666666666659259,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.8181818181280992,
      "eval_crossner_science_precision": 0.8181818181780992,
      "eval_crossner_science_recall": 0.8181818181780992,
      "eval_mit-movie_f1": 0.9230769230259881,
      "eval_mit-movie_precision": 0.9374999999990234,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.8676470587722643,
      "eval_mit-restaurant_precision": 0.8805970149240588,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.8465,
      "eval_samples_per_second": 7.598,
      "eval_steps_per_second": 0.237,
      "step": 384
    },
    {
      "epoch": 1.3586237318041465,
      "grad_norm": 0.33287259262294533,
      "learning_rate": 1.612269938650307e-05,
      "loss": 0.0264,
      "step": 385
    },
    {
      "epoch": 1.3621526246140274,
      "grad_norm": 0.11750380426193635,
      "learning_rate": 1.6110429447852764e-05,
      "loss": 0.027,
      "step": 386
    },
    {
      "epoch": 1.3656815174239083,
      "grad_norm": 0.1373594869271036,
      "learning_rate": 1.6098159509202454e-05,
      "loss": 0.0221,
      "step": 387
    },
    {
      "epoch": 1.3692104102337892,
      "grad_norm": 0.1501261379443388,
      "learning_rate": 1.6085889570552148e-05,
      "loss": 0.0207,
      "step": 388
    },
    {
      "epoch": 1.37273930304367,
      "grad_norm": 0.13372356315066575,
      "learning_rate": 1.6073619631901842e-05,
      "loss": 0.0308,
      "step": 389
    },
    {
      "epoch": 1.376268195853551,
      "grad_norm": 0.09688444322275373,
      "learning_rate": 1.6061349693251536e-05,
      "loss": 0.025,
      "step": 390
    },
    {
      "epoch": 1.3797970886634319,
      "grad_norm": 0.13506342285525882,
      "learning_rate": 1.604907975460123e-05,
      "loss": 0.0301,
      "step": 391
    },
    {
      "epoch": 1.3833259814733128,
      "grad_norm": 0.19956073205620536,
      "learning_rate": 1.603680981595092e-05,
      "loss": 0.0322,
      "step": 392
    },
    {
      "epoch": 1.3833259814733128,
      "eval_average_f1": 0.7385711087924876,
      "eval_crossner_ai_f1": 0.5882352940668973,
      "eval_crossner_ai_precision": 0.714285714282313,
      "eval_crossner_ai_recall": 0.4999999999983334,
      "eval_crossner_literature_f1": 0.5454545454046488,
      "eval_crossner_literature_precision": 0.7058823529370243,
      "eval_crossner_literature_recall": 0.4444444444427984,
      "eval_crossner_music_f1": 0.8805970148742369,
      "eval_crossner_music_precision": 0.9365079365064499,
      "eval_crossner_music_recall": 0.8309859154917874,
      "eval_crossner_politics_f1": 0.720496894359199,
      "eval_crossner_politics_precision": 0.763157894735838,
      "eval_crossner_politics_recall": 0.6823529411756678,
      "eval_crossner_science_f1": 0.634146341410589,
      "eval_crossner_science_precision": 0.6842105263121885,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9222797926951919,
      "eval_mit-movie_precision": 0.9468085106372907,
      "eval_mit-movie_recall": 0.898989898988991,
      "eval_mit-restaurant_f1": 0.8787878787366505,
      "eval_mit-restaurant_precision": 0.9206349206334593,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 16.0381,
      "eval_samples_per_second": 7.981,
      "eval_steps_per_second": 0.249,
      "step": 392
    },
    {
      "epoch": 1.3868548742831937,
      "grad_norm": 0.35729513370548194,
      "learning_rate": 1.6024539877300615e-05,
      "loss": 0.0325,
      "step": 393
    },
    {
      "epoch": 1.3903837670930745,
      "grad_norm": 0.15659801945397647,
      "learning_rate": 1.601226993865031e-05,
      "loss": 0.0286,
      "step": 394
    },
    {
      "epoch": 1.3939126599029554,
      "grad_norm": 0.1759141102009049,
      "learning_rate": 1.6000000000000003e-05,
      "loss": 0.0326,
      "step": 395
    },
    {
      "epoch": 1.3974415527128363,
      "grad_norm": 0.14392256796453326,
      "learning_rate": 1.5987730061349697e-05,
      "loss": 0.0277,
      "step": 396
    },
    {
      "epoch": 1.4009704455227172,
      "grad_norm": 0.18556085835016872,
      "learning_rate": 1.5975460122699387e-05,
      "loss": 0.0177,
      "step": 397
    },
    {
      "epoch": 1.404499338332598,
      "grad_norm": 0.26348552476529935,
      "learning_rate": 1.596319018404908e-05,
      "loss": 0.0341,
      "step": 398
    },
    {
      "epoch": 1.408028231142479,
      "grad_norm": 0.15108184992448956,
      "learning_rate": 1.5950920245398772e-05,
      "loss": 0.0225,
      "step": 399
    },
    {
      "epoch": 1.4115571239523599,
      "grad_norm": 0.16803081607744838,
      "learning_rate": 1.5938650306748466e-05,
      "loss": 0.0278,
      "step": 400
    },
    {
      "epoch": 1.4115571239523599,
      "eval_average_f1": 0.7852193324834398,
      "eval_crossner_ai_f1": 0.7301587301065257,
      "eval_crossner_ai_precision": 0.6969696969675849,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9577464788218905,
      "eval_crossner_music_precision": 0.9577464788718905,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.6941176470080069,
      "eval_crossner_politics_precision": 0.6941176470580069,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.7111111110579753,
      "eval_crossner_science_precision": 0.695652173910019,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.908163265255201,
      "eval_mit-movie_precision": 0.9175257731949303,
      "eval_mit-movie_recall": 0.898989898988991,
      "eval_mit-restaurant_f1": 0.828571428520255,
      "eval_mit-restaurant_precision": 0.8169014084495536,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 16.6592,
      "eval_samples_per_second": 7.683,
      "eval_steps_per_second": 0.24,
      "step": 400
    },
    {
      "epoch": 1.4150860167622408,
      "grad_norm": 0.12634121069195048,
      "learning_rate": 1.592638036809816e-05,
      "loss": 0.0161,
      "step": 401
    },
    {
      "epoch": 1.4186149095721219,
      "grad_norm": 0.22111341436169987,
      "learning_rate": 1.5914110429447854e-05,
      "loss": 0.0357,
      "step": 402
    },
    {
      "epoch": 1.4221438023820028,
      "grad_norm": 0.2801508086953175,
      "learning_rate": 1.5901840490797548e-05,
      "loss": 0.0252,
      "step": 403
    },
    {
      "epoch": 1.4256726951918837,
      "grad_norm": 0.09434125146845802,
      "learning_rate": 1.5889570552147238e-05,
      "loss": 0.0183,
      "step": 404
    },
    {
      "epoch": 1.4292015880017646,
      "grad_norm": 0.08551030291733573,
      "learning_rate": 1.5877300613496932e-05,
      "loss": 0.0184,
      "step": 405
    },
    {
      "epoch": 1.4327304808116454,
      "grad_norm": 0.2150740342975749,
      "learning_rate": 1.5865030674846626e-05,
      "loss": 0.0235,
      "step": 406
    },
    {
      "epoch": 1.4362593736215263,
      "grad_norm": 0.13657501021550472,
      "learning_rate": 1.585276073619632e-05,
      "loss": 0.0225,
      "step": 407
    },
    {
      "epoch": 1.4397882664314072,
      "grad_norm": 0.06915439088820247,
      "learning_rate": 1.5840490797546014e-05,
      "loss": 0.0147,
      "step": 408
    },
    {
      "epoch": 1.4397882664314072,
      "eval_average_f1": 0.7647809382139714,
      "eval_crossner_ai_f1": 0.7419354838186265,
      "eval_crossner_ai_precision": 0.7187499999977539,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6153846153323226,
      "eval_crossner_literature_precision": 0.6399999999974401,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.845070422484021,
      "eval_crossner_music_precision": 0.845070422534021,
      "eval_crossner_music_recall": 0.845070422534021,
      "eval_crossner_politics_f1": 0.6820809248047178,
      "eval_crossner_politics_precision": 0.6704545454537836,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.699999999947,
      "eval_crossner_science_precision": 0.7777777777734569,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9222797926951919,
      "eval_mit-movie_precision": 0.9468085106372907,
      "eval_mit-movie_recall": 0.898989898988991,
      "eval_mit-restaurant_f1": 0.8467153284159198,
      "eval_mit-restaurant_precision": 0.8529411764693339,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 16.5824,
      "eval_samples_per_second": 7.719,
      "eval_steps_per_second": 0.241,
      "step": 408
    },
    {
      "epoch": 1.4433171592412881,
      "grad_norm": 0.11829678514069306,
      "learning_rate": 1.5828220858895708e-05,
      "loss": 0.0246,
      "step": 409
    },
    {
      "epoch": 1.446846052051169,
      "grad_norm": 0.13107242606308003,
      "learning_rate": 1.58159509202454e-05,
      "loss": 0.0175,
      "step": 410
    },
    {
      "epoch": 1.45037494486105,
      "grad_norm": 0.16531458546886063,
      "learning_rate": 1.5803680981595093e-05,
      "loss": 0.0299,
      "step": 411
    },
    {
      "epoch": 1.4539038376709308,
      "grad_norm": 0.11424816123434314,
      "learning_rate": 1.5791411042944787e-05,
      "loss": 0.0369,
      "step": 412
    },
    {
      "epoch": 1.4574327304808117,
      "grad_norm": 0.06724668310872726,
      "learning_rate": 1.577914110429448e-05,
      "loss": 0.0204,
      "step": 413
    },
    {
      "epoch": 1.4609616232906926,
      "grad_norm": 0.10797651761871524,
      "learning_rate": 1.5766871165644175e-05,
      "loss": 0.0217,
      "step": 414
    },
    {
      "epoch": 1.4644905161005735,
      "grad_norm": 0.11729683146940262,
      "learning_rate": 1.5754601226993865e-05,
      "loss": 0.0228,
      "step": 415
    },
    {
      "epoch": 1.4680194089104543,
      "grad_norm": 0.1539088149752585,
      "learning_rate": 1.574233128834356e-05,
      "loss": 0.03,
      "step": 416
    },
    {
      "epoch": 1.4680194089104543,
      "eval_average_f1": 0.741915793996468,
      "eval_crossner_ai_f1": 0.7213114753574845,
      "eval_crossner_ai_precision": 0.7096774193525495,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.5306122448463141,
      "eval_crossner_literature_precision": 0.590909090906405,
      "eval_crossner_literature_recall": 0.48148148147969827,
      "eval_crossner_music_f1": 0.6478873238927494,
      "eval_crossner_music_precision": 0.6478873239427494,
      "eval_crossner_music_recall": 0.6478873239427494,
      "eval_crossner_politics_f1": 0.7052023120879282,
      "eval_crossner_politics_precision": 0.6931818181810304,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.7999999999465,
      "eval_crossner_science_precision": 0.8888888888839507,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9270833332824164,
      "eval_mit-movie_precision": 0.9569892473107989,
      "eval_mit-movie_recall": 0.898989898988991,
      "eval_mit-restaurant_f1": 0.8613138685618839,
      "eval_mit-restaurant_precision": 0.8676470588222535,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.6723,
      "eval_samples_per_second": 7.677,
      "eval_steps_per_second": 0.24,
      "step": 416
    },
    {
      "epoch": 1.4715483017203352,
      "grad_norm": 0.13875068520471276,
      "learning_rate": 1.5730061349693253e-05,
      "loss": 0.032,
      "step": 417
    },
    {
      "epoch": 1.4750771945302161,
      "grad_norm": 0.09317814742486988,
      "learning_rate": 1.5717791411042947e-05,
      "loss": 0.0179,
      "step": 418
    },
    {
      "epoch": 1.478606087340097,
      "grad_norm": 0.1683661948502199,
      "learning_rate": 1.570552147239264e-05,
      "loss": 0.0184,
      "step": 419
    },
    {
      "epoch": 1.482134980149978,
      "grad_norm": 0.11798251210710872,
      "learning_rate": 1.569325153374233e-05,
      "loss": 0.0203,
      "step": 420
    },
    {
      "epoch": 1.4856638729598588,
      "grad_norm": 0.05868641360814007,
      "learning_rate": 1.5680981595092026e-05,
      "loss": 0.0127,
      "step": 421
    },
    {
      "epoch": 1.4891927657697397,
      "grad_norm": 0.12688660713009803,
      "learning_rate": 1.566871165644172e-05,
      "loss": 0.0266,
      "step": 422
    },
    {
      "epoch": 1.4927216585796206,
      "grad_norm": 0.07804290511275092,
      "learning_rate": 1.5656441717791414e-05,
      "loss": 0.024,
      "step": 423
    },
    {
      "epoch": 1.4962505513895015,
      "grad_norm": 0.12286566833757101,
      "learning_rate": 1.5644171779141108e-05,
      "loss": 0.0266,
      "step": 424
    },
    {
      "epoch": 1.4962505513895015,
      "eval_average_f1": 0.7441394912340581,
      "eval_crossner_ai_f1": 0.7540983606032787,
      "eval_crossner_ai_precision": 0.7419354838685744,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.5833333332816841,
      "eval_crossner_literature_precision": 0.6666666666634922,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.6901408450194505,
      "eval_crossner_music_precision": 0.6901408450694505,
      "eval_crossner_music_recall": 0.6901408450694505,
      "eval_crossner_politics_f1": 0.6900584794813583,
      "eval_crossner_politics_precision": 0.6860465116271093,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.684210526263435,
      "eval_crossner_science_precision": 0.812499999994922,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9230769230259881,
      "eval_mit-movie_precision": 0.9374999999990234,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 17.2083,
      "eval_samples_per_second": 7.438,
      "eval_steps_per_second": 0.232,
      "step": 424
    },
    {
      "epoch": 1.4997794441993824,
      "grad_norm": 0.17705952727424154,
      "learning_rate": 1.5631901840490798e-05,
      "loss": 0.0251,
      "step": 425
    },
    {
      "epoch": 1.5033083370092633,
      "grad_norm": 0.1500323350148456,
      "learning_rate": 1.5619631901840492e-05,
      "loss": 0.0238,
      "step": 426
    },
    {
      "epoch": 1.5068372298191441,
      "grad_norm": 0.3689007055115565,
      "learning_rate": 1.5607361963190186e-05,
      "loss": 0.0345,
      "step": 427
    },
    {
      "epoch": 1.510366122629025,
      "grad_norm": 0.07625042398815625,
      "learning_rate": 1.559509202453988e-05,
      "loss": 0.0166,
      "step": 428
    },
    {
      "epoch": 1.513895015438906,
      "grad_norm": 0.18515326260116366,
      "learning_rate": 1.5582822085889574e-05,
      "loss": 0.0211,
      "step": 429
    },
    {
      "epoch": 1.5174239082487868,
      "grad_norm": 0.20098889000753203,
      "learning_rate": 1.5570552147239265e-05,
      "loss": 0.034,
      "step": 430
    },
    {
      "epoch": 1.5209528010586677,
      "grad_norm": 0.11246896835675868,
      "learning_rate": 1.555828220858896e-05,
      "loss": 0.02,
      "step": 431
    },
    {
      "epoch": 1.5244816938685486,
      "grad_norm": 0.10677311669986222,
      "learning_rate": 1.554601226993865e-05,
      "loss": 0.0311,
      "step": 432
    },
    {
      "epoch": 1.5244816938685486,
      "eval_average_f1": 0.7104815072864543,
      "eval_crossner_ai_f1": 0.6874999999480469,
      "eval_crossner_ai_precision": 0.6470588235275087,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.55999999994808,
      "eval_crossner_literature_precision": 0.6086956521712665,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.6901408450194505,
      "eval_crossner_music_precision": 0.6901408450694505,
      "eval_crossner_music_recall": 0.6901408450694505,
      "eval_crossner_politics_f1": 0.5287356321333269,
      "eval_crossner_politics_precision": 0.516853932583689,
      "eval_crossner_politics_recall": 0.5411764705875987,
      "eval_crossner_science_f1": 0.7317073170198692,
      "eval_crossner_science_precision": 0.7894736842063713,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9285714285204862,
      "eval_mit-movie_precision": 0.93814432989594,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8467153284159198,
      "eval_mit-restaurant_precision": 0.8529411764693339,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 17.2031,
      "eval_samples_per_second": 7.441,
      "eval_steps_per_second": 0.233,
      "step": 432
    },
    {
      "epoch": 1.5280105866784295,
      "grad_norm": 0.1689089697408013,
      "learning_rate": 1.5533742331288343e-05,
      "loss": 0.0239,
      "step": 433
    },
    {
      "epoch": 1.5315394794883106,
      "grad_norm": 0.08857886734343712,
      "learning_rate": 1.5521472392638037e-05,
      "loss": 0.0341,
      "step": 434
    },
    {
      "epoch": 1.5350683722981915,
      "grad_norm": 0.0991783869110807,
      "learning_rate": 1.550920245398773e-05,
      "loss": 0.0314,
      "step": 435
    },
    {
      "epoch": 1.5385972651080724,
      "grad_norm": 0.13386920282935671,
      "learning_rate": 1.5496932515337425e-05,
      "loss": 0.0194,
      "step": 436
    },
    {
      "epoch": 1.5421261579179533,
      "grad_norm": 0.2699652094634782,
      "learning_rate": 1.5484662576687116e-05,
      "loss": 0.0237,
      "step": 437
    },
    {
      "epoch": 1.5456550507278342,
      "grad_norm": 0.5387490508387583,
      "learning_rate": 1.547239263803681e-05,
      "loss": 0.0202,
      "step": 438
    },
    {
      "epoch": 1.549183943537715,
      "grad_norm": 0.17111195985571012,
      "learning_rate": 1.5460122699386504e-05,
      "loss": 0.0243,
      "step": 439
    },
    {
      "epoch": 1.552712836347596,
      "grad_norm": 0.12742660699614639,
      "learning_rate": 1.5447852760736198e-05,
      "loss": 0.0301,
      "step": 440
    },
    {
      "epoch": 1.552712836347596,
      "eval_average_f1": 0.7531989611468882,
      "eval_crossner_ai_f1": 0.7187499999479492,
      "eval_crossner_ai_precision": 0.6764705882333044,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.653061224437651,
      "eval_crossner_literature_precision": 0.7272727272694215,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.6760563379772168,
      "eval_crossner_music_precision": 0.6760563380272168,
      "eval_crossner_music_recall": 0.6760563380272168,
      "eval_crossner_politics_f1": 0.7078651684886316,
      "eval_crossner_politics_precision": 0.6774193548379812,
      "eval_crossner_politics_recall": 0.7411764705873634,
      "eval_crossner_science_f1": 0.7555555555022223,
      "eval_crossner_science_precision": 0.7391304347793951,
      "eval_crossner_science_recall": 0.7727272727237604,
      "eval_mit-movie_f1": 0.908163265255201,
      "eval_mit-movie_precision": 0.9175257731949303,
      "eval_mit-movie_recall": 0.898989898988991,
      "eval_mit-restaurant_f1": 0.8529411764193447,
      "eval_mit-restaurant_precision": 0.8656716417897528,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 17.6881,
      "eval_samples_per_second": 7.237,
      "eval_steps_per_second": 0.226,
      "step": 440
    },
    {
      "epoch": 1.5562417291574768,
      "grad_norm": 0.15265020090064002,
      "learning_rate": 1.543558282208589e-05,
      "loss": 0.0242,
      "step": 441
    },
    {
      "epoch": 1.5597706219673577,
      "grad_norm": 0.1429234594513956,
      "learning_rate": 1.5423312883435585e-05,
      "loss": 0.0323,
      "step": 442
    },
    {
      "epoch": 1.5632995147772386,
      "grad_norm": 0.14678418314234312,
      "learning_rate": 1.5411042944785276e-05,
      "loss": 0.0281,
      "step": 443
    },
    {
      "epoch": 1.5668284075871195,
      "grad_norm": 0.17845347885766102,
      "learning_rate": 1.539877300613497e-05,
      "loss": 0.0256,
      "step": 444
    },
    {
      "epoch": 1.5703573003970004,
      "grad_norm": 0.20475595690838214,
      "learning_rate": 1.5386503067484664e-05,
      "loss": 0.017,
      "step": 445
    },
    {
      "epoch": 1.5738861932068815,
      "grad_norm": 0.12918706131737448,
      "learning_rate": 1.5374233128834358e-05,
      "loss": 0.0163,
      "step": 446
    },
    {
      "epoch": 1.5774150860167624,
      "grad_norm": 0.15869464577522743,
      "learning_rate": 1.5361963190184052e-05,
      "loss": 0.0272,
      "step": 447
    },
    {
      "epoch": 1.5809439788266433,
      "grad_norm": 0.20822342814778408,
      "learning_rate": 1.5349693251533743e-05,
      "loss": 0.0232,
      "step": 448
    },
    {
      "epoch": 1.5809439788266433,
      "eval_average_f1": 0.7821729524659783,
      "eval_crossner_ai_f1": 0.7936507935983874,
      "eval_crossner_ai_precision": 0.7575757575734618,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.6666666666146702,
      "eval_crossner_literature_precision": 0.7619047619011339,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9014084506529557,
      "eval_crossner_music_precision": 0.9014084507029557,
      "eval_crossner_music_recall": 0.9014084507029557,
      "eval_crossner_politics_f1": 0.6818181817674651,
      "eval_crossner_politics_precision": 0.6593406593399348,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.6818181817650827,
      "eval_crossner_science_precision": 0.6818181818150827,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.8969072164439419,
      "eval_mit-movie_precision": 0.9157894736832465,
      "eval_mit-movie_recall": 0.8787878787869912,
      "eval_mit-restaurant_f1": 0.8529411764193447,
      "eval_mit-restaurant_precision": 0.8656716417897528,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 16.5312,
      "eval_samples_per_second": 7.743,
      "eval_steps_per_second": 0.242,
      "step": 448
    },
    {
      "epoch": 1.5844728716365242,
      "grad_norm": 0.1315480090381383,
      "learning_rate": 1.5337423312883436e-05,
      "loss": 0.0253,
      "step": 449
    },
    {
      "epoch": 1.588001764446405,
      "grad_norm": 0.3039602623922531,
      "learning_rate": 1.532515337423313e-05,
      "loss": 0.0232,
      "step": 450
    },
    {
      "epoch": 1.591530657256286,
      "grad_norm": 0.1279839262388044,
      "learning_rate": 1.5312883435582824e-05,
      "loss": 0.0214,
      "step": 451
    },
    {
      "epoch": 1.5950595500661668,
      "grad_norm": 0.5191905690115441,
      "learning_rate": 1.530061349693252e-05,
      "loss": 0.0242,
      "step": 452
    },
    {
      "epoch": 1.5985884428760477,
      "grad_norm": 0.23938923554384184,
      "learning_rate": 1.528834355828221e-05,
      "loss": 0.0288,
      "step": 453
    },
    {
      "epoch": 1.6021173356859286,
      "grad_norm": 0.13458803472658046,
      "learning_rate": 1.5276073619631903e-05,
      "loss": 0.0301,
      "step": 454
    },
    {
      "epoch": 1.6056462284958095,
      "grad_norm": 0.2810208791369399,
      "learning_rate": 1.5263803680981597e-05,
      "loss": 0.0245,
      "step": 455
    },
    {
      "epoch": 1.6091751213056904,
      "grad_norm": 0.1789014769837803,
      "learning_rate": 1.5251533742331291e-05,
      "loss": 0.0211,
      "step": 456
    },
    {
      "epoch": 1.6091751213056904,
      "eval_average_f1": 0.7843380409957499,
      "eval_crossner_ai_f1": 0.7037037036517148,
      "eval_crossner_ai_precision": 0.7916666666633682,
      "eval_crossner_ai_recall": 0.6333333333312222,
      "eval_crossner_literature_f1": 0.6086956521227789,
      "eval_crossner_literature_precision": 0.7368421052592798,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.9361702127146321,
      "eval_crossner_music_precision": 0.9428571428557959,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.6904761904253756,
      "eval_crossner_politics_precision": 0.6987951807220496,
      "eval_crossner_politics_recall": 0.6823529411756678,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9693877550510569,
      "eval_mit-movie_precision": 0.9793814432979594,
      "eval_mit-movie_recall": 0.9595959595949903,
      "eval_mit-restaurant_f1": 0.8676470587722643,
      "eval_mit-restaurant_precision": 0.8805970149240588,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.3993,
      "eval_samples_per_second": 7.805,
      "eval_steps_per_second": 0.244,
      "step": 456
    },
    {
      "epoch": 1.6127040141155713,
      "grad_norm": 0.33979514201897554,
      "learning_rate": 1.5239263803680983e-05,
      "loss": 0.0215,
      "step": 457
    },
    {
      "epoch": 1.6162329069254522,
      "grad_norm": 0.24640557046529263,
      "learning_rate": 1.5226993865030677e-05,
      "loss": 0.025,
      "step": 458
    },
    {
      "epoch": 1.619761799735333,
      "grad_norm": 0.24837551723267023,
      "learning_rate": 1.5214723926380371e-05,
      "loss": 0.028,
      "step": 459
    },
    {
      "epoch": 1.623290692545214,
      "grad_norm": 0.14718229607577216,
      "learning_rate": 1.5202453987730063e-05,
      "loss": 0.0248,
      "step": 460
    },
    {
      "epoch": 1.6268195853550949,
      "grad_norm": 0.14567769137985156,
      "learning_rate": 1.5190184049079754e-05,
      "loss": 0.0293,
      "step": 461
    },
    {
      "epoch": 1.6303484781649757,
      "grad_norm": 0.15867507850572307,
      "learning_rate": 1.5177914110429448e-05,
      "loss": 0.0266,
      "step": 462
    },
    {
      "epoch": 1.6338773709748566,
      "grad_norm": 0.20593466133048335,
      "learning_rate": 1.5165644171779142e-05,
      "loss": 0.0257,
      "step": 463
    },
    {
      "epoch": 1.6374062637847375,
      "grad_norm": 0.29100973648155837,
      "learning_rate": 1.5153374233128834e-05,
      "loss": 0.0267,
      "step": 464
    },
    {
      "epoch": 1.6374062637847375,
      "eval_average_f1": 0.7152627090182421,
      "eval_crossner_ai_f1": 0.6666666666148149,
      "eval_crossner_ai_precision": 0.749999999996875,
      "eval_crossner_ai_recall": 0.599999999998,
      "eval_crossner_literature_f1": 0.4999999999503099,
      "eval_crossner_literature_precision": 0.6470588235256056,
      "eval_crossner_literature_recall": 0.40740740740589854,
      "eval_crossner_music_f1": 0.836879432572929,
      "eval_crossner_music_precision": 0.8428571428559387,
      "eval_crossner_music_recall": 0.8309859154917874,
      "eval_crossner_politics_f1": 0.6900584794813583,
      "eval_crossner_politics_precision": 0.6860465116271093,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.5714285713759637,
      "eval_crossner_science_precision": 0.599999999997,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.9387755101531289,
      "eval_mit-movie_precision": 0.9484536082464449,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8030303029791895,
      "eval_mit-restaurant_precision": 0.8412698412685059,
      "eval_mit-restaurant_recall": 0.7681159420278723,
      "eval_runtime": 16.193,
      "eval_samples_per_second": 7.905,
      "eval_steps_per_second": 0.247,
      "step": 464
    },
    {
      "epoch": 1.6409351565946184,
      "grad_norm": 0.1779285983608172,
      "learning_rate": 1.5141104294478528e-05,
      "loss": 0.0217,
      "step": 465
    },
    {
      "epoch": 1.6444640494044993,
      "grad_norm": 0.19086296479467552,
      "learning_rate": 1.5128834355828222e-05,
      "loss": 0.0296,
      "step": 466
    },
    {
      "epoch": 1.6479929422143802,
      "grad_norm": 0.12403694589895332,
      "learning_rate": 1.5116564417177914e-05,
      "loss": 0.0162,
      "step": 467
    },
    {
      "epoch": 1.651521835024261,
      "grad_norm": 0.07822605987649466,
      "learning_rate": 1.5104294478527608e-05,
      "loss": 0.014,
      "step": 468
    },
    {
      "epoch": 1.655050727834142,
      "grad_norm": 0.13193097397852507,
      "learning_rate": 1.50920245398773e-05,
      "loss": 0.0232,
      "step": 469
    },
    {
      "epoch": 1.6585796206440229,
      "grad_norm": 0.2653733772690007,
      "learning_rate": 1.5079754601226995e-05,
      "loss": 0.0355,
      "step": 470
    },
    {
      "epoch": 1.6621085134539038,
      "grad_norm": 0.11733035281590211,
      "learning_rate": 1.5067484662576689e-05,
      "loss": 0.0346,
      "step": 471
    },
    {
      "epoch": 1.6656374062637846,
      "grad_norm": 0.11517101501803102,
      "learning_rate": 1.5055214723926381e-05,
      "loss": 0.0261,
      "step": 472
    },
    {
      "epoch": 1.6656374062637846,
      "eval_average_f1": 0.7738628947017074,
      "eval_crossner_ai_f1": 0.6969696969179981,
      "eval_crossner_ai_precision": 0.6388888888871141,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6666666666141976,
      "eval_crossner_literature_precision": 0.6666666666641976,
      "eval_crossner_literature_recall": 0.6666666666641976,
      "eval_crossner_music_f1": 0.8920863308839915,
      "eval_crossner_music_precision": 0.9117647058810121,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.6900584794813583,
      "eval_crossner_politics_precision": 0.6860465116271093,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.6666666666137285,
      "eval_crossner_science_precision": 0.6521739130406428,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9538461537951873,
      "eval_mit-movie_precision": 0.9687499999989909,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8507462686054911,
      "eval_mit-restaurant_precision": 0.8769230769217278,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 29.6042,
      "eval_samples_per_second": 4.324,
      "eval_steps_per_second": 0.135,
      "step": 472
    },
    {
      "epoch": 1.6691662990736655,
      "grad_norm": 0.20444782216676996,
      "learning_rate": 1.5042944785276075e-05,
      "loss": 0.0239,
      "step": 473
    },
    {
      "epoch": 1.6726951918835464,
      "grad_norm": 0.13024932375071546,
      "learning_rate": 1.5030674846625767e-05,
      "loss": 0.0217,
      "step": 474
    },
    {
      "epoch": 1.6762240846934273,
      "grad_norm": 0.14743478176745115,
      "learning_rate": 1.5018404907975461e-05,
      "loss": 0.0214,
      "step": 475
    },
    {
      "epoch": 1.6797529775033082,
      "grad_norm": 0.23887854897653876,
      "learning_rate": 1.5006134969325155e-05,
      "loss": 0.0191,
      "step": 476
    },
    {
      "epoch": 1.683281870313189,
      "grad_norm": 0.14865241053099454,
      "learning_rate": 1.4993865030674847e-05,
      "loss": 0.0286,
      "step": 477
    },
    {
      "epoch": 1.68681076312307,
      "grad_norm": 0.17002104463292642,
      "learning_rate": 1.4981595092024541e-05,
      "loss": 0.0276,
      "step": 478
    },
    {
      "epoch": 1.6903396559329509,
      "grad_norm": 0.2647497257784179,
      "learning_rate": 1.4969325153374235e-05,
      "loss": 0.0353,
      "step": 479
    },
    {
      "epoch": 1.6938685487428318,
      "grad_norm": 0.253561548855121,
      "learning_rate": 1.4957055214723928e-05,
      "loss": 0.0186,
      "step": 480
    },
    {
      "epoch": 1.6938685487428318,
      "eval_average_f1": 0.7745517310884854,
      "eval_crossner_ai_f1": 0.7666666666141112,
      "eval_crossner_ai_precision": 0.7666666666641112,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.828571428520255,
      "eval_crossner_music_precision": 0.8405797101437092,
      "eval_crossner_music_recall": 0.8169014084495536,
      "eval_crossner_politics_f1": 0.6857142856635428,
      "eval_crossner_politics_precision": 0.6666666666659259,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9285714285204862,
      "eval_mit-movie_precision": 0.93814432989594,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8656716417397973,
      "eval_mit-restaurant_precision": 0.8923076923063195,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 29.3125,
      "eval_samples_per_second": 4.367,
      "eval_steps_per_second": 0.136,
      "step": 480
    },
    {
      "epoch": 1.6973974415527129,
      "grad_norm": 0.12404359068044161,
      "learning_rate": 1.4944785276073622e-05,
      "loss": 0.0304,
      "step": 481
    },
    {
      "epoch": 1.7009263343625938,
      "grad_norm": 0.1236692085749799,
      "learning_rate": 1.4932515337423314e-05,
      "loss": 0.0208,
      "step": 482
    },
    {
      "epoch": 1.7044552271724747,
      "grad_norm": 0.15485289742966518,
      "learning_rate": 1.4920245398773008e-05,
      "loss": 0.0128,
      "step": 483
    },
    {
      "epoch": 1.7079841199823556,
      "grad_norm": 0.09626274561712606,
      "learning_rate": 1.4907975460122702e-05,
      "loss": 0.0237,
      "step": 484
    },
    {
      "epoch": 1.7115130127922364,
      "grad_norm": 0.10584423108184435,
      "learning_rate": 1.4895705521472394e-05,
      "loss": 0.0153,
      "step": 485
    },
    {
      "epoch": 1.7150419056021173,
      "grad_norm": 0.17965728049547555,
      "learning_rate": 1.4883435582822088e-05,
      "loss": 0.0378,
      "step": 486
    },
    {
      "epoch": 1.7185707984119982,
      "grad_norm": 0.2201531232797285,
      "learning_rate": 1.487116564417178e-05,
      "loss": 0.0467,
      "step": 487
    },
    {
      "epoch": 1.7220996912218791,
      "grad_norm": 0.20252862828009743,
      "learning_rate": 1.4858895705521474e-05,
      "loss": 0.0333,
      "step": 488
    },
    {
      "epoch": 1.7220996912218791,
      "eval_average_f1": 0.7699137926250267,
      "eval_crossner_ai_f1": 0.8387096773667014,
      "eval_crossner_ai_precision": 0.8124999999974609,
      "eval_crossner_ai_recall": 0.8666666666637778,
      "eval_crossner_literature_f1": 0.5384615384095415,
      "eval_crossner_literature_precision": 0.5599999999977601,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.8201438848409295,
      "eval_crossner_music_precision": 0.8382352941164143,
      "eval_crossner_music_recall": 0.80281690140732,
      "eval_crossner_politics_f1": 0.7333333332826728,
      "eval_crossner_politics_precision": 0.6947368421045318,
      "eval_crossner_politics_recall": 0.7764705882343806,
      "eval_crossner_science_f1": 0.6666666666137285,
      "eval_crossner_science_precision": 0.6521739130406428,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9538461537951873,
      "eval_mit-movie_precision": 0.9687499999989909,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8382352940664252,
      "eval_mit-restaurant_precision": 0.8507462686554467,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 29.2808,
      "eval_samples_per_second": 4.371,
      "eval_steps_per_second": 0.137,
      "step": 488
    },
    {
      "epoch": 1.72562858403176,
      "grad_norm": 0.13644792435770264,
      "learning_rate": 1.4846625766871168e-05,
      "loss": 0.0169,
      "step": 489
    },
    {
      "epoch": 1.729157476841641,
      "grad_norm": 0.10276737680257231,
      "learning_rate": 1.483435582822086e-05,
      "loss": 0.0291,
      "step": 490
    },
    {
      "epoch": 1.7326863696515218,
      "grad_norm": 0.12115501576745212,
      "learning_rate": 1.4822085889570555e-05,
      "loss": 0.0263,
      "step": 491
    },
    {
      "epoch": 1.736215262461403,
      "grad_norm": 0.14495401797153717,
      "learning_rate": 1.4809815950920248e-05,
      "loss": 0.0202,
      "step": 492
    },
    {
      "epoch": 1.7397441552712838,
      "grad_norm": 0.4613144836217749,
      "learning_rate": 1.4797546012269939e-05,
      "loss": 0.0409,
      "step": 493
    },
    {
      "epoch": 1.7432730480811647,
      "grad_norm": 0.21794612520991224,
      "learning_rate": 1.4785276073619631e-05,
      "loss": 0.0299,
      "step": 494
    },
    {
      "epoch": 1.7468019408910456,
      "grad_norm": 0.13268449870087418,
      "learning_rate": 1.4773006134969325e-05,
      "loss": 0.0197,
      "step": 495
    },
    {
      "epoch": 1.7503308337009265,
      "grad_norm": 0.09925304518491325,
      "learning_rate": 1.476073619631902e-05,
      "loss": 0.0209,
      "step": 496
    },
    {
      "epoch": 1.7503308337009265,
      "eval_average_f1": 0.7820698590983512,
      "eval_crossner_ai_f1": 0.7499999999478516,
      "eval_crossner_ai_precision": 0.7058823529391003,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6792452829663227,
      "eval_crossner_literature_precision": 0.6923076923050296,
      "eval_crossner_literature_recall": 0.6666666666641976,
      "eval_crossner_music_f1": 0.7999999999488674,
      "eval_crossner_music_precision": 0.8115942028973745,
      "eval_crossner_music_recall": 0.7887323943650862,
      "eval_crossner_politics_f1": 0.6941176470080069,
      "eval_crossner_politics_precision": 0.6941176470580069,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.7111111110579753,
      "eval_crossner_science_precision": 0.695652173910019,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9641025640515871,
      "eval_mit-movie_precision": 0.9791666666656467,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.875912408707848,
      "eval_mit-restaurant_precision": 0.8823529411751729,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 29.6043,
      "eval_samples_per_second": 4.324,
      "eval_steps_per_second": 0.135,
      "step": 496
    },
    {
      "epoch": 1.7538597265108073,
      "grad_norm": 0.16294905920294073,
      "learning_rate": 1.4748466257668712e-05,
      "loss": 0.021,
      "step": 497
    },
    {
      "epoch": 1.7573886193206882,
      "grad_norm": 0.1492816799841189,
      "learning_rate": 1.4736196319018406e-05,
      "loss": 0.0267,
      "step": 498
    },
    {
      "epoch": 1.7609175121305691,
      "grad_norm": 0.11627249730109447,
      "learning_rate": 1.47239263803681e-05,
      "loss": 0.0225,
      "step": 499
    },
    {
      "epoch": 1.76444640494045,
      "grad_norm": 0.20524180102263842,
      "learning_rate": 1.4711656441717792e-05,
      "loss": 0.0321,
      "step": 500
    },
    {
      "epoch": 1.767975297750331,
      "grad_norm": 0.19698863836991085,
      "learning_rate": 1.4699386503067486e-05,
      "loss": 0.0275,
      "step": 501
    },
    {
      "epoch": 1.7715041905602118,
      "grad_norm": 0.06989467174256024,
      "learning_rate": 1.4687116564417178e-05,
      "loss": 0.0206,
      "step": 502
    },
    {
      "epoch": 1.7750330833700927,
      "grad_norm": 0.25896899607754753,
      "learning_rate": 1.4674846625766872e-05,
      "loss": 0.0261,
      "step": 503
    },
    {
      "epoch": 1.7785619761799736,
      "grad_norm": 0.11372191533727953,
      "learning_rate": 1.4662576687116566e-05,
      "loss": 0.0224,
      "step": 504
    },
    {
      "epoch": 1.7785619761799736,
      "eval_average_f1": 0.7342156457351398,
      "eval_crossner_ai_f1": 0.7213114753574845,
      "eval_crossner_ai_precision": 0.7096774193525495,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.7083333332811632,
      "eval_crossner_literature_precision": 0.8095238095199547,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.8428571428059489,
      "eval_crossner_music_precision": 0.8550724637668767,
      "eval_crossner_music_recall": 0.8309859154917874,
      "eval_crossner_politics_f1": 0.5730994151540099,
      "eval_crossner_politics_precision": 0.5697674418598025,
      "eval_crossner_politics_recall": 0.5764705882346159,
      "eval_crossner_science_f1": 0.5454545454020662,
      "eval_crossner_science_precision": 0.5454545454520662,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.9484536081964661,
      "eval_mit-movie_precision": 0.9684210526305596,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.7999999999488394,
      "eval_mit-restaurant_precision": 0.8181818181805784,
      "eval_mit-restaurant_recall": 0.7826086956510396,
      "eval_runtime": 41.4973,
      "eval_samples_per_second": 3.085,
      "eval_steps_per_second": 0.096,
      "step": 504
    },
    {
      "epoch": 1.7820908689898545,
      "grad_norm": 0.11246400396334207,
      "learning_rate": 1.4650306748466258e-05,
      "loss": 0.017,
      "step": 505
    },
    {
      "epoch": 1.7856197617997354,
      "grad_norm": 0.08745025408511876,
      "learning_rate": 1.4638036809815952e-05,
      "loss": 0.0162,
      "step": 506
    },
    {
      "epoch": 1.7891486546096163,
      "grad_norm": 0.10034872991737481,
      "learning_rate": 1.4625766871165644e-05,
      "loss": 0.0212,
      "step": 507
    },
    {
      "epoch": 1.7926775474194971,
      "grad_norm": 0.14942673398807288,
      "learning_rate": 1.4613496932515338e-05,
      "loss": 0.0243,
      "step": 508
    },
    {
      "epoch": 1.796206440229378,
      "grad_norm": 0.07802536989372326,
      "learning_rate": 1.4601226993865032e-05,
      "loss": 0.0213,
      "step": 509
    },
    {
      "epoch": 1.799735333039259,
      "grad_norm": 0.22750602122083344,
      "learning_rate": 1.4588957055214725e-05,
      "loss": 0.0187,
      "step": 510
    },
    {
      "epoch": 1.8032642258491398,
      "grad_norm": 0.19211999979381136,
      "learning_rate": 1.4576687116564419e-05,
      "loss": 0.0329,
      "step": 511
    },
    {
      "epoch": 1.8067931186590207,
      "grad_norm": 0.13930265258471744,
      "learning_rate": 1.4564417177914113e-05,
      "loss": 0.0233,
      "step": 512
    },
    {
      "epoch": 1.8067931186590207,
      "eval_average_f1": 0.7315097432731773,
      "eval_crossner_ai_f1": 0.6769230768712899,
      "eval_crossner_ai_precision": 0.6285714285696327,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6938775509680968,
      "eval_crossner_literature_precision": 0.7727272727237604,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.8951048950536455,
      "eval_crossner_music_precision": 0.8888888888876543,
      "eval_crossner_music_recall": 0.9014084507029557,
      "eval_crossner_politics_f1": 0.5581395348330788,
      "eval_crossner_politics_precision": 0.5517241379304003,
      "eval_crossner_politics_recall": 0.5647058823522768,
      "eval_crossner_science_f1": 0.5333333332809878,
      "eval_crossner_science_precision": 0.5217391304325142,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.9690721648974759,
      "eval_mit-movie_precision": 0.9894736842094848,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.7941176470076664,
      "eval_mit-restaurant_precision": 0.8059701492525284,
      "eval_mit-restaurant_recall": 0.7826086956510396,
      "eval_runtime": 40.3457,
      "eval_samples_per_second": 3.173,
      "eval_steps_per_second": 0.099,
      "step": 512
    },
    {
      "epoch": 1.8103220114689016,
      "grad_norm": 0.11832049946283761,
      "learning_rate": 1.4552147239263805e-05,
      "loss": 0.0189,
      "step": 513
    },
    {
      "epoch": 1.8138509042787825,
      "grad_norm": 0.1041524122339729,
      "learning_rate": 1.4539877300613499e-05,
      "loss": 0.0275,
      "step": 514
    },
    {
      "epoch": 1.8173797970886634,
      "grad_norm": 0.14290430044199504,
      "learning_rate": 1.4527607361963191e-05,
      "loss": 0.0179,
      "step": 515
    },
    {
      "epoch": 1.8209086898985443,
      "grad_norm": 0.1230474275243555,
      "learning_rate": 1.4515337423312885e-05,
      "loss": 0.0243,
      "step": 516
    },
    {
      "epoch": 1.8244375827084252,
      "grad_norm": 0.16799713604620337,
      "learning_rate": 1.4503067484662579e-05,
      "loss": 0.019,
      "step": 517
    },
    {
      "epoch": 1.827966475518306,
      "grad_norm": 0.09388346906687785,
      "learning_rate": 1.4490797546012271e-05,
      "loss": 0.0238,
      "step": 518
    },
    {
      "epoch": 1.831495368328187,
      "grad_norm": 0.11208600023203685,
      "learning_rate": 1.4478527607361965e-05,
      "loss": 0.0299,
      "step": 519
    },
    {
      "epoch": 1.8350242611380678,
      "grad_norm": 0.12032070209362572,
      "learning_rate": 1.4466257668711658e-05,
      "loss": 0.032,
      "step": 520
    },
    {
      "epoch": 1.8350242611380678,
      "eval_average_f1": 0.7586030873635881,
      "eval_crossner_ai_f1": 0.6666666666146637,
      "eval_crossner_ai_precision": 0.636363636361708,
      "eval_crossner_ai_recall": 0.6999999999976667,
      "eval_crossner_literature_f1": 0.6666666666146702,
      "eval_crossner_literature_precision": 0.7619047619011339,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9219858155515316,
      "eval_crossner_music_precision": 0.928571428570102,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.6081871344522144,
      "eval_crossner_politics_precision": 0.6046511627899945,
      "eval_crossner_politics_recall": 0.6117647058816332,
      "eval_crossner_science_f1": 0.6666666666137285,
      "eval_crossner_science_precision": 0.6521739130406428,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9333333332823879,
      "eval_mit-movie_precision": 0.9479166666656792,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8467153284159198,
      "eval_mit-restaurant_precision": 0.8529411764693339,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 16.2078,
      "eval_samples_per_second": 7.897,
      "eval_steps_per_second": 0.247,
      "step": 520
    },
    {
      "epoch": 1.8385531539479487,
      "grad_norm": 0.08603553436317447,
      "learning_rate": 1.4453987730061352e-05,
      "loss": 0.0233,
      "step": 521
    },
    {
      "epoch": 1.8420820467578296,
      "grad_norm": 0.1379708448490767,
      "learning_rate": 1.4441717791411046e-05,
      "loss": 0.0239,
      "step": 522
    },
    {
      "epoch": 1.8456109395677105,
      "grad_norm": 0.1742561941578362,
      "learning_rate": 1.4429447852760738e-05,
      "loss": 0.0279,
      "step": 523
    },
    {
      "epoch": 1.8491398323775914,
      "grad_norm": 0.12960790639071718,
      "learning_rate": 1.441717791411043e-05,
      "loss": 0.0304,
      "step": 524
    },
    {
      "epoch": 1.8526687251874723,
      "grad_norm": 0.07060383337087034,
      "learning_rate": 1.4404907975460122e-05,
      "loss": 0.0211,
      "step": 525
    },
    {
      "epoch": 1.8561976179973532,
      "grad_norm": 0.23904744456137508,
      "learning_rate": 1.4392638036809816e-05,
      "loss": 0.0314,
      "step": 526
    },
    {
      "epoch": 1.8597265108072343,
      "grad_norm": 0.12007965881743024,
      "learning_rate": 1.4380368098159509e-05,
      "loss": 0.0215,
      "step": 527
    },
    {
      "epoch": 1.8632554036171152,
      "grad_norm": 0.17188651414797532,
      "learning_rate": 1.4368098159509203e-05,
      "loss": 0.0285,
      "step": 528
    },
    {
      "epoch": 1.8632554036171152,
      "eval_average_f1": 0.7691821179326851,
      "eval_crossner_ai_f1": 0.5862068964997622,
      "eval_crossner_ai_precision": 0.6071428571406888,
      "eval_crossner_ai_recall": 0.5666666666647778,
      "eval_crossner_literature_f1": 0.6666666666141976,
      "eval_crossner_literature_precision": 0.6666666666641976,
      "eval_crossner_literature_recall": 0.6666666666641976,
      "eval_crossner_music_f1": 0.9154929576951895,
      "eval_crossner_music_precision": 0.9154929577451895,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.7167630057295332,
      "eval_crossner_politics_precision": 0.7045454545446539,
      "eval_crossner_politics_recall": 0.7294117647050242,
      "eval_crossner_science_f1": 0.6666666666137285,
      "eval_crossner_science_precision": 0.6521739130406428,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9435897435387876,
      "eval_mit-movie_precision": 0.9583333333323351,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8888888888375966,
      "eval_mit-restaurant_precision": 0.9090909090895316,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 17.279,
      "eval_samples_per_second": 7.408,
      "eval_steps_per_second": 0.231,
      "step": 528
    },
    {
      "epoch": 1.866784296426996,
      "grad_norm": 0.14512751633658222,
      "learning_rate": 1.4355828220858897e-05,
      "loss": 0.0213,
      "step": 529
    },
    {
      "epoch": 1.870313189236877,
      "grad_norm": 0.1654851741582379,
      "learning_rate": 1.4343558282208589e-05,
      "loss": 0.0204,
      "step": 530
    },
    {
      "epoch": 1.8738420820467578,
      "grad_norm": 0.1284079747245487,
      "learning_rate": 1.4331288343558283e-05,
      "loss": 0.0265,
      "step": 531
    },
    {
      "epoch": 1.8773709748566387,
      "grad_norm": 0.16795125581881393,
      "learning_rate": 1.4319018404907977e-05,
      "loss": 0.0213,
      "step": 532
    },
    {
      "epoch": 1.8808998676665196,
      "grad_norm": 0.09841082870101626,
      "learning_rate": 1.4306748466257669e-05,
      "loss": 0.0203,
      "step": 533
    },
    {
      "epoch": 1.8844287604764005,
      "grad_norm": 0.12670924162792632,
      "learning_rate": 1.4294478527607363e-05,
      "loss": 0.0298,
      "step": 534
    },
    {
      "epoch": 1.8879576532862814,
      "grad_norm": 0.1653867647858456,
      "learning_rate": 1.4282208588957055e-05,
      "loss": 0.0399,
      "step": 535
    },
    {
      "epoch": 1.8914865460961623,
      "grad_norm": 0.13471081213327765,
      "learning_rate": 1.426993865030675e-05,
      "loss": 0.0386,
      "step": 536
    },
    {
      "epoch": 1.8914865460961623,
      "eval_average_f1": 0.7968975758259781,
      "eval_crossner_ai_f1": 0.7301587301065257,
      "eval_crossner_ai_precision": 0.6969696969675849,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.8857142856630306,
      "eval_crossner_music_precision": 0.8985507246363789,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.7701149424778768,
      "eval_crossner_politics_precision": 0.7528089887631991,
      "eval_crossner_politics_recall": 0.7882352941167197,
      "eval_crossner_science_f1": 0.7555555555022223,
      "eval_crossner_science_precision": 0.7391304347793951,
      "eval_crossner_science_recall": 0.7727272727237604,
      "eval_mit-movie_f1": 0.9430051812962067,
      "eval_mit-movie_precision": 0.9680851063819488,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8270676691217593,
      "eval_mit-restaurant_precision": 0.8593749999986572,
      "eval_mit-restaurant_recall": 0.7971014492742071,
      "eval_runtime": 16.3511,
      "eval_samples_per_second": 7.828,
      "eval_steps_per_second": 0.245,
      "step": 536
    },
    {
      "epoch": 1.8950154389060432,
      "grad_norm": 0.1687592214696215,
      "learning_rate": 1.4257668711656443e-05,
      "loss": 0.0309,
      "step": 537
    },
    {
      "epoch": 1.8985443317159243,
      "grad_norm": 0.1204717180180846,
      "learning_rate": 1.4245398773006136e-05,
      "loss": 0.0204,
      "step": 538
    },
    {
      "epoch": 1.9020732245258052,
      "grad_norm": 0.1891748212920613,
      "learning_rate": 1.423312883435583e-05,
      "loss": 0.0306,
      "step": 539
    },
    {
      "epoch": 1.905602117335686,
      "grad_norm": 0.08156337311430827,
      "learning_rate": 1.4220858895705522e-05,
      "loss": 0.0211,
      "step": 540
    },
    {
      "epoch": 1.909131010145567,
      "grad_norm": 0.12741035872480544,
      "learning_rate": 1.4208588957055216e-05,
      "loss": 0.0281,
      "step": 541
    },
    {
      "epoch": 1.9126599029554479,
      "grad_norm": 0.19490473142469322,
      "learning_rate": 1.419631901840491e-05,
      "loss": 0.0236,
      "step": 542
    },
    {
      "epoch": 1.9161887957653287,
      "grad_norm": 0.3537272036186208,
      "learning_rate": 1.4184049079754602e-05,
      "loss": 0.0213,
      "step": 543
    },
    {
      "epoch": 1.9197176885752096,
      "grad_norm": 0.09669503176906444,
      "learning_rate": 1.4171779141104296e-05,
      "loss": 0.0161,
      "step": 544
    },
    {
      "epoch": 1.9197176885752096,
      "eval_average_f1": 0.7720650634006194,
      "eval_crossner_ai_f1": 0.7419354838186265,
      "eval_crossner_ai_precision": 0.7187499999977539,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.5769230768709319,
      "eval_crossner_literature_precision": 0.5999999999976,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9219858155515316,
      "eval_crossner_music_precision": 0.928571428570102,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.7356321838572267,
      "eval_crossner_politics_precision": 0.7191011235946976,
      "eval_crossner_politics_recall": 0.7529411764697024,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9230769230259881,
      "eval_mit-movie_precision": 0.9374999999990234,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.8382352940664252,
      "eval_mit-restaurant_precision": 0.8507462686554467,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 16.6701,
      "eval_samples_per_second": 7.678,
      "eval_steps_per_second": 0.24,
      "step": 544
    },
    {
      "epoch": 1.9232465813850905,
      "grad_norm": 0.08829626677240687,
      "learning_rate": 1.415950920245399e-05,
      "loss": 0.0232,
      "step": 545
    },
    {
      "epoch": 1.9267754741949714,
      "grad_norm": 0.1216325540014903,
      "learning_rate": 1.4147239263803682e-05,
      "loss": 0.0287,
      "step": 546
    },
    {
      "epoch": 1.9303043670048523,
      "grad_norm": 0.17667283075434606,
      "learning_rate": 1.4134969325153376e-05,
      "loss": 0.0302,
      "step": 547
    },
    {
      "epoch": 1.9338332598147332,
      "grad_norm": 0.2550965505490875,
      "learning_rate": 1.4122699386503069e-05,
      "loss": 0.0323,
      "step": 548
    },
    {
      "epoch": 1.937362152624614,
      "grad_norm": 0.20839252624422405,
      "learning_rate": 1.4110429447852763e-05,
      "loss": 0.0335,
      "step": 549
    },
    {
      "epoch": 1.940891045434495,
      "grad_norm": 0.28230354552712983,
      "learning_rate": 1.4098159509202456e-05,
      "loss": 0.0239,
      "step": 550
    },
    {
      "epoch": 1.9444199382443759,
      "grad_norm": 0.13770866329319023,
      "learning_rate": 1.4085889570552149e-05,
      "loss": 0.0233,
      "step": 551
    },
    {
      "epoch": 1.9479488310542568,
      "grad_norm": 0.18247123230246723,
      "learning_rate": 1.4073619631901843e-05,
      "loss": 0.0303,
      "step": 552
    },
    {
      "epoch": 1.9479488310542568,
      "eval_average_f1": 0.7988743367596435,
      "eval_crossner_ai_f1": 0.6969696969179981,
      "eval_crossner_ai_precision": 0.6388888888871141,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.7843137254372934,
      "eval_crossner_literature_precision": 0.8333333333298611,
      "eval_crossner_literature_recall": 0.7407407407379973,
      "eval_crossner_music_f1": 0.8201438848409295,
      "eval_crossner_music_precision": 0.8382352941164143,
      "eval_crossner_music_recall": 0.80281690140732,
      "eval_crossner_politics_f1": 0.7441860464607694,
      "eval_crossner_politics_precision": 0.7356321839072004,
      "eval_crossner_politics_recall": 0.7529411764697024,
      "eval_crossner_science_f1": 0.7441860464581938,
      "eval_crossner_science_precision": 0.7619047619011339,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9246231155269615,
      "eval_mit-movie_precision": 0.91999999999908,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8776978416753584,
      "eval_mit-restaurant_precision": 0.8714285714273265,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.7906,
      "eval_samples_per_second": 7.623,
      "eval_steps_per_second": 0.238,
      "step": 552
    },
    {
      "epoch": 1.9514777238641376,
      "grad_norm": 0.12411557177033858,
      "learning_rate": 1.4061349693251537e-05,
      "loss": 0.0274,
      "step": 553
    },
    {
      "epoch": 1.9550066166740185,
      "grad_norm": 0.10814222780833113,
      "learning_rate": 1.4049079754601229e-05,
      "loss": 0.0299,
      "step": 554
    },
    {
      "epoch": 1.9585355094838994,
      "grad_norm": 0.13033486337898828,
      "learning_rate": 1.403680981595092e-05,
      "loss": 0.0219,
      "step": 555
    },
    {
      "epoch": 1.9620644022937803,
      "grad_norm": 0.09806927599845784,
      "learning_rate": 1.4024539877300614e-05,
      "loss": 0.0234,
      "step": 556
    },
    {
      "epoch": 1.9655932951036612,
      "grad_norm": 0.2204664869077419,
      "learning_rate": 1.4012269938650308e-05,
      "loss": 0.0327,
      "step": 557
    },
    {
      "epoch": 1.969122187913542,
      "grad_norm": 0.14885385371011342,
      "learning_rate": 1.4e-05,
      "loss": 0.0243,
      "step": 558
    },
    {
      "epoch": 1.972651080723423,
      "grad_norm": 0.10820221539056515,
      "learning_rate": 1.3987730061349694e-05,
      "loss": 0.0149,
      "step": 559
    },
    {
      "epoch": 1.9761799735333039,
      "grad_norm": 0.16547790846906021,
      "learning_rate": 1.3975460122699388e-05,
      "loss": 0.0281,
      "step": 560
    },
    {
      "epoch": 1.9761799735333039,
      "eval_average_f1": 0.7883461425877147,
      "eval_crossner_ai_f1": 0.7457627118118931,
      "eval_crossner_ai_precision": 0.7586206896525566,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6666666666146702,
      "eval_crossner_literature_precision": 0.7619047619011339,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9078014183884311,
      "eval_crossner_music_precision": 0.9142857142844082,
      "eval_crossner_music_recall": 0.9014084507029557,
      "eval_crossner_politics_f1": 0.6857142856635428,
      "eval_crossner_politics_precision": 0.6666666666659259,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9222797926951919,
      "eval_mit-movie_precision": 0.9468085106372907,
      "eval_mit-movie_recall": 0.898989898988991,
      "eval_mit-restaurant_f1": 0.875912408707848,
      "eval_mit-restaurant_precision": 0.8823529411751729,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.4182,
      "eval_samples_per_second": 7.796,
      "eval_steps_per_second": 0.244,
      "step": 560
    },
    {
      "epoch": 1.9797088663431848,
      "grad_norm": 0.20246055023020895,
      "learning_rate": 1.396319018404908e-05,
      "loss": 0.0239,
      "step": 561
    },
    {
      "epoch": 1.9832377591530657,
      "grad_norm": 0.15177741298494785,
      "learning_rate": 1.3950920245398774e-05,
      "loss": 0.0155,
      "step": 562
    },
    {
      "epoch": 1.9867666519629466,
      "grad_norm": 0.2022979386215434,
      "learning_rate": 1.3938650306748466e-05,
      "loss": 0.03,
      "step": 563
    },
    {
      "epoch": 1.9902955447728274,
      "grad_norm": 0.07737978820533101,
      "learning_rate": 1.392638036809816e-05,
      "loss": 0.0259,
      "step": 564
    },
    {
      "epoch": 1.9938244375827083,
      "grad_norm": 0.3151912792107078,
      "learning_rate": 1.3914110429447854e-05,
      "loss": 0.0254,
      "step": 565
    },
    {
      "epoch": 1.9973533303925892,
      "grad_norm": 0.13333822430143663,
      "learning_rate": 1.3901840490797546e-05,
      "loss": 0.0275,
      "step": 566
    },
    {
      "epoch": 2.00088222320247,
      "grad_norm": 0.1384636833970303,
      "learning_rate": 1.388957055214724e-05,
      "loss": 0.0196,
      "step": 567
    },
    {
      "epoch": 2.004411116012351,
      "grad_norm": 0.06383013554608383,
      "learning_rate": 1.3877300613496933e-05,
      "loss": 0.0111,
      "step": 568
    },
    {
      "epoch": 2.004411116012351,
      "eval_average_f1": 0.7734455427150353,
      "eval_crossner_ai_f1": 0.6249999999482422,
      "eval_crossner_ai_precision": 0.5882352941159169,
      "eval_crossner_ai_recall": 0.6666666666644445,
      "eval_crossner_literature_f1": 0.6666666666141976,
      "eval_crossner_literature_precision": 0.6666666666641976,
      "eval_crossner_literature_recall": 0.6666666666641976,
      "eval_crossner_music_f1": 0.8811188810676513,
      "eval_crossner_music_precision": 0.8749999999987847,
      "eval_crossner_music_recall": 0.8873239436607221,
      "eval_crossner_politics_f1": 0.6553672315877558,
      "eval_crossner_politics_precision": 0.6304347826080103,
      "eval_crossner_politics_recall": 0.6823529411756678,
      "eval_crossner_science_f1": 0.7804878048245093,
      "eval_crossner_science_precision": 0.8421052631534627,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.958762886546971,
      "eval_mit-movie_precision": 0.9789473684200222,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8467153284159198,
      "eval_mit-restaurant_precision": 0.8529411764693339,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 16.9787,
      "eval_samples_per_second": 7.539,
      "eval_steps_per_second": 0.236,
      "step": 568
    },
    {
      "epoch": 2.007940008822232,
      "grad_norm": 0.15855496336280225,
      "learning_rate": 1.3865030674846627e-05,
      "loss": 0.0168,
      "step": 569
    },
    {
      "epoch": 2.011468901632113,
      "grad_norm": 0.11196971361787403,
      "learning_rate": 1.385276073619632e-05,
      "loss": 0.0119,
      "step": 570
    },
    {
      "epoch": 2.0149977944419937,
      "grad_norm": 0.20355637301675222,
      "learning_rate": 1.3840490797546013e-05,
      "loss": 0.0167,
      "step": 571
    },
    {
      "epoch": 2.0185266872518746,
      "grad_norm": 0.07704198409941877,
      "learning_rate": 1.3828220858895707e-05,
      "loss": 0.0135,
      "step": 572
    },
    {
      "epoch": 2.0220555800617555,
      "grad_norm": 0.09748654879497132,
      "learning_rate": 1.3815950920245401e-05,
      "loss": 0.0159,
      "step": 573
    },
    {
      "epoch": 2.0255844728716363,
      "grad_norm": 0.0799631334039627,
      "learning_rate": 1.3803680981595093e-05,
      "loss": 0.0149,
      "step": 574
    },
    {
      "epoch": 2.0291133656815172,
      "grad_norm": 0.12092866334864089,
      "learning_rate": 1.3791411042944787e-05,
      "loss": 0.0135,
      "step": 575
    },
    {
      "epoch": 2.032642258491398,
      "grad_norm": 0.12902824091480306,
      "learning_rate": 1.377914110429448e-05,
      "loss": 0.0137,
      "step": 576
    },
    {
      "epoch": 2.032642258491398,
      "eval_average_f1": 0.804398142073668,
      "eval_crossner_ai_f1": 0.7619047618524566,
      "eval_crossner_ai_precision": 0.7272727272705234,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9219858155515316,
      "eval_crossner_music_precision": 0.928571428570102,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.6703910614018912,
      "eval_crossner_politics_precision": 0.6382978723397464,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.8292682926291495,
      "eval_crossner_science_precision": 0.8947368421005542,
      "eval_crossner_science_recall": 0.7727272727237604,
      "eval_mit-movie_f1": 0.9484536081964661,
      "eval_mit-movie_precision": 0.9684210526305596,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8321167882699557,
      "eval_mit-restaurant_precision": 0.8382352941164143,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 17.2498,
      "eval_samples_per_second": 7.42,
      "eval_steps_per_second": 0.232,
      "step": 576
    },
    {
      "epoch": 2.036171151301279,
      "grad_norm": 0.28628956273550893,
      "learning_rate": 1.3766871165644173e-05,
      "loss": 0.0171,
      "step": 577
    },
    {
      "epoch": 2.03970004411116,
      "grad_norm": 0.15590058835886936,
      "learning_rate": 1.3754601226993867e-05,
      "loss": 0.0127,
      "step": 578
    },
    {
      "epoch": 2.0432289369210412,
      "grad_norm": 0.06815639059182577,
      "learning_rate": 1.374233128834356e-05,
      "loss": 0.0187,
      "step": 579
    },
    {
      "epoch": 2.046757829730922,
      "grad_norm": 0.246989270817013,
      "learning_rate": 1.3730061349693254e-05,
      "loss": 0.0288,
      "step": 580
    },
    {
      "epoch": 2.050286722540803,
      "grad_norm": 0.12407692934629176,
      "learning_rate": 1.3717791411042946e-05,
      "loss": 0.0207,
      "step": 581
    },
    {
      "epoch": 2.053815615350684,
      "grad_norm": 0.12370988679156004,
      "learning_rate": 1.370552147239264e-05,
      "loss": 0.0116,
      "step": 582
    },
    {
      "epoch": 2.057344508160565,
      "grad_norm": 0.1938427172591345,
      "learning_rate": 1.3693251533742334e-05,
      "loss": 0.0197,
      "step": 583
    },
    {
      "epoch": 2.0608734009704457,
      "grad_norm": 0.10606244113321418,
      "learning_rate": 1.3680981595092026e-05,
      "loss": 0.0152,
      "step": 584
    },
    {
      "epoch": 2.0608734009704457,
      "eval_average_f1": 0.8038337049358157,
      "eval_crossner_ai_f1": 0.8064516128506763,
      "eval_crossner_ai_precision": 0.7812499999975585,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.653846153793713,
      "eval_crossner_literature_precision": 0.67999999999728,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9154929576951895,
      "eval_crossner_music_precision": 0.9154929577451895,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.7272727272219589,
      "eval_crossner_politics_precision": 0.7032967032959304,
      "eval_crossner_politics_recall": 0.7529411764697024,
      "eval_crossner_science_f1": 0.8292682926291495,
      "eval_crossner_science_precision": 0.8947368421005542,
      "eval_crossner_science_recall": 0.7727272727237604,
      "eval_mit-movie_f1": 0.9175257731449517,
      "eval_mit-movie_precision": 0.9368421052621717,
      "eval_mit-movie_recall": 0.898989898988991,
      "eval_mit-restaurant_f1": 0.7769784172150715,
      "eval_mit-restaurant_precision": 0.7714285714274693,
      "eval_mit-restaurant_recall": 0.7826086956510396,
      "eval_runtime": 16.99,
      "eval_samples_per_second": 7.534,
      "eval_steps_per_second": 0.235,
      "step": 584
    },
    {
      "epoch": 2.0644022937803266,
      "grad_norm": 0.09490124827630209,
      "learning_rate": 1.366871165644172e-05,
      "loss": 0.0103,
      "step": 585
    },
    {
      "epoch": 2.0679311865902075,
      "grad_norm": 0.1480663515731656,
      "learning_rate": 1.3656441717791414e-05,
      "loss": 0.0248,
      "step": 586
    },
    {
      "epoch": 2.0714600794000884,
      "grad_norm": 0.14832395050313102,
      "learning_rate": 1.3644171779141105e-05,
      "loss": 0.0179,
      "step": 587
    },
    {
      "epoch": 2.0749889722099693,
      "grad_norm": 0.20062024678336515,
      "learning_rate": 1.3631901840490797e-05,
      "loss": 0.0266,
      "step": 588
    },
    {
      "epoch": 2.07851786501985,
      "grad_norm": 0.13935749049815288,
      "learning_rate": 1.3619631901840491e-05,
      "loss": 0.0161,
      "step": 589
    },
    {
      "epoch": 2.082046757829731,
      "grad_norm": 0.10272105279272956,
      "learning_rate": 1.3607361963190185e-05,
      "loss": 0.0139,
      "step": 590
    },
    {
      "epoch": 2.085575650639612,
      "grad_norm": 0.46908122122532997,
      "learning_rate": 1.3595092024539877e-05,
      "loss": 0.0167,
      "step": 591
    },
    {
      "epoch": 2.089104543449493,
      "grad_norm": 0.16262189438161367,
      "learning_rate": 1.3582822085889571e-05,
      "loss": 0.0167,
      "step": 592
    },
    {
      "epoch": 2.089104543449493,
      "eval_average_f1": 0.7814741051643231,
      "eval_crossner_ai_f1": 0.6999999999476667,
      "eval_crossner_ai_precision": 0.6999999999976667,
      "eval_crossner_ai_recall": 0.6999999999976667,
      "eval_crossner_literature_f1": 0.5957446807996378,
      "eval_crossner_literature_precision": 0.6999999999965001,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.8873239436107221,
      "eval_crossner_music_precision": 0.8873239436607221,
      "eval_crossner_music_recall": 0.8873239436607221,
      "eval_crossner_politics_f1": 0.7542857142348931,
      "eval_crossner_politics_precision": 0.7333333333325185,
      "eval_crossner_politics_recall": 0.7764705882343806,
      "eval_crossner_science_f1": 0.7692307691776463,
      "eval_crossner_science_precision": 0.8823529411712804,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9435897435387876,
      "eval_mit-movie_precision": 0.9583333333323351,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8201438848409087,
      "eval_mit-restaurant_precision": 0.814285714284551,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 16.7392,
      "eval_samples_per_second": 7.647,
      "eval_steps_per_second": 0.239,
      "step": 592
    },
    {
      "epoch": 2.0926334362593737,
      "grad_norm": 0.10507439248642068,
      "learning_rate": 1.3570552147239265e-05,
      "loss": 0.0135,
      "step": 593
    },
    {
      "epoch": 2.0961623290692546,
      "grad_norm": 0.11484851729949686,
      "learning_rate": 1.3558282208588957e-05,
      "loss": 0.0168,
      "step": 594
    },
    {
      "epoch": 2.0996912218791355,
      "grad_norm": 0.12489360886155244,
      "learning_rate": 1.3546012269938651e-05,
      "loss": 0.0166,
      "step": 595
    },
    {
      "epoch": 2.1032201146890164,
      "grad_norm": 0.1613779457511903,
      "learning_rate": 1.3533742331288344e-05,
      "loss": 0.0193,
      "step": 596
    },
    {
      "epoch": 2.1067490074988973,
      "grad_norm": 0.08908297466538781,
      "learning_rate": 1.3521472392638038e-05,
      "loss": 0.0139,
      "step": 597
    },
    {
      "epoch": 2.110277900308778,
      "grad_norm": 0.1202112465389209,
      "learning_rate": 1.3509202453987732e-05,
      "loss": 0.012,
      "step": 598
    },
    {
      "epoch": 2.113806793118659,
      "grad_norm": 0.07577996424931188,
      "learning_rate": 1.3496932515337424e-05,
      "loss": 0.0197,
      "step": 599
    },
    {
      "epoch": 2.11733568592854,
      "grad_norm": 0.1194707019016826,
      "learning_rate": 1.3484662576687118e-05,
      "loss": 0.024,
      "step": 600
    },
    {
      "epoch": 2.11733568592854,
      "eval_average_f1": 0.8038678289875004,
      "eval_crossner_ai_f1": 0.7301587301065257,
      "eval_crossner_ai_precision": 0.6969696969675849,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.63999999994776,
      "eval_crossner_literature_precision": 0.695652173910019,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.8794326240622302,
      "eval_crossner_music_precision": 0.8857142857130204,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.757062146841878,
      "eval_crossner_politics_precision": 0.7282608695644258,
      "eval_crossner_politics_recall": 0.7882352941167197,
      "eval_crossner_science_f1": 0.8292682926291495,
      "eval_crossner_science_precision": 0.8947368421005542,
      "eval_crossner_science_recall": 0.7727272727237604,
      "eval_mit-movie_f1": 0.9340101522333171,
      "eval_mit-movie_precision": 0.9387755102031237,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8571428570916427,
      "eval_mit-restaurant_precision": 0.845070422534021,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.8983,
      "eval_samples_per_second": 7.575,
      "eval_steps_per_second": 0.237,
      "step": 600
    },
    {
      "epoch": 2.120864578738421,
      "grad_norm": 0.11438209344280795,
      "learning_rate": 1.347239263803681e-05,
      "loss": 0.0142,
      "step": 601
    },
    {
      "epoch": 2.1243934715483017,
      "grad_norm": 0.10724168750698906,
      "learning_rate": 1.3460122699386504e-05,
      "loss": 0.0143,
      "step": 602
    },
    {
      "epoch": 2.1279223643581826,
      "grad_norm": 0.14365575437561934,
      "learning_rate": 1.3447852760736198e-05,
      "loss": 0.0142,
      "step": 603
    },
    {
      "epoch": 2.1314512571680635,
      "grad_norm": 0.17542869923495236,
      "learning_rate": 1.343558282208589e-05,
      "loss": 0.013,
      "step": 604
    },
    {
      "epoch": 2.1349801499779444,
      "grad_norm": 0.12369159548486124,
      "learning_rate": 1.3423312883435584e-05,
      "loss": 0.0113,
      "step": 605
    },
    {
      "epoch": 2.1385090427878253,
      "grad_norm": 0.13141212866398755,
      "learning_rate": 1.3411042944785278e-05,
      "loss": 0.0132,
      "step": 606
    },
    {
      "epoch": 2.142037935597706,
      "grad_norm": 0.37650272163987997,
      "learning_rate": 1.339877300613497e-05,
      "loss": 0.0141,
      "step": 607
    },
    {
      "epoch": 2.145566828407587,
      "grad_norm": 0.17279090463589702,
      "learning_rate": 1.3386503067484664e-05,
      "loss": 0.0226,
      "step": 608
    },
    {
      "epoch": 2.145566828407587,
      "eval_average_f1": 0.7800846748869911,
      "eval_crossner_ai_f1": 0.7419354838186265,
      "eval_crossner_ai_precision": 0.7187499999977539,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.5714285713767597,
      "eval_crossner_literature_precision": 0.6363636363607439,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.9154929576951895,
      "eval_crossner_music_precision": 0.9154929577451895,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.7325581394840387,
      "eval_crossner_politics_precision": 0.7241379310336504,
      "eval_crossner_politics_recall": 0.7411764705873634,
      "eval_crossner_science_f1": 0.6829268292152292,
      "eval_crossner_science_precision": 0.7368421052592798,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9191919191409907,
      "eval_mit-movie_precision": 0.9191919191909907,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8970588234781033,
      "eval_mit-restaurant_precision": 0.9104477611926709,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 28.7412,
      "eval_samples_per_second": 4.454,
      "eval_steps_per_second": 0.139,
      "step": 608
    },
    {
      "epoch": 2.149095721217468,
      "grad_norm": 0.16650883955373183,
      "learning_rate": 1.3374233128834357e-05,
      "loss": 0.0141,
      "step": 609
    },
    {
      "epoch": 2.152624614027349,
      "grad_norm": 0.14500319263626607,
      "learning_rate": 1.336196319018405e-05,
      "loss": 0.022,
      "step": 610
    },
    {
      "epoch": 2.1561535068372297,
      "grad_norm": 0.14176239876622004,
      "learning_rate": 1.3349693251533745e-05,
      "loss": 0.0202,
      "step": 611
    },
    {
      "epoch": 2.1596823996471106,
      "grad_norm": 0.15497776887269052,
      "learning_rate": 1.3337423312883437e-05,
      "loss": 0.0227,
      "step": 612
    },
    {
      "epoch": 2.1632112924569915,
      "grad_norm": 0.12778195740235043,
      "learning_rate": 1.3325153374233131e-05,
      "loss": 0.0157,
      "step": 613
    },
    {
      "epoch": 2.1667401852668724,
      "grad_norm": 0.20452649540106782,
      "learning_rate": 1.3312883435582823e-05,
      "loss": 0.0217,
      "step": 614
    },
    {
      "epoch": 2.1702690780767533,
      "grad_norm": 0.12515897625252945,
      "learning_rate": 1.3300613496932517e-05,
      "loss": 0.0157,
      "step": 615
    },
    {
      "epoch": 2.173797970886634,
      "grad_norm": 0.12411970997661036,
      "learning_rate": 1.3288343558282211e-05,
      "loss": 0.0075,
      "step": 616
    },
    {
      "epoch": 2.173797970886634,
      "eval_average_f1": 0.7774346350702618,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6666666666146702,
      "eval_crossner_literature_precision": 0.7619047619011339,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9064748200926039,
      "eval_crossner_music_precision": 0.9264705882339317,
      "eval_crossner_music_recall": 0.8873239436607221,
      "eval_crossner_politics_f1": 0.7471264367307767,
      "eval_crossner_politics_precision": 0.7303370786508647,
      "eval_crossner_politics_recall": 0.7647058823520415,
      "eval_crossner_science_f1": 0.648648648596932,
      "eval_crossner_science_precision": 0.7999999999946666,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.9137055837054188,
      "eval_mit-movie_precision": 0.9183673469378384,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.8260869564705419,
      "eval_mit-restaurant_precision": 0.8260869565205419,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 28.7634,
      "eval_samples_per_second": 4.45,
      "eval_steps_per_second": 0.139,
      "step": 616
    },
    {
      "epoch": 2.177326863696515,
      "grad_norm": 0.2394636242106489,
      "learning_rate": 1.3276073619631903e-05,
      "loss": 0.0177,
      "step": 617
    },
    {
      "epoch": 2.180855756506396,
      "grad_norm": 0.12118600207751182,
      "learning_rate": 1.3263803680981596e-05,
      "loss": 0.014,
      "step": 618
    },
    {
      "epoch": 2.184384649316277,
      "grad_norm": 0.09157707907934164,
      "learning_rate": 1.3251533742331288e-05,
      "loss": 0.0166,
      "step": 619
    },
    {
      "epoch": 2.1879135421261577,
      "grad_norm": 0.18527223439060228,
      "learning_rate": 1.3239263803680982e-05,
      "loss": 0.0315,
      "step": 620
    },
    {
      "epoch": 2.1914424349360386,
      "grad_norm": 0.08971943755170401,
      "learning_rate": 1.3226993865030674e-05,
      "loss": 0.0182,
      "step": 621
    },
    {
      "epoch": 2.1949713277459195,
      "grad_norm": 0.06715238951214994,
      "learning_rate": 1.3214723926380368e-05,
      "loss": 0.0105,
      "step": 622
    },
    {
      "epoch": 2.1985002205558004,
      "grad_norm": 0.042962924605741686,
      "learning_rate": 1.3202453987730062e-05,
      "loss": 0.0094,
      "step": 623
    },
    {
      "epoch": 2.2020291133656817,
      "grad_norm": 0.0786697765191691,
      "learning_rate": 1.3190184049079754e-05,
      "loss": 0.0199,
      "step": 624
    },
    {
      "epoch": 2.2020291133656817,
      "eval_average_f1": 0.7494485987249346,
      "eval_crossner_ai_f1": 0.7213114753574845,
      "eval_crossner_ai_precision": 0.7096774193525495,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.5833333332816841,
      "eval_crossner_literature_precision": 0.6666666666634922,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.9014084506529557,
      "eval_crossner_music_precision": 0.9014084507029557,
      "eval_crossner_music_recall": 0.9014084507029557,
      "eval_crossner_politics_f1": 0.5599999999494009,
      "eval_crossner_politics_precision": 0.5444444444438395,
      "eval_crossner_politics_recall": 0.5764705882346159,
      "eval_crossner_science_f1": 0.7179487178958581,
      "eval_crossner_science_precision": 0.8235294117598617,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9154228855212394,
      "eval_mit-movie_precision": 0.9019607843128412,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8467153284159198,
      "eval_mit-restaurant_precision": 0.8529411764693339,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 28.7161,
      "eval_samples_per_second": 4.457,
      "eval_steps_per_second": 0.139,
      "step": 624
    },
    {
      "epoch": 2.205558006175562,
      "grad_norm": 0.09642873812432741,
      "learning_rate": 1.3177914110429448e-05,
      "loss": 0.0184,
      "step": 625
    },
    {
      "epoch": 2.2090868989854435,
      "grad_norm": 0.10862365729343851,
      "learning_rate": 1.3165644171779142e-05,
      "loss": 0.0096,
      "step": 626
    },
    {
      "epoch": 2.2126157917953244,
      "grad_norm": 0.09693401773157502,
      "learning_rate": 1.3153374233128835e-05,
      "loss": 0.0105,
      "step": 627
    },
    {
      "epoch": 2.2161446846052053,
      "grad_norm": 0.09293762034470616,
      "learning_rate": 1.3141104294478529e-05,
      "loss": 0.0138,
      "step": 628
    },
    {
      "epoch": 2.219673577415086,
      "grad_norm": 0.11054866624196447,
      "learning_rate": 1.3128834355828221e-05,
      "loss": 0.0241,
      "step": 629
    },
    {
      "epoch": 2.223202470224967,
      "grad_norm": 0.11495782512893743,
      "learning_rate": 1.3116564417177915e-05,
      "loss": 0.0129,
      "step": 630
    },
    {
      "epoch": 2.226731363034848,
      "grad_norm": 0.07611512858062953,
      "learning_rate": 1.3104294478527609e-05,
      "loss": 0.014,
      "step": 631
    },
    {
      "epoch": 2.230260255844729,
      "grad_norm": 0.10998265033415096,
      "learning_rate": 1.3092024539877301e-05,
      "loss": 0.0129,
      "step": 632
    },
    {
      "epoch": 2.230260255844729,
      "eval_average_f1": 0.7995329677760025,
      "eval_crossner_ai_f1": 0.8064516128506763,
      "eval_crossner_ai_precision": 0.7812499999975585,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.653061224437651,
      "eval_crossner_literature_precision": 0.7272727272694215,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.8811188810676513,
      "eval_crossner_music_precision": 0.8749999999987847,
      "eval_crossner_music_recall": 0.8873239436607221,
      "eval_crossner_politics_f1": 0.6857142856635428,
      "eval_crossner_politics_precision": 0.6666666666659259,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.7999999999465,
      "eval_crossner_science_precision": 0.8888888888839507,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.8944723617581476,
      "eval_mit-movie_precision": 0.88999999999911,
      "eval_mit-movie_recall": 0.898989898988991,
      "eval_mit-restaurant_f1": 0.875912408707848,
      "eval_mit-restaurant_precision": 0.8823529411751729,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 28.7798,
      "eval_samples_per_second": 4.448,
      "eval_steps_per_second": 0.139,
      "step": 632
    },
    {
      "epoch": 2.2337891486546098,
      "grad_norm": 0.2800623002097352,
      "learning_rate": 1.3079754601226995e-05,
      "loss": 0.0206,
      "step": 633
    },
    {
      "epoch": 2.2373180414644906,
      "grad_norm": 0.07498255889475447,
      "learning_rate": 1.3067484662576687e-05,
      "loss": 0.0136,
      "step": 634
    },
    {
      "epoch": 2.2408469342743715,
      "grad_norm": 0.15567216720446422,
      "learning_rate": 1.3055214723926381e-05,
      "loss": 0.0196,
      "step": 635
    },
    {
      "epoch": 2.2443758270842524,
      "grad_norm": 0.1463285443818803,
      "learning_rate": 1.3042944785276075e-05,
      "loss": 0.0128,
      "step": 636
    },
    {
      "epoch": 2.2479047198941333,
      "grad_norm": 0.18040497218488444,
      "learning_rate": 1.3030674846625768e-05,
      "loss": 0.0155,
      "step": 637
    },
    {
      "epoch": 2.251433612704014,
      "grad_norm": 0.14855708776075122,
      "learning_rate": 1.3018404907975462e-05,
      "loss": 0.0128,
      "step": 638
    },
    {
      "epoch": 2.254962505513895,
      "grad_norm": 0.14373213641462357,
      "learning_rate": 1.3006134969325156e-05,
      "loss": 0.0133,
      "step": 639
    },
    {
      "epoch": 2.258491398323776,
      "grad_norm": 0.21906719031325997,
      "learning_rate": 1.2993865030674848e-05,
      "loss": 0.026,
      "step": 640
    },
    {
      "epoch": 2.258491398323776,
      "eval_average_f1": 0.7990686787884551,
      "eval_crossner_ai_f1": 0.7741935483346514,
      "eval_crossner_ai_precision": 0.7499999999976562,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6122448979072054,
      "eval_crossner_literature_precision": 0.6818181818150827,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.8732394365684885,
      "eval_crossner_music_precision": 0.8732394366184884,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.7630057802959538,
      "eval_crossner_politics_precision": 0.7499999999991477,
      "eval_crossner_politics_recall": 0.7764705882343806,
      "eval_crossner_science_f1": 0.8205128204594346,
      "eval_crossner_science_precision": 0.941176470582699,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.8743718592456049,
      "eval_mit-movie_precision": 0.86999999999913,
      "eval_mit-movie_recall": 0.8787878787869912,
      "eval_mit-restaurant_f1": 0.875912408707848,
      "eval_mit-restaurant_precision": 0.8823529411751729,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 28.5074,
      "eval_samples_per_second": 4.49,
      "eval_steps_per_second": 0.14,
      "step": 640
    },
    {
      "epoch": 2.262020291133657,
      "grad_norm": 0.16006315828789025,
      "learning_rate": 1.2981595092024542e-05,
      "loss": 0.0165,
      "step": 641
    },
    {
      "epoch": 2.2655491839435378,
      "grad_norm": 0.2805950034704398,
      "learning_rate": 1.2969325153374234e-05,
      "loss": 0.0197,
      "step": 642
    },
    {
      "epoch": 2.2690780767534187,
      "grad_norm": 0.2881210435070208,
      "learning_rate": 1.2957055214723928e-05,
      "loss": 0.0156,
      "step": 643
    },
    {
      "epoch": 2.2726069695632996,
      "grad_norm": 0.16086135308159694,
      "learning_rate": 1.2944785276073622e-05,
      "loss": 0.01,
      "step": 644
    },
    {
      "epoch": 2.2761358623731804,
      "grad_norm": 0.09460493518356462,
      "learning_rate": 1.2932515337423314e-05,
      "loss": 0.0195,
      "step": 645
    },
    {
      "epoch": 2.2796647551830613,
      "grad_norm": 0.3743103560295368,
      "learning_rate": 1.2920245398773008e-05,
      "loss": 0.0176,
      "step": 646
    },
    {
      "epoch": 2.283193647992942,
      "grad_norm": 0.10555661038692908,
      "learning_rate": 1.29079754601227e-05,
      "loss": 0.0142,
      "step": 647
    },
    {
      "epoch": 2.286722540802823,
      "grad_norm": 0.1889737247873916,
      "learning_rate": 1.2895705521472395e-05,
      "loss": 0.0149,
      "step": 648
    },
    {
      "epoch": 2.286722540802823,
      "eval_average_f1": 0.8025448041498902,
      "eval_crossner_ai_f1": 0.7666666666141112,
      "eval_crossner_ai_precision": 0.7666666666641112,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.63999999994776,
      "eval_crossner_literature_precision": 0.695652173910019,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9230769230256345,
      "eval_crossner_music_precision": 0.9166666666653935,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.7398843930127434,
      "eval_crossner_politics_precision": 0.7272727272719008,
      "eval_crossner_politics_recall": 0.7529411764697024,
      "eval_crossner_science_f1": 0.7804878048245093,
      "eval_crossner_science_precision": 0.8421052631534627,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.889999999949115,
      "eval_mit-movie_precision": 0.8811881188110087,
      "eval_mit-movie_recall": 0.898989898988991,
      "eval_mit-restaurant_f1": 0.8776978416753584,
      "eval_mit-restaurant_precision": 0.8714285714273265,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 28.3963,
      "eval_samples_per_second": 4.508,
      "eval_steps_per_second": 0.141,
      "step": 648
    },
    {
      "epoch": 2.290251433612704,
      "grad_norm": 0.10907220651066732,
      "learning_rate": 1.2883435582822085e-05,
      "loss": 0.0105,
      "step": 649
    },
    {
      "epoch": 2.293780326422585,
      "grad_norm": 0.12463792874357475,
      "learning_rate": 1.2871165644171779e-05,
      "loss": 0.0157,
      "step": 650
    },
    {
      "epoch": 2.297309219232466,
      "grad_norm": 0.13015288046359794,
      "learning_rate": 1.2858895705521473e-05,
      "loss": 0.0128,
      "step": 651
    },
    {
      "epoch": 2.3008381120423467,
      "grad_norm": 0.10097793932332093,
      "learning_rate": 1.2846625766871165e-05,
      "loss": 0.013,
      "step": 652
    },
    {
      "epoch": 2.3043670048522276,
      "grad_norm": 0.11583683159860618,
      "learning_rate": 1.283435582822086e-05,
      "loss": 0.0149,
      "step": 653
    },
    {
      "epoch": 2.3078958976621085,
      "grad_norm": 0.08042554389836697,
      "learning_rate": 1.2822085889570552e-05,
      "loss": 0.0144,
      "step": 654
    },
    {
      "epoch": 2.3114247904719893,
      "grad_norm": 0.12872820898939571,
      "learning_rate": 1.2809815950920246e-05,
      "loss": 0.0168,
      "step": 655
    },
    {
      "epoch": 2.3149536832818702,
      "grad_norm": 0.14162765131057,
      "learning_rate": 1.279754601226994e-05,
      "loss": 0.0251,
      "step": 656
    },
    {
      "epoch": 2.3149536832818702,
      "eval_average_f1": 0.7784214157306166,
      "eval_crossner_ai_f1": 0.7868852458490728,
      "eval_crossner_ai_precision": 0.7741935483845994,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.59999999994792,
      "eval_crossner_literature_precision": 0.6521739130406428,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9230769230256345,
      "eval_crossner_music_precision": 0.9166666666653935,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.7398843930127434,
      "eval_crossner_politics_precision": 0.7272727272719008,
      "eval_crossner_politics_recall": 0.7529411764697024,
      "eval_crossner_science_f1": 0.6315789473163436,
      "eval_crossner_science_precision": 0.7499999999953126,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.8979591836225583,
      "eval_mit-movie_precision": 0.9072164948444256,
      "eval_mit-movie_recall": 0.888888888887991,
      "eval_mit-restaurant_f1": 0.8695652173400441,
      "eval_mit-restaurant_precision": 0.8695652173900441,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 28.3952,
      "eval_samples_per_second": 4.508,
      "eval_steps_per_second": 0.141,
      "step": 656
    },
    {
      "epoch": 2.318482576091751,
      "grad_norm": 0.14780078656035048,
      "learning_rate": 1.2785276073619632e-05,
      "loss": 0.012,
      "step": 657
    },
    {
      "epoch": 2.322011468901632,
      "grad_norm": 0.09441415651795737,
      "learning_rate": 1.2773006134969326e-05,
      "loss": 0.0163,
      "step": 658
    },
    {
      "epoch": 2.325540361711513,
      "grad_norm": 0.18245538897888916,
      "learning_rate": 1.276073619631902e-05,
      "loss": 0.0172,
      "step": 659
    },
    {
      "epoch": 2.329069254521394,
      "grad_norm": 0.11976138583952955,
      "learning_rate": 1.2748466257668712e-05,
      "loss": 0.0258,
      "step": 660
    },
    {
      "epoch": 2.3325981473312747,
      "grad_norm": 0.1281750996844479,
      "learning_rate": 1.2736196319018406e-05,
      "loss": 0.0174,
      "step": 661
    },
    {
      "epoch": 2.3361270401411556,
      "grad_norm": 0.14960696664171522,
      "learning_rate": 1.2723926380368098e-05,
      "loss": 0.0113,
      "step": 662
    },
    {
      "epoch": 2.3396559329510365,
      "grad_norm": 0.21903240333141363,
      "learning_rate": 1.2711656441717792e-05,
      "loss": 0.0193,
      "step": 663
    },
    {
      "epoch": 2.3431848257609174,
      "grad_norm": 0.06573158333900517,
      "learning_rate": 1.2699386503067486e-05,
      "loss": 0.0113,
      "step": 664
    },
    {
      "epoch": 2.3431848257609174,
      "eval_average_f1": 0.78389259718001,
      "eval_crossner_ai_f1": 0.7096774193026015,
      "eval_crossner_ai_precision": 0.6874999999978515,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6785714285190688,
      "eval_crossner_literature_precision": 0.6551724137908442,
      "eval_crossner_literature_recall": 0.7037037037010975,
      "eval_crossner_music_f1": 0.9295774647374231,
      "eval_crossner_music_precision": 0.9295774647874231,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.693641618446323,
      "eval_crossner_politics_precision": 0.681818181817407,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.7441860464581938,
      "eval_crossner_science_precision": 0.7619047619011339,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.919999999949085,
      "eval_mit-movie_precision": 0.910891089108009,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8115942028473745,
      "eval_mit-restaurant_precision": 0.8115942028973745,
      "eval_mit-restaurant_recall": 0.8115942028973745,
      "eval_runtime": 28.956,
      "eval_samples_per_second": 4.421,
      "eval_steps_per_second": 0.138,
      "step": 664
    },
    {
      "epoch": 2.3467137185707982,
      "grad_norm": 0.14450217289594358,
      "learning_rate": 1.2687116564417179e-05,
      "loss": 0.0137,
      "step": 665
    },
    {
      "epoch": 2.350242611380679,
      "grad_norm": 0.2073249892347612,
      "learning_rate": 1.2674846625766873e-05,
      "loss": 0.0176,
      "step": 666
    },
    {
      "epoch": 2.35377150419056,
      "grad_norm": 0.1489589154014241,
      "learning_rate": 1.2662576687116565e-05,
      "loss": 0.0212,
      "step": 667
    },
    {
      "epoch": 2.357300397000441,
      "grad_norm": 0.08103510033970336,
      "learning_rate": 1.2650306748466259e-05,
      "loss": 0.0133,
      "step": 668
    },
    {
      "epoch": 2.3608292898103223,
      "grad_norm": 0.09007601155682625,
      "learning_rate": 1.2638036809815953e-05,
      "loss": 0.0187,
      "step": 669
    },
    {
      "epoch": 2.3643581826202027,
      "grad_norm": 0.13061150518780162,
      "learning_rate": 1.2625766871165645e-05,
      "loss": 0.0165,
      "step": 670
    },
    {
      "epoch": 2.367887075430084,
      "grad_norm": 0.14965868229501506,
      "learning_rate": 1.2613496932515339e-05,
      "loss": 0.0137,
      "step": 671
    },
    {
      "epoch": 2.3714159682399645,
      "grad_norm": 0.1715673652073732,
      "learning_rate": 1.2601226993865033e-05,
      "loss": 0.0183,
      "step": 672
    },
    {
      "epoch": 2.3714159682399645,
      "eval_average_f1": 0.7883252535584104,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6382978722888186,
      "eval_crossner_literature_precision": 0.7499999999962501,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9208633093012162,
      "eval_crossner_music_precision": 0.9411764705868512,
      "eval_crossner_music_recall": 0.9014084507029557,
      "eval_crossner_politics_f1": 0.746987951756358,
      "eval_crossner_politics_precision": 0.7654320987644871,
      "eval_crossner_politics_recall": 0.7294117647050242,
      "eval_crossner_science_f1": 0.684210526263435,
      "eval_crossner_science_precision": 0.812499999994922,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.958762886546971,
      "eval_mit-movie_precision": 0.9789473684200222,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8358208954711852,
      "eval_mit-restaurant_precision": 0.8615384615371361,
      "eval_mit-restaurant_recall": 0.8115942028973745,
      "eval_runtime": 16.5819,
      "eval_samples_per_second": 7.719,
      "eval_steps_per_second": 0.241,
      "step": 672
    },
    {
      "epoch": 2.374944861049846,
      "grad_norm": 0.09862843116622566,
      "learning_rate": 1.2588957055214725e-05,
      "loss": 0.0168,
      "step": 673
    },
    {
      "epoch": 2.3784737538597267,
      "grad_norm": 0.1227724443699536,
      "learning_rate": 1.257668711656442e-05,
      "loss": 0.0115,
      "step": 674
    },
    {
      "epoch": 2.3820026466696076,
      "grad_norm": 0.0778594092124605,
      "learning_rate": 1.2564417177914111e-05,
      "loss": 0.0108,
      "step": 675
    },
    {
      "epoch": 2.3855315394794885,
      "grad_norm": 0.1750839708876618,
      "learning_rate": 1.2552147239263805e-05,
      "loss": 0.0204,
      "step": 676
    },
    {
      "epoch": 2.3890604322893694,
      "grad_norm": 0.10707620143178496,
      "learning_rate": 1.25398773006135e-05,
      "loss": 0.0172,
      "step": 677
    },
    {
      "epoch": 2.3925893250992503,
      "grad_norm": 0.25122355022985804,
      "learning_rate": 1.2527607361963192e-05,
      "loss": 0.0234,
      "step": 678
    },
    {
      "epoch": 2.396118217909131,
      "grad_norm": 0.19657057886547938,
      "learning_rate": 1.2515337423312886e-05,
      "loss": 0.0176,
      "step": 679
    },
    {
      "epoch": 2.399647110719012,
      "grad_norm": 0.11353652653375,
      "learning_rate": 1.250306748466258e-05,
      "loss": 0.013,
      "step": 680
    },
    {
      "epoch": 2.399647110719012,
      "eval_average_f1": 0.7945592126032398,
      "eval_crossner_ai_f1": 0.7540983606032787,
      "eval_crossner_ai_precision": 0.7419354838685744,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.5957446807996378,
      "eval_crossner_literature_precision": 0.6999999999965001,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.9361702127146321,
      "eval_crossner_music_precision": 0.9428571428557959,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.7398843930127434,
      "eval_crossner_politics_precision": 0.7272727272719008,
      "eval_crossner_politics_recall": 0.7529411764697024,
      "eval_crossner_science_f1": 0.7179487178958581,
      "eval_crossner_science_precision": 0.8235294117598617,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9340101522333171,
      "eval_mit-movie_precision": 0.9387755102031237,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 28.5908,
      "eval_samples_per_second": 4.477,
      "eval_steps_per_second": 0.14,
      "step": 680
    },
    {
      "epoch": 2.403176003528893,
      "grad_norm": 0.2813359632515094,
      "learning_rate": 1.249079754601227e-05,
      "loss": 0.0175,
      "step": 681
    },
    {
      "epoch": 2.406704896338774,
      "grad_norm": 0.08914250390248271,
      "learning_rate": 1.2478527607361963e-05,
      "loss": 0.0156,
      "step": 682
    },
    {
      "epoch": 2.4102337891486547,
      "grad_norm": 0.08662636965539951,
      "learning_rate": 1.2466257668711656e-05,
      "loss": 0.0124,
      "step": 683
    },
    {
      "epoch": 2.4137626819585356,
      "grad_norm": 0.11482590273929572,
      "learning_rate": 1.245398773006135e-05,
      "loss": 0.0119,
      "step": 684
    },
    {
      "epoch": 2.4172915747684165,
      "grad_norm": 0.19942787824963634,
      "learning_rate": 1.2441717791411043e-05,
      "loss": 0.0178,
      "step": 685
    },
    {
      "epoch": 2.4208204675782974,
      "grad_norm": 0.1294342411935137,
      "learning_rate": 1.2429447852760737e-05,
      "loss": 0.0103,
      "step": 686
    },
    {
      "epoch": 2.4243493603881783,
      "grad_norm": 0.29900196728410017,
      "learning_rate": 1.241717791411043e-05,
      "loss": 0.0199,
      "step": 687
    },
    {
      "epoch": 2.427878253198059,
      "grad_norm": 0.11859462652900254,
      "learning_rate": 1.2404907975460123e-05,
      "loss": 0.0102,
      "step": 688
    },
    {
      "epoch": 2.427878253198059,
      "eval_average_f1": 0.7831269289063458,
      "eval_crossner_ai_f1": 0.6885245901116905,
      "eval_crossner_ai_precision": 0.6774193548365245,
      "eval_crossner_ai_recall": 0.6999999999976667,
      "eval_crossner_literature_f1": 0.6249999999481771,
      "eval_crossner_literature_precision": 0.714285714282313,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9208633093012162,
      "eval_crossner_music_precision": 0.9411764705868512,
      "eval_crossner_music_recall": 0.9014084507029557,
      "eval_crossner_politics_f1": 0.7185628742006525,
      "eval_crossner_politics_precision": 0.7317073170722784,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.7179487178958581,
      "eval_crossner_science_precision": 0.8235294117598617,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9538461537951873,
      "eval_mit-movie_precision": 0.9687499999989909,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8571428570916388,
      "eval_mit-restaurant_precision": 0.8906249999986083,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 28.401,
      "eval_samples_per_second": 4.507,
      "eval_steps_per_second": 0.141,
      "step": 688
    },
    {
      "epoch": 2.43140714600794,
      "grad_norm": 0.46127360423098485,
      "learning_rate": 1.2392638036809817e-05,
      "loss": 0.0282,
      "step": 689
    },
    {
      "epoch": 2.434936038817821,
      "grad_norm": 0.12107388656504181,
      "learning_rate": 1.238036809815951e-05,
      "loss": 0.0129,
      "step": 690
    },
    {
      "epoch": 2.438464931627702,
      "grad_norm": 0.31092751165861465,
      "learning_rate": 1.2368098159509203e-05,
      "loss": 0.0147,
      "step": 691
    },
    {
      "epoch": 2.4419938244375827,
      "grad_norm": 0.17368634474686884,
      "learning_rate": 1.2355828220858897e-05,
      "loss": 0.0214,
      "step": 692
    },
    {
      "epoch": 2.4455227172474636,
      "grad_norm": 0.1343887906982184,
      "learning_rate": 1.234355828220859e-05,
      "loss": 0.0132,
      "step": 693
    },
    {
      "epoch": 2.4490516100573445,
      "grad_norm": 0.1714119525819732,
      "learning_rate": 1.2331288343558283e-05,
      "loss": 0.0205,
      "step": 694
    },
    {
      "epoch": 2.4525805028672254,
      "grad_norm": 0.21551447122536463,
      "learning_rate": 1.2319018404907976e-05,
      "loss": 0.0168,
      "step": 695
    },
    {
      "epoch": 2.4561093956771063,
      "grad_norm": 0.09758443098865253,
      "learning_rate": 1.230674846625767e-05,
      "loss": 0.0182,
      "step": 696
    },
    {
      "epoch": 2.4561093956771063,
      "eval_average_f1": 0.7952231065697857,
      "eval_crossner_ai_f1": 0.8135593219811549,
      "eval_crossner_ai_precision": 0.827586206893698,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.5909090908589876,
      "eval_crossner_literature_precision": 0.764705882348443,
      "eval_crossner_literature_recall": 0.48148148147969827,
      "eval_crossner_music_f1": 0.9208633093012162,
      "eval_crossner_music_precision": 0.9411764705868512,
      "eval_crossner_music_recall": 0.9014084507029557,
      "eval_crossner_politics_f1": 0.7361963189675937,
      "eval_crossner_politics_precision": 0.7692307692297831,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.7027027026506939,
      "eval_crossner_science_precision": 0.8666666666608889,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9538461537951873,
      "eval_mit-movie_precision": 0.9687499999989909,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8484848484336663,
      "eval_mit-restaurant_precision": 0.888888888887478,
      "eval_mit-restaurant_recall": 0.8115942028973745,
      "eval_runtime": 28.2318,
      "eval_samples_per_second": 4.534,
      "eval_steps_per_second": 0.142,
      "step": 696
    },
    {
      "epoch": 2.459638288486987,
      "grad_norm": 0.11466397938832858,
      "learning_rate": 1.2294478527607364e-05,
      "loss": 0.0209,
      "step": 697
    },
    {
      "epoch": 2.463167181296868,
      "grad_norm": 0.1588690951373279,
      "learning_rate": 1.2282208588957056e-05,
      "loss": 0.0219,
      "step": 698
    },
    {
      "epoch": 2.466696074106749,
      "grad_norm": 0.11661664283780117,
      "learning_rate": 1.226993865030675e-05,
      "loss": 0.0106,
      "step": 699
    },
    {
      "epoch": 2.47022496691663,
      "grad_norm": 0.20088159117065732,
      "learning_rate": 1.2257668711656444e-05,
      "loss": 0.0178,
      "step": 700
    },
    {
      "epoch": 2.4737538597265107,
      "grad_norm": 0.09144444991971472,
      "learning_rate": 1.2245398773006136e-05,
      "loss": 0.0149,
      "step": 701
    },
    {
      "epoch": 2.4772827525363916,
      "grad_norm": 0.1592395763421753,
      "learning_rate": 1.223312883435583e-05,
      "loss": 0.0289,
      "step": 702
    },
    {
      "epoch": 2.4808116453462725,
      "grad_norm": 0.1320595222878536,
      "learning_rate": 1.2220858895705522e-05,
      "loss": 0.0154,
      "step": 703
    },
    {
      "epoch": 2.4843405381561534,
      "grad_norm": 0.17007976080861725,
      "learning_rate": 1.2208588957055216e-05,
      "loss": 0.0176,
      "step": 704
    },
    {
      "epoch": 2.4843405381561534,
      "eval_average_f1": 0.8179961570739386,
      "eval_crossner_ai_f1": 0.7666666666141112,
      "eval_crossner_ai_precision": 0.7666666666641112,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6382978722888186,
      "eval_crossner_literature_precision": 0.7499999999962501,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9208633093012162,
      "eval_crossner_music_precision": 0.9411764705868512,
      "eval_crossner_music_recall": 0.9014084507029557,
      "eval_crossner_politics_f1": 0.7630057802959538,
      "eval_crossner_politics_precision": 0.7499999999991477,
      "eval_crossner_politics_recall": 0.7764705882343806,
      "eval_crossner_science_f1": 0.8205128204594346,
      "eval_crossner_science_precision": 0.941176470582699,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8676470587722643,
      "eval_mit-restaurant_precision": 0.8805970149240588,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 28.4627,
      "eval_samples_per_second": 4.497,
      "eval_steps_per_second": 0.141,
      "step": 704
    },
    {
      "epoch": 2.4878694309660343,
      "grad_norm": 0.08392955965055279,
      "learning_rate": 1.219631901840491e-05,
      "loss": 0.0197,
      "step": 705
    },
    {
      "epoch": 2.491398323775915,
      "grad_norm": 0.12116465492307857,
      "learning_rate": 1.2184049079754603e-05,
      "loss": 0.0144,
      "step": 706
    },
    {
      "epoch": 2.494927216585796,
      "grad_norm": 0.15973479945885669,
      "learning_rate": 1.2171779141104297e-05,
      "loss": 0.0169,
      "step": 707
    },
    {
      "epoch": 2.498456109395677,
      "grad_norm": 0.14826055696886792,
      "learning_rate": 1.2159509202453989e-05,
      "loss": 0.0122,
      "step": 708
    },
    {
      "epoch": 2.501985002205558,
      "grad_norm": 0.12707611142285427,
      "learning_rate": 1.2147239263803683e-05,
      "loss": 0.0185,
      "step": 709
    },
    {
      "epoch": 2.5055138950154388,
      "grad_norm": 0.20332695934559836,
      "learning_rate": 1.2134969325153377e-05,
      "loss": 0.0137,
      "step": 710
    },
    {
      "epoch": 2.5090427878253196,
      "grad_norm": 0.08283135507618401,
      "learning_rate": 1.2122699386503069e-05,
      "loss": 0.0188,
      "step": 711
    },
    {
      "epoch": 2.512571680635201,
      "grad_norm": 0.07131266203313602,
      "learning_rate": 1.2110429447852761e-05,
      "loss": 0.0139,
      "step": 712
    },
    {
      "epoch": 2.512571680635201,
      "eval_average_f1": 0.7894173364652819,
      "eval_crossner_ai_f1": 0.7999999999473334,
      "eval_crossner_ai_precision": 0.7999999999973334,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.57777777772721,
      "eval_crossner_literature_precision": 0.72222222221821,
      "eval_crossner_literature_recall": 0.48148148147969827,
      "eval_crossner_music_f1": 0.9219858155515316,
      "eval_crossner_music_precision": 0.928571428570102,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.7485380116450326,
      "eval_crossner_politics_precision": 0.7441860465107626,
      "eval_crossner_politics_recall": 0.7529411764697024,
      "eval_crossner_science_f1": 0.7179487178958581,
      "eval_crossner_science_precision": 0.8235294117598617,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9387755101531289,
      "eval_mit-movie_precision": 0.9484536082464449,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8208955223368791,
      "eval_mit-restaurant_precision": 0.8461538461525444,
      "eval_mit-restaurant_recall": 0.7971014492742071,
      "eval_runtime": 16.4554,
      "eval_samples_per_second": 7.779,
      "eval_steps_per_second": 0.243,
      "step": 712
    },
    {
      "epoch": 2.5161005734450814,
      "grad_norm": 0.3055313236509213,
      "learning_rate": 1.2098159509202454e-05,
      "loss": 0.0158,
      "step": 713
    },
    {
      "epoch": 2.5196294662549628,
      "grad_norm": 0.1815678030083295,
      "learning_rate": 1.2085889570552148e-05,
      "loss": 0.0241,
      "step": 714
    },
    {
      "epoch": 2.523158359064843,
      "grad_norm": 0.3538853519155816,
      "learning_rate": 1.207361963190184e-05,
      "loss": 0.0144,
      "step": 715
    },
    {
      "epoch": 2.5266872518747245,
      "grad_norm": 0.15157269094557535,
      "learning_rate": 1.2061349693251534e-05,
      "loss": 0.0169,
      "step": 716
    },
    {
      "epoch": 2.530216144684605,
      "grad_norm": 0.06437762491189265,
      "learning_rate": 1.2049079754601228e-05,
      "loss": 0.012,
      "step": 717
    },
    {
      "epoch": 2.5337450374944863,
      "grad_norm": 0.14757379419485886,
      "learning_rate": 1.203680981595092e-05,
      "loss": 0.0195,
      "step": 718
    },
    {
      "epoch": 2.5372739303043668,
      "grad_norm": 0.11340611662768807,
      "learning_rate": 1.2024539877300614e-05,
      "loss": 0.013,
      "step": 719
    },
    {
      "epoch": 2.540802823114248,
      "grad_norm": 0.09575175490059729,
      "learning_rate": 1.2012269938650308e-05,
      "loss": 0.0215,
      "step": 720
    },
    {
      "epoch": 2.540802823114248,
      "eval_average_f1": 0.789204078400496,
      "eval_crossner_ai_f1": 0.7118644067272623,
      "eval_crossner_ai_precision": 0.7241379310319858,
      "eval_crossner_ai_recall": 0.6999999999976667,
      "eval_crossner_literature_f1": 0.6666666666146702,
      "eval_crossner_literature_precision": 0.7619047619011339,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9219858155515316,
      "eval_crossner_music_precision": 0.928571428570102,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.7199999999492178,
      "eval_crossner_politics_precision": 0.6999999999992222,
      "eval_crossner_politics_recall": 0.7411764705873634,
      "eval_crossner_science_f1": 0.7692307691776463,
      "eval_crossner_science_precision": 0.8823529411712804,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9025641025131886,
      "eval_mit-movie_precision": 0.9166666666657118,
      "eval_mit-movie_recall": 0.888888888887991,
      "eval_mit-restaurant_f1": 0.8321167882699557,
      "eval_mit-restaurant_precision": 0.8382352941164143,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 16.741,
      "eval_samples_per_second": 7.646,
      "eval_steps_per_second": 0.239,
      "step": 720
    },
    {
      "epoch": 2.5443317159241285,
      "grad_norm": 0.07560403703256854,
      "learning_rate": 1.2e-05,
      "loss": 0.0153,
      "step": 721
    },
    {
      "epoch": 2.54786060873401,
      "grad_norm": 0.15434169118434532,
      "learning_rate": 1.1987730061349694e-05,
      "loss": 0.0139,
      "step": 722
    },
    {
      "epoch": 2.5513895015438908,
      "grad_norm": 0.13361229321689255,
      "learning_rate": 1.1975460122699387e-05,
      "loss": 0.0223,
      "step": 723
    },
    {
      "epoch": 2.5549183943537717,
      "grad_norm": 0.13286620344577138,
      "learning_rate": 1.196319018404908e-05,
      "loss": 0.0191,
      "step": 724
    },
    {
      "epoch": 2.5584472871636526,
      "grad_norm": 0.08088791592309312,
      "learning_rate": 1.1950920245398774e-05,
      "loss": 0.0192,
      "step": 725
    },
    {
      "epoch": 2.5619761799735334,
      "grad_norm": 0.06926832798081765,
      "learning_rate": 1.1938650306748467e-05,
      "loss": 0.0084,
      "step": 726
    },
    {
      "epoch": 2.5655050727834143,
      "grad_norm": 0.16380304301900078,
      "learning_rate": 1.192638036809816e-05,
      "loss": 0.0255,
      "step": 727
    },
    {
      "epoch": 2.569033965593295,
      "grad_norm": 0.11138938172200781,
      "learning_rate": 1.1914110429447853e-05,
      "loss": 0.021,
      "step": 728
    },
    {
      "epoch": 2.569033965593295,
      "eval_average_f1": 0.7846755188847924,
      "eval_crossner_ai_f1": 0.7719298245088336,
      "eval_crossner_ai_precision": 0.8148148148117971,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.57777777772721,
      "eval_crossner_literature_precision": 0.72222222221821,
      "eval_crossner_literature_recall": 0.48148148147969827,
      "eval_crossner_music_f1": 0.8857142856630306,
      "eval_crossner_music_precision": 0.8985507246363789,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.7294117646550242,
      "eval_crossner_politics_precision": 0.7294117647050242,
      "eval_crossner_politics_recall": 0.7294117647050242,
      "eval_crossner_science_f1": 0.7894736841576178,
      "eval_crossner_science_precision": 0.9374999999941407,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9175257731449517,
      "eval_mit-movie_precision": 0.9368421052621717,
      "eval_mit-movie_recall": 0.898989898988991,
      "eval_mit-restaurant_f1": 0.8208955223368791,
      "eval_mit-restaurant_precision": 0.8461538461525444,
      "eval_mit-restaurant_recall": 0.7971014492742071,
      "eval_runtime": 16.471,
      "eval_samples_per_second": 7.771,
      "eval_steps_per_second": 0.243,
      "step": 728
    },
    {
      "epoch": 2.572562858403176,
      "grad_norm": 0.10775969445684266,
      "learning_rate": 1.1901840490797547e-05,
      "loss": 0.0135,
      "step": 729
    },
    {
      "epoch": 2.576091751213057,
      "grad_norm": 0.11467949578161149,
      "learning_rate": 1.1889570552147241e-05,
      "loss": 0.017,
      "step": 730
    },
    {
      "epoch": 2.579620644022938,
      "grad_norm": 0.18766728528344567,
      "learning_rate": 1.1877300613496933e-05,
      "loss": 0.0353,
      "step": 731
    },
    {
      "epoch": 2.583149536832819,
      "grad_norm": 0.15789129584546016,
      "learning_rate": 1.1865030674846627e-05,
      "loss": 0.0167,
      "step": 732
    },
    {
      "epoch": 2.5866784296426997,
      "grad_norm": 0.11891253321935999,
      "learning_rate": 1.1852760736196321e-05,
      "loss": 0.0235,
      "step": 733
    },
    {
      "epoch": 2.5902073224525806,
      "grad_norm": 0.0904936298689146,
      "learning_rate": 1.1840490797546013e-05,
      "loss": 0.0173,
      "step": 734
    },
    {
      "epoch": 2.5937362152624615,
      "grad_norm": 0.15611918456972598,
      "learning_rate": 1.1828220858895707e-05,
      "loss": 0.0162,
      "step": 735
    },
    {
      "epoch": 2.5972651080723423,
      "grad_norm": 0.10979004856319102,
      "learning_rate": 1.18159509202454e-05,
      "loss": 0.0218,
      "step": 736
    },
    {
      "epoch": 2.5972651080723423,
      "eval_average_f1": 0.795600518741849,
      "eval_crossner_ai_f1": 0.7540983606032787,
      "eval_crossner_ai_precision": 0.7419354838685744,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.653061224437651,
      "eval_crossner_literature_precision": 0.7272727272694215,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9154929576951895,
      "eval_crossner_music_precision": 0.9154929577451895,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.7199999999492178,
      "eval_crossner_politics_precision": 0.6999999999992222,
      "eval_crossner_politics_recall": 0.7411764705873634,
      "eval_crossner_science_f1": 0.7692307691776463,
      "eval_crossner_science_precision": 0.8823529411712804,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.8877551019899157,
      "eval_mit-movie_precision": 0.8969072164939207,
      "eval_mit-movie_recall": 0.8787878787869912,
      "eval_mit-restaurant_f1": 0.8695652173400441,
      "eval_mit-restaurant_precision": 0.8695652173900441,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.701,
      "eval_samples_per_second": 7.664,
      "eval_steps_per_second": 0.24,
      "step": 736
    },
    {
      "epoch": 2.6007940008822232,
      "grad_norm": 0.07723046119295912,
      "learning_rate": 1.1803680981595094e-05,
      "loss": 0.0107,
      "step": 737
    },
    {
      "epoch": 2.604322893692104,
      "grad_norm": 0.060751570884508964,
      "learning_rate": 1.1791411042944788e-05,
      "loss": 0.0109,
      "step": 738
    },
    {
      "epoch": 2.607851786501985,
      "grad_norm": 0.08033243074417483,
      "learning_rate": 1.177914110429448e-05,
      "loss": 0.0177,
      "step": 739
    },
    {
      "epoch": 2.611380679311866,
      "grad_norm": 0.09828644546215728,
      "learning_rate": 1.1766871165644174e-05,
      "loss": 0.0193,
      "step": 740
    },
    {
      "epoch": 2.614909572121747,
      "grad_norm": 0.10770884243803633,
      "learning_rate": 1.1754601226993866e-05,
      "loss": 0.0141,
      "step": 741
    },
    {
      "epoch": 2.6184384649316277,
      "grad_norm": 0.2512081428553895,
      "learning_rate": 1.174233128834356e-05,
      "loss": 0.0129,
      "step": 742
    },
    {
      "epoch": 2.6219673577415086,
      "grad_norm": 0.0841849366385793,
      "learning_rate": 1.173006134969325e-05,
      "loss": 0.0132,
      "step": 743
    },
    {
      "epoch": 2.6254962505513895,
      "grad_norm": 0.147883044762464,
      "learning_rate": 1.1717791411042945e-05,
      "loss": 0.013,
      "step": 744
    },
    {
      "epoch": 2.6254962505513895,
      "eval_average_f1": 0.8129684774580561,
      "eval_crossner_ai_f1": 0.7868852458490728,
      "eval_crossner_ai_precision": 0.7741935483845994,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.653061224437651,
      "eval_crossner_literature_precision": 0.7272727272694215,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9285714285201122,
      "eval_crossner_music_precision": 0.9420289855058811,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.7471264367307767,
      "eval_crossner_politics_precision": 0.7303370786508647,
      "eval_crossner_politics_recall": 0.7647058823520415,
      "eval_crossner_science_f1": 0.7804878048245093,
      "eval_crossner_science_precision": 0.8421052631534627,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9333333332823879,
      "eval_mit-movie_precision": 0.9479166666656792,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8613138685618839,
      "eval_mit-restaurant_precision": 0.8676470588222535,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.3747,
      "eval_samples_per_second": 7.817,
      "eval_steps_per_second": 0.244,
      "step": 744
    },
    {
      "epoch": 2.6290251433612704,
      "grad_norm": 0.23203204825935098,
      "learning_rate": 1.1705521472392639e-05,
      "loss": 0.0177,
      "step": 745
    },
    {
      "epoch": 2.6325540361711512,
      "grad_norm": 0.1164198928237283,
      "learning_rate": 1.1693251533742331e-05,
      "loss": 0.0183,
      "step": 746
    },
    {
      "epoch": 2.636082928981032,
      "grad_norm": 0.10494025147592718,
      "learning_rate": 1.1680981595092025e-05,
      "loss": 0.0176,
      "step": 747
    },
    {
      "epoch": 2.639611821790913,
      "grad_norm": 0.07364594433502741,
      "learning_rate": 1.1668711656441717e-05,
      "loss": 0.012,
      "step": 748
    },
    {
      "epoch": 2.643140714600794,
      "grad_norm": 0.09961648522973802,
      "learning_rate": 1.1656441717791411e-05,
      "loss": 0.0199,
      "step": 749
    },
    {
      "epoch": 2.646669607410675,
      "grad_norm": 0.08557821074393203,
      "learning_rate": 1.1644171779141105e-05,
      "loss": 0.0095,
      "step": 750
    },
    {
      "epoch": 2.6501985002205557,
      "grad_norm": 0.11854924326152966,
      "learning_rate": 1.1631901840490797e-05,
      "loss": 0.0168,
      "step": 751
    },
    {
      "epoch": 2.6537273930304366,
      "grad_norm": 0.1313747250484199,
      "learning_rate": 1.1619631901840491e-05,
      "loss": 0.032,
      "step": 752
    },
    {
      "epoch": 2.6537273930304366,
      "eval_average_f1": 0.8093588488419312,
      "eval_crossner_ai_f1": 0.7936507935983874,
      "eval_crossner_ai_precision": 0.7575757575734618,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.8714285713773366,
      "eval_crossner_music_precision": 0.8840579710132115,
      "eval_crossner_music_recall": 0.8591549295762547,
      "eval_crossner_politics_f1": 0.7231638417571705,
      "eval_crossner_politics_precision": 0.6956521739122873,
      "eval_crossner_politics_recall": 0.7529411764697024,
      "eval_crossner_science_f1": 0.8205128204594346,
      "eval_crossner_science_precision": 0.941176470582699,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9278350514954564,
      "eval_mit-movie_precision": 0.9473684210516343,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.8489208632581337,
      "eval_mit-restaurant_precision": 0.8428571428559387,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.4854,
      "eval_samples_per_second": 7.764,
      "eval_steps_per_second": 0.243,
      "step": 752
    },
    {
      "epoch": 2.6572562858403175,
      "grad_norm": 0.12036703842885746,
      "learning_rate": 1.1607361963190185e-05,
      "loss": 0.013,
      "step": 753
    },
    {
      "epoch": 2.6607851786501984,
      "grad_norm": 0.09475237209645843,
      "learning_rate": 1.1595092024539878e-05,
      "loss": 0.0134,
      "step": 754
    },
    {
      "epoch": 2.6643140714600793,
      "grad_norm": 0.08758766091774157,
      "learning_rate": 1.1582822085889572e-05,
      "loss": 0.0159,
      "step": 755
    },
    {
      "epoch": 2.66784296426996,
      "grad_norm": 0.08303664492096226,
      "learning_rate": 1.1570552147239264e-05,
      "loss": 0.0181,
      "step": 756
    },
    {
      "epoch": 2.671371857079841,
      "grad_norm": 0.1377070875070614,
      "learning_rate": 1.1558282208588958e-05,
      "loss": 0.0166,
      "step": 757
    },
    {
      "epoch": 2.674900749889722,
      "grad_norm": 0.11177557820377695,
      "learning_rate": 1.1546012269938652e-05,
      "loss": 0.0168,
      "step": 758
    },
    {
      "epoch": 2.6784296426996033,
      "grad_norm": 0.11292417747015583,
      "learning_rate": 1.1533742331288344e-05,
      "loss": 0.0137,
      "step": 759
    },
    {
      "epoch": 2.6819585355094837,
      "grad_norm": 0.11116535434378431,
      "learning_rate": 1.1521472392638038e-05,
      "loss": 0.0235,
      "step": 760
    },
    {
      "epoch": 2.6819585355094837,
      "eval_average_f1": 0.7760147954010276,
      "eval_crossner_ai_f1": 0.8387096773667014,
      "eval_crossner_ai_precision": 0.8124999999974609,
      "eval_crossner_ai_recall": 0.8666666666637778,
      "eval_crossner_literature_f1": 0.6122448979072054,
      "eval_crossner_literature_precision": 0.6818181818150827,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.8321167882700198,
      "eval_crossner_music_precision": 0.863636363635055,
      "eval_crossner_music_recall": 0.80281690140732,
      "eval_crossner_politics_f1": 0.704545454494712,
      "eval_crossner_politics_precision": 0.6813186813179326,
      "eval_crossner_politics_recall": 0.7294117647050242,
      "eval_crossner_science_f1": 0.74999999994675,
      "eval_crossner_science_precision": 0.8333333333287037,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9183673468878436,
      "eval_mit-movie_precision": 0.9278350515454352,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.7761194029339608,
      "eval_mit-restaurant_precision": 0.7999999999987693,
      "eval_mit-restaurant_recall": 0.7536231884047049,
      "eval_runtime": 27.7389,
      "eval_samples_per_second": 4.614,
      "eval_steps_per_second": 0.144,
      "step": 760
    },
    {
      "epoch": 2.685487428319365,
      "grad_norm": 0.27043751378122166,
      "learning_rate": 1.150920245398773e-05,
      "loss": 0.0137,
      "step": 761
    },
    {
      "epoch": 2.6890163211292455,
      "grad_norm": 0.1659846781889746,
      "learning_rate": 1.1496932515337424e-05,
      "loss": 0.0122,
      "step": 762
    },
    {
      "epoch": 2.692545213939127,
      "grad_norm": 0.11863476815368687,
      "learning_rate": 1.1484662576687118e-05,
      "loss": 0.022,
      "step": 763
    },
    {
      "epoch": 2.6960741067490073,
      "grad_norm": 0.16369173184394203,
      "learning_rate": 1.147239263803681e-05,
      "loss": 0.0218,
      "step": 764
    },
    {
      "epoch": 2.6996029995588886,
      "grad_norm": 0.24597248995436874,
      "learning_rate": 1.1460122699386505e-05,
      "loss": 0.0164,
      "step": 765
    },
    {
      "epoch": 2.703131892368769,
      "grad_norm": 0.17317485210348504,
      "learning_rate": 1.1447852760736199e-05,
      "loss": 0.0104,
      "step": 766
    },
    {
      "epoch": 2.7066607851786504,
      "grad_norm": 0.599273065445384,
      "learning_rate": 1.143558282208589e-05,
      "loss": 0.02,
      "step": 767
    },
    {
      "epoch": 2.710189677988531,
      "grad_norm": 0.1291977880143508,
      "learning_rate": 1.1423312883435585e-05,
      "loss": 0.0207,
      "step": 768
    },
    {
      "epoch": 2.710189677988531,
      "eval_average_f1": 0.805519855112375,
      "eval_crossner_ai_f1": 0.7936507935983874,
      "eval_crossner_ai_precision": 0.7575757575734618,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.7307692307164942,
      "eval_crossner_literature_precision": 0.7599999999969601,
      "eval_crossner_literature_recall": 0.7037037037010975,
      "eval_crossner_music_f1": 0.8873239436107221,
      "eval_crossner_music_precision": 0.8873239436607221,
      "eval_crossner_music_recall": 0.8873239436607221,
      "eval_crossner_politics_f1": 0.7303370786009659,
      "eval_crossner_politics_precision": 0.6989247311820441,
      "eval_crossner_politics_recall": 0.7647058823520415,
      "eval_crossner_science_f1": 0.7179487178958581,
      "eval_crossner_science_precision": 0.8235294117598617,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8296296295784252,
      "eval_mit-restaurant_precision": 0.8484848484835629,
      "eval_mit-restaurant_recall": 0.8115942028973745,
      "eval_runtime": 16.2114,
      "eval_samples_per_second": 7.896,
      "eval_steps_per_second": 0.247,
      "step": 768
    },
    {
      "epoch": 2.713718570798412,
      "grad_norm": 0.14623061771010745,
      "learning_rate": 1.1411042944785277e-05,
      "loss": 0.0194,
      "step": 769
    },
    {
      "epoch": 2.717247463608293,
      "grad_norm": 0.1593194418259392,
      "learning_rate": 1.1398773006134971e-05,
      "loss": 0.0144,
      "step": 770
    },
    {
      "epoch": 2.720776356418174,
      "grad_norm": 0.15796290515119596,
      "learning_rate": 1.1386503067484665e-05,
      "loss": 0.0188,
      "step": 771
    },
    {
      "epoch": 2.724305249228055,
      "grad_norm": 0.14602371898565009,
      "learning_rate": 1.1374233128834357e-05,
      "loss": 0.0178,
      "step": 772
    },
    {
      "epoch": 2.7278341420379357,
      "grad_norm": 0.1082367636451582,
      "learning_rate": 1.1361963190184051e-05,
      "loss": 0.0182,
      "step": 773
    },
    {
      "epoch": 2.7313630348478166,
      "grad_norm": 0.06539513259081652,
      "learning_rate": 1.1349693251533744e-05,
      "loss": 0.0121,
      "step": 774
    },
    {
      "epoch": 2.7348919276576975,
      "grad_norm": 0.12639443635847095,
      "learning_rate": 1.1337423312883436e-05,
      "loss": 0.0177,
      "step": 775
    },
    {
      "epoch": 2.7384208204675784,
      "grad_norm": 0.15202489521079143,
      "learning_rate": 1.1325153374233128e-05,
      "loss": 0.0211,
      "step": 776
    },
    {
      "epoch": 2.7384208204675784,
      "eval_average_f1": 0.7876937129284256,
      "eval_crossner_ai_f1": 0.7868852458490728,
      "eval_crossner_ai_precision": 0.7741935483845994,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.4999999999503099,
      "eval_crossner_literature_precision": 0.6470588235256056,
      "eval_crossner_literature_recall": 0.40740740740589854,
      "eval_crossner_music_f1": 0.90510948899984,
      "eval_crossner_music_precision": 0.939393939392516,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.7764705881843806,
      "eval_crossner_politics_precision": 0.7764705882343806,
      "eval_crossner_politics_recall": 0.7764705882343806,
      "eval_crossner_science_f1": 0.7179487178958581,
      "eval_crossner_science_precision": 0.8235294117598617,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.953367875596714,
      "eval_mit-movie_precision": 0.978723404254278,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8740740740228038,
      "eval_mit-restaurant_precision": 0.8939393939380395,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 15.4667,
      "eval_samples_per_second": 8.276,
      "eval_steps_per_second": 0.259,
      "step": 776
    },
    {
      "epoch": 2.7419497132774593,
      "grad_norm": 0.09822622855914072,
      "learning_rate": 1.1312883435582822e-05,
      "loss": 0.0154,
      "step": 777
    },
    {
      "epoch": 2.74547860608734,
      "grad_norm": 0.11471636772643783,
      "learning_rate": 1.1300613496932516e-05,
      "loss": 0.0225,
      "step": 778
    },
    {
      "epoch": 2.749007498897221,
      "grad_norm": 0.36555644232142515,
      "learning_rate": 1.1288343558282208e-05,
      "loss": 0.0142,
      "step": 779
    },
    {
      "epoch": 2.752536391707102,
      "grad_norm": 0.11614882788910383,
      "learning_rate": 1.1276073619631902e-05,
      "loss": 0.0119,
      "step": 780
    },
    {
      "epoch": 2.756065284516983,
      "grad_norm": 0.1860169529456907,
      "learning_rate": 1.1263803680981595e-05,
      "loss": 0.0225,
      "step": 781
    },
    {
      "epoch": 2.7595941773268637,
      "grad_norm": 0.16245916843941605,
      "learning_rate": 1.1251533742331289e-05,
      "loss": 0.0135,
      "step": 782
    },
    {
      "epoch": 2.7631230701367446,
      "grad_norm": 0.09454286105988272,
      "learning_rate": 1.1239263803680983e-05,
      "loss": 0.0127,
      "step": 783
    },
    {
      "epoch": 2.7666519629466255,
      "grad_norm": 0.20641965318661762,
      "learning_rate": 1.1226993865030675e-05,
      "loss": 0.0171,
      "step": 784
    },
    {
      "epoch": 2.7666519629466255,
      "eval_average_f1": 0.7985834627918627,
      "eval_crossner_ai_f1": 0.7419354838186265,
      "eval_crossner_ai_precision": 0.7187499999977539,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6382978722888186,
      "eval_crossner_literature_precision": 0.7499999999962501,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.8985507245864208,
      "eval_crossner_music_precision": 0.925373134326977,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.6971428570921011,
      "eval_crossner_politics_precision": 0.6777777777770246,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.8205128204594346,
      "eval_crossner_science_precision": 0.941176470582699,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9447236180395041,
      "eval_mit-movie_precision": 0.93999999999906,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8489208632581337,
      "eval_mit-restaurant_precision": 0.8428571428559387,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 27.2682,
      "eval_samples_per_second": 4.694,
      "eval_steps_per_second": 0.147,
      "step": 784
    },
    {
      "epoch": 2.7701808557565064,
      "grad_norm": 0.15172254012546765,
      "learning_rate": 1.1214723926380369e-05,
      "loss": 0.0214,
      "step": 785
    },
    {
      "epoch": 2.7737097485663873,
      "grad_norm": 0.1735741656781825,
      "learning_rate": 1.1202453987730063e-05,
      "loss": 0.02,
      "step": 786
    },
    {
      "epoch": 2.777238641376268,
      "grad_norm": 0.24372940526669584,
      "learning_rate": 1.1190184049079755e-05,
      "loss": 0.0234,
      "step": 787
    },
    {
      "epoch": 2.780767534186149,
      "grad_norm": 0.11948272181240609,
      "learning_rate": 1.1177914110429449e-05,
      "loss": 0.0255,
      "step": 788
    },
    {
      "epoch": 2.78429642699603,
      "grad_norm": 0.1266740225184874,
      "learning_rate": 1.1165644171779141e-05,
      "loss": 0.0145,
      "step": 789
    },
    {
      "epoch": 2.787825319805911,
      "grad_norm": 0.08452419406683785,
      "learning_rate": 1.1153374233128835e-05,
      "loss": 0.0155,
      "step": 790
    },
    {
      "epoch": 2.7913542126157918,
      "grad_norm": 0.09760394603042065,
      "learning_rate": 1.114110429447853e-05,
      "loss": 0.0157,
      "step": 791
    },
    {
      "epoch": 2.7948831054256726,
      "grad_norm": 0.10729589878303128,
      "learning_rate": 1.1128834355828221e-05,
      "loss": 0.0218,
      "step": 792
    },
    {
      "epoch": 2.7948831054256726,
      "eval_average_f1": 0.7621754332664723,
      "eval_crossner_ai_f1": 0.7419354838186265,
      "eval_crossner_ai_precision": 0.7187499999977539,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.5652173912534029,
      "eval_crossner_literature_precision": 0.6842105263121885,
      "eval_crossner_literature_recall": 0.48148148147969827,
      "eval_crossner_music_f1": 0.8794326240622302,
      "eval_crossner_music_precision": 0.8857142857130204,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.7411764705373632,
      "eval_crossner_politics_precision": 0.7411764705873634,
      "eval_crossner_politics_recall": 0.7411764705873634,
      "eval_crossner_science_f1": 0.6666666666140697,
      "eval_crossner_science_precision": 0.764705882348443,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9025641025131886,
      "eval_mit-movie_precision": 0.9166666666657118,
      "eval_mit-movie_recall": 0.888888888887991,
      "eval_mit-restaurant_f1": 0.8382352940664252,
      "eval_mit-restaurant_precision": 0.8507462686554467,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 17.2898,
      "eval_samples_per_second": 7.403,
      "eval_steps_per_second": 0.231,
      "step": 792
    },
    {
      "epoch": 2.7984119982355535,
      "grad_norm": 0.1304116846680326,
      "learning_rate": 1.1116564417177915e-05,
      "loss": 0.0122,
      "step": 793
    },
    {
      "epoch": 2.8019408910454344,
      "grad_norm": 0.15811609888656228,
      "learning_rate": 1.1104294478527608e-05,
      "loss": 0.0165,
      "step": 794
    },
    {
      "epoch": 2.8054697838553153,
      "grad_norm": 0.12009540220742587,
      "learning_rate": 1.1092024539877302e-05,
      "loss": 0.0207,
      "step": 795
    },
    {
      "epoch": 2.808998676665196,
      "grad_norm": 0.11913932737050287,
      "learning_rate": 1.1079754601226996e-05,
      "loss": 0.0219,
      "step": 796
    },
    {
      "epoch": 2.812527569475077,
      "grad_norm": 0.1497592740311708,
      "learning_rate": 1.1067484662576688e-05,
      "loss": 0.0171,
      "step": 797
    },
    {
      "epoch": 2.816056462284958,
      "grad_norm": 0.16929432204697673,
      "learning_rate": 1.1055214723926382e-05,
      "loss": 0.0297,
      "step": 798
    },
    {
      "epoch": 2.819585355094839,
      "grad_norm": 0.06875585292697692,
      "learning_rate": 1.1042944785276076e-05,
      "loss": 0.0099,
      "step": 799
    },
    {
      "epoch": 2.8231142479047198,
      "grad_norm": 0.07739320086881471,
      "learning_rate": 1.1030674846625768e-05,
      "loss": 0.0159,
      "step": 800
    },
    {
      "epoch": 2.8231142479047198,
      "eval_average_f1": 0.7981379903222787,
      "eval_crossner_ai_f1": 0.7301587301065257,
      "eval_crossner_ai_precision": 0.6969696969675849,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6909090908565951,
      "eval_crossner_literature_precision": 0.6785714285690051,
      "eval_crossner_literature_recall": 0.7037037037010975,
      "eval_crossner_music_f1": 0.8840579709632536,
      "eval_crossner_music_precision": 0.9104477611926709,
      "eval_crossner_music_recall": 0.8591549295762547,
      "eval_crossner_politics_f1": 0.7272727272219589,
      "eval_crossner_politics_precision": 0.7032967032959304,
      "eval_crossner_politics_recall": 0.7529411764697024,
      "eval_crossner_science_f1": 0.74999999994675,
      "eval_crossner_science_precision": 0.8333333333287037,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9494949494439904,
      "eval_mit-movie_precision": 0.9494949494939904,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8550724637168767,
      "eval_mit-restaurant_precision": 0.8550724637668767,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.1138,
      "eval_samples_per_second": 7.944,
      "eval_steps_per_second": 0.248,
      "step": 800
    },
    {
      "epoch": 2.8266431407146007,
      "grad_norm": 0.12800496922100837,
      "learning_rate": 1.1018404907975462e-05,
      "loss": 0.0237,
      "step": 801
    },
    {
      "epoch": 2.8301720335244815,
      "grad_norm": 0.1389229006812538,
      "learning_rate": 1.1006134969325154e-05,
      "loss": 0.0211,
      "step": 802
    },
    {
      "epoch": 2.8337009263343624,
      "grad_norm": 0.1083407334366299,
      "learning_rate": 1.0993865030674848e-05,
      "loss": 0.0186,
      "step": 803
    },
    {
      "epoch": 2.8372298191442438,
      "grad_norm": 0.11654584374452923,
      "learning_rate": 1.0981595092024542e-05,
      "loss": 0.0222,
      "step": 804
    },
    {
      "epoch": 2.840758711954124,
      "grad_norm": 0.11878063581584379,
      "learning_rate": 1.0969325153374235e-05,
      "loss": 0.0134,
      "step": 805
    },
    {
      "epoch": 2.8442876047640056,
      "grad_norm": 0.08346917460214696,
      "learning_rate": 1.0957055214723927e-05,
      "loss": 0.014,
      "step": 806
    },
    {
      "epoch": 2.847816497573886,
      "grad_norm": 0.08785194499507969,
      "learning_rate": 1.094478527607362e-05,
      "loss": 0.0174,
      "step": 807
    },
    {
      "epoch": 2.8513453903837673,
      "grad_norm": 0.08242212279675735,
      "learning_rate": 1.0932515337423313e-05,
      "loss": 0.0176,
      "step": 808
    },
    {
      "epoch": 2.8513453903837673,
      "eval_average_f1": 0.7789108195578287,
      "eval_crossner_ai_f1": 0.7741935483346514,
      "eval_crossner_ai_precision": 0.7499999999976562,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.5833333332816841,
      "eval_crossner_literature_precision": 0.6666666666634922,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.8840579709632536,
      "eval_crossner_music_precision": 0.9104477611926709,
      "eval_crossner_music_recall": 0.8591549295762547,
      "eval_crossner_politics_f1": 0.7630057802959538,
      "eval_crossner_politics_precision": 0.7499999999991477,
      "eval_crossner_politics_recall": 0.7764705882343806,
      "eval_crossner_science_f1": 0.6829268292152292,
      "eval_crossner_science_precision": 0.7368421052592798,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9119170983946844,
      "eval_mit-movie_precision": 0.9361702127649615,
      "eval_mit-movie_recall": 0.888888888887991,
      "eval_mit-restaurant_f1": 0.8529411764193447,
      "eval_mit-restaurant_precision": 0.8656716417897528,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 16.606,
      "eval_samples_per_second": 7.708,
      "eval_steps_per_second": 0.241,
      "step": 808
    },
    {
      "epoch": 2.8548742831936478,
      "grad_norm": 0.1907266697458872,
      "learning_rate": 1.0920245398773005e-05,
      "loss": 0.0165,
      "step": 809
    },
    {
      "epoch": 2.858403176003529,
      "grad_norm": 0.18180602756767142,
      "learning_rate": 1.09079754601227e-05,
      "loss": 0.0126,
      "step": 810
    },
    {
      "epoch": 2.8619320688134096,
      "grad_norm": 0.21916632544076364,
      "learning_rate": 1.0895705521472393e-05,
      "loss": 0.0252,
      "step": 811
    },
    {
      "epoch": 2.865460961623291,
      "grad_norm": 0.07670487478455253,
      "learning_rate": 1.0883435582822086e-05,
      "loss": 0.016,
      "step": 812
    },
    {
      "epoch": 2.8689898544331713,
      "grad_norm": 0.13923992079498446,
      "learning_rate": 1.087116564417178e-05,
      "loss": 0.0195,
      "step": 813
    },
    {
      "epoch": 2.8725187472430527,
      "grad_norm": 0.09628375698429954,
      "learning_rate": 1.0858895705521472e-05,
      "loss": 0.0199,
      "step": 814
    },
    {
      "epoch": 2.876047640052933,
      "grad_norm": 0.1465471260450161,
      "learning_rate": 1.0846625766871166e-05,
      "loss": 0.0213,
      "step": 815
    },
    {
      "epoch": 2.8795765328628145,
      "grad_norm": 0.1117361522005642,
      "learning_rate": 1.083435582822086e-05,
      "loss": 0.0116,
      "step": 816
    },
    {
      "epoch": 2.8795765328628145,
      "eval_average_f1": 0.783802914136252,
      "eval_crossner_ai_f1": 0.7868852458490728,
      "eval_crossner_ai_precision": 0.7741935483845994,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6153846153323226,
      "eval_crossner_literature_precision": 0.6399999999974401,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.8999999999487245,
      "eval_crossner_music_precision": 0.9130434782595462,
      "eval_crossner_music_recall": 0.8873239436607221,
      "eval_crossner_politics_f1": 0.7118644067289349,
      "eval_crossner_politics_precision": 0.6847826086949078,
      "eval_crossner_politics_recall": 0.7411764705873634,
      "eval_crossner_science_f1": 0.6829268292152292,
      "eval_crossner_science_precision": 0.7368421052592798,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8405797100937092,
      "eval_mit-restaurant_precision": 0.8405797101437092,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 16.3632,
      "eval_samples_per_second": 7.822,
      "eval_steps_per_second": 0.244,
      "step": 816
    },
    {
      "epoch": 2.8831054256726953,
      "grad_norm": 0.09209597841219691,
      "learning_rate": 1.0822085889570552e-05,
      "loss": 0.0105,
      "step": 817
    },
    {
      "epoch": 2.8866343184825762,
      "grad_norm": 0.11092046922598138,
      "learning_rate": 1.0809815950920246e-05,
      "loss": 0.0152,
      "step": 818
    },
    {
      "epoch": 2.890163211292457,
      "grad_norm": 0.12864197025084506,
      "learning_rate": 1.079754601226994e-05,
      "loss": 0.013,
      "step": 819
    },
    {
      "epoch": 2.893692104102338,
      "grad_norm": 0.25618261928427405,
      "learning_rate": 1.0785276073619632e-05,
      "loss": 0.0238,
      "step": 820
    },
    {
      "epoch": 2.897220996912219,
      "grad_norm": 0.06361593205214412,
      "learning_rate": 1.0773006134969326e-05,
      "loss": 0.0146,
      "step": 821
    },
    {
      "epoch": 2.9007498897221,
      "grad_norm": 0.15144593436226875,
      "learning_rate": 1.0760736196319019e-05,
      "loss": 0.0199,
      "step": 822
    },
    {
      "epoch": 2.9042787825319807,
      "grad_norm": 0.256506284044672,
      "learning_rate": 1.0748466257668713e-05,
      "loss": 0.0179,
      "step": 823
    },
    {
      "epoch": 2.9078076753418616,
      "grad_norm": 0.08627634275481544,
      "learning_rate": 1.0736196319018407e-05,
      "loss": 0.0142,
      "step": 824
    },
    {
      "epoch": 2.9078076753418616,
      "eval_average_f1": 0.759610934880907,
      "eval_crossner_ai_f1": 0.7741935483346514,
      "eval_crossner_ai_precision": 0.7499999999976562,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.59999999994792,
      "eval_crossner_literature_precision": 0.6521739130406428,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.8920863308839915,
      "eval_crossner_music_precision": 0.9117647058810121,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.6931818181310885,
      "eval_crossner_politics_precision": 0.6703296703289336,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.5714285713759637,
      "eval_crossner_science_precision": 0.599999999997,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.9230769230259881,
      "eval_mit-movie_precision": 0.9374999999990234,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.863309352466746,
      "eval_mit-restaurant_precision": 0.8571428571416326,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.2395,
      "eval_samples_per_second": 7.882,
      "eval_steps_per_second": 0.246,
      "step": 824
    },
    {
      "epoch": 2.9113365681517425,
      "grad_norm": 0.27790165248436505,
      "learning_rate": 1.0723926380368099e-05,
      "loss": 0.0118,
      "step": 825
    },
    {
      "epoch": 2.9148654609616234,
      "grad_norm": 0.10960383403935825,
      "learning_rate": 1.0711656441717793e-05,
      "loss": 0.0143,
      "step": 826
    },
    {
      "epoch": 2.9183943537715042,
      "grad_norm": 0.1628711565307833,
      "learning_rate": 1.0699386503067487e-05,
      "loss": 0.0145,
      "step": 827
    },
    {
      "epoch": 2.921923246581385,
      "grad_norm": 0.08125935704656097,
      "learning_rate": 1.0687116564417179e-05,
      "loss": 0.013,
      "step": 828
    },
    {
      "epoch": 2.925452139391266,
      "grad_norm": 0.2810695652208383,
      "learning_rate": 1.0674846625766873e-05,
      "loss": 0.0238,
      "step": 829
    },
    {
      "epoch": 2.928981032201147,
      "grad_norm": 0.09833383096687563,
      "learning_rate": 1.0662576687116565e-05,
      "loss": 0.0142,
      "step": 830
    },
    {
      "epoch": 2.932509925011028,
      "grad_norm": 0.15492771455086493,
      "learning_rate": 1.065030674846626e-05,
      "loss": 0.0161,
      "step": 831
    },
    {
      "epoch": 2.9360388178209087,
      "grad_norm": 0.060116029041002515,
      "learning_rate": 1.0638036809815953e-05,
      "loss": 0.008,
      "step": 832
    },
    {
      "epoch": 2.9360388178209087,
      "eval_average_f1": 0.7606696256466895,
      "eval_crossner_ai_f1": 0.7868852458490728,
      "eval_crossner_ai_precision": 0.7741935483845994,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.5833333332816841,
      "eval_crossner_literature_precision": 0.6666666666634922,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.8905109488538762,
      "eval_crossner_music_precision": 0.9242424242410239,
      "eval_crossner_music_recall": 0.8591549295762547,
      "eval_crossner_politics_f1": 0.6867469879010089,
      "eval_crossner_politics_precision": 0.703703703702835,
      "eval_crossner_politics_recall": 0.6705882352933287,
      "eval_crossner_science_f1": 0.5853658536059488,
      "eval_crossner_science_precision": 0.631578947365097,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.9222797926951919,
      "eval_mit-movie_precision": 0.9468085106372907,
      "eval_mit-movie_recall": 0.898989898988991,
      "eval_mit-restaurant_f1": 0.8695652173400441,
      "eval_mit-restaurant_precision": 0.8695652173900441,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 15.8446,
      "eval_samples_per_second": 8.078,
      "eval_steps_per_second": 0.252,
      "step": 832
    },
    {
      "epoch": 2.9395677106307896,
      "grad_norm": 0.10446898033334648,
      "learning_rate": 1.0625766871165646e-05,
      "loss": 0.0172,
      "step": 833
    },
    {
      "epoch": 2.9430966034406705,
      "grad_norm": 0.14999851285674087,
      "learning_rate": 1.061349693251534e-05,
      "loss": 0.0212,
      "step": 834
    },
    {
      "epoch": 2.9466254962505514,
      "grad_norm": 0.09120932706272955,
      "learning_rate": 1.0601226993865032e-05,
      "loss": 0.0195,
      "step": 835
    },
    {
      "epoch": 2.9501543890604323,
      "grad_norm": 0.12553451665977752,
      "learning_rate": 1.0588957055214726e-05,
      "loss": 0.0129,
      "step": 836
    },
    {
      "epoch": 2.953683281870313,
      "grad_norm": 0.14913435541626074,
      "learning_rate": 1.0576687116564416e-05,
      "loss": 0.0172,
      "step": 837
    },
    {
      "epoch": 2.957212174680194,
      "grad_norm": 0.07909608162623759,
      "learning_rate": 1.056441717791411e-05,
      "loss": 0.0164,
      "step": 838
    },
    {
      "epoch": 2.960741067490075,
      "grad_norm": 0.11220264359835597,
      "learning_rate": 1.0552147239263804e-05,
      "loss": 0.019,
      "step": 839
    },
    {
      "epoch": 2.964269960299956,
      "grad_norm": 0.1393569625421564,
      "learning_rate": 1.0539877300613497e-05,
      "loss": 0.0243,
      "step": 840
    },
    {
      "epoch": 2.964269960299956,
      "eval_average_f1": 0.7715916943117167,
      "eval_crossner_ai_f1": 0.8064516128506763,
      "eval_crossner_ai_precision": 0.7812499999975585,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.6666666666141976,
      "eval_crossner_literature_precision": 0.6666666666641976,
      "eval_crossner_literature_recall": 0.6666666666641976,
      "eval_crossner_music_f1": 0.8840579709632536,
      "eval_crossner_music_precision": 0.9104477611926709,
      "eval_crossner_music_recall": 0.8591549295762547,
      "eval_crossner_politics_f1": 0.6931818181310885,
      "eval_crossner_politics_precision": 0.6703296703289336,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.5714285713759637,
      "eval_crossner_science_precision": 0.599999999997,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.9387755101531289,
      "eval_mit-movie_precision": 0.9484536082464449,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8405797100937092,
      "eval_mit-restaurant_precision": 0.8405797101437092,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 15.9668,
      "eval_samples_per_second": 8.017,
      "eval_steps_per_second": 0.251,
      "step": 840
    },
    {
      "epoch": 2.9677988531098367,
      "grad_norm": 0.09671215032545362,
      "learning_rate": 1.052760736196319e-05,
      "loss": 0.0131,
      "step": 841
    },
    {
      "epoch": 2.9713277459197176,
      "grad_norm": 0.09575385092294574,
      "learning_rate": 1.0515337423312883e-05,
      "loss": 0.0163,
      "step": 842
    },
    {
      "epoch": 2.9748566387295985,
      "grad_norm": 0.04753802694887002,
      "learning_rate": 1.0503067484662577e-05,
      "loss": 0.0059,
      "step": 843
    },
    {
      "epoch": 2.9783855315394794,
      "grad_norm": 0.1335583807319401,
      "learning_rate": 1.049079754601227e-05,
      "loss": 0.0143,
      "step": 844
    },
    {
      "epoch": 2.9819144243493603,
      "grad_norm": 0.128019665241215,
      "learning_rate": 1.0478527607361963e-05,
      "loss": 0.0227,
      "step": 845
    },
    {
      "epoch": 2.985443317159241,
      "grad_norm": 0.10220674980178766,
      "learning_rate": 1.0466257668711657e-05,
      "loss": 0.0179,
      "step": 846
    },
    {
      "epoch": 2.988972209969122,
      "grad_norm": 0.13260046076734097,
      "learning_rate": 1.0453987730061351e-05,
      "loss": 0.016,
      "step": 847
    },
    {
      "epoch": 2.992501102779003,
      "grad_norm": 0.06523318460943082,
      "learning_rate": 1.0441717791411043e-05,
      "loss": 0.015,
      "step": 848
    },
    {
      "epoch": 2.992501102779003,
      "eval_average_f1": 0.7694199415175283,
      "eval_crossner_ai_f1": 0.7096774193026015,
      "eval_crossner_ai_precision": 0.6874999999978515,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6296296295772977,
      "eval_crossner_literature_precision": 0.6296296296272977,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.8970588234781899,
      "eval_crossner_music_precision": 0.9384615384600946,
      "eval_crossner_music_recall": 0.8591549295762547,
      "eval_crossner_politics_f1": 0.6999999999493766,
      "eval_crossner_politics_precision": 0.6631578947361441,
      "eval_crossner_politics_recall": 0.7411764705873634,
      "eval_crossner_science_f1": 0.6976744185514333,
      "eval_crossner_science_precision": 0.714285714282313,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8029197079780275,
      "eval_mit-restaurant_precision": 0.8088235294105752,
      "eval_mit-restaurant_recall": 0.7971014492742071,
      "eval_runtime": 27.4792,
      "eval_samples_per_second": 4.658,
      "eval_steps_per_second": 0.146,
      "step": 848
    },
    {
      "epoch": 2.996029995588884,
      "grad_norm": 0.060134969003192144,
      "learning_rate": 1.0429447852760737e-05,
      "loss": 0.0131,
      "step": 849
    },
    {
      "epoch": 2.9995588883987647,
      "grad_norm": 0.19280145087957554,
      "learning_rate": 1.041717791411043e-05,
      "loss": 0.0281,
      "step": 850
    },
    {
      "epoch": 3.0030877812086456,
      "grad_norm": 0.11878076910756072,
      "learning_rate": 1.0404907975460123e-05,
      "loss": 0.0112,
      "step": 851
    },
    {
      "epoch": 3.0066166740185265,
      "grad_norm": 0.10330945752960788,
      "learning_rate": 1.0392638036809817e-05,
      "loss": 0.0117,
      "step": 852
    },
    {
      "epoch": 3.0101455668284074,
      "grad_norm": 0.13987490823672832,
      "learning_rate": 1.038036809815951e-05,
      "loss": 0.01,
      "step": 853
    },
    {
      "epoch": 3.0136744596382883,
      "grad_norm": 0.08236984637905156,
      "learning_rate": 1.0368098159509204e-05,
      "loss": 0.0113,
      "step": 854
    },
    {
      "epoch": 3.017203352448169,
      "grad_norm": 0.09987275047925939,
      "learning_rate": 1.0355828220858896e-05,
      "loss": 0.008,
      "step": 855
    },
    {
      "epoch": 3.02073224525805,
      "grad_norm": 0.10410846683730714,
      "learning_rate": 1.034355828220859e-05,
      "loss": 0.0054,
      "step": 856
    },
    {
      "epoch": 3.02073224525805,
      "eval_average_f1": 0.7737165009700797,
      "eval_crossner_ai_f1": 0.7540983606032787,
      "eval_crossner_ai_precision": 0.7419354838685744,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6153846153323226,
      "eval_crossner_literature_precision": 0.6399999999974401,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.90510948899984,
      "eval_crossner_music_precision": 0.939393939392516,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.6892655366724632,
      "eval_crossner_politics_precision": 0.6630434782601489,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9387755101531289,
      "eval_mit-movie_precision": 0.9484536082464449,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8467153284159198,
      "eval_mit-restaurant_precision": 0.8529411764693339,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 27.4822,
      "eval_samples_per_second": 4.658,
      "eval_steps_per_second": 0.146,
      "step": 856
    },
    {
      "epoch": 3.0242611380679314,
      "grad_norm": 0.07465858384528677,
      "learning_rate": 1.0331288343558284e-05,
      "loss": 0.0105,
      "step": 857
    },
    {
      "epoch": 3.0277900308778123,
      "grad_norm": 0.06525248401808044,
      "learning_rate": 1.0319018404907976e-05,
      "loss": 0.0107,
      "step": 858
    },
    {
      "epoch": 3.031318923687693,
      "grad_norm": 0.1591011333295886,
      "learning_rate": 1.030674846625767e-05,
      "loss": 0.0119,
      "step": 859
    },
    {
      "epoch": 3.034847816497574,
      "grad_norm": 0.0905818320508268,
      "learning_rate": 1.0294478527607364e-05,
      "loss": 0.0058,
      "step": 860
    },
    {
      "epoch": 3.038376709307455,
      "grad_norm": 0.13908609795460677,
      "learning_rate": 1.0282208588957056e-05,
      "loss": 0.0096,
      "step": 861
    },
    {
      "epoch": 3.041905602117336,
      "grad_norm": 0.09959464805791818,
      "learning_rate": 1.026993865030675e-05,
      "loss": 0.0113,
      "step": 862
    },
    {
      "epoch": 3.0454344949272167,
      "grad_norm": 0.30002469120141656,
      "learning_rate": 1.0257668711656443e-05,
      "loss": 0.013,
      "step": 863
    },
    {
      "epoch": 3.0489633877370976,
      "grad_norm": 0.17426943747270576,
      "learning_rate": 1.0245398773006137e-05,
      "loss": 0.0135,
      "step": 864
    },
    {
      "epoch": 3.0489633877370976,
      "eval_average_f1": 0.7948379095419639,
      "eval_crossner_ai_f1": 0.7666666666141112,
      "eval_crossner_ai_precision": 0.7666666666641112,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.90510948899984,
      "eval_crossner_music_precision": 0.939393939392516,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.7150837988320089,
      "eval_crossner_politics_precision": 0.6808510638290629,
      "eval_crossner_politics_recall": 0.7529411764697024,
      "eval_crossner_science_f1": 0.6829268292152292,
      "eval_crossner_science_precision": 0.7368421052592798,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9641025640515871,
      "eval_mit-movie_precision": 0.9791666666656467,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.863309352466746,
      "eval_mit-restaurant_precision": 0.8571428571416326,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 27.5051,
      "eval_samples_per_second": 4.654,
      "eval_steps_per_second": 0.145,
      "step": 864
    },
    {
      "epoch": 3.0524922805469785,
      "grad_norm": 0.06435679693399932,
      "learning_rate": 1.023312883435583e-05,
      "loss": 0.0071,
      "step": 865
    },
    {
      "epoch": 3.0560211733568594,
      "grad_norm": 0.06946437652298619,
      "learning_rate": 1.0220858895705523e-05,
      "loss": 0.0102,
      "step": 866
    },
    {
      "epoch": 3.0595500661667403,
      "grad_norm": 0.16937784822469212,
      "learning_rate": 1.0208588957055217e-05,
      "loss": 0.0152,
      "step": 867
    },
    {
      "epoch": 3.063078958976621,
      "grad_norm": 0.14552845904422465,
      "learning_rate": 1.0196319018404909e-05,
      "loss": 0.0162,
      "step": 868
    },
    {
      "epoch": 3.066607851786502,
      "grad_norm": 0.05888620743284396,
      "learning_rate": 1.0184049079754601e-05,
      "loss": 0.0102,
      "step": 869
    },
    {
      "epoch": 3.070136744596383,
      "grad_norm": 0.08779644688727446,
      "learning_rate": 1.0171779141104294e-05,
      "loss": 0.0075,
      "step": 870
    },
    {
      "epoch": 3.073665637406264,
      "grad_norm": 0.07136628788765766,
      "learning_rate": 1.0159509202453988e-05,
      "loss": 0.0136,
      "step": 871
    },
    {
      "epoch": 3.0771945302161448,
      "grad_norm": 0.11087477129506217,
      "learning_rate": 1.0147239263803682e-05,
      "loss": 0.0127,
      "step": 872
    },
    {
      "epoch": 3.0771945302161448,
      "eval_average_f1": 0.7889165726311231,
      "eval_crossner_ai_f1": 0.7999999999473334,
      "eval_crossner_ai_precision": 0.7999999999973334,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9275362318327557,
      "eval_crossner_music_precision": 0.9552238805955892,
      "eval_crossner_music_recall": 0.9014084507029557,
      "eval_crossner_politics_f1": 0.6779661016442273,
      "eval_crossner_politics_precision": 0.6521739130427694,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.634146341410589,
      "eval_crossner_science_precision": 0.6842105263121885,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9484536081964661,
      "eval_mit-movie_precision": 0.9684210526305596,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8676470587722643,
      "eval_mit-restaurant_precision": 0.8805970149240588,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.1548,
      "eval_samples_per_second": 7.923,
      "eval_steps_per_second": 0.248,
      "step": 872
    },
    {
      "epoch": 3.0807234230260256,
      "grad_norm": 0.11288836602323493,
      "learning_rate": 1.0134969325153374e-05,
      "loss": 0.0057,
      "step": 873
    },
    {
      "epoch": 3.0842523158359065,
      "grad_norm": 0.0820641877259763,
      "learning_rate": 1.0122699386503068e-05,
      "loss": 0.0117,
      "step": 874
    },
    {
      "epoch": 3.0877812086457874,
      "grad_norm": 0.08755182557897317,
      "learning_rate": 1.011042944785276e-05,
      "loss": 0.0069,
      "step": 875
    },
    {
      "epoch": 3.0913101014556683,
      "grad_norm": 0.04926122042160615,
      "learning_rate": 1.0098159509202454e-05,
      "loss": 0.0081,
      "step": 876
    },
    {
      "epoch": 3.094838994265549,
      "grad_norm": 0.07704643556356972,
      "learning_rate": 1.0085889570552148e-05,
      "loss": 0.0098,
      "step": 877
    },
    {
      "epoch": 3.09836788707543,
      "grad_norm": 0.07254864713076532,
      "learning_rate": 1.007361963190184e-05,
      "loss": 0.0118,
      "step": 878
    },
    {
      "epoch": 3.101896779885311,
      "grad_norm": 0.12829878029821468,
      "learning_rate": 1.0061349693251534e-05,
      "loss": 0.0135,
      "step": 879
    },
    {
      "epoch": 3.105425672695192,
      "grad_norm": 0.19164061308781818,
      "learning_rate": 1.0049079754601228e-05,
      "loss": 0.0075,
      "step": 880
    },
    {
      "epoch": 3.105425672695192,
      "eval_average_f1": 0.7792396314701785,
      "eval_crossner_ai_f1": 0.7666666666141112,
      "eval_crossner_ai_precision": 0.7666666666641112,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.653846153793713,
      "eval_crossner_literature_precision": 0.67999999999728,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.90510948899984,
      "eval_crossner_music_precision": 0.939393939392516,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.6892655366724632,
      "eval_crossner_politics_precision": 0.6630434782601489,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.6829268292152292,
      "eval_crossner_science_precision": 0.7368421052592798,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9333333332823879,
      "eval_mit-movie_precision": 0.9479166666656792,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8235294117135056,
      "eval_mit-restaurant_precision": 0.8358208955211406,
      "eval_mit-restaurant_recall": 0.8115942028973745,
      "eval_runtime": 27.5091,
      "eval_samples_per_second": 4.653,
      "eval_steps_per_second": 0.145,
      "step": 880
    },
    {
      "epoch": 3.1089545655050728,
      "grad_norm": 0.11722987918680725,
      "learning_rate": 1.003680981595092e-05,
      "loss": 0.0087,
      "step": 881
    },
    {
      "epoch": 3.1124834583149537,
      "grad_norm": 0.10281021053292745,
      "learning_rate": 1.0024539877300615e-05,
      "loss": 0.008,
      "step": 882
    },
    {
      "epoch": 3.1160123511248345,
      "grad_norm": 0.21812713877495585,
      "learning_rate": 1.0012269938650307e-05,
      "loss": 0.0072,
      "step": 883
    },
    {
      "epoch": 3.1195412439347154,
      "grad_norm": 0.09255462239510204,
      "learning_rate": 1e-05,
      "loss": 0.0167,
      "step": 884
    },
    {
      "epoch": 3.1230701367445963,
      "grad_norm": 0.07531024769228663,
      "learning_rate": 9.987730061349695e-06,
      "loss": 0.0083,
      "step": 885
    },
    {
      "epoch": 3.126599029554477,
      "grad_norm": 0.07323023380643325,
      "learning_rate": 9.975460122699387e-06,
      "loss": 0.0082,
      "step": 886
    },
    {
      "epoch": 3.130127922364358,
      "grad_norm": 0.12979033770526321,
      "learning_rate": 9.963190184049081e-06,
      "loss": 0.0108,
      "step": 887
    },
    {
      "epoch": 3.133656815174239,
      "grad_norm": 0.18079371568436375,
      "learning_rate": 9.950920245398773e-06,
      "loss": 0.0109,
      "step": 888
    },
    {
      "epoch": 3.133656815174239,
      "eval_average_f1": 0.7742564405651232,
      "eval_crossner_ai_f1": 0.7666666666141112,
      "eval_crossner_ai_precision": 0.7666666666641112,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.63999999994776,
      "eval_crossner_literature_precision": 0.695652173910019,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.8905109488538762,
      "eval_crossner_music_precision": 0.9242424242410239,
      "eval_crossner_music_recall": 0.8591549295762547,
      "eval_crossner_politics_f1": 0.705882352890346,
      "eval_crossner_politics_precision": 0.705882352940346,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.634146341410589,
      "eval_crossner_science_precision": 0.6842105263121885,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9381443298459613,
      "eval_mit-movie_precision": 0.9578947368410969,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8444444443932181,
      "eval_mit-restaurant_precision": 0.863636363635055,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 15.7907,
      "eval_samples_per_second": 8.106,
      "eval_steps_per_second": 0.253,
      "step": 888
    },
    {
      "epoch": 3.13718570798412,
      "grad_norm": 0.16732506173051637,
      "learning_rate": 9.938650306748467e-06,
      "loss": 0.006,
      "step": 889
    },
    {
      "epoch": 3.1407146007940008,
      "grad_norm": 0.0971658984399812,
      "learning_rate": 9.926380368098161e-06,
      "loss": 0.0089,
      "step": 890
    },
    {
      "epoch": 3.1442434936038817,
      "grad_norm": 0.10469109329775231,
      "learning_rate": 9.914110429447854e-06,
      "loss": 0.0076,
      "step": 891
    },
    {
      "epoch": 3.1477723864137626,
      "grad_norm": 0.11324213849053798,
      "learning_rate": 9.901840490797546e-06,
      "loss": 0.0103,
      "step": 892
    },
    {
      "epoch": 3.1513012792236434,
      "grad_norm": 0.2546542812987067,
      "learning_rate": 9.88957055214724e-06,
      "loss": 0.0126,
      "step": 893
    },
    {
      "epoch": 3.1548301720335243,
      "grad_norm": 0.1921915856369061,
      "learning_rate": 9.877300613496934e-06,
      "loss": 0.0062,
      "step": 894
    },
    {
      "epoch": 3.1583590648434052,
      "grad_norm": 0.1078135545651372,
      "learning_rate": 9.865030674846626e-06,
      "loss": 0.0108,
      "step": 895
    },
    {
      "epoch": 3.161887957653286,
      "grad_norm": 0.07224714197459044,
      "learning_rate": 9.85276073619632e-06,
      "loss": 0.0098,
      "step": 896
    },
    {
      "epoch": 3.161887957653286,
      "eval_average_f1": 0.7763492994833573,
      "eval_crossner_ai_f1": 0.7741935483346514,
      "eval_crossner_ai_precision": 0.7499999999976562,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6274509803398693,
      "eval_crossner_literature_precision": 0.666666666663889,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.8920863308839915,
      "eval_crossner_music_precision": 0.9117647058810121,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.7167630057295332,
      "eval_crossner_politics_precision": 0.7045454545446539,
      "eval_crossner_politics_recall": 0.7294117647050242,
      "eval_crossner_science_f1": 0.6190476189947846,
      "eval_crossner_science_precision": 0.6499999999967501,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9435897435387876,
      "eval_mit-movie_precision": 0.9583333333323351,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8613138685618839,
      "eval_mit-restaurant_precision": 0.8676470588222535,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 15.952,
      "eval_samples_per_second": 8.024,
      "eval_steps_per_second": 0.251,
      "step": 896
    },
    {
      "epoch": 3.165416850463167,
      "grad_norm": 0.1010157651081574,
      "learning_rate": 9.840490797546012e-06,
      "loss": 0.0162,
      "step": 897
    },
    {
      "epoch": 3.168945743273048,
      "grad_norm": 0.10573068077039931,
      "learning_rate": 9.828220858895706e-06,
      "loss": 0.0079,
      "step": 898
    },
    {
      "epoch": 3.172474636082929,
      "grad_norm": 0.08687652355854673,
      "learning_rate": 9.8159509202454e-06,
      "loss": 0.0128,
      "step": 899
    },
    {
      "epoch": 3.1760035288928097,
      "grad_norm": 0.3072920882766331,
      "learning_rate": 9.803680981595093e-06,
      "loss": 0.0123,
      "step": 900
    },
    {
      "epoch": 3.1795324217026906,
      "grad_norm": 0.22596118317465314,
      "learning_rate": 9.791411042944786e-06,
      "loss": 0.0121,
      "step": 901
    },
    {
      "epoch": 3.183061314512572,
      "grad_norm": 0.06072672351173895,
      "learning_rate": 9.779141104294479e-06,
      "loss": 0.0065,
      "step": 902
    },
    {
      "epoch": 3.1865902073224524,
      "grad_norm": 0.19881203217198032,
      "learning_rate": 9.766871165644173e-06,
      "loss": 0.0143,
      "step": 903
    },
    {
      "epoch": 3.1901191001323337,
      "grad_norm": 0.11009555845781169,
      "learning_rate": 9.754601226993867e-06,
      "loss": 0.0185,
      "step": 904
    },
    {
      "epoch": 3.1901191001323337,
      "eval_average_f1": 0.7735545283471953,
      "eval_crossner_ai_f1": 0.7419354838186265,
      "eval_crossner_ai_precision": 0.7187499999977539,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6153846153323226,
      "eval_crossner_literature_precision": 0.6399999999974401,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9130434782095883,
      "eval_crossner_music_precision": 0.9402985074612831,
      "eval_crossner_music_recall": 0.8873239436607221,
      "eval_crossner_politics_f1": 0.7167630057295332,
      "eval_crossner_politics_precision": 0.7045454545446539,
      "eval_crossner_politics_recall": 0.7294117647050242,
      "eval_crossner_science_f1": 0.6222222221694815,
      "eval_crossner_science_precision": 0.6086956521712665,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9278350514954564,
      "eval_mit-movie_precision": 0.9473684210516343,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.8776978416753584,
      "eval_mit-restaurant_precision": 0.8714285714273265,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.0248,
      "eval_samples_per_second": 7.988,
      "eval_steps_per_second": 0.25,
      "step": 904
    },
    {
      "epoch": 3.1936479929422146,
      "grad_norm": 0.07314399176629105,
      "learning_rate": 9.742331288343559e-06,
      "loss": 0.0157,
      "step": 905
    },
    {
      "epoch": 3.1971768857520955,
      "grad_norm": 0.1600382474823139,
      "learning_rate": 9.730061349693253e-06,
      "loss": 0.0115,
      "step": 906
    },
    {
      "epoch": 3.2007057785619764,
      "grad_norm": 0.06932114053715524,
      "learning_rate": 9.717791411042947e-06,
      "loss": 0.01,
      "step": 907
    },
    {
      "epoch": 3.2042346713718572,
      "grad_norm": 0.13442573670716948,
      "learning_rate": 9.705521472392638e-06,
      "loss": 0.0128,
      "step": 908
    },
    {
      "epoch": 3.207763564181738,
      "grad_norm": 0.13784512695873552,
      "learning_rate": 9.693251533742331e-06,
      "loss": 0.0064,
      "step": 909
    },
    {
      "epoch": 3.211292456991619,
      "grad_norm": 0.09684387387633671,
      "learning_rate": 9.680981595092025e-06,
      "loss": 0.0109,
      "step": 910
    },
    {
      "epoch": 3.2148213498015,
      "grad_norm": 0.07069832195385767,
      "learning_rate": 9.668711656441718e-06,
      "loss": 0.0084,
      "step": 911
    },
    {
      "epoch": 3.218350242611381,
      "grad_norm": 0.20635246166549953,
      "learning_rate": 9.656441717791412e-06,
      "loss": 0.0151,
      "step": 912
    },
    {
      "epoch": 3.218350242611381,
      "eval_average_f1": 0.7889593601526962,
      "eval_crossner_ai_f1": 0.8064516128506763,
      "eval_crossner_ai_precision": 0.7812499999975585,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.653846153793713,
      "eval_crossner_literature_precision": 0.67999999999728,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9130434782095883,
      "eval_crossner_music_precision": 0.9402985074612831,
      "eval_crossner_music_recall": 0.8873239436607221,
      "eval_crossner_politics_f1": 0.705882352890346,
      "eval_crossner_politics_precision": 0.705882352940346,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.6818181817650827,
      "eval_crossner_science_precision": 0.6818181818150827,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9381443298459613,
      "eval_mit-movie_precision": 0.9578947368410969,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8235294117135056,
      "eval_mit-restaurant_precision": 0.8358208955211406,
      "eval_mit-restaurant_recall": 0.8115942028973745,
      "eval_runtime": 15.9359,
      "eval_samples_per_second": 8.032,
      "eval_steps_per_second": 0.251,
      "step": 912
    },
    {
      "epoch": 3.2218791354212617,
      "grad_norm": 0.08988973228678367,
      "learning_rate": 9.644171779141106e-06,
      "loss": 0.0106,
      "step": 913
    },
    {
      "epoch": 3.2254080282311426,
      "grad_norm": 0.13189638659317462,
      "learning_rate": 9.631901840490798e-06,
      "loss": 0.0085,
      "step": 914
    },
    {
      "epoch": 3.2289369210410235,
      "grad_norm": 0.13360481383704612,
      "learning_rate": 9.619631901840492e-06,
      "loss": 0.0143,
      "step": 915
    },
    {
      "epoch": 3.2324658138509044,
      "grad_norm": 0.08986209599145036,
      "learning_rate": 9.607361963190184e-06,
      "loss": 0.0064,
      "step": 916
    },
    {
      "epoch": 3.2359947066607853,
      "grad_norm": 0.1399918315624696,
      "learning_rate": 9.595092024539878e-06,
      "loss": 0.0126,
      "step": 917
    },
    {
      "epoch": 3.239523599470666,
      "grad_norm": 0.08637583361515304,
      "learning_rate": 9.582822085889572e-06,
      "loss": 0.0075,
      "step": 918
    },
    {
      "epoch": 3.243052492280547,
      "grad_norm": 0.05553816268630387,
      "learning_rate": 9.570552147239264e-06,
      "loss": 0.0074,
      "step": 919
    },
    {
      "epoch": 3.246581385090428,
      "grad_norm": 0.14512699653671557,
      "learning_rate": 9.558282208588958e-06,
      "loss": 0.0206,
      "step": 920
    },
    {
      "epoch": 3.246581385090428,
      "eval_average_f1": 0.7943426089694513,
      "eval_crossner_ai_f1": 0.7868852458490728,
      "eval_crossner_ai_precision": 0.7741935483845994,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.7058823528885814,
      "eval_crossner_literature_precision": 0.749999999996875,
      "eval_crossner_literature_recall": 0.6666666666641976,
      "eval_crossner_music_f1": 0.8985507245864208,
      "eval_crossner_music_precision": 0.925373134326977,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.705882352890346,
      "eval_crossner_politics_precision": 0.705882352940346,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9435897435387876,
      "eval_mit-movie_precision": 0.9583333333323351,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8529411764193447,
      "eval_mit-restaurant_precision": 0.8656716417897528,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 15.8569,
      "eval_samples_per_second": 8.072,
      "eval_steps_per_second": 0.252,
      "step": 920
    },
    {
      "epoch": 3.250110277900309,
      "grad_norm": 0.19701355229214046,
      "learning_rate": 9.54601226993865e-06,
      "loss": 0.0128,
      "step": 921
    },
    {
      "epoch": 3.2536391707101897,
      "grad_norm": 0.0950478238192146,
      "learning_rate": 9.533742331288345e-06,
      "loss": 0.0096,
      "step": 922
    },
    {
      "epoch": 3.2571680635200706,
      "grad_norm": 0.0625682100973288,
      "learning_rate": 9.521472392638039e-06,
      "loss": 0.0057,
      "step": 923
    },
    {
      "epoch": 3.2606969563299515,
      "grad_norm": 0.06559301033757024,
      "learning_rate": 9.509202453987731e-06,
      "loss": 0.0111,
      "step": 924
    },
    {
      "epoch": 3.2642258491398324,
      "grad_norm": 0.15704078242088507,
      "learning_rate": 9.496932515337423e-06,
      "loss": 0.0181,
      "step": 925
    },
    {
      "epoch": 3.2677547419497133,
      "grad_norm": 0.07778745389387894,
      "learning_rate": 9.484662576687117e-06,
      "loss": 0.0061,
      "step": 926
    },
    {
      "epoch": 3.271283634759594,
      "grad_norm": 0.16828914514012489,
      "learning_rate": 9.472392638036811e-06,
      "loss": 0.0063,
      "step": 927
    },
    {
      "epoch": 3.274812527569475,
      "grad_norm": 0.09853049910912856,
      "learning_rate": 9.460122699386503e-06,
      "loss": 0.0139,
      "step": 928
    },
    {
      "epoch": 3.274812527569475,
      "eval_average_f1": 0.7789048976060499,
      "eval_crossner_ai_f1": 0.7868852458490728,
      "eval_crossner_ai_precision": 0.7741935483845994,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.63999999994776,
      "eval_crossner_literature_precision": 0.695652173910019,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9064748200926039,
      "eval_crossner_music_precision": 0.9264705882339317,
      "eval_crossner_music_recall": 0.8873239436607221,
      "eval_crossner_politics_f1": 0.720930232507308,
      "eval_crossner_politics_precision": 0.7126436781601004,
      "eval_crossner_politics_recall": 0.7294117647050242,
      "eval_crossner_science_f1": 0.590909090856405,
      "eval_crossner_science_precision": 0.590909090906405,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9230769230259881,
      "eval_mit-movie_precision": 0.9374999999990234,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 15.9286,
      "eval_samples_per_second": 8.036,
      "eval_steps_per_second": 0.251,
      "step": 928
    },
    {
      "epoch": 3.278341420379356,
      "grad_norm": 0.08904778292009516,
      "learning_rate": 9.447852760736197e-06,
      "loss": 0.0075,
      "step": 929
    },
    {
      "epoch": 3.281870313189237,
      "grad_norm": 0.1877892071788091,
      "learning_rate": 9.43558282208589e-06,
      "loss": 0.0122,
      "step": 930
    },
    {
      "epoch": 3.2853992059991177,
      "grad_norm": 0.11401435394409831,
      "learning_rate": 9.423312883435584e-06,
      "loss": 0.0114,
      "step": 931
    },
    {
      "epoch": 3.2889280988089986,
      "grad_norm": 0.10050395754327607,
      "learning_rate": 9.411042944785278e-06,
      "loss": 0.0066,
      "step": 932
    },
    {
      "epoch": 3.2924569916188795,
      "grad_norm": 0.11822784688467827,
      "learning_rate": 9.39877300613497e-06,
      "loss": 0.0115,
      "step": 933
    },
    {
      "epoch": 3.2959858844287604,
      "grad_norm": 0.12432208111943946,
      "learning_rate": 9.386503067484664e-06,
      "loss": 0.0111,
      "step": 934
    },
    {
      "epoch": 3.2995147772386413,
      "grad_norm": 0.17611268903808933,
      "learning_rate": 9.374233128834356e-06,
      "loss": 0.0133,
      "step": 935
    },
    {
      "epoch": 3.303043670048522,
      "grad_norm": 0.06264689810327102,
      "learning_rate": 9.36196319018405e-06,
      "loss": 0.0081,
      "step": 936
    },
    {
      "epoch": 3.303043670048522,
      "eval_average_f1": 0.7613295858280328,
      "eval_crossner_ai_f1": 0.7868852458490728,
      "eval_crossner_ai_precision": 0.7741935483845994,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6938775509680968,
      "eval_crossner_literature_precision": 0.7727272727237604,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.8920863308839915,
      "eval_crossner_music_precision": 0.9117647058810121,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.6741573033201299,
      "eval_crossner_politics_precision": 0.645161290321887,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.5454545454020662,
      "eval_crossner_science_precision": 0.5454545454520662,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.9072164947944468,
      "eval_mit-movie_precision": 0.9263157894727091,
      "eval_mit-movie_recall": 0.888888888887991,
      "eval_mit-restaurant_f1": 0.8296296295784252,
      "eval_mit-restaurant_precision": 0.8484848484835629,
      "eval_mit-restaurant_recall": 0.8115942028973745,
      "eval_runtime": 16.1852,
      "eval_samples_per_second": 7.908,
      "eval_steps_per_second": 0.247,
      "step": 936
    },
    {
      "epoch": 3.306572562858403,
      "grad_norm": 0.09763792819765701,
      "learning_rate": 9.349693251533744e-06,
      "loss": 0.0151,
      "step": 937
    },
    {
      "epoch": 3.310101455668284,
      "grad_norm": 0.1162215957241914,
      "learning_rate": 9.337423312883436e-06,
      "loss": 0.0146,
      "step": 938
    },
    {
      "epoch": 3.313630348478165,
      "grad_norm": 0.09072272893042471,
      "learning_rate": 9.325153374233129e-06,
      "loss": 0.0089,
      "step": 939
    },
    {
      "epoch": 3.3171592412880457,
      "grad_norm": 0.11768077903043296,
      "learning_rate": 9.312883435582823e-06,
      "loss": 0.0111,
      "step": 940
    },
    {
      "epoch": 3.3206881340979266,
      "grad_norm": 0.17233640336938197,
      "learning_rate": 9.300613496932515e-06,
      "loss": 0.0133,
      "step": 941
    },
    {
      "epoch": 3.3242170269078075,
      "grad_norm": 0.09647661131538567,
      "learning_rate": 9.288343558282209e-06,
      "loss": 0.0106,
      "step": 942
    },
    {
      "epoch": 3.3277459197176884,
      "grad_norm": 0.12229709920915019,
      "learning_rate": 9.276073619631903e-06,
      "loss": 0.008,
      "step": 943
    },
    {
      "epoch": 3.3312748125275693,
      "grad_norm": 0.04773334575423764,
      "learning_rate": 9.263803680981595e-06,
      "loss": 0.0042,
      "step": 944
    },
    {
      "epoch": 3.3312748125275693,
      "eval_average_f1": 0.7658298633278223,
      "eval_crossner_ai_f1": 0.758620689602616,
      "eval_crossner_ai_precision": 0.7857142857114796,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6382978722888186,
      "eval_crossner_literature_precision": 0.7499999999962501,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9197080291458043,
      "eval_crossner_music_precision": 0.9545454545440082,
      "eval_crossner_music_recall": 0.8873239436607221,
      "eval_crossner_politics_f1": 0.6971428570921011,
      "eval_crossner_politics_precision": 0.6777777777770246,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.5853658536059488,
      "eval_crossner_science_precision": 0.631578947365097,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.9381443298459613,
      "eval_mit-movie_precision": 0.9578947368410969,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8235294117135056,
      "eval_mit-restaurant_precision": 0.8358208955211406,
      "eval_mit-restaurant_recall": 0.8115942028973745,
      "eval_runtime": 16.216,
      "eval_samples_per_second": 7.893,
      "eval_steps_per_second": 0.247,
      "step": 944
    },
    {
      "epoch": 3.33480370533745,
      "grad_norm": 0.06566298228617222,
      "learning_rate": 9.251533742331289e-06,
      "loss": 0.0051,
      "step": 945
    },
    {
      "epoch": 3.338332598147331,
      "grad_norm": 0.07732341335230809,
      "learning_rate": 9.239263803680983e-06,
      "loss": 0.0111,
      "step": 946
    },
    {
      "epoch": 3.3418614909572124,
      "grad_norm": 0.3160320706797698,
      "learning_rate": 9.226993865030675e-06,
      "loss": 0.0082,
      "step": 947
    },
    {
      "epoch": 3.345390383767093,
      "grad_norm": 0.18429847155466036,
      "learning_rate": 9.21472392638037e-06,
      "loss": 0.0088,
      "step": 948
    },
    {
      "epoch": 3.348919276576974,
      "grad_norm": 0.12299275343325627,
      "learning_rate": 9.202453987730062e-06,
      "loss": 0.0103,
      "step": 949
    },
    {
      "epoch": 3.3524481693868546,
      "grad_norm": 0.10459870408654455,
      "learning_rate": 9.190184049079756e-06,
      "loss": 0.01,
      "step": 950
    },
    {
      "epoch": 3.355977062196736,
      "grad_norm": 0.1776675213850777,
      "learning_rate": 9.17791411042945e-06,
      "loss": 0.0088,
      "step": 951
    },
    {
      "epoch": 3.359505955006617,
      "grad_norm": 0.09759468999343414,
      "learning_rate": 9.165644171779142e-06,
      "loss": 0.0119,
      "step": 952
    },
    {
      "epoch": 3.359505955006617,
      "eval_average_f1": 0.7888260418223758,
      "eval_crossner_ai_f1": 0.7868852458490728,
      "eval_crossner_ai_precision": 0.7741935483845994,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.7058823528885814,
      "eval_crossner_literature_precision": 0.749999999996875,
      "eval_crossner_literature_recall": 0.6666666666641976,
      "eval_crossner_music_f1": 0.8985507245864208,
      "eval_crossner_music_precision": 0.925373134326977,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.685393258376297,
      "eval_crossner_politics_precision": 0.6559139784939184,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.634146341410589,
      "eval_crossner_science_precision": 0.6842105263121885,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9285714285204862,
      "eval_mit-movie_precision": 0.93814432989594,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8823529411251837,
      "eval_mit-restaurant_precision": 0.8955223880583648,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.2129,
      "eval_samples_per_second": 7.895,
      "eval_steps_per_second": 0.247,
      "step": 952
    },
    {
      "epoch": 3.3630348478164978,
      "grad_norm": 0.10876877131498879,
      "learning_rate": 9.153374233128836e-06,
      "loss": 0.0076,
      "step": 953
    },
    {
      "epoch": 3.3665637406263786,
      "grad_norm": 0.11444803037794313,
      "learning_rate": 9.14110429447853e-06,
      "loss": 0.0102,
      "step": 954
    },
    {
      "epoch": 3.3700926334362595,
      "grad_norm": 0.1063199880388859,
      "learning_rate": 9.12883435582822e-06,
      "loss": 0.0162,
      "step": 955
    },
    {
      "epoch": 3.3736215262461404,
      "grad_norm": 0.1791328598349149,
      "learning_rate": 9.116564417177914e-06,
      "loss": 0.0122,
      "step": 956
    },
    {
      "epoch": 3.3771504190560213,
      "grad_norm": 0.10085154621195944,
      "learning_rate": 9.104294478527608e-06,
      "loss": 0.0091,
      "step": 957
    },
    {
      "epoch": 3.380679311865902,
      "grad_norm": 0.17368104691681452,
      "learning_rate": 9.0920245398773e-06,
      "loss": 0.0155,
      "step": 958
    },
    {
      "epoch": 3.384208204675783,
      "grad_norm": 0.16640115179391987,
      "learning_rate": 9.079754601226994e-06,
      "loss": 0.0087,
      "step": 959
    },
    {
      "epoch": 3.387737097485664,
      "grad_norm": 0.08214974927013605,
      "learning_rate": 9.067484662576688e-06,
      "loss": 0.0068,
      "step": 960
    },
    {
      "epoch": 3.387737097485664,
      "eval_average_f1": 0.7847726840679561,
      "eval_crossner_ai_f1": 0.7540983606032787,
      "eval_crossner_ai_precision": 0.7419354838685744,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6792452829663227,
      "eval_crossner_literature_precision": 0.6923076923050296,
      "eval_crossner_literature_recall": 0.6666666666641976,
      "eval_crossner_music_f1": 0.9064748200926039,
      "eval_crossner_music_precision": 0.9264705882339317,
      "eval_crossner_music_recall": 0.8873239436607221,
      "eval_crossner_politics_f1": 0.6888888888382777,
      "eval_crossner_politics_precision": 0.6526315789466814,
      "eval_crossner_politics_recall": 0.7294117647050242,
      "eval_crossner_science_f1": 0.6666666666140697,
      "eval_crossner_science_precision": 0.764705882348443,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9387755101531289,
      "eval_mit-movie_precision": 0.9484536082464449,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8592592592080108,
      "eval_mit-restaurant_precision": 0.8787878787865473,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 16.4163,
      "eval_samples_per_second": 7.797,
      "eval_steps_per_second": 0.244,
      "step": 960
    },
    {
      "epoch": 3.391265990295545,
      "grad_norm": 0.18470480726447472,
      "learning_rate": 9.05521472392638e-06,
      "loss": 0.0152,
      "step": 961
    },
    {
      "epoch": 3.3947948831054258,
      "grad_norm": 0.16579577530659864,
      "learning_rate": 9.042944785276075e-06,
      "loss": 0.0083,
      "step": 962
    },
    {
      "epoch": 3.3983237759153067,
      "grad_norm": 0.09437027024107525,
      "learning_rate": 9.030674846625767e-06,
      "loss": 0.008,
      "step": 963
    },
    {
      "epoch": 3.4018526687251875,
      "grad_norm": 0.14472729686597077,
      "learning_rate": 9.018404907975461e-06,
      "loss": 0.0142,
      "step": 964
    },
    {
      "epoch": 3.4053815615350684,
      "grad_norm": 0.13347981904176406,
      "learning_rate": 9.006134969325155e-06,
      "loss": 0.0127,
      "step": 965
    },
    {
      "epoch": 3.4089104543449493,
      "grad_norm": 0.11105185243125569,
      "learning_rate": 8.993865030674847e-06,
      "loss": 0.0102,
      "step": 966
    },
    {
      "epoch": 3.41243934715483,
      "grad_norm": 0.1567026495911582,
      "learning_rate": 8.981595092024541e-06,
      "loss": 0.0093,
      "step": 967
    },
    {
      "epoch": 3.415968239964711,
      "grad_norm": 0.14760451873942804,
      "learning_rate": 8.969325153374233e-06,
      "loss": 0.0136,
      "step": 968
    },
    {
      "epoch": 3.415968239964711,
      "eval_average_f1": 0.7707340780637156,
      "eval_crossner_ai_f1": 0.7666666666141112,
      "eval_crossner_ai_precision": 0.7666666666641112,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6666666666146702,
      "eval_crossner_literature_precision": 0.7619047619011339,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.8985507245864208,
      "eval_crossner_music_precision": 0.925373134326977,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.686046511577116,
      "eval_crossner_politics_precision": 0.6781609195394503,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.5641025640504932,
      "eval_crossner_science_precision": 0.6470588235256056,
      "eval_crossner_science_recall": 0.4999999999977273,
      "eval_mit-movie_f1": 0.9538461537951873,
      "eval_mit-movie_precision": 0.9687499999989909,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8592592592080108,
      "eval_mit-restaurant_precision": 0.8787878787865473,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 15.861,
      "eval_samples_per_second": 8.07,
      "eval_steps_per_second": 0.252,
      "step": 968
    },
    {
      "epoch": 3.419497132774592,
      "grad_norm": 0.11348599806671032,
      "learning_rate": 8.957055214723927e-06,
      "loss": 0.0125,
      "step": 969
    },
    {
      "epoch": 3.423026025584473,
      "grad_norm": 0.08382435457757331,
      "learning_rate": 8.944785276073621e-06,
      "loss": 0.0075,
      "step": 970
    },
    {
      "epoch": 3.4265549183943538,
      "grad_norm": 0.1370950968554257,
      "learning_rate": 8.932515337423314e-06,
      "loss": 0.0094,
      "step": 971
    },
    {
      "epoch": 3.4300838112042347,
      "grad_norm": 0.0915892413785728,
      "learning_rate": 8.920245398773006e-06,
      "loss": 0.0124,
      "step": 972
    },
    {
      "epoch": 3.4336127040141156,
      "grad_norm": 0.18155453805651797,
      "learning_rate": 8.9079754601227e-06,
      "loss": 0.0124,
      "step": 973
    },
    {
      "epoch": 3.4371415968239964,
      "grad_norm": 0.11680929028666594,
      "learning_rate": 8.895705521472394e-06,
      "loss": 0.0089,
      "step": 974
    },
    {
      "epoch": 3.4406704896338773,
      "grad_norm": 0.11929000807248787,
      "learning_rate": 8.883435582822086e-06,
      "loss": 0.0091,
      "step": 975
    },
    {
      "epoch": 3.4441993824437582,
      "grad_norm": 0.15774009785843984,
      "learning_rate": 8.87116564417178e-06,
      "loss": 0.0131,
      "step": 976
    },
    {
      "epoch": 3.4441993824437582,
      "eval_average_f1": 0.7906177338279952,
      "eval_crossner_ai_f1": 0.6984126983605946,
      "eval_crossner_ai_precision": 0.6666666666646465,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9064748200926039,
      "eval_crossner_music_precision": 0.9264705882339317,
      "eval_crossner_music_recall": 0.8873239436607221,
      "eval_crossner_politics_f1": 0.7150837988320089,
      "eval_crossner_politics_precision": 0.6808510638290629,
      "eval_crossner_politics_recall": 0.7529411764697024,
      "eval_crossner_science_f1": 0.7317073170198692,
      "eval_crossner_science_precision": 0.7894736842063713,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9285714285204862,
      "eval_mit-movie_precision": 0.93814432989594,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8740740740228038,
      "eval_mit-restaurant_precision": 0.8939393939380395,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.1487,
      "eval_samples_per_second": 7.926,
      "eval_steps_per_second": 0.248,
      "step": 976
    },
    {
      "epoch": 3.447728275253639,
      "grad_norm": 0.18807892038599122,
      "learning_rate": 8.858895705521472e-06,
      "loss": 0.0076,
      "step": 977
    },
    {
      "epoch": 3.45125716806352,
      "grad_norm": 0.15447496445981537,
      "learning_rate": 8.846625766871166e-06,
      "loss": 0.0058,
      "step": 978
    },
    {
      "epoch": 3.454786060873401,
      "grad_norm": 0.26280961510234946,
      "learning_rate": 8.83435582822086e-06,
      "loss": 0.0215,
      "step": 979
    },
    {
      "epoch": 3.458314953683282,
      "grad_norm": 0.12399465121713679,
      "learning_rate": 8.822085889570553e-06,
      "loss": 0.0109,
      "step": 980
    },
    {
      "epoch": 3.4618438464931627,
      "grad_norm": 0.06867176611395873,
      "learning_rate": 8.809815950920247e-06,
      "loss": 0.0096,
      "step": 981
    },
    {
      "epoch": 3.4653727393030436,
      "grad_norm": 0.31176671780054555,
      "learning_rate": 8.797546012269939e-06,
      "loss": 0.011,
      "step": 982
    },
    {
      "epoch": 3.4689016321129245,
      "grad_norm": 0.09638142092908353,
      "learning_rate": 8.785276073619633e-06,
      "loss": 0.0113,
      "step": 983
    },
    {
      "epoch": 3.4724305249228054,
      "grad_norm": 0.23750696077937863,
      "learning_rate": 8.773006134969327e-06,
      "loss": 0.0116,
      "step": 984
    },
    {
      "epoch": 3.4724305249228054,
      "eval_average_f1": 0.7941771677622391,
      "eval_crossner_ai_f1": 0.7868852458490728,
      "eval_crossner_ai_precision": 0.7741935483845994,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9577464788218905,
      "eval_crossner_music_precision": 0.9577464788718905,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7011494252365768,
      "eval_crossner_politics_precision": 0.6853932584261961,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.6222222221694815,
      "eval_crossner_science_precision": 0.6086956521712665,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9435897435387876,
      "eval_mit-movie_precision": 0.9583333333323351,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8676470587722643,
      "eval_mit-restaurant_precision": 0.8805970149240588,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.6434,
      "eval_samples_per_second": 7.691,
      "eval_steps_per_second": 0.24,
      "step": 984
    },
    {
      "epoch": 3.4759594177326862,
      "grad_norm": 0.1079460791953819,
      "learning_rate": 8.760736196319019e-06,
      "loss": 0.0093,
      "step": 985
    },
    {
      "epoch": 3.479488310542567,
      "grad_norm": 0.06978578443375273,
      "learning_rate": 8.748466257668711e-06,
      "loss": 0.0049,
      "step": 986
    },
    {
      "epoch": 3.483017203352448,
      "grad_norm": 0.07718443030940696,
      "learning_rate": 8.736196319018405e-06,
      "loss": 0.0076,
      "step": 987
    },
    {
      "epoch": 3.486546096162329,
      "grad_norm": 0.12073886305231855,
      "learning_rate": 8.723926380368098e-06,
      "loss": 0.0098,
      "step": 988
    },
    {
      "epoch": 3.49007498897221,
      "grad_norm": 0.17990756035467195,
      "learning_rate": 8.711656441717792e-06,
      "loss": 0.01,
      "step": 989
    },
    {
      "epoch": 3.4936038817820907,
      "grad_norm": 0.11052190663792992,
      "learning_rate": 8.699386503067486e-06,
      "loss": 0.0069,
      "step": 990
    },
    {
      "epoch": 3.4971327745919716,
      "grad_norm": 0.07072029142474054,
      "learning_rate": 8.687116564417178e-06,
      "loss": 0.0068,
      "step": 991
    },
    {
      "epoch": 3.500661667401853,
      "grad_norm": 0.08341102840067839,
      "learning_rate": 8.674846625766872e-06,
      "loss": 0.0119,
      "step": 992
    },
    {
      "epoch": 3.500661667401853,
      "eval_average_f1": 0.7741916578022944,
      "eval_crossner_ai_f1": 0.8064516128506763,
      "eval_crossner_ai_precision": 0.7812499999975585,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.6415094339098613,
      "eval_crossner_literature_precision": 0.6538461538436391,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9436619717796568,
      "eval_crossner_music_precision": 0.9436619718296568,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7159090908583355,
      "eval_crossner_politics_precision": 0.6923076923069316,
      "eval_crossner_politics_recall": 0.7411764705873634,
      "eval_crossner_science_f1": 0.49999999994800004,
      "eval_crossner_science_precision": 0.5555555555524692,
      "eval_crossner_science_recall": 0.45454545454338846,
      "eval_mit-movie_f1": 0.944162436497266,
      "eval_mit-movie_precision": 0.9489795918357663,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8676470587722643,
      "eval_mit-restaurant_precision": 0.8805970149240588,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.9214,
      "eval_samples_per_second": 7.564,
      "eval_steps_per_second": 0.236,
      "step": 992
    },
    {
      "epoch": 3.5041905602117334,
      "grad_norm": 0.07716497413040357,
      "learning_rate": 8.662576687116566e-06,
      "loss": 0.0069,
      "step": 993
    },
    {
      "epoch": 3.5077194530216147,
      "grad_norm": 0.07914575178424238,
      "learning_rate": 8.650306748466258e-06,
      "loss": 0.0069,
      "step": 994
    },
    {
      "epoch": 3.511248345831495,
      "grad_norm": 0.11356731174029153,
      "learning_rate": 8.638036809815952e-06,
      "loss": 0.0094,
      "step": 995
    },
    {
      "epoch": 3.5147772386413765,
      "grad_norm": 0.10994757067946523,
      "learning_rate": 8.625766871165644e-06,
      "loss": 0.0106,
      "step": 996
    },
    {
      "epoch": 3.518306131451257,
      "grad_norm": 0.38786832854852077,
      "learning_rate": 8.613496932515338e-06,
      "loss": 0.0089,
      "step": 997
    },
    {
      "epoch": 3.5218350242611383,
      "grad_norm": 0.07771969206077604,
      "learning_rate": 8.601226993865032e-06,
      "loss": 0.0105,
      "step": 998
    },
    {
      "epoch": 3.5253639170710187,
      "grad_norm": 0.20974135338360259,
      "learning_rate": 8.588957055214725e-06,
      "loss": 0.0201,
      "step": 999
    },
    {
      "epoch": 3.5288928098809,
      "grad_norm": 0.1273771394357147,
      "learning_rate": 8.576687116564419e-06,
      "loss": 0.0157,
      "step": 1000
    },
    {
      "epoch": 3.5288928098809,
      "eval_average_f1": 0.7916686611067798,
      "eval_crossner_ai_f1": 0.8387096773667014,
      "eval_crossner_ai_precision": 0.8124999999974609,
      "eval_crossner_ai_recall": 0.8666666666637778,
      "eval_crossner_literature_f1": 0.63999999994776,
      "eval_crossner_literature_precision": 0.695652173910019,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9154929576951895,
      "eval_crossner_music_precision": 0.9154929577451895,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.6703910614018912,
      "eval_crossner_politics_precision": 0.6382978723397464,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.699999999947,
      "eval_crossner_science_precision": 0.7777777777734569,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9326424869956992,
      "eval_mit-movie_precision": 0.9574468085096197,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.8444444443932181,
      "eval_mit-restaurant_precision": 0.863636363635055,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 16.6787,
      "eval_samples_per_second": 7.674,
      "eval_steps_per_second": 0.24,
      "step": 1000
    },
    {
      "epoch": 3.532421702690781,
      "grad_norm": 0.14809812473992437,
      "learning_rate": 8.56441717791411e-06,
      "loss": 0.0097,
      "step": 1001
    },
    {
      "epoch": 3.535950595500662,
      "grad_norm": 0.08434916714525341,
      "learning_rate": 8.552147239263803e-06,
      "loss": 0.0136,
      "step": 1002
    },
    {
      "epoch": 3.5394794883105427,
      "grad_norm": 0.06445980759858926,
      "learning_rate": 8.539877300613497e-06,
      "loss": 0.0058,
      "step": 1003
    },
    {
      "epoch": 3.5430083811204236,
      "grad_norm": 0.07807862276163273,
      "learning_rate": 8.527607361963191e-06,
      "loss": 0.0055,
      "step": 1004
    },
    {
      "epoch": 3.5465372739303045,
      "grad_norm": 0.16215612702503787,
      "learning_rate": 8.515337423312883e-06,
      "loss": 0.0113,
      "step": 1005
    },
    {
      "epoch": 3.5500661667401854,
      "grad_norm": 0.08861621588660536,
      "learning_rate": 8.503067484662577e-06,
      "loss": 0.0082,
      "step": 1006
    },
    {
      "epoch": 3.5535950595500663,
      "grad_norm": 0.1698638334569062,
      "learning_rate": 8.490797546012271e-06,
      "loss": 0.0108,
      "step": 1007
    },
    {
      "epoch": 3.557123952359947,
      "grad_norm": 0.11934543031811622,
      "learning_rate": 8.478527607361964e-06,
      "loss": 0.0086,
      "step": 1008
    },
    {
      "epoch": 3.557123952359947,
      "eval_average_f1": 0.784577189052163,
      "eval_crossner_ai_f1": 0.819672131094867,
      "eval_crossner_ai_precision": 0.8064516129006244,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.63999999994776,
      "eval_crossner_literature_precision": 0.695652173910019,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9295774647374231,
      "eval_crossner_music_precision": 0.9295774647874231,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.6704545454038416,
      "eval_crossner_politics_precision": 0.6483516483509358,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.6499999999472501,
      "eval_crossner_science_precision": 0.72222222221821,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9230769230259881,
      "eval_mit-movie_precision": 0.9374999999990234,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.8592592592080108,
      "eval_mit-restaurant_precision": 0.8787878787865473,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 16.679,
      "eval_samples_per_second": 7.674,
      "eval_steps_per_second": 0.24,
      "step": 1008
    },
    {
      "epoch": 3.560652845169828,
      "grad_norm": 0.11498400191407107,
      "learning_rate": 8.466257668711658e-06,
      "loss": 0.0087,
      "step": 1009
    },
    {
      "epoch": 3.564181737979709,
      "grad_norm": 0.18987006095379158,
      "learning_rate": 8.45398773006135e-06,
      "loss": 0.0186,
      "step": 1010
    },
    {
      "epoch": 3.56771063078959,
      "grad_norm": 0.20180616104139137,
      "learning_rate": 8.441717791411044e-06,
      "loss": 0.0128,
      "step": 1011
    },
    {
      "epoch": 3.5712395235994707,
      "grad_norm": 0.11454123207609775,
      "learning_rate": 8.429447852760738e-06,
      "loss": 0.0108,
      "step": 1012
    },
    {
      "epoch": 3.5747684164093516,
      "grad_norm": 0.10127988797295687,
      "learning_rate": 8.41717791411043e-06,
      "loss": 0.0092,
      "step": 1013
    },
    {
      "epoch": 3.5782973092192325,
      "grad_norm": 0.16720199789486012,
      "learning_rate": 8.404907975460124e-06,
      "loss": 0.0073,
      "step": 1014
    },
    {
      "epoch": 3.5818262020291134,
      "grad_norm": 0.11254438445282668,
      "learning_rate": 8.392638036809816e-06,
      "loss": 0.0114,
      "step": 1015
    },
    {
      "epoch": 3.5853550948389943,
      "grad_norm": 0.11475852330842287,
      "learning_rate": 8.38036809815951e-06,
      "loss": 0.004,
      "step": 1016
    },
    {
      "epoch": 3.5853550948389943,
      "eval_average_f1": 0.7698225759535017,
      "eval_crossner_ai_f1": 0.819672131094867,
      "eval_crossner_ai_precision": 0.8064516129006244,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.63999999994776,
      "eval_crossner_literature_precision": 0.695652173910019,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9436619717796568,
      "eval_crossner_music_precision": 0.9436619718296568,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7011494252365768,
      "eval_crossner_politics_precision": 0.6853932584261961,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.49999999994800004,
      "eval_crossner_science_precision": 0.5555555555524692,
      "eval_crossner_science_recall": 0.45454545454338846,
      "eval_mit-movie_f1": 0.9484536081964661,
      "eval_mit-movie_precision": 0.9684210526305596,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8358208954711852,
      "eval_mit-restaurant_precision": 0.8615384615371361,
      "eval_mit-restaurant_recall": 0.8115942028973745,
      "eval_runtime": 16.6676,
      "eval_samples_per_second": 7.68,
      "eval_steps_per_second": 0.24,
      "step": 1016
    },
    {
      "epoch": 3.588883987648875,
      "grad_norm": 0.08096295359246508,
      "learning_rate": 8.368098159509204e-06,
      "loss": 0.0112,
      "step": 1017
    },
    {
      "epoch": 3.592412880458756,
      "grad_norm": 0.06706259492007965,
      "learning_rate": 8.355828220858896e-06,
      "loss": 0.0088,
      "step": 1018
    },
    {
      "epoch": 3.595941773268637,
      "grad_norm": 0.12568113151219182,
      "learning_rate": 8.343558282208589e-06,
      "loss": 0.0064,
      "step": 1019
    },
    {
      "epoch": 3.599470666078518,
      "grad_norm": 0.34506617343372686,
      "learning_rate": 8.331288343558283e-06,
      "loss": 0.0122,
      "step": 1020
    },
    {
      "epoch": 3.6029995588883987,
      "grad_norm": 0.10072315448950654,
      "learning_rate": 8.319018404907977e-06,
      "loss": 0.0065,
      "step": 1021
    },
    {
      "epoch": 3.6065284516982796,
      "grad_norm": 0.05643077113600281,
      "learning_rate": 8.306748466257669e-06,
      "loss": 0.0097,
      "step": 1022
    },
    {
      "epoch": 3.6100573445081605,
      "grad_norm": 0.08232406172731749,
      "learning_rate": 8.294478527607363e-06,
      "loss": 0.0093,
      "step": 1023
    },
    {
      "epoch": 3.6135862373180414,
      "grad_norm": 0.23026575426738843,
      "learning_rate": 8.282208588957055e-06,
      "loss": 0.0136,
      "step": 1024
    },
    {
      "epoch": 3.6135862373180414,
      "eval_average_f1": 0.785824350492544,
      "eval_crossner_ai_f1": 0.7868852458490728,
      "eval_crossner_ai_precision": 0.7741935483845994,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6938775509680968,
      "eval_crossner_literature_precision": 0.7727272727237604,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9295774647374231,
      "eval_crossner_music_precision": 0.9295774647874231,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.7159090908583355,
      "eval_crossner_politics_precision": 0.6923076923069316,
      "eval_crossner_politics_recall": 0.7411764705873634,
      "eval_crossner_science_f1": 0.5499999999477501,
      "eval_crossner_science_precision": 0.6111111111077161,
      "eval_crossner_science_recall": 0.4999999999977273,
      "eval_mit-movie_f1": 0.9340101522333171,
      "eval_mit-movie_precision": 0.9387755102031237,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8905109488538122,
      "eval_mit-restaurant_precision": 0.8970588235280925,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.7505,
      "eval_samples_per_second": 7.642,
      "eval_steps_per_second": 0.239,
      "step": 1024
    },
    {
      "epoch": 3.6171151301279223,
      "grad_norm": 0.07071565904217388,
      "learning_rate": 8.26993865030675e-06,
      "loss": 0.0087,
      "step": 1025
    },
    {
      "epoch": 3.620644022937803,
      "grad_norm": 0.08670531074242321,
      "learning_rate": 8.257668711656443e-06,
      "loss": 0.0089,
      "step": 1026
    },
    {
      "epoch": 3.624172915747684,
      "grad_norm": 0.1560894558576509,
      "learning_rate": 8.245398773006135e-06,
      "loss": 0.0127,
      "step": 1027
    },
    {
      "epoch": 3.627701808557565,
      "grad_norm": 0.11714355121482319,
      "learning_rate": 8.23312883435583e-06,
      "loss": 0.0157,
      "step": 1028
    },
    {
      "epoch": 3.631230701367446,
      "grad_norm": 0.16105990060700415,
      "learning_rate": 8.220858895705522e-06,
      "loss": 0.0093,
      "step": 1029
    },
    {
      "epoch": 3.6347595941773267,
      "grad_norm": 0.1623746356277254,
      "learning_rate": 8.208588957055216e-06,
      "loss": 0.0114,
      "step": 1030
    },
    {
      "epoch": 3.6382884869872076,
      "grad_norm": 0.09681609674111445,
      "learning_rate": 8.19631901840491e-06,
      "loss": 0.0063,
      "step": 1031
    },
    {
      "epoch": 3.6418173797970885,
      "grad_norm": 0.09436828165425078,
      "learning_rate": 8.184049079754602e-06,
      "loss": 0.0086,
      "step": 1032
    },
    {
      "epoch": 3.6418173797970885,
      "eval_average_f1": 0.7920742670630425,
      "eval_crossner_ai_f1": 0.819672131094867,
      "eval_crossner_ai_precision": 0.8064516129006244,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.6382978722888186,
      "eval_crossner_literature_precision": 0.7499999999962501,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9219858155515316,
      "eval_crossner_music_precision": 0.928571428570102,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.6976744185538466,
      "eval_crossner_politics_precision": 0.6896551724130003,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.5999999999475001,
      "eval_crossner_science_precision": 0.666666666662963,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.9538461537951873,
      "eval_mit-movie_precision": 0.9687499999989909,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.9130434782095462,
      "eval_mit-restaurant_precision": 0.9130434782595462,
      "eval_mit-restaurant_recall": 0.9130434782595462,
      "eval_runtime": 16.4621,
      "eval_samples_per_second": 7.775,
      "eval_steps_per_second": 0.243,
      "step": 1032
    },
    {
      "epoch": 3.6453462726069694,
      "grad_norm": 0.04460296732650755,
      "learning_rate": 8.171779141104294e-06,
      "loss": 0.0057,
      "step": 1033
    },
    {
      "epoch": 3.6488751654168503,
      "grad_norm": 0.10821824519006348,
      "learning_rate": 8.159509202453988e-06,
      "loss": 0.0068,
      "step": 1034
    },
    {
      "epoch": 3.652404058226731,
      "grad_norm": 0.06809841139678914,
      "learning_rate": 8.14723926380368e-06,
      "loss": 0.0076,
      "step": 1035
    },
    {
      "epoch": 3.655932951036612,
      "grad_norm": 0.08898836823530314,
      "learning_rate": 8.134969325153374e-06,
      "loss": 0.01,
      "step": 1036
    },
    {
      "epoch": 3.6594618438464934,
      "grad_norm": 0.0654451828727973,
      "learning_rate": 8.122699386503068e-06,
      "loss": 0.0093,
      "step": 1037
    },
    {
      "epoch": 3.662990736656374,
      "grad_norm": 0.04471972321604914,
      "learning_rate": 8.11042944785276e-06,
      "loss": 0.0086,
      "step": 1038
    },
    {
      "epoch": 3.666519629466255,
      "grad_norm": 0.13346303769148288,
      "learning_rate": 8.098159509202455e-06,
      "loss": 0.0092,
      "step": 1039
    },
    {
      "epoch": 3.6700485222761356,
      "grad_norm": 0.09963877561915821,
      "learning_rate": 8.085889570552149e-06,
      "loss": 0.0065,
      "step": 1040
    },
    {
      "epoch": 3.6700485222761356,
      "eval_average_f1": 0.7888145087922632,
      "eval_crossner_ai_f1": 0.7868852458490728,
      "eval_crossner_ai_precision": 0.7741935483845994,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6666666666146702,
      "eval_crossner_literature_precision": 0.7619047619011339,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9361702127146321,
      "eval_crossner_music_precision": 0.9428571428557959,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.7159090908583355,
      "eval_crossner_politics_precision": 0.6923076923069316,
      "eval_crossner_politics_recall": 0.7411764705873634,
      "eval_crossner_science_f1": 0.6499999999472501,
      "eval_crossner_science_precision": 0.72222222221821,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9278350514954564,
      "eval_mit-movie_precision": 0.9473684210516343,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.8382352940664252,
      "eval_mit-restaurant_precision": 0.8507462686554467,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 16.755,
      "eval_samples_per_second": 7.64,
      "eval_steps_per_second": 0.239,
      "step": 1040
    },
    {
      "epoch": 3.673577415086017,
      "grad_norm": 0.4951110177342256,
      "learning_rate": 8.073619631901841e-06,
      "loss": 0.005,
      "step": 1041
    },
    {
      "epoch": 3.6771063078958974,
      "grad_norm": 0.13729980834496425,
      "learning_rate": 8.061349693251535e-06,
      "loss": 0.0094,
      "step": 1042
    },
    {
      "epoch": 3.6806352007057788,
      "grad_norm": 0.15507403952071896,
      "learning_rate": 8.049079754601227e-06,
      "loss": 0.015,
      "step": 1043
    },
    {
      "epoch": 3.684164093515659,
      "grad_norm": 0.05324371603676861,
      "learning_rate": 8.036809815950921e-06,
      "loss": 0.0052,
      "step": 1044
    },
    {
      "epoch": 3.6876929863255405,
      "grad_norm": 0.07576598020951389,
      "learning_rate": 8.024539877300615e-06,
      "loss": 0.0112,
      "step": 1045
    },
    {
      "epoch": 3.691221879135421,
      "grad_norm": 0.37626680342959923,
      "learning_rate": 8.012269938650307e-06,
      "loss": 0.0145,
      "step": 1046
    },
    {
      "epoch": 3.6947507719453023,
      "grad_norm": 0.056002944730763664,
      "learning_rate": 8.000000000000001e-06,
      "loss": 0.0061,
      "step": 1047
    },
    {
      "epoch": 3.698279664755183,
      "grad_norm": 0.10578363267280251,
      "learning_rate": 7.987730061349694e-06,
      "loss": 0.0116,
      "step": 1048
    },
    {
      "epoch": 3.698279664755183,
      "eval_average_f1": 0.812373402413783,
      "eval_crossner_ai_f1": 0.819672131094867,
      "eval_crossner_ai_precision": 0.8064516129006244,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.6382978722888186,
      "eval_crossner_literature_precision": 0.7499999999962501,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9219858155515316,
      "eval_crossner_music_precision": 0.928571428570102,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.7017543859140932,
      "eval_crossner_politics_precision": 0.6976744186038399,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.7999999999465,
      "eval_crossner_science_precision": 0.8888888888839507,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9435897435387876,
      "eval_mit-movie_precision": 0.9583333333323351,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8613138685618839,
      "eval_mit-restaurant_precision": 0.8676470588222535,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 28.4951,
      "eval_samples_per_second": 4.492,
      "eval_steps_per_second": 0.14,
      "step": 1048
    },
    {
      "epoch": 3.701808557565064,
      "grad_norm": 0.19628245783839465,
      "learning_rate": 7.975460122699386e-06,
      "loss": 0.0148,
      "step": 1049
    },
    {
      "epoch": 3.705337450374945,
      "grad_norm": 0.1622572419816241,
      "learning_rate": 7.96319018404908e-06,
      "loss": 0.0115,
      "step": 1050
    },
    {
      "epoch": 3.708866343184826,
      "grad_norm": 0.08888802965170269,
      "learning_rate": 7.950920245398774e-06,
      "loss": 0.0065,
      "step": 1051
    },
    {
      "epoch": 3.7123952359947068,
      "grad_norm": 0.2963849967893952,
      "learning_rate": 7.938650306748466e-06,
      "loss": 0.0137,
      "step": 1052
    },
    {
      "epoch": 3.7159241288045877,
      "grad_norm": 0.14920366969974413,
      "learning_rate": 7.92638036809816e-06,
      "loss": 0.0077,
      "step": 1053
    },
    {
      "epoch": 3.7194530216144686,
      "grad_norm": 0.19690849533944021,
      "learning_rate": 7.914110429447854e-06,
      "loss": 0.0138,
      "step": 1054
    },
    {
      "epoch": 3.7229819144243494,
      "grad_norm": 0.11038919246119341,
      "learning_rate": 7.901840490797546e-06,
      "loss": 0.0088,
      "step": 1055
    },
    {
      "epoch": 3.7265108072342303,
      "grad_norm": 0.14354370360620144,
      "learning_rate": 7.88957055214724e-06,
      "loss": 0.0102,
      "step": 1056
    },
    {
      "epoch": 3.7265108072342303,
      "eval_average_f1": 0.7555553920719016,
      "eval_crossner_ai_f1": 0.7213114753574845,
      "eval_crossner_ai_precision": 0.7096774193525495,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6382978722888186,
      "eval_crossner_literature_precision": 0.7499999999962501,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9154929576951895,
      "eval_crossner_music_precision": 0.9154929577451895,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.686046511577116,
      "eval_crossner_politics_precision": 0.6781609195394503,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.5641025640504932,
      "eval_crossner_science_precision": 0.6470588235256056,
      "eval_crossner_science_recall": 0.4999999999977273,
      "eval_mit-movie_f1": 0.9191919191409907,
      "eval_mit-movie_precision": 0.9191919191909907,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8444444443932181,
      "eval_mit-restaurant_precision": 0.863636363635055,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 41.1159,
      "eval_samples_per_second": 3.113,
      "eval_steps_per_second": 0.097,
      "step": 1056
    },
    {
      "epoch": 3.7300397000441112,
      "grad_norm": 0.14691680024245182,
      "learning_rate": 7.877300613496933e-06,
      "loss": 0.0105,
      "step": 1057
    },
    {
      "epoch": 3.733568592853992,
      "grad_norm": 0.051629336615875154,
      "learning_rate": 7.865030674846627e-06,
      "loss": 0.0084,
      "step": 1058
    },
    {
      "epoch": 3.737097485663873,
      "grad_norm": 0.1830063004568652,
      "learning_rate": 7.85276073619632e-06,
      "loss": 0.0104,
      "step": 1059
    },
    {
      "epoch": 3.740626378473754,
      "grad_norm": 0.1408379518569377,
      "learning_rate": 7.840490797546013e-06,
      "loss": 0.0098,
      "step": 1060
    },
    {
      "epoch": 3.744155271283635,
      "grad_norm": 0.18067600107698178,
      "learning_rate": 7.828220858895707e-06,
      "loss": 0.0082,
      "step": 1061
    },
    {
      "epoch": 3.7476841640935157,
      "grad_norm": 0.14407154381720588,
      "learning_rate": 7.815950920245399e-06,
      "loss": 0.0154,
      "step": 1062
    },
    {
      "epoch": 3.7512130569033966,
      "grad_norm": 0.1688598901961942,
      "learning_rate": 7.803680981595093e-06,
      "loss": 0.013,
      "step": 1063
    },
    {
      "epoch": 3.7547419497132775,
      "grad_norm": 0.10023552308945148,
      "learning_rate": 7.791411042944787e-06,
      "loss": 0.0133,
      "step": 1064
    },
    {
      "epoch": 3.7547419497132775,
      "eval_average_f1": 0.7691558163710706,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6382978722888186,
      "eval_crossner_literature_precision": 0.7499999999962501,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9154929576951895,
      "eval_crossner_music_precision": 0.9154929577451895,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.7052023120879282,
      "eval_crossner_politics_precision": 0.6931818181810304,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.6153846153322814,
      "eval_crossner_science_precision": 0.7058823529370243,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.9381443298459613,
      "eval_mit-movie_precision": 0.9578947368410969,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8382352940664252,
      "eval_mit-restaurant_precision": 0.8507462686554467,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 41.7515,
      "eval_samples_per_second": 3.066,
      "eval_steps_per_second": 0.096,
      "step": 1064
    },
    {
      "epoch": 3.7582708425231584,
      "grad_norm": 0.07889406634210437,
      "learning_rate": 7.77914110429448e-06,
      "loss": 0.0087,
      "step": 1065
    },
    {
      "epoch": 3.7617997353330392,
      "grad_norm": 0.22269773096880124,
      "learning_rate": 7.766871165644172e-06,
      "loss": 0.0104,
      "step": 1066
    },
    {
      "epoch": 3.76532862814292,
      "grad_norm": 0.12955453444826515,
      "learning_rate": 7.754601226993866e-06,
      "loss": 0.0152,
      "step": 1067
    },
    {
      "epoch": 3.768857520952801,
      "grad_norm": 0.11611171702108533,
      "learning_rate": 7.742331288343558e-06,
      "loss": 0.0154,
      "step": 1068
    },
    {
      "epoch": 3.772386413762682,
      "grad_norm": 0.16618795426504968,
      "learning_rate": 7.730061349693252e-06,
      "loss": 0.0084,
      "step": 1069
    },
    {
      "epoch": 3.775915306572563,
      "grad_norm": 0.11339450797836,
      "learning_rate": 7.717791411042946e-06,
      "loss": 0.0123,
      "step": 1070
    },
    {
      "epoch": 3.7794441993824437,
      "grad_norm": 0.11791161296627704,
      "learning_rate": 7.705521472392638e-06,
      "loss": 0.01,
      "step": 1071
    },
    {
      "epoch": 3.7829730921923246,
      "grad_norm": 0.09134555007565148,
      "learning_rate": 7.693251533742332e-06,
      "loss": 0.0105,
      "step": 1072
    },
    {
      "epoch": 3.7829730921923246,
      "eval_average_f1": 0.7938375883351327,
      "eval_crossner_ai_f1": 0.819672131094867,
      "eval_crossner_ai_precision": 0.8064516129006244,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.5833333332816841,
      "eval_crossner_literature_precision": 0.6666666666634922,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.9219858155515316,
      "eval_crossner_music_precision": 0.928571428570102,
      "eval_crossner_music_recall": 0.9154929577451895,
      "eval_crossner_politics_f1": 0.7386363635855824,
      "eval_crossner_politics_precision": 0.7142857142849294,
      "eval_crossner_politics_recall": 0.7647058823520415,
      "eval_crossner_science_f1": 0.7179487178958581,
      "eval_crossner_science_precision": 0.8235294117598617,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9285714285204862,
      "eval_mit-movie_precision": 0.93814432989594,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8467153284159198,
      "eval_mit-restaurant_precision": 0.8529411764693339,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 41.032,
      "eval_samples_per_second": 3.12,
      "eval_steps_per_second": 0.097,
      "step": 1072
    },
    {
      "epoch": 3.7865019850022055,
      "grad_norm": 0.0825496737036229,
      "learning_rate": 7.680981595092026e-06,
      "loss": 0.0069,
      "step": 1073
    },
    {
      "epoch": 3.7900308778120864,
      "grad_norm": 0.12248485368152606,
      "learning_rate": 7.668711656441718e-06,
      "loss": 0.0122,
      "step": 1074
    },
    {
      "epoch": 3.7935597706219673,
      "grad_norm": 0.08912415608084094,
      "learning_rate": 7.656441717791412e-06,
      "loss": 0.0149,
      "step": 1075
    },
    {
      "epoch": 3.797088663431848,
      "grad_norm": 0.08965142542195985,
      "learning_rate": 7.644171779141104e-06,
      "loss": 0.0081,
      "step": 1076
    },
    {
      "epoch": 3.800617556241729,
      "grad_norm": 0.07623499920122911,
      "learning_rate": 7.631901840490798e-06,
      "loss": 0.0078,
      "step": 1077
    },
    {
      "epoch": 3.80414644905161,
      "grad_norm": 0.16248256745732975,
      "learning_rate": 7.619631901840492e-06,
      "loss": 0.0096,
      "step": 1078
    },
    {
      "epoch": 3.807675341861491,
      "grad_norm": 0.09129863317495128,
      "learning_rate": 7.6073619631901856e-06,
      "loss": 0.0103,
      "step": 1079
    },
    {
      "epoch": 3.8112042346713717,
      "grad_norm": 0.1674196394176101,
      "learning_rate": 7.595092024539877e-06,
      "loss": 0.0135,
      "step": 1080
    },
    {
      "epoch": 3.8112042346713717,
      "eval_average_f1": 0.7979080898324779,
      "eval_crossner_ai_f1": 0.7096774193026015,
      "eval_crossner_ai_precision": 0.6874999999978515,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.63999999994776,
      "eval_crossner_literature_precision": 0.695652173910019,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9295774647374231,
      "eval_crossner_music_precision": 0.9295774647874231,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.7085714285206596,
      "eval_crossner_politics_precision": 0.6888888888881235,
      "eval_crossner_politics_recall": 0.7294117647050242,
      "eval_crossner_science_f1": 0.74999999994675,
      "eval_crossner_science_precision": 0.8333333333287037,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8985507245863789,
      "eval_mit-restaurant_precision": 0.8985507246363789,
      "eval_mit-restaurant_recall": 0.8985507246363789,
      "eval_runtime": 15.9773,
      "eval_samples_per_second": 8.011,
      "eval_steps_per_second": 0.25,
      "step": 1080
    },
    {
      "epoch": 3.8147331274812526,
      "grad_norm": 0.13798201587920786,
      "learning_rate": 7.582822085889571e-06,
      "loss": 0.0132,
      "step": 1081
    },
    {
      "epoch": 3.818262020291134,
      "grad_norm": 0.30287832713848184,
      "learning_rate": 7.570552147239264e-06,
      "loss": 0.0178,
      "step": 1082
    },
    {
      "epoch": 3.8217909131010144,
      "grad_norm": 0.28629321917299516,
      "learning_rate": 7.558282208588957e-06,
      "loss": 0.0078,
      "step": 1083
    },
    {
      "epoch": 3.8253198059108957,
      "grad_norm": 0.06943484759908809,
      "learning_rate": 7.54601226993865e-06,
      "loss": 0.0114,
      "step": 1084
    },
    {
      "epoch": 3.828848698720776,
      "grad_norm": 0.12930664820772253,
      "learning_rate": 7.533742331288344e-06,
      "loss": 0.0126,
      "step": 1085
    },
    {
      "epoch": 3.8323775915306575,
      "grad_norm": 0.10633179534950966,
      "learning_rate": 7.5214723926380374e-06,
      "loss": 0.0067,
      "step": 1086
    },
    {
      "epoch": 3.835906484340538,
      "grad_norm": 0.20305169139813734,
      "learning_rate": 7.5092024539877306e-06,
      "loss": 0.0176,
      "step": 1087
    },
    {
      "epoch": 3.8394353771504193,
      "grad_norm": 0.13349064032376404,
      "learning_rate": 7.496932515337424e-06,
      "loss": 0.0116,
      "step": 1088
    },
    {
      "epoch": 3.8394353771504193,
      "eval_average_f1": 0.8116434347122834,
      "eval_crossner_ai_f1": 0.8064516128506763,
      "eval_crossner_ai_precision": 0.7812499999975585,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.6274509803398693,
      "eval_crossner_literature_precision": 0.666666666663889,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9295774647374231,
      "eval_crossner_music_precision": 0.9295774647874231,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.6931818181310885,
      "eval_crossner_politics_precision": 0.6703296703289336,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.7999999999465,
      "eval_crossner_science_precision": 0.8888888888839507,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9183673468878436,
      "eval_mit-movie_precision": 0.9278350515454352,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.9064748200925831,
      "eval_mit-restaurant_precision": 0.8999999999987143,
      "eval_mit-restaurant_recall": 0.9130434782595462,
      "eval_runtime": 15.9232,
      "eval_samples_per_second": 8.039,
      "eval_steps_per_second": 0.251,
      "step": 1088
    },
    {
      "epoch": 3.8429642699602997,
      "grad_norm": 0.13253301075442817,
      "learning_rate": 7.484662576687118e-06,
      "loss": 0.0131,
      "step": 1089
    },
    {
      "epoch": 3.846493162770181,
      "grad_norm": 0.1799986053452504,
      "learning_rate": 7.472392638036811e-06,
      "loss": 0.0117,
      "step": 1090
    },
    {
      "epoch": 3.8500220555800615,
      "grad_norm": 0.10424773419442733,
      "learning_rate": 7.460122699386504e-06,
      "loss": 0.0084,
      "step": 1091
    },
    {
      "epoch": 3.853550948389943,
      "grad_norm": 0.19267413549015197,
      "learning_rate": 7.447852760736197e-06,
      "loss": 0.0142,
      "step": 1092
    },
    {
      "epoch": 3.8570798411998233,
      "grad_norm": 0.10340557456379565,
      "learning_rate": 7.43558282208589e-06,
      "loss": 0.0097,
      "step": 1093
    },
    {
      "epoch": 3.8606087340097046,
      "grad_norm": 0.09721025668130155,
      "learning_rate": 7.423312883435584e-06,
      "loss": 0.0103,
      "step": 1094
    },
    {
      "epoch": 3.8641376268195855,
      "grad_norm": 0.23056761997005235,
      "learning_rate": 7.411042944785277e-06,
      "loss": 0.0127,
      "step": 1095
    },
    {
      "epoch": 3.8676665196294664,
      "grad_norm": 0.14042716547965464,
      "learning_rate": 7.3987730061349695e-06,
      "loss": 0.0144,
      "step": 1096
    },
    {
      "epoch": 3.8676665196294664,
      "eval_average_f1": 0.7948405130869879,
      "eval_crossner_ai_f1": 0.7666666666141112,
      "eval_crossner_ai_precision": 0.7666666666641112,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6122448979072054,
      "eval_crossner_literature_precision": 0.6818181818150827,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9230769230256345,
      "eval_crossner_music_precision": 0.9166666666653935,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.7344632767854065,
      "eval_crossner_politics_precision": 0.7065217391296668,
      "eval_crossner_politics_recall": 0.7647058823520415,
      "eval_crossner_science_f1": 0.7692307691776463,
      "eval_crossner_science_precision": 0.8823529411712804,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9285714285204862,
      "eval_mit-movie_precision": 0.93814432989594,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8296296295784252,
      "eval_mit-restaurant_precision": 0.8484848484835629,
      "eval_mit-restaurant_recall": 0.8115942028973745,
      "eval_runtime": 15.9511,
      "eval_samples_per_second": 8.025,
      "eval_steps_per_second": 0.251,
      "step": 1096
    },
    {
      "epoch": 3.8711954124393473,
      "grad_norm": 0.059356114794148464,
      "learning_rate": 7.386503067484663e-06,
      "loss": 0.004,
      "step": 1097
    },
    {
      "epoch": 3.874724305249228,
      "grad_norm": 0.13737716172923792,
      "learning_rate": 7.374233128834356e-06,
      "loss": 0.0094,
      "step": 1098
    },
    {
      "epoch": 3.878253198059109,
      "grad_norm": 0.0981986006026067,
      "learning_rate": 7.36196319018405e-06,
      "loss": 0.0066,
      "step": 1099
    },
    {
      "epoch": 3.88178209086899,
      "grad_norm": 0.16538076747084193,
      "learning_rate": 7.349693251533743e-06,
      "loss": 0.0143,
      "step": 1100
    },
    {
      "epoch": 3.885310983678871,
      "grad_norm": 0.09634931064517722,
      "learning_rate": 7.337423312883436e-06,
      "loss": 0.0117,
      "step": 1101
    },
    {
      "epoch": 3.8888398764887517,
      "grad_norm": 0.2261594689875302,
      "learning_rate": 7.325153374233129e-06,
      "loss": 0.0094,
      "step": 1102
    },
    {
      "epoch": 3.8923687692986326,
      "grad_norm": 0.17025713895752811,
      "learning_rate": 7.312883435582822e-06,
      "loss": 0.0088,
      "step": 1103
    },
    {
      "epoch": 3.8958976621085135,
      "grad_norm": 0.20471879268547385,
      "learning_rate": 7.300613496932516e-06,
      "loss": 0.012,
      "step": 1104
    },
    {
      "epoch": 3.8958976621085135,
      "eval_average_f1": 0.7782078228903603,
      "eval_crossner_ai_f1": 0.7666666666141112,
      "eval_crossner_ai_precision": 0.7666666666641112,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.653846153793713,
      "eval_crossner_literature_precision": 0.67999999999728,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.8776978416753791,
      "eval_crossner_music_precision": 0.8970588235280925,
      "eval_crossner_music_recall": 0.8591549295762547,
      "eval_crossner_politics_f1": 0.686046511577116,
      "eval_crossner_politics_precision": 0.6781609195394503,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.6666666666140697,
      "eval_crossner_science_precision": 0.764705882348443,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9435897435387876,
      "eval_mit-movie_precision": 0.9583333333323351,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8529411764193447,
      "eval_mit-restaurant_precision": 0.8656716417897528,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 15.7032,
      "eval_samples_per_second": 8.151,
      "eval_steps_per_second": 0.255,
      "step": 1104
    },
    {
      "epoch": 3.8994265549183944,
      "grad_norm": 0.13682073467849284,
      "learning_rate": 7.288343558282209e-06,
      "loss": 0.0122,
      "step": 1105
    },
    {
      "epoch": 3.9029554477282753,
      "grad_norm": 0.12639481951378911,
      "learning_rate": 7.2760736196319025e-06,
      "loss": 0.0054,
      "step": 1106
    },
    {
      "epoch": 3.906484340538156,
      "grad_norm": 0.17342489063546268,
      "learning_rate": 7.263803680981596e-06,
      "loss": 0.0131,
      "step": 1107
    },
    {
      "epoch": 3.910013233348037,
      "grad_norm": 0.09732946556657461,
      "learning_rate": 7.2515337423312896e-06,
      "loss": 0.0096,
      "step": 1108
    },
    {
      "epoch": 3.913542126157918,
      "grad_norm": 0.364921272422703,
      "learning_rate": 7.239263803680983e-06,
      "loss": 0.0197,
      "step": 1109
    },
    {
      "epoch": 3.917071018967799,
      "grad_norm": 0.16891151156245307,
      "learning_rate": 7.226993865030676e-06,
      "loss": 0.008,
      "step": 1110
    },
    {
      "epoch": 3.9205999117776797,
      "grad_norm": 0.1266743533634412,
      "learning_rate": 7.214723926380369e-06,
      "loss": 0.0074,
      "step": 1111
    },
    {
      "epoch": 3.9241288045875606,
      "grad_norm": 0.07979999483626571,
      "learning_rate": 7.202453987730061e-06,
      "loss": 0.0113,
      "step": 1112
    },
    {
      "epoch": 3.9241288045875606,
      "eval_average_f1": 0.809944140774768,
      "eval_crossner_ai_f1": 0.7999999999478343,
      "eval_crossner_ai_precision": 0.7428571428550204,
      "eval_crossner_ai_recall": 0.8666666666637778,
      "eval_crossner_literature_f1": 0.6909090908565951,
      "eval_crossner_literature_precision": 0.6785714285690051,
      "eval_crossner_literature_recall": 0.7037037037010975,
      "eval_crossner_music_f1": 0.9370629370116289,
      "eval_crossner_music_precision": 0.9305555555542631,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.6931818181310885,
      "eval_crossner_politics_precision": 0.6703296703289336,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.7111111110579753,
      "eval_crossner_science_precision": 0.695652173910019,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9696969696459902,
      "eval_mit-movie_precision": 0.9696969696959902,
      "eval_mit-movie_recall": 0.9696969696959902,
      "eval_mit-restaurant_f1": 0.8676470587722643,
      "eval_mit-restaurant_precision": 0.8805970149240588,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.1751,
      "eval_samples_per_second": 7.913,
      "eval_steps_per_second": 0.247,
      "step": 1112
    },
    {
      "epoch": 3.9276576973974415,
      "grad_norm": 0.2719331366763869,
      "learning_rate": 7.190184049079754e-06,
      "loss": 0.0169,
      "step": 1113
    },
    {
      "epoch": 3.9311865902073224,
      "grad_norm": 0.30178534126016837,
      "learning_rate": 7.177914110429448e-06,
      "loss": 0.0138,
      "step": 1114
    },
    {
      "epoch": 3.9347154830172033,
      "grad_norm": 0.09016498895203379,
      "learning_rate": 7.1656441717791414e-06,
      "loss": 0.0065,
      "step": 1115
    },
    {
      "epoch": 3.938244375827084,
      "grad_norm": 0.11417999225775458,
      "learning_rate": 7.1533742331288346e-06,
      "loss": 0.006,
      "step": 1116
    },
    {
      "epoch": 3.941773268636965,
      "grad_norm": 0.22721836099550674,
      "learning_rate": 7.141104294478528e-06,
      "loss": 0.0192,
      "step": 1117
    },
    {
      "epoch": 3.945302161446846,
      "grad_norm": 0.13195392838770492,
      "learning_rate": 7.128834355828222e-06,
      "loss": 0.0067,
      "step": 1118
    },
    {
      "epoch": 3.948831054256727,
      "grad_norm": 0.11391306487838866,
      "learning_rate": 7.116564417177915e-06,
      "loss": 0.0155,
      "step": 1119
    },
    {
      "epoch": 3.9523599470666078,
      "grad_norm": 0.03758951352858502,
      "learning_rate": 7.104294478527608e-06,
      "loss": 0.0039,
      "step": 1120
    },
    {
      "epoch": 3.9523599470666078,
      "eval_average_f1": 0.8227441474669425,
      "eval_crossner_ai_f1": 0.8571428570902494,
      "eval_crossner_ai_precision": 0.8181818181793388,
      "eval_crossner_ai_recall": 0.8999999999970001,
      "eval_crossner_literature_f1": 0.6938775509680968,
      "eval_crossner_literature_precision": 0.7727272727237604,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9370629370116289,
      "eval_crossner_music_precision": 0.9305555555542631,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7093023255305773,
      "eval_crossner_politics_precision": 0.7011494252865504,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.7692307691776463,
      "eval_crossner_science_precision": 0.8823529411712804,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9333333332823879,
      "eval_mit-movie_precision": 0.9479166666656792,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8592592592080108,
      "eval_mit-restaurant_precision": 0.8787878787865473,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 15.906,
      "eval_samples_per_second": 8.047,
      "eval_steps_per_second": 0.251,
      "step": 1120
    },
    {
      "epoch": 3.9558888398764886,
      "grad_norm": 0.09453007386003982,
      "learning_rate": 7.092024539877301e-06,
      "loss": 0.0056,
      "step": 1121
    },
    {
      "epoch": 3.9594177326863695,
      "grad_norm": 0.17394372760728288,
      "learning_rate": 7.079754601226995e-06,
      "loss": 0.0041,
      "step": 1122
    },
    {
      "epoch": 3.9629466254962504,
      "grad_norm": 0.15862668991954748,
      "learning_rate": 7.067484662576688e-06,
      "loss": 0.0128,
      "step": 1123
    },
    {
      "epoch": 3.9664755183061313,
      "grad_norm": 0.22994333927239896,
      "learning_rate": 7.055214723926381e-06,
      "loss": 0.014,
      "step": 1124
    },
    {
      "epoch": 3.970004411116012,
      "grad_norm": 0.12241307772910359,
      "learning_rate": 7.042944785276074e-06,
      "loss": 0.0149,
      "step": 1125
    },
    {
      "epoch": 3.973533303925893,
      "grad_norm": 0.1823517609497889,
      "learning_rate": 7.030674846625768e-06,
      "loss": 0.0116,
      "step": 1126
    },
    {
      "epoch": 3.977062196735774,
      "grad_norm": 0.18141981259820472,
      "learning_rate": 7.01840490797546e-06,
      "loss": 0.0141,
      "step": 1127
    },
    {
      "epoch": 3.980591089545655,
      "grad_norm": 0.1078266191566722,
      "learning_rate": 7.006134969325154e-06,
      "loss": 0.0103,
      "step": 1128
    },
    {
      "epoch": 3.980591089545655,
      "eval_average_f1": 0.8156829031818813,
      "eval_crossner_ai_f1": 0.8571428570902494,
      "eval_crossner_ai_precision": 0.8181818181793388,
      "eval_crossner_ai_recall": 0.8999999999970001,
      "eval_crossner_literature_f1": 0.5882352940655133,
      "eval_crossner_literature_precision": 0.6249999999973959,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.8920863308839915,
      "eval_crossner_music_precision": 0.9117647058810121,
      "eval_crossner_music_recall": 0.8732394366184884,
      "eval_crossner_politics_f1": 0.7647058823020415,
      "eval_crossner_politics_precision": 0.7647058823520415,
      "eval_crossner_politics_recall": 0.7647058823520415,
      "eval_crossner_science_f1": 0.7804878048245093,
      "eval_crossner_science_precision": 0.8421052631534627,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9285714285204862,
      "eval_mit-movie_precision": 0.93814432989594,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8985507245863789,
      "eval_mit-restaurant_precision": 0.8985507246363789,
      "eval_mit-restaurant_recall": 0.8985507246363789,
      "eval_runtime": 15.6564,
      "eval_samples_per_second": 8.176,
      "eval_steps_per_second": 0.255,
      "step": 1128
    },
    {
      "epoch": 3.984119982355536,
      "grad_norm": 0.18984702543093937,
      "learning_rate": 6.993865030674847e-06,
      "loss": 0.0157,
      "step": 1129
    },
    {
      "epoch": 3.9876488751654167,
      "grad_norm": 0.0943229077899296,
      "learning_rate": 6.98159509202454e-06,
      "loss": 0.0161,
      "step": 1130
    },
    {
      "epoch": 3.991177767975298,
      "grad_norm": 0.1369015289316352,
      "learning_rate": 6.969325153374233e-06,
      "loss": 0.0087,
      "step": 1131
    },
    {
      "epoch": 3.9947066607851784,
      "grad_norm": 0.07416683815930038,
      "learning_rate": 6.957055214723927e-06,
      "loss": 0.0141,
      "step": 1132
    },
    {
      "epoch": 3.9982355535950598,
      "grad_norm": 0.1357928874337059,
      "learning_rate": 6.94478527607362e-06,
      "loss": 0.0092,
      "step": 1133
    },
    {
      "epoch": 4.00176444640494,
      "grad_norm": 0.12908024316104233,
      "learning_rate": 6.932515337423313e-06,
      "loss": 0.0111,
      "step": 1134
    },
    {
      "epoch": 4.005293339214822,
      "grad_norm": 0.08146734499972721,
      "learning_rate": 6.9202453987730065e-06,
      "loss": 0.0045,
      "step": 1135
    },
    {
      "epoch": 4.008822232024702,
      "grad_norm": 0.05757142730963404,
      "learning_rate": 6.9079754601227004e-06,
      "loss": 0.0048,
      "step": 1136
    },
    {
      "epoch": 4.008822232024702,
      "eval_average_f1": 0.8000045195684263,
      "eval_crossner_ai_f1": 0.8437499999475586,
      "eval_crossner_ai_precision": 0.7941176470564878,
      "eval_crossner_ai_recall": 0.8999999999970001,
      "eval_crossner_literature_f1": 0.6415094339098613,
      "eval_crossner_literature_precision": 0.6538461538436391,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9379310344314862,
      "eval_crossner_music_precision": 0.9189189189176771,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7305389221048298,
      "eval_crossner_politics_precision": 0.7439024390234831,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.6521739129907372,
      "eval_crossner_science_precision": 0.6249999999973959,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9595959595449903,
      "eval_mit-movie_precision": 0.9595959595949903,
      "eval_mit-movie_recall": 0.9595959595949903,
      "eval_mit-restaurant_f1": 0.8345323740495212,
      "eval_mit-restaurant_precision": 0.8285714285702449,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 16.7097,
      "eval_samples_per_second": 7.66,
      "eval_steps_per_second": 0.239,
      "step": 1136
    },
    {
      "epoch": 4.012351124834583,
      "grad_norm": 0.43234183889724215,
      "learning_rate": 6.8957055214723936e-06,
      "loss": 0.0076,
      "step": 1137
    },
    {
      "epoch": 4.015880017644464,
      "grad_norm": 0.1038287393538854,
      "learning_rate": 6.883435582822087e-06,
      "loss": 0.0074,
      "step": 1138
    },
    {
      "epoch": 4.019408910454345,
      "grad_norm": 0.08752524669469304,
      "learning_rate": 6.87116564417178e-06,
      "loss": 0.0064,
      "step": 1139
    },
    {
      "epoch": 4.022937803264226,
      "grad_norm": 0.21130089011422326,
      "learning_rate": 6.858895705521473e-06,
      "loss": 0.0051,
      "step": 1140
    },
    {
      "epoch": 4.026466696074107,
      "grad_norm": 0.2239372147354153,
      "learning_rate": 6.846625766871167e-06,
      "loss": 0.0062,
      "step": 1141
    },
    {
      "epoch": 4.029995588883987,
      "grad_norm": 0.061010351862885746,
      "learning_rate": 6.83435582822086e-06,
      "loss": 0.003,
      "step": 1142
    },
    {
      "epoch": 4.033524481693869,
      "grad_norm": 0.07665408165061922,
      "learning_rate": 6.822085889570552e-06,
      "loss": 0.0059,
      "step": 1143
    },
    {
      "epoch": 4.037053374503749,
      "grad_norm": 0.07118726346765362,
      "learning_rate": 6.8098159509202454e-06,
      "loss": 0.0057,
      "step": 1144
    },
    {
      "epoch": 4.037053374503749,
      "eval_average_f1": 0.7847139756096063,
      "eval_crossner_ai_f1": 0.7666666666141112,
      "eval_crossner_ai_precision": 0.7666666666641112,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6274509803398693,
      "eval_crossner_literature_precision": 0.666666666663889,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9370629370116289,
      "eval_crossner_music_precision": 0.9305555555542631,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7393939393430891,
      "eval_crossner_politics_precision": 0.7624999999990468,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.6521739129907372,
      "eval_crossner_science_precision": 0.6249999999973959,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.944162436497266,
      "eval_mit-movie_precision": 0.9489795918357663,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8260869564705419,
      "eval_mit-restaurant_precision": 0.8260869565205419,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 16.4566,
      "eval_samples_per_second": 7.778,
      "eval_steps_per_second": 0.243,
      "step": 1144
    },
    {
      "epoch": 4.0405822673136305,
      "grad_norm": 0.05465205781136722,
      "learning_rate": 6.7975460122699386e-06,
      "loss": 0.0054,
      "step": 1145
    },
    {
      "epoch": 4.044111160123511,
      "grad_norm": 0.04124556669566303,
      "learning_rate": 6.7852760736196325e-06,
      "loss": 0.0031,
      "step": 1146
    },
    {
      "epoch": 4.047640052933392,
      "grad_norm": 0.08803272680713543,
      "learning_rate": 6.773006134969326e-06,
      "loss": 0.0062,
      "step": 1147
    },
    {
      "epoch": 4.051168945743273,
      "grad_norm": 0.07995361732087557,
      "learning_rate": 6.760736196319019e-06,
      "loss": 0.0065,
      "step": 1148
    },
    {
      "epoch": 4.054697838553154,
      "grad_norm": 0.07765274062468554,
      "learning_rate": 6.748466257668712e-06,
      "loss": 0.0035,
      "step": 1149
    },
    {
      "epoch": 4.0582267313630345,
      "grad_norm": 0.18576709327627663,
      "learning_rate": 6.736196319018405e-06,
      "loss": 0.0049,
      "step": 1150
    },
    {
      "epoch": 4.061755624172916,
      "grad_norm": 0.29424862088566534,
      "learning_rate": 6.723926380368099e-06,
      "loss": 0.0067,
      "step": 1151
    },
    {
      "epoch": 4.065284516982796,
      "grad_norm": 0.14186218218521174,
      "learning_rate": 6.711656441717792e-06,
      "loss": 0.0077,
      "step": 1152
    },
    {
      "epoch": 4.065284516982796,
      "eval_average_f1": 0.7762404739649666,
      "eval_crossner_ai_f1": 0.7666666666141112,
      "eval_crossner_ai_precision": 0.7666666666641112,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6274509803398693,
      "eval_crossner_literature_precision": 0.666666666663889,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9370629370116289,
      "eval_crossner_music_precision": 0.9305555555542631,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7195121950711407,
      "eval_crossner_politics_precision": 0.7468354430370293,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.590909090856405,
      "eval_crossner_science_precision": 0.590909090906405,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9538461537951873,
      "eval_mit-movie_precision": 0.9687499999989909,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8382352940664252,
      "eval_mit-restaurant_precision": 0.8507462686554467,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 16.1029,
      "eval_samples_per_second": 7.949,
      "eval_steps_per_second": 0.248,
      "step": 1152
    },
    {
      "epoch": 4.068813409792678,
      "grad_norm": 0.05783924655228578,
      "learning_rate": 6.699386503067485e-06,
      "loss": 0.0069,
      "step": 1153
    },
    {
      "epoch": 4.072342302602558,
      "grad_norm": 0.11433932257992212,
      "learning_rate": 6.687116564417178e-06,
      "loss": 0.0054,
      "step": 1154
    },
    {
      "epoch": 4.075871195412439,
      "grad_norm": 0.055864162673382387,
      "learning_rate": 6.674846625766872e-06,
      "loss": 0.0059,
      "step": 1155
    },
    {
      "epoch": 4.07940008822232,
      "grad_norm": 0.0955373681252181,
      "learning_rate": 6.6625766871165655e-06,
      "loss": 0.0081,
      "step": 1156
    },
    {
      "epoch": 4.082928981032201,
      "grad_norm": 0.10916872242440019,
      "learning_rate": 6.650306748466259e-06,
      "loss": 0.0062,
      "step": 1157
    },
    {
      "epoch": 4.0864578738420825,
      "grad_norm": 0.1294264484123862,
      "learning_rate": 6.638036809815952e-06,
      "loss": 0.0071,
      "step": 1158
    },
    {
      "epoch": 4.089986766651963,
      "grad_norm": 0.18758181932553786,
      "learning_rate": 6.625766871165644e-06,
      "loss": 0.0104,
      "step": 1159
    },
    {
      "epoch": 4.093515659461844,
      "grad_norm": 0.16101231548937547,
      "learning_rate": 6.613496932515337e-06,
      "loss": 0.006,
      "step": 1160
    },
    {
      "epoch": 4.093515659461844,
      "eval_average_f1": 0.7913971507748178,
      "eval_crossner_ai_f1": 0.7999999999478343,
      "eval_crossner_ai_precision": 0.7428571428550204,
      "eval_crossner_ai_recall": 0.8666666666637778,
      "eval_crossner_literature_f1": 0.6274509803398693,
      "eval_crossner_literature_precision": 0.666666666663889,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.714285714234871,
      "eval_crossner_politics_precision": 0.7228915662641893,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.6511627906446729,
      "eval_crossner_science_precision": 0.6666666666634922,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9494949494439904,
      "eval_mit-movie_precision": 0.9494949494939904,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8529411764193447,
      "eval_mit-restaurant_precision": 0.8656716417897528,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 16.0427,
      "eval_samples_per_second": 7.979,
      "eval_steps_per_second": 0.249,
      "step": 1160
    },
    {
      "epoch": 4.097044552271725,
      "grad_norm": 0.027378001218933308,
      "learning_rate": 6.601226993865031e-06,
      "loss": 0.0034,
      "step": 1161
    },
    {
      "epoch": 4.100573445081606,
      "grad_norm": 0.06363301436952497,
      "learning_rate": 6.588957055214724e-06,
      "loss": 0.0029,
      "step": 1162
    },
    {
      "epoch": 4.1041023378914865,
      "grad_norm": 0.23172713074862525,
      "learning_rate": 6.576687116564417e-06,
      "loss": 0.0066,
      "step": 1163
    },
    {
      "epoch": 4.107631230701368,
      "grad_norm": 0.1615309507105603,
      "learning_rate": 6.5644171779141105e-06,
      "loss": 0.0064,
      "step": 1164
    },
    {
      "epoch": 4.111160123511248,
      "grad_norm": 0.19483265105621053,
      "learning_rate": 6.5521472392638045e-06,
      "loss": 0.0089,
      "step": 1165
    },
    {
      "epoch": 4.11468901632113,
      "grad_norm": 0.08799255341669933,
      "learning_rate": 6.539877300613498e-06,
      "loss": 0.0062,
      "step": 1166
    },
    {
      "epoch": 4.11821790913101,
      "grad_norm": 0.08964661033280935,
      "learning_rate": 6.527607361963191e-06,
      "loss": 0.0034,
      "step": 1167
    },
    {
      "epoch": 4.121746801940891,
      "grad_norm": 0.07269294476125013,
      "learning_rate": 6.515337423312884e-06,
      "loss": 0.0046,
      "step": 1168
    },
    {
      "epoch": 4.121746801940891,
      "eval_average_f1": 0.8070914660522787,
      "eval_crossner_ai_f1": 0.7540983606032787,
      "eval_crossner_ai_precision": 0.7419354838685744,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9510489509976233,
      "eval_crossner_music_precision": 0.9444444444431327,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.725146198779563,
      "eval_crossner_politics_precision": 0.7209302325573013,
      "eval_crossner_politics_recall": 0.7294117647050242,
      "eval_crossner_science_f1": 0.7272727272194216,
      "eval_crossner_science_precision": 0.7272727272694215,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9494949494439904,
      "eval_mit-movie_precision": 0.9494949494939904,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.875912408707848,
      "eval_mit-restaurant_precision": 0.8823529411751729,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.4355,
      "eval_samples_per_second": 7.788,
      "eval_steps_per_second": 0.243,
      "step": 1168
    },
    {
      "epoch": 4.125275694750772,
      "grad_norm": 0.057431471275802504,
      "learning_rate": 6.503067484662578e-06,
      "loss": 0.003,
      "step": 1169
    },
    {
      "epoch": 4.128804587560653,
      "grad_norm": 0.0316317751952979,
      "learning_rate": 6.490797546012271e-06,
      "loss": 0.002,
      "step": 1170
    },
    {
      "epoch": 4.132333480370534,
      "grad_norm": 0.04460628455809321,
      "learning_rate": 6.478527607361964e-06,
      "loss": 0.0034,
      "step": 1171
    },
    {
      "epoch": 4.135862373180415,
      "grad_norm": 0.06509310350197771,
      "learning_rate": 6.466257668711657e-06,
      "loss": 0.005,
      "step": 1172
    },
    {
      "epoch": 4.139391265990295,
      "grad_norm": 0.29063134135229274,
      "learning_rate": 6.45398773006135e-06,
      "loss": 0.006,
      "step": 1173
    },
    {
      "epoch": 4.142920158800177,
      "grad_norm": 0.13691465102870554,
      "learning_rate": 6.441717791411043e-06,
      "loss": 0.0101,
      "step": 1174
    },
    {
      "epoch": 4.146449051610057,
      "grad_norm": 0.10415550162163535,
      "learning_rate": 6.4294478527607365e-06,
      "loss": 0.0062,
      "step": 1175
    },
    {
      "epoch": 4.1499779444199385,
      "grad_norm": 0.15066212138319315,
      "learning_rate": 6.41717791411043e-06,
      "loss": 0.0038,
      "step": 1176
    },
    {
      "epoch": 4.1499779444199385,
      "eval_average_f1": 0.7931139997695659,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7195121950711407,
      "eval_crossner_politics_precision": 0.7468354430370293,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.6829268292152292,
      "eval_crossner_science_precision": 0.7368421052592798,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9743589743079869,
      "eval_mit-movie_precision": 0.9895833333323025,
      "eval_mit-movie_recall": 0.9595959595949903,
      "eval_mit-restaurant_f1": 0.8444444443932181,
      "eval_mit-restaurant_precision": 0.863636363635055,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 16.3455,
      "eval_samples_per_second": 7.831,
      "eval_steps_per_second": 0.245,
      "step": 1176
    },
    {
      "epoch": 4.153506837229819,
      "grad_norm": 0.12280382265856271,
      "learning_rate": 6.404907975460123e-06,
      "loss": 0.005,
      "step": 1177
    },
    {
      "epoch": 4.1570357300397,
      "grad_norm": 0.15485746818502433,
      "learning_rate": 6.392638036809816e-06,
      "loss": 0.0103,
      "step": 1178
    },
    {
      "epoch": 4.160564622849581,
      "grad_norm": 0.11012364371630397,
      "learning_rate": 6.38036809815951e-06,
      "loss": 0.0107,
      "step": 1179
    },
    {
      "epoch": 4.164093515659462,
      "grad_norm": 0.19948445160407832,
      "learning_rate": 6.368098159509203e-06,
      "loss": 0.0057,
      "step": 1180
    },
    {
      "epoch": 4.1676224084693425,
      "grad_norm": 0.11149000227826668,
      "learning_rate": 6.355828220858896e-06,
      "loss": 0.0125,
      "step": 1181
    },
    {
      "epoch": 4.171151301279224,
      "grad_norm": 0.06018589840423757,
      "learning_rate": 6.343558282208589e-06,
      "loss": 0.0074,
      "step": 1182
    },
    {
      "epoch": 4.174680194089104,
      "grad_norm": 0.08392542056191936,
      "learning_rate": 6.331288343558282e-06,
      "loss": 0.0022,
      "step": 1183
    },
    {
      "epoch": 4.178209086898986,
      "grad_norm": 0.10675346322382792,
      "learning_rate": 6.319018404907976e-06,
      "loss": 0.0059,
      "step": 1184
    },
    {
      "epoch": 4.178209086898986,
      "eval_average_f1": 0.7856087779754405,
      "eval_crossner_ai_f1": 0.8253968253443184,
      "eval_crossner_ai_precision": 0.7878787878764003,
      "eval_crossner_ai_recall": 0.8666666666637778,
      "eval_crossner_literature_f1": 0.6274509803398693,
      "eval_crossner_literature_precision": 0.666666666663889,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9370629370116289,
      "eval_crossner_music_precision": 0.9305555555542631,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7398843930127434,
      "eval_crossner_politics_precision": 0.7272727272719008,
      "eval_crossner_politics_recall": 0.7529411764697024,
      "eval_crossner_science_f1": 0.565217391251985,
      "eval_crossner_science_precision": 0.5416666666644098,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9746192892891133,
      "eval_mit-movie_precision": 0.9795918367336943,
      "eval_mit-movie_recall": 0.9696969696959902,
      "eval_mit-restaurant_f1": 0.8296296295784252,
      "eval_mit-restaurant_precision": 0.8484848484835629,
      "eval_mit-restaurant_recall": 0.8115942028973745,
      "eval_runtime": 16.1184,
      "eval_samples_per_second": 7.941,
      "eval_steps_per_second": 0.248,
      "step": 1184
    },
    {
      "epoch": 4.181737979708866,
      "grad_norm": 0.03974135426843342,
      "learning_rate": 6.3067484662576695e-06,
      "loss": 0.0033,
      "step": 1185
    },
    {
      "epoch": 4.185266872518747,
      "grad_norm": 0.1236683990161122,
      "learning_rate": 6.294478527607363e-06,
      "loss": 0.0103,
      "step": 1186
    },
    {
      "epoch": 4.188795765328628,
      "grad_norm": 0.08045749238347821,
      "learning_rate": 6.282208588957056e-06,
      "loss": 0.0029,
      "step": 1187
    },
    {
      "epoch": 4.192324658138509,
      "grad_norm": 0.09814610646060799,
      "learning_rate": 6.26993865030675e-06,
      "loss": 0.0064,
      "step": 1188
    },
    {
      "epoch": 4.19585355094839,
      "grad_norm": 0.14111555157059386,
      "learning_rate": 6.257668711656443e-06,
      "loss": 0.0049,
      "step": 1189
    },
    {
      "epoch": 4.199382443758271,
      "grad_norm": 0.1439450723343273,
      "learning_rate": 6.245398773006135e-06,
      "loss": 0.008,
      "step": 1190
    },
    {
      "epoch": 4.202911336568151,
      "grad_norm": 0.06211097538283715,
      "learning_rate": 6.233128834355828e-06,
      "loss": 0.0047,
      "step": 1191
    },
    {
      "epoch": 4.206440229378033,
      "grad_norm": 0.09923188824661905,
      "learning_rate": 6.220858895705521e-06,
      "loss": 0.0058,
      "step": 1192
    },
    {
      "epoch": 4.206440229378033,
      "eval_average_f1": 0.7758997027429676,
      "eval_crossner_ai_f1": 0.8333333332805556,
      "eval_crossner_ai_precision": 0.8333333333305556,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.63999999994776,
      "eval_crossner_literature_precision": 0.695652173910019,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7218934910734078,
      "eval_crossner_politics_precision": 0.7261904761896116,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.48888888883674075,
      "eval_crossner_science_precision": 0.478260869563138,
      "eval_crossner_science_recall": 0.4999999999977273,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8676470587722643,
      "eval_mit-restaurant_precision": 0.8805970149240588,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.4313,
      "eval_samples_per_second": 7.79,
      "eval_steps_per_second": 0.243,
      "step": 1192
    },
    {
      "epoch": 4.209969122187913,
      "grad_norm": 0.046139010538714435,
      "learning_rate": 6.208588957055215e-06,
      "loss": 0.0028,
      "step": 1193
    },
    {
      "epoch": 4.2134980149977945,
      "grad_norm": 0.09789804565208798,
      "learning_rate": 6.1963190184049085e-06,
      "loss": 0.0055,
      "step": 1194
    },
    {
      "epoch": 4.217026907807675,
      "grad_norm": 0.13808658820920264,
      "learning_rate": 6.184049079754602e-06,
      "loss": 0.0043,
      "step": 1195
    },
    {
      "epoch": 4.220555800617556,
      "grad_norm": 0.11021728701522307,
      "learning_rate": 6.171779141104295e-06,
      "loss": 0.0068,
      "step": 1196
    },
    {
      "epoch": 4.224084693427437,
      "grad_norm": 0.09355207252431587,
      "learning_rate": 6.159509202453988e-06,
      "loss": 0.0027,
      "step": 1197
    },
    {
      "epoch": 4.227613586237318,
      "grad_norm": 0.1285793320463225,
      "learning_rate": 6.147239263803682e-06,
      "loss": 0.0046,
      "step": 1198
    },
    {
      "epoch": 4.2311424790471985,
      "grad_norm": 0.0757903054322917,
      "learning_rate": 6.134969325153375e-06,
      "loss": 0.0064,
      "step": 1199
    },
    {
      "epoch": 4.23467137185708,
      "grad_norm": 0.1305546629024937,
      "learning_rate": 6.122699386503068e-06,
      "loss": 0.0059,
      "step": 1200
    },
    {
      "epoch": 4.23467137185708,
      "eval_average_f1": 0.7697185100573203,
      "eval_crossner_ai_f1": 0.7666666666141112,
      "eval_crossner_ai_precision": 0.7666666666641112,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6122448979072054,
      "eval_crossner_literature_precision": 0.6818181818150827,
      "eval_crossner_literature_recall": 0.555555555553498,
      "eval_crossner_music_f1": 0.9230769230256345,
      "eval_crossner_music_precision": 0.9166666666653935,
      "eval_crossner_music_recall": 0.9295774647874231,
      "eval_crossner_politics_f1": 0.7558139534375,
      "eval_crossner_politics_precision": 0.7471264367807504,
      "eval_crossner_politics_recall": 0.7647058823520415,
      "eval_crossner_science_f1": 0.49999999994772737,
      "eval_crossner_science_precision": 0.4999999999977273,
      "eval_crossner_science_recall": 0.4999999999977273,
      "eval_mit-movie_f1": 0.9543147207612152,
      "eval_mit-movie_precision": 0.9591836734684089,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.875912408707848,
      "eval_mit-restaurant_precision": 0.8823529411751729,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 15.898,
      "eval_samples_per_second": 8.051,
      "eval_steps_per_second": 0.252,
      "step": 1200
    },
    {
      "epoch": 4.23820026466696,
      "grad_norm": 0.07121416952464657,
      "learning_rate": 6.110429447852761e-06,
      "loss": 0.0053,
      "step": 1201
    },
    {
      "epoch": 4.241729157476842,
      "grad_norm": 0.08473872305015835,
      "learning_rate": 6.098159509202455e-06,
      "loss": 0.0081,
      "step": 1202
    },
    {
      "epoch": 4.245258050286722,
      "grad_norm": 0.08470658342429102,
      "learning_rate": 6.085889570552148e-06,
      "loss": 0.0039,
      "step": 1203
    },
    {
      "epoch": 4.248786943096603,
      "grad_norm": 0.09969250164967504,
      "learning_rate": 6.073619631901841e-06,
      "loss": 0.0042,
      "step": 1204
    },
    {
      "epoch": 4.252315835906485,
      "grad_norm": 0.059672865377939116,
      "learning_rate": 6.0613496932515345e-06,
      "loss": 0.0033,
      "step": 1205
    },
    {
      "epoch": 4.255844728716365,
      "grad_norm": 0.04115906678128461,
      "learning_rate": 6.049079754601227e-06,
      "loss": 0.0034,
      "step": 1206
    },
    {
      "epoch": 4.2593736215262465,
      "grad_norm": 0.11930989529142956,
      "learning_rate": 6.03680981595092e-06,
      "loss": 0.01,
      "step": 1207
    },
    {
      "epoch": 4.262902514336127,
      "grad_norm": 0.0347266037150118,
      "learning_rate": 6.024539877300614e-06,
      "loss": 0.0026,
      "step": 1208
    },
    {
      "epoch": 4.262902514336127,
      "eval_average_f1": 0.7905572687663819,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9370629370116289,
      "eval_crossner_music_precision": 0.9305555555542631,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7272727272218916,
      "eval_crossner_politics_precision": 0.7499999999990625,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9333333332823879,
      "eval_mit-movie_precision": 0.9479166666656792,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8695652173400441,
      "eval_mit-restaurant_precision": 0.8695652173900441,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 15.9902,
      "eval_samples_per_second": 8.005,
      "eval_steps_per_second": 0.25,
      "step": 1208
    },
    {
      "epoch": 4.266431407146008,
      "grad_norm": 0.05354951530260783,
      "learning_rate": 6.012269938650307e-06,
      "loss": 0.0045,
      "step": 1209
    },
    {
      "epoch": 4.269960299955889,
      "grad_norm": 0.190153851396013,
      "learning_rate": 6e-06,
      "loss": 0.0075,
      "step": 1210
    },
    {
      "epoch": 4.27348919276577,
      "grad_norm": 0.06063585703084576,
      "learning_rate": 5.987730061349693e-06,
      "loss": 0.0027,
      "step": 1211
    },
    {
      "epoch": 4.2770180855756506,
      "grad_norm": 0.11364188011553647,
      "learning_rate": 5.975460122699387e-06,
      "loss": 0.0106,
      "step": 1212
    },
    {
      "epoch": 4.280546978385532,
      "grad_norm": 0.1446126695646119,
      "learning_rate": 5.96319018404908e-06,
      "loss": 0.0089,
      "step": 1213
    },
    {
      "epoch": 4.284075871195412,
      "grad_norm": 0.09243171397678308,
      "learning_rate": 5.9509202453987735e-06,
      "loss": 0.0033,
      "step": 1214
    },
    {
      "epoch": 4.287604764005294,
      "grad_norm": 0.16281679639254504,
      "learning_rate": 5.938650306748467e-06,
      "loss": 0.0044,
      "step": 1215
    },
    {
      "epoch": 4.291133656815174,
      "grad_norm": 0.14142691266329444,
      "learning_rate": 5.926380368098161e-06,
      "loss": 0.0041,
      "step": 1216
    },
    {
      "epoch": 4.291133656815174,
      "eval_average_f1": 0.7672240165824448,
      "eval_crossner_ai_f1": 0.7936507935983874,
      "eval_crossner_ai_precision": 0.7575757575734618,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.653846153793713,
      "eval_crossner_literature_precision": 0.67999999999728,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9370629370116289,
      "eval_crossner_music_precision": 0.9305555555542631,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.694610778392298,
      "eval_crossner_politics_precision": 0.7073170731698691,
      "eval_crossner_politics_recall": 0.6823529411756678,
      "eval_crossner_science_f1": 0.558139534831152,
      "eval_crossner_science_precision": 0.5714285714258504,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.9538461537951873,
      "eval_mit-movie_precision": 0.9687499999989909,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.779411764654747,
      "eval_mit-restaurant_precision": 0.7910447761182223,
      "eval_mit-restaurant_recall": 0.7681159420278723,
      "eval_runtime": 15.8819,
      "eval_samples_per_second": 8.06,
      "eval_steps_per_second": 0.252,
      "step": 1216
    },
    {
      "epoch": 4.2946625496250554,
      "grad_norm": 0.12644425891087271,
      "learning_rate": 5.914110429447854e-06,
      "loss": 0.0058,
      "step": 1217
    },
    {
      "epoch": 4.298191442434936,
      "grad_norm": 0.07875182696524158,
      "learning_rate": 5.901840490797547e-06,
      "loss": 0.0072,
      "step": 1218
    },
    {
      "epoch": 4.301720335244817,
      "grad_norm": 0.09621309142464354,
      "learning_rate": 5.88957055214724e-06,
      "loss": 0.0034,
      "step": 1219
    },
    {
      "epoch": 4.305249228054698,
      "grad_norm": 0.12186371295577486,
      "learning_rate": 5.877300613496933e-06,
      "loss": 0.0051,
      "step": 1220
    },
    {
      "epoch": 4.308778120864579,
      "grad_norm": 0.11216716224052331,
      "learning_rate": 5.865030674846625e-06,
      "loss": 0.0057,
      "step": 1221
    },
    {
      "epoch": 4.3123070136744595,
      "grad_norm": 0.07073177394757153,
      "learning_rate": 5.852760736196319e-06,
      "loss": 0.0058,
      "step": 1222
    },
    {
      "epoch": 4.315835906484341,
      "grad_norm": 0.1042167384283796,
      "learning_rate": 5.8404907975460125e-06,
      "loss": 0.0088,
      "step": 1223
    },
    {
      "epoch": 4.319364799294221,
      "grad_norm": 0.09239444619090469,
      "learning_rate": 5.828220858895706e-06,
      "loss": 0.007,
      "step": 1224
    },
    {
      "epoch": 4.319364799294221,
      "eval_average_f1": 0.7673316352617949,
      "eval_crossner_ai_f1": 0.7457627118118931,
      "eval_crossner_ai_precision": 0.7586206896525566,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6274509803398693,
      "eval_crossner_literature_precision": 0.666666666663889,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9370629370116289,
      "eval_crossner_music_precision": 0.9305555555542631,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7065868262964754,
      "eval_crossner_politics_precision": 0.7195121951210738,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.5853658536059488,
      "eval_crossner_science_precision": 0.631578947365097,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.9430051812962067,
      "eval_mit-movie_precision": 0.9680851063819488,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8260869564705419,
      "eval_mit-restaurant_precision": 0.8260869565205419,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 16.4009,
      "eval_samples_per_second": 7.804,
      "eval_steps_per_second": 0.244,
      "step": 1224
    },
    {
      "epoch": 4.322893692104103,
      "grad_norm": 0.30249171222020954,
      "learning_rate": 5.815950920245399e-06,
      "loss": 0.008,
      "step": 1225
    },
    {
      "epoch": 4.326422584913983,
      "grad_norm": 0.2848861538685709,
      "learning_rate": 5.803680981595093e-06,
      "loss": 0.0076,
      "step": 1226
    },
    {
      "epoch": 4.329951477723864,
      "grad_norm": 0.07192972124123179,
      "learning_rate": 5.791411042944786e-06,
      "loss": 0.0044,
      "step": 1227
    },
    {
      "epoch": 4.333480370533745,
      "grad_norm": 0.07131452954699077,
      "learning_rate": 5.779141104294479e-06,
      "loss": 0.0029,
      "step": 1228
    },
    {
      "epoch": 4.337009263343626,
      "grad_norm": 0.10982325778536009,
      "learning_rate": 5.766871165644172e-06,
      "loss": 0.007,
      "step": 1229
    },
    {
      "epoch": 4.340538156153507,
      "grad_norm": 0.12220172293804588,
      "learning_rate": 5.754601226993865e-06,
      "loss": 0.0033,
      "step": 1230
    },
    {
      "epoch": 4.344067048963388,
      "grad_norm": 0.0867649417148218,
      "learning_rate": 5.742331288343559e-06,
      "loss": 0.0051,
      "step": 1231
    },
    {
      "epoch": 4.347595941773268,
      "grad_norm": 0.12821286288280073,
      "learning_rate": 5.730061349693252e-06,
      "loss": 0.0034,
      "step": 1232
    },
    {
      "epoch": 4.347595941773268,
      "eval_average_f1": 0.8006097478221973,
      "eval_crossner_ai_f1": 0.7457627118118931,
      "eval_crossner_ai_precision": 0.7586206896525566,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9370629370116289,
      "eval_crossner_music_precision": 0.9305555555542631,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7393939393430891,
      "eval_crossner_politics_precision": 0.7624999999990468,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.7619047618512471,
      "eval_crossner_science_precision": 0.799999999996,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9333333332823879,
      "eval_mit-movie_precision": 0.9479166666656792,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8201438848409087,
      "eval_mit-restaurant_precision": 0.814285714284551,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 15.8839,
      "eval_samples_per_second": 8.058,
      "eval_steps_per_second": 0.252,
      "step": 1232
    },
    {
      "epoch": 4.35112483458315,
      "grad_norm": 0.134409274398721,
      "learning_rate": 5.717791411042945e-06,
      "loss": 0.0078,
      "step": 1233
    },
    {
      "epoch": 4.35465372739303,
      "grad_norm": 0.25732666535634957,
      "learning_rate": 5.7055214723926385e-06,
      "loss": 0.0072,
      "step": 1234
    },
    {
      "epoch": 4.3581826202029115,
      "grad_norm": 0.11693967204163994,
      "learning_rate": 5.6932515337423325e-06,
      "loss": 0.004,
      "step": 1235
    },
    {
      "epoch": 4.361711513012792,
      "grad_norm": 0.10809343403695328,
      "learning_rate": 5.680981595092026e-06,
      "loss": 0.0052,
      "step": 1236
    },
    {
      "epoch": 4.365240405822673,
      "grad_norm": 0.03556233884522641,
      "learning_rate": 5.668711656441718e-06,
      "loss": 0.0043,
      "step": 1237
    },
    {
      "epoch": 4.368769298632554,
      "grad_norm": 0.07780616790024988,
      "learning_rate": 5.656441717791411e-06,
      "loss": 0.0059,
      "step": 1238
    },
    {
      "epoch": 4.372298191442435,
      "grad_norm": 0.07499064111431629,
      "learning_rate": 5.644171779141104e-06,
      "loss": 0.006,
      "step": 1239
    },
    {
      "epoch": 4.3758270842523155,
      "grad_norm": 0.11625995323553732,
      "learning_rate": 5.631901840490797e-06,
      "loss": 0.0049,
      "step": 1240
    },
    {
      "epoch": 4.3758270842523155,
      "eval_average_f1": 0.8044612616546433,
      "eval_crossner_ai_f1": 0.7457627118118931,
      "eval_crossner_ai_precision": 0.7586206896525566,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9370629370116289,
      "eval_crossner_music_precision": 0.9305555555542631,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7218934910734078,
      "eval_crossner_politics_precision": 0.7261904761896116,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.7804878048245093,
      "eval_crossner_science_precision": 0.8421052631534627,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9387755101531289,
      "eval_mit-movie_precision": 0.9484536082464449,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8405797100937092,
      "eval_mit-restaurant_precision": 0.8405797101437092,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 15.9297,
      "eval_samples_per_second": 8.035,
      "eval_steps_per_second": 0.251,
      "step": 1240
    },
    {
      "epoch": 4.379355977062197,
      "grad_norm": 0.16819465217210874,
      "learning_rate": 5.619631901840491e-06,
      "loss": 0.0038,
      "step": 1241
    },
    {
      "epoch": 4.382884869872077,
      "grad_norm": 0.16308997486165241,
      "learning_rate": 5.607361963190184e-06,
      "loss": 0.0106,
      "step": 1242
    },
    {
      "epoch": 4.386413762681959,
      "grad_norm": 0.10970494198214685,
      "learning_rate": 5.5950920245398775e-06,
      "loss": 0.008,
      "step": 1243
    },
    {
      "epoch": 4.389942655491839,
      "grad_norm": 0.21947304405401516,
      "learning_rate": 5.582822085889571e-06,
      "loss": 0.0096,
      "step": 1244
    },
    {
      "epoch": 4.39347154830172,
      "grad_norm": 0.09308456151748101,
      "learning_rate": 5.570552147239265e-06,
      "loss": 0.0027,
      "step": 1245
    },
    {
      "epoch": 4.397000441111601,
      "grad_norm": 0.09411963470208344,
      "learning_rate": 5.558282208588958e-06,
      "loss": 0.008,
      "step": 1246
    },
    {
      "epoch": 4.400529333921482,
      "grad_norm": 0.09279680459732337,
      "learning_rate": 5.546012269938651e-06,
      "loss": 0.0059,
      "step": 1247
    },
    {
      "epoch": 4.4040582267313635,
      "grad_norm": 0.18216578133293185,
      "learning_rate": 5.533742331288344e-06,
      "loss": 0.0094,
      "step": 1248
    },
    {
      "epoch": 4.4040582267313635,
      "eval_average_f1": 0.7962732798024307,
      "eval_crossner_ai_f1": 0.7540983606032787,
      "eval_crossner_ai_precision": 0.7419354838685744,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9370629370116289,
      "eval_crossner_music_precision": 0.9305555555542631,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7185628742006525,
      "eval_crossner_politics_precision": 0.7317073170722784,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.7272727272194216,
      "eval_crossner_science_precision": 0.7272727272694215,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.944162436497266,
      "eval_mit-movie_precision": 0.9489795918357663,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8260869564705419,
      "eval_mit-restaurant_precision": 0.8260869565205419,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 15.9431,
      "eval_samples_per_second": 8.029,
      "eval_steps_per_second": 0.251,
      "step": 1248
    },
    {
      "epoch": 4.407587119541244,
      "grad_norm": 0.260283377233032,
      "learning_rate": 5.521472392638038e-06,
      "loss": 0.0064,
      "step": 1249
    },
    {
      "epoch": 4.411116012351124,
      "grad_norm": 0.1690749514431498,
      "learning_rate": 5.509202453987731e-06,
      "loss": 0.0061,
      "step": 1250
    },
    {
      "epoch": 4.414644905161006,
      "grad_norm": 0.2218830671321388,
      "learning_rate": 5.496932515337424e-06,
      "loss": 0.0075,
      "step": 1251
    },
    {
      "epoch": 4.418173797970887,
      "grad_norm": 0.28581016498531947,
      "learning_rate": 5.484662576687117e-06,
      "loss": 0.0084,
      "step": 1252
    },
    {
      "epoch": 4.4217026907807675,
      "grad_norm": 0.0895540085006141,
      "learning_rate": 5.47239263803681e-06,
      "loss": 0.0042,
      "step": 1253
    },
    {
      "epoch": 4.425231583590649,
      "grad_norm": 0.1402549975540931,
      "learning_rate": 5.460122699386503e-06,
      "loss": 0.0067,
      "step": 1254
    },
    {
      "epoch": 4.428760476400529,
      "grad_norm": 0.06970471045066215,
      "learning_rate": 5.447852760736197e-06,
      "loss": 0.0071,
      "step": 1255
    },
    {
      "epoch": 4.432289369210411,
      "grad_norm": 0.14344324600121544,
      "learning_rate": 5.43558282208589e-06,
      "loss": 0.0045,
      "step": 1256
    },
    {
      "epoch": 4.432289369210411,
      "eval_average_f1": 0.786554400530474,
      "eval_crossner_ai_f1": 0.8387096773667014,
      "eval_crossner_ai_precision": 0.8124999999974609,
      "eval_crossner_ai_recall": 0.8666666666637778,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9370629370116289,
      "eval_crossner_music_precision": 0.9305555555542631,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.6783625730486236,
      "eval_crossner_politics_precision": 0.6744186046503786,
      "eval_crossner_politics_recall": 0.6823529411756678,
      "eval_crossner_science_f1": 0.6363636363107439,
      "eval_crossner_science_precision": 0.6363636363607439,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9285714285204862,
      "eval_mit-movie_precision": 0.93814432989594,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8201438848409087,
      "eval_mit-restaurant_precision": 0.814285714284551,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 15.9174,
      "eval_samples_per_second": 8.042,
      "eval_steps_per_second": 0.251,
      "step": 1256
    },
    {
      "epoch": 4.435818262020291,
      "grad_norm": 0.24020449910049696,
      "learning_rate": 5.423312883435583e-06,
      "loss": 0.0053,
      "step": 1257
    },
    {
      "epoch": 4.439347154830172,
      "grad_norm": 0.09586337355565003,
      "learning_rate": 5.411042944785276e-06,
      "loss": 0.0075,
      "step": 1258
    },
    {
      "epoch": 4.442876047640053,
      "grad_norm": 0.11464539025073371,
      "learning_rate": 5.39877300613497e-06,
      "loss": 0.0053,
      "step": 1259
    },
    {
      "epoch": 4.446404940449934,
      "grad_norm": 0.17384366169621598,
      "learning_rate": 5.386503067484663e-06,
      "loss": 0.0108,
      "step": 1260
    },
    {
      "epoch": 4.449933833259815,
      "grad_norm": 0.07865450352861875,
      "learning_rate": 5.374233128834356e-06,
      "loss": 0.006,
      "step": 1261
    },
    {
      "epoch": 4.453462726069696,
      "grad_norm": 0.06210535594244451,
      "learning_rate": 5.361963190184049e-06,
      "loss": 0.0029,
      "step": 1262
    },
    {
      "epoch": 4.456991618879576,
      "grad_norm": 0.14648501713929893,
      "learning_rate": 5.349693251533743e-06,
      "loss": 0.01,
      "step": 1263
    },
    {
      "epoch": 4.460520511689458,
      "grad_norm": 0.06429432321585703,
      "learning_rate": 5.3374233128834365e-06,
      "loss": 0.0036,
      "step": 1264
    },
    {
      "epoch": 4.460520511689458,
      "eval_average_f1": 0.7961743915497819,
      "eval_crossner_ai_f1": 0.8064516128506763,
      "eval_crossner_ai_precision": 0.7812499999975585,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7100591715467947,
      "eval_crossner_politics_precision": 0.714285714284864,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.6818181817650827,
      "eval_crossner_science_precision": 0.6818181818150827,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9494949494439904,
      "eval_mit-movie_precision": 0.9494949494939904,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8142857142345612,
      "eval_mit-restaurant_precision": 0.80281690140732,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 15.9932,
      "eval_samples_per_second": 8.003,
      "eval_steps_per_second": 0.25,
      "step": 1264
    },
    {
      "epoch": 4.464049404499338,
      "grad_norm": 0.08541979691925974,
      "learning_rate": 5.32515337423313e-06,
      "loss": 0.007,
      "step": 1265
    },
    {
      "epoch": 4.4675782973092195,
      "grad_norm": 0.16066512233867777,
      "learning_rate": 5.312883435582823e-06,
      "loss": 0.0071,
      "step": 1266
    },
    {
      "epoch": 4.4711071901191,
      "grad_norm": 0.12939155863100418,
      "learning_rate": 5.300613496932516e-06,
      "loss": 0.0102,
      "step": 1267
    },
    {
      "epoch": 4.474636082928981,
      "grad_norm": 0.3042729443014683,
      "learning_rate": 5.288343558282208e-06,
      "loss": 0.0032,
      "step": 1268
    },
    {
      "epoch": 4.478164975738862,
      "grad_norm": 0.04824899797205151,
      "learning_rate": 5.276073619631902e-06,
      "loss": 0.0026,
      "step": 1269
    },
    {
      "epoch": 4.481693868548743,
      "grad_norm": 0.11567850518100643,
      "learning_rate": 5.263803680981595e-06,
      "loss": 0.0081,
      "step": 1270
    },
    {
      "epoch": 4.4852227613586235,
      "grad_norm": 0.12643203162399982,
      "learning_rate": 5.251533742331288e-06,
      "loss": 0.0058,
      "step": 1271
    },
    {
      "epoch": 4.488751654168505,
      "grad_norm": 0.1805572400157148,
      "learning_rate": 5.2392638036809815e-06,
      "loss": 0.0061,
      "step": 1272
    },
    {
      "epoch": 4.488751654168505,
      "eval_average_f1": 0.7931659127338209,
      "eval_crossner_ai_f1": 0.8064516128506763,
      "eval_crossner_ai_precision": 0.7812499999975585,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9370629370116289,
      "eval_crossner_music_precision": 0.9305555555542631,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7023809523301233,
      "eval_crossner_politics_precision": 0.7108433734931194,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.6363636363107439,
      "eval_crossner_science_precision": 0.6363636363607439,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9543147207612152,
      "eval_mit-movie_precision": 0.9591836734684089,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8489208632581337,
      "eval_mit-restaurant_precision": 0.8428571428559387,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.5123,
      "eval_samples_per_second": 7.752,
      "eval_steps_per_second": 0.242,
      "step": 1272
    },
    {
      "epoch": 4.492280546978385,
      "grad_norm": 0.21577913228017137,
      "learning_rate": 5.2269938650306755e-06,
      "loss": 0.009,
      "step": 1273
    },
    {
      "epoch": 4.495809439788267,
      "grad_norm": 0.07068723584352578,
      "learning_rate": 5.214723926380369e-06,
      "loss": 0.0082,
      "step": 1274
    },
    {
      "epoch": 4.499338332598147,
      "grad_norm": 0.13671291202614919,
      "learning_rate": 5.202453987730062e-06,
      "loss": 0.0076,
      "step": 1275
    },
    {
      "epoch": 4.502867225408028,
      "grad_norm": 0.09945987721196316,
      "learning_rate": 5.190184049079755e-06,
      "loss": 0.0054,
      "step": 1276
    },
    {
      "epoch": 4.506396118217909,
      "grad_norm": 0.07012246199722183,
      "learning_rate": 5.177914110429448e-06,
      "loss": 0.0049,
      "step": 1277
    },
    {
      "epoch": 4.50992501102779,
      "grad_norm": 0.15212519569784036,
      "learning_rate": 5.165644171779142e-06,
      "loss": 0.0064,
      "step": 1278
    },
    {
      "epoch": 4.513453903837671,
      "grad_norm": 0.1111191704326447,
      "learning_rate": 5.153374233128835e-06,
      "loss": 0.0031,
      "step": 1279
    },
    {
      "epoch": 4.516982796647552,
      "grad_norm": 0.06607081741387265,
      "learning_rate": 5.141104294478528e-06,
      "loss": 0.0063,
      "step": 1280
    },
    {
      "epoch": 4.516982796647552,
      "eval_average_f1": 0.8124296757874625,
      "eval_crossner_ai_f1": 0.819672131094867,
      "eval_crossner_ai_precision": 0.8064516129006244,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.6792452829663227,
      "eval_crossner_literature_precision": 0.6923076923050296,
      "eval_crossner_literature_recall": 0.6666666666641976,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7701149424778768,
      "eval_crossner_politics_precision": 0.7528089887631991,
      "eval_crossner_politics_recall": 0.7882352941167197,
      "eval_crossner_science_f1": 0.6818181817650827,
      "eval_crossner_science_precision": 0.6818181818150827,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9595959595449903,
      "eval_mit-movie_precision": 0.9595959595949903,
      "eval_mit-movie_recall": 0.9595959595949903,
      "eval_mit-restaurant_f1": 0.8321167882699557,
      "eval_mit-restaurant_precision": 0.8382352941164143,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 15.9672,
      "eval_samples_per_second": 8.016,
      "eval_steps_per_second": 0.251,
      "step": 1280
    },
    {
      "epoch": 4.520511689457432,
      "grad_norm": 0.13920270035233953,
      "learning_rate": 5.128834355828221e-06,
      "loss": 0.004,
      "step": 1281
    },
    {
      "epoch": 4.524040582267314,
      "grad_norm": 0.22517262201395072,
      "learning_rate": 5.116564417177915e-06,
      "loss": 0.0102,
      "step": 1282
    },
    {
      "epoch": 4.527569475077194,
      "grad_norm": 0.08035301211361093,
      "learning_rate": 5.1042944785276084e-06,
      "loss": 0.0066,
      "step": 1283
    },
    {
      "epoch": 4.5310983678870755,
      "grad_norm": 0.07390724551939724,
      "learning_rate": 5.092024539877301e-06,
      "loss": 0.008,
      "step": 1284
    },
    {
      "epoch": 4.534627260696956,
      "grad_norm": 0.063928852438761,
      "learning_rate": 5.079754601226994e-06,
      "loss": 0.0046,
      "step": 1285
    },
    {
      "epoch": 4.538156153506837,
      "grad_norm": 0.10780711886203824,
      "learning_rate": 5.067484662576687e-06,
      "loss": 0.0077,
      "step": 1286
    },
    {
      "epoch": 4.541685046316718,
      "grad_norm": 0.30341906490563786,
      "learning_rate": 5.05521472392638e-06,
      "loss": 0.0063,
      "step": 1287
    },
    {
      "epoch": 4.545213939126599,
      "grad_norm": 0.12594477673893373,
      "learning_rate": 5.042944785276074e-06,
      "loss": 0.0064,
      "step": 1288
    },
    {
      "epoch": 4.545213939126599,
      "eval_average_f1": 0.7785128956227334,
      "eval_crossner_ai_f1": 0.7619047618524566,
      "eval_crossner_ai_precision": 0.7272727272705234,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.653846153793713,
      "eval_crossner_literature_precision": 0.67999999999728,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.6900584794813583,
      "eval_crossner_politics_precision": 0.6860465116271093,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.6222222221694815,
      "eval_crossner_science_precision": 0.6086956521712665,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9595959595449903,
      "eval_mit-movie_precision": 0.9595959595949903,
      "eval_mit-movie_recall": 0.9595959595949903,
      "eval_mit-restaurant_f1": 0.8175182481239917,
      "eval_mit-restaurant_precision": 0.8235294117634948,
      "eval_mit-restaurant_recall": 0.8115942028973745,
      "eval_runtime": 16.3844,
      "eval_samples_per_second": 7.812,
      "eval_steps_per_second": 0.244,
      "step": 1288
    },
    {
      "epoch": 4.5487428319364795,
      "grad_norm": 0.13007244091960662,
      "learning_rate": 5.030674846625767e-06,
      "loss": 0.004,
      "step": 1289
    },
    {
      "epoch": 4.552271724746361,
      "grad_norm": 0.0858566787364055,
      "learning_rate": 5.01840490797546e-06,
      "loss": 0.0037,
      "step": 1290
    },
    {
      "epoch": 4.555800617556242,
      "grad_norm": 0.6661321812101413,
      "learning_rate": 5.006134969325153e-06,
      "loss": 0.0088,
      "step": 1291
    },
    {
      "epoch": 4.559329510366123,
      "grad_norm": 0.0677267226898133,
      "learning_rate": 4.993865030674847e-06,
      "loss": 0.0044,
      "step": 1292
    },
    {
      "epoch": 4.562858403176003,
      "grad_norm": 0.0846016483309972,
      "learning_rate": 4.9815950920245405e-06,
      "loss": 0.0084,
      "step": 1293
    },
    {
      "epoch": 4.566387295985884,
      "grad_norm": 0.2319848335283678,
      "learning_rate": 4.969325153374234e-06,
      "loss": 0.0064,
      "step": 1294
    },
    {
      "epoch": 4.569916188795766,
      "grad_norm": 0.05987473583623574,
      "learning_rate": 4.957055214723927e-06,
      "loss": 0.0025,
      "step": 1295
    },
    {
      "epoch": 4.573445081605646,
      "grad_norm": 0.1205850962370705,
      "learning_rate": 4.94478527607362e-06,
      "loss": 0.0052,
      "step": 1296
    },
    {
      "epoch": 4.573445081605646,
      "eval_average_f1": 0.8080044657371276,
      "eval_crossner_ai_f1": 0.7540983606032787,
      "eval_crossner_ai_precision": 0.7419354838685744,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.71999999994744,
      "eval_crossner_literature_precision": 0.7826086956487713,
      "eval_crossner_literature_recall": 0.6666666666641976,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7325581394840387,
      "eval_crossner_politics_precision": 0.7241379310336504,
      "eval_crossner_politics_recall": 0.7411764705873634,
      "eval_crossner_science_f1": 0.6976744185514333,
      "eval_crossner_science_precision": 0.714285714282313,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9543147207612152,
      "eval_mit-movie_precision": 0.9591836734684089,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8529411764193447,
      "eval_mit-restaurant_precision": 0.8656716417897528,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 15.9633,
      "eval_samples_per_second": 8.018,
      "eval_steps_per_second": 0.251,
      "step": 1296
    },
    {
      "epoch": 4.576973974415527,
      "grad_norm": 0.17418337000846415,
      "learning_rate": 4.932515337423313e-06,
      "loss": 0.0055,
      "step": 1297
    },
    {
      "epoch": 4.580502867225408,
      "grad_norm": 0.059975586105206306,
      "learning_rate": 4.920245398773006e-06,
      "loss": 0.007,
      "step": 1298
    },
    {
      "epoch": 4.584031760035289,
      "grad_norm": 0.11935113460466315,
      "learning_rate": 4.9079754601227e-06,
      "loss": 0.0065,
      "step": 1299
    },
    {
      "epoch": 4.58756065284517,
      "grad_norm": 0.15703416982944307,
      "learning_rate": 4.895705521472393e-06,
      "loss": 0.0076,
      "step": 1300
    },
    {
      "epoch": 4.59108954565505,
      "grad_norm": 0.07818053774896916,
      "learning_rate": 4.883435582822086e-06,
      "loss": 0.0037,
      "step": 1301
    },
    {
      "epoch": 4.594618438464932,
      "grad_norm": 0.09776601821146046,
      "learning_rate": 4.8711656441717795e-06,
      "loss": 0.0073,
      "step": 1302
    },
    {
      "epoch": 4.598147331274813,
      "grad_norm": 0.19976935311942395,
      "learning_rate": 4.8588957055214735e-06,
      "loss": 0.0115,
      "step": 1303
    },
    {
      "epoch": 4.601676224084693,
      "grad_norm": 0.10892104133934838,
      "learning_rate": 4.846625766871166e-06,
      "loss": 0.0072,
      "step": 1304
    },
    {
      "epoch": 4.601676224084693,
      "eval_average_f1": 0.8183592215974077,
      "eval_crossner_ai_f1": 0.7540983606032787,
      "eval_crossner_ai_precision": 0.7419354838685744,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.7058823528885814,
      "eval_crossner_literature_precision": 0.749999999996875,
      "eval_crossner_literature_recall": 0.6666666666641976,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7455621301266343,
      "eval_crossner_politics_precision": 0.7499999999991072,
      "eval_crossner_politics_recall": 0.7411764705873634,
      "eval_crossner_science_f1": 0.7317073170198692,
      "eval_crossner_science_precision": 0.7894736842063713,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9644670050251641,
      "eval_mit-movie_precision": 0.9693877551010516,
      "eval_mit-movie_recall": 0.9595959595949903,
      "eval_mit-restaurant_f1": 0.8823529411251837,
      "eval_mit-restaurant_precision": 0.8955223880583648,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.5741,
      "eval_samples_per_second": 7.723,
      "eval_steps_per_second": 0.241,
      "step": 1304
    },
    {
      "epoch": 4.605205116894575,
      "grad_norm": 0.08701076378603331,
      "learning_rate": 4.834355828220859e-06,
      "loss": 0.005,
      "step": 1305
    },
    {
      "epoch": 4.608734009704455,
      "grad_norm": 0.12546435176223555,
      "learning_rate": 4.822085889570553e-06,
      "loss": 0.0041,
      "step": 1306
    },
    {
      "epoch": 4.6122629025143365,
      "grad_norm": 0.045223513939823226,
      "learning_rate": 4.809815950920246e-06,
      "loss": 0.0043,
      "step": 1307
    },
    {
      "epoch": 4.615791795324217,
      "grad_norm": 0.19618510819575513,
      "learning_rate": 4.797546012269939e-06,
      "loss": 0.0101,
      "step": 1308
    },
    {
      "epoch": 4.619320688134098,
      "grad_norm": 0.10721206549668393,
      "learning_rate": 4.785276073619632e-06,
      "loss": 0.0045,
      "step": 1309
    },
    {
      "epoch": 4.622849580943979,
      "grad_norm": 0.08644901114306973,
      "learning_rate": 4.773006134969325e-06,
      "loss": 0.0066,
      "step": 1310
    },
    {
      "epoch": 4.62637847375386,
      "grad_norm": 0.1666236122718166,
      "learning_rate": 4.760736196319019e-06,
      "loss": 0.0054,
      "step": 1311
    },
    {
      "epoch": 4.6299073665637405,
      "grad_norm": 0.10991034877049957,
      "learning_rate": 4.748466257668712e-06,
      "loss": 0.0034,
      "step": 1312
    },
    {
      "epoch": 4.6299073665637405,
      "eval_average_f1": 0.795863960298268,
      "eval_crossner_ai_f1": 0.7741935483346514,
      "eval_crossner_ai_precision": 0.7499999999976562,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9370629370116289,
      "eval_crossner_music_precision": 0.9305555555542631,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7199999999492178,
      "eval_crossner_politics_precision": 0.6999999999992222,
      "eval_crossner_politics_recall": 0.7411764705873634,
      "eval_crossner_science_f1": 0.6511627906446729,
      "eval_crossner_science_precision": 0.6666666666634922,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9543147207612152,
      "eval_mit-movie_precision": 0.9591836734684089,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8676470587722643,
      "eval_mit-restaurant_precision": 0.8805970149240588,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.5629,
      "eval_samples_per_second": 7.728,
      "eval_steps_per_second": 0.242,
      "step": 1312
    },
    {
      "epoch": 4.633436259373622,
      "grad_norm": 0.15828484320257166,
      "learning_rate": 4.7361963190184056e-06,
      "loss": 0.0038,
      "step": 1313
    },
    {
      "epoch": 4.636965152183502,
      "grad_norm": 0.12345076682828673,
      "learning_rate": 4.723926380368099e-06,
      "loss": 0.0058,
      "step": 1314
    },
    {
      "epoch": 4.640494044993384,
      "grad_norm": 0.09700854080245078,
      "learning_rate": 4.711656441717792e-06,
      "loss": 0.0056,
      "step": 1315
    },
    {
      "epoch": 4.644022937803264,
      "grad_norm": 0.08327174580683872,
      "learning_rate": 4.699386503067485e-06,
      "loss": 0.0045,
      "step": 1316
    },
    {
      "epoch": 4.647551830613145,
      "grad_norm": 0.11475507822778035,
      "learning_rate": 4.687116564417178e-06,
      "loss": 0.0065,
      "step": 1317
    },
    {
      "epoch": 4.651080723423026,
      "grad_norm": 0.11385689602207014,
      "learning_rate": 4.674846625766872e-06,
      "loss": 0.0035,
      "step": 1318
    },
    {
      "epoch": 4.654609616232907,
      "grad_norm": 0.11358964393808305,
      "learning_rate": 4.662576687116564e-06,
      "loss": 0.0057,
      "step": 1319
    },
    {
      "epoch": 4.658138509042788,
      "grad_norm": 0.25489751546845385,
      "learning_rate": 4.6503067484662574e-06,
      "loss": 0.003,
      "step": 1320
    },
    {
      "epoch": 4.658138509042788,
      "eval_average_f1": 0.798486557346544,
      "eval_crossner_ai_f1": 0.779661016896524,
      "eval_crossner_ai_precision": 0.7931034482731273,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.5714285713767597,
      "eval_crossner_literature_precision": 0.6363636363607439,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7023809523301233,
      "eval_crossner_politics_precision": 0.7108433734931194,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.7804878048245093,
      "eval_crossner_science_precision": 0.8421052631534627,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.875912408707848,
      "eval_mit-restaurant_precision": 0.8823529411751729,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.5172,
      "eval_samples_per_second": 7.75,
      "eval_steps_per_second": 0.242,
      "step": 1320
    },
    {
      "epoch": 4.661667401852669,
      "grad_norm": 0.13781220635017372,
      "learning_rate": 4.638036809815951e-06,
      "loss": 0.0043,
      "step": 1321
    },
    {
      "epoch": 4.665196294662549,
      "grad_norm": 0.12876537601542695,
      "learning_rate": 4.6257668711656445e-06,
      "loss": 0.0055,
      "step": 1322
    },
    {
      "epoch": 4.668725187472431,
      "grad_norm": 0.11213964691575366,
      "learning_rate": 4.613496932515338e-06,
      "loss": 0.0035,
      "step": 1323
    },
    {
      "epoch": 4.672254080282311,
      "grad_norm": 0.09671874172660533,
      "learning_rate": 4.601226993865031e-06,
      "loss": 0.0055,
      "step": 1324
    },
    {
      "epoch": 4.6757829730921925,
      "grad_norm": 0.060885017747295524,
      "learning_rate": 4.588957055214725e-06,
      "loss": 0.0048,
      "step": 1325
    },
    {
      "epoch": 4.679311865902073,
      "grad_norm": 0.10104649389262049,
      "learning_rate": 4.576687116564418e-06,
      "loss": 0.005,
      "step": 1326
    },
    {
      "epoch": 4.682840758711954,
      "grad_norm": 0.1439384469163882,
      "learning_rate": 4.56441717791411e-06,
      "loss": 0.0041,
      "step": 1327
    },
    {
      "epoch": 4.686369651521835,
      "grad_norm": 0.1563902702133144,
      "learning_rate": 4.552147239263804e-06,
      "loss": 0.0078,
      "step": 1328
    },
    {
      "epoch": 4.686369651521835,
      "eval_average_f1": 0.7811226660653663,
      "eval_crossner_ai_f1": 0.7931034482231868,
      "eval_crossner_ai_precision": 0.8214285714256379,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.55999999994808,
      "eval_crossner_literature_precision": 0.6086956521712665,
      "eval_crossner_literature_recall": 0.5185185185165981,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.6829268292175268,
      "eval_crossner_politics_precision": 0.7088607594927736,
      "eval_crossner_politics_recall": 0.6588235294109897,
      "eval_crossner_science_f1": 0.6829268292152292,
      "eval_crossner_science_precision": 0.7368421052592798,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9278350514954564,
      "eval_mit-movie_precision": 0.9473684210516343,
      "eval_mit-movie_recall": 0.9090909090899908,
      "eval_mit-restaurant_f1": 0.8905109488538122,
      "eval_mit-restaurant_precision": 0.8970588235280925,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.4587,
      "eval_samples_per_second": 7.777,
      "eval_steps_per_second": 0.243,
      "step": 1328
    },
    {
      "epoch": 4.689898544331716,
      "grad_norm": 0.08146909119630048,
      "learning_rate": 4.539877300613497e-06,
      "loss": 0.0037,
      "step": 1329
    },
    {
      "epoch": 4.6934274371415965,
      "grad_norm": 0.26935373901670384,
      "learning_rate": 4.52760736196319e-06,
      "loss": 0.0049,
      "step": 1330
    },
    {
      "epoch": 4.696956329951478,
      "grad_norm": 0.1400074711683655,
      "learning_rate": 4.5153374233128835e-06,
      "loss": 0.0043,
      "step": 1331
    },
    {
      "epoch": 4.700485222761358,
      "grad_norm": 0.10548471842079726,
      "learning_rate": 4.5030674846625775e-06,
      "loss": 0.0042,
      "step": 1332
    },
    {
      "epoch": 4.70401411557124,
      "grad_norm": 0.11823169097082117,
      "learning_rate": 4.490797546012271e-06,
      "loss": 0.0056,
      "step": 1333
    },
    {
      "epoch": 4.70754300838112,
      "grad_norm": 0.22359704477053932,
      "learning_rate": 4.478527607361964e-06,
      "loss": 0.0059,
      "step": 1334
    },
    {
      "epoch": 4.711071901191001,
      "grad_norm": 0.172084637152818,
      "learning_rate": 4.466257668711657e-06,
      "loss": 0.0052,
      "step": 1335
    },
    {
      "epoch": 4.714600794000882,
      "grad_norm": 0.10827607044063188,
      "learning_rate": 4.45398773006135e-06,
      "loss": 0.0049,
      "step": 1336
    },
    {
      "epoch": 4.714600794000882,
      "eval_average_f1": 0.7920056618775543,
      "eval_crossner_ai_f1": 0.7457627118118931,
      "eval_crossner_ai_precision": 0.7586206896525566,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6153846153323226,
      "eval_crossner_literature_precision": 0.6399999999974401,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7126436781101269,
      "eval_crossner_politics_precision": 0.6966292134823633,
      "eval_crossner_politics_recall": 0.7294117647050242,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9494949494439904,
      "eval_mit-movie_precision": 0.9494949494939904,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.875912408707848,
      "eval_mit-restaurant_precision": 0.8823529411751729,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.4899,
      "eval_samples_per_second": 7.762,
      "eval_steps_per_second": 0.243,
      "step": 1336
    },
    {
      "epoch": 4.718129686810763,
      "grad_norm": 0.21848693424721005,
      "learning_rate": 4.441717791411043e-06,
      "loss": 0.0033,
      "step": 1337
    },
    {
      "epoch": 4.7216585796206445,
      "grad_norm": 0.1861702636155969,
      "learning_rate": 4.429447852760736e-06,
      "loss": 0.0065,
      "step": 1338
    },
    {
      "epoch": 4.725187472430525,
      "grad_norm": 0.2022654245551668,
      "learning_rate": 4.41717791411043e-06,
      "loss": 0.0087,
      "step": 1339
    },
    {
      "epoch": 4.728716365240405,
      "grad_norm": 0.05993129451030797,
      "learning_rate": 4.404907975460123e-06,
      "loss": 0.0029,
      "step": 1340
    },
    {
      "epoch": 4.732245258050287,
      "grad_norm": 0.13326058812327812,
      "learning_rate": 4.3926380368098164e-06,
      "loss": 0.0042,
      "step": 1341
    },
    {
      "epoch": 4.735774150860168,
      "grad_norm": 0.13043566329944234,
      "learning_rate": 4.3803680981595096e-06,
      "loss": 0.0093,
      "step": 1342
    },
    {
      "epoch": 4.7393030436700485,
      "grad_norm": 0.10875056183056014,
      "learning_rate": 4.368098159509203e-06,
      "loss": 0.0066,
      "step": 1343
    },
    {
      "epoch": 4.742831936479929,
      "grad_norm": 0.21961132842572195,
      "learning_rate": 4.355828220858896e-06,
      "loss": 0.0085,
      "step": 1344
    },
    {
      "epoch": 4.742831936479929,
      "eval_average_f1": 0.7738573878792833,
      "eval_crossner_ai_f1": 0.7457627118118931,
      "eval_crossner_ai_precision": 0.7586206896525566,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6153846153323226,
      "eval_crossner_literature_precision": 0.6399999999974401,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.6823529411256678,
      "eval_crossner_politics_precision": 0.6823529411756678,
      "eval_crossner_politics_recall": 0.6823529411756678,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.944162436497266,
      "eval_mit-movie_precision": 0.9489795918357663,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8321167882699557,
      "eval_mit-restaurant_precision": 0.8382352941164143,
      "eval_mit-restaurant_recall": 0.8260869565205419,
      "eval_runtime": 16.4579,
      "eval_samples_per_second": 7.777,
      "eval_steps_per_second": 0.243,
      "step": 1344
    },
    {
      "epoch": 4.74636082928981,
      "grad_norm": 0.08338285597656483,
      "learning_rate": 4.343558282208589e-06,
      "loss": 0.007,
      "step": 1345
    },
    {
      "epoch": 4.749889722099692,
      "grad_norm": 0.0984744131420378,
      "learning_rate": 4.331288343558283e-06,
      "loss": 0.0072,
      "step": 1346
    },
    {
      "epoch": 4.753418614909572,
      "grad_norm": 0.1179286606040437,
      "learning_rate": 4.319018404907976e-06,
      "loss": 0.0054,
      "step": 1347
    },
    {
      "epoch": 4.756947507719453,
      "grad_norm": 0.17457559160610026,
      "learning_rate": 4.306748466257669e-06,
      "loss": 0.0061,
      "step": 1348
    },
    {
      "epoch": 4.760476400529334,
      "grad_norm": 0.13380423689500356,
      "learning_rate": 4.294478527607362e-06,
      "loss": 0.0049,
      "step": 1349
    },
    {
      "epoch": 4.764005293339215,
      "grad_norm": 0.1673237036337192,
      "learning_rate": 4.282208588957055e-06,
      "loss": 0.0065,
      "step": 1350
    },
    {
      "epoch": 4.767534186149096,
      "grad_norm": 0.2593458268408443,
      "learning_rate": 4.2699386503067485e-06,
      "loss": 0.0033,
      "step": 1351
    },
    {
      "epoch": 4.771063078958977,
      "grad_norm": 0.21967786461223635,
      "learning_rate": 4.257668711656442e-06,
      "loss": 0.0157,
      "step": 1352
    },
    {
      "epoch": 4.771063078958977,
      "eval_average_f1": 0.7808709942848571,
      "eval_crossner_ai_f1": 0.779661016896524,
      "eval_crossner_ai_precision": 0.7931034482731273,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6274509803398693,
      "eval_crossner_literature_precision": 0.666666666663889,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7294117646550242,
      "eval_crossner_politics_precision": 0.7294117647050242,
      "eval_crossner_politics_recall": 0.7294117647050242,
      "eval_crossner_science_f1": 0.5454545454020662,
      "eval_crossner_science_precision": 0.5454545454520662,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.9484536081964661,
      "eval_mit-movie_precision": 0.9684210526305596,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.9051094889997762,
      "eval_mit-restaurant_precision": 0.9117647058810121,
      "eval_mit-restaurant_recall": 0.8985507246363789,
      "eval_runtime": 16.3054,
      "eval_samples_per_second": 7.85,
      "eval_steps_per_second": 0.245,
      "step": 1352
    },
    {
      "epoch": 4.774591971768857,
      "grad_norm": 0.09092256983045578,
      "learning_rate": 4.245398773006136e-06,
      "loss": 0.0061,
      "step": 1353
    },
    {
      "epoch": 4.778120864578739,
      "grad_norm": 0.16206610918053446,
      "learning_rate": 4.233128834355829e-06,
      "loss": 0.0064,
      "step": 1354
    },
    {
      "epoch": 4.781649757388619,
      "grad_norm": 0.12303960112613017,
      "learning_rate": 4.220858895705522e-06,
      "loss": 0.0047,
      "step": 1355
    },
    {
      "epoch": 4.7851786501985005,
      "grad_norm": 0.29650775882546676,
      "learning_rate": 4.208588957055215e-06,
      "loss": 0.009,
      "step": 1356
    },
    {
      "epoch": 4.788707543008381,
      "grad_norm": 0.20213845822677626,
      "learning_rate": 4.196319018404908e-06,
      "loss": 0.0069,
      "step": 1357
    },
    {
      "epoch": 4.792236435818262,
      "grad_norm": 0.18591058119491113,
      "learning_rate": 4.184049079754602e-06,
      "loss": 0.0068,
      "step": 1358
    },
    {
      "epoch": 4.795765328628143,
      "grad_norm": 0.07330823463501993,
      "learning_rate": 4.171779141104294e-06,
      "loss": 0.0058,
      "step": 1359
    },
    {
      "epoch": 4.799294221438024,
      "grad_norm": 0.05937462072502267,
      "learning_rate": 4.159509202453988e-06,
      "loss": 0.0059,
      "step": 1360
    },
    {
      "epoch": 4.799294221438024,
      "eval_average_f1": 0.7810315648027226,
      "eval_crossner_ai_f1": 0.758620689602616,
      "eval_crossner_ai_precision": 0.7857142857114796,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6938775509680968,
      "eval_crossner_literature_precision": 0.7727272727237604,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.6626506023588692,
      "eval_crossner_politics_precision": 0.679012345678174,
      "eval_crossner_politics_recall": 0.6470588235286505,
      "eval_crossner_science_f1": 0.5714285713759637,
      "eval_crossner_science_precision": 0.599999999997,
      "eval_crossner_science_recall": 0.5454545454520662,
      "eval_mit-movie_f1": 0.9538461537951873,
      "eval_mit-movie_precision": 0.9687499999989909,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8823529411251837,
      "eval_mit-restaurant_precision": 0.8955223880583648,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.4964,
      "eval_samples_per_second": 7.759,
      "eval_steps_per_second": 0.242,
      "step": 1360
    },
    {
      "epoch": 4.8028231142479045,
      "grad_norm": 0.09258958769708436,
      "learning_rate": 4.1472392638036815e-06,
      "loss": 0.0055,
      "step": 1361
    },
    {
      "epoch": 4.806352007057786,
      "grad_norm": 0.0927807303122115,
      "learning_rate": 4.134969325153375e-06,
      "loss": 0.0084,
      "step": 1362
    },
    {
      "epoch": 4.809880899867666,
      "grad_norm": 0.09525665441150268,
      "learning_rate": 4.122699386503068e-06,
      "loss": 0.0051,
      "step": 1363
    },
    {
      "epoch": 4.813409792677548,
      "grad_norm": 0.37478216122456287,
      "learning_rate": 4.110429447852761e-06,
      "loss": 0.0074,
      "step": 1364
    },
    {
      "epoch": 4.816938685487428,
      "grad_norm": 0.09241224549948736,
      "learning_rate": 4.098159509202455e-06,
      "loss": 0.0056,
      "step": 1365
    },
    {
      "epoch": 4.820467578297309,
      "grad_norm": 0.0476398382107845,
      "learning_rate": 4.085889570552147e-06,
      "loss": 0.0029,
      "step": 1366
    },
    {
      "epoch": 4.82399647110719,
      "grad_norm": 0.08044308848130163,
      "learning_rate": 4.07361963190184e-06,
      "loss": 0.0068,
      "step": 1367
    },
    {
      "epoch": 4.827525363917071,
      "grad_norm": 0.06382081144564895,
      "learning_rate": 4.061349693251534e-06,
      "loss": 0.0028,
      "step": 1368
    },
    {
      "epoch": 4.827525363917071,
      "eval_average_f1": 0.8050895579578422,
      "eval_crossner_ai_f1": 0.7301587301065257,
      "eval_crossner_ai_precision": 0.6969696969675849,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7305389221048298,
      "eval_crossner_politics_precision": 0.7439024390234831,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9538461537951873,
      "eval_mit-movie_precision": 0.9687499999989909,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8823529411251837,
      "eval_mit-restaurant_precision": 0.8955223880583648,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.5482,
      "eval_samples_per_second": 7.735,
      "eval_steps_per_second": 0.242,
      "step": 1368
    },
    {
      "epoch": 4.831054256726952,
      "grad_norm": 0.13004348253488549,
      "learning_rate": 4.049079754601227e-06,
      "loss": 0.0057,
      "step": 1369
    },
    {
      "epoch": 4.834583149536833,
      "grad_norm": 0.14984968670887697,
      "learning_rate": 4.0368098159509204e-06,
      "loss": 0.0093,
      "step": 1370
    },
    {
      "epoch": 4.838112042346713,
      "grad_norm": 0.09387787402880002,
      "learning_rate": 4.0245398773006136e-06,
      "loss": 0.0052,
      "step": 1371
    },
    {
      "epoch": 4.841640935156595,
      "grad_norm": 0.046594858003032424,
      "learning_rate": 4.0122699386503075e-06,
      "loss": 0.004,
      "step": 1372
    },
    {
      "epoch": 4.845169827966475,
      "grad_norm": 0.1537521198076175,
      "learning_rate": 4.000000000000001e-06,
      "loss": 0.0081,
      "step": 1373
    },
    {
      "epoch": 4.8486987207763566,
      "grad_norm": 0.06825143602933742,
      "learning_rate": 3.987730061349693e-06,
      "loss": 0.0033,
      "step": 1374
    },
    {
      "epoch": 4.852227613586237,
      "grad_norm": 0.15369148034304733,
      "learning_rate": 3.975460122699387e-06,
      "loss": 0.0076,
      "step": 1375
    },
    {
      "epoch": 4.855756506396118,
      "grad_norm": 0.05982122577549492,
      "learning_rate": 3.96319018404908e-06,
      "loss": 0.0053,
      "step": 1376
    },
    {
      "epoch": 4.855756506396118,
      "eval_average_f1": 0.7943137868665759,
      "eval_crossner_ai_f1": 0.7187499999479492,
      "eval_crossner_ai_precision": 0.6764705882333044,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7485380116450326,
      "eval_crossner_politics_precision": 0.7441860465107626,
      "eval_crossner_politics_recall": 0.7529411764697024,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9333333332823879,
      "eval_mit-movie_precision": 0.9479166666656792,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8823529411251837,
      "eval_mit-restaurant_precision": 0.8955223880583648,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.5157,
      "eval_samples_per_second": 7.75,
      "eval_steps_per_second": 0.242,
      "step": 1376
    },
    {
      "epoch": 4.859285399205999,
      "grad_norm": 0.08235775025982779,
      "learning_rate": 3.950920245398773e-06,
      "loss": 0.0081,
      "step": 1377
    },
    {
      "epoch": 4.86281429201588,
      "grad_norm": 0.10165342515690444,
      "learning_rate": 3.938650306748466e-06,
      "loss": 0.0049,
      "step": 1378
    },
    {
      "epoch": 4.866343184825761,
      "grad_norm": 0.07338477877546533,
      "learning_rate": 3.92638036809816e-06,
      "loss": 0.0038,
      "step": 1379
    },
    {
      "epoch": 4.869872077635642,
      "grad_norm": 0.0697353704750281,
      "learning_rate": 3.914110429447853e-06,
      "loss": 0.0036,
      "step": 1380
    },
    {
      "epoch": 4.873400970445522,
      "grad_norm": 0.12034224151873414,
      "learning_rate": 3.9018404907975465e-06,
      "loss": 0.0074,
      "step": 1381
    },
    {
      "epoch": 4.876929863255404,
      "grad_norm": 0.10592754674532473,
      "learning_rate": 3.88957055214724e-06,
      "loss": 0.0039,
      "step": 1382
    },
    {
      "epoch": 4.880458756065284,
      "grad_norm": 0.1957924001521954,
      "learning_rate": 3.877300613496933e-06,
      "loss": 0.0073,
      "step": 1383
    },
    {
      "epoch": 4.8839876488751655,
      "grad_norm": 0.09509992266555554,
      "learning_rate": 3.865030674846626e-06,
      "loss": 0.005,
      "step": 1384
    },
    {
      "epoch": 4.8839876488751655,
      "eval_average_f1": 0.7951947327621379,
      "eval_crossner_ai_f1": 0.7936507935983874,
      "eval_crossner_ai_precision": 0.7575757575734618,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7586206896043268,
      "eval_crossner_politics_precision": 0.7415730337070319,
      "eval_crossner_politics_recall": 0.7764705882343806,
      "eval_crossner_science_f1": 0.6222222221694815,
      "eval_crossner_science_precision": 0.6086956521712665,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9333333332823879,
      "eval_mit-movie_precision": 0.9479166666656792,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8613138685618839,
      "eval_mit-restaurant_precision": 0.8676470588222535,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.0388,
      "eval_samples_per_second": 7.981,
      "eval_steps_per_second": 0.249,
      "step": 1384
    },
    {
      "epoch": 4.887516541685047,
      "grad_norm": 0.2214444210204791,
      "learning_rate": 3.852760736196319e-06,
      "loss": 0.0054,
      "step": 1385
    },
    {
      "epoch": 4.891045434494927,
      "grad_norm": 0.08255971404236709,
      "learning_rate": 3.840490797546013e-06,
      "loss": 0.007,
      "step": 1386
    },
    {
      "epoch": 4.894574327304808,
      "grad_norm": 0.057901316317041034,
      "learning_rate": 3.828220858895706e-06,
      "loss": 0.0031,
      "step": 1387
    },
    {
      "epoch": 4.898103220114689,
      "grad_norm": 0.060772368240610636,
      "learning_rate": 3.815950920245399e-06,
      "loss": 0.0036,
      "step": 1388
    },
    {
      "epoch": 4.90163211292457,
      "grad_norm": 0.069990157639238,
      "learning_rate": 3.8036809815950928e-06,
      "loss": 0.0059,
      "step": 1389
    },
    {
      "epoch": 4.905161005734451,
      "grad_norm": 0.06787783088453521,
      "learning_rate": 3.7914110429447855e-06,
      "loss": 0.0043,
      "step": 1390
    },
    {
      "epoch": 4.908689898544331,
      "grad_norm": 0.13263038462845536,
      "learning_rate": 3.7791411042944786e-06,
      "loss": 0.003,
      "step": 1391
    },
    {
      "epoch": 4.912218791354213,
      "grad_norm": 0.08251293426869512,
      "learning_rate": 3.766871165644172e-06,
      "loss": 0.0071,
      "step": 1392
    },
    {
      "epoch": 4.912218791354213,
      "eval_average_f1": 0.8003948822313579,
      "eval_crossner_ai_f1": 0.7812499999477539,
      "eval_crossner_ai_precision": 0.7352941176448962,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.720930232507308,
      "eval_crossner_politics_precision": 0.7126436781601004,
      "eval_crossner_politics_recall": 0.7294117647050242,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9381443298459613,
      "eval_mit-movie_precision": 0.9578947368410969,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.8985507245863789,
      "eval_mit-restaurant_precision": 0.8985507246363789,
      "eval_mit-restaurant_recall": 0.8985507246363789,
      "eval_runtime": 16.0156,
      "eval_samples_per_second": 7.992,
      "eval_steps_per_second": 0.25,
      "step": 1392
    },
    {
      "epoch": 4.915747684164094,
      "grad_norm": 0.11873080559626427,
      "learning_rate": 3.7546012269938653e-06,
      "loss": 0.0114,
      "step": 1393
    },
    {
      "epoch": 4.919276576973974,
      "grad_norm": 0.10928780196311891,
      "learning_rate": 3.742331288343559e-06,
      "loss": 0.0048,
      "step": 1394
    },
    {
      "epoch": 4.922805469783856,
      "grad_norm": 0.05952374600314117,
      "learning_rate": 3.730061349693252e-06,
      "loss": 0.0039,
      "step": 1395
    },
    {
      "epoch": 4.926334362593736,
      "grad_norm": 0.07410198048165557,
      "learning_rate": 3.717791411042945e-06,
      "loss": 0.0057,
      "step": 1396
    },
    {
      "epoch": 4.9298632554036175,
      "grad_norm": 0.1377250490477089,
      "learning_rate": 3.7055214723926386e-06,
      "loss": 0.0066,
      "step": 1397
    },
    {
      "epoch": 4.933392148213498,
      "grad_norm": 0.09862584854318161,
      "learning_rate": 3.6932515337423313e-06,
      "loss": 0.0049,
      "step": 1398
    },
    {
      "epoch": 4.936921041023379,
      "grad_norm": 0.0570434232352916,
      "learning_rate": 3.680981595092025e-06,
      "loss": 0.0026,
      "step": 1399
    },
    {
      "epoch": 4.94044993383326,
      "grad_norm": 0.2582230333741414,
      "learning_rate": 3.668711656441718e-06,
      "loss": 0.0056,
      "step": 1400
    },
    {
      "epoch": 4.94044993383326,
      "eval_average_f1": 0.7906249754974741,
      "eval_crossner_ai_f1": 0.7692307691786981,
      "eval_crossner_ai_precision": 0.7142857142836734,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.6037735848533999,
      "eval_crossner_literature_precision": 0.6153846153822485,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7100591715467947,
      "eval_crossner_politics_precision": 0.714285714284864,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.9051094889997762,
      "eval_mit-restaurant_precision": 0.9117647058810121,
      "eval_mit-restaurant_recall": 0.8985507246363789,
      "eval_runtime": 16.0229,
      "eval_samples_per_second": 7.989,
      "eval_steps_per_second": 0.25,
      "step": 1400
    },
    {
      "epoch": 4.943978826643141,
      "grad_norm": 0.13094267400201443,
      "learning_rate": 3.656441717791411e-06,
      "loss": 0.0062,
      "step": 1401
    },
    {
      "epoch": 4.9475077194530215,
      "grad_norm": 0.3146035681568277,
      "learning_rate": 3.6441717791411047e-06,
      "loss": 0.0076,
      "step": 1402
    },
    {
      "epoch": 4.951036612262903,
      "grad_norm": 0.21112235234804175,
      "learning_rate": 3.631901840490798e-06,
      "loss": 0.0054,
      "step": 1403
    },
    {
      "epoch": 4.954565505072783,
      "grad_norm": 0.29483016906172027,
      "learning_rate": 3.6196319018404913e-06,
      "loss": 0.0096,
      "step": 1404
    },
    {
      "epoch": 4.958094397882665,
      "grad_norm": 0.1205511665878251,
      "learning_rate": 3.6073619631901845e-06,
      "loss": 0.0054,
      "step": 1405
    },
    {
      "epoch": 4.961623290692545,
      "grad_norm": 0.08415734698267291,
      "learning_rate": 3.595092024539877e-06,
      "loss": 0.0031,
      "step": 1406
    },
    {
      "epoch": 4.965152183502426,
      "grad_norm": 0.3001848227679169,
      "learning_rate": 3.5828220858895707e-06,
      "loss": 0.0069,
      "step": 1407
    },
    {
      "epoch": 4.968681076312307,
      "grad_norm": 0.08203074761645567,
      "learning_rate": 3.570552147239264e-06,
      "loss": 0.0047,
      "step": 1408
    },
    {
      "epoch": 4.968681076312307,
      "eval_average_f1": 0.8060397785707376,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6153846153323226,
      "eval_crossner_literature_precision": 0.6399999999974401,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7011494252365768,
      "eval_crossner_politics_precision": 0.6853932584261961,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.8095238094700682,
      "eval_crossner_science_precision": 0.84999999999575,
      "eval_crossner_science_recall": 0.7727272727237604,
      "eval_mit-movie_f1": 0.9333333332823879,
      "eval_mit-movie_precision": 0.9479166666656792,
      "eval_mit-movie_recall": 0.9191919191909907,
      "eval_mit-restaurant_f1": 0.9051094889997762,
      "eval_mit-restaurant_precision": 0.9117647058810121,
      "eval_mit-restaurant_recall": 0.8985507246363789,
      "eval_runtime": 15.9993,
      "eval_samples_per_second": 8.0,
      "eval_steps_per_second": 0.25,
      "step": 1408
    },
    {
      "epoch": 4.972209969122188,
      "grad_norm": 0.08796088536290868,
      "learning_rate": 3.5582822085889574e-06,
      "loss": 0.0044,
      "step": 1409
    },
    {
      "epoch": 4.975738861932069,
      "grad_norm": 0.07380671379903199,
      "learning_rate": 3.5460122699386505e-06,
      "loss": 0.0024,
      "step": 1410
    },
    {
      "epoch": 4.97926775474195,
      "grad_norm": 0.08684623675631221,
      "learning_rate": 3.533742331288344e-06,
      "loss": 0.0034,
      "step": 1411
    },
    {
      "epoch": 4.98279664755183,
      "grad_norm": 0.08528174200436754,
      "learning_rate": 3.521472392638037e-06,
      "loss": 0.0063,
      "step": 1412
    },
    {
      "epoch": 4.986325540361712,
      "grad_norm": 0.1284164167535212,
      "learning_rate": 3.50920245398773e-06,
      "loss": 0.0079,
      "step": 1413
    },
    {
      "epoch": 4.989854433171592,
      "grad_norm": 0.13745542014893178,
      "learning_rate": 3.4969325153374234e-06,
      "loss": 0.0038,
      "step": 1414
    },
    {
      "epoch": 4.9933833259814735,
      "grad_norm": 0.13288081740126848,
      "learning_rate": 3.4846625766871166e-06,
      "loss": 0.0048,
      "step": 1415
    },
    {
      "epoch": 4.996912218791354,
      "grad_norm": 0.1824791738633019,
      "learning_rate": 3.47239263803681e-06,
      "loss": 0.0064,
      "step": 1416
    },
    {
      "epoch": 4.996912218791354,
      "eval_average_f1": 0.8037346214648152,
      "eval_crossner_ai_f1": 0.7384615384095621,
      "eval_crossner_ai_precision": 0.6857142857123265,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7558139534375,
      "eval_crossner_politics_precision": 0.7471264367807504,
      "eval_crossner_politics_recall": 0.7647058823520415,
      "eval_crossner_science_f1": 0.6666666666137285,
      "eval_crossner_science_precision": 0.6521739130406428,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.9051094889997762,
      "eval_mit-restaurant_precision": 0.9117647058810121,
      "eval_mit-restaurant_recall": 0.8985507246363789,
      "eval_runtime": 16.0381,
      "eval_samples_per_second": 7.981,
      "eval_steps_per_second": 0.249,
      "step": 1416
    },
    {
      "epoch": 5.000441111601235,
      "grad_norm": 0.19986186286201535,
      "learning_rate": 3.4601226993865032e-06,
      "loss": 0.0094,
      "step": 1417
    },
    {
      "epoch": 5.003970004411116,
      "grad_norm": 0.12803630899309065,
      "learning_rate": 3.4478527607361968e-06,
      "loss": 0.0049,
      "step": 1418
    },
    {
      "epoch": 5.007498897220997,
      "grad_norm": 0.035214354593134704,
      "learning_rate": 3.43558282208589e-06,
      "loss": 0.003,
      "step": 1419
    },
    {
      "epoch": 5.0110277900308775,
      "grad_norm": 0.06137247714137648,
      "learning_rate": 3.4233128834355835e-06,
      "loss": 0.0015,
      "step": 1420
    },
    {
      "epoch": 5.014556682840759,
      "grad_norm": 0.06821479835655102,
      "learning_rate": 3.411042944785276e-06,
      "loss": 0.0039,
      "step": 1421
    },
    {
      "epoch": 5.018085575650639,
      "grad_norm": 0.06378852041589998,
      "learning_rate": 3.3987730061349693e-06,
      "loss": 0.0013,
      "step": 1422
    },
    {
      "epoch": 5.021614468460521,
      "grad_norm": 0.05316132765468894,
      "learning_rate": 3.386503067484663e-06,
      "loss": 0.0018,
      "step": 1423
    },
    {
      "epoch": 5.025143361270401,
      "grad_norm": 0.07783386702149503,
      "learning_rate": 3.374233128834356e-06,
      "loss": 0.0026,
      "step": 1424
    },
    {
      "epoch": 5.025143361270401,
      "eval_average_f1": 0.7879585452442418,
      "eval_crossner_ai_f1": 0.7499999999478516,
      "eval_crossner_ai_precision": 0.7058823529391003,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.6982248520201814,
      "eval_crossner_politics_precision": 0.7023809523801162,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.6190476189947846,
      "eval_crossner_science_precision": 0.6499999999967501,
      "eval_crossner_science_recall": 0.590909090906405,
      "eval_mit-movie_f1": 0.9387755101531289,
      "eval_mit-movie_precision": 0.9484536082464449,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8985507245863789,
      "eval_mit-restaurant_precision": 0.8985507246363789,
      "eval_mit-restaurant_recall": 0.8985507246363789,
      "eval_runtime": 16.0345,
      "eval_samples_per_second": 7.983,
      "eval_steps_per_second": 0.249,
      "step": 1424
    },
    {
      "epoch": 5.028672254080282,
      "grad_norm": 0.12163923825041298,
      "learning_rate": 3.3619631901840495e-06,
      "loss": 0.0043,
      "step": 1425
    },
    {
      "epoch": 5.032201146890163,
      "grad_norm": 0.06591310836392592,
      "learning_rate": 3.3496932515337426e-06,
      "loss": 0.0016,
      "step": 1426
    },
    {
      "epoch": 5.035730039700044,
      "grad_norm": 0.07198052292149622,
      "learning_rate": 3.337423312883436e-06,
      "loss": 0.0072,
      "step": 1427
    },
    {
      "epoch": 5.039258932509925,
      "grad_norm": 0.1733842497347379,
      "learning_rate": 3.3251533742331293e-06,
      "loss": 0.0023,
      "step": 1428
    },
    {
      "epoch": 5.042787825319806,
      "grad_norm": 0.032010306512800554,
      "learning_rate": 3.312883435582822e-06,
      "loss": 0.0028,
      "step": 1429
    },
    {
      "epoch": 5.046316718129686,
      "grad_norm": 0.3142718509362149,
      "learning_rate": 3.3006134969325156e-06,
      "loss": 0.0044,
      "step": 1430
    },
    {
      "epoch": 5.049845610939568,
      "grad_norm": 0.030560083352305832,
      "learning_rate": 3.2883435582822087e-06,
      "loss": 0.0019,
      "step": 1431
    },
    {
      "epoch": 5.053374503749448,
      "grad_norm": 0.04784816369812139,
      "learning_rate": 3.2760736196319022e-06,
      "loss": 0.0023,
      "step": 1432
    },
    {
      "epoch": 5.053374503749448,
      "eval_average_f1": 0.8194525410301879,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6923076922551037,
      "eval_crossner_literature_precision": 0.71999999999712,
      "eval_crossner_literature_recall": 0.6666666666641976,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7398843930127434,
      "eval_crossner_politics_precision": 0.7272727272719008,
      "eval_crossner_politics_recall": 0.7529411764697024,
      "eval_crossner_science_f1": 0.7619047618512471,
      "eval_crossner_science_precision": 0.799999999996,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9591836734184143,
      "eval_mit-movie_precision": 0.9690721649474545,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.9051094889997762,
      "eval_mit-restaurant_precision": 0.9117647058810121,
      "eval_mit-restaurant_recall": 0.8985507246363789,
      "eval_runtime": 15.9693,
      "eval_samples_per_second": 8.015,
      "eval_steps_per_second": 0.25,
      "step": 1432
    },
    {
      "epoch": 5.0569033965593295,
      "grad_norm": 0.058070055804897096,
      "learning_rate": 3.2638036809815954e-06,
      "loss": 0.0024,
      "step": 1433
    },
    {
      "epoch": 5.06043228936921,
      "grad_norm": 0.151252247639972,
      "learning_rate": 3.251533742331289e-06,
      "loss": 0.0037,
      "step": 1434
    },
    {
      "epoch": 5.063961182179091,
      "grad_norm": 0.14024091296243454,
      "learning_rate": 3.239263803680982e-06,
      "loss": 0.0019,
      "step": 1435
    },
    {
      "epoch": 5.067490074988973,
      "grad_norm": 0.07492623551162743,
      "learning_rate": 3.226993865030675e-06,
      "loss": 0.007,
      "step": 1436
    },
    {
      "epoch": 5.071018967798853,
      "grad_norm": 0.04395644391329458,
      "learning_rate": 3.2147239263803683e-06,
      "loss": 0.0054,
      "step": 1437
    },
    {
      "epoch": 5.074547860608734,
      "grad_norm": 0.05267052184310046,
      "learning_rate": 3.2024539877300614e-06,
      "loss": 0.0064,
      "step": 1438
    },
    {
      "epoch": 5.078076753418615,
      "grad_norm": 0.11137600755770954,
      "learning_rate": 3.190184049079755e-06,
      "loss": 0.0036,
      "step": 1439
    },
    {
      "epoch": 5.081605646228496,
      "grad_norm": 0.07423833224809917,
      "learning_rate": 3.177914110429448e-06,
      "loss": 0.0014,
      "step": 1440
    },
    {
      "epoch": 5.081605646228496,
      "eval_average_f1": 0.80031786755432,
      "eval_crossner_ai_f1": 0.7118644067272623,
      "eval_crossner_ai_precision": 0.7241379310319858,
      "eval_crossner_ai_recall": 0.6999999999976667,
      "eval_crossner_literature_f1": 0.7058823528885814,
      "eval_crossner_literature_precision": 0.749999999996875,
      "eval_crossner_literature_recall": 0.6666666666641976,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7471264367307767,
      "eval_crossner_politics_precision": 0.7303370786508647,
      "eval_crossner_politics_recall": 0.7647058823520415,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9435897435387876,
      "eval_mit-movie_precision": 0.9583333333323351,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8489208632581337,
      "eval_mit-restaurant_precision": 0.8428571428559387,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 16.5536,
      "eval_samples_per_second": 7.732,
      "eval_steps_per_second": 0.242,
      "step": 1440
    },
    {
      "epoch": 5.085134539038377,
      "grad_norm": 0.04754030680928273,
      "learning_rate": 3.165644171779141e-06,
      "loss": 0.0044,
      "step": 1441
    },
    {
      "epoch": 5.088663431848258,
      "grad_norm": 0.08512921768244937,
      "learning_rate": 3.1533742331288347e-06,
      "loss": 0.0011,
      "step": 1442
    },
    {
      "epoch": 5.092192324658138,
      "grad_norm": 0.03187763724449894,
      "learning_rate": 3.141104294478528e-06,
      "loss": 0.0007,
      "step": 1443
    },
    {
      "epoch": 5.09572121746802,
      "grad_norm": 0.04105286667213228,
      "learning_rate": 3.1288343558282214e-06,
      "loss": 0.002,
      "step": 1444
    },
    {
      "epoch": 5.0992501102779,
      "grad_norm": 0.054512606404784975,
      "learning_rate": 3.116564417177914e-06,
      "loss": 0.0013,
      "step": 1445
    },
    {
      "epoch": 5.1027790030877815,
      "grad_norm": 0.039562937257728525,
      "learning_rate": 3.1042944785276077e-06,
      "loss": 0.0014,
      "step": 1446
    },
    {
      "epoch": 5.106307895897662,
      "grad_norm": 0.04547397169791563,
      "learning_rate": 3.092024539877301e-06,
      "loss": 0.0012,
      "step": 1447
    },
    {
      "epoch": 5.109836788707543,
      "grad_norm": 0.08655195132338953,
      "learning_rate": 3.079754601226994e-06,
      "loss": 0.0028,
      "step": 1448
    },
    {
      "epoch": 5.109836788707543,
      "eval_average_f1": 0.8100005078468578,
      "eval_crossner_ai_f1": 0.7241379309820452,
      "eval_crossner_ai_precision": 0.7499999999973215,
      "eval_crossner_ai_recall": 0.6999999999976667,
      "eval_crossner_literature_f1": 0.653846153793713,
      "eval_crossner_literature_precision": 0.67999999999728,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7093023255305773,
      "eval_crossner_politics_precision": 0.7011494252865504,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.8095238094700682,
      "eval_crossner_science_precision": 0.84999999999575,
      "eval_crossner_science_recall": 0.7727272727237604,
      "eval_mit-movie_f1": 0.9591836734184143,
      "eval_mit-movie_precision": 0.9690721649474545,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8695652173400441,
      "eval_mit-restaurant_precision": 0.8695652173900441,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 15.9482,
      "eval_samples_per_second": 8.026,
      "eval_steps_per_second": 0.251,
      "step": 1448
    },
    {
      "epoch": 5.113365681517424,
      "grad_norm": 0.10414271426984743,
      "learning_rate": 3.0674846625766875e-06,
      "loss": 0.0041,
      "step": 1449
    },
    {
      "epoch": 5.116894574327305,
      "grad_norm": 0.027498217654989974,
      "learning_rate": 3.0552147239263806e-06,
      "loss": 0.0013,
      "step": 1450
    },
    {
      "epoch": 5.1204234671371855,
      "grad_norm": 0.08190035026250367,
      "learning_rate": 3.042944785276074e-06,
      "loss": 0.002,
      "step": 1451
    },
    {
      "epoch": 5.123952359947067,
      "grad_norm": 0.08634101352883822,
      "learning_rate": 3.0306748466257673e-06,
      "loss": 0.0052,
      "step": 1452
    },
    {
      "epoch": 5.127481252756947,
      "grad_norm": 0.12742510870426893,
      "learning_rate": 3.01840490797546e-06,
      "loss": 0.0031,
      "step": 1453
    },
    {
      "epoch": 5.131010145566829,
      "grad_norm": 0.10174489607446392,
      "learning_rate": 3.0061349693251535e-06,
      "loss": 0.0039,
      "step": 1454
    },
    {
      "epoch": 5.134539038376709,
      "grad_norm": 0.039128173814986386,
      "learning_rate": 2.9938650306748466e-06,
      "loss": 0.0018,
      "step": 1455
    },
    {
      "epoch": 5.13806793118659,
      "grad_norm": 0.04109346009377015,
      "learning_rate": 2.98159509202454e-06,
      "loss": 0.0013,
      "step": 1456
    },
    {
      "epoch": 5.13806793118659,
      "eval_average_f1": 0.8061360686307524,
      "eval_crossner_ai_f1": 0.7384615384095621,
      "eval_crossner_ai_precision": 0.6857142857123265,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.6976744185538466,
      "eval_crossner_politics_precision": 0.6896551724130003,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.7391304347294897,
      "eval_crossner_science_precision": 0.708333333330382,
      "eval_crossner_science_recall": 0.7727272727237604,
      "eval_mit-movie_f1": 0.9591836734184143,
      "eval_mit-movie_precision": 0.9690721649474545,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.0264,
      "eval_samples_per_second": 7.987,
      "eval_steps_per_second": 0.25,
      "step": 1456
    },
    {
      "epoch": 5.141596823996471,
      "grad_norm": 0.055479192338498365,
      "learning_rate": 2.9693251533742333e-06,
      "loss": 0.0037,
      "step": 1457
    },
    {
      "epoch": 5.145125716806352,
      "grad_norm": 0.07417634015819691,
      "learning_rate": 2.957055214723927e-06,
      "loss": 0.003,
      "step": 1458
    },
    {
      "epoch": 5.148654609616233,
      "grad_norm": 0.06922526660946547,
      "learning_rate": 2.94478527607362e-06,
      "loss": 0.0019,
      "step": 1459
    },
    {
      "epoch": 5.152183502426114,
      "grad_norm": 0.06150008318428291,
      "learning_rate": 2.9325153374233127e-06,
      "loss": 0.0022,
      "step": 1460
    },
    {
      "epoch": 5.1557123952359944,
      "grad_norm": 0.11558981808036284,
      "learning_rate": 2.9202453987730062e-06,
      "loss": 0.0042,
      "step": 1461
    },
    {
      "epoch": 5.159241288045876,
      "grad_norm": 0.1039716240655035,
      "learning_rate": 2.9079754601226994e-06,
      "loss": 0.0033,
      "step": 1462
    },
    {
      "epoch": 5.162770180855756,
      "grad_norm": 0.04902941222846489,
      "learning_rate": 2.895705521472393e-06,
      "loss": 0.0017,
      "step": 1463
    },
    {
      "epoch": 5.166299073665638,
      "grad_norm": 0.05864284763067795,
      "learning_rate": 2.883435582822086e-06,
      "loss": 0.001,
      "step": 1464
    },
    {
      "epoch": 5.166299073665638,
      "eval_average_f1": 0.8028486272401868,
      "eval_crossner_ai_f1": 0.7619047618524566,
      "eval_crossner_ai_precision": 0.7272727272705234,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.7058823528885814,
      "eval_crossner_literature_precision": 0.749999999996875,
      "eval_crossner_literature_recall": 0.6666666666641976,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7218934910734078,
      "eval_crossner_politics_precision": 0.7261904761896116,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.0563,
      "eval_samples_per_second": 7.972,
      "eval_steps_per_second": 0.249,
      "step": 1464
    },
    {
      "epoch": 5.169827966475518,
      "grad_norm": 0.07089216648563167,
      "learning_rate": 2.8711656441717796e-06,
      "loss": 0.0012,
      "step": 1465
    },
    {
      "epoch": 5.173356859285399,
      "grad_norm": 0.04326996966788445,
      "learning_rate": 2.8588957055214727e-06,
      "loss": 0.0037,
      "step": 1466
    },
    {
      "epoch": 5.17688575209528,
      "grad_norm": 0.10643646220940552,
      "learning_rate": 2.8466257668711663e-06,
      "loss": 0.0036,
      "step": 1467
    },
    {
      "epoch": 5.180414644905161,
      "grad_norm": 0.07783673869295647,
      "learning_rate": 2.834355828220859e-06,
      "loss": 0.003,
      "step": 1468
    },
    {
      "epoch": 5.183943537715042,
      "grad_norm": 0.06467074804460059,
      "learning_rate": 2.822085889570552e-06,
      "loss": 0.0036,
      "step": 1469
    },
    {
      "epoch": 5.187472430524923,
      "grad_norm": 0.16280673448820837,
      "learning_rate": 2.8098159509202456e-06,
      "loss": 0.0029,
      "step": 1470
    },
    {
      "epoch": 5.191001323334803,
      "grad_norm": 0.07721018734326002,
      "learning_rate": 2.7975460122699388e-06,
      "loss": 0.0017,
      "step": 1471
    },
    {
      "epoch": 5.194530216144685,
      "grad_norm": 0.09055240658640536,
      "learning_rate": 2.7852760736196323e-06,
      "loss": 0.0046,
      "step": 1472
    },
    {
      "epoch": 5.194530216144685,
      "eval_average_f1": 0.8099529921433172,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6938775509680968,
      "eval_crossner_literature_precision": 0.7727272727237604,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7398843930127434,
      "eval_crossner_politics_precision": 0.7272727272719008,
      "eval_crossner_politics_recall": 0.7529411764697024,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9591836734184143,
      "eval_mit-movie_precision": 0.9690721649474545,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8985507245863789,
      "eval_mit-restaurant_precision": 0.8985507246363789,
      "eval_mit-restaurant_recall": 0.8985507246363789,
      "eval_runtime": 15.9071,
      "eval_samples_per_second": 8.047,
      "eval_steps_per_second": 0.251,
      "step": 1472
    },
    {
      "epoch": 5.198059108954565,
      "grad_norm": 0.1703618645215412,
      "learning_rate": 2.7730061349693254e-06,
      "loss": 0.0018,
      "step": 1473
    },
    {
      "epoch": 5.2015880017644465,
      "grad_norm": 0.07918845807973471,
      "learning_rate": 2.760736196319019e-06,
      "loss": 0.0031,
      "step": 1474
    },
    {
      "epoch": 5.205116894574327,
      "grad_norm": 0.12040637258631834,
      "learning_rate": 2.748466257668712e-06,
      "loss": 0.0052,
      "step": 1475
    },
    {
      "epoch": 5.208645787384208,
      "grad_norm": 0.044244818488631744,
      "learning_rate": 2.736196319018405e-06,
      "loss": 0.001,
      "step": 1476
    },
    {
      "epoch": 5.212174680194089,
      "grad_norm": 0.0147370050104519,
      "learning_rate": 2.7239263803680983e-06,
      "loss": 0.0013,
      "step": 1477
    },
    {
      "epoch": 5.21570357300397,
      "grad_norm": 0.04843815045535284,
      "learning_rate": 2.7116564417177915e-06,
      "loss": 0.0028,
      "step": 1478
    },
    {
      "epoch": 5.2192324658138505,
      "grad_norm": 0.15396609643477124,
      "learning_rate": 2.699386503067485e-06,
      "loss": 0.004,
      "step": 1479
    },
    {
      "epoch": 5.222761358623732,
      "grad_norm": 0.029559505996929036,
      "learning_rate": 2.687116564417178e-06,
      "loss": 0.001,
      "step": 1480
    },
    {
      "epoch": 5.222761358623732,
      "eval_average_f1": 0.7923360073295583,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6666666666146702,
      "eval_crossner_literature_precision": 0.7619047619011339,
      "eval_crossner_literature_recall": 0.5925925925903979,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.6982248520201814,
      "eval_crossner_politics_precision": 0.7023809523801162,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9538461537951873,
      "eval_mit-movie_precision": 0.9687499999989909,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8970588234781033,
      "eval_mit-restaurant_precision": 0.9104477611926709,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.3586,
      "eval_samples_per_second": 7.825,
      "eval_steps_per_second": 0.245,
      "step": 1480
    },
    {
      "epoch": 5.226290251433612,
      "grad_norm": 0.04317606687835253,
      "learning_rate": 2.6748466257668717e-06,
      "loss": 0.0033,
      "step": 1481
    },
    {
      "epoch": 5.229819144243494,
      "grad_norm": 0.03706501832004339,
      "learning_rate": 2.662576687116565e-06,
      "loss": 0.0007,
      "step": 1482
    },
    {
      "epoch": 5.233348037053375,
      "grad_norm": 0.07387606033421097,
      "learning_rate": 2.650306748466258e-06,
      "loss": 0.0025,
      "step": 1483
    },
    {
      "epoch": 5.236876929863255,
      "grad_norm": 0.04572444005058763,
      "learning_rate": 2.638036809815951e-06,
      "loss": 0.0009,
      "step": 1484
    },
    {
      "epoch": 5.240405822673137,
      "grad_norm": 0.16699301290029245,
      "learning_rate": 2.625766871165644e-06,
      "loss": 0.0043,
      "step": 1485
    },
    {
      "epoch": 5.243934715483017,
      "grad_norm": 0.09729908478731339,
      "learning_rate": 2.6134969325153377e-06,
      "loss": 0.003,
      "step": 1486
    },
    {
      "epoch": 5.2474636082928985,
      "grad_norm": 0.05433037499724049,
      "learning_rate": 2.601226993865031e-06,
      "loss": 0.0023,
      "step": 1487
    },
    {
      "epoch": 5.250992501102779,
      "grad_norm": 0.1172347053183552,
      "learning_rate": 2.588957055214724e-06,
      "loss": 0.0026,
      "step": 1488
    },
    {
      "epoch": 5.250992501102779,
      "eval_average_f1": 0.7987669945871055,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.7058823528885814,
      "eval_crossner_literature_precision": 0.749999999996875,
      "eval_crossner_literature_recall": 0.6666666666641976,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7218934910734078,
      "eval_crossner_politics_precision": 0.7261904761896116,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 15.9579,
      "eval_samples_per_second": 8.021,
      "eval_steps_per_second": 0.251,
      "step": 1488
    },
    {
      "epoch": 5.25452139391266,
      "grad_norm": 0.05522454900089003,
      "learning_rate": 2.5766871165644175e-06,
      "loss": 0.0012,
      "step": 1489
    },
    {
      "epoch": 5.258050286722541,
      "grad_norm": 0.10495035937363822,
      "learning_rate": 2.5644171779141107e-06,
      "loss": 0.0015,
      "step": 1490
    },
    {
      "epoch": 5.261579179532422,
      "grad_norm": 0.037031969793281626,
      "learning_rate": 2.5521472392638042e-06,
      "loss": 0.0011,
      "step": 1491
    },
    {
      "epoch": 5.2651080723423025,
      "grad_norm": 0.11770136938832908,
      "learning_rate": 2.539877300613497e-06,
      "loss": 0.0019,
      "step": 1492
    },
    {
      "epoch": 5.268636965152184,
      "grad_norm": 0.08431589045133768,
      "learning_rate": 2.52760736196319e-06,
      "loss": 0.0013,
      "step": 1493
    },
    {
      "epoch": 5.272165857962064,
      "grad_norm": 0.09223594059617887,
      "learning_rate": 2.5153374233128836e-06,
      "loss": 0.0007,
      "step": 1494
    },
    {
      "epoch": 5.275694750771946,
      "grad_norm": 0.048602553513876254,
      "learning_rate": 2.5030674846625767e-06,
      "loss": 0.0008,
      "step": 1495
    },
    {
      "epoch": 5.279223643581826,
      "grad_norm": 0.04460179018502747,
      "learning_rate": 2.4907975460122703e-06,
      "loss": 0.0012,
      "step": 1496
    },
    {
      "epoch": 5.279223643581826,
      "eval_average_f1": 0.7866008700703558,
      "eval_crossner_ai_f1": 0.7118644067272623,
      "eval_crossner_ai_precision": 0.7241379310319858,
      "eval_crossner_ai_recall": 0.6999999999976667,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7093023255305773,
      "eval_crossner_politics_precision": 0.7011494252865504,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.6222222221694815,
      "eval_crossner_science_precision": 0.6086956521712665,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9543147207612152,
      "eval_mit-movie_precision": 0.9591836734684089,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 15.9661,
      "eval_samples_per_second": 8.017,
      "eval_steps_per_second": 0.251,
      "step": 1496
    },
    {
      "epoch": 5.282752536391707,
      "grad_norm": 0.060199896967718915,
      "learning_rate": 2.4785276073619634e-06,
      "loss": 0.0035,
      "step": 1497
    },
    {
      "epoch": 5.286281429201588,
      "grad_norm": 0.06907226232428633,
      "learning_rate": 2.4662576687116565e-06,
      "loss": 0.0042,
      "step": 1498
    },
    {
      "epoch": 5.289810322011469,
      "grad_norm": 0.11681356773250764,
      "learning_rate": 2.45398773006135e-06,
      "loss": 0.0037,
      "step": 1499
    },
    {
      "epoch": 5.29333921482135,
      "grad_norm": 0.04028387791617514,
      "learning_rate": 2.441717791411043e-06,
      "loss": 0.0012,
      "step": 1500
    },
    {
      "epoch": 5.296868107631231,
      "grad_norm": 0.11983052604757836,
      "learning_rate": 2.4294478527607367e-06,
      "loss": 0.0039,
      "step": 1501
    },
    {
      "epoch": 5.300397000441111,
      "grad_norm": 0.09923925075346479,
      "learning_rate": 2.4171779141104294e-06,
      "loss": 0.002,
      "step": 1502
    },
    {
      "epoch": 5.303925893250993,
      "grad_norm": 0.08426560124514185,
      "learning_rate": 2.404907975460123e-06,
      "loss": 0.0027,
      "step": 1503
    },
    {
      "epoch": 5.307454786060873,
      "grad_norm": 0.09192295749616339,
      "learning_rate": 2.392638036809816e-06,
      "loss": 0.0031,
      "step": 1504
    },
    {
      "epoch": 5.307454786060873,
      "eval_average_f1": 0.779924002867025,
      "eval_crossner_ai_f1": 0.6885245901116905,
      "eval_crossner_ai_precision": 0.6774193548365245,
      "eval_crossner_ai_recall": 0.6999999999976667,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.714285714234871,
      "eval_crossner_politics_precision": 0.7228915662641893,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.6222222221694815,
      "eval_crossner_science_precision": 0.6086956521712665,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9543147207612152,
      "eval_mit-movie_precision": 0.9591836734684089,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8695652173400441,
      "eval_mit-restaurant_precision": 0.8695652173900441,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 16.0571,
      "eval_samples_per_second": 7.972,
      "eval_steps_per_second": 0.249,
      "step": 1504
    },
    {
      "epoch": 5.3109836788707545,
      "grad_norm": 0.03484829659424722,
      "learning_rate": 2.3803680981595097e-06,
      "loss": 0.0015,
      "step": 1505
    },
    {
      "epoch": 5.314512571680635,
      "grad_norm": 0.22251522885401084,
      "learning_rate": 2.3680981595092028e-06,
      "loss": 0.0044,
      "step": 1506
    },
    {
      "epoch": 5.318041464490516,
      "grad_norm": 0.019293552390867365,
      "learning_rate": 2.355828220858896e-06,
      "loss": 0.0006,
      "step": 1507
    },
    {
      "epoch": 5.321570357300397,
      "grad_norm": 0.08212067917793225,
      "learning_rate": 2.343558282208589e-06,
      "loss": 0.0014,
      "step": 1508
    },
    {
      "epoch": 5.325099250110278,
      "grad_norm": 0.11256312198853623,
      "learning_rate": 2.331288343558282e-06,
      "loss": 0.0054,
      "step": 1509
    },
    {
      "epoch": 5.3286281429201585,
      "grad_norm": 0.0866772071033082,
      "learning_rate": 2.3190184049079757e-06,
      "loss": 0.0019,
      "step": 1510
    },
    {
      "epoch": 5.33215703573004,
      "grad_norm": 0.06617774286003543,
      "learning_rate": 2.306748466257669e-06,
      "loss": 0.001,
      "step": 1511
    },
    {
      "epoch": 5.33568592853992,
      "grad_norm": 0.08066158163860099,
      "learning_rate": 2.2944785276073624e-06,
      "loss": 0.0023,
      "step": 1512
    },
    {
      "epoch": 5.33568592853992,
      "eval_average_f1": 0.8113275655933856,
      "eval_crossner_ai_f1": 0.7499999999478516,
      "eval_crossner_ai_precision": 0.7058823529391003,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7093023255305773,
      "eval_crossner_politics_precision": 0.7011494252865504,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.7619047618512471,
      "eval_crossner_science_precision": 0.799999999996,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8985507245863789,
      "eval_mit-restaurant_precision": 0.8985507246363789,
      "eval_mit-restaurant_recall": 0.8985507246363789,
      "eval_runtime": 16.0512,
      "eval_samples_per_second": 7.975,
      "eval_steps_per_second": 0.249,
      "step": 1512
    },
    {
      "epoch": 5.339214821349802,
      "grad_norm": 0.07445965118866098,
      "learning_rate": 2.282208588957055e-06,
      "loss": 0.0013,
      "step": 1513
    },
    {
      "epoch": 5.342743714159682,
      "grad_norm": 0.32130014696103015,
      "learning_rate": 2.2699386503067486e-06,
      "loss": 0.006,
      "step": 1514
    },
    {
      "epoch": 5.346272606969563,
      "grad_norm": 0.12840258575088923,
      "learning_rate": 2.2576687116564417e-06,
      "loss": 0.0019,
      "step": 1515
    },
    {
      "epoch": 5.349801499779444,
      "grad_norm": 0.05083114437850148,
      "learning_rate": 2.2453987730061353e-06,
      "loss": 0.0026,
      "step": 1516
    },
    {
      "epoch": 5.353330392589325,
      "grad_norm": 0.6597769962683728,
      "learning_rate": 2.2331288343558284e-06,
      "loss": 0.0049,
      "step": 1517
    },
    {
      "epoch": 5.356859285399206,
      "grad_norm": 0.05302303834160424,
      "learning_rate": 2.2208588957055215e-06,
      "loss": 0.0002,
      "step": 1518
    },
    {
      "epoch": 5.360388178209087,
      "grad_norm": 0.03308236552382745,
      "learning_rate": 2.208588957055215e-06,
      "loss": 0.0009,
      "step": 1519
    },
    {
      "epoch": 5.363917071018967,
      "grad_norm": 0.08706915337367328,
      "learning_rate": 2.1963190184049082e-06,
      "loss": 0.0013,
      "step": 1520
    },
    {
      "epoch": 5.363917071018967,
      "eval_average_f1": 0.7989709471570359,
      "eval_crossner_ai_f1": 0.7272727272209366,
      "eval_crossner_ai_precision": 0.6666666666648148,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7017543859140932,
      "eval_crossner_politics_precision": 0.6976744186038399,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9543147207612152,
      "eval_mit-movie_precision": 0.9591836734684089,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.2095,
      "eval_samples_per_second": 7.897,
      "eval_steps_per_second": 0.247,
      "step": 1520
    },
    {
      "epoch": 5.367445963828849,
      "grad_norm": 0.04377016024497182,
      "learning_rate": 2.1840490797546013e-06,
      "loss": 0.0017,
      "step": 1521
    },
    {
      "epoch": 5.370974856638729,
      "grad_norm": 0.028248438806115474,
      "learning_rate": 2.1717791411042945e-06,
      "loss": 0.0009,
      "step": 1522
    },
    {
      "epoch": 5.3745037494486105,
      "grad_norm": 0.0585216177563369,
      "learning_rate": 2.159509202453988e-06,
      "loss": 0.0023,
      "step": 1523
    },
    {
      "epoch": 5.378032642258491,
      "grad_norm": 0.048473174278755304,
      "learning_rate": 2.147239263803681e-06,
      "loss": 0.0015,
      "step": 1524
    },
    {
      "epoch": 5.381561535068372,
      "grad_norm": 0.08857291619164272,
      "learning_rate": 2.1349693251533743e-06,
      "loss": 0.0068,
      "step": 1525
    },
    {
      "epoch": 5.385090427878254,
      "grad_norm": 0.032620350461633425,
      "learning_rate": 2.122699386503068e-06,
      "loss": 0.0011,
      "step": 1526
    },
    {
      "epoch": 5.388619320688134,
      "grad_norm": 0.061148583873175406,
      "learning_rate": 2.110429447852761e-06,
      "loss": 0.0009,
      "step": 1527
    },
    {
      "epoch": 5.3921482134980145,
      "grad_norm": 0.08030380744221763,
      "learning_rate": 2.098159509202454e-06,
      "loss": 0.001,
      "step": 1528
    },
    {
      "epoch": 5.3921482134980145,
      "eval_average_f1": 0.8147304485928741,
      "eval_crossner_ai_f1": 0.7499999999478516,
      "eval_crossner_ai_precision": 0.7058823529391003,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7514450866543485,
      "eval_crossner_politics_precision": 0.7386363636355242,
      "eval_crossner_politics_recall": 0.7647058823520415,
      "eval_crossner_science_f1": 0.7441860464581938,
      "eval_crossner_science_precision": 0.7619047619011339,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.0232,
      "eval_samples_per_second": 7.988,
      "eval_steps_per_second": 0.25,
      "step": 1528
    },
    {
      "epoch": 5.395677106307896,
      "grad_norm": 0.07975976743122928,
      "learning_rate": 2.085889570552147e-06,
      "loss": 0.0031,
      "step": 1529
    },
    {
      "epoch": 5.399205999117777,
      "grad_norm": 0.04553769771646619,
      "learning_rate": 2.0736196319018407e-06,
      "loss": 0.0006,
      "step": 1530
    },
    {
      "epoch": 5.402734891927658,
      "grad_norm": 0.2116226706734327,
      "learning_rate": 2.061349693251534e-06,
      "loss": 0.0024,
      "step": 1531
    },
    {
      "epoch": 5.406263784737539,
      "grad_norm": 0.04253439385477676,
      "learning_rate": 2.0490797546012274e-06,
      "loss": 0.0018,
      "step": 1532
    },
    {
      "epoch": 5.409792677547419,
      "grad_norm": 0.06151895847365166,
      "learning_rate": 2.03680981595092e-06,
      "loss": 0.0026,
      "step": 1533
    },
    {
      "epoch": 5.413321570357301,
      "grad_norm": 0.02214961584091866,
      "learning_rate": 2.0245398773006137e-06,
      "loss": 0.0005,
      "step": 1534
    },
    {
      "epoch": 5.416850463167181,
      "grad_norm": 0.0343150065754846,
      "learning_rate": 2.0122699386503068e-06,
      "loss": 0.0015,
      "step": 1535
    },
    {
      "epoch": 5.4203793559770626,
      "grad_norm": 0.07936580990075358,
      "learning_rate": 2.0000000000000003e-06,
      "loss": 0.0015,
      "step": 1536
    },
    {
      "epoch": 5.4203793559770626,
      "eval_average_f1": 0.7977655638326626,
      "eval_crossner_ai_f1": 0.7096774193026015,
      "eval_crossner_ai_precision": 0.6874999999978515,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7017543859140932,
      "eval_crossner_politics_precision": 0.6976744186038399,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8985507245863789,
      "eval_mit-restaurant_precision": 0.8985507246363789,
      "eval_mit-restaurant_recall": 0.8985507246363789,
      "eval_runtime": 16.2091,
      "eval_samples_per_second": 7.897,
      "eval_steps_per_second": 0.247,
      "step": 1536
    },
    {
      "epoch": 5.423908248786943,
      "grad_norm": 0.09121394734450426,
      "learning_rate": 1.9877300613496935e-06,
      "loss": 0.0014,
      "step": 1537
    },
    {
      "epoch": 5.427437141596824,
      "grad_norm": 0.034286217206637964,
      "learning_rate": 1.9754601226993866e-06,
      "loss": 0.0009,
      "step": 1538
    },
    {
      "epoch": 5.430966034406705,
      "grad_norm": 0.0364115633586747,
      "learning_rate": 1.96319018404908e-06,
      "loss": 0.0015,
      "step": 1539
    },
    {
      "epoch": 5.434494927216586,
      "grad_norm": 0.15307034324731744,
      "learning_rate": 1.9509202453987733e-06,
      "loss": 0.0028,
      "step": 1540
    },
    {
      "epoch": 5.438023820026467,
      "grad_norm": 0.035311051796337777,
      "learning_rate": 1.9386503067484664e-06,
      "loss": 0.0014,
      "step": 1541
    },
    {
      "epoch": 5.441552712836348,
      "grad_norm": 0.16184811623851147,
      "learning_rate": 1.9263803680981595e-06,
      "loss": 0.0048,
      "step": 1542
    },
    {
      "epoch": 5.445081605646228,
      "grad_norm": 0.0679127339029418,
      "learning_rate": 1.914110429447853e-06,
      "loss": 0.0017,
      "step": 1543
    },
    {
      "epoch": 5.44861049845611,
      "grad_norm": 0.21446116802370915,
      "learning_rate": 1.9018404907975464e-06,
      "loss": 0.0016,
      "step": 1544
    },
    {
      "epoch": 5.44861049845611,
      "eval_average_f1": 0.805728026367757,
      "eval_crossner_ai_f1": 0.7499999999478516,
      "eval_crossner_ai_precision": 0.7058823529391003,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7349397589852882,
      "eval_crossner_politics_precision": 0.7530864197521566,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.6976744185514333,
      "eval_crossner_science_precision": 0.714285714282313,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 15.9548,
      "eval_samples_per_second": 8.023,
      "eval_steps_per_second": 0.251,
      "step": 1544
    },
    {
      "epoch": 5.45213939126599,
      "grad_norm": 0.11197154764012208,
      "learning_rate": 1.8895705521472393e-06,
      "loss": 0.0041,
      "step": 1545
    },
    {
      "epoch": 5.4556682840758715,
      "grad_norm": 0.03554463759391994,
      "learning_rate": 1.8773006134969326e-06,
      "loss": 0.0015,
      "step": 1546
    },
    {
      "epoch": 5.459197176885752,
      "grad_norm": 0.1136143571639731,
      "learning_rate": 1.865030674846626e-06,
      "loss": 0.002,
      "step": 1547
    },
    {
      "epoch": 5.462726069695633,
      "grad_norm": 0.033953769854470905,
      "learning_rate": 1.8527607361963193e-06,
      "loss": 0.002,
      "step": 1548
    },
    {
      "epoch": 5.466254962505514,
      "grad_norm": 0.040580040211367685,
      "learning_rate": 1.8404907975460124e-06,
      "loss": 0.002,
      "step": 1549
    },
    {
      "epoch": 5.469783855315395,
      "grad_norm": 0.03131939960318755,
      "learning_rate": 1.8282208588957056e-06,
      "loss": 0.0005,
      "step": 1550
    },
    {
      "epoch": 5.4733127481252755,
      "grad_norm": 0.06449779855197314,
      "learning_rate": 1.815950920245399e-06,
      "loss": 0.0024,
      "step": 1551
    },
    {
      "epoch": 5.476841640935157,
      "grad_norm": 0.08855059055161602,
      "learning_rate": 1.8036809815950922e-06,
      "loss": 0.002,
      "step": 1552
    },
    {
      "epoch": 5.476841640935157,
      "eval_average_f1": 0.7978158038618675,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7218934910734078,
      "eval_crossner_politics_precision": 0.7261904761896116,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9543147207612152,
      "eval_mit-movie_precision": 0.9591836734684089,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 15.9468,
      "eval_samples_per_second": 8.027,
      "eval_steps_per_second": 0.251,
      "step": 1552
    },
    {
      "epoch": 5.480370533745037,
      "grad_norm": 0.02602512174292078,
      "learning_rate": 1.7914110429447854e-06,
      "loss": 0.0006,
      "step": 1553
    },
    {
      "epoch": 5.483899426554919,
      "grad_norm": 0.07991662545643557,
      "learning_rate": 1.7791411042944787e-06,
      "loss": 0.0029,
      "step": 1554
    },
    {
      "epoch": 5.487428319364799,
      "grad_norm": 0.05355838656084574,
      "learning_rate": 1.766871165644172e-06,
      "loss": 0.0017,
      "step": 1555
    },
    {
      "epoch": 5.49095721217468,
      "grad_norm": 0.13820696521898804,
      "learning_rate": 1.754601226993865e-06,
      "loss": 0.0025,
      "step": 1556
    },
    {
      "epoch": 5.494486104984561,
      "grad_norm": 0.054151651627465894,
      "learning_rate": 1.7423312883435583e-06,
      "loss": 0.0012,
      "step": 1557
    },
    {
      "epoch": 5.498014997794442,
      "grad_norm": 0.044815307658085196,
      "learning_rate": 1.7300613496932516e-06,
      "loss": 0.0018,
      "step": 1558
    },
    {
      "epoch": 5.501543890604323,
      "grad_norm": 0.08536970798953483,
      "learning_rate": 1.717791411042945e-06,
      "loss": 0.0018,
      "step": 1559
    },
    {
      "epoch": 5.505072783414204,
      "grad_norm": 0.05695458044997177,
      "learning_rate": 1.705521472392638e-06,
      "loss": 0.0025,
      "step": 1560
    },
    {
      "epoch": 5.505072783414204,
      "eval_average_f1": 0.8036152134697679,
      "eval_crossner_ai_f1": 0.7301587301065257,
      "eval_crossner_ai_precision": 0.6969696969675849,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7261904761396187,
      "eval_crossner_politics_precision": 0.7349397590352591,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9543147207612152,
      "eval_mit-movie_precision": 0.9591836734684089,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.875912408707848,
      "eval_mit-restaurant_precision": 0.8823529411751729,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 15.9562,
      "eval_samples_per_second": 8.022,
      "eval_steps_per_second": 0.251,
      "step": 1560
    },
    {
      "epoch": 5.508601676224084,
      "grad_norm": 0.03132022663267872,
      "learning_rate": 1.6932515337423314e-06,
      "loss": 0.0011,
      "step": 1561
    },
    {
      "epoch": 5.512130569033966,
      "grad_norm": 0.06657037368286316,
      "learning_rate": 1.6809815950920248e-06,
      "loss": 0.0012,
      "step": 1562
    },
    {
      "epoch": 5.515659461843846,
      "grad_norm": 0.06890518071411089,
      "learning_rate": 1.668711656441718e-06,
      "loss": 0.0039,
      "step": 1563
    },
    {
      "epoch": 5.5191883546537275,
      "grad_norm": 0.051595209775296615,
      "learning_rate": 1.656441717791411e-06,
      "loss": 0.0016,
      "step": 1564
    },
    {
      "epoch": 5.522717247463608,
      "grad_norm": 0.09524783516567631,
      "learning_rate": 1.6441717791411043e-06,
      "loss": 0.0029,
      "step": 1565
    },
    {
      "epoch": 5.526246140273489,
      "grad_norm": 0.0534467136864067,
      "learning_rate": 1.6319018404907977e-06,
      "loss": 0.0033,
      "step": 1566
    },
    {
      "epoch": 5.52977503308337,
      "grad_norm": 0.12417262033522283,
      "learning_rate": 1.619631901840491e-06,
      "loss": 0.0025,
      "step": 1567
    },
    {
      "epoch": 5.533303925893251,
      "grad_norm": 0.07875587136845345,
      "learning_rate": 1.6073619631901841e-06,
      "loss": 0.0015,
      "step": 1568
    },
    {
      "epoch": 5.533303925893251,
      "eval_average_f1": 0.8012026205256192,
      "eval_crossner_ai_f1": 0.7301587301065257,
      "eval_crossner_ai_precision": 0.6969696969675849,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7093023255305773,
      "eval_crossner_politics_precision": 0.7011494252865504,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9543147207612152,
      "eval_mit-movie_precision": 0.9591836734684089,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.875912408707848,
      "eval_mit-restaurant_precision": 0.8823529411751729,
      "eval_mit-restaurant_recall": 0.8695652173900441,
      "eval_runtime": 15.9432,
      "eval_samples_per_second": 8.028,
      "eval_steps_per_second": 0.251,
      "step": 1568
    },
    {
      "epoch": 5.536832818703132,
      "grad_norm": 0.025129085373206714,
      "learning_rate": 1.5950920245398775e-06,
      "loss": 0.0005,
      "step": 1569
    },
    {
      "epoch": 5.540361711513013,
      "grad_norm": 0.14991202307648865,
      "learning_rate": 1.5828220858895706e-06,
      "loss": 0.0031,
      "step": 1570
    },
    {
      "epoch": 5.543890604322893,
      "grad_norm": 0.09239239995778163,
      "learning_rate": 1.570552147239264e-06,
      "loss": 0.0028,
      "step": 1571
    },
    {
      "epoch": 5.547419497132775,
      "grad_norm": 0.09155822737427746,
      "learning_rate": 1.558282208588957e-06,
      "loss": 0.0035,
      "step": 1572
    },
    {
      "epoch": 5.550948389942656,
      "grad_norm": 0.1149799075014942,
      "learning_rate": 1.5460122699386504e-06,
      "loss": 0.0029,
      "step": 1573
    },
    {
      "epoch": 5.554477282752536,
      "grad_norm": 0.19395116353500974,
      "learning_rate": 1.5337423312883437e-06,
      "loss": 0.0037,
      "step": 1574
    },
    {
      "epoch": 5.558006175562417,
      "grad_norm": 0.09284251301775032,
      "learning_rate": 1.521472392638037e-06,
      "loss": 0.0018,
      "step": 1575
    },
    {
      "epoch": 5.561535068372298,
      "grad_norm": 0.04089081029081071,
      "learning_rate": 1.50920245398773e-06,
      "loss": 0.0015,
      "step": 1576
    },
    {
      "epoch": 5.561535068372298,
      "eval_average_f1": 0.8134220664170496,
      "eval_crossner_ai_f1": 0.7936507935983874,
      "eval_crossner_ai_precision": 0.7575757575734618,
      "eval_crossner_ai_recall": 0.8333333333305556,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7261904761396187,
      "eval_crossner_politics_precision": 0.7349397590352591,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.944162436497266,
      "eval_mit-movie_precision": 0.9489795918357663,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.9051094889997762,
      "eval_mit-restaurant_precision": 0.9117647058810121,
      "eval_mit-restaurant_recall": 0.8985507246363789,
      "eval_runtime": 15.975,
      "eval_samples_per_second": 8.012,
      "eval_steps_per_second": 0.25,
      "step": 1576
    },
    {
      "epoch": 5.5650639611821795,
      "grad_norm": 0.0638090960223347,
      "learning_rate": 1.4969325153374233e-06,
      "loss": 0.001,
      "step": 1577
    },
    {
      "epoch": 5.56859285399206,
      "grad_norm": 0.1124989281046983,
      "learning_rate": 1.4846625766871167e-06,
      "loss": 0.0027,
      "step": 1578
    },
    {
      "epoch": 5.572121746801941,
      "grad_norm": 0.20078766808846252,
      "learning_rate": 1.47239263803681e-06,
      "loss": 0.0052,
      "step": 1579
    },
    {
      "epoch": 5.575650639611822,
      "grad_norm": 0.044484057433023534,
      "learning_rate": 1.4601226993865031e-06,
      "loss": 0.0015,
      "step": 1580
    },
    {
      "epoch": 5.579179532421703,
      "grad_norm": 0.03172161875339825,
      "learning_rate": 1.4478527607361965e-06,
      "loss": 0.0016,
      "step": 1581
    },
    {
      "epoch": 5.5827084252315835,
      "grad_norm": 0.0946011450390745,
      "learning_rate": 1.4355828220858898e-06,
      "loss": 0.0042,
      "step": 1582
    },
    {
      "epoch": 5.586237318041465,
      "grad_norm": 0.3652169126528216,
      "learning_rate": 1.4233128834355831e-06,
      "loss": 0.0025,
      "step": 1583
    },
    {
      "epoch": 5.589766210851345,
      "grad_norm": 0.0677011853657086,
      "learning_rate": 1.411042944785276e-06,
      "loss": 0.0022,
      "step": 1584
    },
    {
      "epoch": 5.589766210851345,
      "eval_average_f1": 0.8034953098822253,
      "eval_crossner_ai_f1": 0.7301587301065257,
      "eval_crossner_ai_precision": 0.6969696969675849,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7305389221048298,
      "eval_crossner_politics_precision": 0.7439024390234831,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9543147207612152,
      "eval_mit-movie_precision": 0.9591836734684089,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 15.8893,
      "eval_samples_per_second": 8.056,
      "eval_steps_per_second": 0.252,
      "step": 1584
    },
    {
      "epoch": 5.593295103661227,
      "grad_norm": 0.18471071665369512,
      "learning_rate": 1.3987730061349694e-06,
      "loss": 0.0014,
      "step": 1585
    },
    {
      "epoch": 5.596823996471107,
      "grad_norm": 0.12468041604218401,
      "learning_rate": 1.3865030674846627e-06,
      "loss": 0.004,
      "step": 1586
    },
    {
      "epoch": 5.600352889280988,
      "grad_norm": 0.13238841959227188,
      "learning_rate": 1.374233128834356e-06,
      "loss": 0.0034,
      "step": 1587
    },
    {
      "epoch": 5.603881782090869,
      "grad_norm": 0.04027799803888953,
      "learning_rate": 1.3619631901840492e-06,
      "loss": 0.0015,
      "step": 1588
    },
    {
      "epoch": 5.60741067490075,
      "grad_norm": 0.04238597181296794,
      "learning_rate": 1.3496932515337425e-06,
      "loss": 0.0022,
      "step": 1589
    },
    {
      "epoch": 5.610939567710631,
      "grad_norm": 0.10893216916553973,
      "learning_rate": 1.3374233128834358e-06,
      "loss": 0.0017,
      "step": 1590
    },
    {
      "epoch": 5.614468460520512,
      "grad_norm": 0.11547452343094775,
      "learning_rate": 1.325153374233129e-06,
      "loss": 0.003,
      "step": 1591
    },
    {
      "epoch": 5.617997353330392,
      "grad_norm": 0.18490786692363811,
      "learning_rate": 1.312883435582822e-06,
      "loss": 0.0023,
      "step": 1592
    },
    {
      "epoch": 5.617997353330392,
      "eval_average_f1": 0.7939810549722116,
      "eval_crossner_ai_f1": 0.7301587301065257,
      "eval_crossner_ai_precision": 0.6969696969675849,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.6982248520201814,
      "eval_crossner_politics_precision": 0.7023809523801162,
      "eval_crossner_politics_recall": 0.6941176470580069,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9543147207612152,
      "eval_mit-movie_precision": 0.9591836734684089,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.0664,
      "eval_samples_per_second": 7.967,
      "eval_steps_per_second": 0.249,
      "step": 1592
    },
    {
      "epoch": 5.621526246140274,
      "grad_norm": 0.1150816009770853,
      "learning_rate": 1.3006134969325154e-06,
      "loss": 0.001,
      "step": 1593
    },
    {
      "epoch": 5.625055138950154,
      "grad_norm": 0.065734792441894,
      "learning_rate": 1.2883435582822088e-06,
      "loss": 0.0021,
      "step": 1594
    },
    {
      "epoch": 5.6285840317600355,
      "grad_norm": 0.07336850399988666,
      "learning_rate": 1.2760736196319021e-06,
      "loss": 0.0024,
      "step": 1595
    },
    {
      "epoch": 5.632112924569916,
      "grad_norm": 0.06559885986010792,
      "learning_rate": 1.263803680981595e-06,
      "loss": 0.0017,
      "step": 1596
    },
    {
      "epoch": 5.635641817379797,
      "grad_norm": 0.05290906942679911,
      "learning_rate": 1.2515337423312884e-06,
      "loss": 0.0013,
      "step": 1597
    },
    {
      "epoch": 5.639170710189678,
      "grad_norm": 0.10333341180153657,
      "learning_rate": 1.2392638036809817e-06,
      "loss": 0.0031,
      "step": 1598
    },
    {
      "epoch": 5.642699602999559,
      "grad_norm": 0.16249898259137843,
      "learning_rate": 1.226993865030675e-06,
      "loss": 0.005,
      "step": 1599
    },
    {
      "epoch": 5.6462284958094395,
      "grad_norm": 0.09181338527004654,
      "learning_rate": 1.2147239263803684e-06,
      "loss": 0.0012,
      "step": 1600
    },
    {
      "epoch": 5.6462284958094395,
      "eval_average_f1": 0.7881387370127388,
      "eval_crossner_ai_f1": 0.7301587301065257,
      "eval_crossner_ai_precision": 0.6969696969675849,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7218934910734078,
      "eval_crossner_politics_precision": 0.7261904761896116,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9543147207612152,
      "eval_mit-movie_precision": 0.9591836734684089,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8467153284159198,
      "eval_mit-restaurant_precision": 0.8529411764693339,
      "eval_mit-restaurant_recall": 0.8405797101437092,
      "eval_runtime": 15.9741,
      "eval_samples_per_second": 8.013,
      "eval_steps_per_second": 0.25,
      "step": 1600
    },
    {
      "epoch": 5.649757388619321,
      "grad_norm": 0.07913554309427559,
      "learning_rate": 1.2024539877300615e-06,
      "loss": 0.0024,
      "step": 1601
    },
    {
      "epoch": 5.653286281429201,
      "grad_norm": 0.04556278757654101,
      "learning_rate": 1.1901840490797548e-06,
      "loss": 0.001,
      "step": 1602
    },
    {
      "epoch": 5.656815174239083,
      "grad_norm": 0.08328280059201165,
      "learning_rate": 1.177914110429448e-06,
      "loss": 0.0029,
      "step": 1603
    },
    {
      "epoch": 5.660344067048963,
      "grad_norm": 0.08836189390803109,
      "learning_rate": 1.165644171779141e-06,
      "loss": 0.0016,
      "step": 1604
    },
    {
      "epoch": 5.663872959858844,
      "grad_norm": 0.027520399964891046,
      "learning_rate": 1.1533742331288344e-06,
      "loss": 0.0009,
      "step": 1605
    },
    {
      "epoch": 5.667401852668725,
      "grad_norm": 0.02280568705154023,
      "learning_rate": 1.1411042944785275e-06,
      "loss": 0.0008,
      "step": 1606
    },
    {
      "epoch": 5.670930745478606,
      "grad_norm": 0.12425812346159595,
      "learning_rate": 1.1288343558282209e-06,
      "loss": 0.0053,
      "step": 1607
    },
    {
      "epoch": 5.674459638288487,
      "grad_norm": 0.11659099636414941,
      "learning_rate": 1.1165644171779142e-06,
      "loss": 0.0043,
      "step": 1608
    },
    {
      "epoch": 5.674459638288487,
      "eval_average_f1": 0.7869512919269939,
      "eval_crossner_ai_f1": 0.7118644067272623,
      "eval_crossner_ai_precision": 0.7241379310319858,
      "eval_crossner_ai_recall": 0.6999999999976667,
      "eval_crossner_literature_f1": 0.6666666666142254,
      "eval_crossner_literature_precision": 0.708333333330382,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.746987951756358,
      "eval_crossner_politics_precision": 0.7654320987644871,
      "eval_crossner_politics_recall": 0.7294117647050242,
      "eval_crossner_science_f1": 0.6222222221694815,
      "eval_crossner_science_precision": 0.6086956521712665,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9387755101531289,
      "eval_mit-movie_precision": 0.9484536082464449,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8776978416753584,
      "eval_mit-restaurant_precision": 0.8714285714273265,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 15.9553,
      "eval_samples_per_second": 8.022,
      "eval_steps_per_second": 0.251,
      "step": 1608
    },
    {
      "epoch": 5.677988531098368,
      "grad_norm": 0.4644658451889072,
      "learning_rate": 1.1042944785276075e-06,
      "loss": 0.0039,
      "step": 1609
    },
    {
      "epoch": 5.681517423908248,
      "grad_norm": 0.0786536065922183,
      "learning_rate": 1.0920245398773007e-06,
      "loss": 0.0014,
      "step": 1610
    },
    {
      "epoch": 5.68504631671813,
      "grad_norm": 0.0911241187614247,
      "learning_rate": 1.079754601226994e-06,
      "loss": 0.002,
      "step": 1611
    },
    {
      "epoch": 5.68857520952801,
      "grad_norm": 0.06786788960310276,
      "learning_rate": 1.0674846625766871e-06,
      "loss": 0.0011,
      "step": 1612
    },
    {
      "epoch": 5.6921041023378915,
      "grad_norm": 0.04807037083343111,
      "learning_rate": 1.0552147239263805e-06,
      "loss": 0.0018,
      "step": 1613
    },
    {
      "epoch": 5.695632995147772,
      "grad_norm": 0.0690370248235059,
      "learning_rate": 1.0429447852760736e-06,
      "loss": 0.0034,
      "step": 1614
    },
    {
      "epoch": 5.699161887957653,
      "grad_norm": 0.15729112661871084,
      "learning_rate": 1.030674846625767e-06,
      "loss": 0.0032,
      "step": 1615
    },
    {
      "epoch": 5.702690780767535,
      "grad_norm": 0.061450671124804304,
      "learning_rate": 1.01840490797546e-06,
      "loss": 0.0031,
      "step": 1616
    },
    {
      "epoch": 5.702690780767535,
      "eval_average_f1": 0.8065691829470208,
      "eval_crossner_ai_f1": 0.7619047618524566,
      "eval_crossner_ai_precision": 0.7272727272705234,
      "eval_crossner_ai_recall": 0.7999999999973334,
      "eval_crossner_literature_f1": 0.6938775509680968,
      "eval_crossner_literature_precision": 0.7727272727237604,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7398843930127434,
      "eval_crossner_politics_precision": 0.7272727272719008,
      "eval_crossner_politics_recall": 0.7529411764697024,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.944162436497266,
      "eval_mit-movie_precision": 0.9489795918357663,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8613138685618839,
      "eval_mit-restaurant_precision": 0.8676470588222535,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 15.9804,
      "eval_samples_per_second": 8.01,
      "eval_steps_per_second": 0.25,
      "step": 1616
    },
    {
      "epoch": 5.706219673577415,
      "grad_norm": 0.10407018116934899,
      "learning_rate": 1.0061349693251534e-06,
      "loss": 0.0047,
      "step": 1617
    },
    {
      "epoch": 5.7097485663872956,
      "grad_norm": 0.0196243347827619,
      "learning_rate": 9.938650306748467e-07,
      "loss": 0.0004,
      "step": 1618
    },
    {
      "epoch": 5.713277459197177,
      "grad_norm": 0.06353026099450808,
      "learning_rate": 9.8159509202454e-07,
      "loss": 0.0019,
      "step": 1619
    },
    {
      "epoch": 5.716806352007058,
      "grad_norm": 0.041139761083335814,
      "learning_rate": 9.693251533742332e-07,
      "loss": 0.0015,
      "step": 1620
    },
    {
      "epoch": 5.720335244816939,
      "grad_norm": 0.055246259609243974,
      "learning_rate": 9.570552147239265e-07,
      "loss": 0.0021,
      "step": 1621
    },
    {
      "epoch": 5.723864137626819,
      "grad_norm": 0.13844035526715293,
      "learning_rate": 9.447852760736197e-07,
      "loss": 0.0025,
      "step": 1622
    },
    {
      "epoch": 5.7273930304367004,
      "grad_norm": 0.06432316060283805,
      "learning_rate": 9.32515337423313e-07,
      "loss": 0.0028,
      "step": 1623
    },
    {
      "epoch": 5.730921923246582,
      "grad_norm": 0.08764857765184508,
      "learning_rate": 9.202453987730062e-07,
      "loss": 0.0035,
      "step": 1624
    },
    {
      "epoch": 5.730921923246582,
      "eval_average_f1": 0.8024760702664743,
      "eval_crossner_ai_f1": 0.7118644067272623,
      "eval_crossner_ai_precision": 0.7241379310319858,
      "eval_crossner_ai_recall": 0.6999999999976667,
      "eval_crossner_literature_f1": 0.6938775509680968,
      "eval_crossner_literature_precision": 0.7727272727237604,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7674418604142307,
      "eval_crossner_politics_precision": 0.7586206896543004,
      "eval_crossner_politics_recall": 0.7764705882343806,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.5793,
      "eval_samples_per_second": 7.72,
      "eval_steps_per_second": 0.241,
      "step": 1624
    },
    {
      "epoch": 5.734450816056462,
      "grad_norm": 0.06414908526981702,
      "learning_rate": 9.079754601226994e-07,
      "loss": 0.0013,
      "step": 1625
    },
    {
      "epoch": 5.737979708866344,
      "grad_norm": 0.05142538360285493,
      "learning_rate": 8.957055214723927e-07,
      "loss": 0.0008,
      "step": 1626
    },
    {
      "epoch": 5.741508601676224,
      "grad_norm": 0.07646615695921892,
      "learning_rate": 8.83435582822086e-07,
      "loss": 0.0013,
      "step": 1627
    },
    {
      "epoch": 5.745037494486105,
      "grad_norm": 0.06520529244356126,
      "learning_rate": 8.711656441717791e-07,
      "loss": 0.0029,
      "step": 1628
    },
    {
      "epoch": 5.748566387295986,
      "grad_norm": 0.0406737054523024,
      "learning_rate": 8.588957055214725e-07,
      "loss": 0.0006,
      "step": 1629
    },
    {
      "epoch": 5.752095280105867,
      "grad_norm": 0.018174842390314732,
      "learning_rate": 8.466257668711657e-07,
      "loss": 0.0004,
      "step": 1630
    },
    {
      "epoch": 5.755624172915748,
      "grad_norm": 0.1170197955817227,
      "learning_rate": 8.34355828220859e-07,
      "loss": 0.0013,
      "step": 1631
    },
    {
      "epoch": 5.759153065725629,
      "grad_norm": 0.10006518884116243,
      "learning_rate": 8.220858895705522e-07,
      "loss": 0.0014,
      "step": 1632
    },
    {
      "epoch": 5.759153065725629,
      "eval_average_f1": 0.794995478181498,
      "eval_crossner_ai_f1": 0.6999999999476667,
      "eval_crossner_ai_precision": 0.6999999999976667,
      "eval_crossner_ai_recall": 0.6999999999976667,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7017543859140932,
      "eval_crossner_politics_precision": 0.6976744186038399,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9543147207612152,
      "eval_mit-movie_precision": 0.9591836734684089,
      "eval_mit-movie_recall": 0.9494949494939904,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 15.9964,
      "eval_samples_per_second": 8.002,
      "eval_steps_per_second": 0.25,
      "step": 1632
    },
    {
      "epoch": 5.762681958535509,
      "grad_norm": 0.0593213954619696,
      "learning_rate": 8.098159509202455e-07,
      "loss": 0.0017,
      "step": 1633
    },
    {
      "epoch": 5.766210851345391,
      "grad_norm": 0.5165690824685821,
      "learning_rate": 7.975460122699387e-07,
      "loss": 0.0039,
      "step": 1634
    },
    {
      "epoch": 5.769739744155271,
      "grad_norm": 0.046861587035911076,
      "learning_rate": 7.85276073619632e-07,
      "loss": 0.0023,
      "step": 1635
    },
    {
      "epoch": 5.7732686369651525,
      "grad_norm": 0.05572603774348436,
      "learning_rate": 7.730061349693252e-07,
      "loss": 0.0016,
      "step": 1636
    },
    {
      "epoch": 5.776797529775033,
      "grad_norm": 0.0322082243386664,
      "learning_rate": 7.607361963190185e-07,
      "loss": 0.0009,
      "step": 1637
    },
    {
      "epoch": 5.780326422584914,
      "grad_norm": 0.08326713501890161,
      "learning_rate": 7.484662576687117e-07,
      "loss": 0.0038,
      "step": 1638
    },
    {
      "epoch": 5.783855315394795,
      "grad_norm": 0.10623072508977228,
      "learning_rate": 7.36196319018405e-07,
      "loss": 0.0027,
      "step": 1639
    },
    {
      "epoch": 5.787384208204676,
      "grad_norm": 0.06979932874935599,
      "learning_rate": 7.239263803680982e-07,
      "loss": 0.0016,
      "step": 1640
    },
    {
      "epoch": 5.787384208204676,
      "eval_average_f1": 0.7990126070614796,
      "eval_crossner_ai_f1": 0.6999999999476667,
      "eval_crossner_ai_precision": 0.6999999999976667,
      "eval_crossner_ai_recall": 0.6999999999976667,
      "eval_crossner_literature_f1": 0.6938775509680968,
      "eval_crossner_literature_precision": 0.7727272727237604,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.717647058772685,
      "eval_crossner_politics_precision": 0.7176470588226851,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9387755101531289,
      "eval_mit-movie_precision": 0.9484536082464449,
      "eval_mit-movie_recall": 0.9292929292919906,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.5872,
      "eval_samples_per_second": 7.717,
      "eval_steps_per_second": 0.241,
      "step": 1640
    },
    {
      "epoch": 5.7909131010145565,
      "grad_norm": 0.03818476673919691,
      "learning_rate": 7.116564417177916e-07,
      "loss": 0.0015,
      "step": 1641
    },
    {
      "epoch": 5.794441993824438,
      "grad_norm": 0.09846331885991198,
      "learning_rate": 6.993865030674847e-07,
      "loss": 0.0015,
      "step": 1642
    },
    {
      "epoch": 5.797970886634318,
      "grad_norm": 0.1018485707903383,
      "learning_rate": 6.87116564417178e-07,
      "loss": 0.0059,
      "step": 1643
    },
    {
      "epoch": 5.8014997794442,
      "grad_norm": 0.11104599858661827,
      "learning_rate": 6.748466257668713e-07,
      "loss": 0.0009,
      "step": 1644
    },
    {
      "epoch": 5.80502867225408,
      "grad_norm": 0.4844352007956407,
      "learning_rate": 6.625766871165645e-07,
      "loss": 0.0043,
      "step": 1645
    },
    {
      "epoch": 5.808557565063961,
      "grad_norm": 0.0751999321051111,
      "learning_rate": 6.503067484662577e-07,
      "loss": 0.0044,
      "step": 1646
    },
    {
      "epoch": 5.812086457873842,
      "grad_norm": 0.09595075994054465,
      "learning_rate": 6.380368098159511e-07,
      "loss": 0.0024,
      "step": 1647
    },
    {
      "epoch": 5.815615350683723,
      "grad_norm": 0.03266052818679681,
      "learning_rate": 6.257668711656442e-07,
      "loss": 0.0011,
      "step": 1648
    },
    {
      "epoch": 5.815615350683723,
      "eval_average_f1": 0.7995533871946285,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6938775509680968,
      "eval_crossner_literature_precision": 0.7727272727237604,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7558139534375,
      "eval_crossner_politics_precision": 0.7471264367807504,
      "eval_crossner_politics_recall": 0.7647058823520415,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8676470587722643,
      "eval_mit-restaurant_precision": 0.8805970149240588,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 15.9532,
      "eval_samples_per_second": 8.023,
      "eval_steps_per_second": 0.251,
      "step": 1648
    },
    {
      "epoch": 5.819144243493604,
      "grad_norm": 0.043069977623045735,
      "learning_rate": 6.134969325153375e-07,
      "loss": 0.0015,
      "step": 1649
    },
    {
      "epoch": 5.822673136303485,
      "grad_norm": 0.0870294474726696,
      "learning_rate": 6.012269938650307e-07,
      "loss": 0.0017,
      "step": 1650
    },
    {
      "epoch": 5.826202029113365,
      "grad_norm": 0.03730676099746718,
      "learning_rate": 5.88957055214724e-07,
      "loss": 0.0009,
      "step": 1651
    },
    {
      "epoch": 5.829730921923247,
      "grad_norm": 0.04518547154559008,
      "learning_rate": 5.766871165644172e-07,
      "loss": 0.0014,
      "step": 1652
    },
    {
      "epoch": 5.833259814733127,
      "grad_norm": 0.041238608663602856,
      "learning_rate": 5.644171779141104e-07,
      "loss": 0.0013,
      "step": 1653
    },
    {
      "epoch": 5.8367887075430085,
      "grad_norm": 0.20567180441461255,
      "learning_rate": 5.521472392638038e-07,
      "loss": 0.0062,
      "step": 1654
    },
    {
      "epoch": 5.840317600352889,
      "grad_norm": 0.07543377098733126,
      "learning_rate": 5.39877300613497e-07,
      "loss": 0.0042,
      "step": 1655
    },
    {
      "epoch": 5.84384649316277,
      "grad_norm": 0.11998440218300604,
      "learning_rate": 5.276073619631902e-07,
      "loss": 0.0032,
      "step": 1656
    },
    {
      "epoch": 5.84384649316277,
      "eval_average_f1": 0.7863015831873419,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7261904761396187,
      "eval_crossner_politics_precision": 0.7349397590352591,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.6222222221694815,
      "eval_crossner_science_precision": 0.6086956521712665,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.944162436497266,
      "eval_mit-movie_precision": 0.9489795918357663,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8676470587722643,
      "eval_mit-restaurant_precision": 0.8805970149240588,
      "eval_mit-restaurant_recall": 0.8550724637668767,
      "eval_runtime": 15.971,
      "eval_samples_per_second": 8.015,
      "eval_steps_per_second": 0.25,
      "step": 1656
    },
    {
      "epoch": 5.847375385972651,
      "grad_norm": 0.24856139308739567,
      "learning_rate": 5.153374233128835e-07,
      "loss": 0.0059,
      "step": 1657
    },
    {
      "epoch": 5.850904278782532,
      "grad_norm": 0.09362579104531166,
      "learning_rate": 5.030674846625767e-07,
      "loss": 0.0022,
      "step": 1658
    },
    {
      "epoch": 5.8544331715924125,
      "grad_norm": 0.07583183721306196,
      "learning_rate": 4.9079754601227e-07,
      "loss": 0.0013,
      "step": 1659
    },
    {
      "epoch": 5.857962064402294,
      "grad_norm": 0.10308838342335738,
      "learning_rate": 4.785276073619633e-07,
      "loss": 0.0011,
      "step": 1660
    },
    {
      "epoch": 5.861490957212174,
      "grad_norm": 0.05500228577708265,
      "learning_rate": 4.662576687116565e-07,
      "loss": 0.0019,
      "step": 1661
    },
    {
      "epoch": 5.865019850022056,
      "grad_norm": 0.04651134714215375,
      "learning_rate": 4.539877300613497e-07,
      "loss": 0.0012,
      "step": 1662
    },
    {
      "epoch": 5.868548742831937,
      "grad_norm": 0.15904705981614758,
      "learning_rate": 4.41717791411043e-07,
      "loss": 0.002,
      "step": 1663
    },
    {
      "epoch": 5.872077635641817,
      "grad_norm": 0.5445582773077078,
      "learning_rate": 4.2944785276073624e-07,
      "loss": 0.0022,
      "step": 1664
    },
    {
      "epoch": 5.872077635641817,
      "eval_average_f1": 0.8167659578742731,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6799999999476001,
      "eval_crossner_literature_precision": 0.7391304347793951,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.7630057802959538,
      "eval_crossner_politics_precision": 0.7499999999991477,
      "eval_crossner_politics_recall": 0.7764705882343806,
      "eval_crossner_science_f1": 0.7619047618512471,
      "eval_crossner_science_precision": 0.799999999996,
      "eval_crossner_science_recall": 0.7272727272694215,
      "eval_mit-movie_f1": 0.944162436497266,
      "eval_mit-movie_precision": 0.9489795918357663,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8905109488538122,
      "eval_mit-restaurant_precision": 0.8970588235280925,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.5345,
      "eval_samples_per_second": 7.741,
      "eval_steps_per_second": 0.242,
      "step": 1664
    },
    {
      "epoch": 5.875606528451698,
      "grad_norm": 0.03042950222922448,
      "learning_rate": 4.171779141104295e-07,
      "loss": 0.0008,
      "step": 1665
    },
    {
      "epoch": 5.879135421261579,
      "grad_norm": 0.10346272754452902,
      "learning_rate": 4.0490797546012275e-07,
      "loss": 0.0068,
      "step": 1666
    },
    {
      "epoch": 5.8826643140714605,
      "grad_norm": 0.08050054979673403,
      "learning_rate": 3.92638036809816e-07,
      "loss": 0.0011,
      "step": 1667
    },
    {
      "epoch": 5.886193206881341,
      "grad_norm": 0.17975321894762428,
      "learning_rate": 3.8036809815950927e-07,
      "loss": 0.0016,
      "step": 1668
    },
    {
      "epoch": 5.889722099691221,
      "grad_norm": 0.12282365654849857,
      "learning_rate": 3.680981595092025e-07,
      "loss": 0.0041,
      "step": 1669
    },
    {
      "epoch": 5.893250992501103,
      "grad_norm": 0.0588302132437244,
      "learning_rate": 3.558282208588958e-07,
      "loss": 0.0009,
      "step": 1670
    },
    {
      "epoch": 5.896779885310984,
      "grad_norm": 0.04897842639565549,
      "learning_rate": 3.43558282208589e-07,
      "loss": 0.0024,
      "step": 1671
    },
    {
      "epoch": 5.9003087781208645,
      "grad_norm": 0.14030134519913,
      "learning_rate": 3.3128834355828224e-07,
      "loss": 0.0045,
      "step": 1672
    },
    {
      "epoch": 5.9003087781208645,
      "eval_average_f1": 0.801960407684713,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.6938775509680968,
      "eval_crossner_literature_precision": 0.7727272727237604,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.7134502923468281,
      "eval_crossner_politics_precision": 0.7093023255805706,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.944162436497266,
      "eval_mit-movie_precision": 0.9489795918357663,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 15.9586,
      "eval_samples_per_second": 8.021,
      "eval_steps_per_second": 0.251,
      "step": 1672
    },
    {
      "epoch": 5.903837670930746,
      "grad_norm": 0.011050591998091119,
      "learning_rate": 3.1901840490797553e-07,
      "loss": 0.0003,
      "step": 1673
    },
    {
      "epoch": 5.907366563740626,
      "grad_norm": 0.022422782739597046,
      "learning_rate": 3.0674846625766876e-07,
      "loss": 0.0011,
      "step": 1674
    },
    {
      "epoch": 5.910895456550508,
      "grad_norm": 0.058478605859637864,
      "learning_rate": 2.94478527607362e-07,
      "loss": 0.0014,
      "step": 1675
    },
    {
      "epoch": 5.914424349360388,
      "grad_norm": 0.11330692641531727,
      "learning_rate": 2.822085889570552e-07,
      "loss": 0.0026,
      "step": 1676
    },
    {
      "epoch": 5.917953242170269,
      "grad_norm": 0.07675233086093274,
      "learning_rate": 2.699386503067485e-07,
      "loss": 0.0035,
      "step": 1677
    },
    {
      "epoch": 5.92148213498015,
      "grad_norm": 0.013050006865138715,
      "learning_rate": 2.5766871165644173e-07,
      "loss": 0.0005,
      "step": 1678
    },
    {
      "epoch": 5.925011027790031,
      "grad_norm": 0.13355550079008816,
      "learning_rate": 2.45398773006135e-07,
      "loss": 0.0011,
      "step": 1679
    },
    {
      "epoch": 5.928539920599912,
      "grad_norm": 0.06504928262274323,
      "learning_rate": 2.3312883435582825e-07,
      "loss": 0.0014,
      "step": 1680
    },
    {
      "epoch": 5.928539920599912,
      "eval_average_f1": 0.7959935960777044,
      "eval_crossner_ai_f1": 0.7419354838186265,
      "eval_crossner_ai_precision": 0.7187499999977539,
      "eval_crossner_ai_recall": 0.7666666666641112,
      "eval_crossner_literature_f1": 0.6938775509680968,
      "eval_crossner_literature_precision": 0.7727272727237604,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.705882352890346,
      "eval_crossner_politics_precision": 0.705882352940346,
      "eval_crossner_politics_recall": 0.705882352940346,
      "eval_crossner_science_f1": 0.6666666666136055,
      "eval_crossner_science_precision": 0.6999999999965001,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 15.9279,
      "eval_samples_per_second": 8.036,
      "eval_steps_per_second": 0.251,
      "step": 1680
    },
    {
      "epoch": 5.932068813409793,
      "grad_norm": 0.07526224427275587,
      "learning_rate": 2.208588957055215e-07,
      "loss": 0.0015,
      "step": 1681
    },
    {
      "epoch": 5.935597706219673,
      "grad_norm": 0.06830741406142661,
      "learning_rate": 2.0858895705521476e-07,
      "loss": 0.0014,
      "step": 1682
    },
    {
      "epoch": 5.939126599029555,
      "grad_norm": 0.05891373403713343,
      "learning_rate": 1.96319018404908e-07,
      "loss": 0.0017,
      "step": 1683
    },
    {
      "epoch": 5.942655491839435,
      "grad_norm": 0.0949851370779479,
      "learning_rate": 1.8404907975460125e-07,
      "loss": 0.0022,
      "step": 1684
    },
    {
      "epoch": 5.9461843846493165,
      "grad_norm": 0.12966233376981723,
      "learning_rate": 1.717791411042945e-07,
      "loss": 0.0026,
      "step": 1685
    },
    {
      "epoch": 5.949713277459197,
      "grad_norm": 0.11337108995830415,
      "learning_rate": 1.5950920245398776e-07,
      "loss": 0.0101,
      "step": 1686
    },
    {
      "epoch": 5.953242170269078,
      "grad_norm": 0.14657389130886303,
      "learning_rate": 1.47239263803681e-07,
      "loss": 0.0037,
      "step": 1687
    },
    {
      "epoch": 5.956771063078959,
      "grad_norm": 0.037568444818866085,
      "learning_rate": 1.3496932515337425e-07,
      "loss": 0.0017,
      "step": 1688
    },
    {
      "epoch": 5.956771063078959,
      "eval_average_f1": 0.7965148025213341,
      "eval_crossner_ai_f1": 0.733333333280889,
      "eval_crossner_ai_precision": 0.7333333333308889,
      "eval_crossner_ai_recall": 0.7333333333308889,
      "eval_crossner_literature_f1": 0.71999999994744,
      "eval_crossner_literature_precision": 0.7826086956487713,
      "eval_crossner_literature_recall": 0.6666666666641976,
      "eval_crossner_music_f1": 0.9444444443931423,
      "eval_crossner_music_precision": 0.9315068493137925,
      "eval_crossner_music_recall": 0.9577464788718905,
      "eval_crossner_politics_f1": 0.720930232507308,
      "eval_crossner_politics_precision": 0.7126436781601004,
      "eval_crossner_politics_recall": 0.7294117647050242,
      "eval_crossner_science_f1": 0.6222222221694815,
      "eval_crossner_science_precision": 0.6086956521712665,
      "eval_crossner_science_recall": 0.6363636363607439,
      "eval_mit-movie_f1": 0.944162436497266,
      "eval_mit-movie_precision": 0.9489795918357663,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8905109488538122,
      "eval_mit-restaurant_precision": 0.8970588235280925,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 16.0798,
      "eval_samples_per_second": 7.96,
      "eval_steps_per_second": 0.249,
      "step": 1688
    },
    {
      "epoch": 5.96029995588884,
      "grad_norm": 0.058114858477708024,
      "learning_rate": 1.226993865030675e-07,
      "loss": 0.0013,
      "step": 1689
    },
    {
      "epoch": 5.9638288486987205,
      "grad_norm": 0.11707157520635308,
      "learning_rate": 1.1042944785276075e-07,
      "loss": 0.003,
      "step": 1690
    },
    {
      "epoch": 5.967357741508602,
      "grad_norm": 0.054444531141595136,
      "learning_rate": 9.8159509202454e-08,
      "loss": 0.0014,
      "step": 1691
    },
    {
      "epoch": 5.970886634318482,
      "grad_norm": 0.040283398194274234,
      "learning_rate": 8.588957055214725e-08,
      "loss": 0.0008,
      "step": 1692
    },
    {
      "epoch": 5.974415527128364,
      "grad_norm": 0.20873016868549038,
      "learning_rate": 7.36196319018405e-08,
      "loss": 0.0017,
      "step": 1693
    },
    {
      "epoch": 5.977944419938244,
      "grad_norm": 0.04662597840413306,
      "learning_rate": 6.134969325153375e-08,
      "loss": 0.001,
      "step": 1694
    },
    {
      "epoch": 5.981473312748125,
      "grad_norm": 0.02519162764698186,
      "learning_rate": 4.9079754601227e-08,
      "loss": 0.0008,
      "step": 1695
    },
    {
      "epoch": 5.985002205558006,
      "grad_norm": 0.1897103932280223,
      "learning_rate": 3.680981595092025e-08,
      "loss": 0.0015,
      "step": 1696
    },
    {
      "epoch": 5.985002205558006,
      "eval_average_f1": 0.8001811212791037,
      "eval_crossner_ai_f1": 0.7118644067272623,
      "eval_crossner_ai_precision": 0.7241379310319858,
      "eval_crossner_ai_recall": 0.6999999999976667,
      "eval_crossner_literature_f1": 0.6938775509680968,
      "eval_crossner_literature_precision": 0.7727272727237604,
      "eval_crossner_literature_recall": 0.6296296296272977,
      "eval_crossner_music_f1": 0.9305555555042727,
      "eval_crossner_music_precision": 0.9178082191768249,
      "eval_crossner_music_recall": 0.9436619718296568,
      "eval_crossner_politics_f1": 0.717647058772685,
      "eval_crossner_politics_precision": 0.7176470588226851,
      "eval_crossner_politics_recall": 0.7176470588226851,
      "eval_crossner_science_f1": 0.7142857142324263,
      "eval_crossner_science_precision": 0.7499999999962501,
      "eval_crossner_science_recall": 0.6818181818150827,
      "eval_mit-movie_f1": 0.9489795917857716,
      "eval_mit-movie_precision": 0.9587628865969497,
      "eval_mit-movie_recall": 0.9393939393929905,
      "eval_mit-restaurant_f1": 0.8840579709632115,
      "eval_mit-restaurant_precision": 0.8840579710132115,
      "eval_mit-restaurant_recall": 0.8840579710132115,
      "eval_runtime": 15.962,
      "eval_samples_per_second": 8.019,
      "eval_steps_per_second": 0.251,
      "step": 1696
    },
    {
      "epoch": 5.988531098367887,
      "grad_norm": 0.08747738009376764,
      "learning_rate": 2.45398773006135e-08,
      "loss": 0.003,
      "step": 1697
    },
    {
      "epoch": 5.992059991177768,
      "grad_norm": 0.0498003069503236,
      "learning_rate": 1.226993865030675e-08,
      "loss": 0.002,
      "step": 1698
    },
    {
      "epoch": 5.992059991177768,
      "step": 1698,
      "total_flos": 1.3833208539486618e+17,
      "train_loss": 0.021385273862323254,
      "train_runtime": 5122.0246,
      "train_samples_per_second": 21.244,
      "train_steps_per_second": 0.332
    }
  ],
  "logging_steps": 1.0,
  "max_steps": 1698,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": false,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.3833208539486618e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
