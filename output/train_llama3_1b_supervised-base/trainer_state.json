{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 6.0,
  "eval_steps": 10,
  "global_step": 426,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.14084507042253522,
      "grad_norm": 4.853010026167977,
      "learning_rate": 1.593279540393825e-05,
      "loss": 0.6269,
      "step": 10
    },
    {
      "epoch": 0.14084507042253522,
      "eval_average_f1": 0.2281744320199361,
      "eval_crossner_ai_f1": 0.12885985743228215,
      "eval_crossner_ai_precision": 0.12322544009085047,
      "eval_crossner_ai_recall": 0.13503422526445955,
      "eval_crossner_literature_f1": 0.12863949174471467,
      "eval_crossner_literature_precision": 0.13530066815144012,
      "eval_crossner_literature_recall": 0.12260343087789492,
      "eval_crossner_music_f1": 0.16550968208471664,
      "eval_crossner_music_precision": 0.1927659574468003,
      "eval_crossner_music_recall": 0.14500640204865092,
      "eval_crossner_politics_f1": 0.20147239258812405,
      "eval_crossner_politics_precision": 0.19317647058823076,
      "eval_crossner_politics_recall": 0.2105128205128151,
      "eval_crossner_science_f1": 0.14590199951115515,
      "eval_crossner_science_precision": 0.1642751113310161,
      "eval_crossner_science_recall": 0.13122529644268255,
      "eval_mit-movie_f1": 0.516157989178084,
      "eval_mit-movie_precision": 0.4956042061713412,
      "eval_mit-movie_recall": 0.5384903539988661,
      "eval_mit-restaurant_f1": 0.3106796116004759,
      "eval_mit-restaurant_precision": 0.31152250239386176,
      "eval_mit-restaurant_recall": 0.30984126984126,
      "eval_runtime": 380.8687,
      "eval_samples_per_second": 16.987,
      "eval_steps_per_second": 0.068,
      "step": 10
    },
    {
      "epoch": 0.28169014084507044,
      "grad_norm": 0.44900656955092894,
      "learning_rate": 1.9950980392156866e-05,
      "loss": 0.1057,
      "step": 20
    },
    {
      "epoch": 0.28169014084507044,
      "eval_average_f1": 0.48181799773744477,
      "eval_crossner_ai_f1": 0.31351733894596634,
      "eval_crossner_ai_precision": 0.3634126333059587,
      "eval_crossner_ai_recall": 0.27566894835094735,
      "eval_crossner_literature_f1": 0.39588235289252904,
      "eval_crossner_literature_precision": 0.4746121297601922,
      "eval_crossner_literature_recall": 0.3395560040363098,
      "eval_crossner_music_f1": 0.44361036634923534,
      "eval_crossner_music_precision": 0.5022258195062524,
      "eval_crossner_music_recall": 0.3972471190780923,
      "eval_crossner_politics_f1": 0.4075983717276354,
      "eval_crossner_politics_precision": 0.4328530259365869,
      "eval_crossner_politics_recall": 0.38512820512819523,
      "eval_crossner_science_f1": 0.4063674551795173,
      "eval_crossner_science_precision": 0.4611138986452353,
      "eval_crossner_science_recall": 0.3632411067193532,
      "eval_mit-movie_f1": 0.754188357735131,
      "eval_mit-movie_precision": 0.7623421354764492,
      "eval_mit-movie_recall": 0.746207154897907,
      "eval_mit-restaurant_f1": 0.6515617413320988,
      "eval_mit-restaurant_precision": 0.6645757675800374,
      "eval_mit-restaurant_recall": 0.6390476190475988,
      "eval_runtime": 467.9611,
      "eval_samples_per_second": 13.826,
      "eval_steps_per_second": 0.056,
      "step": 20
    },
    {
      "epoch": 0.4225352112676056,
      "grad_norm": 0.21148076564835833,
      "learning_rate": 1.946078431372549e-05,
      "loss": 0.0577,
      "step": 30
    },
    {
      "epoch": 0.4225352112676056,
      "eval_average_f1": 0.5992137533115126,
      "eval_crossner_ai_f1": 0.3969521044506263,
      "eval_crossner_ai_precision": 0.4760661444734137,
      "eval_crossner_ai_recall": 0.340385812072163,
      "eval_crossner_literature_f1": 0.5761073825005104,
      "eval_crossner_literature_precision": 0.6156052782558453,
      "eval_crossner_literature_recall": 0.5413723511604167,
      "eval_crossner_music_f1": 0.6201680671769952,
      "eval_crossner_music_precision": 0.6528662420381934,
      "eval_crossner_music_recall": 0.5905889884762935,
      "eval_crossner_politics_f1": 0.5800028444961655,
      "eval_crossner_politics_precision": 0.6512296390929208,
      "eval_crossner_politics_recall": 0.5228205128204995,
      "eval_crossner_science_f1": 0.46709401704432674,
      "eval_crossner_science_precision": 0.5083720930232322,
      "eval_crossner_science_recall": 0.43201581027666275,
      "eval_mit-movie_f1": 0.8302277432212071,
      "eval_mit-movie_precision": 0.8343105731038238,
      "eval_mit-movie_recall": 0.8261846787787821,
      "eval_mit-restaurant_f1": 0.7239441142907577,
      "eval_mit-restaurant_precision": 0.7325316867078084,
      "eval_mit-restaurant_recall": 0.7155555555555329,
      "eval_runtime": 186.8879,
      "eval_samples_per_second": 34.62,
      "eval_steps_per_second": 0.139,
      "step": 30
    },
    {
      "epoch": 0.5633802816901409,
      "grad_norm": 0.30319437334286325,
      "learning_rate": 1.897058823529412e-05,
      "loss": 0.0497,
      "step": 40
    },
    {
      "epoch": 0.5633802816901409,
      "eval_average_f1": 0.6457876309471003,
      "eval_crossner_ai_f1": 0.440984793579113,
      "eval_crossner_ai_precision": 0.5272727272726816,
      "eval_crossner_ai_recall": 0.37896701929058,
      "eval_crossner_literature_f1": 0.5956797966462877,
      "eval_crossner_literature_precision": 0.6001024065539887,
      "eval_crossner_literature_recall": 0.5913218970736331,
      "eval_crossner_music_f1": 0.6555140489998514,
      "eval_crossner_music_precision": 0.6653478404220025,
      "eval_crossner_music_recall": 0.6459667093469703,
      "eval_crossner_politics_f1": 0.607880434732771,
      "eval_crossner_politics_precision": 0.6465317919074958,
      "eval_crossner_politics_recall": 0.5735897435897289,
      "eval_crossner_science_f1": 0.6140644620681894,
      "eval_crossner_science_precision": 0.6525800711743481,
      "eval_crossner_science_recall": 0.5798418972331787,
      "eval_mit-movie_f1": 0.8399514154409684,
      "eval_mit-movie_precision": 0.8379940343027436,
      "eval_mit-movie_recall": 0.8419179621651837,
      "eval_mit-restaurant_f1": 0.7664384651625219,
      "eval_mit-restaurant_precision": 0.7687639731714861,
      "eval_mit-restaurant_recall": 0.7641269841269599,
      "eval_runtime": 227.7938,
      "eval_samples_per_second": 28.403,
      "eval_steps_per_second": 0.114,
      "step": 40
    },
    {
      "epoch": 0.704225352112676,
      "grad_norm": 0.21388565053456227,
      "learning_rate": 1.8480392156862748e-05,
      "loss": 0.0427,
      "step": 50
    },
    {
      "epoch": 0.704225352112676,
      "eval_average_f1": 0.6735211185225658,
      "eval_crossner_ai_f1": 0.5425210083536475,
      "eval_crossner_ai_precision": 0.5899122807017112,
      "eval_crossner_ai_recall": 0.502177971375202,
      "eval_crossner_literature_f1": 0.5703344395721379,
      "eval_crossner_literature_precision": 0.5772609819121148,
      "eval_crossner_literature_recall": 0.5635721493440684,
      "eval_crossner_music_f1": 0.7165706973268166,
      "eval_crossner_music_precision": 0.7161125319692866,
      "eval_crossner_music_recall": 0.7170294494237927,
      "eval_crossner_politics_f1": 0.6605892455571106,
      "eval_crossner_politics_precision": 0.6813845734532384,
      "eval_crossner_politics_recall": 0.6410256410256245,
      "eval_crossner_science_f1": 0.6160749121622354,
      "eval_crossner_science_precision": 0.608243451463767,
      "eval_crossner_science_recall": 0.6241106719367342,
      "eval_mit-movie_f1": 0.8589887639949277,
      "eval_mit-movie_precision": 0.8588279348436462,
      "eval_mit-movie_recall": 0.8591496534931474,
      "eval_mit-restaurant_f1": 0.7495687626910846,
      "eval_mit-restaurant_precision": 0.7406259683916722,
      "eval_mit-restaurant_recall": 0.7587301587301346,
      "eval_runtime": 223.7615,
      "eval_samples_per_second": 28.915,
      "eval_steps_per_second": 0.116,
      "step": 50
    },
    {
      "epoch": 0.8450704225352113,
      "grad_norm": 0.18177906011384082,
      "learning_rate": 1.7990196078431373e-05,
      "loss": 0.041,
      "step": 60
    },
    {
      "epoch": 0.8450704225352113,
      "eval_average_f1": 0.6518429871335434,
      "eval_crossner_ai_f1": 0.5317745802857663,
      "eval_crossner_ai_precision": 0.513013302486957,
      "eval_crossner_ai_recall": 0.5519601742376756,
      "eval_crossner_literature_f1": 0.5551311433164136,
      "eval_crossner_literature_precision": 0.5604113110539557,
      "eval_crossner_literature_recall": 0.5499495459131912,
      "eval_crossner_music_f1": 0.6601016517924215,
      "eval_crossner_music_precision": 0.6551071878940524,
      "eval_crossner_music_recall": 0.6651728553136791,
      "eval_crossner_politics_f1": 0.6271991780673591,
      "eval_crossner_politics_precision": 0.6282480061744115,
      "eval_crossner_politics_recall": 0.6261538461538301,
      "eval_crossner_science_f1": 0.6248131539112615,
      "eval_crossner_science_precision": 0.592487597448597,
      "eval_crossner_science_recall": 0.6608695652173652,
      "eval_mit-movie_f1": 0.8335351652969818,
      "eval_mit-movie_precision": 0.8291326908821195,
      "eval_mit-movie_recall": 0.8379846413185833,
      "eval_mit-restaurant_f1": 0.7303460372646006,
      "eval_mit-restaurant_precision": 0.7337391861582591,
      "eval_mit-restaurant_recall": 0.7269841269841039,
      "eval_runtime": 156.8233,
      "eval_samples_per_second": 41.257,
      "eval_steps_per_second": 0.166,
      "step": 60
    },
    {
      "epoch": 0.9859154929577465,
      "grad_norm": 0.14496452238707136,
      "learning_rate": 1.7500000000000002e-05,
      "loss": 0.0389,
      "step": 70
    },
    {
      "epoch": 0.9859154929577465,
      "eval_average_f1": 0.714662724852153,
      "eval_crossner_ai_f1": 0.5977235771858356,
      "eval_crossner_ai_precision": 0.62602179836508,
      "eval_crossner_ai_recall": 0.5718730553826651,
      "eval_crossner_literature_f1": 0.6647697878444623,
      "eval_crossner_literature_precision": 0.6820594479829787,
      "eval_crossner_literature_recall": 0.6483350151361933,
      "eval_crossner_music_f1": 0.6745123326922023,
      "eval_crossner_music_precision": 0.6794413770704554,
      "eval_crossner_music_recall": 0.6696542893725778,
      "eval_crossner_politics_f1": 0.7348680806535545,
      "eval_crossner_politics_precision": 0.7413883089770161,
      "eval_crossner_politics_recall": 0.7284615384615197,
      "eval_crossner_science_f1": 0.676376695879746,
      "eval_crossner_science_precision": 0.6829170024173777,
      "eval_crossner_science_recall": 0.6699604743082739,
      "eval_mit-movie_f1": 0.8683914510186002,
      "eval_mit-movie_precision": 0.869206230061909,
      "eval_mit-movie_recall": 0.867578198164434,
      "eval_mit-restaurant_f1": 0.7859971486906692,
      "eval_mit-restaurant_precision": 0.7843819159025993,
      "eval_mit-restaurant_recall": 0.7876190476190226,
      "eval_runtime": 137.9148,
      "eval_samples_per_second": 46.913,
      "eval_steps_per_second": 0.189,
      "step": 70
    },
    {
      "epoch": 1.1267605633802817,
      "grad_norm": 0.15061128207615093,
      "learning_rate": 1.7009803921568627e-05,
      "loss": 0.0324,
      "step": 80
    },
    {
      "epoch": 1.1267605633802817,
      "eval_average_f1": 0.7329711932187563,
      "eval_crossner_ai_f1": 0.621499548278831,
      "eval_crossner_ai_precision": 0.6021003500583079,
      "eval_crossner_ai_recall": 0.642190416925909,
      "eval_crossner_literature_f1": 0.687658709955046,
      "eval_crossner_literature_precision": 0.6922290388547703,
      "eval_crossner_literature_recall": 0.6831483350151017,
      "eval_crossner_music_f1": 0.7349567949224997,
      "eval_crossner_music_precision": 0.7216908361616562,
      "eval_crossner_music_recall": 0.748719590268862,
      "eval_crossner_politics_f1": 0.7485893416427953,
      "eval_crossner_politics_precision": 0.7325153374232949,
      "eval_crossner_politics_recall": 0.7653846153845958,
      "eval_crossner_science_f1": 0.7055040665284653,
      "eval_crossner_science_precision": 0.6764599202030948,
      "eval_crossner_science_recall": 0.7371541501975993,
      "eval_mit-movie_f1": 0.860757172926448,
      "eval_mit-movie_precision": 0.8429084380610261,
      "eval_mit-movie_recall": 0.8793781607042352,
      "eval_mit-restaurant_f1": 0.7718327182772087,
      "eval_mit-restaurant_precision": 0.7483601669647958,
      "eval_mit-restaurant_recall": 0.7968253968253716,
      "eval_runtime": 157.996,
      "eval_samples_per_second": 40.95,
      "eval_steps_per_second": 0.165,
      "step": 80
    },
    {
      "epoch": 1.267605633802817,
      "grad_norm": 0.16751327971759533,
      "learning_rate": 1.6519607843137256e-05,
      "loss": 0.0304,
      "step": 90
    },
    {
      "epoch": 1.267605633802817,
      "eval_average_f1": 0.7462558343360922,
      "eval_crossner_ai_f1": 0.6212218649017843,
      "eval_crossner_ai_precision": 0.6427145708582407,
      "eval_crossner_ai_recall": 0.6011200995643683,
      "eval_crossner_literature_f1": 0.6974921629594312,
      "eval_crossner_literature_precision": 0.7231852654387474,
      "eval_crossner_literature_recall": 0.6735620585267067,
      "eval_crossner_music_f1": 0.7709443098773395,
      "eval_crossner_music_precision": 0.7775968739823909,
      "eval_crossner_music_recall": 0.7644046094750075,
      "eval_crossner_politics_f1": 0.7723080919099834,
      "eval_crossner_politics_precision": 0.7825743616741042,
      "eval_crossner_politics_recall": 0.7623076923076727,
      "eval_crossner_science_f1": 0.707046714122577,
      "eval_crossner_science_precision": 0.7081681205392265,
      "eval_crossner_science_recall": 0.7059288537549128,
      "eval_mit-movie_f1": 0.8713019131557705,
      "eval_mit-movie_precision": 0.8683035714285553,
      "eval_mit-movie_recall": 0.8743210339014633,
      "eval_mit-restaurant_f1": 0.7834757834257591,
      "eval_mit-restaurant_precision": 0.7812499999999754,
      "eval_mit-restaurant_recall": 0.7857142857142607,
      "eval_runtime": 107.7494,
      "eval_samples_per_second": 60.047,
      "eval_steps_per_second": 0.241,
      "step": 90
    },
    {
      "epoch": 1.408450704225352,
      "grad_norm": 0.1191516738208451,
      "learning_rate": 1.6029411764705884e-05,
      "loss": 0.0292,
      "step": 100
    },
    {
      "epoch": 1.408450704225352,
      "eval_average_f1": 0.7468637068880802,
      "eval_crossner_ai_f1": 0.647559055068077,
      "eval_crossner_ai_precision": 0.6556122448979174,
      "eval_crossner_ai_recall": 0.6397013067827854,
      "eval_crossner_literature_f1": 0.6678635547076097,
      "eval_crossner_literature_precision": 0.6791862284819676,
      "eval_crossner_literature_recall": 0.6569122098889678,
      "eval_crossner_music_f1": 0.7490745211152761,
      "eval_crossner_music_precision": 0.7533182259630704,
      "eval_crossner_music_recall": 0.7448783610755203,
      "eval_crossner_politics_f1": 0.7622116251594159,
      "eval_crossner_politics_precision": 0.7662606892977256,
      "eval_crossner_politics_recall": 0.7582051282051088,
      "eval_crossner_science_f1": 0.7435395457604687,
      "eval_crossner_science_precision": 0.736617532971267,
      "eval_crossner_science_recall": 0.7505928853754644,
      "eval_mit-movie_f1": 0.8747906976244046,
      "eval_mit-movie_precision": 0.8689706154130314,
      "eval_mit-movie_recall": 0.8806892676531021,
      "eval_mit-restaurant_f1": 0.7830069487813095,
      "eval_mit-restaurant_precision": 0.779069767441836,
      "eval_mit-restaurant_recall": 0.786984126984102,
      "eval_runtime": 158.1766,
      "eval_samples_per_second": 40.904,
      "eval_steps_per_second": 0.164,
      "step": 100
    },
    {
      "epoch": 1.5492957746478875,
      "grad_norm": 0.14231653908135516,
      "learning_rate": 1.5539215686274513e-05,
      "loss": 0.029,
      "step": 110
    },
    {
      "epoch": 1.5492957746478875,
      "eval_average_f1": 0.7512325979830272,
      "eval_crossner_ai_f1": 0.6412556053311682,
      "eval_crossner_ai_precision": 0.660726072607217,
      "eval_crossner_ai_recall": 0.6228998133167005,
      "eval_crossner_literature_f1": 0.6872062662685632,
      "eval_crossner_literature_precision": 0.7121212121211735,
      "eval_crossner_literature_recall": 0.6639757820383116,
      "eval_crossner_music_f1": 0.7681041496652047,
      "eval_crossner_music_precision": 0.7811982787156312,
      "eval_crossner_music_recall": 0.7554417413572101,
      "eval_crossner_politics_f1": 0.7852614896489152,
      "eval_crossner_politics_precision": 0.8096405228757949,
      "eval_crossner_politics_recall": 0.7623076923076727,
      "eval_crossner_science_f1": 0.7100326263774278,
      "eval_crossner_science_precision": 0.7333614153327408,
      "eval_crossner_science_recall": 0.6881422924900914,
      "eval_mit-movie_f1": 0.8780624648835914,
      "eval_mit-movie_precision": 0.8767507002800957,
      "eval_mit-movie_recall": 0.8793781607042352,
      "eval_mit-restaurant_f1": 0.7887055837063202,
      "eval_mit-restaurant_precision": 0.7882054533924925,
      "eval_mit-restaurant_recall": 0.7892063492063242,
      "eval_runtime": 226.847,
      "eval_samples_per_second": 28.521,
      "eval_steps_per_second": 0.115,
      "step": 110
    },
    {
      "epoch": 1.6901408450704225,
      "grad_norm": 0.15010895298755259,
      "learning_rate": 1.5049019607843138e-05,
      "loss": 0.0301,
      "step": 120
    },
    {
      "epoch": 1.6901408450704225,
      "eval_average_f1": 0.7535718330000934,
      "eval_crossner_ai_f1": 0.6679233427083667,
      "eval_crossner_ai_precision": 0.6744923857867592,
      "eval_crossner_ai_recall": 0.6614810205351175,
      "eval_crossner_literature_f1": 0.6830403308703678,
      "eval_crossner_literature_precision": 0.7004241781547879,
      "eval_crossner_literature_recall": 0.666498486377363,
      "eval_crossner_music_f1": 0.7635777598210495,
      "eval_crossner_music_precision": 0.7689061992859212,
      "eval_crossner_music_recall": 0.7583226632522164,
      "eval_crossner_politics_f1": 0.8033946251267831,
      "eval_crossner_politics_precision": 0.8057776631415835,
      "eval_crossner_politics_recall": 0.8010256410256205,
      "eval_crossner_science_f1": 0.7097541633123832,
      "eval_crossner_science_precision": 0.7120127287191443,
      "eval_crossner_science_recall": 0.707509881422897,
      "eval_mit-movie_f1": 0.8666291502032196,
      "eval_mit-movie_precision": 0.8679316175089072,
      "eval_mit-movie_recall": 0.8653305862520909,
      "eval_mit-restaurant_f1": 0.7806834589584839,
      "eval_mit-restaurant_precision": 0.7891663963671492,
      "eval_mit-restaurant_recall": 0.7723809523809279,
      "eval_runtime": 224.7394,
      "eval_samples_per_second": 28.789,
      "eval_steps_per_second": 0.116,
      "step": 120
    },
    {
      "epoch": 1.8309859154929577,
      "grad_norm": 0.10032364525631253,
      "learning_rate": 1.4558823529411765e-05,
      "loss": 0.0284,
      "step": 130
    },
    {
      "epoch": 1.8309859154929577,
      "eval_average_f1": 0.7707514403193453,
      "eval_crossner_ai_f1": 0.6798343420971659,
      "eval_crossner_ai_precision": 0.6964751958224088,
      "eval_crossner_ai_recall": 0.6639701306782412,
      "eval_crossner_literature_f1": 0.7062467733108578,
      "eval_crossner_literature_precision": 0.7230443974629639,
      "eval_crossner_literature_recall": 0.6902119071644455,
      "eval_crossner_music_f1": 0.7816833279087004,
      "eval_crossner_music_precision": 0.7875243664717093,
      "eval_crossner_music_recall": 0.7759282970550327,
      "eval_crossner_politics_f1": 0.7821257909870482,
      "eval_crossner_politics_precision": 0.7879260994014887,
      "eval_crossner_politics_recall": 0.7764102564102365,
      "eval_crossner_science_f1": 0.7684064297971623,
      "eval_crossner_science_precision": 0.7716221602231658,
      "eval_crossner_science_recall": 0.7652173913043175,
      "eval_mit-movie_f1": 0.8803773584405521,
      "eval_mit-movie_precision": 0.8869036304884834,
      "eval_mit-movie_recall": 0.8739464319160728,
      "eval_mit-restaurant_f1": 0.7965860596939303,
      "eval_mit-restaurant_precision": 0.793201133144451,
      "eval_mit-restaurant_recall": 0.7999999999999746,
      "eval_runtime": 225.5179,
      "eval_samples_per_second": 28.69,
      "eval_steps_per_second": 0.115,
      "step": 130
    },
    {
      "epoch": 1.971830985915493,
      "grad_norm": 0.13522687015171175,
      "learning_rate": 1.4068627450980394e-05,
      "loss": 0.0298,
      "step": 140
    },
    {
      "epoch": 1.971830985915493,
      "eval_average_f1": 0.7705279704097074,
      "eval_crossner_ai_f1": 0.6847009317203802,
      "eval_crossner_ai_precision": 0.6622093023255429,
      "eval_crossner_ai_recall": 0.7087741132544674,
      "eval_crossner_literature_f1": 0.7310071763914568,
      "eval_crossner_literature_precision": 0.7173385138416358,
      "eval_crossner_literature_recall": 0.7452068617557646,
      "eval_crossner_music_f1": 0.7996817819706605,
      "eval_crossner_music_precision": 0.7950015817778933,
      "eval_crossner_music_recall": 0.8044174135723174,
      "eval_crossner_politics_f1": 0.7957044233710999,
      "eval_crossner_politics_precision": 0.7934727180009996,
      "eval_crossner_politics_recall": 0.7979487179486975,
      "eval_crossner_science_f1": 0.7299326274764735,
      "eval_crossner_science_precision": 0.7114446529080408,
      "eval_crossner_science_recall": 0.7494071146244763,
      "eval_mit-movie_f1": 0.8737578210762414,
      "eval_mit-movie_precision": 0.8587448001446761,
      "eval_mit-movie_recall": 0.889305113317084,
      "eval_mit-restaurant_f1": 0.7789110308616407,
      "eval_mit-restaurant_precision": 0.7700899782810806,
      "eval_mit-restaurant_recall": 0.787936507936483,
      "eval_runtime": 103.7428,
      "eval_samples_per_second": 62.366,
      "eval_steps_per_second": 0.251,
      "step": 140
    },
    {
      "epoch": 2.112676056338028,
      "grad_norm": 0.14495340759983374,
      "learning_rate": 1.357843137254902e-05,
      "loss": 0.0245,
      "step": 150
    },
    {
      "epoch": 2.112676056338028,
      "eval_average_f1": 0.7524035146778031,
      "eval_crossner_ai_f1": 0.6722859068956852,
      "eval_crossner_ai_precision": 0.7336276674024478,
      "eval_crossner_ai_recall": 0.6204107031735768,
      "eval_crossner_literature_f1": 0.6938556479244069,
      "eval_crossner_literature_precision": 0.7409742120343414,
      "eval_crossner_literature_recall": 0.6523713420786754,
      "eval_crossner_music_f1": 0.7673510944690761,
      "eval_crossner_music_precision": 0.816540267244463,
      "eval_crossner_music_recall": 0.7237516005121407,
      "eval_crossner_politics_f1": 0.7500684368513852,
      "eval_crossner_politics_precision": 0.8044627128596358,
      "eval_crossner_politics_recall": 0.7025641025640845,
      "eval_crossner_science_f1": 0.7082035305835371,
      "eval_crossner_science_precision": 0.7461706783369476,
      "eval_crossner_science_recall": 0.6739130434782342,
      "eval_mit-movie_f1": 0.8877919118543023,
      "eval_mit-movie_precision": 0.900096246390743,
      "eval_mit-movie_recall": 0.8758194418430254,
      "eval_mit-restaurant_f1": 0.7872680741662286,
      "eval_mit-restaurant_precision": 0.7933591231463316,
      "eval_mit-restaurant_recall": 0.7812698412698165,
      "eval_runtime": 123.5735,
      "eval_samples_per_second": 52.358,
      "eval_steps_per_second": 0.21,
      "step": 150
    },
    {
      "epoch": 2.2535211267605635,
      "grad_norm": 0.0808899164109845,
      "learning_rate": 1.3088235294117648e-05,
      "loss": 0.0222,
      "step": 160
    },
    {
      "epoch": 2.2535211267605635,
      "eval_average_f1": 0.7768047925486884,
      "eval_crossner_ai_f1": 0.6780185758013524,
      "eval_crossner_ai_precision": 0.6746765249537476,
      "eval_crossner_ai_recall": 0.6813939016801069,
      "eval_crossner_literature_f1": 0.724112242314019,
      "eval_crossner_literature_precision": 0.7129584352077891,
      "eval_crossner_literature_recall": 0.7356205852673695,
      "eval_crossner_music_f1": 0.8015785319152563,
      "eval_crossner_music_precision": 0.7907194020554098,
      "eval_crossner_music_recall": 0.8127400768245578,
      "eval_crossner_politics_f1": 0.7865283646573689,
      "eval_crossner_politics_precision": 0.7856229214632697,
      "eval_crossner_politics_recall": 0.7874358974358773,
      "eval_crossner_science_f1": 0.7569028769532796,
      "eval_crossner_science_precision": 0.7399018497545965,
      "eval_crossner_science_recall": 0.7747035573122223,
      "eval_mit-movie_f1": 0.8848981110507579,
      "eval_mit-movie_precision": 0.8792529585798654,
      "eval_mit-movie_recall": 0.8906162202659508,
      "eval_mit-restaurant_f1": 0.8055948451487851,
      "eval_mit-restaurant_precision": 0.797696856520361,
      "eval_mit-restaurant_recall": 0.8136507936507678,
      "eval_runtime": 161.8994,
      "eval_samples_per_second": 39.963,
      "eval_steps_per_second": 0.161,
      "step": 160
    },
    {
      "epoch": 2.3943661971830985,
      "grad_norm": 0.16999945568600636,
      "learning_rate": 1.2598039215686275e-05,
      "loss": 0.0216,
      "step": 170
    },
    {
      "epoch": 2.3943661971830985,
      "eval_average_f1": 0.7719122274992254,
      "eval_crossner_ai_f1": 0.6562287269755838,
      "eval_crossner_ai_precision": 0.7242674680690665,
      "eval_crossner_ai_recall": 0.5998755444928064,
      "eval_crossner_literature_f1": 0.7236771779298273,
      "eval_crossner_literature_precision": 0.769318181818138,
      "eval_crossner_literature_recall": 0.6831483350151017,
      "eval_crossner_music_f1": 0.7952847417730552,
      "eval_crossner_music_precision": 0.8261469472231519,
      "eval_crossner_music_recall": 0.7666453265044569,
      "eval_crossner_politics_f1": 0.7994076466921609,
      "eval_crossner_politics_precision": 0.8415532879818356,
      "eval_crossner_politics_recall": 0.7612820512820317,
      "eval_crossner_science_f1": 0.7481296757605757,
      "eval_crossner_science_precision": 0.7887817703768278,
      "eval_crossner_science_recall": 0.7114624505928573,
      "eval_mit-movie_f1": 0.887389919380375,
      "eval_mit-movie_precision": 0.8877225866916422,
      "eval_mit-movie_recall": 0.8870575014047408,
      "eval_mit-restaurant_f1": 0.7932677039830012,
      "eval_mit-restaurant_precision": 0.7935196950444474,
      "eval_mit-restaurant_recall": 0.7930158730158479,
      "eval_runtime": 209.9362,
      "eval_samples_per_second": 30.819,
      "eval_steps_per_second": 0.124,
      "step": 170
    },
    {
      "epoch": 2.535211267605634,
      "grad_norm": 0.10001023825001198,
      "learning_rate": 1.2107843137254901e-05,
      "loss": 0.0216,
      "step": 180
    },
    {
      "epoch": 2.535211267605634,
      "eval_average_f1": 0.7750806830253143,
      "eval_crossner_ai_f1": 0.6739479601170507,
      "eval_crossner_ai_precision": 0.6965471447542698,
      "eval_crossner_ai_recall": 0.6527691350341847,
      "eval_crossner_literature_f1": 0.7364038511079608,
      "eval_crossner_literature_precision": 0.7603439011283847,
      "eval_crossner_literature_recall": 0.7139253279515281,
      "eval_crossner_music_f1": 0.763500325259032,
      "eval_crossner_music_precision": 0.776124338624313,
      "eval_crossner_music_recall": 0.7512804097310899,
      "eval_crossner_politics_f1": 0.8069081936910599,
      "eval_crossner_politics_precision": 0.8174164693501494,
      "eval_crossner_politics_recall": 0.7966666666666462,
      "eval_crossner_science_f1": 0.7654171704457565,
      "eval_crossner_science_precision": 0.7808388157894416,
      "eval_crossner_science_recall": 0.7505928853754644,
      "eval_mit-movie_f1": 0.8821485585000824,
      "eval_mit-movie_precision": 0.8845574387947103,
      "eval_mit-movie_recall": 0.8797527626896258,
      "eval_mit-restaurant_f1": 0.7972387220562579,
      "eval_mit-restaurant_precision": 0.806430659304943,
      "eval_mit-restaurant_recall": 0.7882539682539432,
      "eval_runtime": 195.652,
      "eval_samples_per_second": 33.069,
      "eval_steps_per_second": 0.133,
      "step": 180
    },
    {
      "epoch": 2.676056338028169,
      "grad_norm": 0.09631077384299239,
      "learning_rate": 1.1617647058823532e-05,
      "loss": 0.0219,
      "step": 190
    },
    {
      "epoch": 2.676056338028169,
      "eval_average_f1": 0.786774928669667,
      "eval_crossner_ai_f1": 0.7085899513276751,
      "eval_crossner_ai_precision": 0.7395128552096928,
      "eval_crossner_ai_recall": 0.6801493466085451,
      "eval_crossner_literature_f1": 0.7274606044449538,
      "eval_crossner_literature_precision": 0.7453679195341055,
      "eval_crossner_literature_recall": 0.7103935418768562,
      "eval_crossner_music_f1": 0.7844925883194315,
      "eval_crossner_music_precision": 0.798673300165811,
      "eval_crossner_music_recall": 0.7708066581305771,
      "eval_crossner_politics_f1": 0.8334614398679905,
      "eval_crossner_politics_precision": 0.8328213005632147,
      "eval_crossner_politics_recall": 0.8341025641025427,
      "eval_crossner_science_f1": 0.7707999999499763,
      "eval_crossner_science_precision": 0.7801619433198065,
      "eval_crossner_science_recall": 0.7616600790513532,
      "eval_mit-movie_f1": 0.8888679244782878,
      "eval_mit-movie_precision": 0.8954571374263278,
      "eval_mit-movie_recall": 0.8823749765873594,
      "eval_mit-restaurant_f1": 0.793751992299354,
      "eval_mit-restaurant_precision": 0.7970550576184123,
      "eval_mit-restaurant_recall": 0.7904761904761654,
      "eval_runtime": 178.1071,
      "eval_samples_per_second": 36.326,
      "eval_steps_per_second": 0.146,
      "step": 190
    },
    {
      "epoch": 2.816901408450704,
      "grad_norm": 0.18117238265505553,
      "learning_rate": 1.1127450980392159e-05,
      "loss": 0.0228,
      "step": 200
    },
    {
      "epoch": 2.816901408450704,
      "eval_average_f1": 0.7914800603238122,
      "eval_crossner_ai_f1": 0.7138403989524494,
      "eval_crossner_ai_precision": 0.7151780137413669,
      "eval_crossner_ai_recall": 0.7125077784691529,
      "eval_crossner_literature_f1": 0.7350993376983116,
      "eval_crossner_literature_precision": 0.7422839506172457,
      "eval_crossner_literature_recall": 0.7280524722502155,
      "eval_crossner_music_f1": 0.8123784834237338,
      "eval_crossner_music_precision": 0.822506561679763,
      "eval_crossner_music_recall": 0.8024967989756465,
      "eval_crossner_politics_f1": 0.8259866734507477,
      "eval_crossner_politics_precision": 0.8255635245901428,
      "eval_crossner_politics_recall": 0.8264102564102352,
      "eval_crossner_science_f1": 0.7793650793150492,
      "eval_crossner_science_precision": 0.7824701195218812,
      "eval_crossner_science_recall": 0.7762845849802065,
      "eval_mit-movie_f1": 0.8877378036795007,
      "eval_mit-movie_precision": 0.8927827240007408,
      "eval_mit-movie_recall": 0.8827495785727499,
      "eval_mit-restaurant_f1": 0.7859526457468923,
      "eval_mit-restaurant_precision": 0.7868278714603631,
      "eval_mit-restaurant_recall": 0.7850793650793402,
      "eval_runtime": 176.5655,
      "eval_samples_per_second": 36.644,
      "eval_steps_per_second": 0.147,
      "step": 200
    },
    {
      "epoch": 2.9577464788732395,
      "grad_norm": 0.12996346257115557,
      "learning_rate": 1.0637254901960786e-05,
      "loss": 0.0214,
      "step": 210
    },
    {
      "epoch": 2.9577464788732395,
      "eval_average_f1": 0.7767234107177281,
      "eval_crossner_ai_f1": 0.6966067863773389,
      "eval_crossner_ai_precision": 0.748391708363063,
      "eval_crossner_ai_recall": 0.6515245799626228,
      "eval_crossner_literature_f1": 0.7113765951172358,
      "eval_crossner_literature_precision": 0.7701352145796138,
      "eval_crossner_literature_recall": 0.6609485368314499,
      "eval_crossner_music_f1": 0.7795753285648762,
      "eval_crossner_music_precision": 0.8231316725978355,
      "eval_crossner_music_recall": 0.7403969270166216,
      "eval_crossner_politics_f1": 0.803361792906304,
      "eval_crossner_politics_precision": 0.8373192436039811,
      "eval_crossner_politics_recall": 0.7720512820512623,
      "eval_crossner_science_f1": 0.7618243242745262,
      "eval_crossner_science_precision": 0.8177697189482857,
      "eval_crossner_science_recall": 0.7130434782608414,
      "eval_mit-movie_f1": 0.8871499952361173,
      "eval_mit-movie_precision": 0.8931283219437948,
      "eval_mit-movie_recall": 0.8812511706311879,
      "eval_mit-restaurant_f1": 0.7971690525476992,
      "eval_mit-restaurant_precision": 0.8079556569937787,
      "eval_mit-restaurant_recall": 0.7866666666666416,
      "eval_runtime": 177.9484,
      "eval_samples_per_second": 36.359,
      "eval_steps_per_second": 0.146,
      "step": 210
    },
    {
      "epoch": 3.0985915492957745,
      "grad_norm": 0.11047843796201202,
      "learning_rate": 1.0147058823529413e-05,
      "loss": 0.0174,
      "step": 220
    },
    {
      "epoch": 3.0985915492957745,
      "eval_average_f1": 0.7886540651395014,
      "eval_crossner_ai_f1": 0.7062219370891939,
      "eval_crossner_ai_precision": 0.7286565188616327,
      "eval_crossner_ai_recall": 0.6851275668947925,
      "eval_crossner_literature_f1": 0.7321521625346953,
      "eval_crossner_literature_precision": 0.7570043103447868,
      "eval_crossner_literature_recall": 0.7088799192734254,
      "eval_crossner_music_f1": 0.8031802692993263,
      "eval_crossner_music_precision": 0.8144126357354124,
      "eval_crossner_music_recall": 0.7922535211267352,
      "eval_crossner_politics_f1": 0.8145951520902592,
      "eval_crossner_politics_precision": 0.8192427385891904,
      "eval_crossner_politics_recall": 0.8099999999999792,
      "eval_crossner_science_f1": 0.7744510977543653,
      "eval_crossner_science_precision": 0.7822580645160975,
      "eval_crossner_science_recall": 0.7667984189723017,
      "eval_mit-movie_f1": 0.8857786961739178,
      "eval_mit-movie_precision": 0.8884492180139271,
      "eval_mit-movie_recall": 0.8831241805581405,
      "eval_mit-restaurant_f1": 0.8041991410347525,
      "eval_mit-restaurant_precision": 0.8058654765699456,
      "eval_mit-restaurant_recall": 0.802539682539657,
      "eval_runtime": 160.2736,
      "eval_samples_per_second": 40.368,
      "eval_steps_per_second": 0.162,
      "step": 220
    },
    {
      "epoch": 3.23943661971831,
      "grad_norm": 0.08688021654480746,
      "learning_rate": 9.65686274509804e-06,
      "loss": 0.0167,
      "step": 230
    },
    {
      "epoch": 3.23943661971831,
      "eval_average_f1": 0.7802108791483163,
      "eval_crossner_ai_f1": 0.6995967741438212,
      "eval_crossner_ai_precision": 0.7604090577062994,
      "eval_crossner_ai_recall": 0.6477909147479373,
      "eval_crossner_literature_f1": 0.7252568955694731,
      "eval_crossner_literature_precision": 0.7814685314684859,
      "eval_crossner_literature_recall": 0.6765893037335683,
      "eval_crossner_music_f1": 0.7895263508506624,
      "eval_crossner_music_precision": 0.824164345403871,
      "eval_crossner_music_recall": 0.7576824583866595,
      "eval_crossner_politics_f1": 0.7967758984701104,
      "eval_crossner_politics_precision": 0.8219738276989961,
      "eval_crossner_politics_recall": 0.7730769230769032,
      "eval_crossner_science_f1": 0.7602865915569932,
      "eval_crossner_science_precision": 0.7885350318471003,
      "eval_crossner_science_recall": 0.7339920948616311,
      "eval_mit-movie_f1": 0.8881151782693596,
      "eval_mit-movie_precision": 0.8923978819969574,
      "eval_mit-movie_recall": 0.8838733845289214,
      "eval_mit-restaurant_f1": 0.8019184651777946,
      "eval_mit-restaurant_precision": 0.8077294685990077,
      "eval_mit-restaurant_recall": 0.7961904761904509,
      "eval_runtime": 164.3542,
      "eval_samples_per_second": 39.366,
      "eval_steps_per_second": 0.158,
      "step": 230
    },
    {
      "epoch": 3.380281690140845,
      "grad_norm": 0.13950849069799098,
      "learning_rate": 9.166666666666666e-06,
      "loss": 0.0159,
      "step": 240
    },
    {
      "epoch": 3.380281690140845,
      "eval_average_f1": 0.7904227105091595,
      "eval_crossner_ai_f1": 0.7260057015654281,
      "eval_crossner_ai_precision": 0.7393548387096297,
      "eval_crossner_ai_recall": 0.7131300560049338,
      "eval_crossner_literature_f1": 0.7353317960042917,
      "eval_crossner_literature_precision": 0.7470067673086545,
      "eval_crossner_literature_recall": 0.7240161453077334,
      "eval_crossner_music_f1": 0.8022782749703297,
      "eval_crossner_music_precision": 0.8159549817940809,
      "eval_crossner_music_recall": 0.7890524967989504,
      "eval_crossner_politics_f1": 0.8182759512831442,
      "eval_crossner_politics_precision": 0.8261892315734233,
      "eval_crossner_politics_recall": 0.8105128205127997,
      "eval_crossner_science_f1": 0.7681073613077752,
      "eval_crossner_science_precision": 0.7670476941268913,
      "eval_crossner_science_recall": 0.7691699604742779,
      "eval_mit-movie_f1": 0.8870513060954452,
      "eval_mit-movie_precision": 0.8900622289270056,
      "eval_mit-movie_recall": 0.8840606855216167,
      "eval_mit-restaurant_f1": 0.7959085823377027,
      "eval_mit-restaurant_precision": 0.8014161570646668,
      "eval_mit-restaurant_recall": 0.7904761904761654,
      "eval_runtime": 147.2889,
      "eval_samples_per_second": 43.927,
      "eval_steps_per_second": 0.177,
      "step": 240
    },
    {
      "epoch": 3.52112676056338,
      "grad_norm": 0.09338147029615831,
      "learning_rate": 8.676470588235295e-06,
      "loss": 0.0158,
      "step": 250
    },
    {
      "epoch": 3.52112676056338,
      "eval_average_f1": 0.7954138894367625,
      "eval_crossner_ai_f1": 0.7281220802987557,
      "eval_crossner_ai_precision": 0.7288029925186578,
      "eval_crossner_ai_recall": 0.727442439327895,
      "eval_crossner_literature_f1": 0.742698892195683,
      "eval_crossner_literature_precision": 0.7412060301507165,
      "eval_crossner_literature_recall": 0.744197780020144,
      "eval_crossner_music_f1": 0.8110806892719279,
      "eval_crossner_music_precision": 0.8162074554294712,
      "eval_crossner_music_recall": 0.8060179257362098,
      "eval_crossner_politics_f1": 0.8268270830166627,
      "eval_crossner_politics_precision": 0.8254536161512694,
      "eval_crossner_politics_recall": 0.8282051282051069,
      "eval_crossner_science_f1": 0.7798058251927035,
      "eval_crossner_science_precision": 0.7664122137404288,
      "eval_crossner_science_recall": 0.7936758893280319,
      "eval_mit-movie_f1": 0.8860949520986043,
      "eval_mit-movie_precision": 0.8877608573039878,
      "eval_mit-movie_recall": 0.8844352875070072,
      "eval_mit-restaurant_f1": 0.7932677039830012,
      "eval_mit-restaurant_precision": 0.7935196950444474,
      "eval_mit-restaurant_recall": 0.7930158730158479,
      "eval_runtime": 161.3396,
      "eval_samples_per_second": 40.102,
      "eval_steps_per_second": 0.161,
      "step": 250
    },
    {
      "epoch": 3.6619718309859155,
      "grad_norm": 0.12622217882131187,
      "learning_rate": 8.186274509803922e-06,
      "loss": 0.0155,
      "step": 260
    },
    {
      "epoch": 3.6619718309859155,
      "eval_average_f1": 0.7988353449547548,
      "eval_crossner_ai_f1": 0.7356608478302534,
      "eval_crossner_ai_precision": 0.7370393504059501,
      "eval_crossner_ai_recall": 0.734287492221485,
      "eval_crossner_literature_f1": 0.7497507477066996,
      "eval_crossner_literature_precision": 0.7408866995073526,
      "eval_crossner_literature_recall": 0.7588294651866418,
      "eval_crossner_music_f1": 0.8208667736257366,
      "eval_crossner_music_precision": 0.8232453316162002,
      "eval_crossner_music_recall": 0.8185019206145705,
      "eval_crossner_politics_f1": 0.8200128286863484,
      "eval_crossner_politics_precision": 0.8205391527599276,
      "eval_crossner_politics_recall": 0.8194871794871584,
      "eval_crossner_science_f1": 0.7868217053763449,
      "eval_crossner_science_precision": 0.7718631178706931,
      "eval_crossner_science_recall": 0.8023715415019446,
      "eval_mit-movie_f1": 0.8866710194444344,
      "eval_mit-movie_precision": 0.8831289483463233,
      "eval_mit-movie_recall": 0.8902416182805603,
      "eval_mit-restaurant_f1": 0.7920634920134669,
      "eval_mit-restaurant_precision": 0.7920634920634669,
      "eval_mit-restaurant_recall": 0.7920634920634669,
      "eval_runtime": 145.0025,
      "eval_samples_per_second": 44.62,
      "eval_steps_per_second": 0.179,
      "step": 260
    },
    {
      "epoch": 3.802816901408451,
      "grad_norm": 0.19384496576474353,
      "learning_rate": 7.69607843137255e-06,
      "loss": 0.0162,
      "step": 270
    },
    {
      "epoch": 3.802816901408451,
      "eval_average_f1": 0.7965541459466806,
      "eval_crossner_ai_f1": 0.7254471289113638,
      "eval_crossner_ai_precision": 0.7316455696202069,
      "eval_crossner_ai_recall": 0.719352831362743,
      "eval_crossner_literature_f1": 0.75057324835761,
      "eval_crossner_literature_precision": 0.7581060216160186,
      "eval_crossner_literature_recall": 0.7431886982845235,
      "eval_crossner_music_f1": 0.8069145465905655,
      "eval_crossner_music_precision": 0.8224734042552918,
      "eval_crossner_music_recall": 0.7919334186939567,
      "eval_crossner_politics_f1": 0.8202921230590179,
      "eval_crossner_politics_precision": 0.8346602972398929,
      "eval_crossner_politics_recall": 0.8064102564102357,
      "eval_crossner_science_f1": 0.7792463996343252,
      "eval_crossner_science_precision": 0.7778653012996937,
      "eval_crossner_science_recall": 0.7806324110671629,
      "eval_mit-movie_f1": 0.8853408029378457,
      "eval_mit-movie_precision": 0.8827034071867271,
      "eval_mit-movie_recall": 0.8879940063682171,
      "eval_mit-restaurant_f1": 0.8080647721360356,
      "eval_mit-restaurant_precision": 0.808193077167329,
      "eval_mit-restaurant_recall": 0.8079365079364823,
      "eval_runtime": 160.7877,
      "eval_samples_per_second": 40.239,
      "eval_steps_per_second": 0.162,
      "step": 270
    },
    {
      "epoch": 3.943661971830986,
      "grad_norm": 0.10246834814313366,
      "learning_rate": 7.205882352941177e-06,
      "loss": 0.015,
      "step": 280
    },
    {
      "epoch": 3.943661971830986,
      "eval_average_f1": 0.7931038406423988,
      "eval_crossner_ai_f1": 0.7241276468334307,
      "eval_crossner_ai_precision": 0.6953035509736142,
      "eval_crossner_ai_recall": 0.7554449284380363,
      "eval_crossner_literature_f1": 0.7410124724371712,
      "eval_crossner_literature_precision": 0.7190317987659839,
      "eval_crossner_literature_recall": 0.7643794147325548,
      "eval_crossner_music_f1": 0.8031645569120064,
      "eval_crossner_music_precision": 0.7941176470587987,
      "eval_crossner_music_recall": 0.8124199743917794,
      "eval_crossner_politics_f1": 0.8156495314769561,
      "eval_crossner_politics_precision": 0.8056528264131865,
      "eval_crossner_politics_recall": 0.8258974358974147,
      "eval_crossner_science_f1": 0.7803030302530874,
      "eval_crossner_science_precision": 0.7490909090908818,
      "eval_crossner_science_recall": 0.8142292490118255,
      "eval_mit-movie_f1": 0.8811604983691789,
      "eval_mit-movie_precision": 0.8749769159741297,
      "eval_mit-movie_recall": 0.8874321033901313,
      "eval_mit-restaurant_f1": 0.8063091482149608,
      "eval_mit-restaurant_precision": 0.8012539184952727,
      "eval_mit-restaurant_recall": 0.8114285714285456,
      "eval_runtime": 144.3211,
      "eval_samples_per_second": 44.831,
      "eval_steps_per_second": 0.18,
      "step": 280
    },
    {
      "epoch": 4.084507042253521,
      "grad_norm": 0.08565250738012338,
      "learning_rate": 6.715686274509804e-06,
      "loss": 0.0131,
      "step": 290
    },
    {
      "epoch": 4.084507042253521,
      "eval_average_f1": 0.7956766867925502,
      "eval_crossner_ai_f1": 0.7304878048280246,
      "eval_crossner_ai_precision": 0.7160789001792758,
      "eval_crossner_ai_recall": 0.7454884878655417,
      "eval_crossner_literature_f1": 0.7278672031692797,
      "eval_crossner_literature_precision": 0.7256770310932434,
      "eval_crossner_literature_recall": 0.7300706357214566,
      "eval_crossner_music_f1": 0.8140912729683886,
      "eval_crossner_music_precision": 0.8144825376481636,
      "eval_crossner_music_recall": 0.8137003841228932,
      "eval_crossner_politics_f1": 0.8255679629842492,
      "eval_crossner_politics_precision": 0.8265227447956611,
      "eval_crossner_politics_recall": 0.8246153846153634,
      "eval_crossner_science_f1": 0.781456953592364,
      "eval_crossner_science_precision": 0.7703533026113375,
      "eval_crossner_science_recall": 0.7928853754940398,
      "eval_mit-movie_f1": 0.8842242746024704,
      "eval_mit-movie_precision": 0.8808550185873442,
      "eval_mit-movie_recall": 0.8876194043828266,
      "eval_mit-restaurant_f1": 0.8060413354030747,
      "eval_mit-restaurant_precision": 0.8073248407643054,
      "eval_mit-restaurant_recall": 0.8047619047618793,
      "eval_runtime": 125.9791,
      "eval_samples_per_second": 51.358,
      "eval_steps_per_second": 0.206,
      "step": 290
    },
    {
      "epoch": 4.225352112676056,
      "grad_norm": 0.10852730024983802,
      "learning_rate": 6.225490196078432e-06,
      "loss": 0.0114,
      "step": 300
    },
    {
      "epoch": 4.225352112676056,
      "eval_average_f1": 0.7920652421760216,
      "eval_crossner_ai_f1": 0.7255025125127729,
      "eval_crossner_ai_precision": 0.7324032974000804,
      "eval_crossner_ai_recall": 0.718730553826962,
      "eval_crossner_literature_f1": 0.7355670102592637,
      "eval_crossner_literature_precision": 0.7518440463645547,
      "eval_crossner_literature_recall": 0.7199798183652513,
      "eval_crossner_music_f1": 0.8098717323740791,
      "eval_crossner_music_precision": 0.8217462932454425,
      "eval_crossner_music_recall": 0.7983354673495263,
      "eval_crossner_politics_f1": 0.8185478633088651,
      "eval_crossner_politics_precision": 0.8294287970518339,
      "eval_crossner_politics_recall": 0.8079487179486973,
      "eval_crossner_science_f1": 0.7680698966934177,
      "eval_crossner_science_precision": 0.7717478052673276,
      "eval_crossner_science_recall": 0.7644268774703254,
      "eval_mit-movie_f1": 0.8850499579292536,
      "eval_mit-movie_precision": 0.8824953445065012,
      "eval_mit-movie_recall": 0.8876194043828266,
      "eval_mit-restaurant_f1": 0.8018477221544988,
      "eval_mit-restaurant_precision": 0.80466751918156,
      "eval_mit-restaurant_recall": 0.7990476190475937,
      "eval_runtime": 121.7267,
      "eval_samples_per_second": 53.152,
      "eval_steps_per_second": 0.214,
      "step": 300
    },
    {
      "epoch": 4.366197183098592,
      "grad_norm": 0.1463106333028122,
      "learning_rate": 5.735294117647059e-06,
      "loss": 0.0113,
      "step": 310
    },
    {
      "epoch": 4.366197183098592,
      "eval_average_f1": 0.7937452699209658,
      "eval_crossner_ai_f1": 0.7270424318689058,
      "eval_crossner_ai_precision": 0.7401676337846074,
      "eval_crossner_ai_recall": 0.7143746110764957,
      "eval_crossner_literature_f1": 0.7393535145728599,
      "eval_crossner_literature_precision": 0.7520876826721945,
      "eval_crossner_literature_recall": 0.727043390514595,
      "eval_crossner_music_f1": 0.815665965316544,
      "eval_crossner_music_precision": 0.8248772504091383,
      "eval_crossner_music_recall": 0.8066581306017667,
      "eval_crossner_politics_f1": 0.8077619663147956,
      "eval_crossner_politics_precision": 0.8151436031331379,
      "eval_crossner_politics_recall": 0.8005128205127999,
      "eval_crossner_science_f1": 0.7722068692748807,
      "eval_crossner_science_precision": 0.7712933753942913,
      "eval_crossner_science_recall": 0.7731225296442382,
      "eval_mit-movie_f1": 0.8874259380671664,
      "eval_mit-movie_precision": 0.8911975821684758,
      "eval_mit-movie_recall": 0.8836860835362261,
      "eval_mit-restaurant_f1": 0.8067602040316079,
      "eval_mit-restaurant_precision": 0.8103779628443046,
      "eval_mit-restaurant_recall": 0.8031746031745777,
      "eval_runtime": 173.861,
      "eval_samples_per_second": 37.214,
      "eval_steps_per_second": 0.15,
      "step": 310
    },
    {
      "epoch": 4.507042253521127,
      "grad_norm": 0.12604554823213443,
      "learning_rate": 5.245098039215687e-06,
      "loss": 0.0121,
      "step": 320
    },
    {
      "epoch": 4.507042253521127,
      "eval_average_f1": 0.7932636755941294,
      "eval_crossner_ai_f1": 0.739171672412818,
      "eval_crossner_ai_precision": 0.7512853470436535,
      "eval_crossner_ai_recall": 0.727442439327895,
      "eval_crossner_literature_f1": 0.7455983668803079,
      "eval_crossner_literature_precision": 0.7542591636550978,
      "eval_crossner_literature_recall": 0.7371342078708003,
      "eval_crossner_music_f1": 0.8137461500553472,
      "eval_crossner_music_precision": 0.8243021346469351,
      "eval_crossner_music_recall": 0.8034571062739819,
      "eval_crossner_politics_f1": 0.8106363753210928,
      "eval_crossner_politics_precision": 0.8162204315050476,
      "eval_crossner_politics_recall": 0.8051282051281845,
      "eval_crossner_science_f1": 0.7663405087562372,
      "eval_crossner_science_precision": 0.7589147286821412,
      "eval_crossner_science_recall": 0.7739130434782303,
      "eval_mit-movie_f1": 0.8870300751379537,
      "eval_mit-movie_precision": 0.8902093944538598,
      "eval_mit-movie_recall": 0.8838733845289214,
      "eval_mit-restaurant_f1": 0.7903225805951487,
      "eval_mit-restaurant_precision": 0.8032786885245639,
      "eval_mit-restaurant_recall": 0.777777777777753,
      "eval_runtime": 209.8579,
      "eval_samples_per_second": 30.83,
      "eval_steps_per_second": 0.124,
      "step": 320
    },
    {
      "epoch": 4.647887323943662,
      "grad_norm": 0.102504615447174,
      "learning_rate": 4.754901960784314e-06,
      "loss": 0.0107,
      "step": 330
    },
    {
      "epoch": 4.647887323943662,
      "eval_average_f1": 0.7957491220707551,
      "eval_crossner_ai_f1": 0.7301486870742956,
      "eval_crossner_ai_precision": 0.7425997425996947,
      "eval_crossner_ai_recall": 0.7181082762911812,
      "eval_crossner_literature_f1": 0.7527542915205603,
      "eval_crossner_literature_precision": 0.7647058823529014,
      "eval_crossner_literature_recall": 0.7411705348132824,
      "eval_crossner_music_f1": 0.8035190615335676,
      "eval_crossner_music_precision": 0.818181818181791,
      "eval_crossner_music_recall": 0.7893725992317289,
      "eval_crossner_politics_f1": 0.8217616579810721,
      "eval_crossner_politics_precision": 0.8303664921465751,
      "eval_crossner_politics_recall": 0.8133333333333125,
      "eval_crossner_science_f1": 0.7767028627337799,
      "eval_crossner_science_precision": 0.7759368836291607,
      "eval_crossner_science_recall": 0.7774703557311946,
      "eval_mit-movie_f1": 0.8862466060730055,
      "eval_mit-movie_precision": 0.8859977536503016,
      "eval_mit-movie_recall": 0.886495598426655,
      "eval_mit-restaurant_f1": 0.7991106875790043,
      "eval_mit-restaurant_precision": 0.7994915792818303,
      "eval_mit-restaurant_recall": 0.7987301587301333,
      "eval_runtime": 177.7494,
      "eval_samples_per_second": 36.4,
      "eval_steps_per_second": 0.146,
      "step": 330
    },
    {
      "epoch": 4.788732394366197,
      "grad_norm": 0.14228183190017102,
      "learning_rate": 4.264705882352942e-06,
      "loss": 0.0112,
      "step": 340
    },
    {
      "epoch": 4.788732394366197,
      "eval_average_f1": 0.7980742641748033,
      "eval_crossner_ai_f1": 0.7319361901282477,
      "eval_crossner_ai_precision": 0.7358490566037272,
      "eval_crossner_ai_recall": 0.7280647168636759,
      "eval_crossner_literature_f1": 0.7522889114453871,
      "eval_crossner_literature_precision": 0.7584615384614996,
      "eval_crossner_literature_recall": 0.7462159434913851,
      "eval_crossner_music_f1": 0.816201387718252,
      "eval_crossner_music_precision": 0.822974292222557,
      "eval_crossner_music_recall": 0.809539052496773,
      "eval_crossner_politics_f1": 0.8237113401561657,
      "eval_crossner_politics_precision": 0.8279792746113775,
      "eval_crossner_politics_recall": 0.8194871794871584,
      "eval_crossner_science_f1": 0.7694722385216812,
      "eval_crossner_science_precision": 0.7639267627580536,
      "eval_crossner_science_recall": 0.7750988142292183,
      "eval_mit-movie_f1": 0.8864530634688141,
      "eval_mit-movie_precision": 0.886785379568868,
      "eval_mit-movie_recall": 0.8861209964412645,
      "eval_mit-restaurant_f1": 0.8064567177850754,
      "eval_mit-restaurant_precision": 0.8040391290627704,
      "eval_mit-restaurant_recall": 0.8088888888888632,
      "eval_runtime": 159.7432,
      "eval_samples_per_second": 40.503,
      "eval_steps_per_second": 0.163,
      "step": 340
    },
    {
      "epoch": 4.929577464788732,
      "grad_norm": 0.08713067846285208,
      "learning_rate": 3.774509803921569e-06,
      "loss": 0.011,
      "step": 350
    },
    {
      "epoch": 4.929577464788732,
      "eval_average_f1": 0.803803337366661,
      "eval_crossner_ai_f1": 0.7387276095855346,
      "eval_crossner_ai_precision": 0.733292458614302,
      "eval_crossner_ai_recall": 0.7442439327939798,
      "eval_crossner_literature_f1": 0.7577671128572616,
      "eval_crossner_literature_precision": 0.7587253414263653,
      "eval_crossner_literature_recall": 0.7568113017154008,
      "eval_crossner_music_f1": 0.8201923076422815,
      "eval_crossner_music_precision": 0.8212451861360455,
      "eval_crossner_music_recall": 0.8191421254801274,
      "eval_crossner_politics_f1": 0.8244039989246008,
      "eval_crossner_politics_precision": 0.824192721681168,
      "eval_crossner_politics_recall": 0.8246153846153634,
      "eval_crossner_science_f1": 0.7953307392495919,
      "eval_crossner_science_precision": 0.7831417624520772,
      "eval_crossner_science_recall": 0.807905138339889,
      "eval_mit-movie_f1": 0.8874718679169752,
      "eval_mit-movie_precision": 0.8886384976525654,
      "eval_mit-movie_recall": 0.8863082974339598,
      "eval_mit-restaurant_f1": 0.8027297253903808,
      "eval_mit-restaurant_precision": 0.8026023484607806,
      "eval_mit-restaurant_recall": 0.8028571428571174,
      "eval_runtime": 177.0341,
      "eval_samples_per_second": 36.547,
      "eval_steps_per_second": 0.147,
      "step": 350
    },
    {
      "epoch": 5.070422535211268,
      "grad_norm": 0.08079410800666396,
      "learning_rate": 3.2843137254901964e-06,
      "loss": 0.0095,
      "step": 360
    },
    {
      "epoch": 5.070422535211268,
      "eval_average_f1": 0.7999729461472785,
      "eval_crossner_ai_f1": 0.737492279134639,
      "eval_crossner_ai_precision": 0.7320662170447129,
      "eval_crossner_ai_recall": 0.742999377722418,
      "eval_crossner_literature_f1": 0.7510697205641079,
      "eval_crossner_literature_precision": 0.7493721747865018,
      "eval_crossner_literature_recall": 0.7527749747729187,
      "eval_crossner_music_f1": 0.8098061207639463,
      "eval_crossner_music_precision": 0.8107154315046259,
      "eval_crossner_music_recall": 0.8088988476312161,
      "eval_crossner_politics_f1": 0.8203224980303477,
      "eval_crossner_politics_precision": 0.8188553909044246,
      "eval_crossner_politics_recall": 0.8217948717948507,
      "eval_crossner_science_f1": 0.7877725856197622,
      "eval_crossner_science_precision": 0.7762854950114821,
      "eval_crossner_science_recall": 0.7996047430829724,
      "eval_mit-movie_f1": 0.8868702003620454,
      "eval_mit-movie_precision": 0.8868702004120456,
      "eval_mit-movie_recall": 0.8868702004120456,
      "eval_mit-restaurant_f1": 0.8064772185561023,
      "eval_mit-restaurant_precision": 0.8066052715147409,
      "eval_mit-restaurant_recall": 0.8063492063491807,
      "eval_runtime": 177.0777,
      "eval_samples_per_second": 36.538,
      "eval_steps_per_second": 0.147,
      "step": 360
    },
    {
      "epoch": 5.211267605633803,
      "grad_norm": 0.10374321300644081,
      "learning_rate": 2.7941176470588237e-06,
      "loss": 0.0082,
      "step": 370
    },
    {
      "epoch": 5.211267605633803,
      "eval_average_f1": 0.8010027367859915,
      "eval_crossner_ai_f1": 0.7431761786100041,
      "eval_crossner_ai_precision": 0.7408781694495522,
      "eval_crossner_ai_recall": 0.7454884878655417,
      "eval_crossner_literature_f1": 0.7539742618727477,
      "eval_crossner_literature_precision": 0.7541645633518044,
      "eval_crossner_literature_recall": 0.7537840565085392,
      "eval_crossner_music_f1": 0.8122593067535685,
      "eval_crossner_music_precision": 0.8143500643500381,
      "eval_crossner_music_recall": 0.81017925736233,
      "eval_crossner_politics_f1": 0.8238607270365126,
      "eval_crossner_politics_precision": 0.8225971370142939,
      "eval_crossner_politics_recall": 0.825128205128184,
      "eval_crossner_science_f1": 0.7813647786084122,
      "eval_crossner_science_precision": 0.7646613696556653,
      "eval_crossner_science_recall": 0.7988142292489803,
      "eval_mit-movie_f1": 0.8883686083036076,
      "eval_mit-movie_precision": 0.8883686083536076,
      "eval_mit-movie_recall": 0.8883686083536076,
      "eval_mit-restaurant_f1": 0.8040152963170878,
      "eval_mit-restaurant_precision": 0.8071017274471911,
      "eval_mit-restaurant_recall": 0.8009523809523555,
      "eval_runtime": 159.6362,
      "eval_samples_per_second": 40.53,
      "eval_steps_per_second": 0.163,
      "step": 370
    },
    {
      "epoch": 5.352112676056338,
      "grad_norm": 0.07851556664080372,
      "learning_rate": 2.303921568627451e-06,
      "loss": 0.0073,
      "step": 380
    },
    {
      "epoch": 5.352112676056338,
      "eval_average_f1": 0.7972743226631751,
      "eval_crossner_ai_f1": 0.7360199937019073,
      "eval_crossner_ai_precision": 0.7390213299874066,
      "eval_crossner_ai_recall": 0.7330429371499233,
      "eval_crossner_literature_f1": 0.7479140328197476,
      "eval_crossner_literature_precision": 0.7496198682209453,
      "eval_crossner_literature_recall": 0.7462159434913851,
      "eval_crossner_music_f1": 0.812078380932949,
      "eval_crossner_music_precision": 0.8149580915538099,
      "eval_crossner_music_recall": 0.8092189500639946,
      "eval_crossner_politics_f1": 0.8176583492781941,
      "eval_crossner_politics_precision": 0.8160919540229676,
      "eval_crossner_politics_recall": 0.8192307692307482,
      "eval_crossner_science_f1": 0.785728136463464,
      "eval_crossner_science_precision": 0.7712219261514742,
      "eval_crossner_science_recall": 0.8007905138339604,
      "eval_mit-movie_f1": 0.8869353782348428,
      "eval_mit-movie_precision": 0.8856929398580335,
      "eval_mit-movie_recall": 0.8881813073609124,
      "eval_mit-restaurant_f1": 0.7945859872111217,
      "eval_mit-restaurant_precision": 0.7971246006389522,
      "eval_mit-restaurant_recall": 0.7920634920634669,
      "eval_runtime": 125.744,
      "eval_samples_per_second": 51.454,
      "eval_steps_per_second": 0.207,
      "step": 380
    },
    {
      "epoch": 5.492957746478873,
      "grad_norm": 0.08859635573798719,
      "learning_rate": 1.8137254901960786e-06,
      "loss": 0.0067,
      "step": 390
    },
    {
      "epoch": 5.492957746478873,
      "eval_average_f1": 0.7996178122911741,
      "eval_crossner_ai_f1": 0.7431761786100041,
      "eval_crossner_ai_precision": 0.7408781694495522,
      "eval_crossner_ai_recall": 0.7454884878655417,
      "eval_crossner_literature_f1": 0.7510080644660913,
      "eval_crossner_literature_precision": 0.7502517623363167,
      "eval_crossner_literature_recall": 0.7517658930372981,
      "eval_crossner_music_f1": 0.8100048099547842,
      "eval_crossner_music_precision": 0.8114359139093861,
      "eval_crossner_music_recall": 0.8085787451984376,
      "eval_crossner_politics_f1": 0.8279734557947986,
      "eval_crossner_politics_precision": 0.8241869918698977,
      "eval_crossner_politics_recall": 0.8317948717948505,
      "eval_crossner_science_f1": 0.7816180729370594,
      "eval_crossner_science_precision": 0.7640619101547466,
      "eval_crossner_science_recall": 0.7999999999999684,
      "eval_mit-movie_f1": 0.8889716840036361,
      "eval_mit-movie_precision": 0.8841949231054126,
      "eval_mit-movie_recall": 0.8938003371417701,
      "eval_mit-restaurant_f1": 0.7945724202718449,
      "eval_mit-restaurant_precision": 0.7898368883312173,
      "eval_mit-restaurant_recall": 0.799365079365054,
      "eval_runtime": 144.3084,
      "eval_samples_per_second": 44.835,
      "eval_steps_per_second": 0.18,
      "step": 390
    },
    {
      "epoch": 5.633802816901408,
      "grad_norm": 0.0851726962315702,
      "learning_rate": 1.323529411764706e-06,
      "loss": 0.0074,
      "step": 400
    },
    {
      "epoch": 5.633802816901408,
      "eval_average_f1": 0.8005553172576868,
      "eval_crossner_ai_f1": 0.7371714642803685,
      "eval_crossner_ai_precision": 0.7413467589678576,
      "eval_crossner_ai_recall": 0.7330429371499233,
      "eval_crossner_literature_f1": 0.7529291899652489,
      "eval_crossner_literature_precision": 0.7602880658435822,
      "eval_crossner_literature_recall": 0.7457114026235748,
      "eval_crossner_music_f1": 0.818781154476426,
      "eval_crossner_music_precision": 0.8226171243941576,
      "eval_crossner_music_recall": 0.8149807938540072,
      "eval_crossner_politics_f1": 0.8283399768048074,
      "eval_crossner_politics_precision": 0.8307970079958517,
      "eval_crossner_politics_recall": 0.8258974358974147,
      "eval_crossner_science_f1": 0.7818288164834158,
      "eval_crossner_science_precision": 0.7714505579068576,
      "eval_crossner_science_recall": 0.7924901185770438,
      "eval_mit-movie_f1": 0.8905328213752105,
      "eval_mit-movie_precision": 0.8904494382022305,
      "eval_mit-movie_recall": 0.8906162202659508,
      "eval_mit-restaurant_f1": 0.7943037974183299,
      "eval_mit-restaurant_precision": 0.7917981072554955,
      "eval_mit-restaurant_recall": 0.7968253968253716,
      "eval_runtime": 141.751,
      "eval_samples_per_second": 45.643,
      "eval_steps_per_second": 0.183,
      "step": 400
    },
    {
      "epoch": 5.774647887323944,
      "grad_norm": 0.09374285710462266,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.0078,
      "step": 410
    },
    {
      "epoch": 5.774647887323944,
      "eval_average_f1": 0.7986570566328519,
      "eval_crossner_ai_f1": 0.7341614906331844,
      "eval_crossner_ai_precision": 0.7327960322380203,
      "eval_crossner_ai_recall": 0.7355320472930469,
      "eval_crossner_literature_f1": 0.7496190959377753,
      "eval_crossner_literature_precision": 0.7546012269938265,
      "eval_crossner_literature_recall": 0.7447023208879543,
      "eval_crossner_music_f1": 0.8136137421236779,
      "eval_crossner_music_precision": 0.8161030595812941,
      "eval_crossner_music_recall": 0.8111395646606654,
      "eval_crossner_politics_f1": 0.8232426885082139,
      "eval_crossner_politics_precision": 0.823665297741252,
      "eval_crossner_politics_recall": 0.8228205128204917,
      "eval_crossner_science_f1": 0.7820686977959009,
      "eval_crossner_science_precision": 0.76820434616848,
      "eval_crossner_science_recall": 0.7964426877470041,
      "eval_mit-movie_f1": 0.8877035372885573,
      "eval_mit-movie_precision": 0.8870394613801966,
      "eval_mit-movie_recall": 0.8883686083536076,
      "eval_mit-restaurant_f1": 0.8001901441426543,
      "eval_mit-restaurant_precision": 0.7987978487820058,
      "eval_mit-restaurant_recall": 0.8015873015872761,
      "eval_runtime": 183.0912,
      "eval_samples_per_second": 35.338,
      "eval_steps_per_second": 0.142,
      "step": 410
    },
    {
      "epoch": 5.915492957746479,
      "grad_norm": 0.08462872173043584,
      "learning_rate": 3.4313725490196084e-07,
      "loss": 0.0071,
      "step": 420
    },
    {
      "epoch": 5.915492957746479,
      "eval_average_f1": 0.8004085363818889,
      "eval_crossner_ai_f1": 0.7391710812837028,
      "eval_crossner_ai_precision": 0.7403245942571323,
      "eval_crossner_ai_recall": 0.7380211574361706,
      "eval_crossner_literature_f1": 0.7529232333002441,
      "eval_crossner_literature_precision": 0.7587090163934037,
      "eval_crossner_literature_recall": 0.7472250252270056,
      "eval_crossner_music_f1": 0.8120083479994206,
      "eval_crossner_music_precision": 0.8144927536231622,
      "eval_crossner_music_recall": 0.809539052496773,
      "eval_crossner_politics_f1": 0.8200514138317273,
      "eval_crossner_politics_precision": 0.822164948453587,
      "eval_crossner_politics_recall": 0.817948717948697,
      "eval_crossner_science_f1": 0.7883525502724299,
      "eval_crossner_science_precision": 0.779667568612262,
      "eval_crossner_science_recall": 0.7972332015809962,
      "eval_mit-movie_f1": 0.8885770417686756,
      "eval_mit-movie_precision": 0.8876635514018526,
      "eval_mit-movie_recall": 0.8894924143097792,
      "eval_mit-restaurant_f1": 0.8017760862170218,
      "eval_mit-restaurant_precision": 0.801013941698327,
      "eval_mit-restaurant_recall": 0.802539682539657,
      "eval_runtime": 126.4215,
      "eval_samples_per_second": 51.178,
      "eval_steps_per_second": 0.206,
      "step": 420
    },
    {
      "epoch": 6.0,
      "step": 426,
      "total_flos": 1.6771388427311514e+17,
      "train_loss": 0.03708959315759196,
      "train_runtime": 7943.8265,
      "train_samples_per_second": 13.697,
      "train_steps_per_second": 0.054
    }
  ],
  "logging_steps": 10,
  "max_steps": 426,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": false,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6771388427311514e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
