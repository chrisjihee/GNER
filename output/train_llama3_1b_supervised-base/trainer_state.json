{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 6.0,
  "eval_steps": 500,
  "global_step": 426,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.14084507042253522,
      "grad_norm": 2.8013940031801075,
      "learning_rate": 1.593279540393825e-05,
      "loss": 0.6483,
      "step": 10
    },
    {
      "epoch": 0.28169014084507044,
      "grad_norm": 0.6151579309837313,
      "learning_rate": 1.9950980392156866e-05,
      "loss": 0.1296,
      "step": 20
    },
    {
      "epoch": 0.4225352112676056,
      "grad_norm": 0.35113575784642753,
      "learning_rate": 1.946078431372549e-05,
      "loss": 0.0592,
      "step": 30
    },
    {
      "epoch": 0.5633802816901409,
      "grad_norm": 0.21220750312735157,
      "learning_rate": 1.897058823529412e-05,
      "loss": 0.0498,
      "step": 40
    },
    {
      "epoch": 0.704225352112676,
      "grad_norm": 0.24367285357367763,
      "learning_rate": 1.8480392156862748e-05,
      "loss": 0.043,
      "step": 50
    },
    {
      "epoch": 0.8450704225352113,
      "grad_norm": 0.19511426402241625,
      "learning_rate": 1.7990196078431373e-05,
      "loss": 0.042,
      "step": 60
    },
    {
      "epoch": 0.9859154929577465,
      "grad_norm": 0.1827895019606836,
      "learning_rate": 1.7500000000000002e-05,
      "loss": 0.0388,
      "step": 70
    },
    {
      "epoch": 1.0,
      "eval_average_f1": 0.71085446940181,
      "eval_crossner_ai_f1": 0.5702280911865231,
      "eval_crossner_ai_precision": 0.5507246376811274,
      "eval_crossner_ai_recall": 0.5911636589918736,
      "eval_crossner_literature_f1": 0.675865529302702,
      "eval_crossner_literature_precision": 0.6721556886227209,
      "eval_crossner_literature_recall": 0.6796165489404299,
      "eval_crossner_music_f1": 0.6690989847215564,
      "eval_crossner_music_precision": 0.6632075471697905,
      "eval_crossner_music_recall": 0.675096030729812,
      "eval_crossner_politics_f1": 0.7310379874992169,
      "eval_crossner_politics_precision": 0.7245026441702159,
      "eval_crossner_politics_recall": 0.7376923076922888,
      "eval_crossner_science_f1": 0.6795590794317219,
      "eval_crossner_science_precision": 0.665278303672826,
      "eval_crossner_science_recall": 0.6944664031620279,
      "eval_mit-movie_f1": 0.8681104191339257,
      "eval_mit-movie_precision": 0.8616236162361465,
      "eval_mit-movie_recall": 0.8746956358868538,
      "eval_mit-restaurant_f1": 0.7820811945370233,
      "eval_mit-restaurant_precision": 0.7666971637694185,
      "eval_mit-restaurant_recall": 0.7980952380952128,
      "eval_runtime": 315.9789,
      "eval_samples_per_second": 20.476,
      "eval_steps_per_second": 0.082,
      "step": 71
    },
    {
      "epoch": 1.1267605633802817,
      "grad_norm": 0.11168184888068486,
      "learning_rate": 1.7009803921568627e-05,
      "loss": 0.0322,
      "step": 80
    },
    {
      "epoch": 1.267605633802817,
      "grad_norm": 0.17928457695831485,
      "learning_rate": 1.6519607843137256e-05,
      "loss": 0.0295,
      "step": 90
    },
    {
      "epoch": 1.408450704225352,
      "grad_norm": 0.10572426362562602,
      "learning_rate": 1.6029411764705884e-05,
      "loss": 0.0296,
      "step": 100
    },
    {
      "epoch": 1.5492957746478875,
      "grad_norm": 0.1257725253544512,
      "learning_rate": 1.5539215686274513e-05,
      "loss": 0.028,
      "step": 110
    },
    {
      "epoch": 1.6901408450704225,
      "grad_norm": 0.17933078270717906,
      "learning_rate": 1.5049019607843138e-05,
      "loss": 0.0301,
      "step": 120
    },
    {
      "epoch": 1.8309859154929577,
      "grad_norm": 0.10642427271520971,
      "learning_rate": 1.4558823529411765e-05,
      "loss": 0.0283,
      "step": 130
    },
    {
      "epoch": 1.971830985915493,
      "grad_norm": 0.119490368091291,
      "learning_rate": 1.4068627450980394e-05,
      "loss": 0.0291,
      "step": 140
    },
    {
      "epoch": 2.0,
      "eval_average_f1": 0.771292878610191,
      "eval_crossner_ai_f1": 0.6931524547303896,
      "eval_crossner_ai_precision": 0.7206178643384338,
      "eval_crossner_ai_recall": 0.6677037958929267,
      "eval_crossner_literature_f1": 0.7132104852861061,
      "eval_crossner_literature_precision": 0.7343666488508426,
      "eval_crossner_literature_recall": 0.6932391523713071,
      "eval_crossner_music_f1": 0.7855619359631312,
      "eval_crossner_music_precision": 0.8057892965331267,
      "eval_crossner_music_recall": 0.7663252240716784,
      "eval_crossner_politics_f1": 0.7903544603532331,
      "eval_crossner_politics_precision": 0.8129574410409105,
      "eval_crossner_politics_recall": 0.7689743589743393,
      "eval_crossner_science_f1": 0.7515565374073038,
      "eval_crossner_science_precision": 0.7639853001224678,
      "eval_crossner_science_recall": 0.7395256916995755,
      "eval_mit-movie_f1": 0.8837339818038792,
      "eval_mit-movie_precision": 0.8826606875934065,
      "eval_mit-movie_recall": 0.8848098894923977,
      "eval_mit-restaurant_f1": 0.7814802947272942,
      "eval_mit-restaurant_precision": 0.7888098318240365,
      "eval_mit-restaurant_recall": 0.7742857142856897,
      "eval_runtime": 281.051,
      "eval_samples_per_second": 23.021,
      "eval_steps_per_second": 0.093,
      "step": 142
    },
    {
      "epoch": 2.112676056338028,
      "grad_norm": 0.09731028309111735,
      "learning_rate": 1.357843137254902e-05,
      "loss": 0.0235,
      "step": 150
    },
    {
      "epoch": 2.2535211267605635,
      "grad_norm": 0.09240489336945938,
      "learning_rate": 1.3088235294117648e-05,
      "loss": 0.0214,
      "step": 160
    },
    {
      "epoch": 2.3943661971830985,
      "grad_norm": 0.1768551026566422,
      "learning_rate": 1.2598039215686275e-05,
      "loss": 0.021,
      "step": 170
    },
    {
      "epoch": 2.535211267605634,
      "grad_norm": 0.09572205550918479,
      "learning_rate": 1.2107843137254901e-05,
      "loss": 0.0208,
      "step": 180
    },
    {
      "epoch": 2.676056338028169,
      "grad_norm": 0.13222723922669014,
      "learning_rate": 1.1617647058823532e-05,
      "loss": 0.0216,
      "step": 190
    },
    {
      "epoch": 2.816901408450704,
      "grad_norm": 0.1309494377625759,
      "learning_rate": 1.1127450980392159e-05,
      "loss": 0.0217,
      "step": 200
    },
    {
      "epoch": 2.9577464788732395,
      "grad_norm": 0.12253158393294837,
      "learning_rate": 1.0637254901960786e-05,
      "loss": 0.0207,
      "step": 210
    },
    {
      "epoch": 3.0,
      "eval_average_f1": 0.7910643081513559,
      "eval_crossner_ai_f1": 0.7298202862734622,
      "eval_crossner_ai_precision": 0.7147971360381435,
      "eval_crossner_ai_recall": 0.7454884878655417,
      "eval_crossner_literature_f1": 0.7445475055904759,
      "eval_crossner_literature_precision": 0.7399103139013085,
      "eval_crossner_literature_recall": 0.7492431886982467,
      "eval_crossner_music_f1": 0.7936354869316532,
      "eval_crossner_music_precision": 0.7969657843769917,
      "eval_crossner_music_recall": 0.7903329065300643,
      "eval_crossner_politics_f1": 0.8337576374245231,
      "eval_crossner_politics_precision": 0.8278564206268749,
      "eval_crossner_politics_recall": 0.8397435897435682,
      "eval_crossner_science_f1": 0.7465527286351675,
      "eval_crossner_science_precision": 0.7338678885070358,
      "eval_crossner_science_recall": 0.7596837944663731,
      "eval_mit-movie_f1": 0.8856664807084916,
      "eval_mit-movie_precision": 0.8791289905886532,
      "eval_mit-movie_recall": 0.892301929200208,
      "eval_mit-restaurant_f1": 0.8034700314957179,
      "eval_mit-restaurant_precision": 0.7984326018808527,
      "eval_mit-restaurant_recall": 0.8085714285714029,
      "eval_runtime": 178.1039,
      "eval_samples_per_second": 36.327,
      "eval_steps_per_second": 0.146,
      "step": 213
    },
    {
      "epoch": 3.0985915492957745,
      "grad_norm": 0.13522286868054606,
      "learning_rate": 1.0147058823529413e-05,
      "loss": 0.0172,
      "step": 220
    },
    {
      "epoch": 3.23943661971831,
      "grad_norm": 0.08497324394019688,
      "learning_rate": 9.65686274509804e-06,
      "loss": 0.0162,
      "step": 230
    },
    {
      "epoch": 3.380281690140845,
      "grad_norm": 0.13880870005908275,
      "learning_rate": 9.166666666666666e-06,
      "loss": 0.0155,
      "step": 240
    },
    {
      "epoch": 3.52112676056338,
      "grad_norm": 0.11692338426455218,
      "learning_rate": 8.676470588235295e-06,
      "loss": 0.0156,
      "step": 250
    },
    {
      "epoch": 3.6619718309859155,
      "grad_norm": 0.1410301187838851,
      "learning_rate": 8.186274509803922e-06,
      "loss": 0.0146,
      "step": 260
    },
    {
      "epoch": 3.802816901408451,
      "grad_norm": 0.11701609473911602,
      "learning_rate": 7.69607843137255e-06,
      "loss": 0.0154,
      "step": 270
    },
    {
      "epoch": 3.943661971830986,
      "grad_norm": 0.10192974798166073,
      "learning_rate": 7.205882352941177e-06,
      "loss": 0.0149,
      "step": 280
    },
    {
      "epoch": 4.0,
      "eval_average_f1": 0.7911522873728626,
      "eval_crossner_ai_f1": 0.7185741087679675,
      "eval_crossner_ai_precision": 0.722187303582607,
      "eval_crossner_ai_recall": 0.7149968886122766,
      "eval_crossner_literature_f1": 0.7333164171029714,
      "eval_crossner_literature_precision": 0.7376212353241073,
      "eval_crossner_literature_recall": 0.729061553985836,
      "eval_crossner_music_f1": 0.790477743305609,
      "eval_crossner_music_precision": 0.8055832502492255,
      "eval_crossner_music_recall": 0.7759282970550327,
      "eval_crossner_politics_f1": 0.8272632673797498,
      "eval_crossner_politics_precision": 0.8394931362196189,
      "eval_crossner_politics_recall": 0.8153846153845945,
      "eval_crossner_science_f1": 0.7759778743079306,
      "eval_crossner_science_precision": 0.7756714060031289,
      "eval_crossner_science_recall": 0.7762845849802065,
      "eval_mit-movie_f1": 0.8904779826372256,
      "eval_mit-movie_precision": 0.8946870864057308,
      "eval_mit-movie_recall": 0.8863082974339598,
      "eval_mit-restaurant_f1": 0.8019786181085843,
      "eval_mit-restaurant_precision": 0.8062239332691432,
      "eval_mit-restaurant_recall": 0.7977777777777525,
      "eval_runtime": 210.0991,
      "eval_samples_per_second": 30.795,
      "eval_steps_per_second": 0.124,
      "step": 284
    },
    {
      "epoch": 4.084507042253521,
      "grad_norm": 0.08713156830107896,
      "learning_rate": 6.715686274509804e-06,
      "loss": 0.0129,
      "step": 290
    },
    {
      "epoch": 4.225352112676056,
      "grad_norm": 0.09132675765411427,
      "learning_rate": 6.225490196078432e-06,
      "loss": 0.0109,
      "step": 300
    },
    {
      "epoch": 4.366197183098592,
      "grad_norm": 0.12503315068778392,
      "learning_rate": 5.735294117647059e-06,
      "loss": 0.0109,
      "step": 310
    },
    {
      "epoch": 4.507042253521127,
      "grad_norm": 0.12401790306178145,
      "learning_rate": 5.245098039215687e-06,
      "loss": 0.0114,
      "step": 320
    },
    {
      "epoch": 4.647887323943662,
      "grad_norm": 0.09102096234990799,
      "learning_rate": 4.754901960784314e-06,
      "loss": 0.0105,
      "step": 330
    },
    {
      "epoch": 4.788732394366197,
      "grad_norm": 0.10019970785691355,
      "learning_rate": 4.264705882352942e-06,
      "loss": 0.011,
      "step": 340
    },
    {
      "epoch": 4.929577464788732,
      "grad_norm": 0.10440122798783476,
      "learning_rate": 3.774509803921569e-06,
      "loss": 0.0109,
      "step": 350
    },
    {
      "epoch": 5.0,
      "eval_average_f1": 0.8043985767157746,
      "eval_crossner_ai_f1": 0.732900030900126,
      "eval_crossner_ai_precision": 0.729064039408822,
      "eval_crossner_ai_recall": 0.7367766023646087,
      "eval_crossner_literature_f1": 0.7618100447038263,
      "eval_crossner_literature_precision": 0.7509803921568259,
      "eval_crossner_literature_recall": 0.7729566094853293,
      "eval_crossner_music_f1": 0.8120493510155085,
      "eval_crossner_music_precision": 0.8129611806223672,
      "eval_crossner_music_recall": 0.8111395646606654,
      "eval_crossner_politics_f1": 0.8426966291634626,
      "eval_crossner_politics_precision": 0.8392675483214436,
      "eval_crossner_politics_recall": 0.8461538461538245,
      "eval_crossner_science_f1": 0.790181180546122,
      "eval_crossner_science_precision": 0.7791010372646646,
      "eval_crossner_science_recall": 0.8015810276679525,
      "eval_mit-movie_f1": 0.8854790418661512,
      "eval_mit-movie_precision": 0.8846513366984318,
      "eval_mit-movie_recall": 0.8863082974339598,
      "eval_mit-restaurant_f1": 0.8056737588152253,
      "eval_mit-restaurant_precision": 0.799999999999975,
      "eval_mit-restaurant_recall": 0.8114285714285456,
      "eval_runtime": 125.1428,
      "eval_samples_per_second": 51.701,
      "eval_steps_per_second": 0.208,
      "step": 355
    },
    {
      "epoch": 5.070422535211268,
      "grad_norm": 0.08484894946449151,
      "learning_rate": 3.2843137254901964e-06,
      "loss": 0.0093,
      "step": 360
    },
    {
      "epoch": 5.211267605633803,
      "grad_norm": 0.09437932937683212,
      "learning_rate": 2.7941176470588237e-06,
      "loss": 0.0078,
      "step": 370
    },
    {
      "epoch": 5.352112676056338,
      "grad_norm": 0.08721157651634841,
      "learning_rate": 2.303921568627451e-06,
      "loss": 0.0072,
      "step": 380
    },
    {
      "epoch": 5.492957746478873,
      "grad_norm": 0.09097755315297781,
      "learning_rate": 1.8137254901960786e-06,
      "loss": 0.0066,
      "step": 390
    },
    {
      "epoch": 5.633802816901408,
      "grad_norm": 0.12411929728770889,
      "learning_rate": 1.323529411764706e-06,
      "loss": 0.0072,
      "step": 400
    },
    {
      "epoch": 5.774647887323944,
      "grad_norm": 0.0829055520880966,
      "learning_rate": 8.333333333333333e-07,
      "loss": 0.0074,
      "step": 410
    },
    {
      "epoch": 5.915492957746479,
      "grad_norm": 0.07036863841003846,
      "learning_rate": 3.4313725490196084e-07,
      "loss": 0.007,
      "step": 420
    },
    {
      "epoch": 6.0,
      "eval_average_f1": 0.8000553013180688,
      "eval_crossner_ai_f1": 0.7253499221894572,
      "eval_crossner_ai_precision": 0.7251243781094077,
      "eval_crossner_ai_recall": 0.7255756067205522,
      "eval_crossner_literature_f1": 0.7554552294458253,
      "eval_crossner_literature_precision": 0.7511221945136782,
      "eval_crossner_literature_recall": 0.7598385469222624,
      "eval_crossner_music_f1": 0.8164516128532024,
      "eval_crossner_music_precision": 0.8228218465539394,
      "eval_crossner_music_recall": 0.81017925736233,
      "eval_crossner_politics_f1": 0.8296485622502988,
      "eval_crossner_politics_precision": 0.8270063694267306,
      "eval_crossner_politics_recall": 0.832307692307671,
      "eval_crossner_science_f1": 0.7806778339975062,
      "eval_crossner_science_precision": 0.7695852534561917,
      "eval_crossner_science_recall": 0.7920948616600477,
      "eval_mit-movie_f1": 0.8895745675049158,
      "eval_mit-movie_precision": 0.8881628080657041,
      "eval_mit-movie_recall": 0.8909908222513412,
      "eval_mit-restaurant_f1": 0.8032293809852764,
      "eval_mit-restaurant_precision": 0.8010735712030059,
      "eval_mit-restaurant_recall": 0.8053968253967998,
      "eval_runtime": 109.5693,
      "eval_samples_per_second": 59.049,
      "eval_steps_per_second": 0.237,
      "step": 426
    },
    {
      "epoch": 6.0,
      "step": 426,
      "total_flos": 1.6771388427311514e+17,
      "train_loss": 0.03787959897846963,
      "train_runtime": 1633.1348,
      "train_samples_per_second": 66.626,
      "train_steps_per_second": 0.261
    }
  ],
  "logging_steps": 10,
  "max_steps": 426,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 6,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": false,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6771388427311514e+17,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
