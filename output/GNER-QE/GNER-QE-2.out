(GNER) chrisjihee@dgx-a100:~/proj/GNER$ ./run-GNER-QE-2.sh
+ DEEPSPEED_CONFIG=configs/deepspeed/ds1_t5.json
++ shuf -i 25000-30000 -n 1
+ DEEPSPEED_PORT=28095
+ CUDA_DEVICES=2,3
+ SOURCE_FILE=run_glue.py
+ TRAIN_FILE=data/GNER-QE/ZSE-validation-pred-by_beam-num=20-train.json
+ VALID_FILE=data/GNER-QE/ZSE-validation-pred-by_beam-num=20-val.json
+ OUTPUT_NAME=GNER-QE
+ MODEL_NAMES=("google-bert/bert-base-cased" "FacebookAI/roberta-base" "FacebookAI/roberta-large")
+ for MODEL_NAME in "${MODEL_NAMES[@]}"
+ python -m deepspeed.launcher.runner --include=localhost:2,3 --master_port 28095 run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=20-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=20-val.json --model_name_or_path google-bert/bert-base-cased --output_dir output/GNER-QE/google-bert/bert-base-cased-num=20 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 13:42:51,218] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 13:42:54,347] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-03-10 13:42:54,348] [INFO] [runner.py:607:main] cmd = /raid/chrisjihee/miniforge3/envs/GNER/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMiwgM119 --master_addr=127.0.0.1 --master_port=28095 --enable_each_rank_log=None run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=20-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=20-val.json --model_name_or_path google-bert/bert-base-cased --output_dir output/GNER-QE/google-bert/bert-base-cased-num=20 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 13:42:59,213] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 13:43:02,225] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [2, 3]}
[2025-03-10 13:43:02,225] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-03-10 13:43:02,225] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-03-10 13:43:02,225] [INFO] [launch.py:164:main] dist_world_size=2
[2025-03-10 13:43:02,225] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=2,3
[2025-03-10 13:43:02,226] [INFO] [launch.py:256:main] process 4021198 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=0', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=20-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=20-val.json', '--model_name_or_path', 'google-bert/bert-base-cased', '--output_dir', 'output/GNER-QE/google-bert/bert-base-cased-num=20', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 13:43:02,227] [INFO] [launch.py:256:main] process 4021199 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=1', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=20-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=20-val.json', '--model_name_or_path', 'google-bert/bert-base-cased', '--output_dir', 'output/GNER-QE/google-bert/bert-base-cased-num=20', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 13:43:07,343] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 13:43:07,355] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 13:43:08,080] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-10 13:43:08,080] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-03-10 13:43:08,156] [INFO] [comm.py:658:init_distributed] cdb=None
03/10/2025 13:43:10 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 13:43:10 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/deepspeed/ds1_t5.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.EPOCH,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/GNER-QE/google-bert/bert-base-cased-num=20/runs/Mar10_13-43-07_dgx-a100,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=40.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=output/GNER-QE/google-bert/bert-base-cased-num=20,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output/GNER-QE/google-bert/bert-base-cased-num=20,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.EPOCH,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/10/2025 13:43:10 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 13:43:10 - INFO - __main__ - load a local file for train: data/GNER-QE/ZSE-validation-pred-by_beam-num=20-train.json
03/10/2025 13:43:10 - INFO - __main__ - load a local file for validation: data/GNER-QE/ZSE-validation-pred-by_beam-num=20-val.json
Using custom data configuration default-cc9ee36d8392f1d6
03/10/2025 13:43:10 - INFO - datasets.builder - Using custom data configuration default-cc9ee36d8392f1d6
Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
03/10/2025 13:43:10 - INFO - datasets.info - Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
03/10/2025 13:43:10 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from .cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 13:43:10 - INFO - datasets.info - Loading Dataset info from .cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
03/10/2025 13:43:10 - INFO - datasets.builder - Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 13:43:10 - INFO - datasets.info - Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
[INFO|configuration_utils.py:699] 2025-03-10 13:43:11,154 >> loading configuration file config.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json
[INFO|configuration_utils.py:771] 2025-03-10 13:43:11,156 >> Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-cased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|configuration_utils.py:699] 2025-03-10 13:43:11,374 >> loading configuration file config.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json
[INFO|configuration_utils.py:771] 2025-03-10 13:43:11,374 >> Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-cased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:11,375 >> loading file vocab.txt from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/vocab.txt
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:11,375 >> loading file tokenizer.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:11,375 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:11,375 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:11,375 >> loading file tokenizer_config.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 13:43:11,375 >> loading file chat_template.jinja from cache at None
[INFO|configuration_utils.py:699] 2025-03-10 13:43:11,375 >> loading configuration file config.json from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/config.json
[INFO|configuration_utils.py:771] 2025-03-10 13:43:11,376 >> Model config BertConfig {
  "_name_or_path": "google-bert/bert-base-cased",
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 28996
}

[INFO|modeling_utils.py:4128] 2025-03-10 13:43:11,442 >> loading weights file model.safetensors from cache at .cache/models--google-bert--bert-base-cased/snapshots/cd5ef92a9fb2f889e972770a36d4ed042daf221e/model.safetensors
[WARNING|modeling_utils.py:5103] 2025-03-10 13:43:11,473 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
[INFO|logging.py:344] 2025-03-10 13:43:11,481 >> A pretrained model of type `BertForSequenceClassification` contains parameters that have been renamed internally (a few are listed below but more are present in the model):
* `cls.predictions.transform.LayerNorm.beta` -> `bert.cls.predictions.transform.LayerNorm.bias`
* `cls.predictions.transform.LayerNorm.gamma` -> `bert.cls.predictions.transform.LayerNorm.weight`
If you are using a model from the Hub, consider submitting a PR to adjust these weights and help future users.
[INFO|modeling_utils.py:5091] 2025-03-10 13:43:11,485 >> Some weights of the model checkpoint at google-bert/bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']
- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:5103] 2025-03-10 13:43:11,485 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/26253 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-5a77fe8bf650459b.arrow
03/10/2025 13:43:11 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-5a77fe8bf650459b.arrow
Running tokenizer on dataset:  30%|████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                   | 8000/26253 [00:02<00:05, 3093.80 examples/s][rank1]:[W310 13:43:14.995670687 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26253/26253 [00:07<00:00, 3391.08 examples/s]
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                              | 0/1384 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b1b1d8e0426e1bc3.arrow
03/10/2025 13:43:19 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-b1b1d8e0426e1bc3.arrow
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1384/1384 [00:00<00:00, 2356.79 examples/s]
[rank0]:[W310 13:43:21.603782710 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
03/10/2025 13:43:22 - INFO - __main__ - Sample 20952 of the training set: {'sentence1': 'who(B-actor) starred(O) in(O) marley(B-title) and(I-title) me(I-title)', 'sentence2': 'who starred in marley and me', 'label': 2.71, 'idx': 20952, 'input_ids': [101, 1150, 113, 139, 118, 2811, 114, 4950, 113, 152, 114, 1107, 113, 152, 114, 12477, 12586, 113, 139, 118, 1641, 114, 1105, 113, 146, 118, 1641, 114, 1143, 113, 146, 118, 1641, 114, 102, 1150, 4950, 1107, 12477, 12586, 1105, 1143, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 13:43:22 - INFO - __main__ - Sample 3648 of the training set: {'sentence1': 'The(O) Spanish(B-language) edition(O) of(O) Campus(B-conference) Party(I-conference) has(O) been(O) held(O) at(O) the(O) Colegio(B-university) Miguel(I-university) Hernández(I-university),(O) Ceulaj(B-location),(O) and(O) the(O) Municipal(B-location) Sport(I-location) Arena(I-location) of(I-location) Benalmádena(I-location) in(O) Málaga(B-location),(O) Spain(B-country) ;(O) and(O) at(O) both(O) the(O) Valencia(B-location) County(I-location) Fair(I-location) and(O) the(O) City(B-location) of(I-location) Arts(I-location) and(I-location) Sciences(I-location) in(O) Valencia(B-location) over(O) the(O) past(O) 15(O) years(O).(O)', 'sentence2': 'The Spanish edition of Campus Party has been held at the Colegio Miguel Hernández , Ceulaj , and the Municipal Sport Arena of Benalmádena in Málaga , Spain ; and at both the Valencia County Fair and the City of Arts and Sciences in Valencia over the past 15 years .', 'label': 3.32, 'idx': 3648, 'input_ids': [101, 1109, 113, 152, 114, 2124, 113, 139, 118, 1846, 114, 2596, 113, 152, 114, 1104, 113, 152, 114, 8838, 113, 139, 118, 3511, 114, 1786, 113, 146, 118, 3511, 114, 1144, 113, 152, 114, 1151, 113, 152, 114, 1316, 113, 152, 114, 1120, 113, 152, 114, 1103, 113, 152, 114, 4922, 10712, 113, 139, 118, 2755, 114, 7975, 113, 146, 118, 2755, 114, 22844, 113, 146, 118, 2755, 114, 117, 113, 152, 114, 24664, 5886, 3361, 113, 139, 118, 2450, 114, 117, 113, 152, 114, 1105, 113, 152, 114, 1103, 113, 152, 114, 7360, 113, 139, 118, 2450, 114, 7331, 113, 146, 118, 2450, 114, 6492, 113, 146, 118, 2450, 114, 1104, 113, 146, 118, 2450, 114, 3096, 1348, 1306, 5589, 2883, 1161, 113, 146, 118, 2450, 114, 1107, 113, 152, 114, 150, 5589, 18974, 1161, 113, 139, 118, 2450, 114, 117, 113, 152, 114, 2722, 113, 139, 118, 1583, 114, 132, 113, 152, 114, 1105, 113, 152, 114, 1120, 113, 152, 114, 1241, 113, 152, 114, 1103, 113, 152, 114, 13697, 113, 139, 118, 2450, 114, 1391, 113, 146, 118, 2450, 114, 6632, 113, 146, 118, 2450, 114, 1105, 113, 152, 114, 1103, 113, 152, 114, 1392, 113, 139, 118, 2450, 114, 1104, 113, 146, 118, 2450, 114, 2334, 113, 146, 118, 2450, 114, 1105, 113, 146, 118, 2450, 114, 4052, 113, 146, 118, 2450, 114, 1107, 113, 152, 114, 13697, 113, 139, 118, 2450, 114, 1166, 113, 152, 114, 1103, 113, 152, 114, 1763, 113, 152, 114, 1405, 113, 152, 114, 1201, 113, 152, 114, 119, 113, 152, 114, 102, 1109, 2124, 2596, 1104, 8838, 1786, 1144, 1151, 1316, 1120, 1103, 4922, 10712, 7975, 22844, 117, 24664, 5886, 3361, 117, 1105, 1103, 7360, 7331, 6492, 1104, 3096, 1348, 1306, 5589, 2883, 1161, 1107, 150, 5589, 18974, 1161, 117, 2722, 132, 1105, 1120, 1241, 1103, 13697, 1391, 6632, 1105, 1103, 1392, 1104, 2334, 1105, 4052, 1107, 13697, 1166, 1103, 1763, 1405, 1201, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 13:43:22 - INFO - __main__ - Sample 819 of the training set: {'sentence1': 'Two(O) professors(O),(O) Hal(B-person) Abelson(I-person) and(O) Gerald(B-person) Jay(I-person) Sussman(I-person),(O) chose(O) to(O) remain(O) neutral(O) -(O) their(O) group(O) was(O) referred(O) to(O) variously(O) as(O) Switzerland(B-country) and(O) Project(B-event) MAC(I-event) for(O) the(O) next(O) 30(O) years(O).(O)', 'sentence2': 'Two professors , Hal Abelson and Gerald Jay Sussman , chose to remain neutral - their group was referred to variously as Switzerland and Project MAC for the next 30 years .', 'label': 1.16, 'idx': 819, 'input_ids': [101, 1960, 113, 152, 114, 14427, 113, 152, 114, 117, 113, 152, 114, 12193, 113, 139, 118, 1825, 114, 17931, 2142, 113, 146, 118, 1825, 114, 1105, 113, 152, 114, 9532, 113, 139, 118, 1825, 114, 5525, 113, 146, 118, 1825, 114, 15463, 3954, 1399, 113, 146, 118, 1825, 114, 117, 113, 152, 114, 4102, 113, 152, 114, 1106, 113, 152, 114, 3118, 113, 152, 114, 8795, 113, 152, 114, 118, 113, 152, 114, 1147, 113, 152, 114, 1372, 113, 152, 114, 1108, 113, 152, 114, 2752, 113, 152, 114, 1106, 113, 152, 114, 18995, 113, 152, 114, 1112, 113, 152, 114, 4507, 113, 139, 118, 1583, 114, 1105, 113, 152, 114, 4042, 113, 139, 118, 1856, 114, 25424, 113, 146, 118, 1856, 114, 1111, 113, 152, 114, 1103, 113, 152, 114, 1397, 113, 152, 114, 1476, 113, 152, 114, 1201, 113, 152, 114, 119, 113, 152, 114, 102, 1960, 14427, 117, 12193, 17931, 2142, 1105, 9532, 5525, 15463, 3954, 1399, 117, 4102, 1106, 3118, 8795, 118, 1147, 1372, 1108, 2752, 1106, 18995, 1112, 4507, 1105, 4042, 25424, 1111, 1103, 1397, 1476, 1201, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/26253 [00:00<?, ? examples/s][INFO|trainer.py:748] 2025-03-10 13:43:22,557 >> Using auto half precision backend
[INFO|trainer.py:930] 2025-03-10 13:43:22,727 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[2025-03-10 13:43:22,732] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
Running tokenizer on dataset:   4%|███████████▌                                                                                                                                                                                                                                                                                                    | 1000/26253 [00:00<00:09, 2658.42 examples/s][2025-03-10 13:43:23,819] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26253/26253 [00:08<00:00, 2919.45 examples/s]
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1384/1384 [00:00<00:00, 2233.83 examples/s]
[2025-03-10 13:43:33,124] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-03-10 13:43:33,126] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-10 13:43:33,126] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-03-10 13:43:33,131] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-10 13:43:33,131] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-10 13:43:33,131] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer
[2025-03-10 13:43:33,131] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-03-10 13:43:33,131] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-03-10 13:43:33,131] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-03-10 13:43:33,131] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-03-10 13:43:33,365] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 13:43:33,664] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-03-10 13:43:33,664] [INFO] [utils.py:782:see_memory_usage] MA 0.4 GB         Max_MA 0.51 GB         CA 0.51 GB         Max_CA 1 GB
[2025-03-10 13:43:33,665] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.09 GB, percent = 4.8%
[2025-03-10 13:43:33,816] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-03-10 13:43:33,816] [INFO] [utils.py:782:see_memory_usage] MA 0.4 GB         Max_MA 0.61 GB         CA 0.71 GB         Max_CA 1 GB
[2025-03-10 13:43:33,817] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.1 GB, percent = 4.8%
[2025-03-10 13:43:33,817] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized
[2025-03-10 13:43:33,965] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-03-10 13:43:33,966] [INFO] [utils.py:782:see_memory_usage] MA 0.4 GB         Max_MA 0.4 GB         CA 0.71 GB         Max_CA 1 GB
[2025-03-10 13:43:33,966] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.1 GB, percent = 4.8%
[2025-03-10 13:43:33,968] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-03-10 13:43:33,968] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 13:43:33,968] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2025-03-10 13:43:33,968] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7ff641b20380>
[2025-03-10 13:43:33,968] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0], mom=[[0.9, 0.999]]
[2025-03-10 13:43:33,969] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-03-10 13:43:33,969] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-10 13:43:33,969] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-03-10 13:43:33,969] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-03-10 13:43:33,969] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-03-10 13:43:33,969] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-10 13:43:33,969] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True
[2025-03-10 13:43:33,969] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-03-10 13:43:33,969] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-03-10 13:43:33,969] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7ff59c108fe0>
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-03-10 13:43:33,970] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 4
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   loss_scale ................... 1.0
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-03-10 13:43:33,971] [INFO] [config.py:1005:print]   optimizer_name ............... adamw
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   optimizer_params ............. {'lr': 2e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   scheduler_name ............... WarmupLR
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 0}
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   steps_per_print .............. inf
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   train_batch_size ............. 32
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  4
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   world_size ................... 2
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2025-03-10 13:43:33,972] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-10 13:43:33,973] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 1
[2025-03-10 13:43:33,973] [INFO] [config.py:991:print_user_config]   json = {
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-05,
            "betas": [0.9, 0.999],
            "eps": 1e-08,
            "weight_decay": 0.0
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-05,
            "warmup_num_steps": 0
        }
    },
    "zero_optimization": {
        "stage": 1,
        "allgather_partitions": true,
        "allgather_bucket_size": 2.000000e+08,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2.000000e+08,
        "contiguous_gradients": true
    },
    "gradient_accumulation_steps": 4,
    "gradient_clipping": 1.0,
    "train_batch_size": 32,
    "train_micro_batch_size_per_gpu": 4,
    "steps_per_print": inf
}
[INFO|trainer.py:2414] 2025-03-10 13:43:33,974 >> ***** Running training *****
[INFO|trainer.py:2415] 2025-03-10 13:43:33,974 >>   Num examples = 26,253
[INFO|trainer.py:2416] 2025-03-10 13:43:33,974 >>   Num Epochs = 40
[INFO|trainer.py:2417] 2025-03-10 13:43:33,974 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2420] 2025-03-10 13:43:33,974 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2421] 2025-03-10 13:43:33,974 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2422] 2025-03-10 13:43:33,974 >>   Total optimization steps = 32,800
[INFO|trainer.py:2423] 2025-03-10 13:43:33,975 >>   Number of trainable parameters = 108,311,041
{'loss': 1.1688, 'grad_norm': 9.377415657043457, 'learning_rate': 2e-05, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                             | 820/32800 [01:44<1:04:39,  8.24it/s][INFO|trainer.py:930] 2025-03-10 13:45:18,305 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:45:18,309 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:45:18,309 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 13:45:18,309 >>   Batch size = 4
{'eval_loss': 1.3198121786117554, 'eval_mse': 1.3200177674348643, 'eval_pearson': 0.6371493132963625, 'eval_spearmanr': 0.6512369350522079, 'eval_runtime': 1.7598, 'eval_samples_per_second': 786.468, 'eval_steps_per_second': 98.308, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                             | 820/32800 [01:46<1:04:39,  8.24it/s]
[INFO|trainer.py:3955] 2025-03-10 13:45:20,214 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-820
[INFO|configuration_utils.py:423] 2025-03-10 13:45:20,217 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-820/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:45:20,439 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-820/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:45:20,440 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-820/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:45:20,440 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-820/special_tokens_map.json
[2025-03-10 13:45:20,464] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step820 is about to be saved!
[2025-03-10 13:45:20,468] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-820/global_step820/mp_rank_00_model_states.pt
[2025-03-10 13:45:20,468] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-820/global_step820/mp_rank_00_model_states.pt...
[2025-03-10 13:45:20,765] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-820/global_step820/mp_rank_00_model_states.pt.
[2025-03-10 13:45:20,766] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-820/global_step820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:45:21,747] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-820/global_step820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:45:21,748] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-820/global_step820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:45:21,748] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step820 is ready now!
{'loss': 0.7354, 'grad_norm': 7.238852024078369, 'learning_rate': 2e-05, 'epoch': 2.0}
  5%|█████████████████▏                                                                                                                                                                                                                                                                                                                                     | 1640/32800 [03:24<59:47,  8.68it/s][INFO|trainer.py:930] 2025-03-10 13:46:58,742 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:46:58,748 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:46:58,748 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 13:46:58,748 >>   Batch size = 4
{'eval_loss': 1.2887781858444214, 'eval_mse': 1.2885388056666864, 'eval_pearson': 0.6585922092860724, 'eval_spearmanr': 0.6557462738380528, 'eval_runtime': 1.8134, 'eval_samples_per_second': 763.187, 'eval_steps_per_second': 95.398, 'epoch': 2.0}
  5%|█████████████████▏                                                                                                                                                                                                                                                                                                                                     | 1640/32800 [03:26<59:47,  8.68it/s]
[INFO|trainer.py:3955] 2025-03-10 13:47:00,657 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-1640
[INFO|configuration_utils.py:423] 2025-03-10 13:47:00,659 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-1640/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:47:00,880 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-1640/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:47:00,881 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-1640/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:47:00,881 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-1640/special_tokens_map.json
[2025-03-10 13:47:00,906] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1640 is about to be saved!
[2025-03-10 13:47:00,909] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-1640/global_step1640/mp_rank_00_model_states.pt
[2025-03-10 13:47:00,909] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-1640/global_step1640/mp_rank_00_model_states.pt...
[2025-03-10 13:47:01,210] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-1640/global_step1640/mp_rank_00_model_states.pt.
[2025-03-10 13:47:01,212] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-1640/global_step1640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:47:02,183] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-1640/global_step1640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:47:02,184] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-1640/global_step1640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:47:02,184] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1640 is ready now!
{'loss': 0.6149, 'grad_norm': 5.80397367477417, 'learning_rate': 2e-05, 'epoch': 3.0}
  8%|█████████████████████████▋                                                                                                                                                                                                                                                                                                                             | 2460/32800 [05:02<58:19,  8.67it/s][INFO|trainer.py:930] 2025-03-10 13:48:36,800 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:48:36,803 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:48:36,803 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 13:48:36,803 >>   Batch size = 4
{'eval_loss': 1.2490426301956177, 'eval_mse': 1.2487795859403004, 'eval_pearson': 0.6490758690891181, 'eval_spearmanr': 0.6436819295553735, 'eval_runtime': 1.7552, 'eval_samples_per_second': 788.504, 'eval_steps_per_second': 98.563, 'epoch': 3.0}
  8%|█████████████████████████▋                                                                                                                                                                                                                                                                                                                             | 2460/32800 [05:04<58:19,  8.67it/s]
[INFO|trainer.py:3955] 2025-03-10 13:48:38,705 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-2460
[INFO|configuration_utils.py:423] 2025-03-10 13:48:38,707 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-2460/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:48:38,933 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-2460/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:48:38,933 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-2460/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:48:38,934 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-2460/special_tokens_map.json
[2025-03-10 13:48:38,957] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2460 is about to be saved!
[2025-03-10 13:48:38,960] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-2460/global_step2460/mp_rank_00_model_states.pt
[2025-03-10 13:48:38,960] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-2460/global_step2460/mp_rank_00_model_states.pt...
[2025-03-10 13:48:39,267] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-2460/global_step2460/mp_rank_00_model_states.pt.
[2025-03-10 13:48:39,268] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-2460/global_step2460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:48:40,254] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-2460/global_step2460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:48:40,255] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-2460/global_step2460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:48:40,255] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2460 is ready now!
{'loss': 0.5715, 'grad_norm': 3.573800563812256, 'learning_rate': 2e-05, 'epoch': 4.0}
 10%|██████████████████████████████████▎                                                                                                                                                                                                                                                                                                                    | 3280/32800 [06:40<56:28,  8.71it/s][INFO|trainer.py:930] 2025-03-10 13:50:14,336 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:50:14,340 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:50:14,340 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 13:50:14,340 >>   Batch size = 4
{'eval_loss': 1.4837782382965088, 'eval_mse': 1.4834629355827509, 'eval_pearson': 0.6138297362487826, 'eval_spearmanr': 0.6055043540888735, 'eval_runtime': 1.76, 'eval_samples_per_second': 786.349, 'eval_steps_per_second': 98.294, 'epoch': 4.0}
 10%|██████████████████████████████████▎                                                                                                                                                                                                                                                                                                                    | 3280/32800 [06:42<56:28,  8.71it/s]
[INFO|trainer.py:3955] 2025-03-10 13:50:16,245 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-3280
[INFO|configuration_utils.py:423] 2025-03-10 13:50:16,247 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-3280/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:50:16,464 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-3280/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:50:16,465 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-3280/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:50:16,465 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-3280/special_tokens_map.json
[2025-03-10 13:50:16,482] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3281 is about to be saved!
[2025-03-10 13:50:16,485] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-3280/global_step3281/mp_rank_00_model_states.pt
[2025-03-10 13:50:16,485] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-3280/global_step3281/mp_rank_00_model_states.pt...
[2025-03-10 13:50:16,778] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-3280/global_step3281/mp_rank_00_model_states.pt.
[2025-03-10 13:50:16,780] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-3280/global_step3281/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:50:17,752] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-3280/global_step3281/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:50:17,753] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-3280/global_step3281/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:50:17,753] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3281 is ready now!
{'loss': 0.439, 'grad_norm': 9.938470840454102, 'learning_rate': 2e-05, 'epoch': 5.0}
 12%|██████████████████████████████████████████▉                                                                                                                                                                                                                                                                                                            | 4100/32800 [08:19<55:21,  8.64it/s][INFO|trainer.py:930] 2025-03-10 13:51:52,997 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:51:53,000 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:51:53,000 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 13:51:53,000 >>   Batch size = 4
{'eval_loss': 1.2971479892730713, 'eval_mse': 1.2970617983727097, 'eval_pearson': 0.6335809058451306, 'eval_spearmanr': 0.6336266065958699, 'eval_runtime': 1.7626, 'eval_samples_per_second': 785.202, 'eval_steps_per_second': 98.15, 'epoch': 5.0}
 12%|██████████████████████████████████████████▉                                                                                                                                                                                                                                                                                                            | 4100/32800 [08:20<55:21,  8.64it/s]
[INFO|trainer.py:3955] 2025-03-10 13:51:54,907 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4100
[INFO|configuration_utils.py:423] 2025-03-10 13:51:54,909 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4100/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:51:55,123 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4100/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:51:55,123 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:51:55,123 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4100/special_tokens_map.json
[2025-03-10 13:51:55,146] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4101 is about to be saved!
[2025-03-10 13:51:55,149] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4100/global_step4101/mp_rank_00_model_states.pt
[2025-03-10 13:51:55,149] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4100/global_step4101/mp_rank_00_model_states.pt...
[2025-03-10 13:51:55,445] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4100/global_step4101/mp_rank_00_model_states.pt.
[2025-03-10 13:51:55,446] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4100/global_step4101/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:51:56,380] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4100/global_step4101/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:51:56,381] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4100/global_step4101/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:51:56,381] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4101 is ready now!
{'loss': 0.347, 'grad_norm': 12.858927726745605, 'learning_rate': 2e-05, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                                   | 4920/32800 [09:56<53:35,  8.67it/s][INFO|trainer.py:930] 2025-03-10 13:53:30,836 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:53:30,839 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:53:30,840 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 13:53:30,840 >>   Batch size = 4
{'eval_loss': 1.2451918125152588, 'eval_mse': 1.24501676469869, 'eval_pearson': 0.6741758384192998, 'eval_spearmanr': 0.6749736171626147, 'eval_runtime': 1.7606, 'eval_samples_per_second': 786.096, 'eval_steps_per_second': 98.262, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                                   | 4920/32800 [09:58<53:35,  8.67it/s]
[INFO|trainer.py:3955] 2025-03-10 13:53:32,744 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4920
[INFO|configuration_utils.py:423] 2025-03-10 13:53:32,746 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4920/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:53:32,957 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4920/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:53:32,958 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4920/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:53:32,958 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4920/special_tokens_map.json
[2025-03-10 13:53:32,982] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4921 is about to be saved!
[2025-03-10 13:53:32,985] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4920/global_step4921/mp_rank_00_model_states.pt
[2025-03-10 13:53:32,985] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4920/global_step4921/mp_rank_00_model_states.pt...
[2025-03-10 13:53:33,278] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4920/global_step4921/mp_rank_00_model_states.pt.
[2025-03-10 13:53:33,280] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4920/global_step4921/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:53:34,240] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4920/global_step4921/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:53:34,240] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-4920/global_step4921/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:53:34,241] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4921 is ready now!
{'loss': 0.3063, 'grad_norm': 5.985681056976318, 'learning_rate': 2e-05, 'epoch': 7.0}
 18%|████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                           | 5740/32800 [11:35<53:19,  8.46it/s][INFO|trainer.py:930] 2025-03-10 13:55:09,669 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:55:09,672 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:55:09,672 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 13:55:09,672 >>   Batch size = 4
{'eval_loss': 1.383831262588501, 'eval_mse': 1.383692003054426, 'eval_pearson': 0.6592594090576565, 'eval_spearmanr': 0.6699867071742623, 'eval_runtime': 1.7567, 'eval_samples_per_second': 787.858, 'eval_steps_per_second': 98.482, 'epoch': 7.0}
 18%|████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                           | 5740/32800 [11:37<53:19,  8.46it/s]
[INFO|trainer.py:3955] 2025-03-10 13:55:11,573 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-5740
[INFO|configuration_utils.py:423] 2025-03-10 13:55:11,575 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-5740/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:55:11,786 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-5740/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:55:11,787 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-5740/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:55:11,787 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-5740/special_tokens_map.json
[2025-03-10 13:55:11,810] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step5741 is about to be saved!
[2025-03-10 13:55:11,813] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-5740/global_step5741/mp_rank_00_model_states.pt
[2025-03-10 13:55:11,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-5740/global_step5741/mp_rank_00_model_states.pt...
[2025-03-10 13:55:12,105] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-5740/global_step5741/mp_rank_00_model_states.pt.
[2025-03-10 13:55:12,107] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-5740/global_step5741/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:55:13,066] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-5740/global_step5741/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:55:13,066] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-5740/global_step5741/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:55:13,066] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5741 is ready now!
{'loss': 0.3088, 'grad_norm': 5.18312406539917, 'learning_rate': 2e-05, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                  | 6560/32800 [13:15<50:05,  8.73it/s][INFO|trainer.py:930] 2025-03-10 13:56:49,126 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:56:49,130 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:56:49,130 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 13:56:49,130 >>   Batch size = 4
{'eval_loss': 1.1937668323516846, 'eval_mse': 1.1932968719501715, 'eval_pearson': 0.6806027305373776, 'eval_spearmanr': 0.6913685864286536, 'eval_runtime': 1.7643, 'eval_samples_per_second': 784.448, 'eval_steps_per_second': 98.056, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                  | 6560/32800 [13:16<50:05,  8.73it/s]
[INFO|trainer.py:3955] 2025-03-10 13:56:51,038 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-6560
[INFO|configuration_utils.py:423] 2025-03-10 13:56:51,040 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-6560/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:56:51,254 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-6560/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:56:51,255 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-6560/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:56:51,255 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-6560/special_tokens_map.json
[2025-03-10 13:56:51,278] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6562 is about to be saved!
[2025-03-10 13:56:51,281] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-6560/global_step6562/mp_rank_00_model_states.pt
[2025-03-10 13:56:51,281] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-6560/global_step6562/mp_rank_00_model_states.pt...
[2025-03-10 13:56:51,574] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-6560/global_step6562/mp_rank_00_model_states.pt.
[2025-03-10 13:56:51,576] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-6560/global_step6562/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:56:52,536] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-6560/global_step6562/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:56:52,537] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-6560/global_step6562/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:56:52,537] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6562 is ready now!
{'loss': 0.2337, 'grad_norm': 4.756198883056641, 'learning_rate': 2e-05, 'epoch': 9.0}
 22%|█████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                         | 7380/32800 [14:55<48:54,  8.66it/s][INFO|trainer.py:930] 2025-03-10 13:58:29,891 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 13:58:29,894 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 13:58:29,894 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 13:58:29,894 >>   Batch size = 4
{'eval_loss': 1.1952263116836548, 'eval_mse': 1.1949260555250796, 'eval_pearson': 0.6916094530125998, 'eval_spearmanr': 0.7044780620280879, 'eval_runtime': 1.7639, 'eval_samples_per_second': 784.624, 'eval_steps_per_second': 98.078, 'epoch': 9.0}
 22%|█████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                         | 7380/32800 [14:57<48:54,  8.66it/s]
[INFO|trainer.py:3955] 2025-03-10 13:58:31,802 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-7380
[INFO|configuration_utils.py:423] 2025-03-10 13:58:31,804 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-7380/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 13:58:32,017 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-7380/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 13:58:32,017 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-7380/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 13:58:32,018 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-7380/special_tokens_map.json
[2025-03-10 13:58:32,040] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7382 is about to be saved!
[2025-03-10 13:58:32,044] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-7380/global_step7382/mp_rank_00_model_states.pt
[2025-03-10 13:58:32,044] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-7380/global_step7382/mp_rank_00_model_states.pt...
[2025-03-10 13:58:32,336] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-7380/global_step7382/mp_rank_00_model_states.pt.
[2025-03-10 13:58:32,337] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-7380/global_step7382/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 13:58:33,272] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-7380/global_step7382/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 13:58:33,273] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-7380/global_step7382/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 13:58:33,273] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7382 is ready now!
{'loss': 0.1825, 'grad_norm': 3.9361214637756348, 'learning_rate': 2e-05, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                                 | 8200/32800 [16:33<47:10,  8.69it/s][INFO|trainer.py:930] 2025-03-10 14:00:07,931 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:00:07,934 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:00:07,934 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:00:07,934 >>   Batch size = 4
{'eval_loss': 1.037643551826477, 'eval_mse': 1.0371949621018646, 'eval_pearson': 0.7269802801368219, 'eval_spearmanr': 0.7393196813131132, 'eval_runtime': 1.7652, 'eval_samples_per_second': 784.044, 'eval_steps_per_second': 98.006, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                                 | 8200/32800 [16:35<47:10,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 14:00:09,841 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-8200
[INFO|configuration_utils.py:423] 2025-03-10 14:00:09,843 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-8200/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:00:10,054 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-8200/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:00:10,055 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-8200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:00:10,055 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-8200/special_tokens_map.json
[2025-03-10 14:00:10,078] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8202 is about to be saved!
[2025-03-10 14:00:10,081] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-8200/global_step8202/mp_rank_00_model_states.pt
[2025-03-10 14:00:10,081] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-8200/global_step8202/mp_rank_00_model_states.pt...
[2025-03-10 14:00:10,375] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-8200/global_step8202/mp_rank_00_model_states.pt.
[2025-03-10 14:00:10,376] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-8200/global_step8202/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:00:11,335] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-8200/global_step8202/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:00:11,336] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-8200/global_step8202/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:00:11,336] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8202 is ready now!
{'loss': 0.1691, 'grad_norm': 1.5098987817764282, 'learning_rate': 2e-05, 'epoch': 11.0}
 28%|██████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                                                        | 9020/32800 [18:13<47:22,  8.37it/s][INFO|trainer.py:930] 2025-03-10 14:01:47,246 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:01:47,249 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:01:47,249 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:01:47,249 >>   Batch size = 4
{'eval_loss': 1.1525521278381348, 'eval_mse': 1.1519551728502175, 'eval_pearson': 0.6960144901425218, 'eval_spearmanr': 0.7084618219613992, 'eval_runtime': 1.7607, 'eval_samples_per_second': 786.041, 'eval_steps_per_second': 98.255, 'epoch': 11.0}
 28%|██████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                                                        | 9020/32800 [18:15<47:22,  8.37it/s]
[INFO|trainer.py:3955] 2025-03-10 14:01:49,152 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9020
[INFO|configuration_utils.py:423] 2025-03-10 14:01:49,154 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9020/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:01:49,365 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9020/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:01:49,366 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9020/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:01:49,366 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9020/special_tokens_map.json
[2025-03-10 14:01:49,389] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9022 is about to be saved!
[2025-03-10 14:01:49,393] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9020/global_step9022/mp_rank_00_model_states.pt
[2025-03-10 14:01:49,393] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9020/global_step9022/mp_rank_00_model_states.pt...
[2025-03-10 14:01:49,686] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9020/global_step9022/mp_rank_00_model_states.pt.
[2025-03-10 14:01:49,687] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9020/global_step9022/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:01:50,638] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9020/global_step9022/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:01:50,639] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9020/global_step9022/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:01:50,639] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9022 is ready now!
{'loss': 0.1806, 'grad_norm': 2.4735183715820312, 'learning_rate': 2e-05, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                                                | 9840/32800 [19:51<43:47,  8.74it/s][INFO|trainer.py:930] 2025-03-10 14:03:25,812 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:03:25,816 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:03:25,816 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:03:25,816 >>   Batch size = 4
{'eval_loss': 1.277835726737976, 'eval_mse': 1.2773508498434387, 'eval_pearson': 0.6566543878525342, 'eval_spearmanr': 0.6801870226372072, 'eval_runtime': 1.7624, 'eval_samples_per_second': 785.273, 'eval_steps_per_second': 98.159, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                                                | 9840/32800 [19:53<43:47,  8.74it/s]
[INFO|trainer.py:3955] 2025-03-10 14:03:27,721 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9840
[INFO|configuration_utils.py:423] 2025-03-10 14:03:27,723 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9840/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:03:27,967 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9840/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:03:27,967 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9840/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:03:27,967 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9840/special_tokens_map.json
[2025-03-10 14:03:27,989] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9843 is about to be saved!
[2025-03-10 14:03:27,992] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9840/global_step9843/mp_rank_00_model_states.pt
[2025-03-10 14:03:27,992] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9840/global_step9843/mp_rank_00_model_states.pt...
[2025-03-10 14:03:28,287] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9840/global_step9843/mp_rank_00_model_states.pt.
[2025-03-10 14:03:28,288] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9840/global_step9843/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:03:29,245] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9840/global_step9843/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:03:29,246] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-9840/global_step9843/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:03:29,246] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9843 is ready now!
{'loss': 0.1373, 'grad_norm': 4.9623942375183105, 'learning_rate': 2e-05, 'epoch': 13.0}
 32%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                      | 10660/32800 [21:31<42:43,  8.64it/s][INFO|trainer.py:930] 2025-03-10 14:05:05,667 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:05:05,670 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:05:05,671 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:05:05,671 >>   Batch size = 4
{'eval_loss': 1.2193208932876587, 'eval_mse': 1.2190494051558434, 'eval_pearson': 0.6950696260067736, 'eval_spearmanr': 0.7173797773152742, 'eval_runtime': 1.7623, 'eval_samples_per_second': 785.343, 'eval_steps_per_second': 98.168, 'epoch': 13.0}
 32%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                      | 10660/32800 [21:33<42:43,  8.64it/s]
[INFO|trainer.py:3955] 2025-03-10 14:05:07,573 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-10660
[INFO|configuration_utils.py:423] 2025-03-10 14:05:07,575 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-10660/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:05:07,789 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-10660/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:05:07,790 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-10660/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:05:07,790 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-10660/special_tokens_map.json
[2025-03-10 14:05:07,812] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step10663 is about to be saved!
[2025-03-10 14:05:07,816] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-10660/global_step10663/mp_rank_00_model_states.pt
[2025-03-10 14:05:07,816] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-10660/global_step10663/mp_rank_00_model_states.pt...
[2025-03-10 14:05:08,109] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-10660/global_step10663/mp_rank_00_model_states.pt.
[2025-03-10 14:05:08,110] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-10660/global_step10663/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:05:09,074] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-10660/global_step10663/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:05:09,075] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-10660/global_step10663/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:05:09,075] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10663 is ready now!
{'loss': 0.1064, 'grad_norm': 2.503457546234131, 'learning_rate': 2e-05, 'epoch': 14.0}
 35%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                              | 11480/32800 [23:11<40:59,  8.67it/s][INFO|trainer.py:930] 2025-03-10 14:06:45,251 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:06:45,254 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:06:45,255 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:06:45,255 >>   Batch size = 4
{'eval_loss': 1.0794988870620728, 'eval_mse': 1.0792728087116528, 'eval_pearson': 0.7182736728182881, 'eval_spearmanr': 0.7385434373112194, 'eval_runtime': 1.7651, 'eval_samples_per_second': 784.1, 'eval_steps_per_second': 98.013, 'epoch': 14.0}
 35%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                              | 11480/32800 [23:13<40:59,  8.67it/s]
[INFO|trainer.py:3955] 2025-03-10 14:06:47,159 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-11480
[INFO|configuration_utils.py:423] 2025-03-10 14:06:47,161 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-11480/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:06:47,372 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-11480/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:06:47,373 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-11480/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:06:47,373 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-11480/special_tokens_map.json
[2025-03-10 14:06:47,394] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11483 is about to be saved!
[2025-03-10 14:06:47,397] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-11480/global_step11483/mp_rank_00_model_states.pt
[2025-03-10 14:06:47,397] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-11480/global_step11483/mp_rank_00_model_states.pt...
[2025-03-10 14:06:47,686] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-11480/global_step11483/mp_rank_00_model_states.pt.
[2025-03-10 14:06:47,687] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-11480/global_step11483/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:06:48,685] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-11480/global_step11483/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:06:48,685] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-11480/global_step11483/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:06:48,686] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11483 is ready now!
{'loss': 0.1003, 'grad_norm': 1.1408196687698364, 'learning_rate': 2e-05, 'epoch': 15.0}
 38%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                     | 12300/32800 [24:49<39:22,  8.68it/s][INFO|trainer.py:930] 2025-03-10 14:08:23,520 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:08:23,523 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:08:23,523 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:08:23,523 >>   Batch size = 4
{'eval_loss': 1.153660774230957, 'eval_mse': 1.1529858515441762, 'eval_pearson': 0.7172594240369704, 'eval_spearmanr': 0.7350369785033668, 'eval_runtime': 1.7576, 'eval_samples_per_second': 787.439, 'eval_steps_per_second': 98.43, 'epoch': 15.0}
 38%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                     | 12300/32800 [24:51<39:22,  8.68it/s]
[INFO|trainer.py:3955] 2025-03-10 14:08:25,418 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-12300
[INFO|configuration_utils.py:423] 2025-03-10 14:08:25,420 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-12300/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:08:25,627 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-12300/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:08:25,628 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-12300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:08:25,628 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-12300/special_tokens_map.json
[2025-03-10 14:08:25,650] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step12303 is about to be saved!
[2025-03-10 14:08:25,654] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-12300/global_step12303/mp_rank_00_model_states.pt
[2025-03-10 14:08:25,654] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-12300/global_step12303/mp_rank_00_model_states.pt...
[2025-03-10 14:08:25,936] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-12300/global_step12303/mp_rank_00_model_states.pt.
[2025-03-10 14:08:25,937] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-12300/global_step12303/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:08:26,882] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-12300/global_step12303/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:08:26,883] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-12300/global_step12303/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:08:26,883] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12303 is ready now!
{'loss': 0.1096, 'grad_norm': 7.707049369812012, 'learning_rate': 2e-05, 'epoch': 16.0}
 40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                             | 13120/32800 [26:26<37:31,  8.74it/s][INFO|trainer.py:930] 2025-03-10 14:10:00,915 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:10:00,918 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:10:00,918 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:10:00,918 >>   Batch size = 4
{'eval_loss': 1.0831937789916992, 'eval_mse': 1.083113787835733, 'eval_pearson': 0.7164836651831814, 'eval_spearmanr': 0.7405284622261834, 'eval_runtime': 1.7543, 'eval_samples_per_second': 788.907, 'eval_steps_per_second': 98.613, 'epoch': 16.0}
 40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                             | 13120/32800 [26:28<37:31,  8.74it/s]
[INFO|trainer.py:3955] 2025-03-10 14:10:02,814 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13120
[INFO|configuration_utils.py:423] 2025-03-10 14:10:02,816 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13120/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:10:03,023 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13120/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:10:03,023 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13120/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:10:03,024 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13120/special_tokens_map.json
[2025-03-10 14:10:03,046] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13124 is about to be saved!
[2025-03-10 14:10:03,049] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13120/global_step13124/mp_rank_00_model_states.pt
[2025-03-10 14:10:03,049] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13120/global_step13124/mp_rank_00_model_states.pt...
[2025-03-10 14:10:03,332] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13120/global_step13124/mp_rank_00_model_states.pt.
[2025-03-10 14:10:03,333] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13120/global_step13124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:10:04,286] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13120/global_step13124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:10:04,286] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13120/global_step13124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:10:04,287] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13124 is ready now!
{'loss': 0.0857, 'grad_norm': 2.6744253635406494, 'learning_rate': 2e-05, 'epoch': 17.0}
 42%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                    | 13940/32800 [28:05<36:25,  8.63it/s][INFO|trainer.py:930] 2025-03-10 14:11:39,342 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:11:39,345 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:11:39,345 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:11:39,345 >>   Batch size = 4
{'eval_loss': 1.1527953147888184, 'eval_mse': 1.1522030785593684, 'eval_pearson': 0.7115762405238493, 'eval_spearmanr': 0.7350429789357714, 'eval_runtime': 1.76, 'eval_samples_per_second': 786.38, 'eval_steps_per_second': 98.297, 'epoch': 17.0}
 42%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                    | 13940/32800 [28:07<36:25,  8.63it/s]
[INFO|trainer.py:3955] 2025-03-10 14:11:41,247 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13940
[INFO|configuration_utils.py:423] 2025-03-10 14:11:41,249 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13940/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:11:41,455 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13940/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:11:41,455 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13940/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:11:41,456 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13940/special_tokens_map.json
[2025-03-10 14:11:41,478] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13944 is about to be saved!
[2025-03-10 14:11:41,481] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13940/global_step13944/mp_rank_00_model_states.pt
[2025-03-10 14:11:41,481] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13940/global_step13944/mp_rank_00_model_states.pt...
[2025-03-10 14:11:41,765] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13940/global_step13944/mp_rank_00_model_states.pt.
[2025-03-10 14:11:41,766] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13940/global_step13944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:11:42,764] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13940/global_step13944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:11:42,765] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-13940/global_step13944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:11:42,765] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13944 is ready now!
{'loss': 0.0689, 'grad_norm': 0.9104573726654053, 'learning_rate': 2e-05, 'epoch': 18.0}
 45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                            | 14760/32800 [29:43<34:43,  8.66it/s][INFO|trainer.py:930] 2025-03-10 14:13:17,459 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:13:17,462 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:13:17,462 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:13:17,462 >>   Batch size = 4
{'eval_loss': 1.1377849578857422, 'eval_mse': 1.1375448240365595, 'eval_pearson': 0.7147962457034399, 'eval_spearmanr': 0.7416026334708874, 'eval_runtime': 1.7662, 'eval_samples_per_second': 783.618, 'eval_steps_per_second': 97.952, 'epoch': 18.0}
 45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                            | 14760/32800 [29:45<34:43,  8.66it/s]
[INFO|trainer.py:3955] 2025-03-10 14:13:19,371 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-14760
[INFO|configuration_utils.py:423] 2025-03-10 14:13:19,373 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-14760/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:13:19,588 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-14760/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:13:19,588 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-14760/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:13:19,588 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-14760/special_tokens_map.json
[2025-03-10 14:13:19,610] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14764 is about to be saved!
[2025-03-10 14:13:19,613] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-14760/global_step14764/mp_rank_00_model_states.pt
[2025-03-10 14:13:19,613] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-14760/global_step14764/mp_rank_00_model_states.pt...
[2025-03-10 14:13:19,898] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-14760/global_step14764/mp_rank_00_model_states.pt.
[2025-03-10 14:13:19,899] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-14760/global_step14764/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:13:20,877] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-14760/global_step14764/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:13:20,878] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-14760/global_step14764/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:13:20,878] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14764 is ready now!
{'loss': 0.0672, 'grad_norm': 6.110968589782715, 'learning_rate': 2e-05, 'epoch': 19.0}
 48%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                   | 15580/32800 [31:21<33:03,  8.68it/s][INFO|trainer.py:930] 2025-03-10 14:14:55,455 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:14:55,458 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:14:55,458 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:14:55,458 >>   Batch size = 4
{'eval_loss': 1.0341612100601196, 'eval_mse': 1.0335427160897006, 'eval_pearson': 0.7375746550447493, 'eval_spearmanr': 0.7580528699257946, 'eval_runtime': 1.7599, 'eval_samples_per_second': 786.428, 'eval_steps_per_second': 98.303, 'epoch': 19.0}
 48%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                   | 15580/32800 [31:23<33:03,  8.68it/s]
[INFO|trainer.py:3955] 2025-03-10 14:14:57,359 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-15580
[INFO|configuration_utils.py:423] 2025-03-10 14:14:57,361 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-15580/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:14:57,573 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-15580/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:14:57,574 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-15580/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:14:57,574 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-15580/special_tokens_map.json
[2025-03-10 14:14:57,597] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step15584 is about to be saved!
[2025-03-10 14:14:57,600] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-15580/global_step15584/mp_rank_00_model_states.pt
[2025-03-10 14:14:57,600] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-15580/global_step15584/mp_rank_00_model_states.pt...
[2025-03-10 14:14:57,886] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-15580/global_step15584/mp_rank_00_model_states.pt.
[2025-03-10 14:14:57,887] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-15580/global_step15584/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:14:58,817] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-15580/global_step15584/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:14:58,817] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-15580/global_step15584/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:14:58,818] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15584 is ready now!
{'loss': 0.0754, 'grad_norm': 2.2864439487457275, 'learning_rate': 2e-05, 'epoch': 20.0}
 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                           | 16400/32800 [32:58<31:23,  8.71it/s][INFO|trainer.py:930] 2025-03-10 14:16:32,941 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:16:32,944 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:16:32,944 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:16:32,944 >>   Batch size = 4
{'eval_loss': 1.096775770187378, 'eval_mse': 1.0962308242141856, 'eval_pearson': 0.7283343003170477, 'eval_spearmanr': 0.7451135224814595, 'eval_runtime': 1.7658, 'eval_samples_per_second': 783.76, 'eval_steps_per_second': 97.97, 'epoch': 20.0}
 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                           | 16400/32800 [33:00<31:23,  8.71it/s]
[INFO|trainer.py:3955] 2025-03-10 14:16:34,853 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-16400
[INFO|configuration_utils.py:423] 2025-03-10 14:16:34,855 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-16400/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:16:35,070 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-16400/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:16:35,071 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-16400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:16:35,071 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-16400/special_tokens_map.json
[2025-03-10 14:16:35,091] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step16405 is about to be saved!
[2025-03-10 14:16:35,095] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-16400/global_step16405/mp_rank_00_model_states.pt
[2025-03-10 14:16:35,095] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-16400/global_step16405/mp_rank_00_model_states.pt...
[2025-03-10 14:16:35,380] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-16400/global_step16405/mp_rank_00_model_states.pt.
[2025-03-10 14:16:35,381] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-16400/global_step16405/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:16:36,344] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-16400/global_step16405/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:16:36,345] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-16400/global_step16405/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:16:36,345] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16405 is ready now!
{'loss': 0.0547, 'grad_norm': 1.850247859954834, 'learning_rate': 2e-05, 'epoch': 21.0}
 52%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                  | 17220/32800 [34:40<30:08,  8.62it/s][INFO|trainer.py:930] 2025-03-10 14:18:14,335 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:18:14,338 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:18:14,338 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:18:14,338 >>   Batch size = 4
{'eval_loss': 1.1497682332992554, 'eval_mse': 1.149274858780679, 'eval_pearson': 0.701511320626167, 'eval_spearmanr': 0.7368338704312263, 'eval_runtime': 1.7721, 'eval_samples_per_second': 780.984, 'eval_steps_per_second': 97.623, 'epoch': 21.0}
 52%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                  | 17220/32800 [34:42<30:08,  8.62it/s]
[INFO|trainer.py:3955] 2025-03-10 14:18:16,245 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-17220
[INFO|configuration_utils.py:423] 2025-03-10 14:18:16,247 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-17220/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:18:16,467 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-17220/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:18:16,467 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-17220/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:18:16,468 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-17220/special_tokens_map.json
[2025-03-10 14:18:16,489] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step17225 is about to be saved!
[2025-03-10 14:18:16,493] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-17220/global_step17225/mp_rank_00_model_states.pt
[2025-03-10 14:18:16,493] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-17220/global_step17225/mp_rank_00_model_states.pt...
[2025-03-10 14:18:16,781] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-17220/global_step17225/mp_rank_00_model_states.pt.
[2025-03-10 14:18:16,782] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-17220/global_step17225/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:18:17,742] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-17220/global_step17225/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:18:17,743] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-17220/global_step17225/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:18:17,743] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17225 is ready now!
{'loss': 0.0447, 'grad_norm': 1.600904107093811, 'learning_rate': 2e-05, 'epoch': 22.0}
 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                          | 18040/32800 [36:20<28:33,  8.62it/s][INFO|trainer.py:930] 2025-03-10 14:19:54,024 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:19:54,027 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:19:54,027 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:19:54,027 >>   Batch size = 4
{'eval_loss': 1.111993670463562, 'eval_mse': 1.110966255554574, 'eval_pearson': 0.7207403422066126, 'eval_spearmanr': 0.7418691970829743, 'eval_runtime': 1.7551, 'eval_samples_per_second': 788.549, 'eval_steps_per_second': 98.569, 'epoch': 22.0}
 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                          | 18040/32800 [36:21<28:33,  8.62it/s]
[INFO|trainer.py:3955] 2025-03-10 14:19:55,925 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18040
[INFO|configuration_utils.py:423] 2025-03-10 14:19:55,927 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18040/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:19:56,152 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18040/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:19:56,152 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18040/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:19:56,153 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18040/special_tokens_map.json
[2025-03-10 14:19:56,175] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step18045 is about to be saved!
[2025-03-10 14:19:56,178] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18040/global_step18045/mp_rank_00_model_states.pt
[2025-03-10 14:19:56,178] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18040/global_step18045/mp_rank_00_model_states.pt...
[2025-03-10 14:19:56,474] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18040/global_step18045/mp_rank_00_model_states.pt.
[2025-03-10 14:19:56,475] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18040/global_step18045/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:19:57,451] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18040/global_step18045/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:19:57,452] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18040/global_step18045/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:19:57,452] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18045 is ready now!
{'loss': 0.044, 'grad_norm': 1.2922087907791138, 'learning_rate': 2e-05, 'epoch': 23.0}
 57%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                 | 18860/32800 [37:58<26:42,  8.70it/s][INFO|trainer.py:930] 2025-03-10 14:21:32,871 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:21:32,874 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:21:32,874 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:21:32,874 >>   Batch size = 4
{'eval_loss': 1.1588013172149658, 'eval_mse': 1.1581968959700855, 'eval_pearson': 0.7078060227533891, 'eval_spearmanr': 0.7339405107509545, 'eval_runtime': 1.765, 'eval_samples_per_second': 784.115, 'eval_steps_per_second': 98.014, 'epoch': 23.0}
 57%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                 | 18860/32800 [38:00<26:42,  8.70it/s]
[INFO|trainer.py:3955] 2025-03-10 14:21:34,783 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18860
[INFO|configuration_utils.py:423] 2025-03-10 14:21:34,785 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18860/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:21:35,014 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18860/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:21:35,015 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18860/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:21:35,015 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18860/special_tokens_map.json
[2025-03-10 14:21:35,038] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step18865 is about to be saved!
[2025-03-10 14:21:35,042] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18860/global_step18865/mp_rank_00_model_states.pt
[2025-03-10 14:21:35,042] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18860/global_step18865/mp_rank_00_model_states.pt...
[2025-03-10 14:21:35,348] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18860/global_step18865/mp_rank_00_model_states.pt.
[2025-03-10 14:21:35,349] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18860/global_step18865/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:21:36,361] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18860/global_step18865/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:21:36,362] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-18860/global_step18865/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:21:36,362] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18865 is ready now!
{'loss': 0.0497, 'grad_norm': 0.43424129486083984, 'learning_rate': 2e-05, 'epoch': 24.0}
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                        | 19680/32800 [39:36<25:38,  8.53it/s][INFO|trainer.py:930] 2025-03-10 14:23:10,517 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:23:10,521 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:23:10,521 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:23:10,521 >>   Batch size = 4
{'eval_loss': 1.1781494617462158, 'eval_mse': 1.1777215813625754, 'eval_pearson': 0.7325357237067138, 'eval_spearmanr': 0.7472498527170659, 'eval_runtime': 1.7615, 'eval_samples_per_second': 785.678, 'eval_steps_per_second': 98.21, 'epoch': 24.0}
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                        | 19680/32800 [39:38<25:38,  8.53it/s]
[INFO|trainer.py:3955] 2025-03-10 14:23:12,427 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-19680
[INFO|configuration_utils.py:423] 2025-03-10 14:23:12,429 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-19680/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:23:12,658 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-19680/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:23:12,659 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-19680/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:23:12,659 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-19680/special_tokens_map.json
[2025-03-10 14:23:12,677] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step19686 is about to be saved!
[2025-03-10 14:23:12,681] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-19680/global_step19686/mp_rank_00_model_states.pt
[2025-03-10 14:23:12,681] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-19680/global_step19686/mp_rank_00_model_states.pt...
[2025-03-10 14:23:12,985] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-19680/global_step19686/mp_rank_00_model_states.pt.
[2025-03-10 14:23:12,986] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-19680/global_step19686/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:23:13,994] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-19680/global_step19686/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:23:13,995] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-19680/global_step19686/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:23:13,995] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19686 is ready now!
{'loss': 0.0396, 'grad_norm': 1.3784207105636597, 'learning_rate': 2e-05, 'epoch': 25.0}
 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                | 20500/32800 [41:16<23:50,  8.60it/s][INFO|trainer.py:930] 2025-03-10 14:24:50,242 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:24:50,245 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:24:50,245 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:24:50,245 >>   Batch size = 4
{'eval_loss': 1.1286466121673584, 'eval_mse': 1.1280655821709964, 'eval_pearson': 0.7176352461961707, 'eval_spearmanr': 0.7442046035031435, 'eval_runtime': 1.7604, 'eval_samples_per_second': 786.184, 'eval_steps_per_second': 98.273, 'epoch': 25.0}
 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                | 20500/32800 [41:18<23:50,  8.60it/s]
[INFO|trainer.py:3955] 2025-03-10 14:24:52,150 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-20500
[INFO|configuration_utils.py:423] 2025-03-10 14:24:52,152 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-20500/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:24:52,378 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-20500/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:24:52,379 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-20500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:24:52,379 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-20500/special_tokens_map.json
[2025-03-10 14:24:52,400] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step20506 is about to be saved!
[2025-03-10 14:24:52,403] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-20500/global_step20506/mp_rank_00_model_states.pt
[2025-03-10 14:24:52,403] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-20500/global_step20506/mp_rank_00_model_states.pt...
[2025-03-10 14:24:52,705] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-20500/global_step20506/mp_rank_00_model_states.pt.
[2025-03-10 14:24:52,706] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-20500/global_step20506/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:24:53,718] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-20500/global_step20506/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:24:53,718] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-20500/global_step20506/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:24:53,718] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20506 is ready now!
{'loss': 0.0326, 'grad_norm': 0.9000791311264038, 'learning_rate': 2e-05, 'epoch': 26.0}
 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                       | 21320/32800 [42:55<22:01,  8.69it/s][INFO|trainer.py:930] 2025-03-10 14:26:29,328 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:26:29,331 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:26:29,331 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:26:29,331 >>   Batch size = 4
{'eval_loss': 1.1399412155151367, 'eval_mse': 1.139419684218878, 'eval_pearson': 0.7128873596080707, 'eval_spearmanr': 0.7374566312912526, 'eval_runtime': 1.7575, 'eval_samples_per_second': 787.493, 'eval_steps_per_second': 98.437, 'epoch': 26.0}
 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                       | 21320/32800 [42:57<22:01,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 14:26:31,227 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-21320
[INFO|configuration_utils.py:423] 2025-03-10 14:26:31,229 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-21320/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:26:31,466 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-21320/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:26:31,466 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-21320/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:26:31,466 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-21320/special_tokens_map.json
[2025-03-10 14:26:31,488] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step21326 is about to be saved!
[2025-03-10 14:26:31,491] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-21320/global_step21326/mp_rank_00_model_states.pt
[2025-03-10 14:26:31,491] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-21320/global_step21326/mp_rank_00_model_states.pt...
[2025-03-10 14:26:31,787] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-21320/global_step21326/mp_rank_00_model_states.pt.
[2025-03-10 14:26:31,788] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-21320/global_step21326/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:26:32,784] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-21320/global_step21326/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:26:32,784] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-21320/global_step21326/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:26:32,784] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21326 is ready now!
{'loss': 0.0328, 'grad_norm': 1.312909483909607, 'learning_rate': 2e-05, 'epoch': 27.0}
 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                               | 22140/32800 [44:33<21:07,  8.41it/s][INFO|trainer.py:930] 2025-03-10 14:28:07,599 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:28:07,602 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:28:07,602 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:28:07,602 >>   Batch size = 4
{'eval_loss': 1.0656388998031616, 'eval_mse': 1.0653531488052683, 'eval_pearson': 0.7356693337178335, 'eval_spearmanr': 0.7637800117117021, 'eval_runtime': 1.7621, 'eval_samples_per_second': 785.414, 'eval_steps_per_second': 98.177, 'epoch': 27.0}
 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                               | 22140/32800 [44:35<21:07,  8.41it/s]
[INFO|trainer.py:3955] 2025-03-10 14:28:09,509 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22140
[INFO|configuration_utils.py:423] 2025-03-10 14:28:09,511 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22140/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:28:09,745 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22140/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:28:09,746 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22140/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:28:09,746 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22140/special_tokens_map.json
[2025-03-10 14:28:09,764] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step22146 is about to be saved!
[2025-03-10 14:28:09,767] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22140/global_step22146/mp_rank_00_model_states.pt
[2025-03-10 14:28:09,767] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22140/global_step22146/mp_rank_00_model_states.pt...
[2025-03-10 14:28:10,069] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22140/global_step22146/mp_rank_00_model_states.pt.
[2025-03-10 14:28:10,070] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22140/global_step22146/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:28:11,064] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22140/global_step22146/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:28:11,064] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22140/global_step22146/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:28:11,064] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22146 is ready now!
{'loss': 0.0363, 'grad_norm': 0.5392686724662781, 'learning_rate': 2e-05, 'epoch': 28.0}
 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                      | 22960/32800 [46:11<18:49,  8.71it/s][INFO|trainer.py:930] 2025-03-10 14:29:45,701 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:29:45,705 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:29:45,705 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:29:45,705 >>   Batch size = 4
{'eval_loss': 1.0272574424743652, 'eval_mse': 1.0272538699855693, 'eval_pearson': 0.7437482870611357, 'eval_spearmanr': 0.7653815849928887, 'eval_runtime': 1.7623, 'eval_samples_per_second': 785.335, 'eval_steps_per_second': 98.167, 'epoch': 28.0}
 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                      | 22960/32800 [46:13<18:49,  8.71it/s]
[INFO|trainer.py:3955] 2025-03-10 14:29:47,612 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22960
[INFO|configuration_utils.py:423] 2025-03-10 14:29:47,614 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22960/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:29:47,845 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22960/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:29:47,845 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22960/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:29:47,846 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22960/special_tokens_map.json
[2025-03-10 14:29:47,869] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step22967 is about to be saved!
[2025-03-10 14:29:47,873] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22960/global_step22967/mp_rank_00_model_states.pt
[2025-03-10 14:29:47,873] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22960/global_step22967/mp_rank_00_model_states.pt...
[2025-03-10 14:29:48,186] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22960/global_step22967/mp_rank_00_model_states.pt.
[2025-03-10 14:29:48,187] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22960/global_step22967/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:29:49,204] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22960/global_step22967/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:29:49,204] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-22960/global_step22967/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:29:49,204] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22967 is ready now!
{'loss': 0.0296, 'grad_norm': 1.9001048803329468, 'learning_rate': 2e-05, 'epoch': 29.0}
 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                              | 23780/32800 [47:50<17:21,  8.66it/s][INFO|trainer.py:930] 2025-03-10 14:31:24,382 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:31:24,385 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:31:24,385 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:31:24,385 >>   Batch size = 4
{'eval_loss': 1.0633548498153687, 'eval_mse': 1.0635621427111543, 'eval_pearson': 0.735748900393225, 'eval_spearmanr': 0.7563916137657358, 'eval_runtime': 1.7664, 'eval_samples_per_second': 783.534, 'eval_steps_per_second': 97.942, 'epoch': 29.0}
 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                              | 23780/32800 [47:52<17:21,  8.66it/s]
[INFO|trainer.py:3955] 2025-03-10 14:31:26,295 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-23780
[INFO|configuration_utils.py:423] 2025-03-10 14:31:26,297 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-23780/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:31:26,527 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-23780/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:31:26,527 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-23780/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:31:26,528 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-23780/special_tokens_map.json
[2025-03-10 14:31:26,549] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step23787 is about to be saved!
[2025-03-10 14:31:26,552] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-23780/global_step23787/mp_rank_00_model_states.pt
[2025-03-10 14:31:26,552] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-23780/global_step23787/mp_rank_00_model_states.pt...
[2025-03-10 14:31:26,860] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-23780/global_step23787/mp_rank_00_model_states.pt.
[2025-03-10 14:31:26,861] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-23780/global_step23787/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:31:27,885] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-23780/global_step23787/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:31:27,886] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-23780/global_step23787/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:31:27,886] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23787 is ready now!
{'loss': 0.0258, 'grad_norm': 1.5952428579330444, 'learning_rate': 2e-05, 'epoch': 30.0}
 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 24600/32800 [49:28<15:43,  8.69it/s][INFO|trainer.py:930] 2025-03-10 14:33:02,382 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:33:02,386 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:33:02,386 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:33:02,386 >>   Batch size = 4
{'eval_loss': 1.1255731582641602, 'eval_mse': 1.1254787834393496, 'eval_pearson': 0.7176658665936091, 'eval_spearmanr': 0.736731118914603, 'eval_runtime': 1.7635, 'eval_samples_per_second': 784.816, 'eval_steps_per_second': 98.102, 'epoch': 30.0}
 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 24600/32800 [49:30<15:43,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 14:33:04,287 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-24600
[INFO|configuration_utils.py:423] 2025-03-10 14:33:04,289 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-24600/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:33:04,512 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-24600/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:33:04,512 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-24600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:33:04,513 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-24600/special_tokens_map.json
[2025-03-10 14:33:04,535] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step24607 is about to be saved!
[2025-03-10 14:33:04,539] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-24600/global_step24607/mp_rank_00_model_states.pt
[2025-03-10 14:33:04,539] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-24600/global_step24607/mp_rank_00_model_states.pt...
[2025-03-10 14:33:04,838] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-24600/global_step24607/mp_rank_00_model_states.pt.
[2025-03-10 14:33:04,839] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-24600/global_step24607/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:33:05,815] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-24600/global_step24607/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:33:05,815] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-24600/global_step24607/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:33:05,815] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24607 is ready now!
{'loss': 0.0259, 'grad_norm': 0.3561428189277649, 'learning_rate': 2e-05, 'epoch': 31.0}
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                             | 25420/32800 [51:06<14:08,  8.70it/s][INFO|trainer.py:930] 2025-03-10 14:34:40,310 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:34:40,313 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:34:40,313 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:34:40,313 >>   Batch size = 4
{'eval_loss': 1.0835191011428833, 'eval_mse': 1.082966903730624, 'eval_pearson': 0.7304713703333268, 'eval_spearmanr': 0.7471527064531742, 'eval_runtime': 1.7633, 'eval_samples_per_second': 784.894, 'eval_steps_per_second': 98.112, 'epoch': 31.0}
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                             | 25420/32800 [51:08<14:08,  8.70it/s]
[INFO|trainer.py:3955] 2025-03-10 14:34:42,218 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-25420
[INFO|configuration_utils.py:423] 2025-03-10 14:34:42,220 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-25420/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:34:42,442 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-25420/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:34:42,443 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-25420/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:34:42,443 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-25420/special_tokens_map.json
[2025-03-10 14:34:42,465] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step25427 is about to be saved!
[2025-03-10 14:34:42,469] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-25420/global_step25427/mp_rank_00_model_states.pt
[2025-03-10 14:34:42,469] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-25420/global_step25427/mp_rank_00_model_states.pt...
[2025-03-10 14:34:42,763] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-25420/global_step25427/mp_rank_00_model_states.pt.
[2025-03-10 14:34:42,764] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-25420/global_step25427/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:34:43,740] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-25420/global_step25427/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:34:43,740] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-25420/global_step25427/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:34:43,741] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25427 is ready now!
{'loss': 0.0278, 'grad_norm': 4.152468681335449, 'learning_rate': 2e-05, 'epoch': 32.0}
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                    | 26240/32800 [52:44<12:33,  8.70it/s][INFO|trainer.py:930] 2025-03-10 14:36:18,201 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:36:18,204 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:36:18,204 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:36:18,204 >>   Batch size = 4
{'eval_loss': 1.1665709018707275, 'eval_mse': 1.1665713647972642, 'eval_pearson': 0.7218239122635415, 'eval_spearmanr': 0.7363207064040151, 'eval_runtime': 1.7668, 'eval_samples_per_second': 783.332, 'eval_steps_per_second': 97.917, 'epoch': 32.0}
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                    | 26240/32800 [52:45<12:33,  8.70it/s]
[INFO|trainer.py:3955] 2025-03-10 14:36:20,113 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-26240
[INFO|configuration_utils.py:423] 2025-03-10 14:36:20,115 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-26240/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:36:20,338 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-26240/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:36:20,339 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-26240/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:36:20,339 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-26240/special_tokens_map.json
[2025-03-10 14:36:20,358] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step26248 is about to be saved!
[2025-03-10 14:36:20,362] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-26240/global_step26248/mp_rank_00_model_states.pt
[2025-03-10 14:36:20,362] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-26240/global_step26248/mp_rank_00_model_states.pt...
[2025-03-10 14:36:20,665] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-26240/global_step26248/mp_rank_00_model_states.pt.
[2025-03-10 14:36:20,666] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-26240/global_step26248/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:36:21,641] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-26240/global_step26248/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:36:21,642] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-26240/global_step26248/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:36:21,642] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26248 is ready now!
{'loss': 0.0257, 'grad_norm': 0.7016260623931885, 'learning_rate': 2e-05, 'epoch': 33.0}
 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                           | 27060/32800 [54:23<11:05,  8.62it/s][INFO|trainer.py:930] 2025-03-10 14:37:57,688 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:37:57,691 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:37:57,691 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:37:57,691 >>   Batch size = 4
{'eval_loss': 1.0299147367477417, 'eval_mse': 1.029676378462356, 'eval_pearson': 0.7437283058492741, 'eval_spearmanr': 0.7669053938455928, 'eval_runtime': 1.7656, 'eval_samples_per_second': 783.883, 'eval_steps_per_second': 97.985, 'epoch': 33.0}
 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                           | 27060/32800 [54:25<11:05,  8.62it/s]
[INFO|trainer.py:3955] 2025-03-10 14:37:59,601 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27060
[INFO|configuration_utils.py:423] 2025-03-10 14:37:59,603 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27060/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:37:59,832 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27060/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:37:59,833 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27060/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:37:59,833 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27060/special_tokens_map.json
[2025-03-10 14:37:59,856] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step27068 is about to be saved!
[2025-03-10 14:37:59,859] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27060/global_step27068/mp_rank_00_model_states.pt
[2025-03-10 14:37:59,859] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27060/global_step27068/mp_rank_00_model_states.pt...
[2025-03-10 14:38:00,167] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27060/global_step27068/mp_rank_00_model_states.pt.
[2025-03-10 14:38:00,168] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27060/global_step27068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:38:01,164] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27060/global_step27068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:38:01,165] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27060/global_step27068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:38:01,165] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27068 is ready now!
{'loss': 0.0258, 'grad_norm': 1.6374202966690063, 'learning_rate': 2e-05, 'epoch': 34.0}
 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                   | 27880/32800 [56:01<09:27,  8.67it/s][INFO|trainer.py:930] 2025-03-10 14:39:35,701 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:39:35,704 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:39:35,704 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:39:35,704 >>   Batch size = 4
{'eval_loss': 1.0050603151321411, 'eval_mse': 1.0049090408641008, 'eval_pearson': 0.7450730345079466, 'eval_spearmanr': 0.7728260299766828, 'eval_runtime': 1.762, 'eval_samples_per_second': 785.458, 'eval_steps_per_second': 98.182, 'epoch': 34.0}
 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                   | 27880/32800 [56:03<09:27,  8.67it/s]
[INFO|trainer.py:3955] 2025-03-10 14:39:37,610 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27880
[INFO|configuration_utils.py:423] 2025-03-10 14:39:37,612 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27880/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:39:37,845 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27880/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:39:37,845 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27880/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:39:37,845 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27880/special_tokens_map.json
[2025-03-10 14:39:37,864] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step27888 is about to be saved!
[2025-03-10 14:39:37,867] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27880/global_step27888/mp_rank_00_model_states.pt
[2025-03-10 14:39:37,867] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27880/global_step27888/mp_rank_00_model_states.pt...
[2025-03-10 14:39:38,180] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27880/global_step27888/mp_rank_00_model_states.pt.
[2025-03-10 14:39:38,181] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27880/global_step27888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:39:39,181] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27880/global_step27888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:39:39,182] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-27880/global_step27888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:39:39,182] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27888 is ready now!
{'loss': 0.0237, 'grad_norm': 0.7242922782897949, 'learning_rate': 2e-05, 'epoch': 35.0}
 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                          | 28700/32800 [57:39<07:52,  8.69it/s][INFO|trainer.py:930] 2025-03-10 14:41:13,783 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:41:13,786 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:41:13,786 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:41:13,786 >>   Batch size = 4
{'eval_loss': 1.1435977220535278, 'eval_mse': 1.1435863999273046, 'eval_pearson': 0.7218796745359143, 'eval_spearmanr': 0.7469776770623086, 'eval_runtime': 1.8121, 'eval_samples_per_second': 763.775, 'eval_steps_per_second': 95.472, 'epoch': 35.0}
 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                          | 28700/32800 [57:41<07:52,  8.69it/s]
[INFO|trainer.py:3955] 2025-03-10 14:41:15,692 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-28700
[INFO|configuration_utils.py:423] 2025-03-10 14:41:15,694 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-28700/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:41:15,922 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-28700/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:41:15,923 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-28700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:41:15,923 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-28700/special_tokens_map.json
[2025-03-10 14:41:15,946] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step28708 is about to be saved!
[2025-03-10 14:41:15,949] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-28700/global_step28708/mp_rank_00_model_states.pt
[2025-03-10 14:41:15,949] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-28700/global_step28708/mp_rank_00_model_states.pt...
[2025-03-10 14:41:16,254] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-28700/global_step28708/mp_rank_00_model_states.pt.
[2025-03-10 14:41:16,255] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-28700/global_step28708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:41:17,233] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-28700/global_step28708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:41:17,233] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-28700/global_step28708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:41:17,233] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28708 is ready now!
{'loss': 0.0264, 'grad_norm': 0.5499597191810608, 'learning_rate': 2e-05, 'epoch': 36.0}
 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                  | 29520/32800 [59:18<06:26,  8.49it/s][INFO|trainer.py:930] 2025-03-10 14:42:52,042 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:42:52,046 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:42:52,046 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:42:52,046 >>   Batch size = 4
{'eval_loss': 1.0621107816696167, 'eval_mse': 1.0615455622601795, 'eval_pearson': 0.7363339803780956, 'eval_spearmanr': 0.7610813978035095, 'eval_runtime': 1.7612, 'eval_samples_per_second': 785.841, 'eval_steps_per_second': 98.23, 'epoch': 36.0}
 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                  | 29520/32800 [59:19<06:26,  8.49it/s]
[INFO|trainer.py:3955] 2025-03-10 14:42:53,949 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-29520
[INFO|configuration_utils.py:423] 2025-03-10 14:42:53,951 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-29520/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:42:54,181 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-29520/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:42:54,182 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-29520/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:42:54,182 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-29520/special_tokens_map.json
[2025-03-10 14:42:54,196] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step29529 is about to be saved!
[2025-03-10 14:42:54,199] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-29520/global_step29529/mp_rank_00_model_states.pt
[2025-03-10 14:42:54,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-29520/global_step29529/mp_rank_00_model_states.pt...
[2025-03-10 14:42:54,509] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-29520/global_step29529/mp_rank_00_model_states.pt.
[2025-03-10 14:42:54,510] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-29520/global_step29529/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:42:55,496] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-29520/global_step29529/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:42:55,496] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-29520/global_step29529/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:42:55,496] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29529 is ready now!
{'loss': 0.0223, 'grad_norm': 2.1863367557525635, 'learning_rate': 2e-05, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 30340/32800 [1:00:56<04:44,  8.63it/s][INFO|trainer.py:930] 2025-03-10 14:44:30,857 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:44:30,860 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:44:30,860 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:44:30,860 >>   Batch size = 4
{'eval_loss': 1.0478781461715698, 'eval_mse': 1.0477341824042046, 'eval_pearson': 0.7393851743701751, 'eval_spearmanr': 0.7644330486054649, 'eval_runtime': 1.7668, 'eval_samples_per_second': 783.346, 'eval_steps_per_second': 97.918, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 30340/32800 [1:00:58<04:44,  8.63it/s]
[INFO|trainer.py:3955] 2025-03-10 14:44:32,801 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-30340
[INFO|configuration_utils.py:423] 2025-03-10 14:44:32,803 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-30340/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:44:33,093 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-30340/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:44:33,094 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-30340/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:44:33,094 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-30340/special_tokens_map.json
[2025-03-10 14:44:33,121] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step30349 is about to be saved!
[2025-03-10 14:44:33,124] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-30340/global_step30349/mp_rank_00_model_states.pt
[2025-03-10 14:44:33,124] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-30340/global_step30349/mp_rank_00_model_states.pt...
[2025-03-10 14:44:33,497] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-30340/global_step30349/mp_rank_00_model_states.pt.
[2025-03-10 14:44:33,498] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-30340/global_step30349/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:44:34,696] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-30340/global_step30349/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:44:34,697] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-30340/global_step30349/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:44:34,697] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30349 is ready now!
{'loss': 0.0188, 'grad_norm': 1.0224850177764893, 'learning_rate': 2e-05, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 31160/32800 [1:02:35<03:09,  8.66it/s][INFO|trainer.py:930] 2025-03-10 14:46:09,188 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:46:09,191 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:46:09,191 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:46:09,191 >>   Batch size = 4
{'eval_loss': 0.9162383079528809, 'eval_mse': 0.9158841011496638, 'eval_pearson': 0.7734919843941082, 'eval_spearmanr': 0.7879850396341549, 'eval_runtime': 1.8143, 'eval_samples_per_second': 762.845, 'eval_steps_per_second': 95.356, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 31160/32800 [1:02:37<03:09,  8.66it/s]
[INFO|trainer.py:3955] 2025-03-10 14:46:11,133 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31160
[INFO|configuration_utils.py:423] 2025-03-10 14:46:11,135 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31160/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:46:11,444 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31160/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:46:11,444 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31160/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:46:11,444 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31160/special_tokens_map.json
[2025-03-10 14:46:11,470] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step31169 is about to be saved!
[2025-03-10 14:46:11,474] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31160/global_step31169/mp_rank_00_model_states.pt
[2025-03-10 14:46:11,474] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31160/global_step31169/mp_rank_00_model_states.pt...
[2025-03-10 14:46:11,862] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31160/global_step31169/mp_rank_00_model_states.pt.
[2025-03-10 14:46:11,863] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31160/global_step31169/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:46:13,062] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31160/global_step31169/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:46:13,062] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31160/global_step31169/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:46:13,062] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31169 is ready now!
{'loss': 0.0182, 'grad_norm': 0.730400800704956, 'learning_rate': 2e-05, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 31980/32800 [1:04:13<01:34,  8.68it/s][INFO|trainer.py:930] 2025-03-10 14:47:47,690 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:47:47,694 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:47:47,694 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:47:47,694 >>   Batch size = 4
{'eval_loss': 1.0377922058105469, 'eval_mse': 1.0373746015432943, 'eval_pearson': 0.7431349298356791, 'eval_spearmanr': 0.7620295146095025, 'eval_runtime': 1.7645, 'eval_samples_per_second': 784.376, 'eval_steps_per_second': 98.047, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 31980/32800 [1:04:15<01:34,  8.68it/s]
[INFO|trainer.py:3955] 2025-03-10 14:47:49,630 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31980
[INFO|configuration_utils.py:423] 2025-03-10 14:47:49,632 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31980/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:47:49,907 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31980/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:47:49,908 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31980/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:47:49,908 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31980/special_tokens_map.json
[2025-03-10 14:47:49,931] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step31989 is about to be saved!
[2025-03-10 14:47:49,935] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31980/global_step31989/mp_rank_00_model_states.pt
[2025-03-10 14:47:49,935] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31980/global_step31989/mp_rank_00_model_states.pt...
[2025-03-10 14:47:50,294] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31980/global_step31989/mp_rank_00_model_states.pt.
[2025-03-10 14:47:50,295] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31980/global_step31989/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:47:51,479] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31980/global_step31989/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:47:51,480] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-31980/global_step31989/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:47:51,480] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31989 is ready now!
{'loss': 0.0252, 'grad_norm': 3.57879638671875, 'learning_rate': 2e-05, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32800/32800 [1:05:51<00:00,  8.72it/s][INFO|trainer.py:930] 2025-03-10 14:49:25,606 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:49:25,610 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:49:25,610 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:49:25,610 >>   Batch size = 4
{'eval_loss': 1.1202529668807983, 'eval_mse': 1.1201041389062913, 'eval_pearson': 0.7274694901413823, 'eval_spearmanr': 0.7451709121895067, 'eval_runtime': 1.7586, 'eval_samples_per_second': 787.011, 'eval_steps_per_second': 98.376, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32800/32800 [1:05:53<00:00,  8.72it/s]
[INFO|trainer.py:3955] 2025-03-10 14:49:27,540 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-32800
[INFO|configuration_utils.py:423] 2025-03-10 14:49:27,542 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-32800/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:49:27,817 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-32800/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:49:27,818 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-32800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:49:27,818 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-32800/special_tokens_map.json
[2025-03-10 14:49:27,839] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step32809 is about to be saved!
[2025-03-10 14:49:27,843] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-32800/global_step32809/mp_rank_00_model_states.pt
[2025-03-10 14:49:27,843] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-32800/global_step32809/mp_rank_00_model_states.pt...
[2025-03-10 14:49:28,206] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-32800/global_step32809/mp_rank_00_model_states.pt.
[2025-03-10 14:49:28,207] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-32800/global_step32809/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:49:29,419] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-32800/global_step32809/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:49:29,420] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/google-bert/bert-base-cased-num=20/checkpoint-32800/global_step32809/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:49:29,420] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32809 is ready now!
[INFO|trainer.py:2670] 2025-03-10 14:49:29,423 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 3955.448, 'train_samples_per_second': 265.487, 'train_steps_per_second': 8.292, 'train_loss': 0.16596401848444126, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32800/32800 [1:05:55<00:00,  8.29it/s]
[INFO|trainer.py:3955] 2025-03-10 14:49:29,534 >> Saving model checkpoint to output/GNER-QE/google-bert/bert-base-cased-num=20
[INFO|configuration_utils.py:423] 2025-03-10 14:49:29,535 >> Configuration saved in output/GNER-QE/google-bert/bert-base-cased-num=20/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:49:29,807 >> Model weights saved in output/GNER-QE/google-bert/bert-base-cased-num=20/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:49:29,808 >> tokenizer config file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:49:29,808 >> Special tokens file saved in output/GNER-QE/google-bert/bert-base-cased-num=20/special_tokens_map.json
***** train metrics *****
  epoch                    =     39.9994
  total_flos               = 257269459GF
  train_loss               =       0.166
  train_runtime            =  1:05:55.44
  train_samples            =       26253
  train_samples_per_second =     265.487
  train_steps_per_second   =       8.292
03/10/2025 14:49:29 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:930] 2025-03-10 14:49:29,828 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:49:29,830 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:49:29,830 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:49:29,830 >>   Batch size = 4
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 173/173 [00:01<00:00, 96.30it/s]
***** eval metrics *****
  epoch                   =    39.9994
  eval_loss               =     1.1203
  eval_mse                =     1.1201
  eval_pearson            =     0.7275
  eval_runtime            = 0:00:01.81
  eval_samples            =       1384
  eval_samples_per_second =    763.579
  eval_spearmanr          =     0.7452
  eval_steps_per_second   =     95.447
[INFO|modelcard.py:449] 2025-03-10 14:49:31,904 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Spearmanr', 'type': 'spearmanr', 'value': 0.7451709121895067}]}
[rank0]:[W310 14:49:32.025583636 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-03-10 14:49:33,502] [INFO] [launch.py:351:main] Process 4021199 exits successfully.
[2025-03-10 14:49:34,503] [INFO] [launch.py:351:main] Process 4021198 exits successfully.
+ for MODEL_NAME in "${MODEL_NAMES[@]}"
+ python -m deepspeed.launcher.runner --include=localhost:2,3 --master_port 28095 run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=20-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=20-val.json --model_name_or_path FacebookAI/roberta-base --output_dir output/GNER-QE/FacebookAI/roberta-base-num=20 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 14:49:40,334] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 14:49:43,186] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-03-10 14:49:43,186] [INFO] [runner.py:607:main] cmd = /raid/chrisjihee/miniforge3/envs/GNER/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMiwgM119 --master_addr=127.0.0.1 --master_port=28095 --enable_each_rank_log=None run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=20-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=20-val.json --model_name_or_path FacebookAI/roberta-base --output_dir output/GNER-QE/FacebookAI/roberta-base-num=20 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 14:49:47,552] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 14:49:50,492] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [2, 3]}
[2025-03-10 14:49:50,492] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-03-10 14:49:50,492] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-03-10 14:49:50,492] [INFO] [launch.py:164:main] dist_world_size=2
[2025-03-10 14:49:50,492] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=2,3
[2025-03-10 14:49:50,493] [INFO] [launch.py:256:main] process 4097901 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=0', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=20-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=20-val.json', '--model_name_or_path', 'FacebookAI/roberta-base', '--output_dir', 'output/GNER-QE/FacebookAI/roberta-base-num=20', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 14:49:50,493] [INFO] [launch.py:256:main] process 4097902 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=1', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=20-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=20-val.json', '--model_name_or_path', 'FacebookAI/roberta-base', '--output_dir', 'output/GNER-QE/FacebookAI/roberta-base-num=20', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 14:49:55,131] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 14:49:55,339] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 14:49:55,924] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-10 14:49:56,120] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-10 14:49:56,120] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
03/10/2025 14:49:57 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 14:49:57 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/deepspeed/ds1_t5.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.EPOCH,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/GNER-QE/FacebookAI/roberta-base-num=20/runs/Mar10_14-49-55_dgx-a100,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=40.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=output/GNER-QE/FacebookAI/roberta-base-num=20,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output/GNER-QE/FacebookAI/roberta-base-num=20,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.EPOCH,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/10/2025 14:49:58 - INFO - __main__ - load a local file for train: data/GNER-QE/ZSE-validation-pred-by_beam-num=20-train.json
03/10/2025 14:49:58 - INFO - __main__ - load a local file for validation: data/GNER-QE/ZSE-validation-pred-by_beam-num=20-val.json
03/10/2025 14:49:58 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Using custom data configuration default-cc9ee36d8392f1d6
03/10/2025 14:49:58 - INFO - datasets.builder - Using custom data configuration default-cc9ee36d8392f1d6
Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
03/10/2025 14:49:58 - INFO - datasets.info - Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
03/10/2025 14:49:58 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from .cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 14:49:58 - INFO - datasets.info - Loading Dataset info from .cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
03/10/2025 14:49:58 - INFO - datasets.builder - Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 14:49:58 - INFO - datasets.info - Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
[INFO|configuration_utils.py:699] 2025-03-10 14:49:58,861 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:771] 2025-03-10 14:49:58,864 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:699] 2025-03-10 14:49:59,063 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:771] 2025-03-10 14:49:59,063 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:49:59,072 >> loading file vocab.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/vocab.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:49:59,072 >> loading file merges.txt from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/merges.txt
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:49:59,072 >> loading file tokenizer.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:49:59,072 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:49:59,072 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:49:59,072 >> loading file tokenizer_config.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 14:49:59,072 >> loading file chat_template.jinja from cache at None
[INFO|configuration_utils.py:699] 2025-03-10 14:49:59,073 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/config.json
[INFO|configuration_utils.py:771] 2025-03-10 14:49:59,073 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-base",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:4128] 2025-03-10 14:49:59,237 >> loading weights file model.safetensors from cache at .cache/models--FacebookAI--roberta-base/snapshots/e2da8e2f811d1448a5b465c236feacd80ffbac7b/model.safetensors
[INFO|modeling_utils.py:5091] 2025-03-10 14:49:59,285 >> Some weights of the model checkpoint at FacebookAI/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:5103] 2025-03-10 14:49:59,285 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/26253 [00:00<?, ? examples/s][WARNING|modeling_utils.py:5103] 2025-03-10 14:49:59,318 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-5903bd8272e685d7.arrow
03/10/2025 14:49:59 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-5903bd8272e685d7.arrow
Running tokenizer on dataset:  30%|████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                   | 8000/26253 [00:02<00:04, 4041.65 examples/s][rank1]:[W310 14:50:01.437671722 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26253/26253 [00:06<00:00, 3987.52 examples/s]
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                              | 0/1384 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-58bd1001cbc345d7.arrow
03/10/2025 14:50:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-58bd1001cbc345d7.arrow
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1384/1384 [00:00<00:00, 4547.89 examples/s]
[rank0]:[W310 14:50:08.457779483 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
03/10/2025 14:50:09 - INFO - __main__ - Sample 20952 of the training set: {'sentence1': 'who(B-actor) starred(O) in(O) marley(B-title) and(I-title) me(I-title)', 'sentence2': 'who starred in marley and me', 'label': 2.71, 'idx': 20952, 'input_ids': [0, 8155, 1640, 387, 12, 24625, 43, 12913, 1640, 673, 43, 11, 1640, 673, 43, 4401, 607, 1640, 387, 12, 14691, 43, 8, 1640, 100, 12, 14691, 43, 162, 1640, 100, 12, 14691, 43, 2, 2, 8155, 12913, 11, 4401, 607, 8, 162, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 14:50:09 - INFO - __main__ - Sample 3648 of the training set: {'sentence1': 'The(O) Spanish(B-language) edition(O) of(O) Campus(B-conference) Party(I-conference) has(O) been(O) held(O) at(O) the(O) Colegio(B-university) Miguel(I-university) Hernández(I-university),(O) Ceulaj(B-location),(O) and(O) the(O) Municipal(B-location) Sport(I-location) Arena(I-location) of(I-location) Benalmádena(I-location) in(O) Málaga(B-location),(O) Spain(B-country) ;(O) and(O) at(O) both(O) the(O) Valencia(B-location) County(I-location) Fair(I-location) and(O) the(O) City(B-location) of(I-location) Arts(I-location) and(I-location) Sciences(I-location) in(O) Valencia(B-location) over(O) the(O) past(O) 15(O) years(O).(O)', 'sentence2': 'The Spanish edition of Campus Party has been held at the Colegio Miguel Hernández , Ceulaj , and the Municipal Sport Arena of Benalmádena in Málaga , Spain ; and at both the Valencia County Fair and the City of Arts and Sciences in Valencia over the past 15 years .', 'label': 3.32, 'idx': 3648, 'input_ids': [0, 133, 1640, 673, 43, 3453, 1640, 387, 12, 19527, 43, 5403, 1640, 673, 43, 9, 1640, 673, 43, 14416, 1640, 387, 12, 16459, 43, 1643, 1640, 100, 12, 16459, 43, 34, 1640, 673, 43, 57, 1640, 673, 43, 547, 1640, 673, 43, 23, 1640, 673, 43, 5, 1640, 673, 43, 944, 4308, 1020, 1640, 387, 12, 879, 31104, 43, 12450, 1640, 100, 12, 879, 31104, 43, 289, 3281, 1526, 1187, 5841, 1640, 100, 12, 879, 31104, 238, 1640, 673, 43, 15858, 922, 1176, 1640, 387, 12, 41829, 238, 1640, 673, 43, 8, 1640, 673, 43, 5, 1640, 673, 43, 11660, 1640, 387, 12, 41829, 43, 5413, 1640, 100, 12, 41829, 43, 5036, 1640, 100, 12, 41829, 43, 9, 1640, 100, 12, 41829, 43, 1664, 30293, 1526, 3898, 102, 1640, 100, 12, 41829, 43, 11, 1640, 673, 43, 256, 1526, 462, 6080, 1640, 387, 12, 41829, 238, 1640, 673, 43, 2809, 1640, 387, 12, 12659, 43, 25606, 1640, 673, 43, 8, 1640, 673, 43, 23, 1640, 673, 43, 258, 1640, 673, 43, 5, 1640, 673, 43, 14567, 1640, 387, 12, 41829, 43, 413, 1640, 100, 12, 41829, 43, 3896, 1640, 100, 12, 41829, 43, 8, 1640, 673, 43, 5, 1640, 673, 43, 412, 1640, 387, 12, 41829, 43, 9, 1640, 100, 12, 41829, 43, 4455, 1640, 100, 12, 41829, 43, 8, 1640, 100, 12, 41829, 43, 8841, 1640, 100, 12, 41829, 43, 11, 1640, 673, 43, 14567, 1640, 387, 12, 41829, 43, 81, 1640, 673, 43, 5, 1640, 673, 43, 375, 1640, 673, 43, 379, 1640, 673, 43, 107, 1640, 673, 322, 1640, 673, 43, 2, 2, 133, 3453, 5403, 9, 14416, 1643, 34, 57, 547, 23, 5, 944, 4308, 1020, 12450, 289, 3281, 1526, 1187, 5841, 2156, 15858, 922, 1176, 2156, 8, 5, 11660, 5413, 5036, 9, 1664, 30293, 1526, 3898, 102, 11, 256, 1526, 462, 6080, 2156, 2809, 25606, 8, 23, 258, 5, 14567, 413, 3896, 8, 5, 412, 9, 4455, 8, 8841, 11, 14567, 81, 5, 375, 379, 107, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 14:50:09 - INFO - __main__ - Sample 819 of the training set: {'sentence1': 'Two(O) professors(O),(O) Hal(B-person) Abelson(I-person) and(O) Gerald(B-person) Jay(I-person) Sussman(I-person),(O) chose(O) to(O) remain(O) neutral(O) -(O) their(O) group(O) was(O) referred(O) to(O) variously(O) as(O) Switzerland(B-country) and(O) Project(B-event) MAC(I-event) for(O) the(O) next(O) 30(O) years(O).(O)', 'sentence2': 'Two professors , Hal Abelson and Gerald Jay Sussman , chose to remain neutral - their group was referred to variously as Switzerland and Project MAC for the next 30 years .', 'label': 1.16, 'idx': 819, 'input_ids': [0, 9058, 1640, 673, 43, 18341, 1640, 673, 238, 1640, 673, 43, 6579, 1640, 387, 12, 5970, 43, 2060, 16475, 1640, 100, 12, 5970, 43, 8, 1640, 673, 43, 14651, 1640, 387, 12, 5970, 43, 3309, 1640, 100, 12, 5970, 43, 208, 4781, 397, 1640, 100, 12, 5970, 238, 1640, 673, 43, 4689, 1640, 673, 43, 7, 1640, 673, 43, 1091, 1640, 673, 43, 7974, 1640, 673, 43, 111, 1640, 673, 43, 49, 1640, 673, 43, 333, 1640, 673, 43, 21, 1640, 673, 43, 4997, 1640, 673, 43, 7, 1640, 673, 43, 1337, 352, 1640, 673, 43, 25, 1640, 673, 43, 6413, 1640, 387, 12, 12659, 43, 8, 1640, 673, 43, 3728, 1640, 387, 12, 21680, 43, 19482, 1640, 100, 12, 21680, 43, 13, 1640, 673, 43, 5, 1640, 673, 43, 220, 1640, 673, 43, 389, 1640, 673, 43, 107, 1640, 673, 322, 1640, 673, 43, 2, 2, 9058, 18341, 2156, 6579, 2060, 16475, 8, 14651, 3309, 208, 4781, 397, 2156, 4689, 7, 1091, 7974, 111, 49, 333, 21, 4997, 7, 1337, 352, 25, 6413, 8, 3728, 19482, 13, 5, 220, 389, 107, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/26253 [00:00<?, ? examples/s][INFO|trainer.py:748] 2025-03-10 14:50:09,408 >> Using auto half precision backend
[INFO|trainer.py:930] 2025-03-10 14:50:09,655 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[2025-03-10 14:50:09,660] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
[2025-03-10 14:50:09,660] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26253/26253 [00:07<00:00, 3743.64 examples/s]
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1384/1384 [00:00<00:00, 4266.02 examples/s]
[2025-03-10 14:50:18,948] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-03-10 14:50:18,950] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-10 14:50:18,951] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-03-10 14:50:18,955] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-10 14:50:18,955] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-10 14:50:18,955] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer
[2025-03-10 14:50:18,955] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-03-10 14:50:18,955] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-03-10 14:50:18,955] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-03-10 14:50:18,955] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-03-10 14:50:19,317] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 14:50:19,520] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-03-10 14:50:19,520] [INFO] [utils.py:782:see_memory_usage] MA 0.46 GB         Max_MA 0.58 GB         CA 0.58 GB         Max_CA 1 GB
[2025-03-10 14:50:19,521] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.9 GB, percent = 5.2%
[2025-03-10 14:50:19,739] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-03-10 14:50:19,740] [INFO] [utils.py:782:see_memory_usage] MA 0.46 GB         Max_MA 0.7 GB         CA 0.82 GB         Max_CA 1 GB
[2025-03-10 14:50:19,740] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.94 GB, percent = 5.3%
[2025-03-10 14:50:19,740] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized
[2025-03-10 14:50:19,934] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-03-10 14:50:19,934] [INFO] [utils.py:782:see_memory_usage] MA 0.46 GB         Max_MA 0.46 GB         CA 0.82 GB         Max_CA 1 GB
[2025-03-10 14:50:19,935] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 52.94 GB, percent = 5.3%
[2025-03-10 14:50:19,936] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-03-10 14:50:19,936] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 14:50:19,936] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2025-03-10 14:50:19,936] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f1282912c60>
[2025-03-10 14:50:19,937] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0], mom=[[0.9, 0.999]]
[2025-03-10 14:50:19,937] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-03-10 14:50:19,937] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f1282a4f800>
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-03-10 14:50:19,938] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 4
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-10 14:50:19,939] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   loss_scale ................... 1.0
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   optimizer_name ............... adamw
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   optimizer_params ............. {'lr': 2e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   scheduler_name ............... WarmupLR
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 0}
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   steps_per_print .............. inf
[2025-03-10 14:50:19,940] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-03-10 14:50:19,941] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-03-10 14:50:19,941] [INFO] [config.py:1005:print]   train_batch_size ............. 32
[2025-03-10 14:50:19,941] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  4
[2025-03-10 14:50:19,941] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-03-10 14:50:19,941] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-03-10 14:50:19,941] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-03-10 14:50:19,941] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-03-10 14:50:19,941] [INFO] [config.py:1005:print]   world_size ................... 2
[2025-03-10 14:50:19,941] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False
[2025-03-10 14:50:19,941] [INFO] [config.py:1005:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-03-10 14:50:19,941] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2025-03-10 14:50:19,941] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-10 14:50:19,941] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 1
[2025-03-10 14:50:19,941] [INFO] [config.py:991:print_user_config]   json = {
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-05,
            "betas": [0.9, 0.999],
            "eps": 1e-08,
            "weight_decay": 0.0
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-05,
            "warmup_num_steps": 0
        }
    },
    "zero_optimization": {
        "stage": 1,
        "allgather_partitions": true,
        "allgather_bucket_size": 2.000000e+08,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2.000000e+08,
        "contiguous_gradients": true
    },
    "gradient_accumulation_steps": 4,
    "gradient_clipping": 1.0,
    "train_batch_size": 32,
    "train_micro_batch_size_per_gpu": 4,
    "steps_per_print": inf
}
[INFO|trainer.py:2414] 2025-03-10 14:50:19,942 >> ***** Running training *****
[INFO|trainer.py:2415] 2025-03-10 14:50:19,942 >>   Num examples = 26,253
[INFO|trainer.py:2416] 2025-03-10 14:50:19,942 >>   Num Epochs = 40
[INFO|trainer.py:2417] 2025-03-10 14:50:19,942 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2420] 2025-03-10 14:50:19,942 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2421] 2025-03-10 14:50:19,943 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2422] 2025-03-10 14:50:19,943 >>   Total optimization steps = 32,800
[INFO|trainer.py:2423] 2025-03-10 14:50:19,943 >>   Number of trainable parameters = 124,646,401
{'loss': 1.2158, 'grad_norm': 28.10597801208496, 'learning_rate': 2e-05, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                             | 820/32800 [01:45<1:06:37,  8.00it/s][INFO|trainer.py:930] 2025-03-10 14:52:05,368 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:52:05,372 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:52:05,372 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:52:05,372 >>   Batch size = 4
{'eval_loss': 1.2336480617523193, 'eval_mse': 1.2334453642712853, 'eval_pearson': 0.6612798243363125, 'eval_spearmanr': 0.6653565106107745, 'eval_runtime': 1.5785, 'eval_samples_per_second': 876.765, 'eval_steps_per_second': 109.596, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                             | 820/32800 [01:47<1:06:37,  8.00it/s]
[INFO|trainer.py:3955] 2025-03-10 14:52:07,108 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-820
[INFO|configuration_utils.py:423] 2025-03-10 14:52:07,110 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-820/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:52:07,351 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-820/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:52:07,352 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-820/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:52:07,352 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-820/special_tokens_map.json
[2025-03-10 14:52:07,396] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step820 is about to be saved!
[2025-03-10 14:52:07,400] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-820/global_step820/mp_rank_00_model_states.pt
[2025-03-10 14:52:07,400] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-820/global_step820/mp_rank_00_model_states.pt...
[2025-03-10 14:52:07,720] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-820/global_step820/mp_rank_00_model_states.pt.
[2025-03-10 14:52:07,721] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-820/global_step820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:52:08,796] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-820/global_step820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:52:08,796] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-820/global_step820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:52:08,796] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step820 is ready now!
{'loss': 0.7954, 'grad_norm': 13.023198127746582, 'learning_rate': 2e-05, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                    | 1640/32800 [03:29<1:03:56,  8.12it/s][INFO|trainer.py:930] 2025-03-10 14:53:49,411 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:53:49,417 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:53:49,417 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:53:49,417 >>   Batch size = 4
{'eval_loss': 1.3399388790130615, 'eval_mse': 1.3397989435002984, 'eval_pearson': 0.6323080113982251, 'eval_spearmanr': 0.6396910917652072, 'eval_runtime': 1.6237, 'eval_samples_per_second': 852.382, 'eval_steps_per_second': 106.548, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                    | 1640/32800 [03:31<1:03:56,  8.12it/s]
[INFO|trainer.py:3955] 2025-03-10 14:53:51,122 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-1640
[INFO|configuration_utils.py:423] 2025-03-10 14:53:51,125 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-1640/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:53:51,366 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-1640/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:53:51,367 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-1640/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:53:51,367 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-1640/special_tokens_map.json
[2025-03-10 14:53:51,414] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1640 is about to be saved!
[2025-03-10 14:53:51,417] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-1640/global_step1640/mp_rank_00_model_states.pt
[2025-03-10 14:53:51,418] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-1640/global_step1640/mp_rank_00_model_states.pt...
[2025-03-10 14:53:51,746] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-1640/global_step1640/mp_rank_00_model_states.pt.
[2025-03-10 14:53:51,747] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-1640/global_step1640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:53:52,813] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-1640/global_step1640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:53:52,813] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-1640/global_step1640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:53:52,813] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1640 is ready now!
{'loss': 0.7052, 'grad_norm': 8.244956016540527, 'learning_rate': 2e-05, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 2460/32800 [05:14<1:02:13,  8.13it/s][INFO|trainer.py:930] 2025-03-10 14:55:34,326 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:55:34,329 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:55:34,329 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:55:34,329 >>   Batch size = 4
{'eval_loss': 1.247873067855835, 'eval_mse': 1.2481342095860166, 'eval_pearson': 0.6654506536799536, 'eval_spearmanr': 0.6631111601608854, 'eval_runtime': 1.5635, 'eval_samples_per_second': 885.169, 'eval_steps_per_second': 110.646, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 2460/32800 [05:15<1:02:13,  8.13it/s]
[INFO|trainer.py:3955] 2025-03-10 14:55:36,028 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-2460
[INFO|configuration_utils.py:423] 2025-03-10 14:55:36,030 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-2460/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:55:36,272 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-2460/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:55:36,273 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-2460/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:55:36,273 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-2460/special_tokens_map.json
[2025-03-10 14:55:36,324] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2460 is about to be saved!
[2025-03-10 14:55:36,327] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-2460/global_step2460/mp_rank_00_model_states.pt
[2025-03-10 14:55:36,327] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-2460/global_step2460/mp_rank_00_model_states.pt...
[2025-03-10 14:55:36,651] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-2460/global_step2460/mp_rank_00_model_states.pt.
[2025-03-10 14:55:36,652] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-2460/global_step2460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:55:37,674] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-2460/global_step2460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:55:37,675] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-2460/global_step2460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:55:37,675] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2460 is ready now!
{'loss': 0.6872, 'grad_norm': 3.5099453926086426, 'learning_rate': 2e-05, 'epoch': 4.0}
 10%|██████████████████████████████████▎                                                                                                                                                                                                                                                                                                                    | 3280/32800 [06:57<59:55,  8.21it/s][INFO|trainer.py:930] 2025-03-10 14:57:17,765 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:57:17,769 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:57:17,769 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:57:17,769 >>   Batch size = 4
{'eval_loss': 1.4869328737258911, 'eval_mse': 1.4868594563076263, 'eval_pearson': 0.6410169053102877, 'eval_spearmanr': 0.6430453389598536, 'eval_runtime': 1.5608, 'eval_samples_per_second': 886.742, 'eval_steps_per_second': 110.843, 'epoch': 4.0}
 10%|██████████████████████████████████▎                                                                                                                                                                                                                                                                                                                    | 3280/32800 [06:59<59:55,  8.21it/s]
[INFO|trainer.py:3955] 2025-03-10 14:57:19,471 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-3280
[INFO|configuration_utils.py:423] 2025-03-10 14:57:19,473 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-3280/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:57:19,762 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-3280/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:57:19,763 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-3280/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:57:19,763 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-3280/special_tokens_map.json
[2025-03-10 14:57:19,809] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3281 is about to be saved!
[2025-03-10 14:57:19,813] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-3280/global_step3281/mp_rank_00_model_states.pt
[2025-03-10 14:57:19,813] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-3280/global_step3281/mp_rank_00_model_states.pt...
[2025-03-10 14:57:20,170] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-3280/global_step3281/mp_rank_00_model_states.pt.
[2025-03-10 14:57:20,171] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-3280/global_step3281/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:57:21,267] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-3280/global_step3281/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:57:21,268] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-3280/global_step3281/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:57:21,268] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3281 is ready now!
{'loss': 0.5413, 'grad_norm': 21.13585662841797, 'learning_rate': 2e-05, 'epoch': 5.0}
 12%|██████████████████████████████████████████▉                                                                                                                                                                                                                                                                                                            | 4100/32800 [08:38<54:03,  8.85it/s][INFO|trainer.py:930] 2025-03-10 14:58:58,656 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 14:58:58,659 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 14:58:58,659 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 14:58:58,659 >>   Batch size = 4
{'eval_loss': 1.2181007862091064, 'eval_mse': 1.218090325077145, 'eval_pearson': 0.6613940826441314, 'eval_spearmanr': 0.6523995529539782, 'eval_runtime': 1.5684, 'eval_samples_per_second': 882.449, 'eval_steps_per_second': 110.306, 'epoch': 5.0}
 12%|██████████████████████████████████████████▉                                                                                                                                                                                                                                                                                                            | 4100/32800 [08:40<54:03,  8.85it/s]
[INFO|trainer.py:3955] 2025-03-10 14:59:00,362 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4100
[INFO|configuration_utils.py:423] 2025-03-10 14:59:00,365 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4100/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 14:59:00,599 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4100/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 14:59:00,600 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 14:59:00,600 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4100/special_tokens_map.json
[2025-03-10 14:59:00,648] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4101 is about to be saved!
[2025-03-10 14:59:00,651] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4100/global_step4101/mp_rank_00_model_states.pt
[2025-03-10 14:59:00,651] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4100/global_step4101/mp_rank_00_model_states.pt...
[2025-03-10 14:59:00,969] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4100/global_step4101/mp_rank_00_model_states.pt.
[2025-03-10 14:59:00,970] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4100/global_step4101/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 14:59:02,014] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4100/global_step4101/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 14:59:02,015] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4100/global_step4101/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 14:59:02,015] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4101 is ready now!
{'loss': 0.4529, 'grad_norm': 25.928796768188477, 'learning_rate': 2e-05, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                                   | 4920/32800 [10:14<51:56,  8.95it/s][INFO|trainer.py:930] 2025-03-10 15:00:34,190 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:00:34,193 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:00:34,193 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:00:34,193 >>   Batch size = 4
{'eval_loss': 1.6348859071731567, 'eval_mse': 1.6352464584256872, 'eval_pearson': 0.6644030472980499, 'eval_spearmanr': 0.6702055230243394, 'eval_runtime': 1.5501, 'eval_samples_per_second': 892.873, 'eval_steps_per_second': 111.609, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▍                                                                                                                                                                                                                                                                                                   | 4920/32800 [10:15<51:56,  8.95it/s]
[INFO|trainer.py:3955] 2025-03-10 15:00:35,883 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4920
[INFO|configuration_utils.py:423] 2025-03-10 15:00:35,885 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4920/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:00:36,134 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4920/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:00:36,134 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4920/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:00:36,135 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4920/special_tokens_map.json
[2025-03-10 15:00:36,183] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4921 is about to be saved!
[2025-03-10 15:00:36,186] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4920/global_step4921/mp_rank_00_model_states.pt
[2025-03-10 15:00:36,186] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4920/global_step4921/mp_rank_00_model_states.pt...
[2025-03-10 15:00:36,516] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4920/global_step4921/mp_rank_00_model_states.pt.
[2025-03-10 15:00:36,517] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4920/global_step4921/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:00:37,639] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4920/global_step4921/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:00:37,640] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-4920/global_step4921/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:00:37,640] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4921 is ready now!
{'loss': 0.4181, 'grad_norm': 8.597796440124512, 'learning_rate': 2e-05, 'epoch': 7.0}
 18%|████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                           | 5740/32800 [11:49<50:35,  8.92it/s][INFO|trainer.py:930] 2025-03-10 15:02:09,818 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:02:09,821 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:02:09,821 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:02:09,821 >>   Batch size = 4
{'eval_loss': 1.4489744901657104, 'eval_mse': 1.4484057919138429, 'eval_pearson': 0.6667616351203584, 'eval_spearmanr': 0.6666832310077828, 'eval_runtime': 1.5666, 'eval_samples_per_second': 883.422, 'eval_steps_per_second': 110.428, 'epoch': 7.0}
 18%|████████████████████████████████████████████████████████████                                                                                                                                                                                                                                                                                           | 5740/32800 [11:51<50:35,  8.92it/s]
[INFO|trainer.py:3955] 2025-03-10 15:02:11,525 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-5740
[INFO|configuration_utils.py:423] 2025-03-10 15:02:11,527 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-5740/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:02:11,779 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-5740/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:02:11,779 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-5740/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:02:11,780 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-5740/special_tokens_map.json
[2025-03-10 15:02:11,829] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step5741 is about to be saved!
[2025-03-10 15:02:11,832] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-5740/global_step5741/mp_rank_00_model_states.pt
[2025-03-10 15:02:11,832] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-5740/global_step5741/mp_rank_00_model_states.pt...
[2025-03-10 15:02:12,169] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-5740/global_step5741/mp_rank_00_model_states.pt.
[2025-03-10 15:02:12,170] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-5740/global_step5741/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:02:13,275] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-5740/global_step5741/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:02:13,276] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-5740/global_step5741/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:02:13,276] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5741 is ready now!
{'loss': 0.4332, 'grad_norm': 4.441623210906982, 'learning_rate': 2e-05, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                  | 6560/32800 [13:25<48:52,  8.95it/s][INFO|trainer.py:930] 2025-03-10 15:03:45,115 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:03:45,118 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:03:45,118 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:03:45,118 >>   Batch size = 4
{'eval_loss': 1.2630763053894043, 'eval_mse': 1.2627506369800237, 'eval_pearson': 0.6798242255315513, 'eval_spearmanr': 0.6856945793689032, 'eval_runtime': 1.5516, 'eval_samples_per_second': 891.995, 'eval_steps_per_second': 111.499, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                                                                  | 6560/32800 [13:26<48:52,  8.95it/s]
[INFO|trainer.py:3955] 2025-03-10 15:03:46,804 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-6560
[INFO|configuration_utils.py:423] 2025-03-10 15:03:46,806 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-6560/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:03:47,070 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-6560/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:03:47,070 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-6560/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:03:47,071 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-6560/special_tokens_map.json
[2025-03-10 15:03:47,116] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6562 is about to be saved!
[2025-03-10 15:03:47,120] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-6560/global_step6562/mp_rank_00_model_states.pt
[2025-03-10 15:03:47,120] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-6560/global_step6562/mp_rank_00_model_states.pt...
[2025-03-10 15:03:47,468] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-6560/global_step6562/mp_rank_00_model_states.pt.
[2025-03-10 15:03:47,470] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-6560/global_step6562/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:03:48,597] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-6560/global_step6562/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:03:48,598] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-6560/global_step6562/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:03:48,598] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6562 is ready now!
{'loss': 0.3427, 'grad_norm': 11.885916709899902, 'learning_rate': 2e-05, 'epoch': 9.0}
 22%|█████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                         | 7380/32800 [15:01<47:38,  8.89it/s][INFO|trainer.py:930] 2025-03-10 15:05:21,261 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:05:21,264 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:05:21,264 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:05:21,264 >>   Batch size = 4
{'eval_loss': 1.1723687648773193, 'eval_mse': 1.1721127498356594, 'eval_pearson': 0.7038981972137128, 'eval_spearmanr': 0.7021502595258915, 'eval_runtime': 1.5546, 'eval_samples_per_second': 890.288, 'eval_steps_per_second': 111.286, 'epoch': 9.0}
 22%|█████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                         | 7380/32800 [15:02<47:38,  8.89it/s]
[INFO|trainer.py:3955] 2025-03-10 15:05:22,953 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-7380
[INFO|configuration_utils.py:423] 2025-03-10 15:05:22,955 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-7380/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:05:23,223 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-7380/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:05:23,224 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-7380/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:05:23,224 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-7380/special_tokens_map.json
[2025-03-10 15:05:23,272] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7382 is about to be saved!
[2025-03-10 15:05:23,276] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-7380/global_step7382/mp_rank_00_model_states.pt
[2025-03-10 15:05:23,276] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-7380/global_step7382/mp_rank_00_model_states.pt...
[2025-03-10 15:05:23,620] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-7380/global_step7382/mp_rank_00_model_states.pt.
[2025-03-10 15:05:23,621] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-7380/global_step7382/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:05:24,781] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-7380/global_step7382/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:05:24,781] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-7380/global_step7382/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:05:24,781] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7382 is ready now!
{'loss': 0.2885, 'grad_norm': 7.919098377227783, 'learning_rate': 2e-05, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                                 | 8200/32800 [16:36<45:54,  8.93it/s][INFO|trainer.py:930] 2025-03-10 15:06:56,769 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:06:56,772 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:06:56,772 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:06:56,772 >>   Batch size = 4
{'eval_loss': 1.0929765701293945, 'eval_mse': 1.0931983342060465, 'eval_pearson': 0.7123445470059485, 'eval_spearmanr': 0.7192088676426869, 'eval_runtime': 1.5458, 'eval_samples_per_second': 895.313, 'eval_steps_per_second': 111.914, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                                 | 8200/32800 [16:38<45:54,  8.93it/s]
[INFO|trainer.py:3955] 2025-03-10 15:06:58,453 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-8200
[INFO|configuration_utils.py:423] 2025-03-10 15:06:58,455 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-8200/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:06:58,719 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-8200/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:06:58,720 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-8200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:06:58,720 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-8200/special_tokens_map.json
[2025-03-10 15:06:58,767] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8202 is about to be saved!
[2025-03-10 15:06:58,771] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-8200/global_step8202/mp_rank_00_model_states.pt
[2025-03-10 15:06:58,771] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-8200/global_step8202/mp_rank_00_model_states.pt...
[2025-03-10 15:06:59,114] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-8200/global_step8202/mp_rank_00_model_states.pt.
[2025-03-10 15:06:59,116] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-8200/global_step8202/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:07:00,243] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-8200/global_step8202/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:07:00,244] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-8200/global_step8202/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:07:00,244] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8202 is ready now!
{'loss': 0.2699, 'grad_norm': 6.915921211242676, 'learning_rate': 2e-05, 'epoch': 11.0}
 28%|██████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                                                        | 9020/32800 [18:13<44:28,  8.91it/s][INFO|trainer.py:930] 2025-03-10 15:08:33,681 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:08:33,684 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:08:33,684 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:08:33,684 >>   Batch size = 4
{'eval_loss': 1.2115463018417358, 'eval_mse': 1.2116554365681775, 'eval_pearson': 0.7083863443904885, 'eval_spearmanr': 0.7118205792730711, 'eval_runtime': 1.5519, 'eval_samples_per_second': 891.815, 'eval_steps_per_second': 111.477, 'epoch': 11.0}
 28%|██████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                                                        | 9020/32800 [18:15<44:28,  8.91it/s]
[INFO|trainer.py:3955] 2025-03-10 15:08:35,369 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9020
[INFO|configuration_utils.py:423] 2025-03-10 15:08:35,371 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9020/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:08:35,637 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9020/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:08:35,638 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9020/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:08:35,638 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9020/special_tokens_map.json
[2025-03-10 15:08:35,686] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9022 is about to be saved!
[2025-03-10 15:08:35,690] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9020/global_step9022/mp_rank_00_model_states.pt
[2025-03-10 15:08:35,690] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9020/global_step9022/mp_rank_00_model_states.pt...
[2025-03-10 15:08:36,037] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9020/global_step9022/mp_rank_00_model_states.pt.
[2025-03-10 15:08:36,038] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9020/global_step9022/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:08:37,172] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9020/global_step9022/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:08:37,172] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9020/global_step9022/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:08:37,173] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9022 is ready now!
{'loss': 0.2795, 'grad_norm': 5.527979373931885, 'learning_rate': 2e-05, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                                                | 9840/32800 [19:49<42:30,  9.00it/s][INFO|trainer.py:930] 2025-03-10 15:10:09,172 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:10:09,176 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:10:09,176 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:10:09,176 >>   Batch size = 4
{'eval_loss': 1.3747836351394653, 'eval_mse': 1.3748503802828707, 'eval_pearson': 0.67662557032658, 'eval_spearmanr': 0.6831332444539288, 'eval_runtime': 1.546, 'eval_samples_per_second': 895.186, 'eval_steps_per_second': 111.898, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                                                | 9840/32800 [19:50<42:30,  9.00it/s]
[INFO|trainer.py:3955] 2025-03-10 15:10:10,856 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9840
[INFO|configuration_utils.py:423] 2025-03-10 15:10:10,858 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9840/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:10:11,116 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9840/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:10:11,117 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9840/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:10:11,117 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9840/special_tokens_map.json
[2025-03-10 15:10:11,165] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9843 is about to be saved!
[2025-03-10 15:10:11,168] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9840/global_step9843/mp_rank_00_model_states.pt
[2025-03-10 15:10:11,168] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9840/global_step9843/mp_rank_00_model_states.pt...
[2025-03-10 15:10:11,569] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9840/global_step9843/mp_rank_00_model_states.pt.
[2025-03-10 15:10:11,570] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9840/global_step9843/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:10:12,725] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9840/global_step9843/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:10:12,726] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-9840/global_step9843/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:10:12,726] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9843 is ready now!
{'loss': 0.222, 'grad_norm': 21.865537643432617, 'learning_rate': 2e-05, 'epoch': 13.0}
 32%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                      | 10660/32800 [21:25<41:46,  8.83it/s][INFO|trainer.py:930] 2025-03-10 15:11:45,201 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:11:45,205 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:11:45,205 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:11:45,205 >>   Batch size = 4
{'eval_loss': 1.1986773014068604, 'eval_mse': 1.1983860169531981, 'eval_pearson': 0.6917521301424845, 'eval_spearmanr': 0.6992191492911365, 'eval_runtime': 1.5612, 'eval_samples_per_second': 886.489, 'eval_steps_per_second': 110.811, 'epoch': 13.0}
 32%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                      | 10660/32800 [21:26<41:46,  8.83it/s]
[INFO|trainer.py:3955] 2025-03-10 15:11:46,901 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-10660
[INFO|configuration_utils.py:423] 2025-03-10 15:11:46,903 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-10660/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:11:47,169 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-10660/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:11:47,170 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-10660/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:11:47,170 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-10660/special_tokens_map.json
[2025-03-10 15:11:47,219] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step10663 is about to be saved!
[2025-03-10 15:11:47,222] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-10660/global_step10663/mp_rank_00_model_states.pt
[2025-03-10 15:11:47,222] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-10660/global_step10663/mp_rank_00_model_states.pt...
[2025-03-10 15:11:47,563] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-10660/global_step10663/mp_rank_00_model_states.pt.
[2025-03-10 15:11:47,565] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-10660/global_step10663/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:11:48,701] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-10660/global_step10663/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:11:48,702] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-10660/global_step10663/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:11:48,702] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10663 is ready now!
{'loss': 0.1857, 'grad_norm': 9.05897045135498, 'learning_rate': 2e-05, 'epoch': 14.0}
 35%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                              | 11480/32800 [23:00<39:48,  8.93it/s][INFO|trainer.py:930] 2025-03-10 15:13:20,638 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:13:20,641 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:13:20,641 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:13:20,641 >>   Batch size = 4
{'eval_loss': 1.2358362674713135, 'eval_mse': 1.2358615711934304, 'eval_pearson': 0.6853062885756978, 'eval_spearmanr': 0.7018147842151398, 'eval_runtime': 1.5507, 'eval_samples_per_second': 892.506, 'eval_steps_per_second': 111.563, 'epoch': 14.0}
 35%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                              | 11480/32800 [23:02<39:48,  8.93it/s]
[INFO|trainer.py:3955] 2025-03-10 15:13:22,326 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-11480
[INFO|configuration_utils.py:423] 2025-03-10 15:13:22,328 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-11480/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:13:22,592 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-11480/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:13:22,593 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-11480/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:13:22,593 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-11480/special_tokens_map.json
[2025-03-10 15:13:22,640] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11483 is about to be saved!
[2025-03-10 15:13:22,644] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-11480/global_step11483/mp_rank_00_model_states.pt
[2025-03-10 15:13:22,644] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-11480/global_step11483/mp_rank_00_model_states.pt...
[2025-03-10 15:13:22,990] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-11480/global_step11483/mp_rank_00_model_states.pt.
[2025-03-10 15:13:22,991] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-11480/global_step11483/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:13:24,126] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-11480/global_step11483/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:13:24,126] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-11480/global_step11483/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:13:24,126] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11483 is ready now!
{'loss': 0.1832, 'grad_norm': 20.29997444152832, 'learning_rate': 2e-05, 'epoch': 15.0}
 38%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                     | 12300/32800 [24:36<38:27,  8.88it/s][INFO|trainer.py:930] 2025-03-10 15:14:56,506 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:14:56,509 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:14:56,509 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:14:56,509 >>   Batch size = 4
{'eval_loss': 1.160454511642456, 'eval_mse': 1.1603516894957924, 'eval_pearson': 0.7009608243912643, 'eval_spearmanr': 0.7112885592456507, 'eval_runtime': 1.5763, 'eval_samples_per_second': 878.013, 'eval_steps_per_second': 109.752, 'epoch': 15.0}
 38%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                     | 12300/32800 [24:38<38:27,  8.88it/s]
[INFO|trainer.py:3955] 2025-03-10 15:14:58,220 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-12300
[INFO|configuration_utils.py:423] 2025-03-10 15:14:58,222 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-12300/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:14:58,488 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-12300/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:14:58,489 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-12300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:14:58,489 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-12300/special_tokens_map.json
[2025-03-10 15:14:58,535] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step12303 is about to be saved!
[2025-03-10 15:14:58,538] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-12300/global_step12303/mp_rank_00_model_states.pt
[2025-03-10 15:14:58,538] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-12300/global_step12303/mp_rank_00_model_states.pt...
[2025-03-10 15:14:58,880] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-12300/global_step12303/mp_rank_00_model_states.pt.
[2025-03-10 15:14:58,881] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-12300/global_step12303/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:15:00,045] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-12300/global_step12303/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:15:00,046] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-12300/global_step12303/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:15:00,046] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12303 is ready now!
{'loss': 0.1955, 'grad_norm': 9.811613082885742, 'learning_rate': 2e-05, 'epoch': 16.0}
 40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                             | 13120/32800 [26:11<36:40,  8.94it/s][INFO|trainer.py:930] 2025-03-10 15:16:31,861 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:16:31,864 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:16:31,864 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:16:31,864 >>   Batch size = 4
{'eval_loss': 1.1536482572555542, 'eval_mse': 1.1531298722145875, 'eval_pearson': 0.7003084220431987, 'eval_spearmanr': 0.7054830760621138, 'eval_runtime': 1.5469, 'eval_samples_per_second': 894.698, 'eval_steps_per_second': 111.837, 'epoch': 16.0}
 40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                             | 13120/32800 [26:13<36:40,  8.94it/s]
[INFO|trainer.py:3955] 2025-03-10 15:16:33,546 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13120
[INFO|configuration_utils.py:423] 2025-03-10 15:16:33,548 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13120/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:16:33,813 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13120/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:16:33,814 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13120/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:16:33,814 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13120/special_tokens_map.json
[2025-03-10 15:16:33,854] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13124 is about to be saved!
[2025-03-10 15:16:33,858] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13120/global_step13124/mp_rank_00_model_states.pt
[2025-03-10 15:16:33,858] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13120/global_step13124/mp_rank_00_model_states.pt...
[2025-03-10 15:16:34,198] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13120/global_step13124/mp_rank_00_model_states.pt.
[2025-03-10 15:16:34,199] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13120/global_step13124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:16:35,391] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13120/global_step13124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:16:35,391] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13120/global_step13124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:16:35,391] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13124 is ready now!
{'loss': 0.1512, 'grad_norm': 4.583343029022217, 'learning_rate': 2e-05, 'epoch': 17.0}
 42%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                    | 13940/32800 [27:47<35:29,  8.86it/s][INFO|trainer.py:930] 2025-03-10 15:18:07,776 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:18:07,779 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:18:07,779 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:18:07,779 >>   Batch size = 4
{'eval_loss': 1.1171430349349976, 'eval_mse': 1.1166760676858054, 'eval_pearson': 0.7062862084211131, 'eval_spearmanr': 0.7160576743974563, 'eval_runtime': 1.5437, 'eval_samples_per_second': 896.522, 'eval_steps_per_second': 112.065, 'epoch': 17.0}
 42%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                    | 13940/32800 [27:49<35:29,  8.86it/s]
[INFO|trainer.py:3955] 2025-03-10 15:18:09,458 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13940
[INFO|configuration_utils.py:423] 2025-03-10 15:18:09,460 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13940/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:18:09,727 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13940/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:18:09,728 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13940/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:18:09,728 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13940/special_tokens_map.json
[2025-03-10 15:18:09,776] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13944 is about to be saved!
[2025-03-10 15:18:09,780] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13940/global_step13944/mp_rank_00_model_states.pt
[2025-03-10 15:18:09,780] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13940/global_step13944/mp_rank_00_model_states.pt...
[2025-03-10 15:18:10,125] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13940/global_step13944/mp_rank_00_model_states.pt.
[2025-03-10 15:18:10,126] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13940/global_step13944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:18:11,299] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13940/global_step13944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:18:11,300] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-13940/global_step13944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:18:11,300] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13944 is ready now!
{'loss': 0.1292, 'grad_norm': 8.155976295471191, 'learning_rate': 2e-05, 'epoch': 18.0}
 45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                            | 14760/32800 [29:23<33:42,  8.92it/s][INFO|trainer.py:930] 2025-03-10 15:19:43,279 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:19:43,282 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:19:43,282 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:19:43,282 >>   Batch size = 4
{'eval_loss': 1.1316418647766113, 'eval_mse': 1.1313323702426314, 'eval_pearson': 0.7280490971931661, 'eval_spearmanr': 0.7342463853228662, 'eval_runtime': 1.5471, 'eval_samples_per_second': 894.558, 'eval_steps_per_second': 111.82, 'epoch': 18.0}
 45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                            | 14760/32800 [29:24<33:42,  8.92it/s]
[INFO|trainer.py:3955] 2025-03-10 15:19:44,964 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-14760
[INFO|configuration_utils.py:423] 2025-03-10 15:19:44,966 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-14760/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:19:45,264 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-14760/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:19:45,265 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-14760/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:19:45,265 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-14760/special_tokens_map.json
[2025-03-10 15:19:45,315] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14764 is about to be saved!
[2025-03-10 15:19:45,319] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-14760/global_step14764/mp_rank_00_model_states.pt
[2025-03-10 15:19:45,319] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-14760/global_step14764/mp_rank_00_model_states.pt...
[2025-03-10 15:19:45,694] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-14760/global_step14764/mp_rank_00_model_states.pt.
[2025-03-10 15:19:45,696] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-14760/global_step14764/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:19:46,904] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-14760/global_step14764/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:19:46,905] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-14760/global_step14764/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:19:46,905] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14764 is ready now!
{'loss': 0.126, 'grad_norm': 19.140893936157227, 'learning_rate': 2e-05, 'epoch': 19.0}
 48%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                   | 15580/32800 [30:59<32:17,  8.89it/s][INFO|trainer.py:930] 2025-03-10 15:21:18,954 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:21:18,957 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:21:18,957 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:21:18,958 >>   Batch size = 4
{'eval_loss': 1.313089370727539, 'eval_mse': 1.3128134883897153, 'eval_pearson': 0.6874845268635281, 'eval_spearmanr': 0.7038053807993894, 'eval_runtime': 1.5439, 'eval_samples_per_second': 896.448, 'eval_steps_per_second': 112.056, 'epoch': 19.0}
 48%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                                   | 15580/32800 [31:00<32:17,  8.89it/s]
[INFO|trainer.py:3955] 2025-03-10 15:21:20,636 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-15580
[INFO|configuration_utils.py:423] 2025-03-10 15:21:20,638 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-15580/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:21:20,906 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-15580/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:21:20,907 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-15580/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:21:20,907 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-15580/special_tokens_map.json
[2025-03-10 15:21:20,955] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step15584 is about to be saved!
[2025-03-10 15:21:20,958] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-15580/global_step15584/mp_rank_00_model_states.pt
[2025-03-10 15:21:20,958] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-15580/global_step15584/mp_rank_00_model_states.pt...
[2025-03-10 15:21:21,309] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-15580/global_step15584/mp_rank_00_model_states.pt.
[2025-03-10 15:21:21,310] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-15580/global_step15584/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:21:22,485] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-15580/global_step15584/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:21:22,485] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-15580/global_step15584/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:21:22,485] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15584 is ready now!
{'loss': 0.1422, 'grad_norm': 1.813002109527588, 'learning_rate': 2e-05, 'epoch': 20.0}
 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                           | 16400/32800 [32:36<30:29,  8.96it/s][INFO|trainer.py:930] 2025-03-10 15:22:56,467 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:22:56,471 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:22:56,471 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:22:56,471 >>   Batch size = 4
{'eval_loss': 1.1330745220184326, 'eval_mse': 1.1326588547298675, 'eval_pearson': 0.6987462577274184, 'eval_spearmanr': 0.7140181466544091, 'eval_runtime': 1.5551, 'eval_samples_per_second': 889.969, 'eval_steps_per_second': 111.246, 'epoch': 20.0}
 50%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                           | 16400/32800 [32:38<30:29,  8.96it/s]
[INFO|trainer.py:3955] 2025-03-10 15:22:58,162 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-16400
[INFO|configuration_utils.py:423] 2025-03-10 15:22:58,164 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-16400/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:22:58,444 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-16400/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:22:58,445 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-16400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:22:58,445 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-16400/special_tokens_map.json
[2025-03-10 15:22:58,491] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step16405 is about to be saved!
[2025-03-10 15:22:58,494] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-16400/global_step16405/mp_rank_00_model_states.pt
[2025-03-10 15:22:58,494] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-16400/global_step16405/mp_rank_00_model_states.pt...
[2025-03-10 15:22:58,853] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-16400/global_step16405/mp_rank_00_model_states.pt.
[2025-03-10 15:22:58,854] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-16400/global_step16405/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:23:00,040] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-16400/global_step16405/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:23:00,041] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-16400/global_step16405/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:23:00,041] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16405 is ready now!
{'loss': 0.1117, 'grad_norm': 3.0687665939331055, 'learning_rate': 2e-05, 'epoch': 21.0}
 52%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                  | 17220/32800 [34:12<29:23,  8.83it/s][INFO|trainer.py:930] 2025-03-10 15:24:32,663 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:24:32,666 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:24:32,666 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:24:32,666 >>   Batch size = 4
{'eval_loss': 1.2137759923934937, 'eval_mse': 1.2137225539009007, 'eval_pearson': 0.7006232119890259, 'eval_spearmanr': 0.7146163063693235, 'eval_runtime': 1.5596, 'eval_samples_per_second': 887.398, 'eval_steps_per_second': 110.925, 'epoch': 21.0}
 52%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                  | 17220/32800 [34:14<29:23,  8.83it/s]
[INFO|trainer.py:3955] 2025-03-10 15:24:34,360 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-17220
[INFO|configuration_utils.py:423] 2025-03-10 15:24:34,362 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-17220/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:24:34,641 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-17220/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:24:34,642 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-17220/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:24:34,642 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-17220/special_tokens_map.json
[2025-03-10 15:24:34,690] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step17225 is about to be saved!
[2025-03-10 15:24:34,693] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-17220/global_step17225/mp_rank_00_model_states.pt
[2025-03-10 15:24:34,693] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-17220/global_step17225/mp_rank_00_model_states.pt...
[2025-03-10 15:24:35,054] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-17220/global_step17225/mp_rank_00_model_states.pt.
[2025-03-10 15:24:35,056] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-17220/global_step17225/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:24:36,233] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-17220/global_step17225/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:24:36,233] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-17220/global_step17225/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:24:36,233] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17225 is ready now!
{'loss': 0.0928, 'grad_norm': 3.6888599395751953, 'learning_rate': 2e-05, 'epoch': 22.0}
 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                          | 18040/32800 [35:48<27:35,  8.92it/s][INFO|trainer.py:930] 2025-03-10 15:26:08,236 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:26:08,240 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:26:08,240 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:26:08,240 >>   Batch size = 4
{'eval_loss': 1.2113100290298462, 'eval_mse': 1.2111896086979463, 'eval_pearson': 0.6944338554436842, 'eval_spearmanr': 0.7123419449963984, 'eval_runtime': 1.557, 'eval_samples_per_second': 888.871, 'eval_steps_per_second': 111.109, 'epoch': 22.0}
 55%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                          | 18040/32800 [35:49<27:35,  8.92it/s]
[INFO|trainer.py:3955] 2025-03-10 15:26:09,931 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18040
[INFO|configuration_utils.py:423] 2025-03-10 15:26:09,933 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18040/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:26:10,209 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18040/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:26:10,210 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18040/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:26:10,210 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18040/special_tokens_map.json
[2025-03-10 15:26:10,258] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step18045 is about to be saved!
[2025-03-10 15:26:10,262] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18040/global_step18045/mp_rank_00_model_states.pt
[2025-03-10 15:26:10,262] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18040/global_step18045/mp_rank_00_model_states.pt...
[2025-03-10 15:26:10,621] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18040/global_step18045/mp_rank_00_model_states.pt.
[2025-03-10 15:26:10,622] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18040/global_step18045/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:26:11,763] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18040/global_step18045/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:26:11,763] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18040/global_step18045/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:26:11,763] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18045 is ready now!
{'loss': 0.0909, 'grad_norm': 1.2911802530288696, 'learning_rate': 2e-05, 'epoch': 23.0}
 57%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                 | 18860/32800 [37:24<26:10,  8.88it/s][INFO|trainer.py:930] 2025-03-10 15:27:44,095 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:27:44,098 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:27:44,098 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:27:44,098 >>   Batch size = 4
{'eval_loss': 1.0712705850601196, 'eval_mse': 1.0708346515032596, 'eval_pearson': 0.7286345315321099, 'eval_spearmanr': 0.7371201562199446, 'eval_runtime': 1.5558, 'eval_samples_per_second': 889.575, 'eval_steps_per_second': 111.197, 'epoch': 23.0}
 57%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                 | 18860/32800 [37:25<26:10,  8.88it/s]
[INFO|trainer.py:3955] 2025-03-10 15:27:45,789 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18860
[INFO|configuration_utils.py:423] 2025-03-10 15:27:45,791 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18860/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:27:46,064 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18860/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:27:46,065 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18860/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:27:46,065 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18860/special_tokens_map.json
[2025-03-10 15:27:46,113] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step18865 is about to be saved!
[2025-03-10 15:27:46,117] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18860/global_step18865/mp_rank_00_model_states.pt
[2025-03-10 15:27:46,117] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18860/global_step18865/mp_rank_00_model_states.pt...
[2025-03-10 15:27:46,473] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18860/global_step18865/mp_rank_00_model_states.pt.
[2025-03-10 15:27:46,475] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18860/global_step18865/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:27:47,642] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18860/global_step18865/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:27:47,643] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-18860/global_step18865/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:27:47,643] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18865 is ready now!
{'loss': 0.1017, 'grad_norm': 1.3399834632873535, 'learning_rate': 2e-05, 'epoch': 24.0}
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                        | 19680/32800 [39:00<25:02,  8.73it/s][INFO|trainer.py:930] 2025-03-10 15:29:20,092 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:29:20,095 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:29:20,095 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:29:20,095 >>   Batch size = 4
{'eval_loss': 1.1870033740997314, 'eval_mse': 1.186674051546637, 'eval_pearson': 0.6992314532396748, 'eval_spearmanr': 0.7204944076372559, 'eval_runtime': 1.5539, 'eval_samples_per_second': 890.641, 'eval_steps_per_second': 111.33, 'epoch': 24.0}
 60%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                        | 19680/32800 [39:01<25:02,  8.73it/s]
[INFO|trainer.py:3955] 2025-03-10 15:29:21,784 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-19680
[INFO|configuration_utils.py:423] 2025-03-10 15:29:21,786 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-19680/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:29:22,059 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-19680/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:29:22,060 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-19680/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:29:22,060 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-19680/special_tokens_map.json
[2025-03-10 15:29:22,101] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step19686 is about to be saved!
[2025-03-10 15:29:22,105] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-19680/global_step19686/mp_rank_00_model_states.pt
[2025-03-10 15:29:22,105] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-19680/global_step19686/mp_rank_00_model_states.pt...
[2025-03-10 15:29:22,464] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-19680/global_step19686/mp_rank_00_model_states.pt.
[2025-03-10 15:29:22,465] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-19680/global_step19686/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:29:23,622] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-19680/global_step19686/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:29:23,623] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-19680/global_step19686/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:29:23,623] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19686 is ready now!
{'loss': 0.0782, 'grad_norm': 1.7739859819412231, 'learning_rate': 2e-05, 'epoch': 25.0}
 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                | 20500/32800 [40:36<23:09,  8.85it/s][INFO|trainer.py:930] 2025-03-10 15:30:56,559 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:30:56,562 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:30:56,563 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:30:56,563 >>   Batch size = 4
{'eval_loss': 1.0831161737442017, 'eval_mse': 1.0831075400286327, 'eval_pearson': 0.7311612703677548, 'eval_spearmanr': 0.746627956070669, 'eval_runtime': 1.5581, 'eval_samples_per_second': 888.287, 'eval_steps_per_second': 111.036, 'epoch': 25.0}
 62%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                | 20500/32800 [40:38<23:09,  8.85it/s]
[INFO|trainer.py:3955] 2025-03-10 15:30:58,247 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-20500
[INFO|configuration_utils.py:423] 2025-03-10 15:30:58,249 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-20500/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:30:58,526 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-20500/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:30:58,527 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-20500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:30:58,527 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-20500/special_tokens_map.json
[2025-03-10 15:30:58,575] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step20506 is about to be saved!
[2025-03-10 15:30:58,578] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-20500/global_step20506/mp_rank_00_model_states.pt
[2025-03-10 15:30:58,579] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-20500/global_step20506/mp_rank_00_model_states.pt...
[2025-03-10 15:30:58,940] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-20500/global_step20506/mp_rank_00_model_states.pt.
[2025-03-10 15:30:58,941] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-20500/global_step20506/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:31:00,105] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-20500/global_step20506/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:31:00,105] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-20500/global_step20506/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:31:00,105] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20506 is ready now!
{'loss': 0.07, 'grad_norm': 2.6750314235687256, 'learning_rate': 2e-05, 'epoch': 26.0}
 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                       | 21320/32800 [42:12<21:32,  8.88it/s][INFO|trainer.py:930] 2025-03-10 15:32:32,287 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:32:32,291 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:32:32,291 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:32:32,291 >>   Batch size = 4
{'eval_loss': 1.1038036346435547, 'eval_mse': 1.1033680573364213, 'eval_pearson': 0.7172917783117121, 'eval_spearmanr': 0.7344849564393153, 'eval_runtime': 1.5594, 'eval_samples_per_second': 887.544, 'eval_steps_per_second': 110.943, 'epoch': 26.0}
 65%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                       | 21320/32800 [42:13<21:32,  8.88it/s]
[INFO|trainer.py:3955] 2025-03-10 15:32:33,986 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-21320
[INFO|configuration_utils.py:423] 2025-03-10 15:32:33,988 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-21320/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:32:34,258 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-21320/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:32:34,259 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-21320/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:32:34,259 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-21320/special_tokens_map.json
[2025-03-10 15:32:34,307] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step21326 is about to be saved!
[2025-03-10 15:32:34,310] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-21320/global_step21326/mp_rank_00_model_states.pt
[2025-03-10 15:32:34,310] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-21320/global_step21326/mp_rank_00_model_states.pt...
[2025-03-10 15:32:34,665] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-21320/global_step21326/mp_rank_00_model_states.pt.
[2025-03-10 15:32:34,666] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-21320/global_step21326/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:32:35,837] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-21320/global_step21326/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:32:35,837] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-21320/global_step21326/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:32:35,837] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21326 is ready now!
{'loss': 0.0677, 'grad_norm': 0.9001121520996094, 'learning_rate': 2e-05, 'epoch': 27.0}
 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                               | 22140/32800 [43:48<19:57,  8.90it/s][INFO|trainer.py:930] 2025-03-10 15:34:08,040 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:34:08,043 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:34:08,043 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:34:08,043 >>   Batch size = 4
{'eval_loss': 1.164591908454895, 'eval_mse': 1.1642043442395382, 'eval_pearson': 0.7112598777529335, 'eval_spearmanr': 0.7282227454888754, 'eval_runtime': 1.5587, 'eval_samples_per_second': 887.93, 'eval_steps_per_second': 110.991, 'epoch': 27.0}
 68%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                               | 22140/32800 [43:49<19:57,  8.90it/s]
[INFO|trainer.py:3955] 2025-03-10 15:34:09,736 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22140
[INFO|configuration_utils.py:423] 2025-03-10 15:34:09,738 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22140/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:34:10,011 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22140/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:34:10,011 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22140/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:34:10,012 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22140/special_tokens_map.json
[2025-03-10 15:34:10,060] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step22146 is about to be saved!
[2025-03-10 15:34:10,063] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22140/global_step22146/mp_rank_00_model_states.pt
[2025-03-10 15:34:10,063] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22140/global_step22146/mp_rank_00_model_states.pt...
[2025-03-10 15:34:10,419] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22140/global_step22146/mp_rank_00_model_states.pt.
[2025-03-10 15:34:10,420] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22140/global_step22146/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:34:11,580] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22140/global_step22146/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:34:11,580] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22140/global_step22146/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:34:11,580] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22146 is ready now!
{'loss': 0.0777, 'grad_norm': 2.032932996749878, 'learning_rate': 2e-05, 'epoch': 28.0}
 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                      | 22960/32800 [45:23<18:21,  8.93it/s][INFO|trainer.py:930] 2025-03-10 15:35:43,367 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:35:43,370 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:35:43,370 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:35:43,370 >>   Batch size = 4
{'eval_loss': 1.2142181396484375, 'eval_mse': 1.2138976930193819, 'eval_pearson': 0.6948036664456247, 'eval_spearmanr': 0.7108683091630427, 'eval_runtime': 1.5588, 'eval_samples_per_second': 887.876, 'eval_steps_per_second': 110.984, 'epoch': 28.0}
 70%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                      | 22960/32800 [45:24<18:21,  8.93it/s]
[INFO|trainer.py:3955] 2025-03-10 15:35:45,065 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22960
[INFO|configuration_utils.py:423] 2025-03-10 15:35:45,067 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22960/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:35:45,346 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22960/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:35:45,346 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22960/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:35:45,347 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22960/special_tokens_map.json
[2025-03-10 15:35:45,396] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step22967 is about to be saved!
[2025-03-10 15:35:45,400] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22960/global_step22967/mp_rank_00_model_states.pt
[2025-03-10 15:35:45,400] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22960/global_step22967/mp_rank_00_model_states.pt...
[2025-03-10 15:35:45,762] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22960/global_step22967/mp_rank_00_model_states.pt.
[2025-03-10 15:35:45,763] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22960/global_step22967/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:35:46,961] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22960/global_step22967/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:35:46,962] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-22960/global_step22967/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:35:46,962] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22967 is ready now!
{'loss': 0.0594, 'grad_norm': 3.9175848960876465, 'learning_rate': 2e-05, 'epoch': 29.0}
 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                              | 23780/32800 [46:59<17:00,  8.84it/s][INFO|trainer.py:930] 2025-03-10 15:37:19,722 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:37:19,725 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:37:19,725 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:37:19,725 >>   Batch size = 4
{'eval_loss': 1.1666814088821411, 'eval_mse': 1.166293514601757, 'eval_pearson': 0.7120015394415218, 'eval_spearmanr': 0.7280800832105222, 'eval_runtime': 1.5567, 'eval_samples_per_second': 889.053, 'eval_steps_per_second': 111.132, 'epoch': 29.0}
 72%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                              | 23780/32800 [47:01<17:00,  8.84it/s]
[INFO|trainer.py:3955] 2025-03-10 15:37:21,418 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-23780
[INFO|configuration_utils.py:423] 2025-03-10 15:37:21,420 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-23780/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:37:21,701 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-23780/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:37:21,702 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-23780/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:37:21,702 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-23780/special_tokens_map.json
[2025-03-10 15:37:21,751] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step23787 is about to be saved!
[2025-03-10 15:37:21,754] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-23780/global_step23787/mp_rank_00_model_states.pt
[2025-03-10 15:37:21,754] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-23780/global_step23787/mp_rank_00_model_states.pt...
[2025-03-10 15:37:22,126] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-23780/global_step23787/mp_rank_00_model_states.pt.
[2025-03-10 15:37:22,127] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-23780/global_step23787/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:37:23,362] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-23780/global_step23787/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:37:23,362] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-23780/global_step23787/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:37:23,362] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23787 is ready now!
{'loss': 0.05, 'grad_norm': 1.227554202079773, 'learning_rate': 2e-05, 'epoch': 30.0}
 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 24600/32800 [48:35<15:18,  8.93it/s][INFO|trainer.py:930] 2025-03-10 15:38:55,403 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:38:55,406 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:38:55,406 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:38:55,406 >>   Batch size = 4
{'eval_loss': 1.0964279174804688, 'eval_mse': 1.0962787752895686, 'eval_pearson': 0.7288083181792027, 'eval_spearmanr': 0.7454005104814027, 'eval_runtime': 1.5542, 'eval_samples_per_second': 890.476, 'eval_steps_per_second': 111.309, 'epoch': 30.0}
 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                     | 24600/32800 [48:37<15:18,  8.93it/s]
[INFO|trainer.py:3955] 2025-03-10 15:38:57,097 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-24600
[INFO|configuration_utils.py:423] 2025-03-10 15:38:57,099 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-24600/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:38:57,378 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-24600/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:38:57,378 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-24600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:38:57,378 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-24600/special_tokens_map.json
[2025-03-10 15:38:57,427] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step24607 is about to be saved!
[2025-03-10 15:38:57,430] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-24600/global_step24607/mp_rank_00_model_states.pt
[2025-03-10 15:38:57,430] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-24600/global_step24607/mp_rank_00_model_states.pt...
[2025-03-10 15:38:57,790] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-24600/global_step24607/mp_rank_00_model_states.pt.
[2025-03-10 15:38:57,792] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-24600/global_step24607/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:38:59,002] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-24600/global_step24607/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:38:59,002] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-24600/global_step24607/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:38:59,003] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24607 is ready now!
{'loss': 0.0545, 'grad_norm': 1.2299076318740845, 'learning_rate': 2e-05, 'epoch': 31.0}
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                             | 25420/32800 [50:11<13:49,  8.89it/s][INFO|trainer.py:930] 2025-03-10 15:40:31,165 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:40:31,168 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:40:31,168 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:40:31,168 >>   Batch size = 4
{'eval_loss': 1.1976745128631592, 'eval_mse': 1.1972524709784227, 'eval_pearson': 0.7260399302217244, 'eval_spearmanr': 0.7419826300296588, 'eval_runtime': 1.5555, 'eval_samples_per_second': 889.737, 'eval_steps_per_second': 111.217, 'epoch': 31.0}
 78%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                             | 25420/32800 [50:12<13:49,  8.89it/s]
[INFO|trainer.py:3955] 2025-03-10 15:40:32,862 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-25420
[INFO|configuration_utils.py:423] 2025-03-10 15:40:32,864 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-25420/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:40:33,173 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-25420/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:40:33,174 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-25420/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:40:33,174 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-25420/special_tokens_map.json
[2025-03-10 15:40:33,224] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step25427 is about to be saved!
[2025-03-10 15:40:33,227] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-25420/global_step25427/mp_rank_00_model_states.pt
[2025-03-10 15:40:33,227] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-25420/global_step25427/mp_rank_00_model_states.pt...
[2025-03-10 15:40:33,609] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-25420/global_step25427/mp_rank_00_model_states.pt.
[2025-03-10 15:40:33,610] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-25420/global_step25427/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:40:34,805] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-25420/global_step25427/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:40:34,805] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-25420/global_step25427/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:40:34,805] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25427 is ready now!
{'loss': 0.0632, 'grad_norm': 6.603608131408691, 'learning_rate': 2e-05, 'epoch': 32.0}
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                    | 26240/32800 [51:46<12:12,  8.96it/s][INFO|trainer.py:930] 2025-03-10 15:42:06,453 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:42:06,457 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:42:06,457 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:42:06,457 >>   Batch size = 4
{'eval_loss': 1.2783764600753784, 'eval_mse': 1.2786954596552547, 'eval_pearson': 0.6878714940469008, 'eval_spearmanr': 0.7081312754888373, 'eval_runtime': 1.5568, 'eval_samples_per_second': 889.028, 'eval_steps_per_second': 111.129, 'epoch': 32.0}
 80%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                    | 26240/32800 [51:48<12:12,  8.96it/s]
[INFO|trainer.py:3955] 2025-03-10 15:42:08,154 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-26240
[INFO|configuration_utils.py:423] 2025-03-10 15:42:08,156 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-26240/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:42:08,495 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-26240/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:42:08,496 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-26240/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:42:08,496 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-26240/special_tokens_map.json
[2025-03-10 15:42:08,544] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step26248 is about to be saved!
[2025-03-10 15:42:08,548] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-26240/global_step26248/mp_rank_00_model_states.pt
[2025-03-10 15:42:08,548] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-26240/global_step26248/mp_rank_00_model_states.pt...
[2025-03-10 15:42:08,932] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-26240/global_step26248/mp_rank_00_model_states.pt.
[2025-03-10 15:42:08,933] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-26240/global_step26248/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:42:10,182] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-26240/global_step26248/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:42:10,182] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-26240/global_step26248/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:42:10,182] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26248 is ready now!
{'loss': 0.0472, 'grad_norm': 1.30409574508667, 'learning_rate': 2e-05, 'epoch': 33.0}
 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                           | 27060/32800 [53:22<10:47,  8.86it/s][INFO|trainer.py:930] 2025-03-10 15:43:42,904 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:43:42,907 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:43:42,907 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:43:42,907 >>   Batch size = 4
{'eval_loss': 1.154372215270996, 'eval_mse': 1.1546472045727547, 'eval_pearson': 0.7169307108114071, 'eval_spearmanr': 0.7300500472424477, 'eval_runtime': 1.5575, 'eval_samples_per_second': 888.591, 'eval_steps_per_second': 111.074, 'epoch': 33.0}
 82%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                           | 27060/32800 [53:24<10:47,  8.86it/s]
[INFO|trainer.py:3955] 2025-03-10 15:43:44,594 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27060
[INFO|configuration_utils.py:423] 2025-03-10 15:43:44,596 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27060/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:43:44,891 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27060/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:43:44,892 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27060/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:43:44,892 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27060/special_tokens_map.json
[2025-03-10 15:43:44,941] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step27068 is about to be saved!
[2025-03-10 15:43:44,944] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27060/global_step27068/mp_rank_00_model_states.pt
[2025-03-10 15:43:44,944] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27060/global_step27068/mp_rank_00_model_states.pt...
[2025-03-10 15:43:45,336] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27060/global_step27068/mp_rank_00_model_states.pt.
[2025-03-10 15:43:45,337] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27060/global_step27068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:43:46,556] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27060/global_step27068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:43:46,557] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27060/global_step27068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:43:46,557] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27068 is ready now!
{'loss': 0.0428, 'grad_norm': 0.9257909059524536, 'learning_rate': 2e-05, 'epoch': 34.0}
 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                   | 27880/32800 [54:58<09:11,  8.93it/s][INFO|trainer.py:930] 2025-03-10 15:45:18,650 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:45:18,654 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:45:18,654 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:45:18,654 >>   Batch size = 4
{'eval_loss': 1.0996663570404053, 'eval_mse': 1.099058649788013, 'eval_pearson': 0.7379954462539666, 'eval_spearmanr': 0.7458124207877419, 'eval_runtime': 1.5582, 'eval_samples_per_second': 888.179, 'eval_steps_per_second': 111.022, 'epoch': 34.0}
 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                                   | 27880/32800 [55:00<09:11,  8.93it/s]
[INFO|trainer.py:3955] 2025-03-10 15:45:20,350 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27880
[INFO|configuration_utils.py:423] 2025-03-10 15:45:20,352 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27880/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:45:20,642 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27880/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:45:20,643 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27880/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:45:20,643 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27880/special_tokens_map.json
[2025-03-10 15:45:20,692] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step27888 is about to be saved!
[2025-03-10 15:45:20,695] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27880/global_step27888/mp_rank_00_model_states.pt
[2025-03-10 15:45:20,695] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27880/global_step27888/mp_rank_00_model_states.pt...
[2025-03-10 15:45:21,066] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27880/global_step27888/mp_rank_00_model_states.pt.
[2025-03-10 15:45:21,067] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27880/global_step27888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:45:22,260] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27880/global_step27888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:45:22,261] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-27880/global_step27888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:45:22,261] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27888 is ready now!
{'loss': 0.0413, 'grad_norm': 1.0630097389221191, 'learning_rate': 2e-05, 'epoch': 35.0}
 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                          | 28700/32800 [56:34<07:40,  8.90it/s][INFO|trainer.py:930] 2025-03-10 15:46:54,415 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:46:54,418 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:46:54,418 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:46:54,418 >>   Batch size = 4
{'eval_loss': 1.075761079788208, 'eval_mse': 1.0758615673621954, 'eval_pearson': 0.7344616115978119, 'eval_spearmanr': 0.7462400391829417, 'eval_runtime': 1.5584, 'eval_samples_per_second': 888.069, 'eval_steps_per_second': 111.009, 'epoch': 35.0}
 88%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                          | 28700/32800 [56:36<07:40,  8.90it/s]
[INFO|trainer.py:3955] 2025-03-10 15:46:56,114 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-28700
[INFO|configuration_utils.py:423] 2025-03-10 15:46:56,116 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-28700/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:46:56,409 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-28700/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:46:56,410 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-28700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:46:56,410 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-28700/special_tokens_map.json
[2025-03-10 15:46:56,460] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step28708 is about to be saved!
[2025-03-10 15:46:56,463] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-28700/global_step28708/mp_rank_00_model_states.pt
[2025-03-10 15:46:56,463] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-28700/global_step28708/mp_rank_00_model_states.pt...
[2025-03-10 15:46:56,839] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-28700/global_step28708/mp_rank_00_model_states.pt.
[2025-03-10 15:46:56,840] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-28700/global_step28708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:46:58,067] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-28700/global_step28708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:46:58,068] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-28700/global_step28708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:46:58,068] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28708 is ready now!
{'loss': 0.0479, 'grad_norm': 0.8880822658538818, 'learning_rate': 2e-05, 'epoch': 36.0}
 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                  | 29520/32800 [58:09<06:06,  8.96it/s][INFO|trainer.py:930] 2025-03-10 15:48:29,767 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:48:29,770 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:48:29,771 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:48:29,771 >>   Batch size = 4
{'eval_loss': 1.190140724182129, 'eval_mse': 1.1898175887978835, 'eval_pearson': 0.7062912895153028, 'eval_spearmanr': 0.7221113495013952, 'eval_runtime': 1.5574, 'eval_samples_per_second': 888.665, 'eval_steps_per_second': 111.083, 'epoch': 36.0}
 90%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                                  | 29520/32800 [58:11<06:06,  8.96it/s]
[INFO|trainer.py:3955] 2025-03-10 15:48:31,467 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-29520
[INFO|configuration_utils.py:423] 2025-03-10 15:48:31,469 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-29520/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:48:31,764 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-29520/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:48:31,765 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-29520/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:48:31,765 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-29520/special_tokens_map.json
[2025-03-10 15:48:31,807] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step29529 is about to be saved!
[2025-03-10 15:48:31,810] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-29520/global_step29529/mp_rank_00_model_states.pt
[2025-03-10 15:48:31,810] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-29520/global_step29529/mp_rank_00_model_states.pt...
[2025-03-10 15:48:32,196] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-29520/global_step29529/mp_rank_00_model_states.pt.
[2025-03-10 15:48:32,197] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-29520/global_step29529/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:48:33,400] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-29520/global_step29529/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:48:33,401] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-29520/global_step29529/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:48:33,401] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29529 is ready now!
{'loss': 0.0425, 'grad_norm': 4.920377731323242, 'learning_rate': 2e-05, 'epoch': 37.0}
 92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                         | 30340/32800 [59:46<04:37,  8.86it/s][INFO|trainer.py:930] 2025-03-10 15:50:06,103 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:50:06,106 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:50:06,106 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:50:06,106 >>   Batch size = 4
{'eval_loss': 1.1322509050369263, 'eval_mse': 1.1315570349638173, 'eval_pearson': 0.7174436217442718, 'eval_spearmanr': 0.7338889579355781, 'eval_runtime': 1.5597, 'eval_samples_per_second': 887.362, 'eval_steps_per_second': 110.92, 'epoch': 37.0}
 92%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                         | 30340/32800 [59:47<04:37,  8.86it/s]
[INFO|trainer.py:3955] 2025-03-10 15:50:07,798 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-30340
[INFO|configuration_utils.py:423] 2025-03-10 15:50:07,800 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-30340/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:50:08,098 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-30340/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:50:08,099 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-30340/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:50:08,099 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-30340/special_tokens_map.json
[2025-03-10 15:50:08,147] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step30349 is about to be saved!
[2025-03-10 15:50:08,151] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-30340/global_step30349/mp_rank_00_model_states.pt
[2025-03-10 15:50:08,151] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-30340/global_step30349/mp_rank_00_model_states.pt...
[2025-03-10 15:50:08,525] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-30340/global_step30349/mp_rank_00_model_states.pt.
[2025-03-10 15:50:08,526] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-30340/global_step30349/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:50:09,753] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-30340/global_step30349/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:50:09,754] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-30340/global_step30349/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:50:09,754] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30349 is ready now!
{'loss': 0.0344, 'grad_norm': 1.4444587230682373, 'learning_rate': 2e-05, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 31160/32800 [1:01:21<03:02,  8.98it/s][INFO|trainer.py:930] 2025-03-10 15:51:41,799 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:51:41,803 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:51:41,803 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:51:41,803 >>   Batch size = 4
{'eval_loss': 1.176619529724121, 'eval_mse': 1.1759952661619022, 'eval_pearson': 0.7091452063728961, 'eval_spearmanr': 0.7279017264551381, 'eval_runtime': 1.5482, 'eval_samples_per_second': 893.957, 'eval_steps_per_second': 111.745, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 31160/32800 [1:01:23<03:02,  8.98it/s]
[INFO|trainer.py:3955] 2025-03-10 15:51:43,486 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31160
[INFO|configuration_utils.py:423] 2025-03-10 15:51:43,488 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31160/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:51:43,794 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31160/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:51:43,795 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31160/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:51:43,795 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31160/special_tokens_map.json
[2025-03-10 15:51:43,846] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step31169 is about to be saved!
[2025-03-10 15:51:43,849] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31160/global_step31169/mp_rank_00_model_states.pt
[2025-03-10 15:51:43,849] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31160/global_step31169/mp_rank_00_model_states.pt...
[2025-03-10 15:51:44,236] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31160/global_step31169/mp_rank_00_model_states.pt.
[2025-03-10 15:51:44,238] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31160/global_step31169/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:51:45,482] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31160/global_step31169/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:51:45,483] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31160/global_step31169/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:51:45,483] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31169 is ready now!
{'loss': 0.0355, 'grad_norm': 4.131590843200684, 'learning_rate': 2e-05, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 31980/32800 [1:02:57<01:31,  8.94it/s][INFO|trainer.py:930] 2025-03-10 15:53:17,641 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:53:17,644 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:53:17,644 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:53:17,644 >>   Batch size = 4
{'eval_loss': 1.0879241228103638, 'eval_mse': 1.0875113172338189, 'eval_pearson': 0.7340294052061451, 'eval_spearmanr': 0.7474243268752133, 'eval_runtime': 1.6017, 'eval_samples_per_second': 864.057, 'eval_steps_per_second': 108.007, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 31980/32800 [1:02:59<01:31,  8.94it/s]
[INFO|trainer.py:3955] 2025-03-10 15:53:19,338 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31980
[INFO|configuration_utils.py:423] 2025-03-10 15:53:19,340 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31980/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:53:19,637 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31980/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:53:19,637 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31980/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:53:19,638 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31980/special_tokens_map.json
[2025-03-10 15:53:19,686] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step31989 is about to be saved!
[2025-03-10 15:53:19,689] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31980/global_step31989/mp_rank_00_model_states.pt
[2025-03-10 15:53:19,689] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31980/global_step31989/mp_rank_00_model_states.pt...
[2025-03-10 15:53:20,071] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31980/global_step31989/mp_rank_00_model_states.pt.
[2025-03-10 15:53:20,072] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31980/global_step31989/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:53:21,409] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31980/global_step31989/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:53:21,410] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-31980/global_step31989/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:53:21,410] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31989 is ready now!
{'loss': 0.0443, 'grad_norm': 6.53105354309082, 'learning_rate': 2e-05, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32800/32800 [1:04:33<00:00,  8.97it/s][INFO|trainer.py:930] 2025-03-10 15:54:53,016 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:54:53,020 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:54:53,020 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:54:53,020 >>   Batch size = 4
{'eval_loss': 1.0892852544784546, 'eval_mse': 1.0894713205409188, 'eval_pearson': 0.7328291629982975, 'eval_spearmanr': 0.7477688519534533, 'eval_runtime': 1.564, 'eval_samples_per_second': 884.926, 'eval_steps_per_second': 110.616, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32800/32800 [1:04:34<00:00,  8.97it/s]
[INFO|trainer.py:3955] 2025-03-10 15:54:54,733 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-32800
[INFO|configuration_utils.py:423] 2025-03-10 15:54:54,736 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-32800/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:54:55,001 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-32800/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:54:55,001 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-32800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:54:55,002 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-32800/special_tokens_map.json
[2025-03-10 15:54:55,046] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step32809 is about to be saved!
[2025-03-10 15:54:55,050] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-32800/global_step32809/mp_rank_00_model_states.pt
[2025-03-10 15:54:55,050] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-32800/global_step32809/mp_rank_00_model_states.pt...
[2025-03-10 15:54:55,386] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-32800/global_step32809/mp_rank_00_model_states.pt.
[2025-03-10 15:54:55,388] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-32800/global_step32809/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:54:56,509] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-32800/global_step32809/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:54:56,510] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-base-num=20/checkpoint-32800/global_step32809/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:54:56,510] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32809 is ready now!
[INFO|trainer.py:2670] 2025-03-10 15:54:56,517 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 3876.5745, 'train_samples_per_second': 270.889, 'train_steps_per_second': 8.461, 'train_loss': 0.22545829063508568, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32800/32800 [1:04:36<00:00,  8.46it/s]
[INFO|trainer.py:3955] 2025-03-10 15:54:56,594 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-base-num=20
[INFO|configuration_utils.py:423] 2025-03-10 15:54:56,596 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-base-num=20/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:54:56,858 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-base-num=20/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:54:56,859 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:54:56,859 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-base-num=20/special_tokens_map.json
***** train metrics *****
  epoch                    =     39.9994
  total_flos               = 257269459GF
  train_loss               =      0.2255
  train_runtime            =  1:04:36.57
  train_samples            =       26253
  train_samples_per_second =     270.889
  train_steps_per_second   =       8.461
03/10/2025 15:54:56 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:930] 2025-03-10 15:54:56,901 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:54:56,903 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:54:56,903 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:54:56,903 >>   Batch size = 4
100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 173/173 [00:01<00:00, 110.76it/s]
***** eval metrics *****
  epoch                   =    39.9994
  eval_loss               =     1.0893
  eval_mse                =     1.0895
  eval_pearson            =     0.7328
  eval_runtime            = 0:00:01.57
  eval_samples            =       1384
  eval_samples_per_second =     878.32
  eval_spearmanr          =     0.7478
  eval_steps_per_second   =     109.79
[INFO|modelcard.py:449] 2025-03-10 15:54:58,801 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Spearmanr', 'type': 'spearmanr', 'value': 0.7477688519534533}]}
[rank0]:[W310 15:54:59.043560030 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-03-10 15:55:01,712] [INFO] [launch.py:351:main] Process 4097902 exits successfully.
[2025-03-10 15:55:01,712] [INFO] [launch.py:351:main] Process 4097901 exits successfully.
+ for MODEL_NAME in "${MODEL_NAMES[@]}"
+ python -m deepspeed.launcher.runner --include=localhost:2,3 --master_port 28095 run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=20-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=20-val.json --model_name_or_path FacebookAI/roberta-large --output_dir output/GNER-QE/FacebookAI/roberta-large-num=20 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 15:55:08,354] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 15:55:11,367] [WARNING] [runner.py:215:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.
[2025-03-10 15:55:11,368] [INFO] [runner.py:607:main] cmd = /raid/chrisjihee/miniforge3/envs/GNER/bin/python -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMiwgM119 --master_addr=127.0.0.1 --master_port=28095 --enable_each_rank_log=None run_glue.py --train_file data/GNER-QE/ZSE-validation-pred-by_beam-num=20-train.json --validation_file data/GNER-QE/ZSE-validation-pred-by_beam-num=20-val.json --model_name_or_path FacebookAI/roberta-large --output_dir output/GNER-QE/FacebookAI/roberta-large-num=20 --cache_dir .cache --do_train --do_eval --bf16 True --tf32 True --max_seq_length 512 --per_device_eval_batch_size 4 --per_device_train_batch_size 4 --gradient_accumulation_steps 4 --learning_rate 2e-5 --num_train_epochs 40 --logging_strategy epoch --eval_strategy epoch --save_strategy epoch --deepspeed configs/deepspeed/ds1_t5.json --overwrite_output_dir --overwrite_cache
[2025-03-10 15:55:15,958] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 15:55:18,939] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [2, 3]}
[2025-03-10 15:55:18,940] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=2, node_rank=0
[2025-03-10 15:55:18,940] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1]})
[2025-03-10 15:55:18,940] [INFO] [launch.py:164:main] dist_world_size=2
[2025-03-10 15:55:18,940] [INFO] [launch.py:168:main] Setting CUDA_VISIBLE_DEVICES=2,3
[2025-03-10 15:55:18,941] [INFO] [launch.py:256:main] process 4171059 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=0', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=20-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=20-val.json', '--model_name_or_path', 'FacebookAI/roberta-large', '--output_dir', 'output/GNER-QE/FacebookAI/roberta-large-num=20', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 15:55:18,941] [INFO] [launch.py:256:main] process 4171060 spawned with command: ['/raid/chrisjihee/miniforge3/envs/GNER/bin/python', '-u', 'run_glue.py', '--local_rank=1', '--train_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=20-train.json', '--validation_file', 'data/GNER-QE/ZSE-validation-pred-by_beam-num=20-val.json', '--model_name_or_path', 'FacebookAI/roberta-large', '--output_dir', 'output/GNER-QE/FacebookAI/roberta-large-num=20', '--cache_dir', '.cache', '--do_train', '--do_eval', '--bf16', 'True', '--tf32', 'True', '--max_seq_length', '512', '--per_device_eval_batch_size', '4', '--per_device_train_batch_size', '4', '--gradient_accumulation_steps', '4', '--learning_rate', '2e-5', '--num_train_epochs', '40', '--logging_strategy', 'epoch', '--eval_strategy', 'epoch', '--save_strategy', 'epoch', '--deepspeed', 'configs/deepspeed/ds1_t5.json', '--overwrite_output_dir', '--overwrite_cache']
[2025-03-10 15:55:23,885] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 15:55:24,205] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2025-03-10 15:55:24,691] [INFO] [comm.py:658:init_distributed] cdb=None
[2025-03-10 15:55:24,691] [INFO] [comm.py:689:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2025-03-10 15:55:25,026] [INFO] [comm.py:658:init_distributed] cdb=None
03/10/2025 15:55:27 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, 16-bits training: False
03/10/2025 15:55:27 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None, 'use_configured_state': False},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
average_tokens_across_devices=False,
batch_eval_metrics=False,
bf16=True,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=configs/deepspeed/ds1_t5.json,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_do_concat_batches=True,
eval_on_start=False,
eval_steps=None,
eval_strategy=IntervalStrategy.EPOCH,
eval_use_gather_object=False,
evaluation_strategy=None,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=4,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=None,
hub_strategy=HubStrategy.EVERY_SAVE,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_for_metrics=[],
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=2e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=output/GNER-QE/FacebookAI/roberta-large-num=20/runs/Mar10_15-55-23_dgx-a100,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=IntervalStrategy.EPOCH,
lr_scheduler_kwargs={},
lr_scheduler_type=SchedulerType.LINEAR,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=40.0,
optim=OptimizerNames.ADAMW_TORCH,
optim_args=None,
optim_target_modules=None,
output_dir=output/GNER-QE/FacebookAI/roberta-large-num=20,
overwrite_output_dir=True,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=['tensorboard'],
restore_callback_states_from_checkpoint=False,
resume_from_checkpoint=None,
run_name=output/GNER-QE/FacebookAI/roberta-large-num=20,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=SaveStrategy.EPOCH,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=True,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torch_empty_cache_steps=None,
torchdynamo=None,
tp_size=0,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_liger_kernel=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
03/10/2025 15:55:27 - INFO - __main__ - load a local file for train: data/GNER-QE/ZSE-validation-pred-by_beam-num=20-train.json
03/10/2025 15:55:27 - INFO - __main__ - load a local file for validation: data/GNER-QE/ZSE-validation-pred-by_beam-num=20-val.json
03/10/2025 15:55:27 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, 16-bits training: False
Using custom data configuration default-cc9ee36d8392f1d6
03/10/2025 15:55:27 - INFO - datasets.builder - Using custom data configuration default-cc9ee36d8392f1d6
Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
03/10/2025 15:55:27 - INFO - datasets.info - Loading Dataset Infos from /raid/chrisjihee/miniforge3/envs/GNER/lib/python3.12/site-packages/datasets/packaged_modules/json
Overwrite dataset info from restored data version if exists.
03/10/2025 15:55:27 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from .cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 15:55:27 - INFO - datasets.info - Loading Dataset info from .cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
03/10/2025 15:55:27 - INFO - datasets.builder - Found cached dataset json (/raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)
Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
03/10/2025 15:55:27 - INFO - datasets.info - Loading Dataset info from /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092
[INFO|configuration_utils.py:699] 2025-03-10 15:55:27,921 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
[INFO|configuration_utils.py:771] 2025-03-10 15:55:27,924 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "id2label": {
    "0": "LABEL_0"
  },
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "label2id": {
    "LABEL_0": 0
  },
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|configuration_utils.py:699] 2025-03-10 15:55:28,132 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
[INFO|configuration_utils.py:771] 2025-03-10 15:55:28,132 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:55:28,141 >> loading file vocab.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/vocab.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:55:28,141 >> loading file merges.txt from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/merges.txt
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:55:28,141 >> loading file tokenizer.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:55:28,141 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:55:28,141 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:55:28,141 >> loading file tokenizer_config.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/tokenizer_config.json
[INFO|tokenization_utils_base.py:2060] 2025-03-10 15:55:28,141 >> loading file chat_template.jinja from cache at None
[INFO|configuration_utils.py:699] 2025-03-10 15:55:28,141 >> loading configuration file config.json from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/config.json
[INFO|configuration_utils.py:771] 2025-03-10 15:55:28,142 >> Model config RobertaConfig {
  "_name_or_path": "FacebookAI/roberta-large",
  "architectures": [
    "RobertaForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "bos_token_id": 0,
  "classifier_dropout": null,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 1024,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 514,
  "model_type": "roberta",
  "num_attention_heads": 16,
  "num_hidden_layers": 24,
  "pad_token_id": 1,
  "position_embedding_type": "absolute",
  "transformers_version": "4.50.0.dev0",
  "type_vocab_size": 1,
  "use_cache": true,
  "vocab_size": 50265
}

[INFO|modeling_utils.py:4128] 2025-03-10 15:55:28,273 >> loading weights file model.safetensors from cache at .cache/models--FacebookAI--roberta-large/snapshots/722cf37b1afa9454edce342e7895e588b6ff1d59/model.safetensors
[INFO|modeling_utils.py:5091] 2025-03-10 15:55:28,404 >> Some weights of the model checkpoint at FacebookAI/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight']
- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
[WARNING|modeling_utils.py:5103] 2025-03-10 15:55:28,404 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/26253 [00:00<?, ? examples/s][WARNING|modeling_utils.py:5103] 2025-03-10 15:55:28,623 >> Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1de7f97200b99edf.arrow
03/10/2025 15:55:28 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-1de7f97200b99edf.arrow
Running tokenizer on dataset:  34%|████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                       | 9000/26253 [00:02<00:04, 3505.19 examples/s][rank1]:[W310 15:55:31.892519962 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26253/26253 [00:06<00:00, 3977.13 examples/s]
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                              | 0/1384 [00:00<?, ? examples/s]Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-77f0ec9809063a21.arrow
03/10/2025 15:55:35 - INFO - datasets.arrow_dataset - Caching processed dataset at /raid/chrisjihee/proj/GNER/.cache/json/default-cc9ee36d8392f1d6/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-77f0ec9809063a21.arrow
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1384/1384 [00:00<00:00, 4161.88 examples/s]
[rank0]:[W310 15:55:38.864579860 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.
03/10/2025 15:55:38 - INFO - __main__ - Sample 20952 of the training set: {'sentence1': 'who(B-actor) starred(O) in(O) marley(B-title) and(I-title) me(I-title)', 'sentence2': 'who starred in marley and me', 'label': 2.71, 'idx': 20952, 'input_ids': [0, 8155, 1640, 387, 12, 24625, 43, 12913, 1640, 673, 43, 11, 1640, 673, 43, 4401, 607, 1640, 387, 12, 14691, 43, 8, 1640, 100, 12, 14691, 43, 162, 1640, 100, 12, 14691, 43, 2, 2, 8155, 12913, 11, 4401, 607, 8, 162, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 15:55:38 - INFO - __main__ - Sample 3648 of the training set: {'sentence1': 'The(O) Spanish(B-language) edition(O) of(O) Campus(B-conference) Party(I-conference) has(O) been(O) held(O) at(O) the(O) Colegio(B-university) Miguel(I-university) Hernández(I-university),(O) Ceulaj(B-location),(O) and(O) the(O) Municipal(B-location) Sport(I-location) Arena(I-location) of(I-location) Benalmádena(I-location) in(O) Málaga(B-location),(O) Spain(B-country) ;(O) and(O) at(O) both(O) the(O) Valencia(B-location) County(I-location) Fair(I-location) and(O) the(O) City(B-location) of(I-location) Arts(I-location) and(I-location) Sciences(I-location) in(O) Valencia(B-location) over(O) the(O) past(O) 15(O) years(O).(O)', 'sentence2': 'The Spanish edition of Campus Party has been held at the Colegio Miguel Hernández , Ceulaj , and the Municipal Sport Arena of Benalmádena in Málaga , Spain ; and at both the Valencia County Fair and the City of Arts and Sciences in Valencia over the past 15 years .', 'label': 3.32, 'idx': 3648, 'input_ids': [0, 133, 1640, 673, 43, 3453, 1640, 387, 12, 19527, 43, 5403, 1640, 673, 43, 9, 1640, 673, 43, 14416, 1640, 387, 12, 16459, 43, 1643, 1640, 100, 12, 16459, 43, 34, 1640, 673, 43, 57, 1640, 673, 43, 547, 1640, 673, 43, 23, 1640, 673, 43, 5, 1640, 673, 43, 944, 4308, 1020, 1640, 387, 12, 879, 31104, 43, 12450, 1640, 100, 12, 879, 31104, 43, 289, 3281, 1526, 1187, 5841, 1640, 100, 12, 879, 31104, 238, 1640, 673, 43, 15858, 922, 1176, 1640, 387, 12, 41829, 238, 1640, 673, 43, 8, 1640, 673, 43, 5, 1640, 673, 43, 11660, 1640, 387, 12, 41829, 43, 5413, 1640, 100, 12, 41829, 43, 5036, 1640, 100, 12, 41829, 43, 9, 1640, 100, 12, 41829, 43, 1664, 30293, 1526, 3898, 102, 1640, 100, 12, 41829, 43, 11, 1640, 673, 43, 256, 1526, 462, 6080, 1640, 387, 12, 41829, 238, 1640, 673, 43, 2809, 1640, 387, 12, 12659, 43, 25606, 1640, 673, 43, 8, 1640, 673, 43, 23, 1640, 673, 43, 258, 1640, 673, 43, 5, 1640, 673, 43, 14567, 1640, 387, 12, 41829, 43, 413, 1640, 100, 12, 41829, 43, 3896, 1640, 100, 12, 41829, 43, 8, 1640, 673, 43, 5, 1640, 673, 43, 412, 1640, 387, 12, 41829, 43, 9, 1640, 100, 12, 41829, 43, 4455, 1640, 100, 12, 41829, 43, 8, 1640, 100, 12, 41829, 43, 8841, 1640, 100, 12, 41829, 43, 11, 1640, 673, 43, 14567, 1640, 387, 12, 41829, 43, 81, 1640, 673, 43, 5, 1640, 673, 43, 375, 1640, 673, 43, 379, 1640, 673, 43, 107, 1640, 673, 322, 1640, 673, 43, 2, 2, 133, 3453, 5403, 9, 14416, 1643, 34, 57, 547, 23, 5, 944, 4308, 1020, 12450, 289, 3281, 1526, 1187, 5841, 2156, 15858, 922, 1176, 2156, 8, 5, 11660, 5413, 5036, 9, 1664, 30293, 1526, 3898, 102, 11, 256, 1526, 462, 6080, 2156, 2809, 25606, 8, 23, 258, 5, 14567, 413, 3896, 8, 5, 412, 9, 4455, 8, 8841, 11, 14567, 81, 5, 375, 379, 107, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
03/10/2025 15:55:38 - INFO - __main__ - Sample 819 of the training set: {'sentence1': 'Two(O) professors(O),(O) Hal(B-person) Abelson(I-person) and(O) Gerald(B-person) Jay(I-person) Sussman(I-person),(O) chose(O) to(O) remain(O) neutral(O) -(O) their(O) group(O) was(O) referred(O) to(O) variously(O) as(O) Switzerland(B-country) and(O) Project(B-event) MAC(I-event) for(O) the(O) next(O) 30(O) years(O).(O)', 'sentence2': 'Two professors , Hal Abelson and Gerald Jay Sussman , chose to remain neutral - their group was referred to variously as Switzerland and Project MAC for the next 30 years .', 'label': 1.16, 'idx': 819, 'input_ids': [0, 9058, 1640, 673, 43, 18341, 1640, 673, 238, 1640, 673, 43, 6579, 1640, 387, 12, 5970, 43, 2060, 16475, 1640, 100, 12, 5970, 43, 8, 1640, 673, 43, 14651, 1640, 387, 12, 5970, 43, 3309, 1640, 100, 12, 5970, 43, 208, 4781, 397, 1640, 100, 12, 5970, 238, 1640, 673, 43, 4689, 1640, 673, 43, 7, 1640, 673, 43, 1091, 1640, 673, 43, 7974, 1640, 673, 43, 111, 1640, 673, 43, 49, 1640, 673, 43, 333, 1640, 673, 43, 21, 1640, 673, 43, 4997, 1640, 673, 43, 7, 1640, 673, 43, 1337, 352, 1640, 673, 43, 25, 1640, 673, 43, 6413, 1640, 387, 12, 12659, 43, 8, 1640, 673, 43, 3728, 1640, 387, 12, 21680, 43, 19482, 1640, 100, 12, 21680, 43, 13, 1640, 673, 43, 5, 1640, 673, 43, 220, 1640, 673, 43, 389, 1640, 673, 43, 107, 1640, 673, 322, 1640, 673, 43, 2, 2, 9058, 18341, 2156, 6579, 2060, 16475, 8, 14651, 3309, 208, 4781, 397, 2156, 4689, 7, 1091, 7974, 111, 49, 333, 21, 4997, 7, 1337, 352, 25, 6413, 8, 3728, 19482, 13, 5, 220, 389, 107, 479, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.
Running tokenizer on dataset:   0%|                                                                                                                                                                                                                                                                                                                             | 0/26253 [00:00<?, ? examples/s][INFO|trainer.py:748] 2025-03-10 15:55:38,924 >> Using auto half precision backend
[INFO|trainer.py:930] 2025-03-10 15:55:39,141 >> The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[2025-03-10 15:55:39,147] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed info: version=0.16.4, git-hash=unknown, git-branch=unknown
[2025-03-10 15:55:39,148] [INFO] [config.py:734:__init__] Config mesh_device None world_size = 2
Running tokenizer on dataset: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 26253/26253 [00:06<00:00, 3955.06 examples/s]
Running tokenizer on dataset: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1384/1384 [00:00<00:00, 4339.22 examples/s]
[2025-03-10 15:55:48,652] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2025-03-10 15:55:48,657] [INFO] [logging.py:128:log_dist] [Rank 0] Using DeepSpeed Optimizer param name adamw as basic optimizer
[2025-03-10 15:55:48,657] [INFO] [logging.py:128:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2025-03-10 15:55:48,673] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam
[2025-03-10 15:55:48,674] [INFO] [utils.py:59:is_zero_supported_optimizer] Checking ZeRO support for optimizer=FusedAdam type=<class 'deepspeed.ops.adam.fused_adam.FusedAdam'>
[2025-03-10 15:55:48,674] [INFO] [logging.py:128:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 1 optimizer
[2025-03-10 15:55:48,674] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000
[2025-03-10 15:55:48,674] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000
[2025-03-10 15:55:48,674] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False
[2025-03-10 15:55:48,674] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False
[2025-03-10 15:55:49,159] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 15:55:49,559] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2025-03-10 15:55:49,560] [INFO] [utils.py:782:see_memory_usage] MA 1.32 GB         Max_MA 1.66 GB         CA 1.66 GB         Max_CA 2 GB
[2025-03-10 15:55:49,560] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 54.79 GB, percent = 5.4%
[2025-03-10 15:55:50,858] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2025-03-10 15:55:50,859] [INFO] [utils.py:782:see_memory_usage] MA 1.32 GB         Max_MA 1.99 GB         CA 2.32 GB         Max_CA 2 GB
[2025-03-10 15:55:50,859] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 55.37 GB, percent = 5.5%
[2025-03-10 15:55:50,860] [INFO] [stage_1_and_2.py:550:__init__] optimizer state initialized
[2025-03-10 15:55:51,082] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2025-03-10 15:55:51,083] [INFO] [utils.py:782:see_memory_usage] MA 1.32 GB         Max_MA 1.32 GB         CA 2.32 GB         Max_CA 2 GB
[2025-03-10 15:55:51,084] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 55.4 GB, percent = 5.5%
[2025-03-10 15:55:51,088] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2025-03-10 15:55:51,088] [WARNING] [lr_schedules.py:683:get_lr] Attempting to get learning rate from scheduler before it has started
[2025-03-10 15:55:51,088] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed using configured LR scheduler = WarmupLR
[2025-03-10 15:55:51,088] [INFO] [logging.py:128:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f59ac4d5e50>
[2025-03-10 15:55:51,088] [INFO] [logging.py:128:log_dist] [Rank 0] step=0, skipped=0, lr=[0], mom=[[0.9, 0.999]]
[2025-03-10 15:55:51,090] [INFO] [config.py:1001:print] DeepSpeedEngine configuration:
[2025-03-10 15:55:51,090] [INFO] [config.py:1005:print]   activation_checkpointing_config  {
    "partition_activations": false,
    "contiguous_memory_optimization": false,
    "cpu_checkpointing": false,
    "number_checkpoints": null,
    "synchronize_checkpoint_boundary": false,
    "profile": false
}
[2025-03-10 15:55:51,090] [INFO] [config.py:1005:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'intra_op_parallelism': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}
[2025-03-10 15:55:51,090] [INFO] [config.py:1005:print]   amp_enabled .................. False
[2025-03-10 15:55:51,090] [INFO] [config.py:1005:print]   amp_params ................... False
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   autotuning_config ............ {
    "enabled": false,
    "start_step": null,
    "end_step": null,
    "metric_path": null,
    "arg_mappings": null,
    "metric": "throughput",
    "model_info": null,
    "results_dir": "autotuning_results",
    "exps_dir": "autotuning_exps",
    "overwrite": true,
    "fast": true,
    "start_profile_step": 3,
    "end_profile_step": 5,
    "tuner_type": "gridsearch",
    "tuner_early_stopping": 5,
    "tuner_num_trials": 50,
    "model_info_path": null,
    "mp_size": 1,
    "max_train_batch_size": null,
    "min_train_batch_size": 1,
    "max_train_micro_batch_size_per_gpu": 1.024000e+03,
    "min_train_micro_batch_size_per_gpu": 1,
    "num_tuning_micro_batch_sizes": 3
}
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   bfloat16_enabled ............. True
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   bfloat16_immediate_grad_update  False
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   checkpoint_parallel_write_pipeline  False
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   checkpoint_tag_validation_enabled  True
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   checkpoint_tag_validation_fail  False
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f59ac7f1bb0>
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   communication_data_type ...... None
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   curriculum_enabled_legacy .... False
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   curriculum_params_legacy ..... False
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   data_efficiency_enabled ...... False
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   dataloader_drop_last ......... False
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   disable_allgather ............ False
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   dump_state ................... False
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   dynamic_loss_scale_args ...... None
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   eigenvalue_enabled ........... False
[2025-03-10 15:55:51,091] [INFO] [config.py:1005:print]   eigenvalue_gas_boundary_resolution  1
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   eigenvalue_layer_num ......... 0
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   eigenvalue_max_iter .......... 100
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   eigenvalue_stability ......... 1e-06
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   eigenvalue_tol ............... 0.01
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   eigenvalue_verbose ........... False
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   elasticity_enabled ........... False
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   flops_profiler_config ........ {
    "enabled": false,
    "recompute_fwd_factor": 0.0,
    "profile_step": 1,
    "module_depth": -1,
    "top_modules": 1,
    "detailed": true,
    "output_file": null
}
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   fp16_auto_cast ............... None
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   fp16_enabled ................. False
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   fp16_master_weights_and_gradients  False
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   global_rank .................. 0
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   grad_accum_dtype ............. None
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   gradient_accumulation_steps .. 4
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   gradient_clipping ............ 1.0
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   gradient_predivide_factor .... 1.0
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   graph_harvesting ............. False
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   initial_dynamic_scale ........ 1
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   load_universal_checkpoint .... False
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   loss_scale ................... 1.0
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   memory_breakdown ............. False
[2025-03-10 15:55:51,092] [INFO] [config.py:1005:print]   mics_hierarchial_params_gather  False
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   mics_shard_size .............. -1
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   nebula_config ................ {
    "enabled": false,
    "persistent_storage_path": null,
    "persistent_time_interval": 100,
    "num_of_version_in_retention": 2,
    "enable_nebula_load": true,
    "load_path": null
}
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   optimizer_legacy_fusion ...... False
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   optimizer_name ............... adamw
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   optimizer_params ............. {'lr': 2e-05, 'betas': [0.9, 0.999], 'eps': 1e-08, 'weight_decay': 0.0}
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   pld_enabled .................. False
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   pld_params ................... False
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   prescale_gradients ........... False
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   scheduler_name ............... WarmupLR
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 2e-05, 'warmup_num_steps': 0}
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   seq_parallel_communication_data_type  torch.float32
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   sparse_attention ............. None
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   sparse_gradients_enabled ..... False
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   steps_per_print .............. inf
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   tensor_parallel_config ....... dtype=torch.float16 autotp_size=0 tensor_parallel=TPConfig(tp_size=1, tp_grain_size=1, mpu=None, tp_group=None) injection_policy_tuple=None keep_module_on_host=False replace_with_kernel_inject=False
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   timers_config ................ enabled=True synchronized=True
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   train_batch_size ............. 32
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   train_micro_batch_size_per_gpu  4
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   use_data_before_expert_parallel_  False
[2025-03-10 15:55:51,093] [INFO] [config.py:1005:print]   use_node_local_storage ....... False
[2025-03-10 15:55:51,094] [INFO] [config.py:1005:print]   wall_clock_breakdown ......... False
[2025-03-10 15:55:51,094] [INFO] [config.py:1005:print]   weight_quantization_config ... None
[2025-03-10 15:55:51,094] [INFO] [config.py:1005:print]   world_size ................... 2
[2025-03-10 15:55:51,094] [INFO] [config.py:1005:print]   zero_allow_untested_optimizer  False
[2025-03-10 15:55:51,094] [INFO] [config.py:1005:print]   zero_config .................. stage=1 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50000000 param_persistence_threshold=100000 model_persistence_threshold=9223372036854775807 max_live_parameters=1000000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False module_granularity_threshold=0 use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False zeropp_loco_param=None mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True log_trace_cache_warnings=False
[2025-03-10 15:55:51,094] [INFO] [config.py:1005:print]   zero_enabled ................. True
[2025-03-10 15:55:51,094] [INFO] [config.py:1005:print]   zero_force_ds_cpu_optimizer .. True
[2025-03-10 15:55:51,094] [INFO] [config.py:1005:print]   zero_optimization_stage ...... 1
[2025-03-10 15:55:51,094] [INFO] [config.py:991:print_user_config]   json = {
    "fp16": {
        "enabled": false
    },
    "bf16": {
        "enabled": true
    },
    "optimizer": {
        "type": "AdamW",
        "params": {
            "lr": 2e-05,
            "betas": [0.9, 0.999],
            "eps": 1e-08,
            "weight_decay": 0.0
        }
    },
    "scheduler": {
        "type": "WarmupLR",
        "params": {
            "warmup_min_lr": 0,
            "warmup_max_lr": 2e-05,
            "warmup_num_steps": 0
        }
    },
    "zero_optimization": {
        "stage": 1,
        "allgather_partitions": true,
        "allgather_bucket_size": 2.000000e+08,
        "overlap_comm": true,
        "reduce_scatter": true,
        "reduce_bucket_size": 2.000000e+08,
        "contiguous_gradients": true
    },
    "gradient_accumulation_steps": 4,
    "gradient_clipping": 1.0,
    "train_batch_size": 32,
    "train_micro_batch_size_per_gpu": 4,
    "steps_per_print": inf
}
[INFO|trainer.py:2414] 2025-03-10 15:55:51,096 >> ***** Running training *****
[INFO|trainer.py:2415] 2025-03-10 15:55:51,096 >>   Num examples = 26,253
[INFO|trainer.py:2416] 2025-03-10 15:55:51,096 >>   Num Epochs = 40
[INFO|trainer.py:2417] 2025-03-10 15:55:51,096 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2420] 2025-03-10 15:55:51,096 >>   Total train batch size (w. parallel, distributed & accumulation) = 32
[INFO|trainer.py:2421] 2025-03-10 15:55:51,096 >>   Gradient Accumulation steps = 4
[INFO|trainer.py:2422] 2025-03-10 15:55:51,096 >>   Total optimization steps = 32,800
[INFO|trainer.py:2423] 2025-03-10 15:55:51,097 >>   Number of trainable parameters = 355,360,769
{'loss': 1.3286, 'grad_norm': 17.16128921508789, 'learning_rate': 2e-05, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                             | 820/32800 [03:16<2:08:39,  4.14it/s][INFO|trainer.py:930] 2025-03-10 15:59:07,705 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 15:59:07,708 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 15:59:07,708 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 15:59:07,708 >>   Batch size = 4
{'eval_loss': 1.2314329147338867, 'eval_mse': 1.2315521726029457, 'eval_pearson': 0.6562386536173423, 'eval_spearmanr': 0.6671931306512808, 'eval_runtime': 2.6837, 'eval_samples_per_second': 515.707, 'eval_steps_per_second': 64.463, 'epoch': 1.0}
  2%|████████▌                                                                                                                                                                                                                                                                                                                                             | 820/32800 [03:19<2:08:39,  4.14it/s]
[INFO|trainer.py:3955] 2025-03-10 15:59:10,710 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-820
[INFO|configuration_utils.py:423] 2025-03-10 15:59:10,712 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-820/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 15:59:11,637 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-820/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 15:59:11,638 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-820/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 15:59:11,638 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-820/special_tokens_map.json
[2025-03-10 15:59:11,703] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step820 is about to be saved!
[2025-03-10 15:59:11,709] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-820/global_step820/mp_rank_00_model_states.pt
[2025-03-10 15:59:11,709] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-820/global_step820/mp_rank_00_model_states.pt...
[2025-03-10 15:59:12,790] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-820/global_step820/mp_rank_00_model_states.pt.
[2025-03-10 15:59:12,792] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-820/global_step820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 15:59:16,500] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-820/global_step820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 15:59:16,501] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-820/global_step820/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 15:59:16,501] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step820 is ready now!
{'loss': 0.8611, 'grad_norm': 16.2548885345459, 'learning_rate': 2e-05, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                    | 1640/32800 [06:37<2:04:24,  4.17it/s][INFO|trainer.py:930] 2025-03-10 16:02:28,947 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:02:28,952 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:02:28,952 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:02:28,952 >>   Batch size = 4
{'eval_loss': 1.2151541709899902, 'eval_mse': 1.2151047463361928, 'eval_pearson': 0.6611379033533235, 'eval_spearmanr': 0.6709211567734705, 'eval_runtime': 2.733, 'eval_samples_per_second': 506.408, 'eval_steps_per_second': 63.301, 'epoch': 2.0}
  5%|█████████████████                                                                                                                                                                                                                                                                                                                                    | 1640/32800 [06:40<2:04:24,  4.17it/s]
[INFO|trainer.py:3955] 2025-03-10 16:02:31,916 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-1640
[INFO|configuration_utils.py:423] 2025-03-10 16:02:31,918 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-1640/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:02:32,615 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-1640/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:02:32,616 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-1640/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:02:32,616 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-1640/special_tokens_map.json
[2025-03-10 16:02:32,680] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step1640 is about to be saved!
[2025-03-10 16:02:32,686] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-1640/global_step1640/mp_rank_00_model_states.pt
[2025-03-10 16:02:32,686] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-1640/global_step1640/mp_rank_00_model_states.pt...
[2025-03-10 16:02:33,622] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-1640/global_step1640/mp_rank_00_model_states.pt.
[2025-03-10 16:02:33,623] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-1640/global_step1640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:02:36,996] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-1640/global_step1640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:02:36,997] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-1640/global_step1640/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:02:36,997] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step1640 is ready now!
{'loss': 0.7764, 'grad_norm': 7.640575885772705, 'learning_rate': 2e-05, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 2460/32800 [09:56<1:56:51,  4.33it/s][INFO|trainer.py:930] 2025-03-10 16:05:47,575 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:05:47,578 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:05:47,578 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:05:47,578 >>   Batch size = 4
{'eval_loss': 1.2308449745178223, 'eval_mse': 1.2308888307885628, 'eval_pearson': 0.6509042839796647, 'eval_spearmanr': 0.6639058622012248, 'eval_runtime': 2.6277, 'eval_samples_per_second': 526.695, 'eval_steps_per_second': 65.837, 'epoch': 3.0}
  8%|█████████████████████████▌                                                                                                                                                                                                                                                                                                                           | 2460/32800 [09:59<1:56:51,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 16:05:50,481 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-2460
[INFO|configuration_utils.py:423] 2025-03-10 16:05:50,483 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-2460/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:05:51,208 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-2460/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:05:51,209 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-2460/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:05:51,210 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-2460/special_tokens_map.json
[2025-03-10 16:05:51,286] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step2460 is about to be saved!
[2025-03-10 16:05:51,291] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-2460/global_step2460/mp_rank_00_model_states.pt
[2025-03-10 16:05:51,291] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-2460/global_step2460/mp_rank_00_model_states.pt...
[2025-03-10 16:05:52,237] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-2460/global_step2460/mp_rank_00_model_states.pt.
[2025-03-10 16:05:52,238] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-2460/global_step2460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:05:55,738] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-2460/global_step2460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:05:55,739] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-2460/global_step2460/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:05:55,739] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step2460 is ready now!
{'loss': 0.7739, 'grad_norm': 3.553500175476074, 'learning_rate': 2e-05, 'epoch': 4.0}
 10%|██████████████████████████████████                                                                                                                                                                                                                                                                                                                   | 3280/32800 [13:11<1:53:18,  4.34it/s][INFO|trainer.py:930] 2025-03-10 16:09:03,097 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:09:03,100 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:09:03,100 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:09:03,100 >>   Batch size = 4
{'eval_loss': 1.4719510078430176, 'eval_mse': 1.4718747356034427, 'eval_pearson': 0.6305261750701554, 'eval_spearmanr': 0.6372981098545617, 'eval_runtime': 2.6301, 'eval_samples_per_second': 526.208, 'eval_steps_per_second': 65.776, 'epoch': 4.0}
 10%|██████████████████████████████████                                                                                                                                                                                                                                                                                                                   | 3280/32800 [13:14<1:53:18,  4.34it/s]
[INFO|trainer.py:3955] 2025-03-10 16:09:06,007 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-3280
[INFO|configuration_utils.py:423] 2025-03-10 16:09:06,009 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-3280/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:09:06,722 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-3280/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:09:06,723 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-3280/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:09:06,723 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-3280/special_tokens_map.json
[2025-03-10 16:09:06,783] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step3281 is about to be saved!
[2025-03-10 16:09:06,788] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-3280/global_step3281/mp_rank_00_model_states.pt
[2025-03-10 16:09:06,788] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-3280/global_step3281/mp_rank_00_model_states.pt...
[2025-03-10 16:09:07,788] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-3280/global_step3281/mp_rank_00_model_states.pt.
[2025-03-10 16:09:07,790] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-3280/global_step3281/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:09:11,162] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-3280/global_step3281/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:09:11,162] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-3280/global_step3281/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:09:11,162] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step3281 is ready now!
{'loss': 0.6286, 'grad_norm': 18.785263061523438, 'learning_rate': 2e-05, 'epoch': 5.0}
 12%|██████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                          | 4100/32800 [16:29<1:52:01,  4.27it/s][INFO|trainer.py:930] 2025-03-10 16:12:20,336 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:12:20,339 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:12:20,339 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:12:20,339 >>   Batch size = 4
{'eval_loss': 1.1851603984832764, 'eval_mse': 1.1852303553867891, 'eval_pearson': 0.6680503120816852, 'eval_spearmanr': 0.6739313738664223, 'eval_runtime': 2.6277, 'eval_samples_per_second': 526.7, 'eval_steps_per_second': 65.838, 'epoch': 5.0}
 12%|██████████████████████████████████████████▋                                                                                                                                                                                                                                                                                                          | 4100/32800 [16:31<1:52:01,  4.27it/s]
[INFO|trainer.py:3955] 2025-03-10 16:12:23,271 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4100
[INFO|configuration_utils.py:423] 2025-03-10 16:12:23,274 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4100/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:12:24,049 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4100/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:12:24,050 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4100/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:12:24,050 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4100/special_tokens_map.json
[2025-03-10 16:12:24,112] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4101 is about to be saved!
[2025-03-10 16:12:24,117] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4100/global_step4101/mp_rank_00_model_states.pt
[2025-03-10 16:12:24,117] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4100/global_step4101/mp_rank_00_model_states.pt...
[2025-03-10 16:12:25,080] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4100/global_step4101/mp_rank_00_model_states.pt.
[2025-03-10 16:12:25,082] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4100/global_step4101/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:12:28,593] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4100/global_step4101/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:12:28,594] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4100/global_step4101/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:12:28,594] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4101 is ready now!
{'loss': 0.5549, 'grad_norm': 28.019567489624023, 'learning_rate': 2e-05, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                 | 4920/32800 [19:46<1:46:29,  4.36it/s][INFO|trainer.py:930] 2025-03-10 16:15:37,347 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:15:37,350 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:15:37,350 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:15:37,350 >>   Batch size = 4
{'eval_loss': 1.5496892929077148, 'eval_mse': 1.549544251378561, 'eval_pearson': 0.6550087987851232, 'eval_spearmanr': 0.6611811284294457, 'eval_runtime': 2.6343, 'eval_samples_per_second': 525.373, 'eval_steps_per_second': 65.672, 'epoch': 6.0}
 15%|███████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                                 | 4920/32800 [19:48<1:46:29,  4.36it/s]
[INFO|trainer.py:3955] 2025-03-10 16:15:40,264 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4920
[INFO|configuration_utils.py:423] 2025-03-10 16:15:40,266 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4920/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:15:41,027 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4920/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:15:41,028 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4920/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:15:41,028 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4920/special_tokens_map.json
[2025-03-10 16:15:41,088] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step4921 is about to be saved!
[2025-03-10 16:15:41,093] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4920/global_step4921/mp_rank_00_model_states.pt
[2025-03-10 16:15:41,093] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4920/global_step4921/mp_rank_00_model_states.pt...
[2025-03-10 16:15:42,084] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4920/global_step4921/mp_rank_00_model_states.pt.
[2025-03-10 16:15:42,086] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4920/global_step4921/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:15:45,217] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4920/global_step4921/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:15:45,218] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-4920/global_step4921/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:15:45,218] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step4921 is ready now!
{'loss': 0.5192, 'grad_norm': 11.910953521728516, 'learning_rate': 2e-05, 'epoch': 7.0}
 18%|███████████████████████████████████████████████████████████▋                                                                                                                                                                                                                                                                                         | 5740/32800 [23:05<1:44:59,  4.30it/s][INFO|trainer.py:930] 2025-03-10 16:18:57,054 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:18:57,058 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:18:57,058 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:18:57,058 >>   Batch size = 4
{'eval_loss': 1.190447449684143, 'eval_mse': 1.1902872346384676, 'eval_pearson': 0.6852144136210996, 'eval_spearmanr': 0.6846593024754232, 'eval_runtime': 2.6795, 'eval_samples_per_second': 516.505, 'eval_steps_per_second': 64.563, 'epoch': 7.0}
 18%|███████████████████████████████████████████████████████████▋                                                                                                                                                                                                                                                                                         | 5740/32800 [23:08<1:44:59,  4.30it/s]
[INFO|trainer.py:3955] 2025-03-10 16:18:59,981 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-5740
[INFO|configuration_utils.py:423] 2025-03-10 16:18:59,983 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-5740/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:19:00,862 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-5740/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:19:00,863 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-5740/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:19:00,863 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-5740/special_tokens_map.json
[2025-03-10 16:19:00,926] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step5741 is about to be saved!
[2025-03-10 16:19:00,931] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-5740/global_step5741/mp_rank_00_model_states.pt
[2025-03-10 16:19:00,931] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-5740/global_step5741/mp_rank_00_model_states.pt...
[2025-03-10 16:19:01,974] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-5740/global_step5741/mp_rank_00_model_states.pt.
[2025-03-10 16:19:01,976] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-5740/global_step5741/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:19:05,130] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-5740/global_step5741/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:19:05,131] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-5740/global_step5741/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:19:05,131] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step5741 is ready now!
{'loss': 0.5529, 'grad_norm': 11.616561889648438, 'learning_rate': 2e-05, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                | 6560/32800 [26:22<1:39:16,  4.41it/s][INFO|trainer.py:930] 2025-03-10 16:22:13,385 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:22:13,389 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:22:13,389 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:22:13,389 >>   Batch size = 4
{'eval_loss': 1.279138207435608, 'eval_mse': 1.279265294529799, 'eval_pearson': 0.6639752297108372, 'eval_spearmanr': 0.6753032439031565, 'eval_runtime': 2.6791, 'eval_samples_per_second': 516.582, 'eval_steps_per_second': 64.573, 'epoch': 8.0}
 20%|████████████████████████████████████████████████████████████████████▏                                                                                                                                                                                                                                                                                | 6560/32800 [26:24<1:39:16,  4.41it/s]
[INFO|trainer.py:3955] 2025-03-10 16:22:16,316 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-6560
[INFO|configuration_utils.py:423] 2025-03-10 16:22:16,318 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-6560/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:22:17,232 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-6560/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:22:17,233 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-6560/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:22:17,233 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-6560/special_tokens_map.json
[2025-03-10 16:22:17,300] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step6562 is about to be saved!
[2025-03-10 16:22:17,306] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-6560/global_step6562/mp_rank_00_model_states.pt
[2025-03-10 16:22:17,306] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-6560/global_step6562/mp_rank_00_model_states.pt...
[2025-03-10 16:22:18,335] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-6560/global_step6562/mp_rank_00_model_states.pt.
[2025-03-10 16:22:18,527] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-6560/global_step6562/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:22:21,908] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-6560/global_step6562/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:22:21,908] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-6560/global_step6562/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:22:21,908] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step6562 is ready now!
{'loss': 0.4123, 'grad_norm': 10.498736381530762, 'learning_rate': 2e-05, 'epoch': 9.0}
 22%|████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                                                                        | 7380/32800 [29:42<1:39:00,  4.28it/s][INFO|trainer.py:930] 2025-03-10 16:25:33,524 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:25:33,528 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:25:33,528 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:25:33,528 >>   Batch size = 4
{'eval_loss': 0.9779749512672424, 'eval_mse': 0.9784126250730084, 'eval_pearson': 0.7518991464857656, 'eval_spearmanr': 0.7642096902493337, 'eval_runtime': 2.6541, 'eval_samples_per_second': 521.462, 'eval_steps_per_second': 65.183, 'epoch': 9.0}
 22%|████████████████████████████████████████████████████████████████████████████▋                                                                                                                                                                                                                                                                        | 7380/32800 [29:45<1:39:00,  4.28it/s]
[INFO|trainer.py:3955] 2025-03-10 16:25:36,470 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-7380
[INFO|configuration_utils.py:423] 2025-03-10 16:25:36,472 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-7380/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:25:37,254 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-7380/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:25:37,255 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-7380/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:25:37,255 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-7380/special_tokens_map.json
[2025-03-10 16:25:37,301] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step7382 is about to be saved!
[2025-03-10 16:25:37,306] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-7380/global_step7382/mp_rank_00_model_states.pt
[2025-03-10 16:25:37,307] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-7380/global_step7382/mp_rank_00_model_states.pt...
[2025-03-10 16:25:38,481] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-7380/global_step7382/mp_rank_00_model_states.pt.
[2025-03-10 16:25:38,483] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-7380/global_step7382/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:25:42,003] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-7380/global_step7382/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:25:42,004] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-7380/global_step7382/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:25:42,004] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step7382 is ready now!
{'loss': 0.3317, 'grad_norm': 6.36337947845459, 'learning_rate': 2e-05, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                                                               | 8200/32800 [33:01<1:39:04,  4.14it/s][INFO|trainer.py:930] 2025-03-10 16:28:52,909 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:28:52,912 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:28:52,912 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:28:52,912 >>   Batch size = 4
{'eval_loss': 1.0092766284942627, 'eval_mse': 1.0092631898863467, 'eval_pearson': 0.7438835443558577, 'eval_spearmanr': 0.7466120379097981, 'eval_runtime': 2.7622, 'eval_samples_per_second': 501.053, 'eval_steps_per_second': 62.632, 'epoch': 10.0}
 25%|█████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                                                               | 8200/32800 [33:04<1:39:04,  4.14it/s]
[INFO|trainer.py:3955] 2025-03-10 16:28:55,942 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-8200
[INFO|configuration_utils.py:423] 2025-03-10 16:28:55,944 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-8200/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:28:56,853 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-8200/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:28:56,854 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-8200/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:28:56,854 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-8200/special_tokens_map.json
[2025-03-10 16:28:56,930] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step8202 is about to be saved!
[2025-03-10 16:28:56,935] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-8200/global_step8202/mp_rank_00_model_states.pt
[2025-03-10 16:28:56,936] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-8200/global_step8202/mp_rank_00_model_states.pt...
[2025-03-10 16:28:57,980] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-8200/global_step8202/mp_rank_00_model_states.pt.
[2025-03-10 16:28:57,982] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-8200/global_step8202/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:29:01,182] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-8200/global_step8202/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:29:01,182] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-8200/global_step8202/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:29:01,182] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step8202 is ready now!
{'loss': 0.3039, 'grad_norm': 6.195324420928955, 'learning_rate': 2e-05, 'epoch': 11.0}
 28%|█████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                       | 9020/32800 [36:22<1:31:32,  4.33it/s][INFO|trainer.py:930] 2025-03-10 16:32:13,334 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:32:13,337 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:32:13,337 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:32:13,337 >>   Batch size = 4
{'eval_loss': 0.927910327911377, 'eval_mse': 0.9279817049902988, 'eval_pearson': 0.7591105435044239, 'eval_spearmanr': 0.7630078406579144, 'eval_runtime': 2.6759, 'eval_samples_per_second': 517.204, 'eval_steps_per_second': 64.651, 'epoch': 11.0}
 28%|█████████████████████████████████████████████████████████████████████████████████████████████▊                                                                                                                                                                                                                                                       | 9020/32800 [36:24<1:31:32,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 16:32:16,265 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9020
[INFO|configuration_utils.py:423] 2025-03-10 16:32:16,267 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9020/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:32:17,222 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9020/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:32:17,223 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9020/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:32:17,223 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9020/special_tokens_map.json
[2025-03-10 16:32:17,290] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9022 is about to be saved!
[2025-03-10 16:32:17,318] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9020/global_step9022/mp_rank_00_model_states.pt
[2025-03-10 16:32:17,318] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9020/global_step9022/mp_rank_00_model_states.pt...
[2025-03-10 16:32:18,434] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9020/global_step9022/mp_rank_00_model_states.pt.
[2025-03-10 16:32:18,436] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9020/global_step9022/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:32:21,522] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9020/global_step9022/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:32:21,523] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9020/global_step9022/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:32:21,523] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9022 is ready now!
{'loss': 0.3225, 'grad_norm': 2.421079635620117, 'learning_rate': 2e-05, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                                              | 9840/32800 [39:40<1:28:30,  4.32it/s][INFO|trainer.py:930] 2025-03-10 16:35:31,846 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:35:31,849 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:35:31,849 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:35:31,849 >>   Batch size = 4
{'eval_loss': 1.1103795766830444, 'eval_mse': 1.1111041634758083, 'eval_pearson': 0.6970874842658143, 'eval_spearmanr': 0.7050491022425487, 'eval_runtime': 2.6904, 'eval_samples_per_second': 514.429, 'eval_steps_per_second': 64.304, 'epoch': 12.0}
 30%|██████████████████████████████████████████████████████████████████████████████████████████████████████▎                                                                                                                                                                                                                                              | 9840/32800 [39:43<1:28:30,  4.32it/s]
[INFO|trainer.py:3955] 2025-03-10 16:35:34,808 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9840
[INFO|configuration_utils.py:423] 2025-03-10 16:35:34,810 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9840/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:35:35,593 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9840/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:35:35,594 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9840/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:35:35,594 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9840/special_tokens_map.json
[2025-03-10 16:35:35,677] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step9843 is about to be saved!
[2025-03-10 16:35:35,684] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9840/global_step9843/mp_rank_00_model_states.pt
[2025-03-10 16:35:35,684] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9840/global_step9843/mp_rank_00_model_states.pt...
[2025-03-10 16:35:36,655] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9840/global_step9843/mp_rank_00_model_states.pt.
[2025-03-10 16:35:36,656] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9840/global_step9843/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:35:39,828] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9840/global_step9843/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:35:39,828] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-9840/global_step9843/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:35:39,828] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step9843 is ready now!
{'loss': 0.2376, 'grad_norm': 29.043916702270508, 'learning_rate': 2e-05, 'epoch': 13.0}
 32%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                     | 10660/32800 [43:00<1:24:49,  4.35it/s][INFO|trainer.py:930] 2025-03-10 16:38:51,150 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:38:51,153 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:38:51,153 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:38:51,153 >>   Batch size = 4
{'eval_loss': 0.9356803894042969, 'eval_mse': 0.9354439846353034, 'eval_pearson': 0.7642230380698192, 'eval_spearmanr': 0.7803553066730008, 'eval_runtime': 2.6322, 'eval_samples_per_second': 525.793, 'eval_steps_per_second': 65.724, 'epoch': 13.0}
 32%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                                     | 10660/32800 [43:02<1:24:49,  4.35it/s]
[INFO|trainer.py:3955] 2025-03-10 16:38:54,118 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-10660
[INFO|configuration_utils.py:423] 2025-03-10 16:38:54,120 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-10660/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:38:55,053 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-10660/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:38:55,054 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-10660/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:38:55,054 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-10660/special_tokens_map.json
[2025-03-10 16:38:55,120] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step10663 is about to be saved!
[2025-03-10 16:38:55,126] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-10660/global_step10663/mp_rank_00_model_states.pt
[2025-03-10 16:38:55,126] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-10660/global_step10663/mp_rank_00_model_states.pt...
[2025-03-10 16:38:56,297] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-10660/global_step10663/mp_rank_00_model_states.pt.
[2025-03-10 16:38:56,298] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-10660/global_step10663/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:38:59,542] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-10660/global_step10663/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:38:59,542] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-10660/global_step10663/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:38:59,542] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step10663 is ready now!
{'loss': 0.1927, 'grad_norm': 14.447552680969238, 'learning_rate': 2e-05, 'epoch': 14.0}
 35%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                             | 11480/32800 [46:17<1:23:02,  4.28it/s][INFO|trainer.py:930] 2025-03-10 16:42:08,202 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:42:08,205 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:42:08,205 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:42:08,205 >>   Batch size = 4
{'eval_loss': 0.9299530982971191, 'eval_mse': 0.9302075870464303, 'eval_pearson': 0.7679961219861933, 'eval_spearmanr': 0.7783688877187047, 'eval_runtime': 2.6439, 'eval_samples_per_second': 523.462, 'eval_steps_per_second': 65.433, 'epoch': 14.0}
 35%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                                                                                                                                             | 11480/32800 [46:19<1:23:02,  4.28it/s]
[INFO|trainer.py:3955] 2025-03-10 16:42:11,141 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-11480
[INFO|configuration_utils.py:423] 2025-03-10 16:42:11,144 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-11480/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:42:12,026 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-11480/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:42:12,027 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-11480/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:42:12,027 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-11480/special_tokens_map.json
[2025-03-10 16:42:12,071] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step11483 is about to be saved!
[2025-03-10 16:42:12,076] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-11480/global_step11483/mp_rank_00_model_states.pt
[2025-03-10 16:42:12,076] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-11480/global_step11483/mp_rank_00_model_states.pt...
[2025-03-10 16:42:13,091] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-11480/global_step11483/mp_rank_00_model_states.pt.
[2025-03-10 16:42:13,134] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-11480/global_step11483/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:42:16,292] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-11480/global_step11483/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:42:16,293] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-11480/global_step11483/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:42:16,293] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step11483 is ready now!
{'loss': 0.1808, 'grad_norm': 2.4238452911376953, 'learning_rate': 2e-05, 'epoch': 15.0}
 38%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                    | 12300/32800 [49:34<1:19:18,  4.31it/s][INFO|trainer.py:930] 2025-03-10 16:45:25,129 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:45:25,132 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:45:25,132 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:45:25,132 >>   Batch size = 4
{'eval_loss': 0.9557899236679077, 'eval_mse': 0.9555050207011272, 'eval_pearson': 0.7704499960249562, 'eval_spearmanr': 0.7791378158869697, 'eval_runtime': 2.6374, 'eval_samples_per_second': 524.75, 'eval_steps_per_second': 65.594, 'epoch': 15.0}
 38%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                                    | 12300/32800 [49:36<1:19:18,  4.31it/s]
[INFO|trainer.py:3955] 2025-03-10 16:45:28,058 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-12300
[INFO|configuration_utils.py:423] 2025-03-10 16:45:28,060 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-12300/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:45:28,878 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-12300/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:45:28,879 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-12300/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:45:28,879 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-12300/special_tokens_map.json
[2025-03-10 16:45:28,943] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step12303 is about to be saved!
[2025-03-10 16:45:28,948] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-12300/global_step12303/mp_rank_00_model_states.pt
[2025-03-10 16:45:28,948] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-12300/global_step12303/mp_rank_00_model_states.pt...
[2025-03-10 16:45:29,949] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-12300/global_step12303/mp_rank_00_model_states.pt.
[2025-03-10 16:45:29,951] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-12300/global_step12303/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:45:33,274] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-12300/global_step12303/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:45:33,275] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-12300/global_step12303/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:45:33,275] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step12303 is ready now!
{'loss': 0.2083, 'grad_norm': 9.155223846435547, 'learning_rate': 2e-05, 'epoch': 16.0}
 40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                            | 13120/32800 [52:50<1:14:36,  4.40it/s][INFO|trainer.py:930] 2025-03-10 16:48:41,156 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:48:41,160 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:48:41,160 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:48:41,160 >>   Batch size = 4
{'eval_loss': 0.9859457015991211, 'eval_mse': 0.9860596660244672, 'eval_pearson': 0.7691725098195281, 'eval_spearmanr': 0.7834197773509456, 'eval_runtime': 2.693, 'eval_samples_per_second': 513.925, 'eval_steps_per_second': 64.241, 'epoch': 16.0}
 40%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                                            | 13120/32800 [52:52<1:14:36,  4.40it/s]
[INFO|trainer.py:3955] 2025-03-10 16:48:44,133 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13120
[INFO|configuration_utils.py:423] 2025-03-10 16:48:44,135 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13120/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:48:44,836 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13120/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:48:44,837 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13120/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:48:44,837 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13120/special_tokens_map.json
[2025-03-10 16:48:44,911] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13124 is about to be saved!
[2025-03-10 16:48:44,918] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13120/global_step13124/mp_rank_00_model_states.pt
[2025-03-10 16:48:44,918] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13120/global_step13124/mp_rank_00_model_states.pt...
[2025-03-10 16:48:45,967] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13120/global_step13124/mp_rank_00_model_states.pt.
[2025-03-10 16:48:46,131] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13120/global_step13124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:48:49,523] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13120/global_step13124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:48:49,523] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13120/global_step13124/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:48:49,523] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13124 is ready now!
{'loss': 0.1436, 'grad_norm': 6.145490646362305, 'learning_rate': 2e-05, 'epoch': 17.0}
 42%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                   | 13940/32800 [56:08<1:12:17,  4.35it/s][INFO|trainer.py:930] 2025-03-10 16:51:59,276 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:51:59,279 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:51:59,279 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:51:59,279 >>   Batch size = 4
{'eval_loss': 0.9542121887207031, 'eval_mse': 0.9542831516679312, 'eval_pearson': 0.764122601490449, 'eval_spearmanr': 0.7767351184738347, 'eval_runtime': 2.6321, 'eval_samples_per_second': 525.818, 'eval_steps_per_second': 65.727, 'epoch': 17.0}
 42%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                                   | 13940/32800 [56:10<1:12:17,  4.35it/s]
[INFO|trainer.py:3955] 2025-03-10 16:52:02,198 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13940
[INFO|configuration_utils.py:423] 2025-03-10 16:52:02,200 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13940/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:52:03,003 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13940/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:52:03,004 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13940/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:52:03,004 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13940/special_tokens_map.json
[2025-03-10 16:52:03,067] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step13944 is about to be saved!
[2025-03-10 16:52:03,072] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13940/global_step13944/mp_rank_00_model_states.pt
[2025-03-10 16:52:03,072] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13940/global_step13944/mp_rank_00_model_states.pt...
[2025-03-10 16:52:04,052] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13940/global_step13944/mp_rank_00_model_states.pt.
[2025-03-10 16:52:04,054] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13940/global_step13944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:52:07,368] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13940/global_step13944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:52:07,369] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-13940/global_step13944/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:52:07,369] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step13944 is ready now!
{'loss': 0.1132, 'grad_norm': 2.6889302730560303, 'learning_rate': 2e-05, 'epoch': 18.0}
 45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                           | 14760/32800 [59:25<1:10:57,  4.24it/s][INFO|trainer.py:930] 2025-03-10 16:55:16,602 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:55:16,606 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:55:16,606 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:55:16,606 >>   Batch size = 4
{'eval_loss': 1.025339126586914, 'eval_mse': 1.0251508699676204, 'eval_pearson': 0.7432142820690766, 'eval_spearmanr': 0.762491833000798, 'eval_runtime': 2.629, 'eval_samples_per_second': 526.43, 'eval_steps_per_second': 65.804, 'epoch': 18.0}
 45%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                                           | 14760/32800 [59:28<1:10:57,  4.24it/s]
[INFO|trainer.py:3955] 2025-03-10 16:55:19,528 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-14760
[INFO|configuration_utils.py:423] 2025-03-10 16:55:19,530 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-14760/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:55:20,395 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-14760/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:55:20,396 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-14760/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:55:20,396 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-14760/special_tokens_map.json
[2025-03-10 16:55:20,458] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step14764 is about to be saved!
[2025-03-10 16:55:20,464] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-14760/global_step14764/mp_rank_00_model_states.pt
[2025-03-10 16:55:20,464] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-14760/global_step14764/mp_rank_00_model_states.pt...
[2025-03-10 16:55:21,515] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-14760/global_step14764/mp_rank_00_model_states.pt.
[2025-03-10 16:55:21,516] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-14760/global_step14764/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:55:24,627] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-14760/global_step14764/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:55:24,627] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-14760/global_step14764/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:55:24,628] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step14764 is ready now!
{'loss': 0.1133, 'grad_norm': 4.152439117431641, 'learning_rate': 2e-05, 'epoch': 19.0}
 48%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                 | 15580/32800 [1:02:43<1:08:52,  4.17it/s][INFO|trainer.py:930] 2025-03-10 16:58:34,135 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 16:58:34,138 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 16:58:34,138 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 16:58:34,138 >>   Batch size = 4
{'eval_loss': 0.9446192383766174, 'eval_mse': 0.9447283183219116, 'eval_pearson': 0.7688393981829895, 'eval_spearmanr': 0.7820024360114127, 'eval_runtime': 2.6861, 'eval_samples_per_second': 515.253, 'eval_steps_per_second': 64.407, 'epoch': 19.0}
 48%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                                                                                 | 15580/32800 [1:02:45<1:08:52,  4.17it/s]
[INFO|trainer.py:3955] 2025-03-10 16:58:37,060 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-15580
[INFO|configuration_utils.py:423] 2025-03-10 16:58:37,063 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-15580/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 16:58:37,899 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-15580/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 16:58:37,900 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-15580/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 16:58:37,900 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-15580/special_tokens_map.json
[2025-03-10 16:58:37,964] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step15584 is about to be saved!
[2025-03-10 16:58:37,970] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-15580/global_step15584/mp_rank_00_model_states.pt
[2025-03-10 16:58:37,970] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-15580/global_step15584/mp_rank_00_model_states.pt...
[2025-03-10 16:58:38,983] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-15580/global_step15584/mp_rank_00_model_states.pt.
[2025-03-10 16:58:39,169] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-15580/global_step15584/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 16:58:42,491] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-15580/global_step15584/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 16:58:42,492] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-15580/global_step15584/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 16:58:42,492] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step15584 is ready now!
{'loss': 0.1298, 'grad_norm': 0.8482984304428101, 'learning_rate': 2e-05, 'epoch': 20.0}
 50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                         | 16400/32800 [1:06:00<1:03:28,  4.31it/s][INFO|trainer.py:930] 2025-03-10 17:01:51,508 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:01:51,511 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:01:51,512 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:01:51,512 >>   Batch size = 4
{'eval_loss': 1.1782313585281372, 'eval_mse': 1.1782257388092878, 'eval_pearson': 0.7244874454360594, 'eval_spearmanr': 0.7439300718791888, 'eval_runtime': 2.6354, 'eval_samples_per_second': 525.16, 'eval_steps_per_second': 65.645, 'epoch': 20.0}
 50%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                                         | 16400/32800 [1:06:03<1:03:28,  4.31it/s]
[INFO|trainer.py:3955] 2025-03-10 17:01:54,431 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-16400
[INFO|configuration_utils.py:423] 2025-03-10 17:01:54,433 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-16400/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:01:55,301 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-16400/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:01:55,305 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-16400/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:01:55,305 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-16400/special_tokens_map.json
[2025-03-10 17:01:55,368] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step16405 is about to be saved!
[2025-03-10 17:01:55,374] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-16400/global_step16405/mp_rank_00_model_states.pt
[2025-03-10 17:01:55,374] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-16400/global_step16405/mp_rank_00_model_states.pt...
[2025-03-10 17:01:56,413] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-16400/global_step16405/mp_rank_00_model_states.pt.
[2025-03-10 17:01:56,415] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-16400/global_step16405/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:01:59,714] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-16400/global_step16405/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:01:59,715] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-16400/global_step16405/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:01:59,715] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step16405 is ready now!
{'loss': 0.0903, 'grad_norm': 4.838738441467285, 'learning_rate': 2e-05, 'epoch': 21.0}
 52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                | 17220/32800 [1:09:19<1:00:07,  4.32it/s][INFO|trainer.py:930] 2025-03-10 17:05:10,540 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:05:10,544 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:05:10,544 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:05:10,544 >>   Batch size = 4
{'eval_loss': 0.8913399577140808, 'eval_mse': 0.8914987093451395, 'eval_pearson': 0.7801771899894177, 'eval_spearmanr': 0.7942938359907508, 'eval_runtime': 2.6846, 'eval_samples_per_second': 515.532, 'eval_steps_per_second': 64.442, 'epoch': 21.0}
 52%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                                | 17220/32800 [1:09:22<1:00:07,  4.32it/s]
[INFO|trainer.py:3955] 2025-03-10 17:05:13,469 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-17220
[INFO|configuration_utils.py:423] 2025-03-10 17:05:13,471 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-17220/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:05:14,199 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-17220/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:05:14,200 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-17220/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:05:14,200 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-17220/special_tokens_map.json
[2025-03-10 17:05:14,242] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step17225 is about to be saved!
[2025-03-10 17:05:14,247] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-17220/global_step17225/mp_rank_00_model_states.pt
[2025-03-10 17:05:14,247] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-17220/global_step17225/mp_rank_00_model_states.pt...
[2025-03-10 17:05:15,253] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-17220/global_step17225/mp_rank_00_model_states.pt.
[2025-03-10 17:05:15,255] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-17220/global_step17225/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:05:18,594] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-17220/global_step17225/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:05:18,594] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-17220/global_step17225/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:05:18,594] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step17225 is ready now!
{'loss': 0.0742, 'grad_norm': 4.028850078582764, 'learning_rate': 2e-05, 'epoch': 22.0}
 55%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                         | 18040/32800 [1:12:35<56:10,  4.38it/s][INFO|trainer.py:930] 2025-03-10 17:08:26,294 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:08:26,297 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:08:26,297 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:08:26,297 >>   Batch size = 4
{'eval_loss': 0.9530389308929443, 'eval_mse': 0.9531840316822074, 'eval_pearson': 0.769589124699656, 'eval_spearmanr': 0.7806560373632189, 'eval_runtime': 2.6823, 'eval_samples_per_second': 515.976, 'eval_steps_per_second': 64.497, 'epoch': 22.0}
 55%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                                         | 18040/32800 [1:12:37<56:10,  4.38it/s]
[INFO|trainer.py:3955] 2025-03-10 17:08:29,228 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18040
[INFO|configuration_utils.py:423] 2025-03-10 17:08:29,230 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18040/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:08:30,003 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18040/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:08:30,004 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18040/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:08:30,004 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18040/special_tokens_map.json
[2025-03-10 17:08:30,068] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step18045 is about to be saved!
[2025-03-10 17:08:30,073] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18040/global_step18045/mp_rank_00_model_states.pt
[2025-03-10 17:08:30,073] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18040/global_step18045/mp_rank_00_model_states.pt...
[2025-03-10 17:08:31,168] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18040/global_step18045/mp_rank_00_model_states.pt.
[2025-03-10 17:08:31,226] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18040/global_step18045/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:08:34,738] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18040/global_step18045/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:08:34,738] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18040/global_step18045/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:08:34,738] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18045 is ready now!
{'loss': 0.0751, 'grad_norm': 1.438896656036377, 'learning_rate': 2e-05, 'epoch': 23.0}
 57%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                | 18860/32800 [1:15:52<53:11,  4.37it/s][INFO|trainer.py:930] 2025-03-10 17:11:43,641 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:11:43,644 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:11:43,644 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:11:43,644 >>   Batch size = 4
{'eval_loss': 1.0145970582962036, 'eval_mse': 1.0142535291655215, 'eval_pearson': 0.7621050380533416, 'eval_spearmanr': 0.7789028826281468, 'eval_runtime': 2.6389, 'eval_samples_per_second': 524.467, 'eval_steps_per_second': 65.558, 'epoch': 23.0}
 57%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                                                                                                | 18860/32800 [1:15:55<53:11,  4.37it/s]
[INFO|trainer.py:3955] 2025-03-10 17:11:46,583 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18860
[INFO|configuration_utils.py:423] 2025-03-10 17:11:46,585 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18860/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:11:47,432 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18860/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:11:47,433 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18860/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:11:47,433 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18860/special_tokens_map.json
[2025-03-10 17:11:47,513] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step18865 is about to be saved!
[2025-03-10 17:11:47,520] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18860/global_step18865/mp_rank_00_model_states.pt
[2025-03-10 17:11:47,520] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18860/global_step18865/mp_rank_00_model_states.pt...
[2025-03-10 17:11:48,575] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18860/global_step18865/mp_rank_00_model_states.pt.
[2025-03-10 17:11:48,576] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18860/global_step18865/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:11:52,050] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18860/global_step18865/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:11:52,051] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-18860/global_step18865/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:11:52,051] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step18865 is ready now!
{'loss': 0.0836, 'grad_norm': 6.993437767028809, 'learning_rate': 2e-05, 'epoch': 24.0}
 60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                        | 19680/32800 [1:19:10<50:01,  4.37it/s][INFO|trainer.py:930] 2025-03-10 17:15:01,894 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:15:01,897 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:15:01,897 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:15:01,897 >>   Batch size = 4
{'eval_loss': 0.953313946723938, 'eval_mse': 0.95353207250551, 'eval_pearson': 0.7946521198333933, 'eval_spearmanr': 0.80289328732759, 'eval_runtime': 2.6358, 'eval_samples_per_second': 525.069, 'eval_steps_per_second': 65.634, 'epoch': 24.0}
 60%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                                        | 19680/32800 [1:19:13<50:01,  4.37it/s]
[INFO|trainer.py:3955] 2025-03-10 17:15:04,830 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-19680
[INFO|configuration_utils.py:423] 2025-03-10 17:15:04,832 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-19680/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:15:05,650 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-19680/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:15:05,651 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-19680/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:15:05,651 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-19680/special_tokens_map.json
[2025-03-10 17:15:05,724] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step19686 is about to be saved!
[2025-03-10 17:15:05,729] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-19680/global_step19686/mp_rank_00_model_states.pt
[2025-03-10 17:15:05,730] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-19680/global_step19686/mp_rank_00_model_states.pt...
[2025-03-10 17:15:06,906] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-19680/global_step19686/mp_rank_00_model_states.pt.
[2025-03-10 17:15:06,907] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-19680/global_step19686/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:15:10,342] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-19680/global_step19686/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:15:10,343] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-19680/global_step19686/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:15:10,343] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step19686 is ready now!
{'loss': 0.0616, 'grad_norm': 3.771700620651245, 'learning_rate': 2e-05, 'epoch': 25.0}
 62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                               | 20500/32800 [1:22:29<47:34,  4.31it/s][INFO|trainer.py:930] 2025-03-10 17:18:20,157 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:18:20,160 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:18:20,160 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:18:20,160 >>   Batch size = 4
{'eval_loss': 1.0580419301986694, 'eval_mse': 1.057939026741623, 'eval_pearson': 0.762409740702042, 'eval_spearmanr': 0.7772206114762437, 'eval_runtime': 2.679, 'eval_samples_per_second': 516.616, 'eval_steps_per_second': 64.577, 'epoch': 25.0}
 62%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                                               | 20500/32800 [1:22:31<47:34,  4.31it/s]
[INFO|trainer.py:3955] 2025-03-10 17:18:23,074 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-20500
[INFO|configuration_utils.py:423] 2025-03-10 17:18:23,076 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-20500/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:18:23,859 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-20500/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:18:23,860 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-20500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:18:23,860 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-20500/special_tokens_map.json
[2025-03-10 17:18:23,924] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step20506 is about to be saved!
[2025-03-10 17:18:23,929] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-20500/global_step20506/mp_rank_00_model_states.pt
[2025-03-10 17:18:23,929] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-20500/global_step20506/mp_rank_00_model_states.pt...
[2025-03-10 17:18:24,981] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-20500/global_step20506/mp_rank_00_model_states.pt.
[2025-03-10 17:18:24,982] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-20500/global_step20506/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:18:28,411] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-20500/global_step20506/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:18:28,411] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-20500/global_step20506/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:18:28,411] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step20506 is ready now!
{'loss': 0.055, 'grad_norm': 2.420288324356079, 'learning_rate': 2e-05, 'epoch': 26.0}
 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                       | 21320/32800 [1:25:47<43:44,  4.37it/s][INFO|trainer.py:930] 2025-03-10 17:21:38,815 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:21:38,819 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:21:38,819 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:21:38,819 >>   Batch size = 4
{'eval_loss': 0.9746262431144714, 'eval_mse': 0.9741825842099383, 'eval_pearson': 0.772235679273105, 'eval_spearmanr': 0.7912723541456304, 'eval_runtime': 2.6814, 'eval_samples_per_second': 516.146, 'eval_steps_per_second': 64.518, 'epoch': 26.0}
 65%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                                                       | 21320/32800 [1:25:50<43:44,  4.37it/s]
[INFO|trainer.py:3955] 2025-03-10 17:21:41,739 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-21320
[INFO|configuration_utils.py:423] 2025-03-10 17:21:41,741 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-21320/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:21:42,550 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-21320/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:21:42,551 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-21320/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:21:42,551 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-21320/special_tokens_map.json
[2025-03-10 17:21:42,619] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step21326 is about to be saved!
[2025-03-10 17:21:42,624] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-21320/global_step21326/mp_rank_00_model_states.pt
[2025-03-10 17:21:42,624] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-21320/global_step21326/mp_rank_00_model_states.pt...
[2025-03-10 17:21:43,685] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-21320/global_step21326/mp_rank_00_model_states.pt.
[2025-03-10 17:21:43,780] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-21320/global_step21326/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:21:47,364] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-21320/global_step21326/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:21:47,364] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-21320/global_step21326/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:21:47,364] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step21326 is ready now!
{'loss': 0.0546, 'grad_norm': 0.7808613181114197, 'learning_rate': 2e-05, 'epoch': 27.0}
 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                              | 22140/32800 [1:29:06<40:29,  4.39it/s][INFO|trainer.py:930] 2025-03-10 17:24:57,789 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:24:57,792 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:24:57,792 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:24:57,792 >>   Batch size = 4
{'eval_loss': 0.9935322403907776, 'eval_mse': 0.99354838877055, 'eval_pearson': 0.7857464151749622, 'eval_spearmanr': 0.7966714502477931, 'eval_runtime': 2.6305, 'eval_samples_per_second': 526.145, 'eval_steps_per_second': 65.768, 'epoch': 27.0}
 68%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                                              | 22140/32800 [1:29:09<40:29,  4.39it/s]
[INFO|trainer.py:3955] 2025-03-10 17:25:00,722 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22140
[INFO|configuration_utils.py:423] 2025-03-10 17:25:00,724 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22140/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:25:01,480 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22140/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:25:01,481 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22140/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:25:01,481 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22140/special_tokens_map.json
[2025-03-10 17:25:01,548] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step22146 is about to be saved!
[2025-03-10 17:25:01,553] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22140/global_step22146/mp_rank_00_model_states.pt
[2025-03-10 17:25:01,553] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22140/global_step22146/mp_rank_00_model_states.pt...
[2025-03-10 17:25:02,531] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22140/global_step22146/mp_rank_00_model_states.pt.
[2025-03-10 17:25:02,533] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22140/global_step22146/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:25:06,269] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22140/global_step22146/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:25:06,269] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22140/global_step22146/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:25:06,269] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22146 is ready now!
{'loss': 0.0723, 'grad_norm': 0.685535728931427, 'learning_rate': 2e-05, 'epoch': 28.0}
 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                      | 22960/32800 [1:32:23<37:25,  4.38it/s][INFO|trainer.py:930] 2025-03-10 17:28:14,957 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:28:14,961 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:28:14,961 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:28:14,961 >>   Batch size = 4
{'eval_loss': 0.919819712638855, 'eval_mse': 0.9199041689751465, 'eval_pearson': 0.7798535512495315, 'eval_spearmanr': 0.7932659908130866, 'eval_runtime': 2.6878, 'eval_samples_per_second': 514.927, 'eval_steps_per_second': 64.366, 'epoch': 28.0}
 70%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                                                                                                      | 22960/32800 [1:32:26<37:25,  4.38it/s]
[INFO|trainer.py:3955] 2025-03-10 17:28:17,892 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22960
[INFO|configuration_utils.py:423] 2025-03-10 17:28:17,894 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22960/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:28:18,629 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22960/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:28:18,630 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22960/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:28:18,630 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22960/special_tokens_map.json
[2025-03-10 17:28:18,695] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step22967 is about to be saved!
[2025-03-10 17:28:18,700] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22960/global_step22967/mp_rank_00_model_states.pt
[2025-03-10 17:28:18,700] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22960/global_step22967/mp_rank_00_model_states.pt...
[2025-03-10 17:28:19,798] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22960/global_step22967/mp_rank_00_model_states.pt.
[2025-03-10 17:28:19,800] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22960/global_step22967/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:28:23,236] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22960/global_step22967/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:28:23,237] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-22960/global_step22967/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:28:23,237] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step22967 is ready now!
{'loss': 0.0505, 'grad_norm': 2.208045482635498, 'learning_rate': 2e-05, 'epoch': 29.0}
 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                             | 23780/32800 [1:35:42<35:32,  4.23it/s][INFO|trainer.py:930] 2025-03-10 17:31:34,035 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:31:34,038 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:31:34,038 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:31:34,038 >>   Batch size = 4
{'eval_loss': 0.9528667330741882, 'eval_mse': 0.9529338582402709, 'eval_pearson': 0.7816521159386662, 'eval_spearmanr': 0.7966785020482143, 'eval_runtime': 2.6841, 'eval_samples_per_second': 515.628, 'eval_steps_per_second': 64.453, 'epoch': 29.0}
 72%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                                             | 23780/32800 [1:35:45<35:32,  4.23it/s]
[INFO|trainer.py:3955] 2025-03-10 17:31:36,968 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-23780
[INFO|configuration_utils.py:423] 2025-03-10 17:31:36,971 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-23780/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:31:37,763 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-23780/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:31:37,764 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-23780/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:31:37,765 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-23780/special_tokens_map.json
[2025-03-10 17:31:37,835] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step23787 is about to be saved!
[2025-03-10 17:31:37,840] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-23780/global_step23787/mp_rank_00_model_states.pt
[2025-03-10 17:31:37,840] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-23780/global_step23787/mp_rank_00_model_states.pt...
[2025-03-10 17:31:38,969] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-23780/global_step23787/mp_rank_00_model_states.pt.
[2025-03-10 17:31:38,970] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-23780/global_step23787/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:31:42,382] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-23780/global_step23787/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:31:42,383] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-23780/global_step23787/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:31:42,383] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step23787 is ready now!
{'loss': 0.0538, 'grad_norm': 1.8786287307739258, 'learning_rate': 2e-05, 'epoch': 30.0}
 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                     | 24600/32800 [1:39:01<31:22,  4.36it/s][INFO|trainer.py:930] 2025-03-10 17:34:52,378 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:34:52,381 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:34:52,381 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:34:52,382 >>   Batch size = 4
{'eval_loss': 0.9637331366539001, 'eval_mse': 0.9637263313883302, 'eval_pearson': 0.7594662569039698, 'eval_spearmanr': 0.7771412943823365, 'eval_runtime': 2.7265, 'eval_samples_per_second': 507.615, 'eval_steps_per_second': 63.452, 'epoch': 30.0}
 75%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                                     | 24600/32800 [1:39:04<31:22,  4.36it/s]
[INFO|trainer.py:3955] 2025-03-10 17:34:55,346 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-24600
[INFO|configuration_utils.py:423] 2025-03-10 17:34:55,348 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-24600/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:34:56,229 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-24600/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:34:56,231 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-24600/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:34:56,231 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-24600/special_tokens_map.json
[2025-03-10 17:34:56,280] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step24607 is about to be saved!
[2025-03-10 17:34:56,286] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-24600/global_step24607/mp_rank_00_model_states.pt
[2025-03-10 17:34:56,286] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-24600/global_step24607/mp_rank_00_model_states.pt...
[2025-03-10 17:34:57,474] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-24600/global_step24607/mp_rank_00_model_states.pt.
[2025-03-10 17:34:57,476] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-24600/global_step24607/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:35:01,076] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-24600/global_step24607/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:35:01,076] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-24600/global_step24607/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:35:01,076] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step24607 is ready now!
{'loss': 0.0452, 'grad_norm': 3.014115571975708, 'learning_rate': 2e-05, 'epoch': 31.0}
 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                            | 25420/32800 [1:42:20<28:06,  4.38it/s][INFO|trainer.py:930] 2025-03-10 17:38:11,117 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:38:11,120 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:38:11,120 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:38:11,120 >>   Batch size = 4
{'eval_loss': 1.0721826553344727, 'eval_mse': 1.071909799396647, 'eval_pearson': 0.7479409430578522, 'eval_spearmanr': 0.7596932505877456, 'eval_runtime': 2.6376, 'eval_samples_per_second': 524.711, 'eval_steps_per_second': 65.589, 'epoch': 31.0}
 78%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                                            | 25420/32800 [1:42:22<28:06,  4.38it/s]
[INFO|trainer.py:3955] 2025-03-10 17:38:14,047 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-25420
[INFO|configuration_utils.py:423] 2025-03-10 17:38:14,049 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-25420/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:38:14,810 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-25420/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:38:14,811 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-25420/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:38:14,811 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-25420/special_tokens_map.json
[2025-03-10 17:38:14,880] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step25427 is about to be saved!
[2025-03-10 17:38:14,885] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-25420/global_step25427/mp_rank_00_model_states.pt
[2025-03-10 17:38:14,885] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-25420/global_step25427/mp_rank_00_model_states.pt...
[2025-03-10 17:38:15,961] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-25420/global_step25427/mp_rank_00_model_states.pt.
[2025-03-10 17:38:16,148] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-25420/global_step25427/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:38:19,522] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-25420/global_step25427/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:38:19,522] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-25420/global_step25427/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:38:19,523] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step25427 is ready now!
{'loss': 0.0606, 'grad_norm': 1.330518126487732, 'learning_rate': 2e-05, 'epoch': 32.0}
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 26240/32800 [1:45:36<24:55,  4.39it/s][INFO|trainer.py:930] 2025-03-10 17:41:27,818 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:41:27,821 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:41:27,821 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:41:27,821 >>   Batch size = 4
{'eval_loss': 1.0396922826766968, 'eval_mse': 1.0390768848747187, 'eval_pearson': 0.7688830143321433, 'eval_spearmanr': 0.7857948836132445, 'eval_runtime': 2.6293, 'eval_samples_per_second': 526.367, 'eval_steps_per_second': 65.796, 'epoch': 32.0}
 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                                    | 26240/32800 [1:45:39<24:55,  4.39it/s]
[INFO|trainer.py:3955] 2025-03-10 17:41:30,747 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-26240
[INFO|configuration_utils.py:423] 2025-03-10 17:41:30,749 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-26240/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:41:31,525 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-26240/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:41:31,526 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-26240/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:41:31,526 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-26240/special_tokens_map.json
[2025-03-10 17:41:31,588] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step26248 is about to be saved!
[2025-03-10 17:41:31,594] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-26240/global_step26248/mp_rank_00_model_states.pt
[2025-03-10 17:41:31,594] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-26240/global_step26248/mp_rank_00_model_states.pt...
[2025-03-10 17:41:32,684] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-26240/global_step26248/mp_rank_00_model_states.pt.
[2025-03-10 17:41:32,685] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-26240/global_step26248/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:41:35,945] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-26240/global_step26248/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:41:35,946] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-26240/global_step26248/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:41:35,946] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step26248 is ready now!
{'loss': 0.0406, 'grad_norm': 2.1025710105895996, 'learning_rate': 2e-05, 'epoch': 33.0}
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 27060/32800 [1:48:54<22:04,  4.33it/s][INFO|trainer.py:930] 2025-03-10 17:44:45,292 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:44:45,296 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:44:45,296 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:44:45,296 >>   Batch size = 4
{'eval_loss': 0.8847517967224121, 'eval_mse': 0.8845161051419429, 'eval_pearson': 0.8142514499919737, 'eval_spearmanr': 0.8264533864945652, 'eval_runtime': 2.631, 'eval_samples_per_second': 526.042, 'eval_steps_per_second': 65.755, 'epoch': 33.0}
 82%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                                           | 27060/32800 [1:48:56<22:04,  4.33it/s]
[INFO|trainer.py:3955] 2025-03-10 17:44:48,215 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27060
[INFO|configuration_utils.py:423] 2025-03-10 17:44:48,217 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27060/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:44:48,936 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27060/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:44:48,937 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27060/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:44:48,937 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27060/special_tokens_map.json
[2025-03-10 17:44:49,000] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step27068 is about to be saved!
[2025-03-10 17:44:49,005] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27060/global_step27068/mp_rank_00_model_states.pt
[2025-03-10 17:44:49,006] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27060/global_step27068/mp_rank_00_model_states.pt...
[2025-03-10 17:44:50,100] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27060/global_step27068/mp_rank_00_model_states.pt.
[2025-03-10 17:44:50,179] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27060/global_step27068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:44:53,598] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27060/global_step27068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:44:53,598] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27060/global_step27068/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:44:53,598] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27068 is ready now!
{'loss': 0.0371, 'grad_norm': 1.4016104936599731, 'learning_rate': 2e-05, 'epoch': 34.0}
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 27880/32800 [1:52:12<18:41,  4.39it/s][INFO|trainer.py:930] 2025-03-10 17:48:03,305 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:48:03,308 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:48:03,308 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:48:03,308 >>   Batch size = 4
{'eval_loss': 0.9132531881332397, 'eval_mse': 0.9133020008919556, 'eval_pearson': 0.8080355185170673, 'eval_spearmanr': 0.8227620654922109, 'eval_runtime': 2.6792, 'eval_samples_per_second': 516.565, 'eval_steps_per_second': 64.571, 'epoch': 34.0}
 85%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                   | 27880/32800 [1:52:14<18:41,  4.39it/s]
[INFO|trainer.py:3955] 2025-03-10 17:48:06,226 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27880
[INFO|configuration_utils.py:423] 2025-03-10 17:48:06,228 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27880/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:48:07,064 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27880/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:48:07,065 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27880/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:48:07,065 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27880/special_tokens_map.json
[2025-03-10 17:48:07,124] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step27888 is about to be saved!
[2025-03-10 17:48:07,129] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27880/global_step27888/mp_rank_00_model_states.pt
[2025-03-10 17:48:07,129] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27880/global_step27888/mp_rank_00_model_states.pt...
[2025-03-10 17:48:08,082] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27880/global_step27888/mp_rank_00_model_states.pt.
[2025-03-10 17:48:08,084] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27880/global_step27888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:48:11,321] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27880/global_step27888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:48:11,321] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-27880/global_step27888/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:48:11,321] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step27888 is ready now!
{'loss': 0.0367, 'grad_norm': 1.1239032745361328, 'learning_rate': 2e-05, 'epoch': 35.0}
 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 28700/32800 [1:55:27<15:35,  4.38it/s][INFO|trainer.py:930] 2025-03-10 17:51:18,750 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:51:18,753 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:51:18,753 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:51:18,753 >>   Batch size = 4
{'eval_loss': 0.9398888349533081, 'eval_mse': 0.9401093779960809, 'eval_pearson': 0.7759657264850855, 'eval_spearmanr': 0.7930109562159171, 'eval_runtime': 2.6284, 'eval_samples_per_second': 526.556, 'eval_steps_per_second': 65.819, 'epoch': 35.0}
 88%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                                          | 28700/32800 [1:55:30<15:35,  4.38it/s]
[INFO|trainer.py:3955] 2025-03-10 17:51:21,661 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-28700
[INFO|configuration_utils.py:423] 2025-03-10 17:51:21,663 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-28700/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:51:22,481 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-28700/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:51:22,481 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-28700/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:51:22,482 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-28700/special_tokens_map.json
[2025-03-10 17:51:22,542] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step28708 is about to be saved!
[2025-03-10 17:51:22,547] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-28700/global_step28708/mp_rank_00_model_states.pt
[2025-03-10 17:51:22,547] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-28700/global_step28708/mp_rank_00_model_states.pt...
[2025-03-10 17:51:23,536] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-28700/global_step28708/mp_rank_00_model_states.pt.
[2025-03-10 17:51:23,538] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-28700/global_step28708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:51:26,844] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-28700/global_step28708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:51:26,844] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-28700/global_step28708/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:51:26,844] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step28708 is ready now!
{'loss': 0.0468, 'grad_norm': 0.6587243676185608, 'learning_rate': 2e-05, 'epoch': 36.0}
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 29520/32800 [1:58:43<12:39,  4.32it/s][INFO|trainer.py:930] 2025-03-10 17:54:34,722 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:54:34,725 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:54:34,725 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:54:34,725 >>   Batch size = 4
{'eval_loss': 0.8787352442741394, 'eval_mse': 0.8787586523618313, 'eval_pearson': 0.7982793209550004, 'eval_spearmanr': 0.815630913434008, 'eval_runtime': 2.6301, 'eval_samples_per_second': 526.224, 'eval_steps_per_second': 65.778, 'epoch': 36.0}
 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                  | 29520/32800 [1:58:46<12:39,  4.32it/s]
[INFO|trainer.py:3955] 2025-03-10 17:54:37,639 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-29520
[INFO|configuration_utils.py:423] 2025-03-10 17:54:37,641 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-29520/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:54:38,434 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-29520/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:54:38,435 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-29520/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:54:38,435 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-29520/special_tokens_map.json
[2025-03-10 17:54:38,502] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step29529 is about to be saved!
[2025-03-10 17:54:38,507] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-29520/global_step29529/mp_rank_00_model_states.pt
[2025-03-10 17:54:38,507] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-29520/global_step29529/mp_rank_00_model_states.pt...
[2025-03-10 17:54:39,528] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-29520/global_step29529/mp_rank_00_model_states.pt.
[2025-03-10 17:54:39,530] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-29520/global_step29529/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:54:42,884] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-29520/global_step29529/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:54:42,884] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-29520/global_step29529/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:54:42,884] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step29529 is ready now!
{'loss': 0.037, 'grad_norm': 2.2294716835021973, 'learning_rate': 2e-05, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 30340/32800 [2:02:01<09:36,  4.27it/s][INFO|trainer.py:930] 2025-03-10 17:57:52,954 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 17:57:52,957 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 17:57:52,957 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 17:57:52,957 >>   Batch size = 4
{'eval_loss': 0.9053379893302917, 'eval_mse': 0.905527627709284, 'eval_pearson': 0.7821073551688291, 'eval_spearmanr': 0.7958038515894275, 'eval_runtime': 2.7027, 'eval_samples_per_second': 512.079, 'eval_steps_per_second': 64.01, 'epoch': 37.0}
 92%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                         | 30340/32800 [2:02:04<09:36,  4.27it/s]
[INFO|trainer.py:3955] 2025-03-10 17:57:55,957 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-30340
[INFO|configuration_utils.py:423] 2025-03-10 17:57:55,959 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-30340/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 17:57:56,793 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-30340/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 17:57:56,794 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-30340/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 17:57:56,794 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-30340/special_tokens_map.json
[2025-03-10 17:57:56,857] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step30349 is about to be saved!
[2025-03-10 17:57:56,863] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-30340/global_step30349/mp_rank_00_model_states.pt
[2025-03-10 17:57:56,863] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-30340/global_step30349/mp_rank_00_model_states.pt...
[2025-03-10 17:57:57,952] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-30340/global_step30349/mp_rank_00_model_states.pt.
[2025-03-10 17:57:57,953] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-30340/global_step30349/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 17:58:01,400] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-30340/global_step30349/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 17:58:01,401] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-30340/global_step30349/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 17:58:01,401] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step30349 is ready now!
{'loss': 0.0338, 'grad_norm': 1.2914962768554688, 'learning_rate': 2e-05, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 31160/32800 [2:05:21<06:20,  4.32it/s][INFO|trainer.py:930] 2025-03-10 18:01:12,457 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:01:12,461 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:01:12,461 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 18:01:12,461 >>   Batch size = 4
{'eval_loss': 1.092093586921692, 'eval_mse': 1.0915490282064229, 'eval_pearson': 0.7631099652162783, 'eval_spearmanr': 0.7808939072483556, 'eval_runtime': 2.6319, 'eval_samples_per_second': 525.849, 'eval_steps_per_second': 65.731, 'epoch': 38.0}
 95%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████                 | 31160/32800 [2:05:23<06:20,  4.32it/s]
[INFO|trainer.py:3955] 2025-03-10 18:01:15,390 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31160
[INFO|configuration_utils.py:423] 2025-03-10 18:01:15,392 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31160/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:01:16,345 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31160/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:01:16,346 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31160/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:01:16,346 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31160/special_tokens_map.json
[2025-03-10 18:01:16,412] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step31169 is about to be saved!
[2025-03-10 18:01:16,417] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31160/global_step31169/mp_rank_00_model_states.pt
[2025-03-10 18:01:16,417] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31160/global_step31169/mp_rank_00_model_states.pt...
[2025-03-10 18:01:17,558] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31160/global_step31169/mp_rank_00_model_states.pt.
[2025-03-10 18:01:17,560] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31160/global_step31169/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:01:20,907] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31160/global_step31169/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:01:20,907] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31160/global_step31169/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:01:20,907] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31169 is ready now!
{'loss': 0.0349, 'grad_norm': 1.2441332340240479, 'learning_rate': 2e-05, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 31980/32800 [2:08:40<03:13,  4.23it/s][INFO|trainer.py:930] 2025-03-10 18:04:31,943 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:04:31,947 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:04:31,947 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 18:04:31,947 >>   Batch size = 4
{'eval_loss': 0.9457695484161377, 'eval_mse': 0.9452932073890818, 'eval_pearson': 0.803399424246279, 'eval_spearmanr': 0.8178882749852872, 'eval_runtime': 2.669, 'eval_samples_per_second': 518.544, 'eval_steps_per_second': 64.818, 'epoch': 39.0}
 98%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌        | 31980/32800 [2:08:43<03:13,  4.23it/s]
[INFO|trainer.py:3955] 2025-03-10 18:04:34,931 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31980
[INFO|configuration_utils.py:423] 2025-03-10 18:04:34,933 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31980/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:04:35,726 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31980/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:04:35,727 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31980/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:04:35,727 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31980/special_tokens_map.json
[2025-03-10 18:04:35,791] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step31989 is about to be saved!
[2025-03-10 18:04:35,796] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31980/global_step31989/mp_rank_00_model_states.pt
[2025-03-10 18:04:35,796] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31980/global_step31989/mp_rank_00_model_states.pt...
[2025-03-10 18:04:36,859] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31980/global_step31989/mp_rank_00_model_states.pt.
[2025-03-10 18:04:36,860] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31980/global_step31989/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:04:40,459] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31980/global_step31989/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:04:40,460] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-31980/global_step31989/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:04:40,460] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step31989 is ready now!
{'loss': 0.0385, 'grad_norm': 6.984105587005615, 'learning_rate': 2e-05, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32800/32800 [2:11:59<00:00,  4.40it/s][INFO|trainer.py:930] 2025-03-10 18:07:50,916 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:07:50,919 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:07:50,919 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 18:07:50,919 >>   Batch size = 4
{'eval_loss': 0.839276909828186, 'eval_mse': 0.8387363938926962, 'eval_pearson': 0.8100559550854988, 'eval_spearmanr': 0.8215554820777464, 'eval_runtime': 2.6395, 'eval_samples_per_second': 524.349, 'eval_steps_per_second': 65.544, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32800/32800 [2:12:02<00:00,  4.40it/s]
[INFO|trainer.py:3955] 2025-03-10 18:07:53,843 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-32800
[INFO|configuration_utils.py:423] 2025-03-10 18:07:53,846 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-32800/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:07:54,576 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-32800/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:07:54,577 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-32800/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:07:54,577 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-32800/special_tokens_map.json
[2025-03-10 18:07:54,644] [INFO] [logging.py:128:log_dist] [Rank 0] [Torch] Checkpoint global_step32809 is about to be saved!
[2025-03-10 18:07:54,649] [INFO] [logging.py:128:log_dist] [Rank 0] Saving model checkpoint: output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-32800/global_step32809/mp_rank_00_model_states.pt
[2025-03-10 18:07:54,650] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-32800/global_step32809/mp_rank_00_model_states.pt...
[2025-03-10 18:07:55,706] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-32800/global_step32809/mp_rank_00_model_states.pt.
[2025-03-10 18:07:55,707] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-32800/global_step32809/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2025-03-10 18:07:59,124] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-32800/global_step32809/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2025-03-10 18:07:59,125] [INFO] [engine.py:3645:_save_zero_checkpoint] zero checkpoint saved output/GNER-QE/FacebookAI/roberta-large-num=20/checkpoint-32800/global_step32809/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2025-03-10 18:07:59,125] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint global_step32809 is ready now!
[INFO|trainer.py:2670] 2025-03-10 18:07:59,168 >>

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 7928.0712, 'train_samples_per_second': 132.456, 'train_steps_per_second': 4.137, 'train_loss': 0.24418995136167945, 'epoch': 40.0}
100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 32800/32800 [2:12:08<00:00,  4.14it/s]
[INFO|trainer.py:3955] 2025-03-10 18:07:59,380 >> Saving model checkpoint to output/GNER-QE/FacebookAI/roberta-large-num=20
[INFO|configuration_utils.py:423] 2025-03-10 18:07:59,383 >> Configuration saved in output/GNER-QE/FacebookAI/roberta-large-num=20/config.json
[INFO|modeling_utils.py:3138] 2025-03-10 18:08:00,228 >> Model weights saved in output/GNER-QE/FacebookAI/roberta-large-num=20/model.safetensors
[INFO|tokenization_utils_base.py:2510] 2025-03-10 18:08:00,229 >> tokenizer config file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/tokenizer_config.json
[INFO|tokenization_utils_base.py:2519] 2025-03-10 18:08:00,229 >> Special tokens file saved in output/GNER-QE/FacebookAI/roberta-large-num=20/special_tokens_map.json
***** train metrics *****
  epoch                    =     39.9994
  total_flos               = 911245696GF
  train_loss               =      0.2442
  train_runtime            =  2:12:08.07
  train_samples            =       26253
  train_samples_per_second =     132.456
  train_steps_per_second   =       4.137
03/10/2025 18:08:00 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:930] 2025-03-10 18:08:00,298 >> The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.
[INFO|trainer.py:4271] 2025-03-10 18:08:00,301 >>
***** Running Evaluation *****
[INFO|trainer.py:4273] 2025-03-10 18:08:00,301 >>   Num examples = 1384
[INFO|trainer.py:4276] 2025-03-10 18:08:00,301 >>   Batch size = 4
100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 173/173 [00:02<00:00, 63.97it/s]
***** eval metrics *****
  epoch                   =    39.9994
  eval_loss               =     0.8393
  eval_mse                =     0.8387
  eval_pearson            =     0.8101
  eval_runtime            = 0:00:02.72
  eval_samples            =       1384
  eval_samples_per_second =    507.399
  eval_spearmanr          =     0.8216
  eval_steps_per_second   =     63.425
[INFO|modelcard.py:449] 2025-03-10 18:08:03,267 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Spearmanr', 'type': 'spearmanr', 'value': 0.8215554820777464}]}
[rank0]:[W310 18:08:03.436829619 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())
[2025-03-10 18:08:07,392] [INFO] [launch.py:351:main] Process 4171060 exits successfully.
[2025-03-10 18:08:07,392] [INFO] [launch.py:351:main] Process 4171059 exits successfully.
(GNER) chrisjihee@dgx-a100:~/proj/GNER$

